<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer025">
<h1 class="chapter-number" id="_idParaDest-21"><a id="_idTextAnchor020"/>1</h1>
<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>An Overview of Regularization</h1>
<p>Let’s embark on a journey into the world of regularization in machine learning. I hope you will learn a lot and find as much joy in reading this book as I did in <span class="No-Break">writing it.</span></p>
<p>Regularization is important for any individual willing to deploy<a id="_idIndexMarker000"/> robust <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) models.</span></p>
<p>This chapter will introduce some context and key concepts about regularization before diving deeper into it in the next chapters. At this point, you may have many questions about this book and about regularization in general. What is regularization? Why do we need regularization for production-grade ML models? How do we diagnose the need for regularization? What are the limits of regularization? What are the approaches <span class="No-Break">to regularization?</span></p>
<p>All the foundational knowledge about regularization will be provided in this chapter in the hope of answering all these questions. Not only will this give you a high-level understanding of what regularization is but it will also allow you to fully appreciate the methods and techniques proposed in the next chapters of <span class="No-Break">this book.</span></p>
<p>In this chapter, we are going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li><span class="No-Break">Introducing regularization</span></li>
<li>Developing intuition about regularization on a <span class="No-Break">toy dataset</span></li>
<li>Introducing the key concepts of underfitting, overfitting, bias, <span class="No-Break">and variance</span></li>
</ul>
<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Technical requirements</h1>
<p>In this chapter, you will have the opportunity to generate a toy dataset, display it, and train basic linear regression on that data. Therefore, the following Python libraries will <span class="No-Break">be required:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">scikit-learn</span></li>
</ul>
<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Introducing regularization</h1>
<p class="author-quote">“Regularization in ML is a technique used to improve the generalization performance of a model by adding additional constraints to the model’s parameters. This forces the model to use simpler representations and helps reduce the risk of overfitting.</p>
<p class="author-quote">Regularization can also help improve the performance of a model on unseen data by encouraging the model to learn more relevant, generalizable features.”</p>
<p>This definition of<a id="_idIndexMarker001"/> regularization, arguably good enough, was actually generated by the famous GPT-3 model when given the following prompt: <em class="italic">Detailed definition of regularization in machine learning</em>. Even more astonishing, this definition passed several plagiarism tests, meaning it’s actually fully original text. Do not worry if you do not yet understand all the words in this definition from GPT-3; it is not meant for beginners. But you will fully understand it by the end of <span class="No-Break">this chapter.</span></p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">GPT-3</strong>, short<a id="_idIndexMarker002"/> for <strong class="bold">Generative Pre-trained Transformer 3</strong>, is a 175 billion-parameter model proposed by OpenAI and is available for use <span class="No-Break">at </span><a href="https://platform.openai.com/playground"><span class="No-Break">platform.openai.com/playground</span></a><span class="No-Break">.</span></p>
<p>You can easily imagine that, to get such a result, not only has GPT-3 been trained on a large amount of data but it is really carefully regularized, so that it won’t just reproduce a learned text but will instead generate a <span class="No-Break">new one.</span></p>
<p>This is exactly what regularization is about: being able to generalize and produce acceptable results when faced with an <span class="No-Break">unknown situation.</span></p>
<p>Why is regularization so crucial to ML? The key to successfully deploying ML in production lies in the model’s ability to effectively adapt to and accommodate new data. Once a model is in production, it will not be fed with well-known, average input data. Most likely, the production model will face unseen data, exceptional scenarios, a drift in feature distribution, or evolving customer behavior. While a well-regularized ML model may not guarantee its robustness in handling various scenarios, a poorly regularized model is almost <a id="_idIndexMarker003"/>certain to encounter failure upon its initial deployment <span class="No-Break">in production.</span></p>
<p>Let’s now have a look at a few examples of models that failed during deployment in recent years, so that we can fully understand why regularization is <span class="No-Break">so important.</span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Examples of models that did not pass the deployment test</h2>
<p>The last few years have <a id="_idIndexMarker004"/>been full of examples of models that failed in the first days of deployment. According to a Gartner report from 2020 (<a href="https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline">https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline</a>), more than 50% of AI prototypes will <em class="italic">not</em> make it to production deployment. Not all failures were only due to regularization issues, but some <span class="No-Break">certainly were.</span></p>
<p>Let’s have a quick look at some failed attempts at deploying models in production over the last <span class="No-Break">few years:</span></p>
<ul>
<li>Amazon had to stop using its AI recruitment model because it was reportedly discriminating against <span class="No-Break">women (</span><a href="https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&amp;guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe"><span class="No-Break">https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&amp;guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe</span></a><span class="No-Break">)</span></li>
<li>Microsoft’s chatbot Tay was shut down after only 16 hours of production after posting offensive <span class="No-Break">tweets (</span><a href="https://en.wikipedia.org/wiki/Tay_(chatbot)"><span class="No-Break">https://en.wikipedia.org/wiki/Tay_(chatbot)</span></a><span class="No-Break">)</span></li>
<li>IBM’s Watson was providing unsafe cancer treatment recommendations to <span class="No-Break">patients (</span><a href="https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science"><span class="No-Break">https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science</span></a><span class="No-Break">)</span></li>
</ul>
<p>Those are just a few examples that made the headlines from tech giants. The number of projects that have experienced failure yet remain undisclosed to the public is staggering. These failures often involve smaller companies<a id="_idIndexMarker005"/> and confidential initiatives. But still, there are several lessons to learn from <span class="No-Break">those examples:</span></p>
<ul>
<li><strong class="bold">Amazon’s case</strong>: The input data<a id="_idIndexMarker006"/> was biased against women, as was <span class="No-Break">the model</span></li>
<li><strong class="bold">Microsoft’s case</strong>: The model<a id="_idIndexMarker007"/> was probably too sensitive to new data since it was feeding on <span class="No-Break">new tweets</span></li>
<li><strong class="bold">IBM’s case</strong>: The model was <a id="_idIndexMarker008"/>perhaps trained on too much synthetic or unrealistic data, and not able to adapt to edge cases and <span class="No-Break">unseen data</span></li>
</ul>
<p>Regularization serves as a valuable approach to enhance the success rate of ML models in production. Effective regularization techniques can prevent AI recruitment models from exhibiting gender biases, either by eliminating certain features or incorporating synthetic data. Additionally, proper regularization enables chatbots to maintain an appropriate level of sensitivity toward new tweets. It also equips models to handle edge cases and previously unseen data proficiently, even when trained on <span class="No-Break">synthetic data.</span></p>
<p>As a disclaimer, there may be many other ways to overcome or prevent these kinds of failures that are not mutually exclusive with regularization. For example, having good-quality data is key. Everyone in the AI field knows the adage <em class="italic">garbage in, </em><span class="No-Break"><em class="italic">garbage out</em></span><span class="No-Break">.</span></p>
<p>MLOps (a field that is getting more and more mature every day) and ML engineering best practices are also key to success for many projects. Subject matter knowledge can sometimes also make <span class="No-Break">a difference.</span></p>
<p>Depending on the context of the project, many other parameters may impact the success of an ML project, but anything besides regularization is beyond the scope of <span class="No-Break">this book.</span></p>
<p>Now that we understand<a id="_idIndexMarker009"/> the need for regularization for production-level ML models, let’s take a step back and gain some intuition about what regularization is with a <span class="No-Break">simple example.</span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Intuition about regularization</h2>
<p>Regularization has been <a id="_idIndexMarker010"/>defined and mentioned already in this book, but let’s try now to develop some intuition about what it <span class="No-Break">really is.</span></p>
<p>Let us consider a typical, real-world use case: the real estate price per square meter, as a function of the surface of an apartment (or house) in the city of Paris. The goal, from a business perspective, is to be able to predict the price per square meter, given the <span class="No-Break">apartment’s surface.</span></p>
<p>We will first need some imports, as well as a helper function to plot the data <span class="No-Break">more conveniently.</span></p>
<p>The <strong class="source-inline">plot_data()</strong> function simply plots the provided data and adds axis labels and a legend <span class="No-Break">if needed:</span></p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
np.random.seed(42)
def plot_data(surface: np.array, price: np.array,
    fit: np.array = None, legend: bool = False):
    plt.scatter(surface, price, label='data')
    if fit is not None:
        plt.plot(surface, fit, label='fit')
    if legend:
        plt.legend()
    plt.ylim(11300, 11550)
    plt.xlabel('Surface (m$^{2}$)')
    plt.ylabel('Price (€/m$^{2}$)')
    plt.grid(True)
    plt.show()</pre>
<p>The following code will now allow us to generate and display our first <span class="No-Break">toy dataset:</span></p>
<pre class="source-code">
# Define the surfaces and prices
surface = np.array([15, 17, 20, 22, 25, 28]).reshape(-1, 1)
price = 12000 - surface*50 + np.square(
    surface) + np.random.normal(0, 30, surface.shape)
# Plot the data
plot_data(surface, price)</pre>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer005">
<img alt="Figure 1.1 – Price per square meter as a function of the apartment surface" height="909" src="image/B19629_01_01.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Price per square meter as a function of the apartment surface</p>
<p>Even if this is a toy dataset, for the sake of pedagogy, we can assume this data would have been collected on the real <span class="No-Break">estate market.</span></p>
<p>We can notice a downward trend in the price per square meter as the apartment surface increases. Indeed, in Paris, the small surfaces <a id="_idIndexMarker011"/>are much more in demand (perhaps because there are many students or because the price is more affordable). That could explain why the price per square meter is actually higher for <span class="No-Break">smaller surfaces.</span></p>
<p>For simplicity, we will omit all the typical ML workflow. Here, we just perform a linear regression on this data and display the result with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
# Perform a linear regression on the data
lr = LinearRegression()
lr.fit(surface, price)
# Compute prediction
y_pred = lr.predict(surface)
# Plot data
plot_data(surface, price, y_pred, True)</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer006">
<img alt="Figure 1.2 – Price per square meter as a function of the apartment surface and resulting fit curve" height="909" src="image/B19629_01_02.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Price per square meter as a function of the apartment surface and resulting fit curve</p>
<p>Good enough! The fit seems to capture the downward trend. While it’s not so close to all the given data samples, the business seems happy about it for the moment, as the model performances are within <span class="No-Break">their expectations.</span></p>
<p>Thanks to this new model, the company has now acquired a few more clients. From those clients, the company collected <a id="_idIndexMarker012"/>some new data from larger apartment sales, so our dataset now looks like <span class="No-Break">the following:</span></p>
<pre class="source-code">
# Generate data
updated_surface = np.array([15, 17, 20, 22, 25, 28, 30, 33,
    35, 37]).reshape(-1, 1)
updated_price = 12000 - updated_surface*50 + np.square(
    updated_surface) + np.random.normal(0, 30, updated_surface.shape)
# Plot data
plot_data(updated_surface, updated_price)</pre>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer007">
<img alt="Figure 1.3 – Updated price per square meter as a function of the apartment surface" height="909" src="image/B19629_01_03.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Updated price per square meter as a function of the apartment surface</p>
<p>This actually changes everything; this is the typical failing deployment test. There is no global downward trend anymore: with larger apartment surfaces, the price per square meter actually seems to follow an upward trend. One simple business explanation could be the following: larger apartments may be less common and thus have a <span class="No-Break">higher price.</span></p>
<p>Without confidence, for <a id="_idIndexMarker013"/>the sake of trying, we try to reuse the exact same method as we did previously: linear regression. The result would be <span class="No-Break">the following:</span></p>
<pre class="source-code">
# Perform linear regression and plot result
lr = LinearRegression()
lr.fit(updated_surface, updated_price)
y_pred_updated = lr.predict(updated_surface)
plot_data(updated_surface, updated_price, y_pred_updated, True)</pre>
<p>Here is <span class="No-Break">the plot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.4 – Example of underfitting: the data complexity is not fully captured by the model" height="909" src="image/B19629_01_04.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Example of underfitting: the data complexity is not fully captured by the model</p>
<p>As expected, linear regression was not able to capture the complexity of the data anymore, leading to this situation. This is called <strong class="bold">underfitting</strong>; the model <a id="_idIndexMarker014"/>is not able to fully capture the complexity of the data. Indeed, with just the surface as a parameter, the best the model can do is a straight line, which is not enough for <span class="No-Break">this data.</span></p>
<p>One way for linear regression to capture more complexity is to provide more features. Given that our current input data is <a id="_idIndexMarker015"/>limited to the surface, a potential straightforward approach could involve utilizing the raised-to-the-power surface. For the sake of this example, let’s take a rather extreme approach and add all the features from <strong class="source-inline">power1</strong> to <strong class="source-inline">power15</strong>, and make them fit this dataset. This can be done pretty easily with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
# Compute power up to 15
x_power15 = np.concatenate([np.power(
    updated_surface, i+1) for i in range(15)], 1)
# Perform linear regression and plot result
lr = LinearRegression()
lr.fit(x_power15, updated_price)
y_pred_power15 = lr.predict(x_power15)
plot_data(updated_surface, updated_price, y_pred_power15, True)</pre>
<p>Here is the output <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.5 – Example of overfitting: the model is capturing the noise in the data" height="909" src="image/B19629_01_05.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Example of overfitting: the model is capturing the noise in the data</p>
<p>As we could expect by adding so many degrees of freedom in our model, the fit is now going exactly through all of the data points. Indeed, without going into the mathematical details, the model has more parameters than data points on which it trains, and is capable of going through all those points. Is it a good fit, though? We can imagine it does not only capture the global trend of the data but also the noise. This is called <strong class="bold">overfitting</strong>: the <a id="_idIndexMarker016"/>model is<a id="_idIndexMarker017"/> too close to the data, and may not be able to make correct predictions for new, unseen data. Any new data point would not be on the curve, and the behavior outside the training range is <span class="No-Break">totally unpredictable.</span></p>
<p>Finally, a more reasonable approach to this situation would be only taking the surface up to the power of 2, <span class="No-Break">for example:</span></p>
<pre class="source-code">
# Compute power up to 2
x_power2 = np.concatenate([np.power(
    updated_surface, i+1) for i in range(2)], 1)
# Perform linear regression and plot result
lr = LinearRegression()
lr.fit(x_power2, updated_price)
y_pred_power2 = lr.predict(x_power2)
plot_data(updated_surface, updated_price, y_pred_power2, True)</pre>
<p>Here is the output <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.6 – Example of right fitting: the model is capturing the overall trend but not the noise" height="909" src="image/B19629_01_06.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Example of right fitting: the model is capturing the overall trend but not the noise</p>
<p>The fit seems much more acceptable now. It does capture the global trend of the data: at first downward for small surfaces, and then upward for larger surfaces. Moreover, it does not try to capture the noise coming from all the data points, making it more robust for predicting new, unseen data. Finally, the behavior is quite predictable beyond the training range (for example, surface &lt; 15<img alt="" height="21" src="image/Formula_01_001.png" width="29"/> or surface &gt; <span class="No-Break">40</span><span class="No-Break"><img alt="" height="22" src="image/Formula_01_002.png" width="30"/>).</span></p>
<p>This is typically the kind of desired behavior from a good model: neither underfitting nor overfitting. Removing some of the raised-to-the-power features allowed our model to generalize better; we effectively<a id="_idIndexMarker018"/> regularized <span class="No-Break">our model.</span></p>
<p>To summarize this example, we explored several <span class="No-Break">concepts here:</span></p>
<ul>
<li>We visualized examples of underfitting, overfitting, and <span class="No-Break">well-regularized models</span></li>
<li>By adding a raised-to-the-power surface to our features, we were able to go from underfitting <span class="No-Break">to overfitting</span></li>
<li>Finally, by removing most of the raised-to-the-power features (keeping only square features), we were able to go from overfitting to a well-regularized model, effectively <span class="No-Break">adding regularization</span></li>
</ul>
<p>Hopefully, you now have a good understanding of underfitting, overfitting, and regularization, as well as why this is so important in ML. We can now build upon this by providing a more formal definition of the key concepts <span class="No-Break">of regularization.</span></p>
<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Key concepts of regularization</h1>
<p>Having gained some intuition <a id="_idIndexMarker019"/>regarding what constitutes a suitable fit, as well as understanding examples of underfitting and overfitting, let us now delve into a more precise definition and explore key concepts that enable us to better <span class="No-Break">comprehend regularization.</span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Bias and variance</h2>
<p>Bias and variance are two<a id="_idIndexMarker020"/> key concepts<a id="_idIndexMarker021"/> when talking about regularization. We can define two main kinds of errors a model <span class="No-Break">can have:</span></p>
<ul>
<li><strong class="bold">Bias</strong> is how bad a model is at<a id="_idIndexMarker022"/> capturing the general behavior of <span class="No-Break">the data</span></li>
<li><strong class="bold">Variance</strong> is how bad a <a id="_idIndexMarker023"/>model is at being robust to small input <span class="No-Break">data fluctuations</span></li>
</ul>
<p>Those two concepts, in general, are not mutually exclusive. If we take a step back from ML, there is a very common figure to visualize bias and variance, assuming the model’s goal is to hit the center of <span class="No-Break">a target:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.7 – Visualization of bias and variance" height="1050" src="image/B19629_01_07.jpg" width="965"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Visualization of bias and variance</p>
<p>Let’s describe those <span class="No-Break">four cases:</span></p>
<ul>
<li><strong class="bold">High bias and low variance</strong>: The model is <a id="_idIndexMarker024"/>hitting away from the center of the target, but in a very <span class="No-Break">consistent manner</span></li>
<li><strong class="bold">Low bias and high variance</strong>: The model is, on average, hitting the center of the target, but is quite noisy and inconsistent in <span class="No-Break">doing so</span></li>
<li><strong class="bold">High bias and high variance</strong>: The <a id="_idIndexMarker025"/>model is hitting away from the center in a <span class="No-Break">noisy way</span></li>
<li><strong class="bold">Low bias and low variance</strong>: The best of both worlds – the model is hitting the center of the <span class="No-Break">target consistently</span></li>
</ul>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Underfitting and overfitting</h2>
<p>We saw a very classic <a id="_idIndexMarker026"/>approach<a id="_idIndexMarker027"/> to bias and <span class="No-Break">variance definition.</span></p>
<p>But now, what does that mean in terms of ML? How does that relate to regularization? Well, before we get there, we will first revisit bias and variance in a more typical ML case: linear regression of real <span class="No-Break">estate prices.</span></p>
<p>Let’s have a look at how a model would behave in all those cases on <span class="No-Break">our data.</span></p>
<h3>High bias and low variance</h3>
<p>The model is robust<a id="_idIndexMarker028"/> to data fluctuations but could not capture the high-level behavior of the data. Refer to the <span class="No-Break">following graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.8 – High bias and low variance in practice for linear regression" height="498" src="image/B19629_01_08.jpg" width="1026"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – High bias and low variance in practice for linear regression</p>
<p>This is <em class="italic">underfitting</em>, as we faced earlier, in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<h3>Low bias and high variance</h3>
<p>The model did capture the global behavior of the data, but could not stay robust to input data fluctuations. Refer to the <span class="No-Break">following graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.9 – Low bias and high variance in practice for linear regression" height="498" src="image/B19629_01_09.jpg" width="1145"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Low bias and high variance in practice for linear regression</p>
<p>This is <em class="italic">overfitting</em>, the case<a id="_idIndexMarker029"/> we faced previously, in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<h3>High bias and high variance</h3>
<p>The model could <a id="_idIndexMarker030"/>neither capture the global behavior nor be robust enough to input data fluctuations. This case never happens, or high variance is hidden behind high bias, but it could look something like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.10 – High bias and high variance in practice for linear regression: this most likely never actually happens on such data" height="687" src="image/B19629_01_10.jpg" width="1041"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – High bias and high variance in practice for linear regression: this most likely never actually happens on such data</p>
<h3>Low bias and low variance</h3>
<p>The model could both<a id="_idIndexMarker031"/> capture the global data behavior and be robust enough for data fluctuation. This is the end goal when training a model. This is the case we faced in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 1.11 – Low bias and low variance in practice for linear regression: the ultimate goal" height="498" src="image/B19629_01_11.jpg" width="1134"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Low bias and low variance in practice for linear regression: the ultimate goal</p>
<p>Of course, the goal is almost always to get both low bias and low variance, even if it’s not always possible. Let’s see how regularization is a means toward <span class="No-Break">this goal.</span></p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Regularization – from overfitting to underfitting</h2>
<p>In light of all<a id="_idIndexMarker032"/> those examples, we can now get a really clear understanding of what <span class="No-Break">regularization is.</span></p>
<p>If we look again at the definition provided by GPT-3, regularization is what allows us to prevent a model from overfitting by adding constraints to the model. Indeed, <em class="italic">adding regularization allows us to reduce variance</em> in a model, and therefore, to have a less <span class="No-Break">overfitting model.</span></p>
<p>We can go one step further: what if regularization is added to an already well-trained model (that is, low bias and low variance)? In other words, what happens if constraints are added to a model that works well? Intuitively, it would degrade the overall performance. It would not allow the model to fully grasp the data behavior, and thus add bias to <span class="No-Break">the model.</span></p>
<p>Indeed, here comes a fundamental drawback of regularization: <strong class="bold">adding regularization increases </strong><span class="No-Break"><strong class="bold">model bias</strong></span><span class="No-Break">.</span></p>
<p>This can be summarized in <span class="No-Break">one figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.12 – A high variance model (bottom); the same model with more regularization and the right level of fitting (middle); and the same model with even more regularization, now underfitting (top)" height="2071" src="image/B19629_01_12.jpg" width="1259"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – A high variance model (bottom); the same model with more regularization and the right level of fitting (middle); and the same model with even more regularization, now underfitting (top)</p>
<p>This is what is <a id="_idIndexMarker033"/>called the <strong class="bold">bias-variance trade-off</strong>, a very important concept. Indeed, adding regularization is always <span class="No-Break">a balance:</span></p>
<ul>
<li>We need to have enough regularization so that the model generalizes well and is not sensitive to small data fluctuations <span class="No-Break">and noise</span></li>
<li>We need to not have too much regularization so that the model is free enough to fully capture the complexity of the data in <span class="No-Break">all cases</span></li>
</ul>
<p>As we go further in the book, more and more tools and techniques will be provided to diagnose our model and find the right <span class="No-Break">bias-variance balance.</span></p>
<p>Throughout the chapters, we will see many ways to regularize a model. We think of regularization as just adding direct constraints to a model, but there are many indirect regularization methods that may help a model better generalize. A non-exhaustive list of the existing regularization methods could be <span class="No-Break">the following:</span></p>
<ul>
<li>Adding constraints to the <span class="No-Break">model architecture</span></li>
<li>Adding constraints to the model training, such as <span class="No-Break">the loss</span></li>
<li>Adding constraints from the input data by engineering <span class="No-Break">it differently</span></li>
<li>Adding constraints from the input by generating <span class="No-Break">more samples</span></li>
</ul>
<p>Other regularization methods may be proposed, but the book will mostly focus on those methods for various cases, such as structured and unstructured data, linear models, tree-based models, deep learning <a id="_idIndexMarker034"/>models, <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) problems, and computer <span class="No-Break">vision problems.</span></p>
<p>As good as a model <a id="_idIndexMarker035"/>can be with the right regularization method, most tasks have a hard limit on the possible performances a model can achieve: this is what we call <strong class="bold">unavoidable bias</strong>. Let’s have a look at what <span class="No-Break">it is.</span></p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Unavoidable bias</h2>
<p>In almost any task, there<a id="_idIndexMarker036"/> is <a id="_idIndexMarker037"/>unavoidable bias. For example, in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.13</em>, there are both Shetland Sheepdogs and Rough Collie dogs. Can you say with 100% accuracy which <span class="No-Break">is which?</span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie dogs" height="695" src="image/B19629_01_13.jpg" width="1236"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie dogs</p>
<p>From the preceding figure, can you tell which is which? Most likely not. If you are a trained expert in dogs, you may have a lower error rate. But the odds are that given a large enough number of images, you might be wrong with some images. The lowest level of error an expert can achieve is what is called <strong class="bold">human-level error</strong>. In <a id="_idIndexMarker038"/>most <a id="_idIndexMarker039"/>cases, human-level error gives a very good idea of the lowest error a model can achieve. Anytime you are evaluating a model, it is a good idea to know (or to wonder) what a human could possibly do on such <span class="No-Break">a task.</span></p>
<p>Indeed, some tasks are much easier <span class="No-Break">than others:</span></p>
<ul>
<li>Humans perform well at classifying dogs and cats, as <span class="No-Break">does AI</span></li>
<li>Humans perform well at classifying songs, as <span class="No-Break">does AI</span></li>
<li>Humans perform quite poorly at hiring people (at least, not all recruiters will agree on a candidate), as <span class="No-Break">does AI</span></li>
</ul>
<p>Another possible source to compute the unavoidable bias is the Bayes error. Most commonly impossible to compute on complex AI tasks, the Bayes error is the lowest possible error rate a classifier can achieve. Most of the time, the Bayes error is lower than the human-level error, but much harder to estimate. This would be the actual theoretical limitation of any <span class="No-Break">model performance.</span></p>
<p>The Bayes error and the human-level error are unavoidable biases. They indicate the irreducible error of any model and are a key concept when evaluating whether a model needs more or <span class="No-Break">less regularization.</span></p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Diagnosing bias and variance</h2>
<p>We usually define bias and<a id="_idIndexMarker040"/> variance using figures with a more or less accurate fit on some data, as we did earlier on the apartment price data. Although useful to explain the concepts, in real life, we mostly have highly dimensional datasets. By using a simple Titanic dataset, we provided a dozen features, so making this kind of visual inspection is <span class="No-Break">simply impossible.</span></p>
<p>Let’s assume we are training a dogs and cats classification model with balanced data. A good method is to compare the evaluation metric (whether it be accuracy, F-score, or whatever you deemed relevant) for both the training set and <span class="No-Break">validation set.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If the concepts of evaluation metrics or training and validation sets are not clear, they will be explained in more detail in <a href="B19629_02.xhtml#_idTextAnchor034"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. Put simply, the model is trained on the training set. The metric is the value used to evaluate the trained model and is computed on both the training and <span class="No-Break">validation sets.</span></p>
<p>For example, let’s assume we get the <span class="No-Break">following results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 1.14 – Hypothetical accuracy on training and validation sets" height="86" src="image/B19629_01_14.jpg" width="331"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Hypothetical accuracy on training and validation sets</p>
<p>If we think about the expected human-level error for such a task, we expect a much higher accuracy. Indeed, most humans can recognize a dog from a cat with very <span class="No-Break">high confidence.</span></p>
<p>So, in this case, the performances of training and validation sets are far below the human-level error rate. This is typical of a <strong class="bold">high-bias</strong> scenario: the evaluation metric is bad on both training and validation sets. In such cases, the model needs to be <span class="No-Break">less regularized.</span></p>
<p>Let’s now assume that after adding lower regularization (perhaps adding raised-to-the-power features as we did in the <em class="italic">Intuition about regularization</em> section), we have the <span class="No-Break">following results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 1.15 – Hypothetical accuracy after adding more features" height="80" src="image/B19629_01_15.jpg" width="336"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Hypothetical accuracy after adding more features</p>
<p>Those results are better; the validation set now has an accuracy of 89%. Nevertheless, there are two <span class="No-Break">issues here:</span></p>
<ul>
<li>The score on the train set is way too good: it is <span class="No-Break">literally perfect</span></li>
<li>The score on the validation is still far below the human-level error rate, which we would expect to be at <span class="No-Break">least 95%</span></li>
</ul>
<p>This is typical of a <strong class="bold">high variance</strong> scenario: results are really good (usually too good) on the training set, and far below on the validation set. In such cases, the model needs <span class="No-Break">more regularization.</span></p>
<p>After adding regularization, let’s assume we now have the <span class="No-Break">following results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 1.16 – Hypothetical accuracy after adding regularization" height="65" src="image/B19629_01_16.jpg" width="315"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – Hypothetical accuracy after adding regularization</p>
<p>This seems much better: both training and validation sets have an accuracy that seems close to human-level performance. Perhaps with more data, a better model, or some other improvements, results could get a little better, but overall, this seems like a <span class="No-Break">solid result.</span></p>
<p>In most cases, diagnosing high bias and high variance is simple, and the method is always <span class="No-Break">the same:</span></p>
<ol>
<li>Evaluate your model on both the training and <span class="No-Break">validation set.</span></li>
<li>Compare the results with each other, as well as with the human-level <span class="No-Break">error rate.</span></li>
</ol>
<p>From that point, there <a id="_idIndexMarker041"/>are mostly <span class="No-Break">three cases:</span></p>
<ul>
<li><strong class="bold">High bias/underfitting</strong>: Both training and validation sets exhibit <span class="No-Break">poor performance</span></li>
<li><strong class="bold">High variance/overfitting</strong>: The training set performance is far better than the validation set performance; validation set performance is well below the human-level <span class="No-Break">error rate</span></li>
<li><strong class="bold">Good fit</strong>: Both training and validation sets exhibit performance close to the human-level <span class="No-Break">error rate</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Most of the time, it is proposed to use this technique with training and test sets, instead of training and validation sets. While the reasoning holds true, doing such optimization on the test set directly may lead to overfitting, and thus overestimate the actual performance of a model. In this book, though, we will use the test set for simplification in the <span class="No-Break">next chapters.</span></p>
<p>Having all the key concepts of regularization in mind, you might now start to understand why regularization might indeed require a whole book. While diagnosing a need for regularization is usually fairly easy, choosing the right <a id="_idIndexMarker042"/>regularization method can be really challenging. Let’s now categorize the regularization approaches that will be covered in <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Regularization – a multi-dimensional problem</h1>
<p>Having the right diagnosis for a <a id="_idIndexMarker043"/>model is crucial, as it allows us to choose the strategy more carefully to improve the model. But from any diagnosis, many paths are possible to improve the model. Those paths can be separated into three main categories, as proposed in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 1.17 – A proposed categorization of regularization types: data, model architecture, and model training" height="579" src="image/B19629_01_17.jpg" width="1276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – A proposed categorization of regularization types: data, model architecture, and model training</p>
<p>At the data level, we may have the following tools <span class="No-Break">for regularization:</span></p>
<ul>
<li>Adding more data, either synthetic <span class="No-Break">or real</span></li>
<li>Adding <span class="No-Break">more features</span></li>
<li><span class="No-Break">Feature engineering</span></li>
<li><span class="No-Break">Data preprocessing</span></li>
</ul>
<p>Indeed, the data is of extreme importance in ML in general, and regularization is no exception. We will see many examples throughout the book of <span class="No-Break">regularizing data.</span></p>
<p>At the model level, the following methods may be used <span class="No-Break">for regularization:</span></p>
<ul>
<li>Choosing a more or less <span class="No-Break">simple architecture</span></li>
<li>In deep learning, many architectural designs allow regularization (for <span class="No-Break">example, dropout)</span></li>
</ul>
<p>The model complexity may strongly impact regularization. An overly complicated architecture can easily lead to overfitting, while a too simplistic one may underfit, as depicted in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 1.18 – Possible visualization of error as a function of model complexity, for both training and validation sets" height="766" src="image/B19629_01_18.jpg" width="935"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – Possible visualization of error as a function of model complexity, for both training and validation sets</p>
<p>Finally, at the training level, some of the methods to regularize are <span class="No-Break">as follows:</span></p>
<ul>
<li><span class="No-Break">Adding penalization</span></li>
<li><span class="No-Break">Weight initialization</span></li>
<li><span class="No-Break">Transfer learning</span></li>
<li><span class="No-Break">Early stopping</span></li>
</ul>
<p>Early stopping is a very common way to avoid overfitting by preventing the model from getting too close to the <span class="No-Break">training set.</span></p>
<p>There may be many ways to <a id="_idIndexMarker044"/>regularize, as it is a multi-dimensional problem: data, model architecture, and model training are just high-level categories. Even though those categories are just some examples and more may exist or be defined, most – if not all – of the techniques this book will cover will fall into one of <span class="No-Break">those categories.</span></p>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Summary</h1>
<p>We started this chapter by demonstrating, with several real-world examples, that regularization is the key to success in ML in a production environment. Along with several other methods and best practices, a robustly regularized model is necessary for production. In production, unseen data and edge cases will appear on a regular basis, thus any deployed model must have an acceptable response to <span class="No-Break">such cases.</span></p>
<p>We then walked through some key concepts of regularization. Overfitting and underfitting are two common problems in ML and relate somehow to bias and variance. Indeed, an overfitting model has high variance, while an underfitting model has high bias. Thus, to perform well, a model is required to have low bias and low variance. We explained how, no matter how good a model can get, unavoidable bias limits its performance. Those key concepts allowed us to propose a method to diagnose bias and variance using the performance of both the training and validation sets, as well as human-level <span class="No-Break">error estimation.</span></p>
<p>This led us to what regularization is: regularization is adding constraints to a model so that it generalizes well to new data, and is not too sensitive to small data fluctuations. Regularization is a great tool to make an overfitting model a robust model. Although, due to the bias-variance trade-off, we must not regularize too much to avoid having an <span class="No-Break">underfitting model.</span></p>
<p>Finally, we categorized the different ways of regularizing that this book will cover. They mainly fall into three categories: the data, the model architecture, and the <span class="No-Break">model training.</span></p>
<p>This chapter did not include any recipes in order to build the foundational knowledge that will be required to fully understand the remainder of this book, but the next chapters will comprise recipes and will be <span class="No-Break">more solution-oriented.</span></p>
</div>
</div></body></html>