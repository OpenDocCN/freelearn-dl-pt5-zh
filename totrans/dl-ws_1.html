<html><head></head><body>
		<div>
			<div id="_idContainer015" class="Content">
			</div>
		</div>
		<div id="_idContainer016" class="Content">
			<h1 id="_idParaDest-16">1. <a id="_idTextAnchor015"/>Building Blocks of Deep Learning</h1>
		</div>
		<div id="_idContainer045" class="Content">
			<p class="callout-heading">Introduction</p>
			<p class="callout">In this chapter, you will be introduced to deep learning and its relationship with artificial intelligence and machine learning. We will also learn about some of the important deep learning architectures, such as the multi-layer perceptron, convolutional neural networks, recurrent neural networks, and generative adversarial networks. As we progress, we will get hands-on experience with the TensorFlow framework and use it to implement a few linear algebra operations. Finally, we will be introduced to the concept of optimizers. We will understand their role in deep learning by utilizing them to solve a quadratic equation. By the end of this chapter, you will have a good understanding of what deep learning is and how programming with TensorFlow works.</p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Introduction</h1>
			<p>You have just come back from your yearly vacation. Being an avid social media user, you are busy uploading your photographs to your favorite social media app. When the photos get uploaded, you notice that the app automatically identifies your face and tags you in them almost instantly. In fact, it does that even in group photos. Even in some poorly lit photos, you notice that the app has, most of the time, tagged you correctly. How does the app learn how to do that?</p>
			<p>To identify a person in a picture, the app requires accurate information on the person's facial structure, bone structure, eye color, and many other details. But when you used that photo app, you didn't have to feed all these details explicitly to the app. All you did was upload your photos, and the app automatically began identifying you in them. How did the app know all these details?</p>
			<p>When you uploaded your first photo to the app, the app would have asked you to tag yourself. When you manually tagged yourself, the app automatically "learned" all the information it needed to know about your face. Then, every time you upload a photo, the app uses the information it learned to identify you. It improves when you manually tag yourself in photos in which the app incorrectly tagged you.</p>
			<p>This ability of the app to learn new details and improve itself with minimal human intervention is possible due to the power of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>). Deep learning is a part of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) that helps a machine learn by recognizing patterns from labeled data. But wait a minute, isn't that what <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) does? Then what is the difference between deep learning and machine learning? What is the point of confluence among domains such as AI, machine learning, and deep learning? Let's take a quick look.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>AI, Machine Learning, and Deep Learning</h2>
			<p>Artificial intelligence is the branch of computer science aimed at developing machines that can simulate human intelligence. Human intelligence, in a simplified manner, can be explained as decisions that are taken based on the inputs received from our five senses – sight, hearing, touch, smell, and taste. AI is not a new field and has been in vogue since the 1950s. Since then, there have been multiple waves of ecstasy and agony within this domain. The 21<span class="superscript">st</span> century has seen a resurgence in AI following the big strides made in computing, the availability of data, and a better understanding of theoretical underpinnings. Machine learning and deep learning are subfields of AI and are increasingly used interchangeably.</p>
			<p>The following figure depicts the relationship between AI, ML, and DL:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B15385_01_01.jpg" alt="Figure 1.1: Relationship between AI, ML, and DL&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1: Relationship between AI, ML, and DL</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Machine Learning</h2>
			<p>Machine learning is the subset of AI that performs specific tasks by identifying patterns within data and extracting inferences. The inferences that are derived from data are then used to predict outcomes on unseen data. Machine learning differs from traditional computer programming in its approach to solving specific tasks. In traditional computer programming, we write and execute specific business rules and heuristics to get the desired outcomes. However, in machine learning, the rules and heuristics are not explicitly written. These rules and heuristics are learned by providing a dataset. The dataset provided for learning the rules and heuristics is called a <strong class="bold">training dataset</strong>. The whole process of learning and inferring is called <strong class="bold">training</strong>.</p>
			<p>Learning rules and heuristics is done using different algorithms that use statistical models for that purpose. These algorithms make use of many representations of data for learning. Each such representation of data is called an <strong class="bold">example</strong>. Each element within an example is called a <strong class="bold">feature</strong>. The following is an example of the famous IRIS dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>). This dataset is a representation of different species of iris flowers based on different characteristics, such as the length and width of their sepals and petals:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B15385_01_02.jpg" alt="Figure 1.2: Sample data from the IRIS dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2: Sample data from the IRIS dataset</p>
			<p>In the preceding dataset, each row of data represents an example, and each column is a feature. Machine learning algorithms make use of these features to draw inferences from the data. The veracity of the models, and thereby the outcomes that are predicted, depend a lot on the features of the data. If the features provided to the machine learning algorithm are a good representation of the problem statement, the chances of getting a good result are high. Some examples of machine learning algorithms are <em class="italic">linear regression</em>, <em class="italic">logistic regression</em>, <em class="italic">support vector machines</em>, <em class="italic">random forest</em>, and <em class="italic">XGBoost</em>.</p>
			<p>Even though traditional machine learning algorithms are useful for a lot of use cases, they have a lot of dependence on the quality of the features to get superior outcomes. The creation of features is a time-consuming art and requires a lot of domain knowledge. However, even with comprehensive domain knowledge, there are still limitations on transferring that knowledge to derive features, thereby encapsulating the nuances of the data generating processes. Also, with the increasing complexity of the problems that are tackled with machine learning, particularly with the advent of unstructured data (images, voice, text, and so on), it can be almost impossible to create features that represent the complex functions, which, in turn, generate data. As a result, there is often a need to find a different approach to solving complex problems; that is where deep learning comes into play.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Deep Learning</h2>
			<p>Deep learning is a subset of machine learning and an extension of a certain kind of algorithm called Artificial Neural Networks (ANNs). Neural networks are not a new phenomenon. Neural networks were created in the first half of the 1940s. The development of neural networks was inspired by the knowledge of how the human brain works. Since then, there have been several ups and downs in this field. One defining moment that renewed enthusiasm around neural networks was the introduction of an algorithm called backpropagation by stalwarts in the field such as Geoffrey Hinton. For this reason, Hinton is widely regarded as the 'Godfather of Deep Learning'. We will be discussing neural networks in depth in <em class="italic">Chapter 2</em>, <em class="italic">Neural Networks.</em></p>
			<p>ANNs with multiple (deep) layers lie at the heart of deep learning. One defining characteristic of deep learning models is their ability to learn features from the input data. Unlike traditional machine learning, where there is the need to create features, deep learning excels in learning different hierarchies of features across multiple layers. Say, for example, we are using a deep learning model to detect faces. The initial layers of the model will learn low-level approximations of a face, such as the edges of the face, as shown in <em class="italic">Figure 1.3</em>. Each succeeding layer takes the lower layers' features and puts them together to form more complex features. In the case of face detection, if the initial layer has learned to detect edges, the subsequent layers will put these edges together to form parts of a face such as the nose or eyes. This process continues with each successive layer, with the final layer generating an image of a human face:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B15385_01_03.jpg" alt="Figure 1.3: Deep learning model for detecting faces&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3: Deep learning model for detecting faces</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding image is sourced from the popular research paper: Lee, Honglak &amp; Grosse, Roger &amp; Ranganath, Rajesh &amp; Ng, Andrew. (2011). <em class="italic">Unsupervised Learning of Hierarchical Representations with Convolutional Deep Belief Networks. </em>Commun. ACM. 54. 95-103. 10.1145/2001269.2001295.</p>
			<p>Deep learning techniques have made great strides over the past decade. There are different factors that have led to the exponential rise of deep learning techniques. At the top of the list is the availability of large quantities of data. The digital age, with its increasing web of connected devices, has generated lots of data, especially unstructured data. This, in turn, has fueled the large-scale adoption of deep learning techniques as they are well-suited to handle large unstructured data.</p>
			<p>Another major factor that has led to the rise in deep learning is the strides that have been made in computing infrastructure. Deep learning models that have large numbers of layers and millions of parameters necessitate great computing power. The advances in computing layers such as <strong class="bold">Graphical Processing Units</strong> (<strong class="bold">GPUs</strong>) and <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>) at an affordable cost has led to the large-scale adoption of deep learning.</p>
			<p>The pervasiveness of deep learning was also accelerated by open sourcing different frameworks in order to build and implement deep learning models. In 2015, the Google Brain team open sourced the TensorFlow framework and since then TensorFlow has grown to be one of the most popular frameworks for deep learning. The other major frameworks available are PyTorch, MXNet, and Caffe. We will be using the TensorFlow framework in this book.</p>
			<p>Before we dive deep into the building blocks of deep learning, let's get our hands dirty with a quick demo that illustrates the power of deep learning models. You don't need to know any of the code that is presented in this demo. Simply follow the instructions and you'll be able to get a quick glimpse of the basic capabilities of deep learning.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Using Deep Learning to Classify an Image</h2>
			<p>In the exercise that follows, we will classify an image of a pizza and convert the resulting class text into speech. To classify the image, we will be using a pre-trained model. The conversion of text into speech will be done using a freely available API called <strong class="bold">Google Text-to-Speech</strong> (<strong class="bold">gTTS</strong>). Before we get into it, let's understand some of the key building blocks of this demo.</p>
			<h3 id="_idParaDest-22"><a id="_idTextAnchor021"/>Pre-Trained Models</h3>
			<p>Training a deep learning model requires a lot of computing infrastructure and time, with big datasets. However, to aid with research and learning, the deep learning community has also made models that have been trained on large datasets available. These pre-trained models can be downloaded and used for predictions or can be used for further training. In this demo, we will be using a pre-trained model called <strong class="source-inline">ResNet50</strong>. This model is available along with the Keras package. This pre-trained model can predict 1,000 different classes of objects that we encounter in our daily lives, such as birds, animals, automobiles, and more.</p>
			<h3 id="_idParaDest-23"><a id="_idTextAnchor022"/>The Google Text-to-Speech API</h3>
			<p>Google has made its Text-to-Speech algorithm available for limited use. We will be using this algorithm to convert the predicted text into speech.</p>
			<h3 id="_idParaDest-24"><a id="_idTextAnchor023"/>Prerequisite Packages for the Demo</h3>
			<p>For this demo to work, you will need the following packages installed on your machine:</p>
			<ul>
				<li>TensorFlow 2.0</li>
				<li>Keras</li>
				<li>gTTS</li>
			</ul>
			<p>Please refer to the <em class="italic">Preface</em> to understand the process of installing the first two packages. Installing gTTS will be shown in the exercise. Let's dig into the demo.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Exercise 1.01: Image and Speech Recognition Demo</h2>
			<p>In this exercise, we will demonstrate image recognition and speech-to-text conversion using deep learning models. At this point, you will not be able to understand each and every line of the code. This will be explained later. For now, just execute the code and find out how easy it is to build deep learning and AI applications with TensorFlow. Follow these steps to complete this exercise:</p>
			<ol>
				<li>Open a Jupyter Notebook and name it <em class="italic">Exercise 1.01.</em> For details on how to start a Jupyter Notebook, please refer to the preface.</li>
				<li>Import all the required libraries:<p class="source-code">from tensorflow.keras.preprocessing.image import load_img</p><p class="source-code">from tensorflow.keras.preprocessing.image import img_to_array</p><p class="source-code">from tensorflow.keras.applications.resnet50 import ResNet50</p><p class="source-code">from tensorflow.keras.preprocessing import image</p><p class="source-code">from tensorflow.keras.applications.resnet50 \</p><p class="source-code">import preprocess_input</p><p class="source-code">from tensorflow.keras.applications.resnet50 \</p><p class="source-code">import decode_predictions</p><p class="callout-heading">Note </p><p class="callout">The code snippet shown here uses a backslash ( <strong class="source-inline">\</strong> ) to split the logic across multiple lines. When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line.</p><p>Here is a brief description of the packages we'll be importing:</p><p><strong class="source-inline">load_img</strong>: Loads the image into the Jupyter Notebook</p><p><strong class="source-inline">img_to_array</strong>: Converts the image into a NumPy array, which is the desired format for Keras</p><p><strong class="source-inline">preprocess_input</strong>: Converts the input into a format that's acceptable for the model</p><p><strong class="source-inline">decode_predictions</strong>: Converts the numeric output of the model prediction into text labels</p><p><strong class="source-inline">Resnet50</strong>: This is the pre-trained image classification model</p></li>
				<li>Create an instance of the pre-trained <strong class="source-inline">Resnet</strong> model:<p class="source-code">mymodel = ResNet50()</p><p>You should get a message similar to the following as it downloads:</p><div id="_idContainer020" class="IMG---Figure"><img src="image/B15385_01_04.jpg" alt="Figure 1.4: Loading Resnet50&#13;&#10;"/></div><p class="figure-caption">Figure 1.4: Loading Resnet50</p><p><strong class="source-inline">Resnet50</strong> is a pre-trained image classification model. For first-time users, it will take some time to download the model into your environment.</p></li>
				<li>Download an image of a pizza from the internet and store it in the same folder that you are running the Jupyter Notebook in. Name the image <strong class="source-inline">im1.jpg</strong>.<p class="callout-heading">Note</p><p class="callout">You can also use the image we are using by downloading it from this link: <a href="https://packt.live/2AHTAC9">https://packt.live/2AHTAC9</a></p></li>
				<li>Load the image to be classified using the following command:<p class="source-code">myimage = load_img('im1.jpg', target_size=(224, 224))</p><p>If you are storing the image in another folder, the complete path of the location where the image is located has to be given in place of the <strong class="source-inline">im1.jpg</strong> command. For example, if the image is stored in <strong class="source-inline">D:/projects/demo</strong>, the code should be as follows:</p><p class="source-code">myimage = load_img('D:/projects/demo/im1.jpg', \</p><p class="source-code">                   target_size=(224, 224))</p></li>
				<li>Let's display the image using the following command:<p class="source-code">myimage</p><p>The output of the preceding command will be as follows:</p><div id="_idContainer021" class="IMG---Figure"><img src="image/B15385_01_05.jpg" alt="Figure 1.5: Output displayed after loading the image&#13;&#10;"/></div><p class="figure-caption">Figure 1.5: Output displayed after loading the image</p></li>
				<li>Convert the image into a <strong class="source-inline">numpy</strong> array as the model expects it in this format:<p class="source-code">myimage = img_to_array(myimage)</p></li>
				<li>Reshape the image into a four-dimensional format since that's what is expected by the model:<p class="source-code">myimage = myimage.reshape((1, 224, 224, 3))</p></li>
				<li>Prepare the image for submission by running the <strong class="source-inline">preprocess_input()</strong> function:<p class="source-code">myimage = preprocess_input(myimage)</p></li>
				<li>Run the prediction:<p class="source-code">myresult = mymodel.predict(myimage)</p></li>
				<li>The prediction results in a number that needs to be converted into the corresponding label in text format:<p class="source-code">mylabel = decode_predictions(myresult)</p></li>
				<li>Next, type in the following code to display the label:<p class="source-code">mylabel = mylabel[0][0]</p></li>
				<li>Print the label using the following code:<p class="source-code">print("This is a : " + mylabel[1])</p><p>If you have followed the steps correctly so far, the output will be as follows:</p><p class="source-code">This is a : pizza</p><p>The model has successfully identified our image. Interesting, isn't it? In the next few steps, we'll take this a step further and convert this result into speech.</p><p class="callout-heading">Tip</p><p class="callout">While we have used an image of a pizza here, you can use just about any image with this model. We urge you to try out this exercise multiple times with different images.</p></li>
				<li>Prepare the text to be converted into speech:<p class="source-code">sayit="This is a "+mylabel[1]</p></li>
				<li>Install the <strong class="source-inline">gtts</strong> package, which is required for converting text into speech. This can be implemented in the Jupyter Notebook, as follows:<p class="source-code">!pip install gtts</p></li>
				<li>Import the required libraries:<p class="source-code">from gtts import gTTS</p><p class="source-code">import os</p><p>The preceding code will import two libraries. One is <strong class="source-inline">gTTS</strong>, that is, Google Text-to-Speech, which is a cloud-based open source API for converting text into speech. Another is the <strong class="source-inline">os</strong> library that is used to play the resulting audio file.</p></li>
				<li>Call the <strong class="source-inline">gTTS</strong> API and pass the text as a parameter:<p class="source-code">myobj = gTTS(text=sayit)</p><p class="callout-heading">Note</p><p class="callout">You need to be online while running the preceding step.</p></li>
				<li>Save the resulting audio file. This file will be saved in the home directory where the Jupyter Notebook is being run. <p class="source-code">myobj.save("prediction.mp3")</p><p class="callout-heading">Note</p><p class="callout">You can also specify the path where you want it to be saved by including the absolute path in front of the name; for example, <strong class="source-inline">(myobj.save('D:/projects/prediction.mp3')</strong>.</p></li>
				<li>Play the audio file:<p class="source-code">os.system("prediction.mp3")</p><p>If you have correctly followed the preceding steps, you will hear the words <strong class="source-inline">This is a pizza</strong> being spoken.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZPZx8B">https://packt.live/2ZPZx8B</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/326cRIu">https://packt.live/326cRIu</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we learned how to build a deep learning model by making use of publicly available models using a few lines of code in TensorFlow. Now that you have got a taste of deep learning, let's move forward and learn about the different building blocks of deep learning.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Deep Learning Models</h2>
			<p>At the heart of most of the popular deep learning models are ANNs, which are inspired by our knowledge of how the brain works. Even though no single model can be called perfect, different models perform better in different scenarios. In the sections that follow, we will learn about some of the most prominent models.</p>
			<h3 id="_idParaDest-27"><a id="_idTextAnchor026"/>The Multi-Layer Perceptron</h3>
			<p>The <strong class="bold">multi-layer perceptron</strong> (<strong class="bold">MLP</strong>) is a basic type of neural network. An MLP is also known as a feed-forward network. A representation of an MLP can be seen in the following figure:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B15385_01_06.jpg" alt="Figure 1.6: MLP representation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6: MLP representation</p>
			<p>One of the basic building blocks of an MLP (or any neural network) is a neuron. A network consists of multiple neurons connected to successive layers. At a very basic level, an MLP will consist of an input layer, a hidden layer, and an output layer. The input layer will have neurons equal to the input data. Each input neuron will have a connection to all the neurons of the hidden layer. The final hidden layer will be connected to the output layer. The MLP is a very useful model and can be tried out on various classification and regression problems. The concept of an MLP will be covered in detail in <em class="italic">Chapter 2</em>, <em class="italic">Neural Networks.</em></p>
			<h3 id="_idParaDest-28"><a id="_idTextAnchor027"/>Convolutional Neural Networks</h3>
			<p>A convolutional neural network (CNN) is a class of deep learning model that is predominantly used for image recognition. When we discussed the MLP, we saw that each neuron in a layer is connected to every other neuron in the subsequent layer. However, CNNs adopt a different approach and do not resort to such a fully connected architecture. Instead, CNNs extract local features from images, which are then fed to the subsequent layers.</p>
			<p>CNNs rose to prominence in 2012 when an architecture called AlexNet won a premier competition called the <strong class="bold">ImageNet Large-Scale Visual Recognition Challenge</strong> <strong class="bold">(ILSVRC)</strong>. ILSVRC is a large-scale computer vision competition where teams from around the globe compete for the prize of the best computer vision model. Through the 2012 research paper titled <em class="italic">ImageNet Classification with Deep Convolutional Neural Networks</em> (<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks</a>), Alex Krizhevsky, et al. (University of Toronto) showcased the true power of CNN architectures, which eventually won them the 2012 ILSVRC challenge. The following figure depicts the structure of the <em class="italic">AlexNet</em> model, a CNN model whose high performance catapulted CNNs to prominence in the deep learning domain. While the structure of this model may look complicated to you, in <em class="italic">Chapter 3</em>, <em class="italic">Image Classification with Convolutional Neural Networks</em>, the working of such CNN networks will be explained to you in detail:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B15385_01_07.jpg" alt="Figure 1.7: CNN architecture of the AlexNet model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7: CNN architecture of the AlexNet model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The aforementioned diagram is sourced from the popular research paper: Krizhevsky, Alex &amp; Sutskever, Ilya &amp; Hinton, Geoffrey. (2012). <em class="italic">ImageNet Classification with Deep Convolutional Neural Networks.</em> Neural Information Processing Systems. 25. 10.1145/3065386.</p>
			<p>Since 2012, there have been many breakthrough CNN architectures expanding the possibilities for computer vision. Some of the prominent architectures are ZFNet, Inception (GoogLeNet), VGG, and ResNet.</p>
			<p>Some of the most prominent use cases where CNNs are put to use are as follows:</p>
			<ul>
				<li>Image recognition and <strong class="bold">optical character recognition </strong>(<strong class="bold">OCR</strong>)</li>
				<li>Face recognition on social media</li>
				<li>Text classification</li>
				<li>Object detection for self-driving cars</li>
				<li>Image analysis for health care</li>
			</ul>
			<p>Another great benefit of working with deep learning is that you needn't always build your models from scratch – you could use models built by others and use them for your own applications. This is known as "transfer learning", and it allows you to benefit from the active deep learning community.</p>
			<p>We will apply transfer learning to image processing and learn about CNNs and their dynamics in detail in <em class="italic">Chapter 3</em>, <em class="italic">Image Classification with Convolutional Neural Networks</em>.</p>
			<h3 id="_idParaDest-29"><a id="_idTextAnchor028"/>Recurrent Neural Networks</h3>
			<p>In traditional neural networks, the inputs are independent of the outputs. However, in cases such as language translation, where there is dependence on the words preceding and succeeding a word, there is a need to understand the dynamics of the sequences in which words appear. This problem was solved by a class of networks called <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). RNNs are a class of deep learning networks where the output from the previous step is sent as input to the current step. A distinct characteristic of an RNN is a hidden layer, which remembers the information of other inputs in a sequence. A high-level representation of an RNN can be seen in the following figure. You'll learn more about the inner workings of these networks in <em class="italic">Chapter 5,</em> <em class="italic">Deep Learning for Sequences</em>:</p>
			<p> </p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B15385_01_08.jpg" alt="Figure 1.8: Structure of RNNs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8: Structure of RNNs</p>
			<p>There are different types of RNN architecture. Some of the most prominent ones are <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) and <strong class="bold">gated recurrent units</strong> (<strong class="bold">GRU</strong>).</p>
			<p>Some of the important use cases for RNNs are as follows:</p>
			<ul>
				<li>Language modeling and text generation</li>
				<li>Machine translation</li>
				<li>Speech recognition</li>
				<li>Generating image descriptions</li>
			</ul>
			<p>RNNs will be covered in detail in <em class="italic">Chapter 5</em>, <em class="italic">Deep Learning for Sequences</em>, and <em class="italic">Chapter 6,</em> <em class="italic">LSTMs, GRUs, and Advanced RNNs</em>.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Generative Adversarial Networks</h2>
			<p><strong class="bold">Generative adversarial networks</strong> (<strong class="bold">GANs</strong>) are networks that are capable of generating data distributions similar to any real data distributions. One of the pioneers of deep learning, Yann LeCun, described GANs as one of the most promising ideas in deep learning in the last decade.</p>
			<p>To give you an example, suppose we want to generate images of dogs from random noise data. For this, we train a GAN network with real images of dogs and the noise data until we generate data that looks like the real images of dogs. The following diagram explains the concept behind GANs. At this stage, you might not fully understand this concept. It will be explained in detail in <em class="italic">Chapter 7</em>, <em class="italic">Generative Adversarial Networks</em>.</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B15385_01_09.jpg" alt="Figure 1.9: Structure of GANs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9: Structure of GANs</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The aforementioned diagram is sourced from the popular research paper: Barrios, Buldain, Comech, Gilbert &amp; Orue (2019). <em class="italic">Partial Discharge Classification Using Deep Learning Methods—Survey of Recent Progress</em> (<a href="https://doi.org/10.3390/en12132485">https://doi.org/10.3390/en12132485</a>).</p>
			<p>GANs are a big area of research, and there are many use cases for them. Some of the useful applications of GANs are as follows:</p>
			<ul>
				<li>Image translation</li>
				<li>Text to image synthesis</li>
				<li>Generating videos</li>
				<li>The restoration of art</li>
			</ul>
			<p>GANs will be covered in detail in <em class="italic">Chapter 7</em>, <em class="italic">Generative Adversarial Networks</em>.</p>
			<p>The possibilities and promises of deep learning are huge. Deep learning applications have become ubiquitous in our daily lives. Some notable examples are as follows:</p>
			<ul>
				<li>Chatbots</li>
				<li>Robots</li>
				<li>Smart speakers (such as Alexa)</li>
				<li>Virtual assistants</li>
				<li>Recommendation engines</li>
				<li>Drones</li>
				<li>Self-driving cars or autonomous vehicles</li>
			</ul>
			<p>This ever-expanding canvas of possibilities makes it a great toolset in the arsenal of a data scientist. This book will progressively introduce you to the amazing world of deep learning and make you adept at applying it to real-world scenarios.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Introduction to TensorFlow</h1>
			<p>TensorFlow is a deep learning library developed by Google. At the time of writing this book, TensorFlow is by far the most popular deep learning library. It was originally developed by a team within Google called the Google Brain team for their internal use and was subsequently open sourced in 2015. The Google Brain team has developed popular applications such as Google Photos and Google Cloud Speech-to-Text, which are deep learning applications based on TensorFlow. TensorFlow 1.0 was released in 2017, and within a short period of time, it became the most popular deep learning library ahead of other existing libraries, such as Caffe, Theano, and PyTorch. It is considered the industry standard, and almost every organization that is doing something in the deep learning space has adopted it. Some of the key features of TensorFlow are as follows:</p>
			<ul>
				<li>It can be used with all common programming languages, such as Python, Java, and R</li>
				<li>It can be deployed on multiple platforms, including Android and Raspberry Pi</li>
				<li>It can run in a highly distributed mode and hence is highly scalable</li>
			</ul>
			<p>After being in Alpha/Beta release for a long time, the final version of TensorFlow 2.0 was released on September 30, 2019. The focus of TF2.0 was to make the development of deep learning applications easier. Let's go ahead and understand the basics of the TensorFlow 2.0 framework.</p>
			<p><strong class="bold">Tensors</strong></p>
			<p>Inside the TensorFlow program, every data element is called a <strong class="bold">tensor</strong>. A tensor is a representation of vectors and matrices in higher dimensions. The rank of a tensor denotes its dimensions. Some of the common data forms represented as tensors are as follows.</p>
			<p><strong class="bold">Scalar</strong></p>
			<p>A scalar is a tensor of rank 0, which only has magnitude.</p>
			<p>For example, <strong class="source-inline">[ 12 ]</strong> is a scalar of magnitude 12.</p>
			<p><strong class="bold">Vector</strong></p>
			<p>A vector is a tensor of rank 1.</p>
			<p>For example, <strong class="source-inline">[ 10 , 11, 12, 13]</strong>.</p>
			<p><strong class="bold">Matrix</strong></p>
			<p>A matrix is a tensor of rank 2.</p>
			<p>For example, <strong class="source-inline">[ [10,11] , [12,13] ]</strong>. This tensor has two rows and two columns.</p>
			<p><strong class="bold">Tensor of rank 3</strong></p>
			<p>This is a tensor in three dimensions. For example, image data is predominantly a three-dimensional tensor with width, height, and the number of channels as its three dimensions. The following is an example of a tensor with three dimensions, that is, it has two rows, three columns, and three channels:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B15385_01_10.jpg" alt="Figure 1.10: Tensor with three dimensions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10: Tensor with three dimensions</p>
			<p>The shape of a tensor is represented by an array and indicates the number of elements in each dimension. For example, if the shape of a tensor is [2,3,5], it means the tensor has three dimensions. If this were to be image data, this shape would mean that this tensor has two rows, three columns, and five channels. We can also get the rank from the shape. In this example, the rank of the tensor is three, since there are three dimensions. This is further illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B15385_01_11.jpg" alt="Figure 1.11: Examples of Tensor rank and shape&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11: Examples of Tensor rank and shape</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Constants</h2>
			<p>Constants are used to store values that are not changed or modified during the course of the program. There are multiple ways in which a constant can be created, but the simplest way is as follows:</p>
			<p class="source-code">a = tf.constant (10)</p>
			<p>This creates a tensor initialized to 10. Keep in mind that a constant's value cannot be updated or modified by reassigning a new value to it. Another example is as follows:</p>
			<p class="source-code">s = tf.constant("Hello")</p>
			<p>In this line, we are instantiating a string as a constant.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Variables</h2>
			<p>A variable is used to store data that can be updated and modified during the course of the program. We will look at this in more detail in <em class="italic">Chapter 2</em>, <em class="italic">Neural Networks</em>. There are multiple ways of creating a variable, but the simplest way is as follows:</p>
			<p class="source-code">b=tf.Variable(20)</p>
			<p>In the preceding code, the variable <strong class="source-inline">b</strong> is initialized to <strong class="source-inline">20</strong>. Note that in TensorFlow, unlike constants, the term <strong class="source-inline">Variable</strong> is written with an uppercase <strong class="source-inline">V</strong>.</p>
			<p>A variable can be reassigned a different value during the course of the program. Variables can be used to assign any type of object, including scalars, vectors, and multi-dimensional arrays. The following is an example of how an array whose dimensions are 3 x 3 can be created in TensorFlow:</p>
			<p class="source-code">C = tf.Variable([[1,2,3],[4,5,6],[7,8,9]])</p>
			<p>This variable can be initialized to a 3 x 3 matrix, as follows:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B15385_01_12.jpg" alt="Figure 1.12: 3 x 3 matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12: 3 x 3 matrix</p>
			<p>Now that we know some of the basic concepts of TensorFlow, let's learn how to put them into practice.</p>
			<h3 id="_idParaDest-34"><a id="_idTextAnchor033"/>Defining Functions in TensorFlow</h3>
			<p>A function can be created in Python using the following syntax:</p>
			<p class="source-code">def myfunc(x,y,c):</p>
			<p class="source-code">    Z=x*x*y+y+c</p>
			<p class="source-code">    return Z</p>
			<p>A function is initiated using the special operator <strong class="source-inline">def</strong>, followed by the name of the function, <strong class="source-inline">myfunc</strong>, and the arguments for the function. In the preceding example, the body of the function is in the second line, and the last line returns the output.</p>
			<p>In the following exercise, we will learn how to implement a small function using the variables and constants we defined earlier.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Exercise 1.02: Implementing a Mathematical Equation</h2>
			<p>In this exercise, we will solve the following mathematical equation using TensorFlow:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B15385_01_13.jpg" alt="Figure 1.13: Mathematical equation to be solved using TensorFlow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13: Mathematical equation to be solved using TensorFlow</p>
			<p>We will use TensorFlow to solve it, as follows:</p>
			<p class="source-code">X=3</p>
			<p class="source-code">Y=4</p>
			<p>While there are multiple ways of doing this, we will only explore one of the ways in this exercise. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and rename it <em class="italic">Exercise 1.02</em>.</li>
				<li>Import the TensorFlow library using the following command:<p class="source-code">import tensorflow as tf</p></li>
				<li>Now, let's solve the equation. For that, you will need to create two variables, <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, and initialize them to the given values of <strong class="source-inline">3</strong> and <strong class="source-inline">4</strong>, respectively:<p class="source-code">X=tf.Variable(3)</p><p class="source-code">Y=tf.Variable(4)</p></li>
				<li>In our equation, the value of <strong class="source-inline">2</strong> isn't changing, so we'll store it as a constant by typing the following code:<p class="source-code">C=tf.constant(2)</p></li>
				<li>Define the function that will solve our equation:<p class="source-code">def myfunc(x,y,c):</p><p class="source-code">    Z=x*x*y+y+c</p><p class="source-code">    return Z</p></li>
				<li>Call the function by passing <strong class="source-inline">X</strong>, <strong class="source-inline">Y</strong>, and <strong class="source-inline">C</strong> as parameters. We'll be storing the output of this function in a variable called <strong class="source-inline">result</strong>:<p class="source-code">result=myfunc(X,Y,C)</p></li>
				<li>Print the result using the <strong class="source-inline">tf.print()</strong> function:<p class="source-code">tf.print(result)</p><p>The output will be as follows:</p><p class="source-code">42</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ClXKjj">https://packt.live/2ClXKjj</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2ZOIN1C">https://packt.live/2ZOIN1C</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we learned how to define and use a function. Those familiar with Python programming will notice that it is not a lot different from normal Python code.</p>
			<p>In the rest of this chapter, we will prepare ourselves by looking at some basic linear algebra and familiarize ourselves with some of the common vector operations, so that understanding neural networks in the next chapter will be much easier.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Linear Algebra with TensorFlow</h2>
			<p>The most important linear algebra topic that will be used in neural networks is matrix multiplication. In this section, we will explain how matrix multiplication works and then use TensorFlow's built-in functions to solve some matrix multiplication examples. This is essential in preparation for neural networks in the next chapter.</p>
			<p>How does matrix multiplication work? You might have studied this as part of high school, but let's do a quick recap.</p>
			<p>Let's say we have to perform a matrix multiplication between two matrices, A and B, where we have the following:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B15385_01_14.jpg" alt="Figure 1.14: Matrix A&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14: Matrix A</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B15385_01_15.jpg" alt="Figure 1.15: Matrix B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15: Matrix B</p>
			<p>The first step would be to check whether multiplying a 2 x 3 matrix by a 3 x 2 matrix is possible. There is a prerequisite for matrix multiplication. Remember that C=R, that is, the number of columns (C) in the first matrix should be equal to the number of rows (R) in the second matrix. And remember the sequence matters here, and that's why, A x B is not equal to B x A. In this example, C=3 and R=3. So, multiplication is possible.</p>
			<p>The resultant matrix would have the number of rows equal to that in A and the number of columns equal to that in B. So, in this case, the result would be a 2 x 2 matrix.</p>
			<p>To begin multiplying the two matrices, take the elements of the first row of A (R<span class="subscript">1</span>) and the elements of the first column of B (C<span class="subscript">1</span>):</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B15385_01_16.jpg" alt="Figure 1.16: Matrix A(R1)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16: Matrix A(R<span class="subscript">1</span>)</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15385_01_17.jpg" alt="Figure 1.17: Matrix B(C1)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17: Matrix B(C<span class="subscript">1</span>)</p>
			<p>Get the sum of the element-wise products, that is, (1 x 7) + (2 x 9) + (3 x 11) = 58. This will be the first element in the resultant 2 x 2 matrix. We'll call this incomplete matrix D(i) for now:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B15385_01_18.jpg" alt="Figure 1.18: Incomplete matrix D(i)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18: Incomplete matrix D(i)</p>
			<p>Repeat this with the first row of A(R<span class="subscript">1</span>) and the second column of B (C<span class="subscript">2</span>):</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B15385_01_19.jpg" alt="Figure 1.19: First row of matrix A&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.19: First row of matrix A</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B15385_01_20.jpg" alt="Figure 1.20: Second column of matrix B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.20: Second column of matrix B</p>
			<p>Get the sum of the products of the corresponding elements, that is, (1 x 8) + (2 x 10) + (3 x 12) = 64. This will be the second element in the resultant matrix:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B15385_01_21.jpg" alt="Figure 1.21: Second element of matrix D(i)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.21: Second element of matrix D(i)</p>
			<p>Repeat the same with the second row to get the final result:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B15385_01_22.jpg" alt="Figure 1.22: Matrix D&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22: Matrix D</p>
			<p>The same matrix multiplication can be performed in TensorFlow using a built-in method called <strong class="source-inline">tf.matmul()</strong>. The matrices that need to be multiplied must be supplied to the model as variables, as shown in the following example:</p>
			<p class="source-code">C = tf.matmul(A,B)</p>
			<p>In the preceding case, A and B are the matrices that we want to multiply. Let's practice this method by using TensorFlow to multiply the two matrices we multiplied manually.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Exercise 1.03: Matrix Multiplication Using TensorFlow</h2>
			<p>In this exercise, we will use the <strong class="source-inline">tf.matmul()</strong> method to multiply two matrices using <strong class="source-inline">tensorflow</strong>. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and rename it <em class="italic">Exercise 1.03</em>.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> library and create two variables, <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, as matrices. <strong class="source-inline">X</strong> is a 2 x 3 matrix and <strong class="source-inline">Y</strong> is a 3 x 2 matrix:<p class="source-code">import tensorflow as tf</p><p class="source-code">X=tf.Variable([[1,2,3],[4,5,6]])</p><p class="source-code">Y=tf.Variable([[7,8],[9,10],[11,12]])</p></li>
				<li>Print and display the values of <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> to make sure the matrices are created correctly. We'll start by printing the value of <strong class="source-inline">X</strong>:<p class="source-code">tf.print(X)</p><p>The output will be as follows:</p><p class="source-code">[[1 2 3]</p><p class="source-code"> [4 5 6]]</p><p>Now, let's print the value of <strong class="source-inline">Y</strong>:</p><p class="source-code">tf.print(Y)</p><p>The output will be as follows:</p><p class="source-code">[[7 8]</p><p class="source-code"> [9 10]</p><p class="source-code"> [11 12]]</p></li>
				<li>Perform matrix multiplication by calling the TensorFlow <strong class="source-inline">tf.matmul()</strong> function:<p class="source-code">c1=tf.matmul(X,Y)</p><p>To display the result, print the value of <strong class="source-inline">c1</strong>:</p><p class="source-code">tf.print(c1)</p><p>The output will be as follows:</p><p class="source-code">[[58 64]</p><p class="source-code"> [139 154]]</p></li>
				<li>Let's perform matrix multiplication by changing the order of the matrices:<p class="source-code">c2=tf.matmul(Y,X)</p><p>To display the result, let's print the value of <strong class="source-inline">c2</strong>:</p><p class="source-code">tf.print(c2)</p><p>The resulting output will be as follows.</p><p class="source-code">[[39 54 69]</p><p class="source-code"> [49 68 87]</p><p class="source-code"> [59 82 105]]</p><p>Note that the results are different since we changed the order.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eevyw4">https://packt.live/3eevyw4</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2CfGGvE">https://packt.live/2CfGGvE</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we learned how to create matrices in TensorFlow and how to perform matrix multiplication. This will come in handy when we create our own neural networks.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>The reshape Function</h2>
			<p>Reshape, as the name suggests, changes the shape of a tensor from its current shape to a new shape. For example, you can reshape a 2 × 3 matrix to a 3 × 2 matrix, as shown here:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B15385_01_23.jpg" alt="Figure 1.23: Reshaped matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.23: Reshaped matrix</p>
			<p>Let's consider the following 2 × 3 matrix, which we defined as follows in the previous exercise:</p>
			<p class="source-code">X=tf.Variable([[1,2,3],[4,5,6]])</p>
			<p>We can print the shape of the matrix using the following code:</p>
			<p class="source-code">X.shape</p>
			<p>From the following output, we can see the shape, which we already know:</p>
			<p class="source-code">TensorShape([2, 3])</p>
			<p>Now, to reshape <strong class="source-inline">X</strong> into a 3 × 2 matrix, TensorFlow provides a handy function called <strong class="source-inline">tf.reshape()</strong>. The function is implemented with the following arguments:</p>
			<p class="source-code">tf.reshape(X,[3,2])</p>
			<p>In the preceding code, <strong class="source-inline">X</strong> is the matrix that needs to be reshaped, and <strong class="source-inline">[3,2]</strong> is the new shape that the <strong class="source-inline">X</strong> matrix has to be reshaped to.</p>
			<p>Reshaping matrices is a handy operation when implementing neural networks. For example, a prerequisite when working with images using CNNs is that the image has to be of rank 3, that is, it has to have three dimensions: width, height, and depth. If our image is a grayscale image that has only two dimensions, the <strong class="source-inline">reshape</strong> operation will come in handy to add a third dimension. In this case, the third dimension will be 1:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B15385_01_24.jpg" alt="Figure 1.24: Changing the dimension using reshape()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.24: Changing the dimension using reshape()</p>
			<p>In the preceding figure, we are reshaping a matrix of shape <strong class="source-inline">[5,4]</strong> to a matrix of shape <strong class="source-inline">[5,4,1]</strong>. In the exercise that follows, we will be using the <strong class="source-inline">reshape()</strong> function to reshape a <strong class="source-inline">[5,4]</strong> matrix.</p>
			<p>There are some important considerations when implementing the <strong class="source-inline">reshape()</strong> function:</p>
			<ul>
				<li>The total number of elements in the new shape should be equal to the total number of elements in the original shape. For example, you can reshape a 2 × 3 matrix (a total of 6 elements) to a 3 × 2 matrix since the new shape also has 6 elements. However, you cannot reshape it to 3 × 3 or 3 × 4.</li>
				<li>The <strong class="source-inline">reshape()</strong> function should not be confused with <strong class="source-inline">transpose()</strong>. In <strong class="source-inline">reshape()</strong>, the sequence of the elements of the matrix is retained and the elements are rearranged in the new shape in the same sequence. However, in the case of <strong class="source-inline">transpose()</strong>, the rows become columns and the columns become rows. Hence the sequence of the elements will change.</li>
				<li>The <strong class="source-inline">reshape()</strong> function will not change the original matrix unless you assign the new shape to it. Otherwise, it simply displays the new shape without actually changing the original variable. For example, let's say <strong class="source-inline">x</strong> has shape [2,3] and you simply run <strong class="source-inline">tf.reshape(x,[3,2])</strong>. When you check the shape of <strong class="source-inline">x</strong> again, it will remain as [2,3]. In order to actually change the shape, you need to assign the new shape to it, like this:<p class="source-code">x=tf.reshape(x,[3,2])</p></li>
			</ul>
			<p>Let's try implementing <strong class="source-inline">reshape()</strong> in TensorFlow in the exercise that follows.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Exercise 1.04: Reshaping Matrices Using the reshape() Function in TensorFlow</h2>
			<p>In this exercise, we will reshape a <strong class="source-inline">[5,4]</strong> matrix into the shape of <strong class="source-inline">[5,4,1]</strong> using the <strong class="source-inline">reshape()</strong> function. This exercise will help us understand how <strong class="source-inline">reshape()</strong> can be used to change the rank of a tensor. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook and rename it <em class="italic">Exercise 1.04</em>. Then, import <strong class="source-inline">tensorflow</strong> and create the matrix we want to reshape:<p class="source-code">import tensorflow as tf</p><p class="source-code">A=tf.Variable([[1,2,3,4], \</p><p class="source-code">               [5,6,7,8], \</p><p class="source-code">               [9,10,11,12], \</p><p class="source-code">               [13,14,15,16], \</p><p class="source-code">               [17,18,19,20]])</p></li>
				<li>First, we'll print the variable <strong class="source-inline">A</strong> to check whether it is created correctly, using the following command:<p class="source-code">tf.print(A)</p><p>The output will be as follows:</p><p class="source-code">[[1 2 3 4]</p><p class="source-code"> [5 6 7 8]</p><p class="source-code"> [9 10 11 12]</p><p class="source-code"> [13 14 15 16]</p><p class="source-code"> [17 18 19 20]]</p></li>
				<li>Let's print the shape of <strong class="source-inline">A</strong>, just to be sure:<p class="source-code">A.shape</p><p>The output will be as follows:</p><p class="source-code">TensorShape([5, 4])</p><p>Currently, it has a rank of 2. We'll be using the <strong class="source-inline">reshape()</strong> function to change its rank to 3.</p></li>
				<li>Now, we will reshape <strong class="source-inline">A</strong> to the shape [5,4,1] using the following command. We've thrown in the <strong class="source-inline">print</strong> command just to see what the output looks like:<p class="source-code">tf.print(tf.reshape(A,[5,4,1]))</p><p>We'll get the following output:</p><p class="source-code">[[[1]</p><p class="source-code">  [2]</p><p class="source-code">  [3]</p><p class="source-code">  [4]]</p><p class="source-code"> [[5]</p><p class="source-code">  [6]</p><p class="source-code">  [7]</p><p class="source-code">  [8]]</p><p class="source-code"> [[9]</p><p class="source-code">  [10]</p><p class="source-code">  [11]</p><p class="source-code">  [12]]</p><p class="source-code"> [[13]</p><p class="source-code">  [14]</p><p class="source-code">  [15]</p><p class="source-code">  [16]]</p><p class="source-code"> [[17]</p><p class="source-code">  [18]</p><p class="source-code">  [19]</p><p class="source-code">  [20]]]</p><p>That worked as expected.</p></li>
				<li>Let's see the new shape of <strong class="source-inline">A</strong>:<p class="source-code">A.shape</p><p>The output will be as follows:</p><p class="source-code">TensorShape([5, 4])</p><p>We can see that <strong class="source-inline">A</strong> still has the same shape. Remember that we discussed that in order to save the new shape, we need to assign it to itself. Let's do that in the next step.</p></li>
				<li>Here, we'll assign the new shape to <strong class="source-inline">A</strong>:<p class="source-code">A = tf.reshape(A,[5,4,1])</p></li>
				<li>Let's check the new shape of <strong class="source-inline">A</strong> once again:<p class="source-code">A.shape</p><p>We will see the following output:</p><p class="source-code">TensorShape([5, 4, 1])</p><p>With that, we have not just reshaped the matrix but also changed its rank from 2 to 3. In the next step, let's print out the contents of <strong class="source-inline">A</strong> just to be sure.</p></li>
				<li>Let's see what <strong class="source-inline">A</strong> contains now:<p class="source-code">tf.print(A)</p><p>The output, as expected, will be as follows:</p><p class="source-code">[[[1]</p><p class="source-code">  [2]</p><p class="source-code">  [3]</p><p class="source-code">  [4]]</p><p class="source-code"> [[5]</p><p class="source-code">  [6]</p><p class="source-code">  [7]</p><p class="source-code">  [8]]</p><p class="source-code"> [[9]</p><p class="source-code">  [10]</p><p class="source-code">  [11]</p><p class="source-code">  [12]]</p><p class="source-code"> [[13]</p><p class="source-code">  [14]</p><p class="source-code">  [15]</p><p class="source-code">  [16]]</p><p class="source-code"> [[17]</p><p class="source-code">  [18]</p><p class="source-code">  [19]</p><p class="source-code">  [20]]]</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gHvyGQ">https://packt.live/3gHvyGQ</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2ZdjdUY">https://packt.live/2ZdjdUY</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how to use the <strong class="source-inline">reshape()</strong> function. Using <strong class="source-inline">reshape()</strong>, we can change the rank and shape of tensors. We also learned that reshaping a matrix changes the shape of the matrix without changing the order of the elements within the matrix. Another important thing that we learned was that the reshape dimension has to align with the number of elements in the matrix. Having learned about the <strong class="source-inline">reshape</strong> function, we will go ahead and learn about the next function, which is Argmax.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>The argmax Function</h2>
			<p>Now, let's understand the <strong class="source-inline">argmax</strong> function, which is frequently used in neural networks. <strong class="source-inline">Argmax</strong> returns the position of the maximum value along a particular axis in a matrix or tensor. It must be noted that it does not return the maximum value, but rather the index position of the maximum value.</p>
			<p>For example, if <strong class="source-inline">x</strong> = <strong class="source-inline">[1,10,3,5]</strong>, then <strong class="source-inline">tf.argmax(x)</strong> will return 1 since the maximum value (which in this case is 10) is in the index position 1.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In Python, the index starts with 0. So, considering the preceding example of <strong class="source-inline">x</strong>, the element 1 will have an index of 0, 10 will have an index of 1, and so on.</p>
			<p>Now, let's say we have the following:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B15385_01_25.jpg" alt="Figure 1.25: An example matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.25: An example matrix</p>
			<p>In this case, <strong class="source-inline">argmax</strong> has to be used with the <strong class="source-inline">axis</strong> parameter. When <strong class="source-inline">axis</strong> equals 0, it returns the position of the maximum value in each column, as shown in the following figure:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B15385_01_26.jpg" alt="Figure 1.26: The argmax operation along axis 0&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.26: The argmax operation along axis 0</p>
			<p>As you can see, the maximum value in the first column is 9, so the index, in this case, will be 2. Similarly, if we move along to the second column, the maximum value is 5, which has an index of 0. In the third column, the maximum value is 8, and hence the index is 1. If we were to run the <strong class="source-inline">argmax</strong> function on the preceding matrix with the <strong class="source-inline">axis</strong> as 0, we would get the following output:</p>
			<p class="source-code">[2,0,1]</p>
			<p>When <strong class="source-inline">axis</strong> = 1, <strong class="source-inline">argmax</strong> returns the position of the maximum value across each row, like this:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B15385_01_27.jpg" alt="Figure 1.27: The argmax operation along axis 1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.27: The argmax operation along axis 1</p>
			<p>Moving along the rows, we have 5 at index 1, 8 at index 2, and 9 at index 0. If we were to run the <strong class="source-inline">argmax</strong> function on the preceding matrix with the axis as 1, we would get the following output:</p>
			<p class="source-code">[1,2,0]</p>
			<p>With that, let's try and implement <strong class="source-inline">argmax</strong> on a matrix.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Exercise 1.05: Implementing the argmax() Function</h2>
			<p>In this exercise, we are going to use the <strong class="source-inline">argmax</strong> function to find the position of the maximum value in a given matrix along axes 0 and 1. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">tensorflow</strong> and create the following matrix:<p class="source-code">import tensorflow as tf</p><p class="source-code">X=tf.Variable([[91,12,15], [11,88,21],[90, 87,75]])</p></li>
				<li>Let's print <strong class="source-inline">X</strong> and see what the matrix looks like:<p class="source-code">tf.print(X)</p><p>The output will be as follows:</p><p class="source-code">[[91 12 15]</p><p class="source-code"> [11 88 21]</p><p class="source-code"> [90 87 75]]</p></li>
				<li>Print the shape of <strong class="source-inline">X</strong>:<p class="source-code">X.shape</p><p>The output will be as follows:</p><p class="source-code">TensorShape([3, 3])</p></li>
				<li>Now, let's use <strong class="source-inline">argmax</strong> to find the positions of the maximum values while keeping <strong class="source-inline">axis</strong> as <strong class="source-inline">0</strong>:<p class="source-code">tf.print(tf.argmax(X,axis=0))</p><p>The output will be as follows:</p><p class="source-code">[0 1 2]</p><p>Referring to the matrix in <em class="italic">Step 2</em>, we can see that, moving across the columns, the index of the maximum value (91) in the first column is 0. Similarly, the index of the maximum value along the second column (88) is 1. And finally, the maximum value across the third column (75) has index 2. Hence, we have the aforementioned output.</p></li>
				<li>Now, let's change the <strong class="source-inline">axis</strong> to <strong class="source-inline">1</strong>:<p class="source-code">tf.print(tf.argmax(X,axis=1))</p><p>The output will be as follows:</p><p class="source-code">[0 1 0]</p></li>
			</ol>
			<p>Again, referring to the matrix in <em class="italic">Step 2</em>, if we move along the rows, the maximum value along the first row is 91, which is at index 0. Similarly, the maximum value along the second row is 88, which is at index 1. Finally, the third row is at index 0 again, with a maximum value of 75.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZR5q5p">https://packt.live/2ZR5q5p</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eewhNO">https://packt.live/3eewhNO</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we learned how to use the <strong class="source-inline">argmax</strong> function to find the position of the maximum value along a given axis of a tensor. This will be used in the subsequent chapters when we perform classification using neural networks.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Optimizers</h2>
			<p>Before we look at neural networks, let's learn about one more important concept, and that is optimizers. Optimizers are extensively used for training neural networks, so it is important to understand their application. In this chapter, let's get a basic introduction to the concept of an optimizer. As you might already be aware, the purpose of machine learning is to find a function (along with its parameters) that maps inputs to outputs.</p>
			<p>For example, let's say the original function of a data distribution is a linear function (linear regression) of the following form:</p>
			<p class="source-code">Y = mX + b</p>
			<p>Here, <strong class="source-inline">Y</strong> is the dependent variable (label), <strong class="source-inline">X</strong> the independent variable (features), and <strong class="source-inline">m</strong> and <strong class="source-inline">b</strong> are the parameters of the model. Solving this problem with machine learning would entail learning the parameters <strong class="source-inline">m</strong> and <strong class="source-inline">b</strong> and thereby the form of the function that connects <strong class="source-inline">X</strong> to <strong class="source-inline">Y</strong>. Once the parameters have been learned, if we are given a new value for <strong class="source-inline">X</strong>, we can calculate or predict the value of <strong class="source-inline">Y</strong>. It is in learning these parameters that optimizers come into play. The learning process entails the following steps:</p>
			<ol>
				<li value="1">Assume some arbitrary random values for the parameters <strong class="source-inline">m</strong> and <strong class="source-inline">b</strong>.</li>
				<li>With these assumed parameters, for a given dataset, estimate the values of <strong class="source-inline">Y</strong> for each <strong class="source-inline">X</strong> variable.</li>
				<li>Find the difference between the predicted value of <strong class="source-inline">Y</strong> and the actual value of <strong class="source-inline">Y</strong> associated with the <strong class="source-inline">X</strong> variable. This difference is called the <strong class="bold">loss function</strong> or <strong class="bold">cost function</strong>. The magnitude of loss will depend on the parameter values we initially assumed. If the assumptions were way off the actual values, then the loss will be high. The way to get toward the right parameter is by changing or altering the initial assumed values of the parameters in such a way that the loss function is minimized. This task of changing the values of the parameters to reduce the loss function is called optimization.</li>
			</ol>
			<p>There are different types of optimizers that are used in deep learning. Some of the most popular ones are stochastic gradient descent, Adam, and RMSprop. The detailed functionality and the internal workings of optimizers will be described in <em class="italic">Chapter 2, Neural Networks</em>, but here, we will see how they are applied in solving certain common problems, such as simple linear regression. In this chapter, we will be using an optimizer called Adam, which is a very popular optimizer. We can define the Adam optimizer in TensorFlow using the following code:</p>
			<p class="source-code">tf.optimizers.Adam()</p>
			<p>Once an optimizer has been defined, we can use it to minimize the loss using the following code:</p>
			<p class="source-code">optimizer.minimize(loss,[m,b])</p>
			<p>The terms <strong class="source-inline">[m,b]</strong> are the parameters that will be changed during the optimization process. Now, let's use an optimizer to train a simple linear regression model using TensorFlow.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Exercise 1.06: Using an Optimizer for a Simple Linear Regression</h2>
			<p>In this exercise, we are going to see how to use an optimizer to train a simple linear regression model. We will start off by assuming arbitrary values for the parameters (<strong class="source-inline">w</strong> and <strong class="source-inline">b</strong>) in a linear equation <strong class="source-inline">w*x + b</strong>. Using the optimizer, we will observe how the values of the parameters change to get to the right parameter values, thus mapping the relationship between the input values (<strong class="source-inline">x</strong>) and output (<strong class="source-inline">y</strong>). Using the optimized parameter values, we will predict the output (<strong class="source-inline">y</strong>) for some given input values (<strong class="source-inline">x</strong>). After completing this exercise, we will see that the linear output, which is predicted by the optimized parameters, is very close to the real values of the output values. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook and rename it <em class="italic">Exercise 1.06</em>.</li>
				<li>Import <strong class="source-inline">tensorflow</strong>, create the variables, and initialize them to 0. Here, our assumed values are zero for both these parameters:<p class="source-code">import tensorflow as tf</p><p class="source-code">w=tf.Variable(0.0)</p><p class="source-code">b=tf.Variable(0.0)</p></li>
				<li>Define a function for the linear regression model. We learned how to create functions in TensorFlow earlier:<p class="source-code">def regression(x):</p><p class="source-code">    model=w*x+b</p><p class="source-code">    return model</p></li>
				<li>Prepare the data in the form of features (<strong class="source-inline">x</strong>) and labels (<strong class="source-inline">y</strong>):<p class="source-code">x=[1,2,3,4]</p><p class="source-code">y=[0,-1,-2,-3]</p></li>
				<li>Define the <strong class="source-inline">loss</strong> function. In this case, this is the absolute value of the difference between the predicted value and the label:<p class="source-code">loss=lambda:abs(regression(x)-y)</p></li>
				<li>Create an <strong class="source-inline">Adam</strong> optimizer instance with a learning rate of <strong class="source-inline">.01</strong>. The learning rate defines at what rate the optimizer should change the assumed parameters. We will discuss the learning rate in subsequent chapters:<p class="source-code">optimizer=tf.optimizers.Adam(.01)</p></li>
				<li>Train the model by running the optimizer for 1,000 iterations to minimize the loss:<p class="source-code">for i in range(1000):</p><p class="source-code">    optimizer.minimize(loss,[w,b])</p></li>
				<li>Print the trained values of the <strong class="source-inline">w</strong> and <strong class="source-inline">b</strong> parameters:<p class="source-code">tf.print(w,b)</p><p>The output will be as follows:</p><p class="source-code">-1.00371706 0.999803364</p><p>We can see that the values of the <strong class="source-inline">w</strong> and <strong class="source-inline">b</strong> parameters have been changed from their original values of 0, which were assumed. This is what is done during the optimizing process. These updated parameter values will be used for predicting the values of <strong class="source-inline">Y</strong>.</p><p class="callout-heading">Note</p><p class="callout">The optimization process is stochastic in nature (having a random probability distribution), and you might get values for <strong class="source-inline">w</strong> and <strong class="source-inline">b</strong> that are different to the value that was printed here.</p></li>
				<li>Use the trained model to predict the output by passing in the <strong class="source-inline">x</strong> values. The model predicts the values, which are very close to the label values (<strong class="source-inline">y</strong>), which means the model was trained to a high level of accuracy:<p class="source-code">tf.print(regression([1,2,3,4]))</p><p>The output of the preceding command will be as follows:</p><p class="source-code">[-0.00391370058 -1.00763083 -2.01134801 -3.01506495]</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gSBs8b">https://packt.live/3gSBs8b</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2OaFs7C">https://packt.live/2OaFs7C</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how to use an optimizer to train a simple linear regression model. During this exercise, we saw how the initially assumed values of the parameters were updated to get the true values. Using the true values of the parameters, we were able to get the predictions close to the actual values. Understanding how to apply the optimizer will help you later with training neural network models.</p>
			<p>Now that we have seen the use of an optimizer, let's take what we've learned and apply the optimization function to solve a quadratic equation in the next activity.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Activity 1.01: Solving a Quadratic Equation Using an Optimizer</h2>
			<p>In this activity, you will use an optimizer to solve the following quadratic equation:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B15385_01_28.jpg" alt="Figure 1.28: A quadratic equation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.28: A quadratic equation</p>
			<p>Here are the high-level steps you need to follow to complete this activity:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the necessary packages, just as we did in the previous exercises.</li>
				<li>Initialize the variable. Please note that, in this example, <strong class="source-inline">x</strong> is the variable that you will need to initialize. You can initialize it to a value of 0.</li>
				<li>Construct the <strong class="source-inline">loss</strong> function using the <strong class="source-inline">lambda</strong> function. The <strong class="source-inline">loss</strong> function will be the quadratic equation that you are trying to solve.</li>
				<li>Use the <strong class="source-inline">Adam</strong> optimizer with a learning rate of <strong class="source-inline">.01</strong>.</li>
				<li>Run the optimizer for different iterations and minimize the loss. You can start the number of iterations at 1,000 and then increase it in subsequent trials until you get the result you desire.</li>
				<li>Print the optimized value of <strong class="source-inline">x</strong>.</li>
			</ol>
			<p>The expected output is as follows:</p>
			<p class="source-code">4.99919891</p>
			<p>Please note that while your actual output might be a little different, it should be a value close to 5.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 388.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Summary</h1>
			<p>That brings us to the end of this chapter. Let's revisit what we have learned so far. We started off by looking at the relationship between AI, machine learning, and deep learning. Then, we implemented a demo of deep learning by classifying an image and then implementing a text to speech conversion using a Google API. This was followed by a brief description of different use cases and types of deep learning, such as MLP, CNN, RNN, and GANs.</p>
			<p>In the next section, we were introduced to the TensorFlow framework and understood some of the basic building blocks, such as tensors and their rank and shape. We also implemented different linear algebra operations using TensorFlow, such as matrix multiplication. Later in the chapter, we performed some useful operations such as <strong class="source-inline">reshape</strong> and <strong class="source-inline">argmax</strong>. Finally, we were introduced to the concept of optimizers and implemented solutions for mathematical expressions using optimizers.</p>
			<p>Now that we have laid the foundations for deep learning and introduced you to the TensorFlow framework, the stage has been set for you to take a deep dive into the fascinating world of neural networks. In the next chapter, you will be introduced to neural networks, and in the successive chapters, we will take a look at more in-depth deep learning concepts. We hope you enjoy this fascinating journey.</p>
		</div>
	</body></html>