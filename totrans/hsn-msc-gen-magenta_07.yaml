- en: Audio Generation with NSynth and GANSynth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll be looking into audio generation. We'll first provide
    an overview of WaveNet, an existing model for audio generation, especially efficient
    in text-to-speech applications. In Magenta, we'll use NSynth, a WaveNet autoencoder
    model, to generate small audio clips that can serve as instruments for a backing
    MIDI score. NSynth also enables audio transformations such as scaling, time stretching,
    and interpolation. We'll also use GANSynth, a faster approach based on **Generative
    Adversarial Network** (**GAN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about WaveNet and temporal structures for music
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural audio synthesis with NSynth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GANSynth as a generative instrument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll use the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: The **command line** or **Bash** to launch Magenta from the Terminal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python** and its libraries to write music generation code using Magenta'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magenta** to generate audio clips'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audacity** to edit audio clips'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any media player to listen to the generated WAV files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Magenta, we'll make the use of the **NSynth** and **GANSynth** models. We'll
    be explaining these models in depth, but if you feel like you need more information,
    the models' README in Magenta's source code ([github.com/tensorflow/magenta/tree/master/magenta/models](https://github.com/tensorflow/magenta/tree/master/magenta/models))
    is a good place to start. You can also take a look at Magenta's code, which is
    well documented. We also provide additional content in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is in this book's GitHub repository in the `Chapter05`
    folder, located at [github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05).
    The examples and code snippets will suppose you are located in this chapter's
    folder. For this chapter, you should do `cd Chapter05` before you start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/37QgQsI](http://bit.ly/37QgQsI)'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about WaveNet and temporal structures for music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we've been generating symbolic content such as MIDI.
    In this chapter, we'll be looking at generating sub-symbolic content, such as **raw
    audio**. We'll be using the Waveform Audio File Format (WAVE or WAV, stored in
    a `.wav` file), a format containing uncompressed audio content, usable on pretty
    much every platform and device. See [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml),
    *Introduction on Magenta and Generative Art*, for more information on waveforms
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: Generating raw audio using neural nets is a rather recent feat, following the
    2016 WaveNet paper, *A Generative Model For Raw Audio*. Other network architectures
    also perform well in audio generation, such as SampleRNN, also released in 2016
    and used since to produce music tracks and albums (see databots for an example).
  prefs: []
  type: TYPE_NORMAL
- en: As stated in [Chapter 2](b60deee5-c58f-45eb-88a2-23718802e580.xhtml), *Generating
    Drum Sequences with DrumsRNN*, convolutional architectures are rather rare in
    music generation, given their shortcomings in handling sequential data. WaveNet
    uses a stack of causal convolution layers to address these problems, somewhat
    analogous to recurrent layers.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling raw audio is hard—you have to handle 16,000 samples per second (at
    least) and keep track of the general structure at a bigger time scale. WaveNet's
    implementation is optimized to handle such data with the use of dilated convolution,
    where the convolution filter is applied over a large area by skipping input values
    with a certain step, enabling the network to preserve the input resolution throughout
    the network by using just a few layers. During training, the predictions can be
    made in parallel, while during generation, the predictions have to be made sequentially
    or **sample by sample**.
  prefs: []
  type: TYPE_NORMAL
- en: The WaveNet architecture has been used with excellent performance in text-to-speech
    application and recently in music generation but is computationally very expensive.
    Magenta's NSynth model is a **WaveNet autoregressive model**, an approach used
    to attain consistent long-term structure. Let's have a look into NSynth and its
    importance in generating music.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at NSynth and WaveNet autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NSynth model can be seen as a neural synthesizer—instead of having a synthesizer
    where you can define envelopes and specify the oscillator wave, pitch, and velocity,
    you have a model that generates new, realistic, instrument sounds. NSynth is **instrument-oriented**,
    or note-oriented, meaning it can be used to generate single notes of a generated
    instrument.
  prefs: []
  type: TYPE_NORMAL
- en: NSynth is a WaveNet-style autoencoder that learns the input data's temporal
    embedding. To understand the WaveNet Autoencoder (AE) network, you can refer to
    concepts explained in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with MusicVAE*, since both networks are AEs. You'll
    see here many of the concepts we've previously shown, such as encoding, latent
    space, and interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simplified view of the WaveNet AE network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af142458-0126-4d26-a16a-906ee26021d0.png)'
  prefs: []
  type: TYPE_IMG
- en: First, the encoder sees the whole input, which is the whole mono waveform (in
    the `.wav` format), and after 30 layers of computation, calculates an average
    pooling to create a temporal embedding (***z*** in the diagram) of 16 dimensions
    for every 512 samples, which is a dimensionality reduction of 32 times. For example,
    a single audio input, with 16,000 samples (1 second of audio with a sample rate
    of 16,000), once encoded, will have a shape of 16 for the latent vector and 16,000/512
    for the time (see the next section, *Encoding the WAV files*, for an example of
    this). Then, the WaveNet decoder will upsample the embedding to its original time
    resolution using a 1x1 convolution, trying to reproduce as closely as possible
    the input sound.
  prefs: []
  type: TYPE_NORMAL
- en: You can see WaveNet's implementation in the `Config` class of the `magenta.models.nsynth.wavenet.h512_bo16`
    module. The fastgen implementation used for the `synthesize` method is in the
    `FastGenerationConfig` class.
  prefs: []
  type: TYPE_NORMAL
- en: The *z* representation, or latent vector, has similar properties to what we
    saw in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml), *Latent Space
    Interpolation with MusicVAE—*similar sounds have similar *z* representations,
    and mixing or interpolation between two latent vectors is possible. This creates
    endless possibilities in terms of sound exploration. While traditional audio mixing
    revolves around the action of changing the volume of two audio clips to hear both
    at the same time, mixing two encodings together is about creating a sound that
    is a **hybrid of two original sounds.**
  prefs: []
  type: TYPE_NORMAL
- en: During this chapter, you'll get to listen to a lot of generated sounds, which
    we recommend you do instead of just looking at spectrograms. You'll probably notice
    that the sounds have a grainy or lo-fi texture. This is because the model works
    on mu-law encoded 8-bit 16 kHz sounds, which are of lower quality than what you
    typically listen to and is necessary for computational reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its training, the model might sometimes fall short while reconstructing
    the audio, which leads to additional harmonics, approximations, or crazy sounds.
    While surprising, these results give an interesting twist to the generated audio.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll be generating audio clips using NSynth, which we can
    then sequence using a previously generated MIDI sequence, for example. We'll listen
    to the sound of the interpolation between a cat sound and a bass sound by adding
    the encodings of both clips and synthesizing the result. We'll be generating a
    handful of sound combinations so we get a feel of what is possible in terms of
    audio interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing audio using a constant-Q transform spectrogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we''ll introduce an audio visualization plot called the **Constant-Q**
    **Transform** (**CQT**) spectrogram. We provide more information about plotting
    audio signals and CQT in the last section, *Further reading*. In the previous
    chapters, we''ve been representing MIDI as a pianoroll plot, and the representations
    are simple and easy to understand. Audio, on the other hand, is hard to represent:
    two spectrograms looking almost the same might sound different.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml), *Introduction to
    Magenta and Generative Art*, in the *Representing music with a spectrogram* section,
    we've shown how a spectrogram is a plot of time and frequency. In this chapter,
    we'll be looking at a CQT spectrogram, which is a spectrogram displayed with the
    magnitude represented by the intensity and the instantaneous frequency by color.
    The colors represent the 16 different dimensions of the embeddings. The intensity
    of lines is proportional to the log magnitude of the power spectrum, and the colors
    are given by the derivative of the phase, making the phase visible as rainbow
    colors, hence the nickname "rainbowgrams" given by the Magenta team.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we are providing four audio samples that we''ll use for our
    example and show as a rainbowgram. As always, the figures are not a replacement
    for listening to the audio content. Those samples are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf087c72-9faa-454a-860f-c3b9cf946c89.png)'
  prefs: []
  type: TYPE_IMG
- en: In the screenshot, you can notice a couple of things. First, the flute and the
    bass plots have a pretty well-defined harmonic series. Second, the metal plot,
    however, is more confused since it is a metal plate being struck. You can clearly
    see the attack of the sound and the following noise spanning the whole frequency
    range.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we'll be combining each pair of those sounds, for example,
    metal and cat, and cat and flute.
  prefs: []
  type: TYPE_NORMAL
- en: The NSynth dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we''ll have a brief look at the NSynth dataset, which was
    used to train the NSynth model. It is available at [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth) and
    is a high-quality and large-scale dataset, an order of magnitude larger than other
    similar datasets. Even if it is difficult to use for training with NSynth, it
    is interesting to look at for its content: over 300,000 musical notes that are
    classified by source, family, and quality. It can also serve as content for producing
    audio clips.'
  prefs: []
  type: TYPE_NORMAL
- en: The audio clips are all 4 seconds long (the note was held for 3 seconds and
    given 1 second for the release) and represent a single note of different instruments.
    Each note has been recorded at every pitch of the standard MIDI piano range of
    21 to 108 at five different velocities.
  prefs: []
  type: TYPE_NORMAL
- en: Since the instruments are classified by source, which is the method of sound
    production (such as acoustic, electronic, or synthetic), the dataset can be split
    for training on a specific instrument source. For example, the pre-trained GANSynth
    model we are going to use, `acoustic_only`, is useful for generating a more classical
    type of sound because the instruments in the training set are varied. The instruments
    are also classified by family, such as piano and bass, and qualities such as bright,
    dark, and percussive.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, a dataset oriented on single notes like the NSynth dataset is
    really useful for producing neural audio synthesis of notes, which can in turn
    be sequenced with the other models in Magenta. In that sense, the NSynth model
    fits well in the Magenta ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Neural audio synthesis with NSynth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be combining different audio clips together. We'll learn
    to encode the audio, optionally saving the resulting encodings on disk, mix (add)
    them, and then decode the added encodings to retrieve a sound clip.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be handling 1-second audio clips only. There are two reasons for this:
    first, **handling audio is costly**, and second, we want to **generate instrument
    notes** in the form of short audio clips. The latter is interesting for us because
    we can then sequence the audio clips using MIDI generated by the models we''ve
    been using in the previous chapters. In that sense, you can view NSynth as a generative
    instrument, and the previous models, such as MusicVAE or Melody RNN, as a generative
    score (partition) composer. With both elements, we can generate full tracks, with
    audio and structure.'
  prefs: []
  type: TYPE_NORMAL
- en: To generate sound clips, we'll be using the `fastgen` module, an external contribution
    to Magenta, which now resides in NSynth's code, implementing optimizations for
    faster sound generation with an easy-to-use API.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the WaveNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Magenta provides two pre-trained NSynth models with included weights. We're
    going to be using the WaveNet pre-trained model. This model is very expensive
    to train, taking around 10 days on 32 K40 GPUs, so we won't be talking about training
    here.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow this example in the `chapter_05_example_01.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also contains audio clips in the `sounds` folders that you can
    use for this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the pre-trained model, use the following method, which will download
    the model and extract it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code downloads the proper checkpoint and then extracts it to the destination
    directory. This is similar to `download_checkpoint`, which we've written in the
    previous chapter. Using the `wavenet-ckpt` checkpoint name, the resulting checkpoint
    will be usable via the `checkpoints/wavenet-ckpt/model.ckpt-200000` path.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this method might download rather big files (the pre-trained models
    in this chapter are big), so it will look as if the program is stuck for a while.
    It only means that the file is being downloaded (once only) locally.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding the WAV files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we''ll encode the WAV files using the `fastgen` library. Let''s define
    the `encode` method and load the audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we first load the audio for each file in the `wav_filenames`
    parameter, using the `load_audio` method from the `magenta.models.nsynth.utils`
    module. To load the audio, two parameters are important, `sample_length` and `sample_rate`:'
  prefs: []
  type: TYPE_NORMAL
- en: The sample rate is set at `16000`, which is the sample rate used by the underlying
    model. Remember, the sample rate is the number of discrete samples for 1 second
    of audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample length can be calculated by multiplying the desired number of seconds
    by the sample rate. For our example, we'll be using audio clips of 1 second for
    a sample length of 16,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We first convert the `audios` list into `ndarray` for the `encode` method, which
    has a shape of (4, 16000), because we have 4 samples of 16,000 samples each. The
    `encode` method from `magenta.models.nsynth.wavenet` returns the encodings for
    the provided audio clips. The returned encodings has a shape of (4, 31,16), with
    a length of 4 representing the number of elements, 31 representing the time, and
    16 representing the size of the latent vector, *z*.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we have a length of 31 for the time in `encodings`.
    Remember our model reduces every 512 samples to 16 (see the *Looking at NSynth
    and WaveNet autoencoders* section), but our number of samples, 16,000, is not
    divisible by 512, so we end up with 31.25. This also impacts the decoding, which
    will result in WAV files of 0.992 seconds long.
  prefs: []
  type: TYPE_NORMAL
- en: Another important point here is that all the encodings are calculated at once
    in the same batch (the batch size is defined in the encode method by taking `audios.shape[0]`),
    which will be faster than doing them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the encodings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The encodings can be visualized by plotting them, with the abscissa being the
    time and the ordinate being the encoded value. Each curve in the figure represents
    a z dimension, with 16 different colors. Here is a diagram for each of the encoded
    sounds of our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b853181-8f38-41df-accf-f90fe26b0688.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use the `save_encoding_plot` plot method in the `audio_utils.py` file
    from this chapter's code to produce the encodings plot.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the encodings for later use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saving and loading the encodings once they have been calculated is a good practice
    since it will speed up your program, even if the longer part of it is still the
    synthesizing.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this code in the `audio_utils.py` file in the source code of this
    chapter. There are more comments and content in the source code, so you should
    go check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'To save the encodings, we''re using NumPy `.npy` files as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see here we are using the `save` method from the `numpy` module. And
    we''re retrieving the encodings from the files using the `load` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can then use the returned encodings instead of calling `fastgen.encode(...)`.
    Now that we have our encodings ready, we'll see how to mix them together.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing encodings together by moving in the latent space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the encodings of our sound files, we can mix them. The term
    mixing is common in audio production and usually refers to superposing two sounds
    and adjusting their volume so that both can be heard properly. This is not what
    we are doing here; we are effectively **adding** the sounds together, resulting
    in a new sound that is more than their mere superposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a `mix_encoding_pairs` method for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The important bit is `encoding1 + encoding2 / 2.0`, where we add the encodings
    together, producing a new encoding that we'll later synthesize. In the rest of
    the method, we iterate on the encodings two by two, producing a new encoded mix
    for each pair without computing the mix of a sample with itself, resulting in
    12 elements in the method's return.
  prefs: []
  type: TYPE_NORMAL
- en: We also keep the name prefix in the `<encoding-prefix-1>_<encoding-prefix-2>` format to
    better identify them when we save the WAV on disk (we're splitting using the `_`
    character because the samples from Freesound have them to split with the unique
    ID).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return `ndarray` containing the mixed encodings as well a corresponding
    list of names for the encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing the mixed encodings to WAV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we''ll now define the `synth` method that takes the encodings and
    turns them into sound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Basically, all that method is doing is calling the `synthesize` method on the
    `magenta.models.nsynth.wavenet.fastgen` module. The `encodings_mix` shape is (12,
    31, 16), where 12 is our `batch_size` (the number of final output audio clips),
    `31` is the time, and `16` is the dimensionality of the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what the `synthesize` method is doing, take a look at this excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, `total_length` is 15,872, just short of our 16,000 sample length for a
    time of 1 second, because the length is calculated by multiplying the time (31)
    by the hop length (512). See the information box in the previous section, *Encoding
    the WAV files*, for more information. This will result in audio files that won't
    be exactly 1 second long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing to notice here is that the process **generates one sample at
    a time**. This might seem inefficient, and that''s because it is: the model is
    really good at reconstructing audio, but also painfully slow. You can see here
    that the bulk of the operation is executed in a serial loop in Python, not in
    parallel on the GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, *Using GANSynth as a generative instrument*, we look
    at a similar, but faster, model.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our three methods, `encode`, `mix`, and `synth`, we can call
    them to create new sounds and textures.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the audio clips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we provided some audio clips in the `sounds` folder that you
    can use. While we recommend you experiment with your own sound, you can test your
    method first with those and experiment with yours later.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many places you can find audio clips from:'
  prefs: []
  type: TYPE_NORMAL
- en: Make your own! It is as easy as opening your mic and hitting a plate with a
    stick (see the following list for how to record with Audacity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Freesound website, [freesound.org](https://freesound.org/), is an amazing
    community that's passionate about sharing audio clips. Freesound is a website
    for sharing copyright-free audio clips (most are under CC0 1.0 Universal (CC0
    1.0) Public Domain Dedication).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's also the NSynth dataset, [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any sample you want, but we recommend keeping it short (1 or 2 seconds)
    since this is a time-consuming process.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever source you choose, having a simple digital audio editor and recording
    application software will help you a lot with cutting, normalizing, and handling
    your sounds. As stated in the introduction, Audacity is amazing open source cross-platform
    (Windows, Linux, and macOS) software for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you downloaded your audio clips from Freesound, they might
    not all be the same length and volume, or they might be badly aligned. Audacity
    is easy to use for such tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/316c1906-b98a-4f74-9a75-12751fe7931b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this screenshot, we see each line corresponds to an audio clip. All of them
    are cropped to 1 second, ready for our example. Here are some pointers for using
    Audacity proficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: To **record** your own sounds, click first on the **Click to Start Monitoring**
    option. If you see red bars, you're good. Then, click on the red record button
    on the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To **cut** your recording, use the **Selection Tool** (*F1*) at the top, select
    a part, and press the *Delete* key to remove that part. You can use the audio
    position at the bottom for a precise selection of 1 second, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To **shift** the content of the audio (move a sharp noise to the beginning of
    the clip, for example), use the **Time Shift Tool** (*F5*) at the top, select
    a part, and drag and drop the part you want to move.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will want your tracks to be in **mono** for this chapter (a single channel
    instead of two channels). If you see two wave lines for a single file, your audio
    is in stereo. On the left, click on the filename. In the dropdown menu, use **Split
    stereo track**, then remove the left or right track. You'll also need to put the
    track panning in the center between **L** and **R**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalizing** is the act of making something louder or quieter without modifying
    the content of the audio, which might be useful if you have a sample that doesn''t
    have the proper volume. To do that, select the whole track and use **Effect**
    > **Normalize**, then change the maximum amplitude to what you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To **export** in WAV, using the **File > Export > Export as WAV** menu. You
    can use the **Solo** button on the left if you have multiple tracks because if
    you don't, they'll be mixed together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how to produce our audio clips, let's write the code to use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Generating new instruments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to render audio clips by mixing the pairs of audio clips. This
    part is an expensive process, depending on the speed of your PC and whether you
    have a GPU or not; this might vary greatly. At the time of writing, a moderately
    powerful i7 laptop will take 20 minutes to compute all of the 12 samples, and
    a PC with an entry-level GPU such as an NVIDIA RTX 2060 will do it in 4 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: You can start by taking only two samples from `WAV_FILENAMES` if you find the
    generation takes too long. We'll see in a later section, *Using GANSynth as a
    generative instrument*, that there are faster alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s call our `encode`, `mix`, and `synth` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you decide to use your own sound, make sure they are in the `sounds` folder.
    Also, you can prefix the sound filenames with an identifier before an underscore
    character; the resulting clips will keep those identifier pairs.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output will sit in the `output/nsynth` folder as WAV files; you
    should have one per unique input pair, so 12 WAV clips if you used 4 input clips.
    Go ahead and listen to them.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing and listening to our results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our clips generated, we can also look at the rainbowgrams.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for the spectrograms and rainbowgrams in the `audio_utils.py` file in
    the source code of this chapter. There are more comments and content in the source
    code, so you should go check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the rainbowgrams for all of the generated audio files for our example,
    let''s call the `save_rainbowgram_plot` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the following plots in `output/nsynth/plots`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a94ef627-1121-47b8-98d8-6ccd8724a1e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are some things to note about the generated audio files and their corresponding
    spectrograms:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the three metal-generated sounds on the left are interesting because
    they show that the generated sound kept the note's envelope since the original
    metal sound had a strong attack. The generated audio sounds like something is
    struck, like the original, but now with better harmonics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the three cat generated sounds are also interesting. In them, the cute
    cat meow becomes an otherworldly growl. Since the NSynth model was trained on
    instrument notes and the cat sound is so different, the model has to guess, which
    results in an interesting sound. Experiment with sounds outside the training dataset,
    such as percussion; it's interesting to see what the model comes up with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some of the clips, such as the flute + bass clip, we can hear some clicks
    in the generated audio. This happens when the model samples an extreme value and
    then corrects itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try and experiment with different sound combinations and durations.
    We've been using rather short samples to speed up the process, but you can use
    samples as long as you want. Just remember that the NSynth dataset contains only
    single notes that are 4 seconds long, meaning the generation of multiple consequent
    notes for longer samples will result in the model guessing the transition between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Using NSynth generated samples as instrument notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a bunch of generated samples from NSynth, we can sequence them
    using MIDI. The easiest way to do this is to use a **Digital Audio Workstation**
    (**DAW**). Since this requires writing specific code to make Magenta send MIDI
    to the DAW, we'll be dedicating a section on this topic in [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml),
    *Making Magenta Interact with Music Applications*. If you want to try that now,
    you can skip ahead and return here later.
  prefs: []
  type: TYPE_NORMAL
- en: Using the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The command line is limited for NSynth, but you can still generate audio clips.
    If you haven''t done already, you''ll need to download and extract the checkpoint
    in the `checkpoints/wavenet-ckpt` folder. While in this chapter''s code, use the
    following command to generate audio from the audio clips in the `sounds` folder
    (warning: this process takes a long time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By using `batch_size=4` and `sample_length=16000`, you make sure that code runs
    as fast as possible. The resulting files will be in the `output/nsynth` folder,
    with names in the `gen_FILENAME.wav` format, where `FILENAME` is the source filename.
    You'll see a generated audio clip for each source sound, resulting in four audio
    clips.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated clips were produced by encoding the audio and then synthesizing
    it. Compare them with the original audio: it will give you a feel of the NSynth
    sound.'
  prefs: []
  type: TYPE_NORMAL
- en: More of NSynth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is more to NSynth than we've shown here, such as more advanced use of
    interpolation and mixing, time stretching, and more. NSynth has produced interesting
    projects, such as a mobile application (mSynth) and physical hardware (NSynth
    Super). Refer to the *Further reading* section for more information on NSynth.
  prefs: []
  type: TYPE_NORMAL
- en: Using GANSynth as a generative instrument
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used NSynth to generate new sound samples by combining
    existing sounds. You may have noticed that the audio synthesis process is very
    time-consuming. This is because autoregressive models, such as WaveNet, focus
    on a single audio sample, which makes the resulting reconstruction of the waveform
    really slow because it has to process them iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: GANSynth, on the other hand, uses upsampling convolutions, making the training
    and generation processing in parallel possible for the entire audio sample. This
    is a major advantage over autoregressive models such as NSynth since those algorithms
    tend to be I/O bound on GPU hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of GANSynth are impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training** on the NSynth dataset converges in ~3-4 days on a single V100
    GPU. For comparison, the NSynth WaveNet model converges in 10 days on 32 K40 GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthesizing** a 4-second audio sample takes 20 milliseconds in GANSynth
    on a TitanX GPU. For comparison, the WaveNet baseline takes 1,077 seconds, which
    is 50,000 times slower.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important implication of GANs is that the model has a spherical Gaussian
    prior, which is decoded to produce the entire sound, making the interpolations
    between two samples smoother and without additional artifacts, unlike WaveNet
    interpolation. This is because WaveNet autoencoders such as NSynth have limited
    scope when learning local latent codes that control generation on the scale of
    milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll be making use of GANSynth to generate a 30-second audio
    clip by taking a MIDI file and playing it using random instruments sampled from
    the model's latent space. Each instrument will be played for a limited amount
    of time over the course of the audio track, for example, for 5 seconds each, blending
    between one another when the instrument changes happen.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the acoustic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Magenta provides two pre-trained GANSynth models: `acoustic_only`, where the
    model was trained only on acoustic instruments, and  `all_instruments`, where
    the model was trained on the whole NSynth dataset (see the previous section, *The
    NSynth dataset*, for more information on the dataset).
  prefs: []
  type: TYPE_NORMAL
- en: We're going to use the `acoustic_only` dataset for our example since the resulting
    audio track of a Bach score will sound more natural in terms of instrument choices.
    If you want to produce a wider generation, use the `all_instruments` model.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow this example in the `chapter_05_example_02.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also contains a MIDI clip in the `midi` folder that we'll use for
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the model, use the following method, which will download the model
    and extract it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using the `acoustic_only` checkpoint name, the resulting checkpoint will be
    usable using the `checkpoints/acoustic_only` path.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the notes information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start this example, we'll be loading a MIDI file that will serve as the backing
    score for the audio generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the MIDI file using the `load_midi` method in the `magenta.models.gansynth.lib.generate_util`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We've provided a MIDI file in the `midi` folder, but you can also provide a
    MIDI file you like, for example, a generation from a previous chapter. The `load_midi`
    method then returns a dictionary of information about the notes in the MIDI file,
    such as a list of pitches, velocities, and start and end times.
  prefs: []
  type: TYPE_NORMAL
- en: 'The provided `cs1-1pre-short.mid` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a83f379a-bade-42e6-968f-ecc1ede516b4.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see the MIDI file is 28 seconds long (14 bars at 120 QPM) and contains
    two instruments.
  prefs: []
  type: TYPE_NORMAL
- en: Gradually sampling from the latent space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the information about the MIDI file (in the `notes` variable),
    we can generate the audio from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the `generate_audio` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This method has four important parts, which we'll explain in the following three
    subsections—getting the random instrument, getting the latent vectors, generating
    the samples from the latent vectors, then combining the notes into a full audio
    clip.
  prefs: []
  type: TYPE_NORMAL
- en: Generating random instruments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_random_instruments` method from `magenta.models.gansynth.lib.generate_util`
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a sample of 28 seconds with 5 seconds per instrument gives us `n_instruments`
    of `5`, then the latent vectors get initialized by the model''s `generate_z` method, which
    is a sampling of the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This results in a `z_instruments` shape of (5, 256), 5 being the number of instruments
    and 256 the size of the latent vector. Finally, we take five steps of equal distance
    between the start and end time of the sequence in `t_instruments`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the latent vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_z_notes` method from `magenta.models.gansynth.lib.generate_util` looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This method takes each start note times and finds which instrument (previous
    instrument, `t_left`, and the next instrument, `t_right`) should be used for it.
    It then finds at what position the note is between the two instruments, in `interp`,
    to call the `slerp` method, which will find the proper latent vector that corresponds
    to the instrument between the two nearest vectors. This enables smooth transition
    from one instrument to the other.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the samples from the encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We won''t be looking into the details of the `generate_samples_from_z` method
    from `magenta.models.gansynth.lib.model`. We''ll just use this code snippet to
    illustrate what we introduced in the *Using GANSynth as a generative instrument *section about
    the model generating the audio clip as a whole:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For our example, this method will iterate 27 times to process all of the labels
    by chunks of 8 `labels` and `z` at the same time (our `batch_size` is 8). The
    bigger the batch size, the more waves it can generate in parallel. You can see
    that, contrary to NSynth, the audio samples are not generated one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once all of the audio chunks are generated, `combine_notes` from the
    `magenta.models.gansynth.lib.generate_util` module will generate the audio using
    the audio clip and the MIDI notes. Basically, what the method does is calculate
    an envelope for each MIDI note that will let the proper portion of the audio clip
    be heard when a note is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve defined and explained the different parts of the code, let''s
    call the corresponding method to generate the audio clip from the MIDI file using
    gradually interpolated instruments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated rainbowgram looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43de3426-6e39-4415-a519-21020de7f544.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram doesn't tell us much about the sound, apart from seeing the progression
    of the notes in the whole audio clip. Go and listen to the generated clip. Multiple
    generations will introduce different instruments; make sure you test it multiple
    times and with multiple MIDI files to get a feel of the possibilities in terms
    of instrument generation.
  prefs: []
  type: TYPE_NORMAL
- en: Using the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GANSynth command-line utility makes it possible to generate an audio clip
    from a MIDI file, like we''ve done with the Python code. If you haven''t done
    already, you''ll need to download and extract the checkpoint into the `checkpoints/wavenet-ckpt` folder. While
    in this chapter''s code, use the following command and an audio clip from the
    MIDI file from the `midi` folder (warning: this process takes a long time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The resulting file will be in the `output/gansynth` folder and will be called
    `generated_clip.wav`. As in our example, the generated clip contains multiple
    instruments that are gradually blending together. You can use the `secs_per_instrument`
    parameter to change the time each instrument will play.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at audio generation using two models, NSynth and
    GANSynth, and produced many audio clips by interpolating samples and generating
    new instruments. We started by explaining what WaveNet models are and why they
    are used in audio generation, particularly in text-to-speech applications. We
    also introduced WaveNet autoencoders, an encoder and decoder network capable of
    learning its own temporal embedding. We talked about audio visualization using
    the reduced dimension of the latent space in rainbowgrams.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we showed the NSynth dataset and the NSynth neural instrument. By showing
    an example of combining pairs of sounds, we learned how to mix two different encodings
    together in order to then synthesize the result into new sounds. Finally, we looked
    at the GANSynth model, a more performant model for audio generation. We showed
    the example of generating random instruments and smoothly interpolating between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter marks the end of the music generation content of this book—you
    can now generate a full song using MIDI as the backing score and neural instruments
    as the audio. During the course of the previous chapters, we've been using pre-trained
    models to show that the models in Magenta are ready to use and quite powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, there are many reasons to train your own models, as we'll see in
    the following chapters. In [Chapter 6](1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml),
    *Data Preparation for Training*, we'll be looking into preparing datasets for
    specific genres of music and for specific instruments. In [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*, we'll be using those datasets to train our own models
    that we can then use to generate new genres and instruments.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is generating audio hard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What makes the WaveNet autoencoder interesting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different colors in a rainbowgram? How many are there?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you timestretch an audio clip, slowing it down by 2 seconds, using
    NSynth?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is GANSynth faster that NSynth?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What code is required to sample 10 instruments from GANSynth latent space?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Audio Signals in Python:** An article on plotting audio signals in Python,
    explaining how to create a CQT plot ([myinspirationinformation.com/uncategorized/audio-signals-in-python/](http://myinspirationinformation.com/uncategorized/audio-signals-in-python/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant-Q transform toolbox for music processing:** A paper (2010) on implementing
    CQTs for music ([www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing](https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet: A generative model for raw audio:** A DeepMind article on WaveNet
    models for raw audio ([deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet: A Generative Model for Raw Audio:** A WaveNet paper (2016) ([arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SampleRNN**: An article explaining the differences between WaveNet and SampleRNN
    ([deepsound.io/samplernn_first.html](http://deepsound.io/samplernn_first.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSynth: Neural Audio Synthesis:** A Magenta article on the NSynth model ([magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making a Neural Synthesizer Instrument:** More ideas on sound combinations
    and modifications ([magenta.tensorflow.org/nsynth-instrument](https://magenta.tensorflow.org/nsynth-instrument))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate your own sounds with NSynth:** An article on fastgen and examples
    of timestretching and mixing ([magenta.tensorflow.org/nsynth-fastgen](https://magenta.tensorflow.org/nsynth-fastgen))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders:** An NSynth
    paper (2017) ([arxiv.org/abs/1704.01279](https://arxiv.org/abs/1704.01279))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GANSynth: Making music with GANs:** Magenta article on GANSynth ([magenta.tensorflow.org/gansynth](https://magenta.tensorflow.org/gansynth))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GANSynth: Adversarial Neural Audio Synthesis:** A GANSynth paper (2019) ([openreview.net/forum?id=H1xQVn09FX](https://openreview.net/forum?id=H1xQVn09FX))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using NSynth to win the Outside Hacks Music Hackathon 2017:** A Magenta article
    on mSynth ([magenta.tensorflow.org/blog/2017/09/12/outside-hacks/](https://magenta.tensorflow.org/blog/2017/09/12/outside-hacks/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is NSynth Super?:** An article on NSynth Super, the NSynth hardware
    synthesizer ([nsynthsuper.withgoogle.com/](https://nsynthsuper.withgoogle.com/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
