- en: Audio Generation with NSynth and GANSynth
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NSynth和GANSynth进行音频生成
- en: In this chapter, we'll be looking into audio generation. We'll first provide
    an overview of WaveNet, an existing model for audio generation, especially efficient
    in text-to-speech applications. In Magenta, we'll use NSynth, a WaveNet autoencoder
    model, to generate small audio clips that can serve as instruments for a backing
    MIDI score. NSynth also enables audio transformations such as scaling, time stretching,
    and interpolation. We'll also use GANSynth, a faster approach based on **Generative
    Adversarial Network** (**GAN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨音频生成。我们将首先概述WaveNet，这是一种现有的音频生成模型，尤其在语音合成应用中效率较高。在Magenta中，我们将使用NSynth，这是一个WaveNet自编码器模型，用于生成可以作为伴奏MIDI乐谱的音频片段。NSynth还支持音频变换，如缩放、时间拉伸和插值。我们还将使用GANSynth，这是基于**生成对抗网络**（**GAN**）的更快速方法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Learning about WaveNet and temporal structures for music
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解WaveNet和音乐的时间结构
- en: Neural audio synthesis with NSynth
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NSynth进行神经音频合成
- en: Using GANSynth as a generative instrument
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GANSynth作为生成乐器
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll use the following tools:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具：
- en: The **command line** or **Bash** to launch Magenta from the Terminal
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**命令行**或**Bash**从终端启动Magenta
- en: '**Python** and its libraries to write music generation code using Magenta'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Python**及其库编写音乐生成代码，利用Magenta
- en: '**Magenta** to generate audio clips'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Magenta**生成音频片段
- en: '**Audacity** to edit audio clips'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Audacity**编辑音频片段
- en: Any media player to listen to the generated WAV files
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用任何媒体播放器播放生成的WAV文件
- en: In Magenta, we'll make the use of the **NSynth** and **GANSynth** models. We'll
    be explaining these models in depth, but if you feel like you need more information,
    the models' README in Magenta's source code ([github.com/tensorflow/magenta/tree/master/magenta/models](https://github.com/tensorflow/magenta/tree/master/magenta/models))
    is a good place to start. You can also take a look at Magenta's code, which is
    well documented. We also provide additional content in the *Further reading* section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Magenta中，我们将使用**NSynth**和**GANSynth**模型。我们会深入解释这些模型，但如果你觉得需要更多信息，可以查看Magenta源代码中的模型README文件（[github.com/tensorflow/magenta/tree/master/magenta/models](https://github.com/tensorflow/magenta/tree/master/magenta/models)），这是一个很好的起点。你还可以查看Magenta的代码，该代码文档完善。此外，我们还在*进一步阅读*部分提供了额外的内容。
- en: The code for this chapter is in this book's GitHub repository in the `Chapter05`
    folder, located at [github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05).
    The examples and code snippets will suppose you are located in this chapter's
    folder. For this chapter, you should do `cd Chapter05` before you start.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于本书的GitHub仓库中的`Chapter05`文件夹，地址为[github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter05)。示例和代码片段假设你已经位于本章的文件夹中。为了开始本章的内容，请先执行`cd
    Chapter05`。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看以下视频，查看代码示范：
- en: '[http://bit.ly/37QgQsI](http://bit.ly/37QgQsI)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/37QgQsI](http://bit.ly/37QgQsI)'
- en: Learning about WaveNet and temporal structures for music
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解WaveNet和音乐的时间结构
- en: In the previous chapters, we've been generating symbolic content such as MIDI.
    In this chapter, we'll be looking at generating sub-symbolic content, such as **raw
    audio**. We'll be using the Waveform Audio File Format (WAVE or WAV, stored in
    a `.wav` file), a format containing uncompressed audio content, usable on pretty
    much every platform and device. See [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml),
    *Introduction on Magenta and Generative Art*, for more information on waveforms
    in general.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们一直在生成符号内容，如MIDI。在本章中，我们将探讨生成非符号内容，如**原始音频**。我们将使用波形音频文件格式（WAVE或WAV，存储在`.wav`文件中），这是一种包含未压缩音频内容的格式，可以在几乎所有平台和设备上使用。有关波形的更多信息，请参见[第1章](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml)，*Magenta与生成艺术简介*。
- en: Generating raw audio using neural nets is a rather recent feat, following the
    2016 WaveNet paper, *A Generative Model For Raw Audio*. Other network architectures
    also perform well in audio generation, such as SampleRNN, also released in 2016
    and used since to produce music tracks and albums (see databots for an example).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: As stated in [Chapter 2](b60deee5-c58f-45eb-88a2-23718802e580.xhtml), *Generating
    Drum Sequences with DrumsRNN*, convolutional architectures are rather rare in
    music generation, given their shortcomings in handling sequential data. WaveNet
    uses a stack of causal convolution layers to address these problems, somewhat
    analogous to recurrent layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Modeling raw audio is hard—you have to handle 16,000 samples per second (at
    least) and keep track of the general structure at a bigger time scale. WaveNet's
    implementation is optimized to handle such data with the use of dilated convolution,
    where the convolution filter is applied over a large area by skipping input values
    with a certain step, enabling the network to preserve the input resolution throughout
    the network by using just a few layers. During training, the predictions can be
    made in parallel, while during generation, the predictions have to be made sequentially
    or **sample by sample**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The WaveNet architecture has been used with excellent performance in text-to-speech
    application and recently in music generation but is computationally very expensive.
    Magenta's NSynth model is a **WaveNet autoregressive model**, an approach used
    to attain consistent long-term structure. Let's have a look into NSynth and its
    importance in generating music.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Looking at NSynth and WaveNet autoencoders
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NSynth model can be seen as a neural synthesizer—instead of having a synthesizer
    where you can define envelopes and specify the oscillator wave, pitch, and velocity,
    you have a model that generates new, realistic, instrument sounds. NSynth is **instrument-oriented**,
    or note-oriented, meaning it can be used to generate single notes of a generated
    instrument.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: NSynth is a WaveNet-style autoencoder that learns the input data's temporal
    embedding. To understand the WaveNet Autoencoder (AE) network, you can refer to
    concepts explained in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with MusicVAE*, since both networks are AEs. You'll
    see here many of the concepts we've previously shown, such as encoding, latent
    space, and interpolation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simplified view of the WaveNet AE network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af142458-0126-4d26-a16a-906ee26021d0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: First, the encoder sees the whole input, which is the whole mono waveform (in
    the `.wav` format), and after 30 layers of computation, calculates an average
    pooling to create a temporal embedding (***z*** in the diagram) of 16 dimensions
    for every 512 samples, which is a dimensionality reduction of 32 times. For example,
    a single audio input, with 16,000 samples (1 second of audio with a sample rate
    of 16,000), once encoded, will have a shape of 16 for the latent vector and 16,000/512
    for the time (see the next section, *Encoding the WAV files*, for an example of
    this). Then, the WaveNet decoder will upsample the embedding to its original time
    resolution using a 1x1 convolution, trying to reproduce as closely as possible
    the input sound.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编码器看到整个输入，即整个单声道波形（`.wav`格式），经过30层计算后，通过平均池化计算出一个时域嵌入（图中的***z***），该嵌入具有16个维度，每512个样本计算一次，这是32倍的维度压缩。例如，一个包含16,000个样本（1秒的音频，采样率为16,000）的音频输入，经过编码后，其潜在向量的维度为16，时间维度为16,000/512（参见下一节*编码WAV文件*，其中有示例）。然后，WaveNet解码器将使用1x1卷积将嵌入上采样至其原始时间分辨率，尽可能精确地重现输入的声音。
- en: You can see WaveNet's implementation in the `Config` class of the `magenta.models.nsynth.wavenet.h512_bo16`
    module. The fastgen implementation used for the `synthesize` method is in the
    `FastGenerationConfig` class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`magenta.models.nsynth.wavenet.h512_bo16`模块的`Config`类中看到WaveNet的实现。用于`synthesize`方法的fastgen实现位于`FastGenerationConfig`类中。
- en: The *z* representation, or latent vector, has similar properties to what we
    saw in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml), *Latent Space
    Interpolation with MusicVAE—*similar sounds have similar *z* representations,
    and mixing or interpolation between two latent vectors is possible. This creates
    endless possibilities in terms of sound exploration. While traditional audio mixing
    revolves around the action of changing the volume of two audio clips to hear both
    at the same time, mixing two encodings together is about creating a sound that
    is a **hybrid of two original sounds.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*表示，或潜在向量，具有与我们在[第4章](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml)中看到的类似的特性，*Latent
    Space Interpolation with MusicVAE—*相似的声音具有相似的*z*表示，且可以在两个潜在向量之间进行混合或插值。这为声音探索创造了无尽的可能性。传统的音频混音围绕着改变两个音频片段的音量，使它们同时播放，而将两个编码混合在一起则是创造一个**由两种原始声音混合而成的声音。**'
- en: During this chapter, you'll get to listen to a lot of generated sounds, which
    we recommend you do instead of just looking at spectrograms. You'll probably notice
    that the sounds have a grainy or lo-fi texture. This is because the model works
    on mu-law encoded 8-bit 16 kHz sounds, which are of lower quality than what you
    typically listen to and is necessary for computational reasons.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，你将听到很多生成的声音，我们建议你多听这些声音，而不仅仅是查看声谱图。你可能会注意到这些声音有一种颗粒感或低保真的质感。这是因为模型使用的是经过μ-law编码的8位16
    kHz声音，这些声音的质量低于你通常听到的声音，这是出于计算原因的需要。
- en: Due to its training, the model might sometimes fall short while reconstructing
    the audio, which leads to additional harmonics, approximations, or crazy sounds.
    While surprising, these results give an interesting twist to the generated audio.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练的原因，模型在重建音频时有时可能会出现不足，导致额外的谐波、近似或奇怪的声音。虽然这些结果令人惊讶，但它们为生成的音频增添了一种有趣的转折。
- en: In this chapter, we'll be generating audio clips using NSynth, which we can
    then sequence using a previously generated MIDI sequence, for example. We'll listen
    to the sound of the interpolation between a cat sound and a bass sound by adding
    the encodings of both clips and synthesizing the result. We'll be generating a
    handful of sound combinations so we get a feel of what is possible in terms of
    audio interpolation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将使用NSynth生成音频片段，然后可以用之前生成的MIDI序列进行排列。例如，我们将听到猫声和低音声之间的插值声音，通过将两段音频的编码相加并合成结果。我们会生成一些音频组合，以便感受音频插值的可能性。
- en: Visualizing audio using a constant-Q transform spectrogram
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用常数-Q变换谱图可视化音频
- en: 'Before we start, we''ll introduce an audio visualization plot called the **Constant-Q**
    **Transform** (**CQT**) spectrogram. We provide more information about plotting
    audio signals and CQT in the last section, *Further reading*. In the previous
    chapters, we''ve been representing MIDI as a pianoroll plot, and the representations
    are simple and easy to understand. Audio, on the other hand, is hard to represent:
    two spectrograms looking almost the same might sound different.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将介绍一种音频可视化图谱，称为**Constant-Q** **Transform**（**CQT**）频谱图。我们将在最后一节*进一步阅读*中提供更多关于绘制音频信号和CQT的资料。在前几章中，我们一直用钢琴卷轴图来表示MIDI，且这些表示方式简单易懂。另一方面，音频的表示较为复杂：两幅几乎相同的频谱图可能听起来却不同。
- en: In [Chapter 1](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml), *Introduction to
    Magenta and Generative Art*, in the *Representing music with a spectrogram* section,
    we've shown how a spectrogram is a plot of time and frequency. In this chapter,
    we'll be looking at a CQT spectrogram, which is a spectrogram displayed with the
    magnitude represented by the intensity and the instantaneous frequency by color.
    The colors represent the 16 different dimensions of the embeddings. The intensity
    of lines is proportional to the log magnitude of the power spectrum, and the colors
    are given by the derivative of the phase, making the phase visible as rainbow
    colors, hence the nickname "rainbowgrams" given by the Magenta team.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](c5602f6c-c094-42f2-936f-98746cf04a49.xhtml)，*Magenta与生成艺术简介*的*用频谱图表示音乐*部分中，我们展示了频谱图是时间与频率的图示。在本章中，我们将查看CQT频谱图，这是一种通过强度表示幅度、通过颜色表示瞬时频率的频谱图。颜色代表嵌入的16个不同维度。线条的强度与功率谱的对数幅度成比例，颜色则由相位的导数决定，使相位以彩虹色的形式呈现，因此Magenta团队将其称为“彩虹图”。
- en: 'For this section, we are providing four audio samples that we''ll use for our
    example and show as a rainbowgram. As always, the figures are not a replacement
    for listening to the audio content. Those samples are shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分，我们提供了四个音频样本，用于我们的示例，并以彩虹图形式展示。像往常一样，这些图形无法替代聆听音频内容。这些样本显示在下图中：
- en: '![](img/cf087c72-9faa-454a-860f-c3b9cf946c89.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf087c72-9faa-454a-860f-c3b9cf946c89.png)'
- en: In the screenshot, you can notice a couple of things. First, the flute and the
    bass plots have a pretty well-defined harmonic series. Second, the metal plot,
    however, is more confused since it is a metal plate being struck. You can clearly
    see the attack of the sound and the following noise spanning the whole frequency
    range.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图中，你可以注意到几点。首先，长笛和低音的频谱图有着相当清晰的谐波系列。其次，金属的频谱图则显得更加混乱，因为它是金属板被敲击的声音。你可以清晰地看到声音的攻击部分，以及随后的噪声覆盖整个频率范围。
- en: For our example, we'll be combining each pair of those sounds, for example,
    metal and cat, and cat and flute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将组合这些声音的每一对，例如金属和猫咪，猫咪和长笛。
- en: The NSynth dataset
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSynth数据集
- en: 'Before we start, we''ll have a brief look at the NSynth dataset, which was
    used to train the NSynth model. It is available at [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth) and
    is a high-quality and large-scale dataset, an order of magnitude larger than other
    similar datasets. Even if it is difficult to use for training with NSynth, it
    is interesting to look at for its content: over 300,000 musical notes that are
    classified by source, family, and quality. It can also serve as content for producing
    audio clips.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们先简要了解一下用于训练NSynth模型的NSynth数据集。该数据集可以在[magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth)找到，是一个高质量且大规模的数据集，比其他类似数据集大一个数量级。即使它在使用NSynth进行训练时可能有些困难，但从其内容来看非常有趣：超过30万个按来源、家族和质量分类的音符。它也可以作为生成音频片段的内容。
- en: The audio clips are all 4 seconds long (the note was held for 3 seconds and
    given 1 second for the release) and represent a single note of different instruments.
    Each note has been recorded at every pitch of the standard MIDI piano range of
    21 to 108 at five different velocities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 音频片段的长度均为4秒（音符持续了3秒，释放音符用了1秒），并代表了不同乐器的单一音符。每个音符在标准MIDI钢琴范围的21到108之间的每个音高上都已录制，且以五种不同的力度进行了录制。
- en: Since the instruments are classified by source, which is the method of sound
    production (such as acoustic, electronic, or synthetic), the dataset can be split
    for training on a specific instrument source. For example, the pre-trained GANSynth
    model we are going to use, `acoustic_only`, is useful for generating a more classical
    type of sound because the instruments in the training set are varied. The instruments
    are also classified by family, such as piano and bass, and qualities such as bright,
    dark, and percussive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乐器是按声音来源分类的，即声音的产生方式（例如声学、电子或合成），因此可以将数据集拆分，以便针对特定的乐器来源进行训练。例如，我们将要使用的预训练GANSynth模型，`acoustic_only`，对于生成更经典类型的声音非常有用，因为训练集中的乐器种类繁多。乐器还按家族分类，如钢琴和低音，以及按音质分类，如明亮、阴暗和打击音。
- en: Interestingly, a dataset oriented on single notes like the NSynth dataset is
    really useful for producing neural audio synthesis of notes, which can in turn
    be sequenced with the other models in Magenta. In that sense, the NSynth model
    fits well in the Magenta ecosystem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，像NSynth数据集这样专注于单个音符的数据集在生成音符的神经音频合成中非常有用，这些音符又可以与Magenta中的其他模型一起进行排序。从这个角度看，NSynth模型非常适合Magenta生态系统。
- en: Neural audio synthesis with NSynth
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NSynth进行神经音频合成
- en: In this section, we'll be combining different audio clips together. We'll learn
    to encode the audio, optionally saving the resulting encodings on disk, mix (add)
    them, and then decode the added encodings to retrieve a sound clip.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将把不同的音频片段结合在一起。我们将学习如何对音频进行编码，并可选择将结果编码保存到磁盘，再对其进行混合（添加），然后解码添加后的编码以检索音频片段。
- en: 'We''ll be handling 1-second audio clips only. There are two reasons for this:
    first, **handling audio is costly**, and second, we want to **generate instrument
    notes** in the form of short audio clips. The latter is interesting for us because
    we can then sequence the audio clips using MIDI generated by the models we''ve
    been using in the previous chapters. In that sense, you can view NSynth as a generative
    instrument, and the previous models, such as MusicVAE or Melody RNN, as a generative
    score (partition) composer. With both elements, we can generate full tracks, with
    audio and structure.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只处理1秒钟的音频片段。这样做有两个原因：首先，**处理音频的成本很高**，其次，我们想要**生成乐器音符**，以短音频片段的形式呈现。后者对我们很有趣，因为我们可以使用我们在前几章中使用的模型生成的MIDI来对音频片段进行排序。从这个角度看，你可以将NSynth视为一个生成性乐器，而将之前的模型，如MusicVAE或Melody
    RNN，视为生成性乐谱（曲谱）作曲器。结合这两个元素，我们可以生成完整的音轨，包含音频和结构。
- en: To generate sound clips, we'll be using the `fastgen` module, an external contribution
    to Magenta, which now resides in NSynth's code, implementing optimizations for
    faster sound generation with an easy-to-use API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成音频片段，我们将使用`fastgen`模块，这是Magenta的一个外部贡献，目前已集成到NSynth的代码中，优化了通过易于使用的API快速生成音频的功能。
- en: Choosing the WaveNet model
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择WaveNet模型
- en: Magenta provides two pre-trained NSynth models with included weights. We're
    going to be using the WaveNet pre-trained model. This model is very expensive
    to train, taking around 10 days on 32 K40 GPUs, so we won't be talking about training
    here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Magenta提供了两个包含权重的预训练NSynth模型。我们将使用WaveNet预训练模型。这个模型的训练非常昂贵，使用32个K40 GPU需要大约10天时间，因此我们在这里不讨论训练。
- en: You can follow this example in the `chapter_05_example_01.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本章源代码中的`chapter_05_example_01.py`文件中参考这个示例。源代码中有更多的注释和内容，所以你应该去查看它。
- en: This chapter also contains audio clips in the `sounds` folders that you can
    use for this section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还包含`sounds`文件夹中的音频片段，你可以在这一部分使用它们。
- en: 'To download the pre-trained model, use the following method, which will download
    the model and extract it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载预训练模型，使用以下方法，它会下载并提取模型：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code downloads the proper checkpoint and then extracts it to the destination
    directory. This is similar to `download_checkpoint`, which we've written in the
    previous chapter. Using the `wavenet-ckpt` checkpoint name, the resulting checkpoint
    will be usable via the `checkpoints/wavenet-ckpt/model.ckpt-200000` path.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码下载合适的检查点并将其提取到目标目录。这类似于我们在上一章中编写的`download_checkpoint`。使用`wavenet-ckpt`检查点名称，得到的检查点可以通过`checkpoints/wavenet-ckpt/model.ckpt-200000`路径使用。
- en: Note that this method might download rather big files (the pre-trained models
    in this chapter are big), so it will look as if the program is stuck for a while.
    It only means that the file is being downloaded (once only) locally.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Encoding the WAV files
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we''ll encode the WAV files using the `fastgen` library. Let''s define
    the `encode` method and load the audio:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we first load the audio for each file in the `wav_filenames`
    parameter, using the `load_audio` method from the `magenta.models.nsynth.utils`
    module. To load the audio, two parameters are important, `sample_length` and `sample_rate`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The sample rate is set at `16000`, which is the sample rate used by the underlying
    model. Remember, the sample rate is the number of discrete samples for 1 second
    of audio.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample length can be calculated by multiplying the desired number of seconds
    by the sample rate. For our example, we'll be using audio clips of 1 second for
    a sample length of 16,000.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We first convert the `audios` list into `ndarray` for the `encode` method, which
    has a shape of (4, 16000), because we have 4 samples of 16,000 samples each. The
    `encode` method from `magenta.models.nsynth.wavenet` returns the encodings for
    the provided audio clips. The returned encodings has a shape of (4, 31,16), with
    a length of 4 representing the number of elements, 31 representing the time, and
    16 representing the size of the latent vector, *z*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we have a length of 31 for the time in `encodings`.
    Remember our model reduces every 512 samples to 16 (see the *Looking at NSynth
    and WaveNet autoencoders* section), but our number of samples, 16,000, is not
    divisible by 512, so we end up with 31.25. This also impacts the decoding, which
    will result in WAV files of 0.992 seconds long.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Another important point here is that all the encodings are calculated at once
    in the same batch (the batch size is defined in the encode method by taking `audios.shape[0]`),
    which will be faster than doing them one by one.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the encodings
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The encodings can be visualized by plotting them, with the abscissa being the
    time and the ordinate being the encoded value. Each curve in the figure represents
    a z dimension, with 16 different colors. Here is a diagram for each of the encoded
    sounds of our example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b853181-8f38-41df-accf-f90fe26b0688.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: You can use the `save_encoding_plot` plot method in the `audio_utils.py` file
    from this chapter's code to produce the encodings plot.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Saving the encodings for later use
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saving and loading the encodings once they have been calculated is a good practice
    since it will speed up your program, even if the longer part of it is still the
    synthesizing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: You can find this code in the `audio_utils.py` file in the source code of this
    chapter. There are more comments and content in the source code, so you should
    go check it out.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'To save the encodings, we''re using NumPy `.npy` files as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see here we are using the `save` method from the `numpy` module. And
    we''re retrieving the encodings from the files using the `load` method as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use the returned encodings instead of calling `fastgen.encode(...)`.
    Now that we have our encodings ready, we'll see how to mix them together.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Mixing encodings together by moving in the latent space
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the encodings of our sound files, we can mix them. The term
    mixing is common in audio production and usually refers to superposing two sounds
    and adjusting their volume so that both can be heard properly. This is not what
    we are doing here; we are effectively **adding** the sounds together, resulting
    in a new sound that is more than their mere superposition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a `mix_encoding_pairs` method for that purpose:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The important bit is `encoding1 + encoding2 / 2.0`, where we add the encodings
    together, producing a new encoding that we'll later synthesize. In the rest of
    the method, we iterate on the encodings two by two, producing a new encoded mix
    for each pair without computing the mix of a sample with itself, resulting in
    12 elements in the method's return.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: We also keep the name prefix in the `<encoding-prefix-1>_<encoding-prefix-2>` format to
    better identify them when we save the WAV on disk (we're splitting using the `_`
    character because the samples from Freesound have them to split with the unique
    ID).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return `ndarray` containing the mixed encodings as well a corresponding
    list of names for the encodings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing the mixed encodings to WAV
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we''ll now define the `synth` method that takes the encodings and
    turns them into sound:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Basically, all that method is doing is calling the `synthesize` method on the
    `magenta.models.nsynth.wavenet.fastgen` module. The `encodings_mix` shape is (12,
    31, 16), where 12 is our `batch_size` (the number of final output audio clips),
    `31` is the time, and `16` is the dimensionality of the latent space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what the `synthesize` method is doing, take a look at this excerpt:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `total_length` is 15,872, just short of our 16,000 sample length for a
    time of 1 second, because the length is calculated by multiplying the time (31)
    by the hop length (512). See the information box in the previous section, *Encoding
    the WAV files*, for more information. This will result in audio files that won't
    be exactly 1 second long.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing to notice here is that the process **generates one sample at
    a time**. This might seem inefficient, and that''s because it is: the model is
    really good at reconstructing audio, but also painfully slow. You can see here
    that the bulk of the operation is executed in a serial loop in Python, not in
    parallel on the GPU.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, *Using GANSynth as a generative instrument*, we look
    at a similar, but faster, model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our three methods, `encode`, `mix`, and `synth`, we can call
    them to create new sounds and textures.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the audio clips
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备音频剪辑
- en: For this example, we provided some audio clips in the `sounds` folder that you
    can use. While we recommend you experiment with your own sound, you can test your
    method first with those and experiment with yours later.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们在`sounds`文件夹中提供了一些音频剪辑供您使用。虽然我们建议您尝试使用自己的声音，但您可以先用这些进行测试，稍后再尝试您自己的方法。
- en: 'There are many places you can find audio clips from:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从许多地方找到音频剪辑：
- en: Make your own! It is as easy as opening your mic and hitting a plate with a
    stick (see the following list for how to record with Audacity).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作您自己的音频！只需打开麦克风，用棍子敲击盘子即可（请参阅以下列表，了解如何使用Audacity录制）。
- en: The Freesound website, [freesound.org](https://freesound.org/), is an amazing
    community that's passionate about sharing audio clips. Freesound is a website
    for sharing copyright-free audio clips (most are under CC0 1.0 Universal (CC0
    1.0) Public Domain Dedication).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freesound网站，[freesound.org](https://freesound.org/)，是一个热衷于分享音频剪辑的令人惊叹的社区。Freesound是一个分享无版权音频剪辑的网站（大多数属于CC0
    1.0通用（CC0 1.0）公共领域贡献）。
- en: There's also the NSynth dataset, [magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有NSynth数据集，[magenta.tensorflow.org/datasets/nsynth](https://magenta.tensorflow.org/datasets/nsynth)。
- en: You can use any sample you want, but we recommend keeping it short (1 or 2 seconds)
    since this is a time-consuming process.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用任何您想要的样本，但我们建议保持较短（1或2秒），因为这是一个耗时的过程。
- en: Whatever source you choose, having a simple digital audio editor and recording
    application software will help you a lot with cutting, normalizing, and handling
    your sounds. As stated in the introduction, Audacity is amazing open source cross-platform
    (Windows, Linux, and macOS) software for that purpose.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪个来源，拥有简单的数字音频编辑器和录音应用程序软件都将帮助您大大简化切割、归一化和处理声音。正如介绍中所述，Audacity是一个出色的开源跨平台（Windows、Linux和macOS）软件。
- en: 'For example, if you downloaded your audio clips from Freesound, they might
    not all be the same length and volume, or they might be badly aligned. Audacity
    is easy to use for such tasks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您从Freesound下载了音频剪辑，它们的长度和音量可能不一致，或者它们可能对齐不良。Audacity非常适合处理这类任务：
- en: '![](img/316c1906-b98a-4f74-9a75-12751fe7931b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/316c1906-b98a-4f74-9a75-12751fe7931b.png)'
- en: 'In this screenshot, we see each line corresponds to an audio clip. All of them
    are cropped to 1 second, ready for our example. Here are some pointers for using
    Audacity proficiently:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个截图中，我们看到每一行对应一个音频剪辑。它们都被裁剪为1秒钟，准备用于我们的示例。以下是熟练使用Audacity的一些提示：
- en: To **record** your own sounds, click first on the **Click to Start Monitoring**
    option. If you see red bars, you're good. Then, click on the red record button
    on the left.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**录制**你自己的声音，请首先点击**点击开始监控**选项。如果你看到红条，就表示一切正常。然后，点击左侧的红色录制按钮。
- en: To **cut** your recording, use the **Selection Tool** (*F1*) at the top, select
    a part, and press the *Delete* key to remove that part. You can use the audio
    position at the bottom for a precise selection of 1 second, for example.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**剪切**您的录音，请在顶部使用**选择工具**（*F1*），选择一个部分，然后按下*删除*键删除该部分。您可以使用底部的音频位置来精确选择1秒钟的部分。
- en: To **shift** the content of the audio (move a sharp noise to the beginning of
    the clip, for example), use the **Time Shift Tool** (*F5*) at the top, select
    a part, and drag and drop the part you want to move.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要**调整**音频内容（例如将尖锐的噪音移到剪辑的开头），请在顶部使用**时间移动工具**（*F5*），选择一个部分，然后拖放您想要移动的部分。
- en: You will want your tracks to be in **mono** for this chapter (a single channel
    instead of two channels). If you see two wave lines for a single file, your audio
    is in stereo. On the left, click on the filename. In the dropdown menu, use **Split
    stereo track**, then remove the left or right track. You'll also need to put the
    track panning in the center between **L** and **R**.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于本章，您将希望将音轨设置为**单声道**（而不是两个声道）。如果您看到单个文件有两条波形线，则说明您的音频是立体声的。在左侧，单击文件名。在下拉菜单中，使用**分离立体声轨道**，然后删除左侧或右侧轨道。您还需要将轨道声道设置在**L**和**R**之间的中心位置。
- en: '**Normalizing** is the act of making something louder or quieter without modifying
    the content of the audio, which might be useful if you have a sample that doesn''t
    have the proper volume. To do that, select the whole track and use **Effect**
    > **Normalize**, then change the maximum amplitude to what you want.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**是在不修改音频内容的情况下使音量更大或更小的操作，这在您的样本音量不合适时可能很有用。要执行此操作，请选择整个轨道，然后使用**效果**
    > **归一化**，然后将最大振幅更改为您想要的值。'
- en: To **export** in WAV, using the **File > Export > Export as WAV** menu. You
    can use the **Solo** button on the left if you have multiple tracks because if
    you don't, they'll be mixed together.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how to produce our audio clips, let's write the code to use
    them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Generating new instruments
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to render audio clips by mixing the pairs of audio clips. This
    part is an expensive process, depending on the speed of your PC and whether you
    have a GPU or not; this might vary greatly. At the time of writing, a moderately
    powerful i7 laptop will take 20 minutes to compute all of the 12 samples, and
    a PC with an entry-level GPU such as an NVIDIA RTX 2060 will do it in 4 minutes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: You can start by taking only two samples from `WAV_FILENAMES` if you find the
    generation takes too long. We'll see in a later section, *Using GANSynth as a
    generative instrument*, that there are faster alternatives.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s call our `encode`, `mix`, and `synth` methods:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you decide to use your own sound, make sure they are in the `sounds` folder.
    Also, you can prefix the sound filenames with an identifier before an underscore
    character; the resulting clips will keep those identifier pairs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output will sit in the `output/nsynth` folder as WAV files; you
    should have one per unique input pair, so 12 WAV clips if you used 4 input clips.
    Go ahead and listen to them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing and listening to our results
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our clips generated, we can also look at the rainbowgrams.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for the spectrograms and rainbowgrams in the `audio_utils.py` file in
    the source code of this chapter. There are more comments and content in the source
    code, so you should go check it out.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the rainbowgrams for all of the generated audio files for our example,
    let''s call the `save_rainbowgram_plot` method:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code outputs the following plots in `output/nsynth/plots`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a94ef627-1121-47b8-98d8-6ccd8724a1e9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'There are some things to note about the generated audio files and their corresponding
    spectrograms:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: First, the three metal-generated sounds on the left are interesting because
    they show that the generated sound kept the note's envelope since the original
    metal sound had a strong attack. The generated audio sounds like something is
    struck, like the original, but now with better harmonics.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the three cat generated sounds are also interesting. In them, the cute
    cat meow becomes an otherworldly growl. Since the NSynth model was trained on
    instrument notes and the cat sound is so different, the model has to guess, which
    results in an interesting sound. Experiment with sounds outside the training dataset,
    such as percussion; it's interesting to see what the model comes up with.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some of the clips, such as the flute + bass clip, we can hear some clicks
    in the generated audio. This happens when the model samples an extreme value and
    then corrects itself.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try and experiment with different sound combinations and durations.
    We've been using rather short samples to speed up the process, but you can use
    samples as long as you want. Just remember that the NSynth dataset contains only
    single notes that are 4 seconds long, meaning the generation of multiple consequent
    notes for longer samples will result in the model guessing the transition between
    them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Using NSynth generated samples as instrument notes
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a bunch of generated samples from NSynth, we can sequence them
    using MIDI. The easiest way to do this is to use a **Digital Audio Workstation**
    (**DAW**). Since this requires writing specific code to make Magenta send MIDI
    to the DAW, we'll be dedicating a section on this topic in [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml),
    *Making Magenta Interact with Music Applications*. If you want to try that now,
    you can skip ahead and return here later.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Using the command line
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The command line is limited for NSynth, but you can still generate audio clips.
    If you haven''t done already, you''ll need to download and extract the checkpoint
    in the `checkpoints/wavenet-ckpt` folder. While in this chapter''s code, use the
    following command to generate audio from the audio clips in the `sounds` folder
    (warning: this process takes a long time):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By using `batch_size=4` and `sample_length=16000`, you make sure that code runs
    as fast as possible. The resulting files will be in the `output/nsynth` folder,
    with names in the `gen_FILENAME.wav` format, where `FILENAME` is the source filename.
    You'll see a generated audio clip for each source sound, resulting in four audio
    clips.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated clips were produced by encoding the audio and then synthesizing
    it. Compare them with the original audio: it will give you a feel of the NSynth
    sound.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: More of NSynth
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is more to NSynth than we've shown here, such as more advanced use of
    interpolation and mixing, time stretching, and more. NSynth has produced interesting
    projects, such as a mobile application (mSynth) and physical hardware (NSynth
    Super). Refer to the *Further reading* section for more information on NSynth.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Using GANSynth as a generative instrument
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used NSynth to generate new sound samples by combining
    existing sounds. You may have noticed that the audio synthesis process is very
    time-consuming. This is because autoregressive models, such as WaveNet, focus
    on a single audio sample, which makes the resulting reconstruction of the waveform
    really slow because it has to process them iteratively.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: GANSynth, on the other hand, uses upsampling convolutions, making the training
    and generation processing in parallel possible for the entire audio sample. This
    is a major advantage over autoregressive models such as NSynth since those algorithms
    tend to be I/O bound on GPU hardware.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of GANSynth are impressive:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**Training** on the NSynth dataset converges in ~3-4 days on a single V100
    GPU. For comparison, the NSynth WaveNet model converges in 10 days on 32 K40 GPUs.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthesizing** a 4-second audio sample takes 20 milliseconds in GANSynth
    on a TitanX GPU. For comparison, the WaveNet baseline takes 1,077 seconds, which
    is 50,000 times slower.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important implication of GANs is that the model has a spherical Gaussian
    prior, which is decoded to produce the entire sound, making the interpolations
    between two samples smoother and without additional artifacts, unlike WaveNet
    interpolation. This is because WaveNet autoencoders such as NSynth have limited
    scope when learning local latent codes that control generation on the scale of
    milliseconds.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll be making use of GANSynth to generate a 30-second audio
    clip by taking a MIDI file and playing it using random instruments sampled from
    the model's latent space. Each instrument will be played for a limited amount
    of time over the course of the audio track, for example, for 5 seconds each, blending
    between one another when the instrument changes happen.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the acoustic model
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Magenta provides two pre-trained GANSynth models: `acoustic_only`, where the
    model was trained only on acoustic instruments, and  `all_instruments`, where
    the model was trained on the whole NSynth dataset (see the previous section, *The
    NSynth dataset*, for more information on the dataset).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: We're going to use the `acoustic_only` dataset for our example since the resulting
    audio track of a Bach score will sound more natural in terms of instrument choices.
    If you want to produce a wider generation, use the `all_instruments` model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: You can follow this example in the `chapter_05_example_02.py` file in the source
    code of this chapter. There are more comments and content in the source code,
    so you should go check it out.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also contains a MIDI clip in the `midi` folder that we'll use for
    this section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the model, use the following method, which will download the model
    and extract it:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using the `acoustic_only` checkpoint name, the resulting checkpoint will be
    usable using the `checkpoints/acoustic_only` path.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Getting the notes information
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start this example, we'll be loading a MIDI file that will serve as the backing
    score for the audio generation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the MIDI file using the `load_midi` method in the `magenta.models.gansynth.lib.generate_util`
    module:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We've provided a MIDI file in the `midi` folder, but you can also provide a
    MIDI file you like, for example, a generation from a previous chapter. The `load_midi`
    method then returns a dictionary of information about the notes in the MIDI file,
    such as a list of pitches, velocities, and start and end times.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The provided `cs1-1pre-short.mid` file looks like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a83f379a-bade-42e6-968f-ecc1ede516b4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: You can see the MIDI file is 28 seconds long (14 bars at 120 QPM) and contains
    two instruments.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Gradually sampling from the latent space
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the information about the MIDI file (in the `notes` variable),
    we can generate the audio from it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the `generate_audio` method:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method has four important parts, which we'll explain in the following three
    subsections—getting the random instrument, getting the latent vectors, generating
    the samples from the latent vectors, then combining the notes into a full audio
    clip.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Generating random instruments
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_random_instruments` method from `magenta.models.gansynth.lib.generate_util`
    looks like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using a sample of 28 seconds with 5 seconds per instrument gives us `n_instruments`
    of `5`, then the latent vectors get initialized by the model''s `generate_z` method, which
    is a sampling of the normal distribution:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This results in a `z_instruments` shape of (5, 256), 5 being the number of instruments
    and 256 the size of the latent vector. Finally, we take five steps of equal distance
    between the start and end time of the sequence in `t_instruments`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Getting the latent vectors
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `get_z_notes` method from `magenta.models.gansynth.lib.generate_util` looks
    like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This method takes each start note times and finds which instrument (previous
    instrument, `t_left`, and the next instrument, `t_right`) should be used for it.
    It then finds at what position the note is between the two instruments, in `interp`,
    to call the `slerp` method, which will find the proper latent vector that corresponds
    to the instrument between the two nearest vectors. This enables smooth transition
    from one instrument to the other.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Generating the samples from the encoding
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We won''t be looking into the details of the `generate_samples_from_z` method
    from `magenta.models.gansynth.lib.model`. We''ll just use this code snippet to
    illustrate what we introduced in the *Using GANSynth as a generative instrument *section about
    the model generating the audio clip as a whole:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For our example, this method will iterate 27 times to process all of the labels
    by chunks of 8 `labels` and `z` at the same time (our `batch_size` is 8). The
    bigger the batch size, the more waves it can generate in parallel. You can see
    that, contrary to NSynth, the audio samples are not generated one by one.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once all of the audio chunks are generated, `combine_notes` from the
    `magenta.models.gansynth.lib.generate_util` module will generate the audio using
    the audio clip and the MIDI notes. Basically, what the method does is calculate
    an envelope for each MIDI note that will let the proper portion of the audio clip
    be heard when a note is triggered.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve defined and explained the different parts of the code, let''s
    call the corresponding method to generate the audio clip from the MIDI file using
    gradually interpolated instruments:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The generated rainbowgram looks like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43de3426-6e39-4415-a519-21020de7f544.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: The diagram doesn't tell us much about the sound, apart from seeing the progression
    of the notes in the whole audio clip. Go and listen to the generated clip. Multiple
    generations will introduce different instruments; make sure you test it multiple
    times and with multiple MIDI files to get a feel of the possibilities in terms
    of instrument generation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Using the command line
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GANSynth command-line utility makes it possible to generate an audio clip
    from a MIDI file, like we''ve done with the Python code. If you haven''t done
    already, you''ll need to download and extract the checkpoint into the `checkpoints/wavenet-ckpt` folder. While
    in this chapter''s code, use the following command and an audio clip from the
    MIDI file from the `midi` folder (warning: this process takes a long time):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The resulting file will be in the `output/gansynth` folder and will be called
    `generated_clip.wav`. As in our example, the generated clip contains multiple
    instruments that are gradually blending together. You can use the `secs_per_instrument`
    parameter to change the time each instrument will play.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at audio generation using two models, NSynth and
    GANSynth, and produced many audio clips by interpolating samples and generating
    new instruments. We started by explaining what WaveNet models are and why they
    are used in audio generation, particularly in text-to-speech applications. We
    also introduced WaveNet autoencoders, an encoder and decoder network capable of
    learning its own temporal embedding. We talked about audio visualization using
    the reduced dimension of the latent space in rainbowgrams.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Then, we showed the NSynth dataset and the NSynth neural instrument. By showing
    an example of combining pairs of sounds, we learned how to mix two different encodings
    together in order to then synthesize the result into new sounds. Finally, we looked
    at the GANSynth model, a more performant model for audio generation. We showed
    the example of generating random instruments and smoothly interpolating between
    them.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: This chapter marks the end of the music generation content of this book—you
    can now generate a full song using MIDI as the backing score and neural instruments
    as the audio. During the course of the previous chapters, we've been using pre-trained
    models to show that the models in Magenta are ready to use and quite powerful.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, there are many reasons to train your own models, as we'll see in
    the following chapters. In [Chapter 6](1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml),
    *Data Preparation for Training*, we'll be looking into preparing datasets for
    specific genres of music and for specific instruments. In [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*, we'll be using those datasets to train our own models
    that we can then use to generate new genres and instruments.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is generating audio hard?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What makes the WaveNet autoencoder interesting?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different colors in a rainbowgram? How many are there?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you timestretch an audio clip, slowing it down by 2 seconds, using
    NSynth?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is GANSynth faster that NSynth?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What code is required to sample 10 instruments from GANSynth latent space?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Audio Signals in Python:** An article on plotting audio signals in Python,
    explaining how to create a CQT plot ([myinspirationinformation.com/uncategorized/audio-signals-in-python/](http://myinspirationinformation.com/uncategorized/audio-signals-in-python/))'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant-Q transform toolbox for music processing:** A paper (2010) on implementing
    CQTs for music ([www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing](https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing))'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet: A generative model for raw audio:** A DeepMind article on WaveNet
    models for raw audio ([deepmind.com/blog/article/wavenet-generative-model-raw-audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio))'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet: A Generative Model for Raw Audio:** A WaveNet paper (2016) ([arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499))'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SampleRNN**: An article explaining the differences between WaveNet and SampleRNN
    ([deepsound.io/samplernn_first.html](http://deepsound.io/samplernn_first.html))'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSynth: Neural Audio Synthesis:** A Magenta article on the NSynth model ([magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth))'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making a Neural Synthesizer Instrument:** More ideas on sound combinations
    and modifications ([magenta.tensorflow.org/nsynth-instrument](https://magenta.tensorflow.org/nsynth-instrument))'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate your own sounds with NSynth:** An article on fastgen and examples
    of timestretching and mixing ([magenta.tensorflow.org/nsynth-fastgen](https://magenta.tensorflow.org/nsynth-fastgen))'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders:** An NSynth
    paper (2017) ([arxiv.org/abs/1704.01279](https://arxiv.org/abs/1704.01279))'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GANSynth: Making music with GANs:** Magenta article on GANSynth ([magenta.tensorflow.org/gansynth](https://magenta.tensorflow.org/gansynth))'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GANSynth: Adversarial Neural Audio Synthesis:** A GANSynth paper (2019) ([openreview.net/forum?id=H1xQVn09FX](https://openreview.net/forum?id=H1xQVn09FX))'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using NSynth to win the Outside Hacks Music Hackathon 2017:** A Magenta article
    on mSynth ([magenta.tensorflow.org/blog/2017/09/12/outside-hacks/](https://magenta.tensorflow.org/blog/2017/09/12/outside-hacks/))'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What is NSynth Super?:** An article on NSynth Super, the NSynth hardware
    synthesizer ([nsynthsuper.withgoogle.com/](https://nsynthsuper.withgoogle.com/))'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
