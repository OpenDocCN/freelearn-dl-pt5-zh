<html><head></head><body>
		<div id="_idContainer319">
			<h1 id="_idParaDest-38"><em class="italic"><a id="_idTextAnchor073"/>Chapter 3: </em>Getting Started with Neural Networks</h1>
			<p>Before we dive into the practical implementation of deep learning networks using KNIME Analytics Platform and its integration with the Keras library, we will briefly introduce a few theoretical concepts behind neural networks and deep learning. This is the only purely theoretical chapter in this book, and it is needed to understand the how and why of the following practical implementations.</p>
			<p>Throughout this chapter, we will cover the following topics:</p>
			<ul>
				<li>Neural Networks and Deep Learning – Basic Concepts</li>
				<li>Designing your Network</li>
				<li>Training a Neural Network</li>
			</ul>
			<p>We will start with the basic concepts of neural networks and deep learning: from the first artificial neuron as a simulation of the biological neuron to the training of a network of neurons, a fully connected feedforward neural network, using a backpropagation algorithm.</p>
			<p>We will then discuss the design of a neural architecture as well as the training of the final neural network. Indeed, when designing a neural architecture, we need to appropriately select its topology, neural layers, and activation functions, and introduce some techniques to avoid overfitting.</p>
			<p>Finally, before training, we need to know when to use which loss function and what the different parameters that have to be set for training are. This will be described in the last part of this chapter.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor074"/>Neural Networks and Deep Learning – Basic Concepts</h1>
			<p>All you hear about at the moment is deep learning. Deep learning <a id="_idIndexMarker169"/>stems from the traditional discipline of neural networks, in the realm of machine learning. </p>
			<p>The field of <a id="_idIndexMarker170"/>neural networks has gone through a number of stop-and-go phases. Since the early excitement for the first perceptron in the '60s and the subsequent lull when it became evident what the perceptron could not do; through the renewed enthusiasm for the backpropagation algorithm applied to multilayer feedforward neural networks and the subsequent lull when it became apparent that training recurrent networks required hardware capabilities that were not available at the time; right up to today's new deep learning paradigms, units, and architectures running on much more powerful, possibly GPU-equipped, hardware.</p>
			<p>Let's start from the beginning and, in this section, go through the basic concepts behind neural networks and deep learning. While these basic notions might be familiar to you, especially if you have already attended a neural networks or deep learning course, we would still like to describe them here as a reference for descriptions of KNIME deep learning functionalities in the coming chapters, and for anyone who might be a neophyte to this field.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor075"/>Artificial Neuron and Artificial Neural Networks </h2>
			<p><strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>) began<a id="_idIndexMarker171"/> with simulating the biological neuron, depicted on the left in <em class="italic">Figure 3.1</em> (Abbott, L.F. (1999) <em class="italic">Lapique's introduction of the integrate-and-fire model neuron</em> (<em class="italic">1907</em><a href="https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.pdf">): https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.</a>pdf. <em class="italic">Brain Research Bulletin. 50 (5/6)</em>: 303–304). The biological neuron<a id="_idIndexMarker172"/> is a nerve cell consisting of a body (soma), a few input dendrites, and one output axon with one or more terminals. When activated, it generates a sharp electrical potential spike. The dendrites accept electrical input signals, usually from other neurons, through chemical synapses. The chemical reaction happening in the synapse enhances or reduces – in other words, it weighs – the input electrical signal before it reaches the body of the neuron. If the total electrical signal in the soma is high enough, the neuron produces an electrical spike along its axon. The axon terminals then bring out the spike to other <a id="_idIndexMarker173"/>neurons, again through chemical reactions in synapses<a id="_idTextAnchor076"/>:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16391_03_001.jpg" alt="Figure 3.1 – On the left, a biological neuron with inputs xj at dendrites and output y at the axon terminal (image from Wikipedia). On the right, an artificial neuron (perceptron) with inputs xj connected to weights wj and producing output y"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – On the left, a biological neuron with inputs xj at dendrites and output y at the axon terminal (image from Wikipedia). On the right, an artificial neuron (perceptron) with inputs xj connected to weights wj and producing output y</p>
			<p>The simplest simulation tries to reproduce a biological neuron with just two inputs and one output, like on the right in <em class="italic">Figure 3.1</em>. The input signals at the dendrites are now called <img src="image/Formula_B16391_03_001.png" alt=""/> and <img src="image/Formula_B16391_03_002.png" alt=""/> and reach the soma of the artificial cell via two weights, <img src="image/Formula_B16391_03_003.png" alt=""/> and <img src="image/Formula_B16391_03_004.png" alt=""/>  simulating the chemical reactions in the synapses. If the total input signal reaching the soma is higher than a given threshold <img src="image/Formula_B16391_03_005.png" alt=""/>, simulating the "high enough" concept, an output signal <img src="image/Formula_B16391_03_006.png" alt=""/> is generated. This simple artificial neuron is<a id="_idIndexMarker174"/> called a <strong class="bold">perceptron</strong>. </p>
			<p>Two details to clarify here: the total input signal and the threshold function. There are many<a id="_idIndexMarker175"/> neural electrical input-output voltage models (Hodgkin, A. L.; Huxley, A. F. (1952), <em class="italic">A quantitative description of membrane current and its application to conduction and excitation in </em><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413"><em class="italic">nerve</em>, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC</a>1392413, <em class="italic">The Journal of Physiology. 117 (4)</em>: 500–544). The simplest way to represent the total input signal uses a weighted sum of all input signals, where the weights represent the role of the synapse reactions. The firing function of the neuron soma can be described via a step function <img src="image/Formula_B16391_03_007.png" alt=""/>. Thus, for our simplified simulation in <em class="italic">Figure 3.1</em>, the output <img src="image/Formula_B16391_03_006.png" alt=""/> is calculated as follows:</p>
			<p><img src="image/Formula_B16391_03_009.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_010.png" alt=""/> is a step function with threshold <img src="image/Formula_B16391_03_011.png" alt=""/>:</p>
			<p><img src="image/Formula_B16391_03_012.png" alt=""/> with <img src="image/Formula_B16391_03_013.png" alt=""/>.</p>
			<p>Generalizing to a neuron with <img src="image/Formula_B16391_03_014.png" alt=""/> input signals and with any other activation function <img src="image/Formula_B16391_03_015.png" alt=""/>, we get the following formula:</p>
			<p><img src="image/Formula_B16391_03_016.png" alt=""/></p>
			<p>Here, the threshold <img src="image/Formula_B16391_03_017.png" alt=""/> has been transformed into a weight <img src="image/Formula_B16391_03_018.png" alt=""/> connected to an input signal <img src="image/Formula_B16391_03_019.png" alt=""/> that is constantly on – that is, constantly set to 1.</p>
			<p>However, one single<a id="_idIndexMarker176"/> artificial neuron, just like one single biological neuron, does not have high computational capability. It can implement just a few simple functions, as we will see later in the next sub-section, <em class="italic">Understanding the need for hidden layers</em>. As in the biological world, networks of neurons have a much bigger computational potential than one single neuron alone. Networks of biological neurons, such as even simple brains, can learn and carry out very complex tasks. Similarly, networks of artificial neurons can learn and carry out very complex tasks. The key to the success of neural networks is this flexibility in forming more or less complex architectures and in training them to perform more or less complex tasks.</p>
			<p>An example of a network of perceptrons is shown in <em class="italic">Figure 3.2</em>. This network has three layers of neurons: an input layer accepting the input signals <img src="image/Formula_B16391_03_020.png" alt=""/>; a first hidden layer with two neurons connected to the outputs of the input layer; a second hidden layer with three neurons connected to the outputs of the first hidden layer; and finally an output layer with one neuron only, fed by the outputs of the hidden layer and producing the final output <img src="image/Formula_B16391_03_021.png" alt=""/> of the network. Neurons are<a id="_idIndexMarker177"/> indicated by a circle including the <img src="image/Formula_B16391_03_022.png" alt=""/> symbol for the weighted sum of the input signals and the <img src="image/Formula_B16391_03_023.png" alt=""/> symbol for the activation function:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B16391_03_002.jpg" alt="Figure 3.2 – On the left, a network of biological neurons (image from Wikipedia). On the right, a network of artificial neurons (multi-layer perceptron)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – On the left, a network o<a id="_idTextAnchor077"/>f biological neurons (image from Wikipedia). On the right, a network of artificial neurons (multi-layer perceptron)</p>
			<p>Notice that in this particular architecture, all connections move from the input to the output layer: this is a fully connected feedforward architecture. Of course, a <strong class="bold">feedforward neural network</strong> can <a id="_idIndexMarker178"/>have as many hidden layers as you want, and each neural layer can have as many artificial neurons as you want. A feedforward network of perceptrons is called <a id="_idIndexMarker179"/>a <strong class="bold">Multi-Layer Perceptron</strong> (<strong class="bold">MLP</strong>).</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor078"/>Signal Propagation within a Feedforward Neural Network</h2>
			<p>A simple <a id="_idIndexMarker180"/>fully connected feedforward <a id="_idIndexMarker181"/>neural network can then be described as a function that transforms the input values <img src="image/Formula_B16391_03_024.png" alt=""/> into the output value <img src="image/Formula_B16391_03_021.png" alt=""/>, through a series of intermediate values <img src="image/Formula_B16391_03_026.png" alt=""/>: the outputs of the hidden layers. For example, for the network in <em class="italic">Figure 3.3</em>, we have the following:</p>
			<p><img src="image/Formula_B16391_03_027.png" alt=""/></p>
			<p><img src="image/Formula_B16391_03_028.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_029.png" alt=""/> is the number of input values (an<a id="_idTextAnchor079"/>d input units) <img src="image/Formula_B16391_03_024.png" alt=""/>, <img src="image/Formula_B16391_03_031.png" alt=""/> is the number of hidden neurons with output values <img src="image/Formula_B16391_03_032.png" alt=""/>, <img src="image/Formula_B16391_03_021.png" alt=""/> is the final output value (the only one in this architecture), and <img src="image/Formula_B16391_03_034.png" alt=""/> is the neurons' activation functions. If we number the neural layers progressively from the input to the output, we will label layer 1 as the input layer, layer 2 as the hidden layer, and layer 3 as the output layer. This progressive numbering of the neural layers is also contained in the weight and hidden unit notations. <img src="image/Formula_B16391_03_035.png" alt=""/> is the output value of the <img src="image/Formula_B16391_03_036.png" alt=""/> neural unit in the <img src="image/Formula_B16391_03_037.png" alt=""/> (hidden) layer, and <img src="image/Formula_B16391_03_038.png" alt=""/> is the<a id="_idIndexMarker182"/> weight connecting <a id="_idIndexMarker183"/>neural unit <img src="image/Formula_B16391_03_039.png" alt=""/> in layer <img src="image/Formula_B16391_03_040.png" alt=""/> with neural unit <img src="image/Formula_B16391_03_036.png" alt=""/> in layer <img src="image/Formula_B16391_03_042.png" alt=""/>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Notice that, in this notation, <img src="image/Formula_B16391_03_043.png" alt=""/> and <img src="image/Formula_B16391_03_044.png" alt=""/> in <img src="image/Formula_B16391_03_045.png" alt=""/> and in <img src="image/Formula_B16391_03_046.png" alt=""/> are NOT exponents. They just describe the network layer of the output unit for <img src="image/Formula_B16391_03_045.png" alt=""/> and the destination layer for <img src="image/Formula_B16391_03_048.png" alt=""/>.</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B16391_03_003.jpg" alt="Figure 3.3 – Fully connected feedforward neural network with just one hidden layer and one output unit"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Fully connected feedforward neural network with just one hidden lay<a id="_idTextAnchor080"/>er and one output unit</p>
			<p>There<a id="_idIndexMarker184"/> are many types of <strong class="bold">activation functions</strong> <img src="image/Formula_B16391_03_049.png" alt=""/>. We <a id="_idIndexMarker185"/>have seen the <a id="_idIndexMarker186"/>step function in the previous section, which however has a main flaw: it is neither continuous nor derivable. Some similar activation functions have been introduced over the years, which are easier to handle since they are continuous and derivable everywhere. Common <a id="_idIndexMarker187"/>examples are the <strong class="bold">sigmoid</strong> and the <strong class="bold">hyperbolic tangent,</strong> <img src="image/Formula_B16391_03_050.png" alt=""/>. Recently, a <a id="_idIndexMarker188"/>new activation function, named <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>), has been introduced, which seems to<a id="_idIndexMarker189"/> perform better with fully connected feedforward neural networks with many hidden layers. We will describe these activation functions in detail in the coming chapters.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Usually, neurons in the same layer have the same activation function <img src="image/Formula_B16391_03_051.png" alt=""/> Different layers, though, can have different activation functions.</p>
			<p>Another <a id="_idIndexMarker190"/>parameter of a network is its <strong class="bold">topology</strong>, or architecture. We have seen a fully connected feedforward network, where<a id="_idIndexMarker191"/> all connections move from the input toward the output and, under this constraint, all units are connected to all units in the next layer. However, this is of course not the only possible neural topology. Cross-connections within the same layer <img src="image/Formula_B16391_03_044.png" alt=""/>, backward connections from layer <img src="image/Formula_B16391_03_044.png" alt=""/> to layer <img src="image/Formula_B16391_03_054.png" alt=""/>, and autoconnections of a single neuron with itself are also possible.</p>
			<p>Different connections and different architectures produce different data processing functions. For example, autoconnections introduce a time component, since the current output of neuron <img src="image/Formula_B16391_03_036.png" alt=""/> at time <img src="image/Formula_B16391_03_056.png" alt=""/> will be an additional input for the same neuron <img src="image/Formula_B16391_03_036.png" alt=""/> at time <img src="image/Formula_B16391_03_058.png" alt=""/>; a feedforward neural network with as many outputs as inputs can implement an autoencoder and be used for compression or for outlier detection. We will see some of these different neural architectures and the tasks they can implement later in this book. For now, we just give you a little taste of possible neural topologies in <em class="italic">Figure 3.4</em>:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B16391_03_004.jpg" alt="Figure 3.4 – Some examples of neural network topologies"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Some examples of neural network topologies</p>
			<p>The first <a id="_idIndexMarker192"/>network from<a id="_idTextAnchor081"/> the left in <em class="italic">Figure 3.4</em> has its <a id="_idIndexMarker193"/>neurons all completely connected, so that the definition of the layer becomes unnecessary. This is a Hopfield network and is generally used as an associative memory.</p>
			<p>The second network is a feedforward autoencoder: three layers, as many <img src="image/Formula_B16391_03_029.png" alt=""/> input units as many <img src="image/Formula_B16391_03_029.png" alt=""/> output units, and a hidden layer with <img src="image/Formula_B16391_03_061.png" alt=""/> units, where usually <img src="image/Formula_B16391_03_062.png" alt=""/>; this network architecture has been adopted for outlier detection or to implement a dimensionality reduction of the input space.</p>
			<p>Finally, the<a id="_idIndexMarker194"/> third network presents units with<a id="_idIndexMarker195"/> autoconnections. As said before, autoconnections introduce a time component within the function implemented by the network and therefore are often adopted for time series analysis. This last network qualifies as a recurrent neural network.</p>
			<p>Let's go back to fully connected feedforward neural networks. Now that we've seen how they are structured, let's try to understand why they are built this way.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor082"/>Understanding the Need for Hidden Layers</h2>
			<p>The <a id="_idIndexMarker196"/>question now is: do we really need such complex neural architectures? What can the perceptron in <em class="italic">Figure 3.1</em> do and what can it not do? A perceptron, using a step function as the activation function, implements a line (a linear combination of the input signals) as a discriminant surface in a two-dimensional space, the parameters of the line being the weights and threshold of the perceptron.</p>
			<p>Classic examples are the <strong class="bold">OR</strong> and <strong class="bold">AND</strong> problems, which can be solved by a line separating the "1" outputs from the "0" outputs. Therefore, a perceptron can implement a solution to both problems. However, it cannot implement a solution to the <strong class="bold">XOR</strong> problem. The XOR function outputs "1" when the two inputs are different (one is "0" and one is "1") and outputs "0" when the two inputs are the same (both are "0" or both are "1"). Indeed, the XOR operator is a nonlinearly separable problem and one line only is not sufficient to separate the "1" outputs from the "0" outputs (<em class="italic">Figure 3.5</em>):</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B16391_03_005.jpg" alt="Figure 3.5 – A perceptron implements a linear discriminant surface, which is a line in a two-dimensional space. All linearly separable problems can be solved by a single perceptron. A perceptron cannot solve non-linearly separable problems"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – A perceptron implements a linear discriminant surface, which is a line in a two-dime<a id="_idTextAnchor083"/>nsional space. All linearly separable problems can be solved by a single perceptron. A perceptron cannot solve non-linearly separable problems</p>
			<p>The only <a id="_idIndexMarker197"/>possibility to solve the XOR problem is to add one hidden layer with two units into the perceptron architecture, making it into an MLP (<em class="italic">Figure 3.6</em>). The two hidden units in green and red each implement one line to separate some "0"s and "1"s. The one unit in the output layer then builds a new line on top of the two previous lines and implements the final discriminant:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B16391_03_006.jpg" alt="Figure 3.6 – One additional hidden layer with two units enables the MLP to solve the XOR problem"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – One additional hidden layer with two units enables the MLP to solve the XOR problem</p>
			<p>The example in <em class="italic">Figure 3.7</em> shows a three-layer network: one input layer receiving input values <img src="image/Formula_B16391_03_063.png" alt=""/> and <img src="image/Formula_B16391_03_064.png" alt=""/>, one hidden layer with two units, and one output layer with one unit only. The two hidden units implement two discrimination lines: <img src="image/Formula_B16391_03_065.png" alt=""/> for the red unit and <img src="image/Formula_B16391_03_066.png" alt=""/> for the orange unit. The output line <a id="_idIndexMarker198"/>implements a discrimination line on top of these two as <img src="image/Formula_B16391_03_067.png" alt=""/>, which is identified by the green area in the plane shown in <em class="italic">Figure 3.7</em>:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B16391_03_007.jpg" alt="Figure 3.7 – The network on the left fires up only for the points in the green zone in the input space, as depicted on the right"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – The network on the left fires up only for the points in the green zone in the input space, as <a id="_idTextAnchor084"/>depicted on the right</p>
			<p>As you <a id="_idIndexMarker199"/>see, adding just one hidden layer makes the neural network much more powerful in terms of possible functions to implement. However, there is more. The <strong class="bold">Universal Approximation Theorem</strong> states that a simple feedforward network<a id="_idIndexMarker200"/> with a single hidden layer and a sufficient number of neurons can approximate any continuous function on compact subsets of <img src="image/Formula_B16391_03_068.png" alt=""/>, under mild assumptions on the activation function and assuming that the network has been sufficiently trained (Hornik K., Stinchcombe M., White H. (1989) <em class="italic">Multilayer feedforward networ</em><a href="http://www.sciencedirect.com/science/article/pii/0893608089900208"><em class="italic">ks are universal approximators</em>: http://www.sciencedirect.com/scie</a>nce/article/pii/0893608089900208 <em class="italic">Neural Networks, Vol. 2, Issue 5</em>, (<em class="italic">1989</em>) Pages 359-366). This theorem proves that neural networks have a kind of <em class="italic">universality</em> property. That is, any function can be approximated by a sufficiently large and sufficiently trained neural network. Sufficiently large refers to the number of neurons in a feedforward network. In addition, the cited paper refers to network architectures with just one single hidden layer with enough neurons.</p>
			<p>Even very simple network architectures, thus, can be very powerful! Of course, this is all true under the assumption of a sufficiently large hidden layer (which might become too large for a reasonable training time) and a sufficient training time.</p>
			<p class="author-quote"> A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly, (Goodfellow I., Bengio Y., Courville A. (2016). <em class="italic">Deep Learning</em>,<em class="italic"> MIT Press</em>).</p>
			<p>We have seen that introducing one or more hidden layers to a feedforward neural network makes it extremely powerful. Let's see how to train it.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor085"/>Training a Multilayer Perceptron</h2>
			<p>A neural network includes a few free parameters: the topology, the weights, and the parameters of the activation functions. Let's consider a fully connected feedforward network with a pre-defined activation function for all neurons, such as a sigmoid function, for example. Then, the only free parameters left are the weights. </p>
			<p>Training a neural network means showing examples from the training set repeatedly and each time adjusting the parameter values, the weights, to fit a loss function, calculated on the desired input-output behavior. To find the weights that best fit the loss functions, the gradient descent algorithm or variants of <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) are used. The idea is to update the weights by taking steps in the direction of steepest descent on the error surface. The direction of steepest descent is equivalent to the negative of the gradient. To calculate the gradient efficiently, the backpropagation algorithm is used. Let's find out how it works.</p>
			<h3>The math behind backpropagation</h3>
			<p>A classic loss function for regression problems is the total squared error, defined as follows:</p>
			<p><img src="image/Formula_B16391_03_069.png" alt=""/></p>
			<p>H<a id="_idTextAnchor086"/>ere, <img src="image/Formula_B16391_03_070.png" alt=""/> and <img src="image/Formula_B16391_03_071.png" alt=""/> are respectively the desired target and the real answer for output unit <img src="image/Formula_B16391_03_072.png" alt=""/>, and the sum runs on all units of the output layer and on all examples in the training set <img src="image/Formula_B16391_03_073.png" alt=""/>.</p>
			<p>If we adopt the gradient descent strategy to reach a minimum in the loss function surface, at each training iteration, each weight of the network must be incremented in the opposite direction of the derivative of <img src="image/Formula_B16391_03_074.png" alt=""/> in the weight space (Goodfellow I., Bengio Y., Courville A. (2016. <em class="italic">Deep Learning</em>, MIT Press):</p>
			<p><img src="image/Formula_B16391_03_075.png" alt=""/></p>
			<p>This partial derivative of the error with respect to the weight is calculated using the chain rule:</p>
			<p><img src="image/Formula_B16391_03_076.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_077.png" alt=""/> is th<a id="_idTextAnchor087"/>e loss function, <img src="image/Formula_B16391_03_078.png" alt=""/> the output of neuron j, <img src="image/Formula_B16391_03_079.png" alt=""/> its total input, and <img src="image/Formula_B16391_03_080.png" alt=""/> its input weight from neuron <img src="image/Formula_B16391_03_081.png" alt=""/> in the previous layer:</p>
			<p>For the weights connecting to units <strong class="bold">in the output layer</strong>, the derivatives will be as follows:</p>
			<p><img src="image/Formula_B16391_03_082.png" alt=""/></p>
			<p><img src="image/Formula_B16391_03_083.png" alt=""/></p>
			<p><img src="image/Formula_B16391_03_084.png" alt=""/></p>
			<p>So, finally:</p>
			<p><img src="image/Formula_B16391_03_085.png" alt=""/></p>
			<p>Therefore, the weight change for weights connecting to output units is as follows: </p>
			<p><img src="image/Formula_B16391_03_086.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_087.png" alt=""/>, <img src="image/Formula_B16391_03_088.png" alt=""/> is the input <img src="image/Formula_B16391_03_089.png" alt=""/> t<a id="_idTextAnchor088"/>o the output node <img src="image/Formula_B16391_03_072.png" alt=""/>, and <img src="image/Formula_B16391_03_091.png" alt=""/> is the learning rate.</p>
			<p>For the weights connecting to the <strong class="bold">units in a hidden layer</strong>, the calculation of the derivative, and therefore of the weight change, is a bit more complicated. While the last two derivatives remain the same also when referring to neurons in hidden layers, <img src="image/Formula_B16391_03_092.png" alt=""/> will need to be recalculated.</p>
			<p>If we consider the loss function <img src="image/Formula_B16391_03_074.png" alt=""/> as a function of all input sums to all neurons <img src="image/Formula_B16391_03_094.png" alt=""/> in the next layer connected to neuron j, as <img src="image/Formula_B16391_03_095.png" alt=""/>, after a few math operations we reach a recursive expression:</p>
			<p><img src="image/Formula_B16391_03_096.png" alt=""/></p>
			<p>Here, the following applies:</p>
			<p><img src="image/Formula_B16391_03_097.png" alt=""/></p>
			<p>The update formula for all weights, leading to output or hidden neurons, is this:</p>
			<p><img src="image/Formula_B16391_03_098.png" alt=""/></p>
			<p>This recursive formula tells us that <img src="image/Formula_B16391_03_099.png" alt=""/> for unit <img src="image/Formula_B16391_03_100.png" alt=""/> in the hidden layer <img src="image/Formula_B16391_03_101.png" alt=""/> can be calculated as the linear combination of all <img src="image/Formula_B16391_03_102.png" alt=""/> in layer <img src="image/Formula_B16391_03_103.png" alt=""/>, which will be <img src="image/Formula_B16391_03_104.png" alt=""/> if this is the output layer or <img src="image/Formula_B16391_03_105.png" alt=""/> if this is another hidden layer. This means that moving from the output layer backward toward the input layer, we can calculate all <img src="image/Formula_B16391_03_106.png" alt=""/>, starting from <img src="image/Formula_B16391_03_107.png" alt=""/> and then through all <img src="image/Formula_B16391_03_108.png" alt=""/>, as a combination of <img src="image/Formula_B16391_03_109.png" alt=""/>from the preceding layer, layer after layer. Together with <img src="image/Formula_B16391_03_106.png" alt=""/>, we can also calculate all weight updates <img src="image/Formula_B16391_03_111.png" alt=""/>.</p>
			<h3>The Idea Behind Backpropagation</h3>
			<p>So, the training of a feedforward neural network can be seen as a two-step process:</p>
			<ol>
				<li>All training vectors are presented, one after the other, to the input layer of the network, and the signal is propagated throughout all network connections (and weights) till the output layer. After all of the training examples have passed through the network, the total squared error is calculated at the output layer as the sum of the single squared errors. This is the <strong class="bold">forward pass</strong>:<div id="_idContainer165" class="IMG---Figure"><img src="image/B16391_03_008.jpg" alt="Figure 3.8 – In the forward pass of the backpropagation algorithm, all training examples are presented at the input layer and forward-propagated through the network till the output layer, to calculate the output values"/></div><p class="figure-caption">Figure 3.8 – In the forward pass of the backpropagation algorithm, all training examples are presented at the input layer and forward-propagated through the network till the output layer, to calculate the output values</p></li>
				<li>All <img src="image/Formula_B16391_03_112.png" alt=""/> are calculated for all units in the output layer. Then, the <img src="image/Formula_B16391_03_113.png" alt=""/>s are backpropagated from the output layer through all network connections (and weights) till the input layer and all <img src="image/Formula_B16391_03_114.png" alt=""/> in the hidden layers are also calculated. This is the <strong class="bold">backward pass</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B16391_03_009.jpg" alt="Figure 3.9 – In the backward pass of the backpropagation algorithm, all s are calculated at the output layer and backpropagated through the network till the input layer. After all examples from the training set have passed through the network forth and back, all weights are updated"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – In the backward pass of the backpropagation algorithm, all <img src="image/Formula_B16391_03_115.png" alt=""/>s are calculated at the output layer and backpropagated through the network till the input layer. After all examples from the training set have passed through the network forth and back, all weights are updated</p>
			<p>This algorithm is called <strong class="bold">backpropagation</strong>, as a reference to the <img src="image/Formula_B16391_03_116.png" alt=""/>s backpropagating through the network during the second pass.</p>
			<p>After all the training data has passed through the network forth and back, all weights are updated.</p>
			<p>Also notice the first derivative of the unit activation function <img src="image/Formula_B16391_03_117.png" alt=""/> in <img src="image/Formula_B16391_03_118.png" alt=""/>. Of course, using a continuous derivable function <img src="image/Formula_B16391_03_119.png" alt=""/> helps with the calculations. This is the reason why the <img src="image/Formula_B16391_03_120.png" alt=""/> and <img src="image/Formula_B16391_03_121.png" alt=""/> function have been so popular with neural architectures.</p>
			<p>The gradient descent algorithm is not guaranteed to reach the global minimum of the error function, but it often ends up in a local minimum. If the local minimum does not ensure satisfactory performance of the network, the training process must be repeated starting from new initial conditions, meaning new initial values for the weights of the network.</p>
			<p>Neural networks are very powerful in implementing input-output models and very flexible in terms of architecture and parameters. It is extremely easy to build huge neural networks, by adding more and more neurons and more and more hidden layers. Besides the longer training times, an additional risk is to run quickly into the <strong class="bold">overfitting</strong> of the training data. Overfitting is a drawback of too complex models, usually with too many free parameters to fit a simple task. The result of an over-dimensioned model for a simple task is that the model, at some point, will start using the extra parameters to memorize noise and errors in the training set, considerably worsening the model's performance. The power and flexibility of neural networks make them prone to overfitting, especially if we are dealing with small training sets.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Another big objection that has been leveled against neural networks since their introduction is their non-interpretability. The adjustment of the weights has no correspondence with any entity in the data domain. When dealing with neural networks, we need to accept that we are dealing with <strong class="bold">black boxes</strong> and we might not understand the decision process.</p>
			<p>If interpretability is a requirement for our project, then maybe neural networks are not the tool for us. A few techniques have been proposed recently to extract knowledge on the decision process followed in black-box models, such as the <strong class="bold">SHAPLEY</strong> values or <strong class="bold">Partial Dependency Plots</strong> (Molnar C. <em class="italic">Interpretable Machine Learning</em>, https://christophm.github.io/interpretable-ml-book/index.html, GitHub). They are currently in their infancy and not immune from criticism. However, they constitute an interesting attempt to fix the interpretability problem of neural networks. These are beyond the scope of this book, so we will not be exploring them in any more detail.</p>
			<p>With the basic theory covered, let's get into the design of a network.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor089"/>Designing your Network</h1>
			<p>In the previous section, we learned that neural networks are characterized by a topology, weights, and activation functions. In particular, feedforward neural networks have an input and an output layer, plus a certain number of hidden layers in between. While the values for the network weights are automatically estimated via the training procedure, the network topology and the activation functions have to be predetermined during network design before training. Different network architectures and different activation functions implement different input-output tasks. Designing the appropriate neural architecture for a given task is still an active research field in the deep learning area (Goodfellow I., Bengio Y., Courville A. (2016). <em class="italic">Deep Learning</em>, MIT Press). </p>
			<p>Other parameters are involved in the training algorithm of neural networks, such as the learning rate or the loss function. We have also seen that neural networks are prone to overfitting; this means that their flexibility makes it easy for them to run into the overfitting problem. Would it be possible to contain the weight growth, to change the loss function, or to self-limit the network structure during training as to avoid the overfitting problem?</p>
			<p>This section gives you an overview of all those remaining parameters: the topology of the network, the parameters in the training algorithm, the possible activation functions, the loss functions, regularization terms, and more, always keeping an eye on containing the overfitting effect, making the training algorithm more efficient, and developing more powerful neural architectures.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor090"/>Commonly Used Activation Functions</h2>
			<p>In summary, a single neural layer has a number of inputs <img src="image/Formula_B16391_03_122.png" alt=""/> and a number of outputs <img src="image/Formula_B16391_03_123.png" alt=""/> The calculation of the output value of a neuron <img src="image/Formula_B16391_03_124.png" alt=""/> is performed in two steps:</p>
			<ol>
				<li value="1">Calculation of the weighted sum of the inputs plus a bias <img src="image/Formula_B16391_03_125.png" alt=""/>:<p><img src="image/Formula_B16391_03_126.png" alt=""/> for <img src="image/Formula_B16391_03_127.png" alt=""/> and <img src="image/Formula_B16391_03_128.png" alt=""/></p></li>
				<li>Application of an activation function <img src="image/Formula_B16391_03_129.png" alt=""/> or <img src="image/Formula_B16391_03_130.png" alt=""/>to calculate the output <img src="image/Formula_B16391_03_131.png" alt=""/> based on the weight matrix <img src="image/Formula_B16391_03_132.png" alt=""/> and either on <img src="image/Formula_B16391_03_133.png" alt=""/> or on <img src="image/Formula_B16391_03_134.png" alt=""/>:</li>
			</ol>
			<p><img src="image/Formula_B16391_03_135.png" alt=""/></p>
			<p>Note that <img src="image/Formula_B16391_03_136.png" alt=""/>is the weighted sum of the input values to the <img src="image/Formula_B16391_03_137.png" alt=""/> th neuron and <img src="image/Formula_B16391_03_138.png" alt=""/> is the vector of all weighted input sums.</p>
			<p>A network can then also be seen as a chain of functions <img src="image/Formula_B16391_03_139.png" alt=""/>, where each function implements a neural layer. Depending on the network architecture, each neural layer has different input values and uses a different activation function <img src="image/Formula_B16391_03_140.png" alt=""/>, and therefore implements a different function <img src="image/Formula_B16391_03_141.png" alt=""/>, using the two calculation steps described previously.</p>
			<p>The complexity of the total function implemented by the full network also depends on the number of layers involved; that is, it depends on the network depth.</p>
			<p>A layer where all neurons are connected to all outputs of the previous layer is called a <strong class="bold">dense layer</strong>. Fully connected feedforward networks are just a chain of dense layers, where each layer has its own activation function. In feedforward neural networks, then, a function <img src="image/Formula_B16391_03_142.png" alt=""/> is based on the number of the layer's neurons, the number of inputs, and the activation function. The key difference between layers is then the activation function. Let's look at the most commonly used activation functions in neural networks.</p>
			<h3>Sigmoid Function</h3>
			<p>The <strong class="bold">sigmoid function</strong> is an S-shaped function with values between <img src="image/Formula_B16391_03_143.png" alt=""/> and <img src="image/Formula_B16391_03_144.png" alt=""/>. For the <img src="image/Formula_B16391_03_137.png" alt=""/>th neuron in the layer, the function is defined as follows:</p>
			<p><img src="image/Formula_B16391_03_146.png" alt=""/></p>
			<p>It is plotted on the left in <em class="italic">Figure 3.10</em>.</p>
			<p>For binary classification problems, this is the go-to function for the output neural layer, as the value range <img src="image/Formula_B16391_03_147.png" alt=""/> allows us to interpret the output as the probability of one of the two classes. In this case, the output neural layer consists of only one neuron, AKA unit size 1, with the sigmoidal activation function. Of course, the same function can also be used as an activation function for output and hidden layers with a bigger unit size:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B16391_03_010.jpg" alt="Figure 3.10 – The sigmoid function (on the left) can be used as the activation function of the single output neuron of a network implementing the solution to a binary classification problem (in the center). It can be used generically as an activation function for neurons placed in hidden or output layers in a network (on the right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – The sigmoid function (on the left) can be used as the activation function of the single output neuron of a network implementing the so<a id="_idTextAnchor091"/>lution to a binary classification problem (in the center). It can be used generically as an activation function for neurons placed in hidden or output layers in a network (on the right)</p>
			<p>One of the biggest advantages of the sigmoid function is its derivability everywhere and its easy derivative expression. Indeed, when using the sigmoid activation function, the weight update rule for the backpropagation algorithm becomes very simple, since the first derivative of the activation function is simply <img src="image/Formula_B16391_03_148.png" alt=""/>, where <img src="image/Formula_B16391_03_149.png" alt=""/> is the output of neuron j.</p>
			<p>On the other hand, one of the biggest disadvantages of using sigmoid as the neurons' activation function in more complex or deep neural architectures is the vanishing gradient problem. Indeed, when calculating the derivatives to update the network weights, the chain multiplication of output values (&lt; 1) from sigmoid functions might produce very small values. In this case, too small gradients are produced at each training iteration, leading to slow convergence for the training algorithm. </p>
			<h3>Hyperbolic Tangent (Tanh)</h3>
			<p>A similar activation function is <strong class="bold">hyperbolic tangent</strong>, <strong class="bold">tanh</strong> for short. It is also an S-shaped function with the difference that the output values fall between <img src="image/Formula_B16391_03_150.png" alt=""/> and 1, instead of between <img src="image/Formula_B16391_03_151.png" alt=""/> and <img src="image/Formula_B16391_03_152.png" alt=""/> For the <img src="image/Formula_B16391_03_124.png" alt=""/> th neuron, the function is defined as follows:</p>
			<p><img src="image/Formula_B16391_03_154.png" alt=""/></p>
			<p>It is plotted on the left in <em class="italic">Figure 3.11</em>:</p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/B16391_03_011.jpg" alt="Figure 3.11 – The hyperbolic tangent function, tanh(), is also often used as an activation function for neural units. In this case, the neuron output value falls in (-1, +1)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – The hyperbolic tangent function, tanh(), is also often used as an activation function for neural units. In this case, the neuron output value falls <a id="_idTextAnchor092"/>in (-1, +1)</p>
			<p>Here also, one of the biggest advantages of the <img src="image/Formula_B16391_03_155.png" alt=""/> function is its continuity and its <a id="_idIndexMarker201"/>derivability everywhere, which leads to simpler formulas<a id="_idIndexMarker202"/> for the updates of the weights in the training algorithm. <img src="image/Formula_B16391_03_156.png" alt=""/> also has the advantage of being centered at 0, which can help to stabilize the training process.</p>
			<p>Again, one of the biggest disadvantages of using tanh as an activation function in complex or deep neural architectures is the vanishing gradient problem.</p>
			<h3>Linear Function</h3>
			<p>A special activation<a id="_idIndexMarker203"/> function<a id="_idIndexMarker204"/> is the <strong class="bold">linear activation function</strong>, also known<a id="_idIndexMarker205"/> as the identity function:</p>
			<p><img src="image/Formula_B16391_03_157.png" alt=""/></p>
			<p>When would such a function be used? A neural layer with a linear activation function implements a linear regression model. Sometimes, a neural layer with a linear activation function is also introduced to keep the original network response, before it is<a id="_idIndexMarker206"/> transformed to get the required range or probability<a id="_idIndexMarker207"/> score. In this case, the last layer of the network is split into two layers: one with the linear activation function preserves the original output and the other one applies another activation function for the required output format.</p>
			<p>In <a href="B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 7</em></a>, <em class="italic">Implementing NLP Applications</em>, where we describe the <em class="italic">Generating Product Name</em> case study, this approach is used to introduce a new parameter called <strong class="bold">temperature</strong> after the linear activation function layer.</p>
			<h3>Rectified Linear Unit</h3>
			<p>We have seen that deep neural networks, using the sigmoid or tanh activation functions, often suffer from the <a id="_idIndexMarker208"/>problem of vanishing gradient.</p>
			<p>An activation function<a id="_idIndexMarker209"/> that helps to overcome the problem of vanishing gradient is the <strong class="bold">Rectified Linear Unit</strong> function, <strong class="bold">ReLU</strong> for short. The ReLU function is like the linear function, at least from 0 on. Indeed, the ReLU function is <img src="image/Formula_B16391_03_158.png" alt=""/> for negative values of <img src="image/Formula_B16391_03_159.png" alt=""/> and is the identity function for positive values of <img src="image/Formula_B16391_03_159.png" alt=""/>:</p>
			<p><img src="image/Formula_B16391_03_161.png" alt=""/>.</p>
			<p><em class="italic">Figure 3.12</em> shows a plot of the ReLU function: </p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/B16391_03_012.jpg" alt="Figure 3.12 – The ReLU activation function"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – The ReLU activation function</p>
			<p>The ReLU activation <a id="_idIndexMarker210"/>function, while helping with the vanishing gradient problem, is not differentiable for <img src="image/Formula_B16391_03_162.png" alt=""/>. In practice, this is not a problem <a id="_idIndexMarker211"/>when training neural networks as usually one of the one-sided derivatives is used rather than reporting that the derivative is not defined.</p>
			<h3>Softmax Function</h3>
			<p>All activation functions<a id="_idIndexMarker212"/> introduced until now are functions that have a single value as output. This means only the weighted sum <img src="image/Formula_B16391_03_163.png" alt=""/> is used to calculate the output value of the <img src="image/Formula_B16391_03_164.png" alt=""/>th neuron, independently from weighted sums <img src="image/Formula_B16391_03_165.png" alt=""/>, with <img src="image/Formula_B16391_03_166.png" alt=""/> being used to calculate the <a id="_idIndexMarker213"/>outputs of the other neurons in the same layer. The <strong class="bold">softmax function</strong>, on the other hand, works on the whole output vector <img src="image/Formula_B16391_03_167.png" alt=""/> and not just on one single value <img src="image/Formula_B16391_03_168.png" alt=""/>.</p>
			<p>In general, the <a id="_idIndexMarker214"/>softmax function transforms a vector <img src="image/Formula_B16391_03_169.png" alt=""/> of size <img src="image/Formula_B16391_03_170.png" alt=""/> into a vector <img src="image/Formula_B16391_03_171.png" alt=""/>, which is a vector <img src="image/Formula_B16391_03_172.png" alt=""/> of the same size <img src="image/Formula_B16391_03_173.png" alt=""/> with values <a id="_idIndexMarker215"/>between <img src="image/Formula_B16391_03_158.png" alt=""/> and <img src="image/Formula_B16391_03_175.png" alt=""/>, with the constraint that all values <img src="image/Formula_B16391_03_176.png" alt=""/> sum to <img src="image/Formula_B16391_03_175.png" alt=""/>:</p>
			<p><img src="image/Formula_B16391_03_178.png" alt=""/></p>
			<p>This additional constraint allows us to interpret the components of vector <img src="image/Formula_B16391_03_179.png" alt=""/> as probabilities of different classes. Therefore, the softmax activation function is often the function of choice for the last neural layer in a multiclass classification problem. The <img src="image/Formula_B16391_03_180.png" alt=""/>th element of the output vector is calculated as follows:</p>
			<p><img src="image/Formula_B16391_03_181.png" alt=""/></p>
			<p><em class="italic">Figure 3.13</em> shows an <a id="_idIndexMarker216"/>example network that uses the softmax function<a id="_idIndexMarker217"/> in the last layer, where all output values sum up to 1:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B16391_03_013.jpg" alt="Figure 3.13 – A simple neural layer with the softmax activation function"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – A simple neural layer with the softmax activation function</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The softmax function is also used by the logistic regression algorithm for multiclass classification problems.</p>
			<h3>Other supported activation functions</h3>
			<p>Many more <a id="_idIndexMarker218"/>activation functions have been introduced over the years, since the sigmoid.</p>
			<p>Variants of<a id="_idIndexMarker219"/> ReLU are <strong class="bold">Leaky Rectified Linear Unit</strong> and <strong class="bold">Parametric Rectified Linear Unit</strong> (<strong class="bold">PReLU</strong>). LeakyReLU offers an almost zero line (<img src="image/Formula_B16391_03_182.png" alt=""/>) for <a id="_idIndexMarker220"/>negative values of the function argument rather than just zero as in the pure ReLU. PReLU makes this line with parametric slope (<img src="image/Formula_B16391_03_183.png" alt=""/>) rather than fixed slope as in the LeakyReLU. Parameter <img src="image/Formula_B16391_03_184.png" alt=""/> becomes part of the parameters that the network must train.</p>
			<p>Here are the definitions of LeakyReLU and PreLU:</p>
			<ul>
				<li>LeakyReLU:        	<img src="image/Formula_B16391_03_185.png" alt=""/></li>
				<li>PReLU: 		<img src="image/Formula_B16391_03_186.png" alt=""/></li>
			</ul>
			<p>Other variants of ReLU, introduced to remedy dead ReLUs, are <strong class="bold">Exponential Linear Unit</strong> (<strong class="bold">ELU</strong>) and <strong class="bold">Scaled Exponential Linear Unit</strong> (<strong class="bold">SELU</strong>). Similar to LeakyReLU, ELU has a<a id="_idIndexMarker221"/> small slope <a id="_idIndexMarker222"/>for negative values. Instead of a straight line, it uses a log curve. Scaled ELU adds one more parameter <img src="image/Formula_B16391_03_187.png" alt=""/> to ELU for the network to train:</p>
			<ul>
				<li>ELU:                            	<img src="image/Formula_B16391_03_188.png" alt=""/></li>
				<li>SELU:  			<img src="image/Formula_B16391_03_189.png" alt=""/></li>
			</ul>
			<p>An approximation of the sigmoid activation function is the <strong class="bold">hard sigmoid activation function</strong>. It is<a id="_idIndexMarker223"/> faster to calculate than sigmoid. Despite being an approximation of the sigmoid activation function, it still provides reasonable results on classification tasks. However, since it's just an approximation, it performs worse on regression tasks:</p>
			<ul>
				<li>Hard-Sigmoid: 		<img src="image/Formula_B16391_03_190.png" alt=""/></li>
			</ul>
			<p>The <strong class="bold">SoftPlus</strong> activation function<a id="_idIndexMarker224"/> is also quite popular. This is a smoothed version of the ReLU activation function:</p>
			<ul>
				<li>SoftPlus:  		<img src="image/Formula_B16391_03_191.png" alt=""/></li>
			</ul>
			<p>Let's look at <em class="italic">Figure 3.14</em>:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B16391_03_014.jpg" alt="Figure 3.14 – Plots of some additional popular activation functions, mainly variants of ReLU and sigmoid functions"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Plots of some additional popular activation functions, mainly variants of ReLU and sigmoid functions</p>
			<p>The images in <em class="italic">Figure 3.14</em> show the plots of the aforementioned activation functions.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor093"/>Regularization Techniques to Avoid Overfitting</h2>
			<p>No matter <a id="_idIndexMarker225"/>what kind of algorithm you use, the goal is always a model that not only performs well on the training data but also generalizes to new data.</p>
			<p>Large neural networks, trained on too small datasets, often incur the problem of fitting the training data too well and missing the capability to generalize to new data. This problem is known as overfitting. <em class="italic">Figure 3.15</em> shows a regression input-output function implemented by a neural network on the training data (full crosses) and on the test data (empty crosses). On the left, we see a regression function that does not even manage to fit the training data properly, much less the test data. This is probably<a id="_idIndexMarker226"/> due to an insufficient architecture size or short training time (<strong class="bold">underfitting</strong>). In the center, we find a regression curve decently fitting both training and test data. On the right, we have a regression curve fitting the training data perfectly and failing in the fit on the test data; this is<a id="_idIndexMarker227"/> the <strong class="bold">overfitting</strong> problem:</p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B16391_03_015.jpg" alt="Figure 3.15 – From left to right, the regression curve implemented by a network underfitting, fitting just fine, and overfitting the training data"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – From left to right, the regression curve implemented by a network underfitting, fitting just fine, and overfitting the training data</p>
			<p>How can we know in advance the right size of the neural architecture and the right number of epochs for the training algorithm? A few tricks can be adopted to address the problem of overfitting without worrying too much about the exact size of the network and the number of epochs: norm regularization, dropout, and early stopping.</p>
			<h3>Norm Regularization</h3>
			<p>One <a id="_idIndexMarker228"/>sign of overfitting<a id="_idIndexMarker229"/> is the high values of the weights. Thus, the idea behind norm regularization is to penalize weights with high values by adding a penalty term <img src="image/Formula_B16391_03_192.png" alt=""/> to the objective function, AKA the loss function:</p>
			<p><img src="image/Formula_B16391_03_193.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_194.png" alt=""/> are the true values and <img src="image/Formula_B16391_03_195.png" alt=""/> are the predicted values. A new loss function <img src="image/Formula_B16391_03_196.png" alt=""/> is obtained with the addition of this penalty term:</p>
			<p><img src="image/Formula_B16391_03_197.png" alt=""/></p>
			<p>The training algorithm, thus, while minimizing this new loss function, will reach a weight configuration <a id="_idIndexMarker230"/>with smaller values. This is a well-known <strong class="bold">regularization</strong> approach you might already<a id="_idIndexMarker231"/> know from the linear or logistic regression algorithms.</p>
			<p>The parameter <img src="image/Formula_B16391_03_198.png" alt=""/> is used to control the penalty effect. <img src="image/Formula_B16391_03_199.png" alt=""/>is equivalent to no regularization. Higher values of <img src="image/Formula_B16391_03_200.png" alt=""/> implement a stronger regularization effect and lead to smaller weights.</p>
			<p>There are two commonly used penalty <a id="_idIndexMarker232"/>norm functions: the <strong class="bold">L1 norm</strong> and the <strong class="bold">L2 norm</strong>. The L1 norm is the sum of the absolute values of the weights and the L2 norm is <a id="_idIndexMarker233"/>the sum of the squares of the weights:</p>
			<p><img src="image/Formula_B16391_03_201.png" alt=""/>		<img src="image/Formula_B16391_03_202.png" alt=""/></p>
			<p><img src="image/Formula_B16391_03_203.png" alt=""/> and <img src="image/Formula_B16391_03_204.png" alt=""/> are both common <a id="_idIndexMarker234"/>methods to avoid overfitting with one big difference. <img src="image/Formula_B16391_03_205.png" alt=""/> regularization generally leads to smaller weights but lacks the ability to reduce the weights all the way to zero. On the other hand, <img src="image/Formula_B16391_03_206.png" alt=""/> regularization allows for a few larger weights while reducing all other weights to zero. When designing a<a id="_idIndexMarker235"/> loss function, it is also possible to use a mixture of both <img src="image/Formula_B16391_03_206.png" alt=""/> and <img src="image/Formula_B16391_03_208.png" alt=""/> regularization.</p>
			<p>In addition, you can also apply regularization terms to weights <a id="_idIndexMarker236"/>of selected layers. Three different norm <a id="_idIndexMarker237"/>regularizations have been <a id="_idIndexMarker238"/>designed to act on single layers: <strong class="bold">kernel regularization</strong>, <strong class="bold">bias regularization</strong>, and <strong class="bold">activity regularization</strong>.</p>
			<p>Kernel regularization penalizes the weights, but not the biases; bias regularization penalizes the biases only; and activity regularization leads to smaller output values for the selected layer. </p>
			<h3>Dropout </h3>
			<p>Another <a id="_idIndexMarker239"/>common approach in machine learning to avoid overfitting is to introduce the dropout<a id="_idIndexMarker240"/> technique, which is another regularization technique.</p>
			<p>The idea is, at each training iteration, to ignore (drop) randomly some of the neurons in either the input layer or a hidden layer with all its input and output connections. At each iteration, different neurons are dropped. Therefore, the number of neurons in the architecture, and which of them are trained, effectively changes from iteration to iteration. The randomization introduced in this way helps to control the overfitting effect.</p>
			<p>Dropout makes sure that individual neurons and layers do not rely on single neurons in the preceding layers, thus becoming more robust and less prone to overfitting:</p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B16391_03_016.jpg" alt="Figure 3.16 – The dropout technique selects some neurons in each layer and drops them from being updated in the current training iteration. The full network on the left is trained only partially in the four training iterations described on the right."/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – The dropout technique selects some neurons in each layer and drops them from being updated in the current training iteration. The full network on the left is trained only partially in the four training iterations described on the right.</p>
			<p><strong class="bold">Dropout</strong> is <a id="_idIndexMarker241"/>applied to each layer of the <a id="_idIndexMarker242"/>network separately. This often translates into a temporary layer, the dropout layer, being inserted after the layer we want to randomize. The dropout layer controls how many neurons of the previous layer are dropped at each training iteration.</p>
			<p>To control how many neurons in a layer are dropped, a new parameter is introduced: the <strong class="bold">drop rate</strong>. The drop rate<a id="_idIndexMarker243"/> defines the fraction of neurons in the layer that should be dropped from training at each iteration.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Here are two quick tips for dropout:</p>
			<p class="callout">First, dropout leads to layers with fewer neurons and therefore reduces the layer capacity. It is recommended to start with a high number of neurons per layer.</p>
			<p class="callout">Second, dropout is only applied to the input or hidden layers, not to the output layer since we want the response of the model to always be the same at each iteration.</p>
			<h3>Early Stopping</h3>
			<p>Another option to<a id="_idIndexMarker244"/> avoid overfitting is to stop the training process before the network starts overfitting, which is <a id="_idIndexMarker245"/>known as <strong class="bold">early stopping</strong>. To detect the point where the algorithm starts to fit the training data better than the test data, an additional validation set with new data is used. During training, the network performances are monitored on both the training set and the validation set. At the beginning of the training phase, the network performance on both the training and validation sets improves. At some point, though, the performance of the network on the training set keeps improving while on the validation set it starts deteriorating. Once the performance starts to get worse on the validation set, the training is stopped.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor094"/>Other Commonly used Layers</h2>
			<p>So far, we have introduced two different kinds of layers: dense layers to design fully connected neural networks with different activation functions and the dropout layer for regularization. With these layers, you can design, for example, an autoencoder, as we will do in <a href="B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152"><em class="italic">Chapter 5</em></a>,<em class="italic"> Autoencoder for Fraud Detection</em>. But actually, there are many more layers available for all kinds of different tasks.</p>
			<h3>Convolutional Layers</h3>
			<p>One area where neural networks are<a id="_idIndexMarker246"/> extremely powerful is image analysis, for example, image classification. Feedforward neural networks are also frequently used in this area. Often, though, the sequence of dense layers is not used alone, but in combination with another series of convolutional layers. <strong class="bold">Convolutional layers</strong> are placed after the input of the neural network, to extract features and then create a better representation of the image to pass to the next dense layers – the feedforward architecture – for the<a id="_idIndexMarker247"/> classification. These networks are called <strong class="bold">Convolutional Neural Networks</strong>, <strong class="bold">CNNs</strong> for short. </p>
			<p><a href="B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316"><em class="italic">Chapter 9</em></a>, <em class="italic">Convolutional Neural Networks for Image Classification</em>, explains in detail how convolutional layers work. It will also introduce some other related neural layers that are suitable to analyze data with spatial relationships, such as the flatten layer and the max pooling layer.</p>
			<h3>Recurrent Neural Networks</h3>
			<p>A family of neural networks that doesn't belong to feedforward networks is that of <strong class="bold">Recurrent Neural Networks</strong>, <strong class="bold">RNNs</strong> for short. RNNs are obtained by introducing <a id="_idIndexMarker248"/>auto- or backward connections (recurrent connections) into feedforward neural networks. This allows the network to take context into account, since it remembers inputs from the past, and therefore it can capture the dynamic of a signal. These networks are really powerful when it comes to sequential data, such as times series data or text.</p>
			<p>Different layers for RNNs <a id="_idIndexMarker249"/>have been introduced in the <a id="_idIndexMarker250"/>past, for example, <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) layers or <strong class="bold">Gated Recurrent Unit</strong> (<strong class="bold">GRU</strong>) layers. <a href="B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181"><em class="italic">Chapter 6</em></a>, <em class="italic">Recurrent Neural Networks for Demand Prediction</em>, covers RNNs in detail as well as the architecture of LSTM units. </p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor095"/>Training a Neural Network</h1>
			<p>After <a id="_idIndexMarker251"/>network architecture and activation functions, the last design step before you can start training a neural network is the choice of loss function.</p>
			<p>We will start with an overview of possible loss functions for regression, binary classification, and multiclass classification problems. Then, we will introduce some optimizers and additional training parameters for the training algorithms.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor096"/>Loss Functions</h2>
			<p>In order to train a feedforward neural network, an appropriate error function, often called<a id="_idIndexMarker252"/> a <strong class="bold">loss function</strong>, and a matching last layer have to be selected. Let's start with an overview of commonly used loss functions for regression problems.</p>
			<h3>Loss Functions for Regression Problems</h3>
			<p>In the case of a<a id="_idIndexMarker253"/> regression problem, where the goal is to predict one single numerical value, the output layer should have one unit only and use the linear activation function. Possible loss functions to train this kind of network must refer to numerical error metrics:</p>
			<ul>
				<li><strong class="bold">Mean Squared Error </strong>(<strong class="bold">MSE</strong>)<strong class="bold"> Loss</strong>: The mean <a id="_idIndexMarker254"/>squared error is the default error metric for regression problems. For <img src="image/Formula_B16391_03_209.png" alt=""/> training samples, it is calculated as follows:</li>
			</ul>
			<p><img src="image/Formula_B16391_03_210.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_211.png" alt=""/> are the true values and <img src="image/Formula_B16391_03_212.png" alt=""/> are the predicted values. The MSE gives more importance to large error values and it is always positive. A <a id="_idIndexMarker255"/>perfect predictor would have an <img src="image/Formula_B16391_03_213.png" alt=""/> of <img src="image/Formula_B16391_03_214.png" alt=""/>.</p>
			<ul>
				<li><strong class="bold">Mean Squared Logarithmic Error </strong>(<strong class="bold">MSLE</strong>) <strong class="bold">Loss</strong>: The MSLE is a loss function that penalizes <a id="_idIndexMarker256"/>large errors less than the MSE. It is calculated by applying the logarithm on the predicted and the true values, before using the MSE. For <img src="image/Formula_B16391_03_215.png" alt=""/> training samples, it is calculated as follows:</li>
			</ul>
			<p><img src="image/Formula_B16391_03_216.png" alt=""/></p>
			<p>MSLE applies to numbers greater or equal to <img src="image/Formula_B16391_03_158.png" alt=""/>, such as prices. 1 is added to both <img src="image/Formula_B16391_03_218.png" alt=""/> and <img src="image/Formula_B16391_03_219.png" alt=""/> to avoid having <img src="image/Formula_B16391_03_220.png" alt=""/></p>
			<p>This loss<a id="_idIndexMarker257"/> function is recommended if the range of the target values is large and larger errors shouldn't be penalized significantly more than smaller errors. The MSLE is always positive and a perfect model has a loss of <img src="image/Formula_B16391_03_221.png" alt=""/>.</p>
			<ul>
				<li><strong class="bold">Mean Absolute Error </strong>(<strong class="bold">MAE</strong>)<strong class="bold"> Loss</strong>: The MAE<a id="_idIndexMarker258"/> loss function is a more robust loss function with regards to outliers. This means it punishes large errors even less than the previous two loss functions, MSE and MSLE. For <img src="image/Formula_B16391_03_222.png" alt=""/> training samples, it is calculated as follows:</li>
			</ul>
			<p><img src="image/Formula_B16391_03_223.png" alt=""/></p>
			<p>In summary, we can say that we can choose between three different loss functions for regression problems: MSE, MSLE, and MAE. Let's continue with loss functions for binary and multiclass classification problems.</p>
			<h3>Loss Functions for Binary Classification Problems</h3>
			<p>The <a id="_idIndexMarker259"/>common approach for binary classification is to encode the two classes with <img src="image/Formula_B16391_03_224.png" alt=""/> and <img src="image/Formula_B16391_03_225.png" alt=""/> and to train a network to predict the probability for class <img src="image/Formula_B16391_03_226.png" alt=""/>. Here the output layer consists of just one unit with the sigmoid activation function. For this <a id="_idIndexMarker260"/>approach, the recommended default loss function is <strong class="bold">binary cross entropy</strong>.</p>
			<p>On a training set of <img src="image/Formula_B16391_03_227.png" alt=""/> samples, the binary cross-entropy can be calculated as follows:</p>
			<p><img src="image/Formula_B16391_03_228.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_229.png" alt=""/> is the class label, the true value (<img src="image/Formula_B16391_03_230.png" alt=""/> 1) for the <em class="italic">i</em>th sample in the training set, and <img src="image/Formula_B16391_03_231.png" alt=""/> is the probability predicted by the network for that class. Since this a binary classification problem, the second part of the loss function calculates exactly the same value for the other class. <img src="image/Formula_B16391_03_232.png" alt=""/> is the predicted value <img src="image/Formula_B16391_03_233.png" alt=""/> in the previously shown loss functions.</p>
			<p>Other possible loss <a id="_idIndexMarker261"/>functions for binary classification <a id="_idIndexMarker262"/>problems are <strong class="bold">Hinge</strong> and <strong class="bold">Squared Hinge</strong>. In this<a id="_idIndexMarker263"/> case, the two classes have to be encoded as <img src="image/Formula_B16391_03_150.png" alt=""/> and <img src="image/Formula_B16391_03_175.png" alt=""/> and therefore the unit in the output layer must use the tanh activation function.</p>
			<h3>Loss Functions for Multiclass Classification Problems</h3>
			<p>In <a id="_idIndexMarker264"/>multiclass classification problems, usually, each class is represented by an integer value (class = 1, 2, 3, …)  and a one-hot encoding is used to represent the different classes. The output layer should have as neural units as many classes all with softmax activation function, so as to predict a score that can be interpreted as the probability of each class.</p>
			<p>The default loss function for <a id="_idIndexMarker265"/>multiclass classification problems is <strong class="bold">categorical cross-entropy</strong>. On a training set of <img src="image/Formula_B16391_03_209.png" alt=""/> samples, the categorical cross-entropy can be calculated as an extension to C classes of the binary cross-entropy:</p>
			<p><img src="image/Formula_B16391_03_237.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_238.png" alt=""/> is the <a id="_idIndexMarker266"/>class label k, the true value (<img src="image/Formula_B16391_03_239.png" alt=""/> 1) for the <em class="italic">i</em>th sample in the training set, and <img src="image/Formula_B16391_03_240.png" alt=""/> is the corresponding probability predicted by the network for class <em class="italic">k</em>. Again, <img src="image/Formula_B16391_03_241.png" alt=""/> is the predicted value <img src="image/Formula_B16391_03_242.png" alt=""/> by output neuron k for training sample <em class="italic">i</em>.</p>
			<p>For multiclass classification problems with too many different classes, such as language modeling where each word in the dictionary is one class, <strong class="bold">sparse categorical cross-entropy</strong> is<a id="_idIndexMarker267"/> used.</p>
			<p>Another commonly used <a id="_idIndexMarker268"/>loss function here is the <strong class="bold">Kullback-Leibler divergence</strong>.</p>
			<p>In addition to the commonly used loss functions, as introduced previously, it is also possible to define custom loss functions to best fit the use case at hand. </p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor097"/>Parameters and Optimization of the Training Algorithm</h2>
			<p>Now that our<a id="_idIndexMarker269"/> network is designed, using the correct activation<a id="_idIndexMarker270"/> function in the output layer as well as an appropriate loss function, we can start training the network. Modern training algorithms are generally based on the SGD strategy, using <a id="_idIndexMarker271"/>backpropagation to update the values of the network weights. Over the last few years, different variants of SGD algorithms (optimizers) have been produced, optimized to train networks on datasets with different properties. For example, <strong class="bold">Adagrad</strong> and its extension <strong class="bold">Adadelta</strong> work well on sparse data. <strong class="bold">Adam</strong> involves a moving average of the gradient and of the squared <a id="_idIndexMarker272"/>gradient for the weight updates. The Keras documentation page gives an overview of all the available training algorithms: <a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Backpropagation is typically referred to as the algorithm that calculates the gradients of the weights. The algorithm to train a neural network is usually some variant of SGD and it makes use of backpropagation to update the network weights.</p>
			<p>A big role in the training<a id="_idIndexMarker273"/> algorithm is played by the <strong class="bold">learning rate</strong> <img src="image/Formula_B16391_03_243.png" alt=""/>. The learning rate defines the size of the step taken along the direction of the gradient descent on the error surface during the learning phase. A too-small <img src="image/Formula_B16391_03_244.png" alt=""/> produces tiny steps and therefore takes a long time to reach the minimum of the loss function, especially if the loss function happens to have flat slopes. A too-large <img src="image/Formula_B16391_03_245.png" alt=""/> produces large steps that might overshoot and miss the minimum of the loss function, especially if the loss function is narrow and with steep slopes. The choice of the right value of learning rate <img src="image/Formula_B16391_03_246.png" alt=""/> is critical. A possible solution could be to use an adaptive learning rate, starting large and progressively decreasing with the number of training iterations.</p>
			<p>In <em class="italic">Figure 3.17</em>, there are examples for moving on the loss function with a too-small, too-large, and adaptive learning rate:</p>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<img src="image/B16391_03_017.jpg" alt="Figure 3.17 – The progressive decrease of the error with a too-small learning rate  (on the left), a too-large learning rate  (in the center), and an adaptive learning rate  in a one-dimensional weight space"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – The progressive decrease of the error with a too-small learning rate <img src="image/Formula_B16391_03_247.png" alt=""/> (on the left), a too-large learning rate <img src="image/Formula_B16391_03_248.png" alt=""/> (in the center), and an adaptive learning rate <img src="image/Formula_B16391_03_249.png" alt=""/> in a one-dimensional weight space</p>
			<p>All loss functions are defined as a sum over all training samples. This leads to algorithms that update the weights after all training samples have passed through the network. This training<a id="_idIndexMarker274"/> strategy is called <strong class="bold">batch training</strong>. It is the correct way to proceed; however, it is also computationally expensive and often slow.</p>
			<p>The alternative is to<a id="_idIndexMarker275"/> use the <strong class="bold">online training</strong> strategy, where weights are updated after the pass of each training sample. This strategy is less computationally expensive, but it is just an approximation of the original backpropagation algorithm. It is also prone to running into oscillations. In this case, it is good practice to use smaller values for the learning rate.</p>
			<p>Virtually all modern deep learning frameworks make use of a mixture of batch and online training, where they use small batches of training examples to perform a single update step.</p>
			<p>The <strong class="bold">Momentum</strong> term is <a id="_idIndexMarker276"/>added to the weight delta <img src="image/Formula_B16391_03_250.png" alt=""/> to increase the weight update as long as they have the same sign as the previous delta. Momentum speeds up the training on long flat error surfaces and can help the network pass a local minimum. The weight update then would include an extra term:</p>
			<p><img src="image/Formula_B16391_03_251.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B16391_03_252.png" alt=""/> is the cur<a id="_idTextAnchor098"/>rent training iteration and <img src="image/Formula_B16391_03_253.png" alt=""/> is the momentum term.</p>
			<h3>Additional Training Parameters</h3>
			<p>Two other important setting options for the training process are the training batch size and the number of epochs.</p>
			<ul>
				<li><strong class="bold">Training batch size</strong>: The training <strong class="bold">batch size</strong> defines the number of samples used in<a id="_idIndexMarker277"/> one training iteration. If the training batch size is set to the full number of samples in the training set, the training will run in the so-called batch mode, which is computationally expensive and slow. In general, it is recommended to train a model in mini-batch mode, where only part of the data is used for each iteration. It is recommended to shuffle the data before each epoch, to have different batches in different epochs.</li>
				<li><strong class="bold">Number of epochs</strong>: The number of epochs<a id="_idIndexMarker278"/> defines the number of cycles that run over the full training dataset. </li>
			</ul>
			<p>To summarize, the algorithm goes through the whole training set <img src="image/Formula_B16391_03_031.png" alt=""/> times, where <img src="image/Formula_B16391_03_255.png" alt=""/> is the number of epochs. Each epoch consists of a number of iterations and, for each iteration, a subset of the training set (a batch) is used. At the end of each iteration, weights are updated following the online training strategy.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor099"/>Summary</h1>
			<p>We have reached the end of this chapter, where we have learned the basic theoretical concepts behind neural networks and deep learning networks. All of this will be helpful to understand the steps for the practical implementation of deep learning networks described in the coming chapters.</p>
			<p>We started with the artificial neuron and moved on to describe how to assemble and train a network of neurons, a fully connected feedforward neural network, via a variant of the gradient descent algorithm, using the backpropagation algorithm to calculate the gradient. </p>
			<p>We concluded the chapter with a few hints on how to design and train a neural network. First, we described some commonly used network topologies, neural layers, and activation functions to design the appropriate neural architecture. </p>
			<p>We then moved to analyze the effects of some parameters involved in the training algorithm. We introduced a few more parameters and techniques to optimize the training algorithm against a selected loss function. </p>
			<p>In the next chapter, you will learn how you can perform all the steps we introduced in this chapter using KNIME Analytics Platform.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor100"/>Questions and Exercises</h1>
			<p>Test how well you have understood the concepts in this chapter by answering the following questions:</p>
			<ol>
				<li value="1">A feedforward neural network is an architecture where:<p>a. Each neuron from the previous layer is connected to each neuron in the next layer.</p><p>b. There are auto and backward connections.</p><p>c. There is just one unit in the output layer.</p><p>d. There are as many input units as there are output units.</p></li>
				<li>Why do we need hidden layers in a feedforward neural network?<p>a. For more computational power</p><p>b. To speed up calculations</p><p>c. To implement more complex functions </p><p>d. For symmetry</p></li>
				<li>The backpropagation algorithm updates the network weights proportionally to:<p>a. The output errors backpropagated through the network</p><p>b. The input values forward propagated through the network</p><p>c. The batch size</p><p>d. The deltas calculated at the output layer and backpropagated through the network</p></li>
				<li>Which loss function is commonly used for a multiclass classification problem?<p>a. MAE</p><p>b. RMSE</p><p>c. Categorical cross-entropy</p><p>d. Binary cross-entropy</p></li>
				<li>What kind of networks are suited for image analysis?<p>a. RNNs</p><p>b. CNNs</p><p>c. Fully connected feedforward networks</p><p>d. Autoencoders</p></li>
				<li>How is the last layer of a network commonly configured when solving a binary classification problem?<p>a. Two units with the sigmoid activation function </p><p>b. One unit with the linear activation function</p><p>c. Two units with the ReLU activation function</p><p>d. One unit with the sigmoid activation function</p></li>
				<li>When are RNNs used?<p>a. On data with many missing values </p><p>b. On image data</p><p>c. On sequential data</p><p>d. On sparse datasets</p></li>
			</ol>
		</div>
	</body></html>