["```py\n    import textstat\n    import nltk\n    nltk.download('punkt')\n    ```", "```py\n    self.metric_family = pb_utils.MetricFamily(\n        name=\"business_metrics\",\n        description=\"\",\n        kind=pb_utils.MetricFamily.GAUGE\n    )\n    ```", "```py\n    self.number_of_input_tokens_metric = self.metric_family.Metric(\n        labels={\"name\": \"Number of input tokens\",\"version\": \"1\"}\n    )\n    self.number_of_tokens_generated_metric = self.metric_family.Metric(\n        labels={\"name\": \"Number of tokens generated\", \"version\": \"1\"}\n    )\n    self.readability_metric = self.metric_family.Metric(\n        labels={\"name\": \"Flesch Readability Score\", \"version\": \"1\"}\n    )\n    ```", "```py\n    def _compute_custom_metrics(self, input_tokens, generated_tokens, generated_text):\n       self.readability_metric.set(textstat.flesch_reading_ease((generated_text)))\n        self.number_of_input_tokens_metric.set(input_tokens.shape[1])\n        self.number_of_tokens_generated_metric.set(generated_tokens.shape[1])\n    ```", "```py\n    self._compute_custom_metrics(input_ids, output_seq, \" \".join(decoded_texts))\n    ```", "```py\n    sudo docker run --gpus=all -it --shm-size=256m --rm -p8000:8000 -p8001:8001 -p8002:8002 -v ${PWD}/models:/models nvcr.io/nvidia/tritonserver:23.05-py3\n    ```", "```py\n    pip install transformers==4.21.3 nvidia-tensorrt==8.4.1.5 git+https://github.com/ELS-RD/transformer-deploy torch==1.12.0  -f https://download.pytorch.org/whl/cu116/torch_stable.html textstat==0.7.3\n    ```", "```py\n    # HELP nv_gpu_power_usage GPU power usage in watts\n    # TYPE nv_gpu_power_usage gauge\n    nv_gpu_power_usage{gpu_uuid=\"GPU-246b298f-13e6-f93e-b6f1-0fb8933ce337\"} 13.676\n    nv_gpu_power_usage{gpu_uuid=\"GPU-0fa69700-a702-0113-f30c-89d3fa1cec2f\"} 19.626\n    # HELP nv_gpu_power_limit GPU power management limit in watts\n    # TYPE nv_gpu_power_limit gauge\n    nv_gpu_power_limit{gpu_uuid=\"GPU-246b298f-13e6-f93e-b6f1-0fb8933ce337\"} 250\n    nv_gpu_power_limit{gpu_uuid=\"GPU-0fa69700-a702-0113-f30c-89d3fa1cec2f\"} 260\n    # HELP nv_cpu_utilization CPU utilization rate [0.0 - 1.0]\n    # TYPE nv_cpu_utilization gauge\n    nv_cpu_utilization 0.04902789518174133\n    # HELP nv_cpu_memory_total_bytes CPU total memory (RAM), in bytes\n    # TYPE nv_cpu_memory_total_bytes gauge\n    nv_cpu_memory_total_bytes 67239776256\n    # HELP nv_cpu_memory_used_bytes CPU used memory (RAM), in bytes\n    # TYPE nv_cpu_memory_used_bytes gauge\n    nv_cpu_memory_used_bytes 14459383808\n    # TYPE business_metrics gauge\n    business_metrics{name=\"Flesch Readability Score\",version=\"1\"} 93.81\n    business_metrics{name=\"Number of tokens generated\",version=\"1\"} 128\n    business_metrics{name=\"Number of input tokens\",version=\"1\"} 11\n    ```", "```py\n    sudo systemctl start node_exporter\n    node_exporter here is a Prometheus exporter that collects system-level metrics from target machines, enabling Prometheus to monitor and analyze their resource usage and performance.\n    ```", "```py\n    - job_name: \"triton\"\n        scrape_interval: 10s\n        static_configs:\n          - targets: [\"localhost:8002\"]\n    ```", "```py\n    3000, so accessing the localhost:3000 link in a web browser will bring you to the home page. The default username and password are admin and admin. Once that is filled in, click on Log In and create a new password, after which you will be greeted with Grafana’s home page. We can set up the Prometheus link now by clicking on the three-line button on the top-left tab and clicking on the Data sources tab under the Administration dropdown, as shown in *Figure 16**.4 (a)*. This will result in the screen shown in *Figure* *16**.4 (b)*.\n    ```"]