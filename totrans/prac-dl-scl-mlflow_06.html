<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer052">
			<h1 id="_idParaDest-51"><em class="italic"><a id="_idTextAnchor050"/>Chapter 4</em>: Tracking Code and Data Versioning</h1>
			<p>DL models are not just models – they are intimately tied to the code that trains and tests the model and the data that's used for training and testing. If we don't track the code and data that's used for the model, it is impossible to reproduce the model or improve it. Furthermore, there have been recent industry-wide awakenings and paradigm shifts toward a <strong class="bold">data-centric AI</strong> (<a href="https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5cbacdc574f5">https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5cbacdc574f5</a>), where the importance of data is being lifted to a first-class artifact in building ML and, especially, DL models. Due to this, in this chapter, we will learn how to track code and data versioning using MLflow. We will learn about the different ways we can track code and pipeline versioning and how to use Delta Lake for data versioning. By the end of this chapter, you will be able to understand and implement tracking techniques for both code and data with MLflow.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Tracking notebook and pipeline versioning</li>
				<li>Tracking locally, privately built Python libraries</li>
				<li>Tracking data versioning in Delta Lake</li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Technical requirements </h1>
			<p>The following are the technical requirements for this chapter:</p>
			<ul>
				<li>VS Code with the Jupyter Notebook extension: <a href="https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks">https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks</a>.</li>
				<li>The code for this chapter, which can be found in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04</a>.</li>
				<li>Access to a Databricks instance so that you can learn how to use Delta Lake to enable versioned data access.</li>
			</ul>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Tracking notebook and pipeline versioning</h1>
			<p>Data scientists usually start by experimenting with Python notebooks offline, where interactive execution is a key benefit. Python notebooks have come a long way since the days of <strong class="bold">Jupyter</strong> notebooks (<a href="https://jupyter-notebook.readthedocs.io/en/stable/">https://jupyter-notebook.readthedocs.io/en/stable/</a>). The success and popularity of Jupyter notebooks are undeniable. However, there are limitations when it comes to using version control for Jupyter notebooks since Jupyter notebooks are stored as JSON data with mixed output and code. This is especially difficult if we trying to track code using MLflow as we're only using Jupyter's native format, whose file extension is <strong class="source-inline">.ipynb</strong>. You may not be able to see the exact Git hash in the MLflow tracking server for each run using a Jupyter notebook either. There are a lot of interesting debates on whether or when a Jupyter notebook should be used, especially in a production environment (see a discussion here: <a href="https://medium.com/mlops-community/jupyter-notebooks-in-production-4e0d38803251">https://medium.com/mlops-community/jupyter-notebooks-in-production-4e0d38803251</a>). There are multiple reasons why we shouldn't use Jupyter notebooks in a production environment, especially when we need reproducibility in an end-to-end pipeline fashion, where unit testing, proper code versioning, and dependency management could be difficult with a lot of notebooks. There are some early innovations around scheduling, parameterizing, and executing Jupyter notebooks in a workflow fashion using the open source tool <strong class="bold">papermill</strong> by Netflix (<a href="https://papermill.readthedocs.io/en/latest/index.html">https://papermill.readthedocs.io/en/latest/index.html</a>). However, a recent innovation by Databricks and VS Code makes notebooks much easier to be version controlled and integrated with MLflow. Let's look at the notebook characteristics that were introduced by these two tools:</p>
			<ul>
				<li><strong class="bold">Interactive execution</strong>: Both Databricks's notebooks and VS Code's notebooks can run the same way as traditional Jupyter notebooks, in a cell-by-cell execution mode. By doing this, you can immediately see the output of the results. </li>
				<li><strong class="bold">File format</strong>: Both Databricks's notebooks and VS Code's notebooks are stored as plain-old Python code with a <strong class="source-inline">.py</strong> file extension. This allows all the regular Python code linting (code format and style checking) to be applied to a notebook.</li>
				<li><strong class="bold">Special symbols for rendering code cells and Mark down cells</strong>: Both Databricks and VS Code leverage some special symbols to render Python files as interactive notebooks. In Databricks, the special symbols to delineate code into different executable cells are as follows:<p class="source-code"># COMMAND ---------- </p><p class="source-code">import mlflow</p><p class="source-code">import torch</p><p class="source-code">from flash.core.data.utils import download_data</p><p class="source-code">from flash.text import TextClassificationData, TextClassifier</p><p class="source-code">import torchmetrics</p></li>
			</ul>
			<p>The code below the special <strong class="source-inline">COMMAND</strong> line will be rendered as an executable cell in the Databricks web UI portal, as follows:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.1.jpg" alt="Figure 4.1 – Databricks executable cell&#13;&#10;" width="1245" height="360"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Databricks executable cell</p>
			<p>To execute the code in this cell, you can just click <strong class="bold">Run Cell</strong> via the top-right drop-down menu.</p>
			<p>To add a large chunk of text to describe and comment on the code in Databricks (also known as Markdown cells), you can use the <strong class="source-inline"># MAGIC</strong> symbol at the beginning of the line, as follows:</p>
			<p class="source-code"># MAGIC %md</p>
			<p class="source-code"># MAGIC #### Notebooks for fine-tuning a pretrained language model to do text-based sentiment classification</p>
			<p>This is then rendered in the Databricks notebook as a Markdown comment cell, as follows:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.2.jpg" alt="Figure 4.2 – Databricks Markdown text cell&#13;&#10;" width="1217" height="246"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Databricks Markdown text cell</p>
			<p>In VS Code, a slightly different set of symbols is used for these two types of cells. For a code cell, the <strong class="source-inline"># %%</strong> symbols are used at the beginning of the cell block:</p>
			<p class="source-code"># %%</p>
			<p class="source-code">download_data("https://pl-flash-data.s3.amazonaws.com/imdb.zip", "./data/")</p>
			<p class="source-code">datamodule = TextClassificationData.from_csv(</p>
			<p class="source-code">    input_fields="review",</p>
			<p class="source-code">    target_fields="sentiment",</p>
			<p class="source-code">    train_file="data/imdb/train.csv",</p>
			<p class="source-code">    val_file="data/imdb/valid.csv",</p>
			<p class="source-code">    test_file="data/imdb/test.csv"</p>
			<p class="source-code">)</p>
			<p>This is then rendered in VS Code's editor, as follows:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.3.jpg" alt="Figure 4.3 – VSCode code cell&#13;&#10;" width="1179" height="388"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – VS Code code cell</p>
			<p>As you can see, there is a <strong class="bold">Run Cell</strong> button before the block of code that you can click to run the code block interactively. If you click the <strong class="bold">Run Cell</strong> button, the code block will start executing in the side panel of the editor window, as shown here:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.4.jpg" alt=" Figure 4.4 – Running code interactively in VSCode&#13;&#10;" width="1190" height="711"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 4.4 – Running code interactively in VS Code</p>
			<p>To add a Markdown cell that contains comments, add the following to the beginning of the line, as well as the necessary symbols:</p>
			<p class="source-code"># %% Notebook for fine-tuning a pretrained language model and sentiment classification</p>
			<p>This will ensure that the text is not an executable code block in VS Code.</p>
			<p>Given the advantages of Databricks and VS Code notebooks, we suggest using either for version tracking. We can use GitHub to track the versioning of either type of notebook since they use a regular Python file format. </p>
			<p class="callout-heading">Two Ways to Use Databricks Notebook Version Control</p>
			<p class="callout">For a managed Databricks instance, a notebook version can be tracked in two ways: by looking at the revision history on the side panel of the notebook on the Databricks web UI, or by linking to a remote GitHub repository. Detailed descriptions are available in the Databricks notebook documentation: <a href="https://docs.databricks.com/notebooks/notebooks-use.html#version-control">https://docs.databricks.com/notebooks/notebooks-use.html#version-control</a>.</p>
			<p>While the Databricks web portal provides excellent support for notebook version control and integration with MLflow experimentation tracking (see this chapter's callout boxes on <em class="italic">Two Ways to Use Databricks Notebook Version Control</em> and <em class="italic">Two Types of MLflow Experiments in Databricks Notebooks</em>), there is one major drawback of writing code in the Databricks notebook web UI. This is because the web UI is not a typical <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) compared to VS Code, where code style and formatting tools such as <strong class="bold">flake8</strong> (<a href="https://flake8.pycqa.org/en/latest/">https://flake8.pycqa.org/en/latest/</a>) and autopep8 (<a href="https://pypi.org/project/autopep8/">https://pypi.org/project/autopep8/</a>) can easily be enforced. This can have a major impact on code quality and maintainability. Thus, it is highly recommended that you use VS Code to author notebook code (either a Databricks notebook or a VS Code notebook). </p>
			<p class="callout-heading">Two Types of MLflow Experiments in Databricks Notebooks</p>
			<p class="callout">For a managed Databricks web portal instance, there are two types of MLflow experiments you can perform: workspace and notebook experiments. A workspace experiment is mainly for a shared experiment folder that is not tied to a single notebook. Remote code execution can write to a workspace experiment folder if needed. On the other hand, a notebook scope experiment is tied to a specific notebook and can be found directly on one of the top-right menu items called <strong class="bold">Experiment</strong> in the notebook page on the Databricks web portal. For more details, please look at the Databricks documentation website: <a href="https://docs.databricks.com/applications/mlflow/tracking.html#experiments">https://docs.databricks.com/applications/mlflow/tracking.html#experiments</a>.</p>
			<p>Using this chapter's VS Code notebook, <strong class="source-inline">fine_tuning.py</strong>, which can be found in this chapter's GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/notebooks/fine_tuning.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/notebooks/fine_tuning.py</a>), you will be able to run it interactively in the VS Code editor and log the experiment in the MLflow Docker server that we set up in <a href="B18120_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a><em class="italic">, Tracking Models, Parameters, and Metrics</em>. As a reminder, note that to run this notebook in VS Code successfully, you will need to set up your virtual environment, called <strong class="source-inline">dl_model</strong>, as described in the <strong class="source-inline">README.md</strong> file in this chapter's GitHub repository. It consists of the following three steps:</p>
			<pre class="source-code">conda create -n dl_model python==3.8.10</pre>
			<pre class="source-code">conda activate dl_model</pre>
			<pre class="source-code">pip install -r requirements.txt</pre>
			<p>If you run this notebook cell-by-cell from beginning to end, your experiment page will look as follows:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.5.jpg" alt="Figure 4.5 – Logged experiment page after running a VSCode notebook interactively &#13;&#10;" width="973" height="318"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Logged experiment page after running a VS Code notebook interactively </p>
			<p>You may immediately notice a problem in the preceding screenshot – <strong class="bold">Source: ipykernel_laucher.py</strong>. This is not the source code file we just ran; that is, the <strong class="source-inline">fine_tuning.py</strong> file. This is because VS Code notebooks are not natively integrated into the MLflow tracking server for source file tracking; it can only show the <strong class="bold">ipykernel</strong> (<a href="https://pypi.org/project/ipykernel/">https://pypi.org/project/ipykernel/</a>) that VS Code uses to execute a VS Code notebook (<a href="https://github.com/microsoft/vscode-jupyter">https://github.com/microsoft/vscode-jupyter</a>). Unfortunately, this is a limitation that, at the time of writing, cannot be addressed by running VS Code notebooks <em class="italic">interactively</em> for experiment code tracking. Databricks notebooks running inside a hosted Databricks web UI have no such problem as they have native integration with the MLflow tracking server that's bundled in the Databricks web portal.</p>
			<p>However, since the VS Code notebooks are just Python code, we can run the notebooks in the command line <em class="italic">non-interactively</em>, as follows:</p>
			<p class="source-code">python fine_tuning.py</p>
			<p>This will log the actual source code's filename and the Git commit hash in the MLflow experiment page without any issues, as shown here:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.6.jpg" alt="Figure 4.6 – Logged experiment page after running a VSCode notebook in the command line &#13;&#10;" width="1031" height="336"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Logged experiment page after running a VS Code notebook in the command line </p>
			<p>The preceding screenshot shows the correct source filename (<strong class="bold">Source: </strong><strong class="source-inline">fine_tuning.py</strong>) and the correct git commit hash (<strong class="bold">661ffeda5ae53cff3623f2fcc8227d822e877e2d</strong>). This workaround does not require us to change the notebook's code and could be very useful if our initial interactive notebook debugging is done and we want to get a complete run of the notebook, along with proper code version tracking in the MLflow tracking server. Note that all the other parameters, metrics, and models are tracked properly, regardless of whether we run the notebook interactively. </p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Pipeline tracking</h2>
			<p>Having discussed notebook code tracking (version and filename), let's turn to the topic of pipeline tracking. Before we discuss pipeline tracking, however, we will discuss the definition of a pipeline in the ML/DL life cycle. Conceptually, a pipeline is a multi-step data processing and task workflow. However, the implementation of such a data/task workflow can be quite different. A pipeline can be defined as a first-class Python API in some ML packages. The two most well-known pipeline APIs are as follows:</p>
			<ul>
				<li><strong class="source-inline">sklearn.pipeline.Pipeline</strong> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html</a>): This is widely used for building tightly integrated multi-step pipelines for classical machine learning or data <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) pipelines using <strong class="bold">pandas DataFrames</strong> (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).</li>
				<li><strong class="source-inline">pyspark.ml.Pipeline</strong> (<a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html">https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html</a>): This is a PySpark version for building simple and tightly integrated multi-step pipelines for machine learning or data ETL pipelines using <strong class="bold">Spark DataFrames </strong>(https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html).</li>
			</ul>
			<p>However, when we're building a DL model pipeline, we need to use multiple different Python packages at different steps of the pipeline, so a one-size-fits-all approach using a single pipeline API doesn't usually work. In addition, neither of the aforementioned pipeline APIs have native support for the current popular DL packages, such as <strong class="bold">Huggingface</strong> or <strong class="bold">PyTorch-Lightning</strong>, which require additional integration work. Although some open source DL pipeline APIs exist such as <strong class="bold">Neuraxle</strong> (<a href="https://github.com/Neuraxio/Neuraxle">https://github.com/Neuraxio/Neuraxle</a>), which tries to provide a sklearn-like pipeline interface and framework, it is not widely used. Furthermore, using these API-based pipelines means that you'll be locked in when you need to add more steps to the pipeline, which could reduce your flexibility to extend or evolve a DL pipeline when new requirements arise.</p>
			<p>In this book, we will take a different approach to define and build a DL pipeline that's based on MLflow's <strong class="bold">MLproject</strong> (<a href="https://www.mlflow.org/docs/latest/projects.html#mlproject-file">https://www.mlflow.org/docs/latest/projects.html#mlproject-file</a>) structure. This will give you the most flexibility to build a multi-step pipeline that can be tracked using MLflow. At the same time, for each step, you will be allowed to use the most appropriate DL or data processing packages without being locked in. Let's walk through this by breaking the single file-based Python notebook, <strong class="source-inline">fine_tuning.py</strong>, into a multiple-step pipeline. This pipeline can be visualized as a three-step flow diagram, as shown here:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.7.jpg" alt="Figure 4.7 – A three-step DL pipeline &#13;&#10;" width="1337" height="111"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – A three-step DL pipeline </p>
			<p>This three-step flow is as follows: </p>
			<ol>
				<li>Download the data to a local execution environment</li>
				<li>Fine-tune the model</li>
				<li>Register the model </li>
			</ol>
			<p>These modular steps<a id="_idIndexMarker174"/> may seem to be overkill for our current example, but the power of having a distinctive functional step is evident when more complexities are involved, or when changes are needed at each step. Each step can be modified without them affecting the other steps if we define the parameters that need to be passed between them. Each step is a standalone Python file that can be executed independently with a set of input parameters. There will be a main pipeline Python file that can run the whole pipeline or a sub-section of the pipeline's steps. In the <strong class="source-inline">MLproject</strong> file, which is a standard YAML file without the file extension, we can define four entry points (<strong class="source-inline">main</strong>, <strong class="source-inline">download_data</strong>, <strong class="source-inline">fine_tuning_model</strong>, and <strong class="source-inline">register_model</strong>), their required input parameters, their types and default values, and the command line to execute each entry point. In our example, these entry points will be provided in a Python command-line execution command. However, you can invoke any kind of execution, such as a batch shell script, if needed for any particular steps. For example, the following lines in the <strong class="source-inline">MLproject</strong> file for this chapter (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/MLproject">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/MLproject</a>) describe the name of the project, the <strong class="source-inline">conda</strong> environment definition filename, and the main entry point:</p>
			<pre class="source-code">name: dl_model_chapter04</pre>
			<pre class="source-code">conda_env: conda.yaml</pre>
			<pre class="source-code">entry_points:</pre>
			<pre class="source-code">  main:</pre>
			<pre class="source-code">    parameters:</pre>
			<pre class="source-code">      pipeline_steps:</pre>
			<pre class="source-code">        description: Comma-separated list of dl pipeline steps to execute </pre>
			<pre class="source-code">        type: str</pre>
			<pre class="source-code">        default: all</pre>
			<pre class="source-code">    command: "python main.py --steps {pipeline_steps}"</pre>
			<p>Here, the name of the project is <strong class="source-inline">dl_model_chapter04</strong>. <strong class="source-inline">conda_env</strong> refers to a local conda<a id="_idIndexMarker175"/> environment's YAML definition file, <strong class="source-inline">conda.yaml</strong>, which is located in the same directory as the <strong class="source-inline">MLproject</strong> file. The <strong class="source-inline">entry_points</strong> section lists the first entry point, called <strong class="source-inline">main</strong>. In the <strong class="source-inline">parameters</strong> section, there is one parameter called <strong class="source-inline">pipeline_steps</strong>, which allows the user to define a comma-separated list of DL pipeline steps to execute. This parameter is of the <strong class="source-inline">str</strong> type and its default value is <strong class="source-inline">all</strong>, which means that all the pipeline steps will run. Lastly, the <strong class="source-inline">command</strong> section lists how to execute this step in the command line.</p>
			<p>The rest of the <strong class="source-inline">MLproject</strong> file defines the other three pipeline step entry points by following the same syntactic convention as the main entry point. For example, the following lines in the same <strong class="source-inline">MLproject</strong> file define the entry point of <strong class="source-inline">download_data</strong>:</p>
			<pre class="source-code">  download_data:</pre>
			<pre class="source-code">    parameters:</pre>
			<pre class="source-code">      download_url:</pre>
			<pre class="source-code">        description: a url to download the data for fine tuning a text sentiment classifier</pre>
			<pre class="source-code">        type: str</pre>
			<pre class="source-code">        default: https://pl-flash-data.s3.amazonaws.com/imdb.zip</pre>
			<pre class="source-code">      local_folder:</pre>
			<pre class="source-code">        description: a local folder to store the downloaded data</pre>
			<pre class="source-code">        type: str</pre>
			<pre class="source-code">        default: ./data</pre>
			<pre class="source-code">      pipeline_run_name:</pre>
			<pre class="source-code">        description: an mlflow run name</pre>
			<pre class="source-code">        type: str</pre>
			<pre class="source-code">        default: chapter04</pre>
			<pre class="source-code">    command:</pre>
			<pre class="source-code">      "python pipeline/download_data.py --download_url {download_url} --local_folder {local_folder} \</pre>
			<pre class="source-code">      --pipeline_run_name {pipeline_run_name}"</pre>
			<p>The <strong class="source-inline">download_data</strong> section, similar to the main entry point, also defines the list of parameters, types, and default<a id="_idIndexMarker176"/> values, as well as the command line to execute this step. We can define the rest of the steps in the same manner as we did in the <strong class="source-inline">MLproject</strong> file that we just checked out from this book's GitHub repository. For more details, take a look at the full content of that <strong class="source-inline">MLproject</strong> file.</p>
			<p>After defining the <strong class="source-inline">MLproject</strong> file, it becomes clear that we have defined a multi-step pipeline in a declarative way. This is like a specification for the pipeline that says each step's name, what input parameters it expects, and how to execute them. Now, the next step is to implement the Python function to execute each step of the pipeline. So, let's look at the core implementation of the main entry point's Python function, which is called <strong class="source-inline">main.py</strong>. The following lines of code (not the entire Python code in <strong class="source-inline">main.py</strong>) illustrate the core component of implementing the entire pipeline with just one step in the pipeline (<strong class="source-inline">download_data</strong>):</p>
			<pre class="source-code">@click.command()</pre>
			<pre class="source-code">@click.option("--steps", default="all", type=str)</pre>
			<pre class="source-code">def run_pipeline(steps):</pre>
			<pre class="source-code">    with mlflow.start_run(run_name='pipeline', nested=True) as active_run:</pre>
			<pre class="source-code">        download_run = mlflow.run(".", "download_data", parameters={})</pre>
			<pre class="source-code">if __name__ == "__main__":</pre>
			<pre class="source-code">    run_pipeline()</pre>
			<p>This main function snippet contains a <strong class="source-inline">run_pipeline</strong> function, which will be run when the <strong class="source-inline">main.py</strong> file is executed in the command line. There is a parameter called <strong class="source-inline">steps</strong>, which will be passed to this function when it's provided. In this example, we are using<a id="_idIndexMarker177"/> the <strong class="source-inline">click</strong> Python package (<a href="https://click.palletsprojects.com/en/8.0.x/">https://click.palletsprojects.com/en/8.0.x/</a>) to parse command-line arguments. The <strong class="source-inline">run_pipeline</strong> function starts an MLflow experiment run by calling <strong class="source-inline">mlflow.start_run</strong> and passing two parameters (<strong class="source-inline">run_name</strong> and <strong class="source-inline">nested</strong>). We have used <strong class="source-inline">run_name</strong> before – it's the descriptive phrase for this run. However, the <strong class="source-inline">nested</strong> parameter<a id="_idIndexMarker178"/> is new, which means that this is a parent experiment run. This parent experiment run contains some child experiment runs that will be hierarchically tracked in MLflow. Each parent run can contain one or more child runs. In the example code, this contains one step of the pipeline run, called <strong class="source-inline">download_data</strong>, which is invoked by calling <strong class="source-inline">mlflow.run</strong>. This is the key MLflow function to invoke an MLproject's entry point programmatically. Once <strong class="source-inline">download_data</strong> has been invoked and the run has finished, the parent run will also finish, thus concluding the pipeline's run. </p>
			<p class="callout-heading">Two Ways to Execute an MLproject's Entry Point</p>
			<p class="callout">There are two ways to execute<a id="_idIndexMarker179"/> an MLproject's entry point. First, you can use MLflow's Python<a id="_idIndexMarker180"/> API, known as <strong class="source-inline">mlflow.run</strong> (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run">https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run</a>). Alternatively, you can use the MLflow's command-line interface tool, called <strong class="source-inline">mlflow run</strong>, which can be called in a command-line shell environment to execute any entry point directly (<a href="https://www.mlflow.org/docs/latest/cli.html#mlflow-run">https://www.mlflow.org/docs/latest/cli.html#mlflow-run</a>). </p>
			<p>Now, let's learn how to implement each step in the pipeline generically. For each pipeline step, we put the Python files in a <strong class="source-inline">pipeline</strong> folder. In this example, we have three files: <strong class="source-inline">download_data.py</strong>, <strong class="source-inline">fine_tuning_model.py</strong>, and <strong class="source-inline">register_model.py</strong>. Thus, the relevant files for successfully building an MLflow supported pipeline project are as follows:</p>
			<pre class="source-code">MLproject</pre>
			<pre class="source-code">conda.yaml</pre>
			<pre class="source-code">main.py</pre>
			<pre class="source-code">pipeline/download_data.py</pre>
			<pre class="source-code">pipeline/fine_tuning_model.py</pre>
			<pre class="source-code">pipeline/register_model.py</pre>
			<p>For the implementation of each pipeline step, we can use the following Python function templates. A placeholder section<a id="_idIndexMarker181"/> is reserved for implementing the actual pipeline step logic:</p>
			<pre class="source-code">import click</pre>
			<pre class="source-code">import mlflow</pre>
			<pre class="source-code">@click.command()</pre>
			<pre class="source-code">@click.option("input")</pre>
			<pre class="source-code">def task(input):</pre>
			<pre class="source-code">    with mlflow.start_run() as mlrun:</pre>
			<pre class="source-code">        # Implement pipeline step logic here </pre>
			<pre class="source-code">        mlflow.log_parameter('parameter', parameter)</pre>
			<pre class="source-code">        mlflow.set_tag('pipeline_step', __file__)</pre>
			<pre class="source-code">        mlflow.log_artifacts(artifacts, artifact_path="data")</pre>
			<pre class="source-code">if __name__ == '__main__':</pre>
			<pre class="source-code">    task()</pre>
			<p>This template allows us to standardize the way we implement the pipeline step task. The main idea here is that for each pipeline step task, it needs to start with <strong class="source-inline">mlflow.start_run</strong> to launch an MLflow experiment run. Once we've implemented specific execution logic in the function, we need to log some parameters using <strong class="source-inline">mlflow.log_parameter</strong>, or some artifacts in the artifact store using <strong class="source-inline">mlflow.log_artifacts</strong>, that can be passed to and used by the next step of the pipeline. This is called <strong class="bold">pipeline chaining</strong>, and it allows multiple<a id="_idIndexMarker182"/> steps of a single pipeline or even different pipelines to share data and artifacts. We also want to set a tag to indicate which step is executed using <strong class="source-inline">mlflow.set_tag</strong>.</p>
			<p>For example, in the <strong class="source-inline">download_data.py</strong> step, the core implementation is as follows:</p>
			<pre class="source-code">import click</pre>
			<pre class="source-code">import mlflow</pre>
			<pre class="source-code">from flash.core.data.utils import download_data</pre>
			<pre class="source-code">@click.command()</pre>
			<pre class="source-code">@click.option("--download_url")</pre>
			<pre class="source-code">@click.option("--local_folder")</pre>
			<pre class="source-code">@click.option("--pipeline_run_name")</pre>
			<pre class="source-code">def task(download_url, local_folder, pipeline_run_name):</pre>
			<pre class="source-code">    with mlflow.start_run(run_name=pipeline_run_name) as mlrun:</pre>
			<pre class="source-code">        <strong class="bold">download_data(download_url, local_folder)</strong></pre>
			<pre class="source-code">        mlflow.log_param("download_url", download_url)</pre>
			<pre class="source-code">        mlflow.log_param("local_folder", local_folder)</pre>
			<pre class="source-code">        mlflow.set_tag('pipeline_step', __file__)</pre>
			<pre class="source-code">        mlflow.log_artifacts(local_folder, artifact_path="data")</pre>
			<pre class="source-code">if __name__ == '__main__':</pre>
			<pre class="source-code">    task()</pre>
			<p>In this <strong class="source-inline">download_data.py</strong> implementation, the task is to download the data for model building from a remote URL to a local folder (<strong class="source-inline">download_data(download_url, local_folder)</strong>). Once we've done this, we will log a few<a id="_idIndexMarker183"/> parameters, such as <strong class="source-inline">download_url</strong> and <strong class="source-inline">local_folder</strong>. We can also log the newly downloaded data into the MLflow artifact store using <strong class="source-inline">mlflow.log_artifacts</strong>. For this example, this may not seem necessary since we only want to execute the next step in a local development environment. However, for a more realistic scenario in a distributed execution environment where each step could be run in different execution environments, this is very desirable since we only need to pass the artifact URL path to the next step of the pipeline to use; we don't need to know how and where the previous step was executed. In this example, when the <strong class="source-inline">mlflow.log_artifacts(local_folder, artifact_path="data")</strong> statement is called, the downloaded data folder is uploaded to the MLflow artifact store. However, we will not use this artifact path for the downstream pipeline step<a id="_idIndexMarker184"/> in this chapter. We will explore how we use this kind of artifact store to pass artifacts to the next step in the pipeline later in this book. Here, we will use the log parameters to pass the downloaded data path to the next step of the pipeline (<strong class="source-inline">mlflow.log_param("local_folder", local_folder)</strong>). So, let's look at how we can do that by extending <strong class="source-inline">main.py</strong> so that it includes the next step, which is the <strong class="source-inline">fine_tuning_model</strong> entry point, as follows:</p>
			<pre class="source-code">        with mlflow.start_run(run_name='pipeline', nested=True) as active_run:</pre>
			<pre class="source-code">            download_run = mlflow.run(".", "download_data", parameters={})</pre>
			<pre class="source-code">            download_run = <strong class="bold">mlflow.tracking.MlflowClient().get_run</strong>(download_run.run_id)</pre>
			<pre class="source-code">            <strong class="bold">file_path_uri</strong> = <strong class="bold">download_run.data.params['local_folder']</strong></pre>
			<pre class="source-code">            fine_tuning_run = mlflow.run(".", "fine_tuning_model", <strong class="bold">parameters={"data_path": file_path_uri}</strong>)</pre>
			<p>We use <strong class="source-inline">mlflow.tracking.MlflowClient().get_run</strong> to get the <strong class="source-inline">download_run</strong> MLflow run object and then use <strong class="source-inline">download_run.data.params</strong> to get <strong class="source-inline">file_path_uri</strong> (in this case, it is just a local folder path). This is then passed to the next <strong class="source-inline">mlflow.run</strong>, which is <strong class="source-inline">fine_tuning_run</strong>, as a key-value parameter (<strong class="source-inline">parameters={"data_path": file_path_uri</strong> ). This way, the <strong class="source-inline">fine_tuning_run</strong> pipeline step can use this parameter to prefix its data source path. This is a very simplified scenario to illustrate how we can pass data from one step to the next. Using the <strong class="source-inline">mlflow.tracking.MlflowClient()</strong> API, which<a id="_idIndexMarker185"/> is provided by MLflow (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html">https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html</a>), makes accessing a run's information (parameters, metrics, and artifacts) straightforward. </p>
			<p>We can also extend the <strong class="source-inline">main.py</strong> file with the third step of the pipeline by adding the <strong class="source-inline">register_model</strong> step. This time, we need the logged model URI to register a trained model, which depends on <strong class="source-inline">run_id</strong> of the <strong class="source-inline">fine_tuning_model</strong> step. So, in the <strong class="source-inline">fine_tuning_model</strong> step, we need<a id="_idIndexMarker186"/> to get the <strong class="source-inline">run_id</strong> property of <strong class="source-inline">fine_tuning_model</strong> run and then pass it through the input parameter for the <strong class="source-inline">register_model</strong> run, as follows:</p>
			<pre class="source-code">fine_tuning_run_id = fine_tuning_run.run_id</pre>
			<pre class="source-code">register_model_run = mlflow.run(".", "register_model", parameters={"mlflow_run_id": fine_tuning_run_id})</pre>
			<p>Now, the <strong class="source-inline">register_model</strong> step can use <strong class="source-inline">fine_tuning_run_id</strong> to locate the logged model. The core implementation of the <strong class="source-inline">register_model</strong> step is as follows:</p>
			<pre class="source-code">    with mlflow.start_run() as mlrun:</pre>
			<pre class="source-code">        logged_model = f'runs:/{mlflow_run_id}/model'</pre>
			<pre class="source-code">        mlflow.register_model(logged_model, registered_model_name)</pre>
			<p>This will register a fine-tuned model at the URI defined by the <strong class="source-inline">logged_model</strong> variable to an MLflow model registry. </p>
			<p>If you have followed these steps, then you should have a working pipeline that can be tracked by MLflow from end to end. As a reminder, a prerequisite is to have the local full-fledged MLflow server set up, as shown in <a href="B18120_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a><em class="italic">, Tracking Models, Parameters, and Metrics</em>. You should have set up the virtual environment, <strong class="source-inline">dl_model</strong>, in the previous section. To test this pipeline, check out this chapter's GitHub repository at <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04</a> and run the following command:</p>
			<p class="source-code">python main.py</p>
			<p>This will run the entire three-step pipeline and log the pipeline's <strong class="source-inline">run_id</strong> (which is the parent run) and each step's run as the child runs in the MLflow tracking server. The last few lines of the console screen's output will display something as follows when it has finished running (you will see lots of outputs on the screen when you run the pipeline):</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.8.jpg" alt="Figure 4.8 – Console output of running the pipeline with MLflow run_ids&#13;&#10;" width="1086" height="122"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Console output of running the pipeline with MLflow run_ids</p>
			<p>This shows the pipeline's <strong class="source-inline">run_id</strong>, which is <strong class="source-inline">f8f21fdf8fff4fd6a400eeb403b776c8</strong>; the last step is the <strong class="source-inline">run_id</strong> property of <strong class="source-inline">fine_tuning_model</strong>, which is <strong class="source-inline">5ba38e059695485396e709b809e9bb8d</strong>. If we go to the MLflow tracking server's UI web page by clicking on <strong class="source-inline">http://localhost</strong>, we should be able to see <a id="_idIndexMarker187"/>the following nested experiment runs in the <strong class="source-inline">dl_model_chapter04</strong> experiment folder, as follows:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.9.jpg" alt="Figure 4.9 – A pipeline being run with nested three-step child runs in the MLflow tracking server&#13;&#10;" width="1253" height="219"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – A pipeline being run with nested three-step child runs in the MLflow tracking server</p>
			<p>The preceding screenshot shows the pipeline run, along with the source <strong class="source-inline">main.py</strong> file and the nested run of the three steps of the pipeline. Each step has a corresponding entry point name defined in <strong class="source-inline">MLproject</strong> with a GitHub commit hash code version of <strong class="bold">d0d416</strong>. If you click on the <strong class="source-inline">register_model</strong> run page, you will see the following information:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.10.jpg" alt="Figure 4.10 – Entry point register_model's run page on the MLflow tracking server&#13;&#10;" width="1232" height="572"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Entry point register_model's run page on the MLflow tracking server</p>
			<p>The preceding screenshot<a id="_idIndexMarker188"/> shows not only some of the familiar information we have seen already, but also some new information such as <strong class="bold">Parent Run: f8f21fdf8fff4fd6a400eeb403b776c8</strong>, <strong class="bold">Entry Point: register_model</strong>, and a fully populated <strong class="bold">Run Command</strong> cell that's automatically generated by MLflow. <strong class="bold">Run Command</strong> contains the file's location URL (a string that starts with <strong class="source-inline">file:///</strong>), the GitHub hash code version, the entry point <strong class="source-inline">(-e register_model</strong>), the execution environment, which is a local dev environment (<strong class="source-inline">-b local</strong>), and the expected parameters for the <strong class="source-inline">register_model</strong> function (<strong class="source-inline">-P</strong>). We will learn how to use MLflow's <strong class="source-inline">MLproject</strong> to run commands to execute tasks remotely later in this book. Here, we just need to understand that the source code is referred to through the entry point (<strong class="source-inline">register_model</strong>), not the filename itself, since the reference is declared as an entry point in the <strong class="source-inline">MLproject</strong> file.</p>
			<p>If you saw the output shown in <em class="italic">Figure 4.9</em> and <em class="italic">Figure 4.10</em> in your MLflow tracking server, then it's time to celebrate – you have successfully executed a multi-step DL pipeline using MLflow!</p>
			<p>In summary, to track a multi-step DL pipeline in MLflow, we can use <strong class="source-inline">MLproject</strong> to define entry points for each pipeline step and a main pipeline entry point. In the main pipeline function, we implement methods so that data can be passed between pipeline steps. Each pipeline step then uses the data that's been shared, as well as other input parameters, to execute a specific task. Both the main pipeline-level function and each step of the pipeline are tracked using the MLflow tracking server, which produces a parent <strong class="source-inline">run_id</strong> to track the main pipeline run and multiple MLflow nested runs to track each pipeline's step. We introduced a template for each pipeline step to implement this task in a standard way. We also explored the powerful pipeline chaining that's done through MLflow's <strong class="source-inline">run</strong> parameter and artifact <a id="_idIndexMarker189"/>store to learn how to pass data between pipeline steps.</p>
			<p>Now that you know how to track notebooks and pipelines, let's learn how to track Python libraries.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Tracking locally, privately built Python libraries</h1>
			<p>Now, let's turn our attention<a id="_idIndexMarker190"/> to tracking locally, privately built Python libraries. For publicly released Python libraries, we can explicitly specify their released version, which is published in PyPI, in a requirements file or a <strong class="source-inline">conda.yaml</strong> file. For example, this chapter's <strong class="source-inline">conda.yaml</strong> file (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/conda.yaml">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/conda.yaml</a>) defines the Python version and provides a reference to a requirements file, as follows:</p>
			<pre class="source-code">name: dl_model </pre>
			<pre class="source-code">channels:</pre>
			<pre class="source-code">  - conda-forge</pre>
			<pre class="source-code">dependencies:</pre>
			<pre class="source-code">  - python=3.8.10</pre>
			<pre class="source-code">  - pip</pre>
			<pre class="source-code">  - pip:</pre>
			<pre class="source-code">    - -r requirements.txt</pre>
			<p>The Python version is defined as <strong class="source-inline">3.8.10</strong> and is being enforced. This <strong class="source-inline">conda.yaml</strong> file also refers to a <strong class="source-inline">requirements.txt</strong> file, which contains the following versioned Python packages as a <strong class="source-inline">requirements.txt</strong> file, which is located in the same directory as the <strong class="source-inline">conda.yaml</strong> file:</p>
			<pre class="source-code">ipykernel==6.4.1</pre>
			<pre class="source-code">lightning-flash[all]==0.5.0</pre>
			<pre class="source-code">mlflow==1.20.2</pre>
			<pre class="source-code">transformers==4.9.2</pre>
			<pre class="source-code">boto3==1.19.7</pre>
			<pre class="source-code">pytorch-lightning==1.4.9</pre>
			<pre class="source-code">datasets==1.9.0</pre>
			<pre class="source-code">click==8.0.3</pre>
			<p>As we can see, all these packages<a id="_idIndexMarker191"/> are being tracked explicitly<a id="_idIndexMarker192"/> using their published PyPI (<a href="https://pypi.org/">https://pypi.org/</a>) version number. When you run the MLflow <strong class="source-inline">MLproject</strong>, MLflow will use the <strong class="source-inline">conda.yaml</strong> file and the referenced <strong class="source-inline">requirements.txt</strong> file to create a conda virtual environment dynamically. This ensures that the execution environment is reproducible and that all the DL model pipelines can be run successfully. You may have noticed that such a virtual environment was created for you the first time you ran the previous section's MLflow pipeline project. You can do this again by running the following command:</p>
			<p class="source-code">conda env list</p>
			<p>This will produce a list of conda virtual environments in your current machine. You should be able to find a virtual environment starting with a <strong class="source-inline">mlflow-</strong> prefix, followed by a long string of alphanumerical characters, as follows:</p>
			<p class="source-code">mlflow-95353930ddb7b60101df80a5d64ef8bf6204a808</p>
			<p>This is the virtual environment that's created by MLflow dynamically, which follows the dependencies that are specified in <strong class="source-inline">conda.yaml</strong> and <strong class="source-inline">requirements.txt</strong>. Subsequently, when you log the fine-tuned model, <strong class="source-inline">conda.yaml</strong> and <strong class="source-inline">requirements.txt</strong> will be automatically logged in the MLflow artifact store, as follows:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.11.jpg" alt="Figure 4.11 – Python packages are being logged and tracked in the MLflow artifact store&#13;&#10;" width="1222" height="625"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Python packages are being logged and tracked in the MLflow artifact store</p>
			<p>As we can see, the <strong class="source-inline">conda.yaml</strong> file was automatically<a id="_idIndexMarker193"/> expanded to include the content of <strong class="source-inline">requirements.txt</strong>, as well as other dependencies that conda decides to include. </p>
			<p>For privately built Python packages, which means the Python packages that are not published to PyPI for public consumption and references, the recommended way to include such a Python package is by using <strong class="source-inline">git+ssh</strong>. Let's assume that you have a privately built project called <strong class="source-inline">cool-dl-utils</strong>, that the organization you work for is called <strong class="source-inline">cool_org</strong>, and that your project's repository has been set up in GitHub. If you want to include this project's Python package in the requirements file, you need to make sure that you add your public key to your GitHub settings. If you want to learn how to generate a public key and load it into GitHub, take a look at GitHub's guide at <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account">https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account</a>. In the <strong class="source-inline">requirements.txt</strong> file, you can add the following line, which will reference a specific GitHub hash (<strong class="source-inline">81218891bbf5a447103884a368a75ffe65b17a44</strong>) and the Python <strong class="source-inline">.egg</strong> package that was built from this private repository (you can also reference a <strong class="source-inline">.whl</strong> package if you wish):</p>
			<pre class="source-code">cool-dl-utils @ git+ssh://git@github.com/cool_org/cool-dl-utils.git@81218891bbf5a447103884a368a75ffe65b17a44#egg=cool-dl-utils</pre>
			<p>If you have a numerically released<a id="_idIndexMarker194"/> version in your privately built package, you can also directly reference the release number in the <strong class="source-inline">requirements.txt</strong> file, as follows:</p>
			<pre class="source-code">git+ssh://git@github.com/cool_org/cool-dl-utils.git@2.11.4</pre>
			<p>Here the release number of <strong class="source-inline">cool-dl-utils</strong> is <strong class="source-inline">2.11.4</strong>. This allows MLflow to pull this privately built package into the virtual environment to execute <strong class="source-inline">MLproject</strong>. In this chapter, we don't need to reference any privately built Python packages, but it is worth noting that MLflow can leverage the <strong class="source-inline">git+ssh</strong> approach to do that.</p>
			<p>Now, let's learn how to track data versioning.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Tracking data versioning in Delta Lake</h1>
			<p>In this section, we'll learn<a id="_idIndexMarker195"/> how data is tracked in MLflow. Historically, data management <a id="_idIndexMarker196"/>and versioning are usually considered as being different from machine learning and data science. However, the advent of data-centric AI is playing an increasingly important role, particularly in DL. Therefore, it is critical to know what and how data is being used to improve the DL model. In the first data-centric AI competition, which was organized by Andrew Ng in the summer of 2021, the requirements to become a winner were not about changing<a id="_idIndexMarker197"/> and tuning a model, but rather improving the dataset of a fixed model (<a href="https://https-deeplearning-ai.github.io/data-centric-comp/">https://https-deeplearning-ai.github.io/data-centric-comp/</a>). Here is a quote from<a id="_idIndexMarker198"/> the competition's web page:</p>
			<p class="author-quote">"The Data-Centric AI Competition inverts the traditional format and asks you to improve a dataset, given a fixed model. We will provide you with a dataset to improve by applying data-centric techniques such as fixing incorrect labels, adding examples that represent edge cases, applying data augmentation, and so on."</p>
			<p>This paradigm shift highlights the importance of data in deep learning, especially supervised deep learning, where labeled data is important. An implied underlying assumption is that different data will produce different model metrics, even if the same model architecture and parameters are used. This requires us to diligently track the data versioning process so that we know which version of the data is being used to produce the winning model.</p>
			<p>There are several<a id="_idIndexMarker199"/> emerging frameworks for tracking data versioning <a id="_idIndexMarker200"/>in the ML/DL life cycle. One of the early<a id="_idIndexMarker201"/> pioneers in this domain is <strong class="bold">DVC</strong> (<a href="http://dvc.org">http://dvc.org</a>). It uses a set of GitHub-like commands to pull/push data as if they are code. It allows the data to be stored remotely in S3, or Google Drive, among many other popular stores. However, the data that's stored in the remote store becomes hashed and isn't human-readable. This becomes a locked-in problem since the only way to access the data is through the DVC tool and configuration. In addition, it is hard to track how the data and its schema have been changed. While it is possible to integrate MLflow with DVC, its usability and flexibility are not as desirable as we want. Thus, we will not deep dive into this approach in this book. If you are interested in this, we suggest that you utilize the <em class="italic">Versioning data and models in ML projects using DVC and AWS</em> reference at the end of this chapter to find more details about using DVC.</p>
			<p>The recently open<a id="_idIndexMarker202"/> sourced and open format-based <strong class="bold">Delta Lake</strong> (<a href="https://delta.io/">https://delta.io/</a>) is a practical solution for integrated data management and version control in a DL/ML project, especially since MLflow can directly support such integration. This is also the foundational<a id="_idIndexMarker203"/> data management layer, called <strong class="bold">Lakehouse</strong> (<a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html</a>), which unifies both data<a id="_idIndexMarker204"/> warehouse and streaming data into one data foundation layer. It supports both schema change tracking and data versioning, which is ideal for a DL/ML data use scenario. Delta<a id="_idIndexMarker205"/> tables are based on the open standard file format called <strong class="bold">Parquet</strong> (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>), which is widely<a id="_idIndexMarker206"/> supported for large-scale data storage. </p>
			<p class="callout-heading">Delta Table in Databricks</p>
			<p class="callout">Note that this section assumes<a id="_idIndexMarker207"/> that you have access to a Databricks<a id="_idIndexMarker208"/> service, which allows you to<a id="_idIndexMarker209"/> experiment with the Delta Lake format in the <strong class="bold">Databricks File System</strong> (<strong class="bold">DBFS</strong>). You can get a trial account<a id="_idIndexMarker210"/> for the community version by going to the Databricks portal: <a href="https://community.cloud.databricks.com/login.html">https://community.cloud.databricks.com/login.html</a>.</p>
			<p>Note that this section<a id="_idIndexMarker211"/> requires you to use <strong class="bold">PySpark</strong> to manipulate the data through both reading/writing data from/into storage such as S3. Delta Lake has a capability called <strong class="bold">Time Travel</strong> that can automatically<a id="_idIndexMarker212"/> version the data. By passing a parameter such as a timestamp or a version number, you can read any historical data for that particular version or timestamp. This makes reproducing and tracking the experiments much easier as the only<a id="_idIndexMarker213"/> temporal metadata about the data is the version<a id="_idIndexMarker214"/> number or timestamp of the data. There are two ways to query the Delta table:</p>
			<ul>
				<li><strong class="source-inline">timestampAsOf</strong>: This lets you read the Delta table, as well as read a version that has a specific timestamp. The following code shows how the data can be read using <strong class="source-inline">timestampAsOf</strong>:<p class="source-code">df = spark.read \</p><p class="source-code">  .format("delta") \</p><p class="source-code">  .option("<strong class="bold">timestampAsOf</strong>", "2020-11-01") \</p><p class="source-code">  .load("/path/to/my/table")</p></li>
				<li><strong class="source-inline">versionAsOf</strong>: This defines the numerical value of the Delta table's version. You also have the option to read a version that has a specific version, starting with version 0. The following PySpark code reads the data with the <strong class="source-inline">versionAsOf</strong> option defined as version <strong class="source-inline">52</strong>:<p class="source-code">df = spark.read \</p><p class="source-code">  .format("delta") \</p><p class="source-code">  .option("<strong class="bold">versionAsOf</strong>", "52") \</p><p class="source-code">  .load("/path/to/my/table")</p></li>
			</ul>
			<p>Having this kind of timestamped or versioned access is a key advantage to tracking any file version using a Delta table. So, let's look at a concrete example of this in MLflow so that we can track the IMDb dataset we have been using.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>An example of tracking data using MLflow</h2>
			<p>For the IMDb datasets<a id="_idIndexMarker215"/> we have been using to fine-tune the sentiment classification<a id="_idIndexMarker216"/> model, we will upload these CSV files into Databricks' data store or any S3 bucket that you can access from your Databricks portal. Once you've done that, follow these steps to create a Delta table that supports versioned and timestamped data access:</p>
			<ol>
				<li value="1">Read the following CSV files into a DataFrame (assuming that you uploaded the <strong class="source-inline">train.csv</strong> file into the <strong class="source-inline">FileStore/imdb/</strong> folder in Databricks):<p class="source-code">imdb_train_df = spark.read.option('header', True).csv('dbfs:/FileStore/imdb/train.csv')</p></li>
				<li>Write the <strong class="source-inline">imdb_train_df</strong> DataFrame in DBFS as a Delta table, as follows:<p class="source-code">imdb_train_df.write.format('delta').option("mergeSchema", "true").mode("overwrite").save('/imdb/training.delta')</p></li>
				<li>Read the <strong class="source-inline">training.delta</strong> file back into memory using the following command:<p class="source-code">imdb_train_delta = spark.read.format('delta').load('/imdb/training.delta')</p></li>
				<li>Now, look at the history of the Delta table via the Databricks UI. You click on the <strong class="bold">History</strong> tab once you've read the Delta table from storage into memory:</li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="Images/B18120_Figure_4.12.jpg" alt="Figure 4.12 – The train_delta table's history with a version and a timestamp column&#13;&#10;" width="653" height="288"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – The train_delta table's history with a version and a timestamp column</p>
			<p>The preceding screenshot shows that the version is <strong class="bold">0</strong> and that the timestamp is <strong class="bold">2021-11-22</strong>. This is the value that we can use to access the versionized data when passing the version<a id="_idIndexMarker217"/> number or timestamp to a Spark<a id="_idIndexMarker218"/> DataFrame reader.</p>
			<ol>
				<li value="5">Read the versioned <strong class="source-inline">imdb/train_delta</strong> file using the following command:<p class="source-code">train_data_version = spark.read.format("delta").option("versionAsOf", "0").load('/imdb/train.delta')  </p></li>
			</ol>
			<p>This will read version <strong class="source-inline">0</strong> of the <strong class="source-inline">train.delta</strong> file. If we had other versions of this file, we could pass a different version number.</p>
			<ol>
				<li value="6">Read the timestamped <strong class="source-inline">imdb/train_delta</strong> file using the following command:<p class="source-code">train_data_timestamped = spark.read.format("delta").option("timestampAsOf", "2021-11-22T03:39:22").load('/imdb/train.delta')  </p></li>
			</ol>
			<p>This will read the timestamped data. At the time of writing, this is the only timestamp we have, which is fine. If we had more timestamped data, we could pass a different version to it.</p>
			<ol>
				<li value="7">Now, if we need to log this data version in the MLflow tracking experiment run, we can just log the path of the data, the version number, and/or the timestamp using <strong class="source-inline">mlflow.log_parameter()</strong>. This will log these as part of the experiment's parameter key-value list:<p class="source-code">mlflow.log_parameter('file_path', '/imdb/train.delta')</p><p class="source-code">mlflow.log_parameter('file_version', '0')</p><p class="source-code">mlflow.log_parameter('file_timestamp', '2021-11-22T03:39:22') </p></li>
			</ol>
			<p>The only requirement for using a Delta table is that the data needs to be stored in a form of storage that supports Delta tables, such as Lakehouse, which is supported by Databricks. This is of great<a id="_idIndexMarker219"/> value for enterprise ML/DL scenarios<a id="_idIndexMarker220"/> since we can track data versioning alongside code and model versioning.</p>
			<p>In summary, Delta Lake provides a simple yet powerful way to version data. MLflow can easily log these version numbers and timestamps as part of the experiment's parameter lists to track the data, as well as all the other parameters, metrics, artifacts, code, and models consistently.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>
			<p>In this chapter, we took a deep dive into how we can track code and data versions in an MLflow experiment run. We started by reviewing the different types of notebooks: Jupyter notebooks, Databricks notebooks, and VS Code notebooks. We compared them and recommended that VS Code should be used to author a notebook due to its IDE support, as well as its Python styling, autocompletion, and many more rich features. </p>
			<p>Then, after reviewing the limitations of existing ML pipeline API frameworks, we discussed how to create a multi-step DL pipeline using MLflow's <strong class="bold">MLproject</strong> framework. We showed a step-by-step approach to creating a three-step DL pipeline using MLproject and how to implement a pipeline function to orchestrate the necessary tasks. We also provided a Python implementation template to help you implement each pipeline task. When running a pipeline with MLflow, we can track the entire pipeline's progress with a parent <strong class="source-inline">run_id</strong>, and then use a child <strong class="source-inline">run_id</strong> for each pipeline step. The flexibility to do pipeline chaining and tracking by passing parameters or artifact store locations to the next step can be done using <strong class="source-inline">mlflow.run()</strong> and <strong class="source-inline">mlflow.tracking.MlflowClient()</strong>. We successfully ran the end-to-end three-step pipeline using the MLflow nested run tracking capability. This will also open doors for us to extend the use of MLproject for running different steps in a distributed way in future chapters.</p>
			<p>We also learned how to track privately built Python packages using the <strong class="source-inline">git+ssh</strong> approach. We then used the Delta Lake approach to gain versioned and timestamped access to data. This allows data to be tracked in two ways using a version number or a timestamp. MLflow can then log these version numbers or timestamps as a parameter during the MLflow experiment run. Since we are entering the data-centric AI era, being able to track data versioning is critical for reproducibility and time travel. </p>
			<p>With that, we've finished learning how to comprehensively track code, data, and models using MLflow. In the next chapter, we will learn how to scale out our DL experiment in a distributed way.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Further reading</h1>
			<p>For more information about the topics that were covered in this chapter, take a look at the following resources:</p>
			<ol>
				<li value="1">MLflow notebook experiment tracking in Databricks: <a href="https://docs.databricks.com/applications/mlflow/tracking.html#create-notebook-experiment">https://docs.databricks.com/applications/mlflow/tracking.html#create-notebook-experiment</a></li>
				<li><em class="italic">Building Multistep Workflows</em>: <a href="https://www.mlflow.org/docs/latest/projects.html#building-multistep-workflows">https://www.mlflow.org/docs/latest/projects.html#building-multistep-workflows</a></li>
				<li><em class="italic">End-to-end ML pipelines with MLflow projects</em>: <a href="https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/">https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/</a></li>
				<li>Installing a privately built Python package: <a href="mailto:https://medium.com/@ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b19436f3e">https://medium.com/@ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b19436f3e</a></li>
				<li><em class="italic">Versioning data and models in ML projects using DVC and AWS</em>: <a href="https://medium.com/analytics-vidhya/versioning-data-and-models-in-ml-projects-using-dvc-and-aws-s3-286e664a7209">https://medium.com/analytics-vidhya/versioning-data-and-models-in-ml-projects-using-dvc-and-aws-s3-286e664a7209</a></li>
				<li><em class="italic">Introducing Delta Time Travel for Large Scale Data Lakes</em>: <a href="https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html">https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html</a></li>
				<li><em class="italic">How We Won the First Data-Centric AI Competition: Synaptic-AnN</em>: <a href="https://www.deeplearning.ai/data-centric-ai-competition-synaptic-ann/">https://www.deeplearning.ai/data-centric-ai-competition-synaptic-ann/</a></li>
				<li><em class="italic">Reproduce Anything: Machine Learning Meets Data Lakehouse</em>: <a href="https://databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html">https://databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html</a></li>
				<li><em class="italic">DATABRICKS COMMUNITY EDITION: A BEGINNER'S GUIDE</em>: <a href="https://www.topcoder.com/thrive/articles/databricks-community-edition-a-beginners-guide">https://www.topcoder.com/thrive/articles/databricks-community-edition-a-beginners-guide</a></li>
			</ol>
		</div>
	</div></body></html>