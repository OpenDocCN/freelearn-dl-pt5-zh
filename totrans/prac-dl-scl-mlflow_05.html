<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer039">
			<h1 id="_idParaDest-41"><em class="italic"><a id="_idTextAnchor040"/>Chapter 3</em>: Tracking Models, Parameters, and Metrics</h1>
			<p>Given that MLflow can support multiple scenarios through the life cycle of DL models, it is common to use MLflow's capabilities incrementally. Usually, people start with MLflow tracking since it is easy to use and can handle many scenarios for reproducibility, provenance tracking, and auditing purposes. In addition, tracking the history of a model from cradle to sunset not only goes beyond the data science experiment management domain but is also important for model governance in the enterprise, where business and regulatory risks need to be managed for using models in production. While the precise business values of tracking models in production are still evolving, the need for tracking a model's entire life cycle is unquestionable and growing. For us to be able to do this, we will begin this chapter by setting up a full-fledged local MLflow tracking server. </p>
			<p>We will then take a deep dive into how we can track a model, along with its parameters and metrics, using MLflow's tracking and registry APIs. By the end of this chapter, you should feel comfortable using MLflow's tracking and registry APIs for various reproducibility and auditing purposes.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Setting up a full-fledged local MLflow tracking server</li>
				<li>Tracking model provenance</li>
				<li>Tracking model metrics</li>
				<li>Tracking model parameters</li>
			</ul>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Technical requirements</h1>
			<p>The following are the requirements you will need to follow the instructions provided in this chapter: </p>
			<ul>
				<li>Docker Desktop: <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>PyTorch <strong class="source-inline">lightning-flash</strong>: 0.5.0.: <a href="https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0">https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0</a>.</li>
				<li>VS Code with the Jupyter Notebook extension: <a href="https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks">https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks</a>.</li>
				<li>The following GitHub URL for the code for this chapter: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03</a>.</li>
				<li>WSL2: If you are a Microsoft Windows user, it is recommended to install WSL2 to run the Bash scripts provided in this book: <a href="https://www.windowscentral.com/how-install-wsl2-windows-10">https://www.windowscentral.com/how-install-wsl2-windows-10</a>.</li>
			</ul>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Setting up a full-fledged local MLflow tracking server</h1>
			<p>In <a href="B18120_02_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with MLflow for Deep Learning</em>, we gained hands-on experience working with a local filesystem-based MLflow tracking server and inspecting the components <a id="_idIndexMarker122"/>of the MLflow experiment. However, there are limitations with a default local filesystem-based MLflow server as the model registry functionality is not available. The benefit of having a model registry is that we can register the model, version control the model, and prepare for model deployment into production. Therefore, this model registry will bridge the gap between offline experimentation and an online deployment production scenario. Thus, we need a full-fledged MLflow tracking server with the following stores to track the complete life cycle of a model:</p>
			<ul>
				<li><strong class="bold">Backend store</strong>: A relational database backend is needed to support MLflow's storage <a id="_idIndexMarker123"/>of metadata (metrics, parameters, and many others) about the experiment. This also allows the query capability of the experiment to be used. We will use a MySQL database as a local backend store.</li>
				<li><strong class="bold">Artifact store</strong>: An <a id="_idIndexMarker124"/>object store that can store arbitrary types of objects, such as serialized models, vocabulary files, figures, and many others. In a production environment, a popular choice is the AWS S3 store. We will use <strong class="bold">MinIO</strong> (<a href="https://min.io/">https://min.io/</a>), a multi-cloud object <a id="_idIndexMarker125"/>store, as a local artifact store, which is fully compatible with the AWS S3 store API but can run on your laptop without you needing to access the cloud.</li>
			</ul>
			<p>To make this <a id="_idIndexMarker126"/>local setup as easy as possible, we will use the <strong class="source-inline">docker-compose</strong> (<a href="https://docs.docker.com/compose/">https://docs.docker.com/compose/</a>) tool with one line of command to start and stop a local full-fledged MLflow tracking server, as described in <a id="_idIndexMarker127"/>the following steps. Note that Docker Desktop (<a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>) must be installed and running on the machine before you can follow these steps. Docker helps build and share containerized applications <a id="_idIndexMarker128"/>and microservices. The following steps will launch the local MLflow tracking server inside your local Docker container:</p>
			<ol>
				<li>Check out the <strong class="source-inline">chapter03</strong> code repository for your local development environment: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03</a>.</li>
				<li>Change directory to the <strong class="source-inline">mlflow_docker_setup</strong> subfolder, which can be found under the <strong class="source-inline">chapter03</strong> folder.</li>
				<li>Run the following command:<p class="source-code"><strong class="bold">bash start_mlflow.sh</strong></p></li>
			</ol>
			<p>If the command is successful, you should see an output similar to the following on your screen:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="Images/B18120_03_001.jpg" alt="Figure 3.1 – A local full-fledged MLflow tracking server is up and running&#13;&#10;" width="361" height="108"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – A local full-fledged MLflow tracking server is up and running</p>
			<ol>
				<li value="4">Go to <strong class="source-inline">http://localhost/</strong> to see the MLflow UI web page. Then, click the <strong class="bold">Models</strong> tab in the UI (<em class="italic">Figure 3.2</em>). Note that this tab would not work if you only had a local filesystem as the backend store for the MLflow tracking server. Hence, the MLflow UI's backend is now running on the Docker container service you just started, not <a id="_idIndexMarker129"/>a local filesystem. Since this is a brand-new server, there are no registered models yet:</li>
			</ol>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="Images/B18120_03_002.jpg" alt="Figure 3.2 – MLflow model registry UI&#13;&#10;" width="902" height="299"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – MLflow model registry UI</p>
			<ol>
				<li value="5">Go to <strong class="source-inline">http://localhost:9000/</strong>,and the following screen (<em class="italic">Figure 3.3</em>) should appear for the MinIO artifact store web UI. Enter <strong class="source-inline">minio</strong> for <strong class="bold">Access Key</strong> and <strong class="source-inline">minio123</strong> for <strong class="bold">Secret Key</strong>. These are defined in the <strong class="source-inline">.env</strong> file, under the <strong class="source-inline">mlflow_docker_setup</strong> folder:</li>
			</ol>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="Images/B18120_03_003.jpg" alt="Figure 3.3 – MinIO Web UI login page and browser page after logging in&#13;&#10;" width="1657" height="519"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – MinIO Web UI login page and browser page after logging in</p>
			<p>At this <a id="_idIndexMarker130"/>point, you should have a full-fledged local MLflow tracking server running successfully! If you want to stop the server, simply type the following command:</p>
			<p class="source-code">bash stop_mlflow.sh</p>
			<p>The Docker-based MLflow tracking server will stop. We are now ready to use this local MLflow server to track model provenance, parameters, and metrics.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Tracking model provenance</h1>
			<p><strong class="bold">Provenance</strong> tracking for digital artifacts has been long studied in the literature. For example, when you're using a piece of patient diagnosis data in the biomedical industry, people usually <a id="_idIndexMarker131"/>want to know where it comes from, what kind of processing and cleaning has been done to the data, who owns the data, and other history and lineage information about the data. The rise of ML/DL models for industrial and business scenarios in production makes provenance tracking a required functionality. The different granularities of provenance tracking are critical for operationalizing and managing not just the data science offline experimentation, but also before/during/after the model is deployed in production. So, what needs to be tracked for provenance?</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Understanding the open provenance tracking framework</h2>
			<p>Let's look <a id="_idIndexMarker132"/>at a general provenance tracking framework to understand the big picture of why provenance tracking is a major effort. The <a id="_idIndexMarker133"/>following diagram is based on the <strong class="bold">Open Provenance Model Vocabulary Specification</strong> (<a href="http://open-biomed.sourceforge.net/opmv/ns.html">http://open-biomed.sourceforge.net/opmv/ns.html</a>): </p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="Images/B18120_03_004.jpg" alt="Figure 3.4 – An open provenance tracking framework&#13;&#10;" width="1187" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – An open provenance tracking framework</p>
			<p>In the <a id="_idIndexMarker134"/>preceding diagram, there are three important items:</p>
			<ul>
				<li><strong class="bold">Artifacts</strong>: Things that are produced or used by processes (<strong class="bold">A1</strong> and <strong class="bold">A2</strong>).</li>
				<li><strong class="bold">Processes</strong>: Actions that are performed by using or producing artifacts (<strong class="bold">P1</strong> and <strong class="bold">P2</strong>).</li>
				<li><strong class="bold">Causal relationships</strong>: Edges or relationships between artifacts and processes, such as <em class="italic">used</em>, <em class="italic">wasGeneratedBy</em>, and <em class="italic">wasDerivedFrom</em> in the preceding diagram (<strong class="bold">R1</strong>, <strong class="bold">R2</strong>, and <strong class="bold">R3</strong>).</li>
			</ul>
			<p>Intuitively, this <strong class="bold">open provenance model</strong> (<strong class="bold">OPM</strong>) framework <a id="_idIndexMarker135"/>allows us to ask the following 5W1H (five Ws and one H) questions, as follows:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="Images/B18120_03_005.jpg" alt="Figure 3.5 – Types of provenance questions&#13;&#10;" width="579" height="377"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Types of provenance questions</p>
			<p>Having a <a id="_idIndexMarker136"/>systematic provenance framework and a set of questions will help us learn how to track model provenance and provide answers to these questions. This will motivate us when we implement MLflow model tracking in the next section.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Implementing MLflow model tracking</h2>
			<p>We can use an MLflow tracking server to answer most of these types of provenance questions if <a id="_idIndexMarker137"/>we implement both MLflow logging and registry for the DL model we use. First, let's review what MLflow provides in terms of model provenance tracking. MLflow provides two sets of APIs for model provenance:</p>
			<ul>
				<li><strong class="bold">Logging API</strong>: This <a id="_idIndexMarker138"/>allows each run of the experiment or a model pipeline to log the model artifact into the artifact store. </li>
				<li><strong class="bold">Registry API</strong>: This <a id="_idIndexMarker139"/>allows a centralized location to track the version of the model and the stages of the model's life cycle (<strong class="bold">None</strong>, <strong class="bold">Archived</strong>, <strong class="bold">Staging</strong>, or <strong class="bold">Production</strong>).<p class="callout-heading">Difference between Model Logging and Model Registry</p><p class="callout">Although every run of the experiment needs to be logged and the model needs to be saved in <a id="_idIndexMarker140"/>the artifact store, not every instance of the model needs to be registered in the model registry. That's because, for many <a id="_idIndexMarker141"/>early exploratory model experimentations, the model might not be good. Thus, it is not necessarily registered to track the version. Only when a model has good offline performance and becomes a candidate for promoting to production do we need to register it in the model registry to go through the model promotion process.</p><p class="callout">Although MLflow's official API documentation separates logging and registry into two components, we will refer to them together as model tracking functionality in MLflow in this book. </p></li>
			</ul>
			<p>We already <a id="_idIndexMarker142"/>saw MLflow's auto-logging for the DL model we built in <a href="B18120_02_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with MLflow for Deep Learning</em>. Although auto-logging is powerful, there are two issues with the current version:</p>
			<ul>
				<li>It does not automatically register the model to the model registry.</li>
				<li>It does not work out of the box for the logged model to work directly with the original input data (in our case, an English sentence) if you just follow MLflow's suggestion to use the <strong class="source-inline">mlflow.pyfunc.load_model</strong> API to load the logged model. This is a limitation that's probably due to the experimental nature of the current auto-logging APIs in MLflow.</li>
			</ul>
			<p>Let's walk through an example to review MLflow's capabilities and auto-logging's limitations and how we can solve them:</p>
			<ol>
				<li value="1">Set up the following environment variables in your Bash terminal, where your MinIO and MySQL-based Docker component is running: <p class="source-code"><strong class="bold">export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000</strong></p><p class="source-code"><strong class="bold">export AWS_ACCESS_KEY_ID=minio</strong></p><p class="source-code"><strong class="bold">export AWS_SECRET_ACCESS_KEY=minio123</strong></p></li>
			</ol>
			<p>Note that <strong class="source-inline">AWS_ACCESS_KEY_ID</strong> and <strong class="source-inline">AWS_SECRET_ACCESS_KEY</strong> are using the same values that were defined in the <strong class="source-inline">.env</strong> file, under the <strong class="source-inline">mlflow_docker_setup</strong> folder. This is done to make sure that we are using the MLflow server that we set up previously. Since these environmental variables are session-based, we can also set up the following environment variables in the notebook's code, as follows:</p>
			<p class="source-code"><strong class="bold">os.environ["AWS_ACCESS_KEY_ID"] = "minio"</strong></p>
			<p class="source-code"><strong class="bold">os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"</strong></p>
			<p class="source-code"><strong class="bold">os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://localhost:9000"</strong></p>
			<p>The preceding <a id="_idIndexMarker143"/>three lines of code can be found in this chapter's notebook file, just after importing the required Python packages. Before you execute the notebook, make sure that you run the following commands to initialize the virtual environment, <strong class="source-inline">dl_model</strong>, which now has additional required packages defined in the <strong class="source-inline">requirements.txt</strong> file:</p>
			<p class="source-code"><strong class="bold">conda create -n dl_model python==3.8.10</strong></p>
			<p class="source-code"><strong class="bold">conda activate dl_model</strong></p>
			<p class="source-code"><strong class="bold">pip install -r requirements.txt</strong></p>
			<p>If you set up the <strong class="source-inline">dl_model</strong> virtual environment in the previous chapters, you can skip the first line on creating a virtual environment called <strong class="source-inline">dl_model</strong>. However, you still need to activate <strong class="source-inline">dl_model</strong> as the currently active virtual environment and then run <strong class="source-inline">pip install -r requirements.txt</strong> to install all the required Python packages. Once the <strong class="source-inline">dl_model</strong> virtual environment has been set up successfully, you may proceed to the next step.</p>
			<ol>
				<li value="2">To follow along with this model tracking implementation, check out the <strong class="source-inline">dl_model_tracking.ipynb</strong> notebook file in VS Code by going to this chapter's GitHub repository: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb</a>.</li>
			</ol>
			<p>Note that, in the fourth cell of the <strong class="source-inline">dl_model_tracking.ipynb</strong> notebook, we need to point it to the correct and new MLflow tracking URI that we just set up in the Docker and define a new experiment, as follows:</p>
			<p class="source-code">EXPERIMENT_NAME = "dl_model_chapter03"</p>
			<p class="source-code">mlflow.set_tracking_uri('http://localhost')</p>
			<ol>
				<li value="3">We will still <a id="_idIndexMarker144"/>use the auto-logging capabilities provided by MLflow but we will assign the run with a variable name, <strong class="source-inline">dl_model_tracking_run</strong>:<p class="source-code">mlflow.pytorch.autolog()</p><p class="source-code">with mlflow.start_run(experiment_id=experiment.experiment_id, run_name="chapter03") as <strong class="bold">dl_model_tracking_run</strong>:</p><p class="source-code">    trainer.finetune(classifier_model, datamodule=datamodule, strategy="freeze")</p><p class="source-code">    trainer.test()</p></li>
			</ol>
			<p><strong class="source-inline">dl_model_tracking_run</strong> allows us to get the <strong class="source-inline">run_id</strong> parameter and other metadata about this run programmatically, as we will see in the next step. Once this code cell has been executed, we will have a trained model logged in the MLflow tracking server with all the required parameters and metrics. However, the model hasn't been registered yet. We can find the logged experiment in the MLflow web UI, along with all the relevant parameters and metrics, at http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47. You can find the model artifacts in the <strong class="bold">MinIO</strong> storage backend. Go to <strong class="source-inline">http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/</strong> to see the storage UI, as shown here:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="Images/B18120_03_006.jpg" alt="Figure 3.6 – Model artifacts logged In the MinIO storage backend &#13;&#10;" width="782" height="526"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Model artifacts logged In the MinIO storage backend </p>
			<p>The folder <a id="_idIndexMarker145"/>structure is similar to what we saw in <a href="B18120_02_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with MLflow for Deep Learning</em>, when we used a plain filesystem to store the model artifacts. However, here, we are using a <strong class="bold">MinIO</strong> bucket to store these model artifacts.</p>
			<ol>
				<li value="4">Retrieve the <strong class="source-inline">run_id</strong> parameter from <strong class="source-inline">dl_model_tracking_run</strong>, as well as other metadata, as follows:<p class="source-code">run_id = dl_model_tracking_run.info.run_id</p><p class="source-code">print("run_id: {}; lifecycle_stage: {}".format(run_id,</p><p class="source-code">    mlflow.get_run(run_id).info.lifecycle_stage))</p></li>
			</ol>
			<p>This will print out something like the following:</p>
			<p class="source-code">run_id: 37a3fe9b6faf41d89001eca13ad6ca47; lifecycle_stage: active</p>
			<ol>
				<li value="5">Retrieve the logged model by defining the logged model URI. This will allow us to reload the logged model at this specific location:<p class="source-code">logged_model = f'runs:/{run_id}/model'</p></li>
				<li>Use <strong class="source-inline">mlflow.pytorch.load_model</strong> and the following <strong class="source-inline">logged_model</strong> URI to load the model <a id="_idIndexMarker146"/>back into memory and make a new prediction for a given input sentence, as follows:<p class="source-code">model = mlflow.pytorch.load_model(logged_model)</p><p class="source-code">model.predict({'This is great news'})</p></li>
			</ol>
			<p>This will output a model prediction label, as follows:</p>
			<p class="source-code">['positive']</p>
			<p class="callout-heading">mlflow.pytorch.load_model versus mlflow.pyfunc.load_model</p>
			<p class="callout">By default, and in the MLflow experiment tracking page's artifact section, if you have a logged <a id="_idIndexMarker147"/>model, MLflow will recommend using <strong class="source-inline">mlflow.pyfunc.load_model</strong> to load back a logged model for <a id="_idIndexMarker148"/>prediction. However, this only works for inputs such as a pandas DataFrame, NumPy array, or tensor; this does not work for an NLP text input. Since auto-logging for PyTorch lightning uses <strong class="source-inline">mlflow.pytorch.log_model</strong> to save the model, the correct way to load a logged model back is to use <strong class="source-inline">mlflow.pytorch.load_model</strong>, as we have shown here. This is because MLflow's default design is to use <strong class="source-inline">mlflow.pyfunc.load_model</strong> with standardization and a known limitation that can only accept input formats in terms of numbers. For text and image data, it requires a tokenization step as a preprocessing step. However, since the PyTorch model we saved here already performs tokenization as part of the serialized model, we can use the native <strong class="source-inline">mlflow.pytorch.load_model</strong> to directly load the model that accepts text as inputs.</p>
			<p>With that, we have successfully logged the model and loaded the model back to make a prediction. If we think this model is performing well enough, then we can register it. </p>
			<ol>
				<li value="7">Let's register the model by using the <strong class="source-inline">mlflow.register_model</strong> API:<p class="source-code">model_registry_version = mlflow.register_model(logged_model, 'nlp_dl_model')</p><p class="source-code">print(f'Model Name: {model_registry_version.name}')</p><p class="source-code">print(f'Model Version: {model_registry_version.version}')</p></li>
			</ol>
			<p>This <a id="_idIndexMarker149"/>will produce the following output:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="Images/B18120_03_007.jpg" alt="Figure 3.7 – Model registration success message &#13;&#10;" width="1071" height="217"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Model registration success message </p>
			<p>This shows that the model has been successfully registered as version 1 in the model registry, under the name <strong class="source-inline">nlp_dl_model</strong>.</p>
			<p>We can also find this registered model in the MLflow web UI by clicking <strong class="source-inline">http://localhost/#/models/nlp_dl_model/versions/1</strong>:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="Images/B18120_03_008.jpg" alt="Figure 3.8 – MLflow tracking server web UI showing the newly registered model&#13;&#10;" width="1051" height="363"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – MLflow tracking server web UI showing the newly registered model</p>
			<p>By default, a newly registered model's stage is <strong class="bold">None</strong>, as shown in the preceding screenshot.</p>
			<p>By having a model registered with a version number and stage label, we have laid the foundation for deployment to staging (also known as pre-production) and then production. We will discuss how to perform model deployment based on registered models later in this book.</p>
			<p>At this <a id="_idIndexMarker150"/>point, we have solved the two issues we <a id="_idIndexMarker151"/>raised at the beginning of this section regarding the limitations of auto-logging:</p>
			<ul>
				<li>How to load a logged DL PyTorch model using the <strong class="source-inline">mlflow.pytorch.load_model</strong> API instead of the <strong class="source-inline">mlflow.pyfunc.load_model</strong> API</li>
				<li>How to register a logged DL PyTorch model using the <strong class="source-inline">mlflow.register_model</strong> API<p class="callout-heading">Choices of MLflow DL Model Logging APIs</p><p class="callout">For DL models, the <a id="_idIndexMarker152"/>auto-logging for PyTorch only works for <strong class="bold">PyTorch lightning</strong> frameworks. There <a id="_idIndexMarker153"/>are other DL frameworks, such <a id="_idIndexMarker154"/>as <strong class="bold">TensorFlow</strong>, <strong class="bold">Keras</strong>, <strong class="bold">fastai</strong>, and <strong class="bold">MXNet</strong>, that are also <a id="_idIndexMarker155"/>supported by <a id="_idIndexMarker156"/>the corresponding MLflow auto-logging APIs. For other PyTorch frameworks such as <strong class="bold">Hugging Face</strong>, we can use MLflow's <strong class="source-inline">mlflow.pyfunc.log_model</strong> to log <a id="_idIndexMarker157"/>the model, especially when we need to have multi-step DL model pipelines. We will implement such custom MLflow model flavors later in this book. If you don't want to use auto-logging for PyTorch, then you can directly use <strong class="source-inline">mlflow.pytorch.log_model</strong>. PyTorch's auto-logging uses <strong class="source-inline">mlflow.pytorch.log_model</strong> inside its implementation (see the official MLflow open source implementation here: <a href="https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314">https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314</a>).</p></li>
			</ul>
			<p>If we <a id="_idIndexMarker158"/>don't want to use auto-logging, then we can use MLflow's model logging API directly. This also gives us an alternative way to simultaneously register the model in one call. You can use the following line of code to both log and register the trained model:</p>
			<pre class="source-code">mlflow.pytorch.log_model(pytorch_model=trainer.model, artifact_path='dl_model', registered_model_name='nlp_dl_model')</pre>
			<p>Note that this line of code does not log any parameters or metrics of the model.</p>
			<p>With that, we have not only logged many experiments and models in the tracking server for offline experimentation but also registered performant models for production deployment in the future with version control and provenance tracking. We can now answer some of the provenance questions that we posted at the beginning of this chapter:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="Images/B18120_03_009.jpg" alt="Figure 3.9 – Answers to model provenance questions&#13;&#10;" width="594" height="414"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Answers to model provenance questions</p>
			<p>The why and where provenance questions are yet to be fully answered but will be done so later in this book. This is because the why provenance question for the production model can <a id="_idIndexMarker159"/>only be tracked and logged when the model is ready for deployment, where we need to add comments and reasons to justify the model's deployment. The where provenance question can be answered fully when we have a multiple-step model pipeline. However, here, we only have a single-step pipeline, which is the simplest case. A multi-step pipeline contains explicitly separate modulized code to specify which step performs what functionality so that we can easily change the detailed implementation of any of the steps without changing the flow of the pipeline. In the next two sections, we will investigate how we can track metrics and the parameters of models without using auto-logging.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Tracking model metrics</h1>
			<p>The default metric <a id="_idIndexMarker160"/>for the text classification model in the PyTorch <strong class="source-inline">lightning-flash</strong> package is <strong class="bold">Accuracy</strong>. If we want to change the metric to <strong class="bold">F1 score</strong> (a harmonic mean of precision and recall), which is a very common metric for measuring a classifier's performance, then we need to change the configuration of the classifier model before we start the model training process. Let's learn how to make this change and then use MLflow's non-auto-logging API to log the metrics:</p>
			<ol>
				<li value="1">When defining the classifier variable, instead of using the default metric, we will pass a metric function called <strong class="source-inline">torchmetrics.F1</strong> as a variable, as follows:<p class="source-code">classifier_model = TextClassifier(backbone="prajjwal1/bert-tiny", num_classes=datamodule.num_classes, metrics=<strong class="bold">torchmetrics.F1(datamodule.num_classes)</strong>)</p></li>
			</ol>
			<p>This uses the built-in metrics function of <strong class="source-inline">torchmetrics</strong>, the <strong class="source-inline">F1</strong> module, along with the number of classes in the data we need to classify as a parameter. This makes sure that the model is trained and tested using this new metric. You will see an output similar to the following:</p>
			<p class="source-code">{'test_cross_entropy': 0.785443127155304, 'test_f1': 0.5343999862670898}</p>
			<p>This shows that the model training and testing were using the F1 score as the metric, not the default accuracy metric. For more information on how you can use <strong class="source-inline">torchmetrics</strong> for customized <a id="_idIndexMarker161"/>metrics, please consult its documentation site: <a href="https://torchmetrics.readthedocs.io/en/latest/">https://torchmetrics.readthedocs.io/en/latest/</a>.</p>
			<ol>
				<li value="2">Now, if we want to log all the metrics to the MLflow tracking server, including the training, validation, and testing metrics, we need to get all the current metrics by calling the trainer's callback function, as follows:<p class="source-code">    cur_metrics = trainer.callback_metrics</p></li>
			</ol>
			<p>Then, we need to cast all the metric values to <strong class="source-inline">float</strong> to make sure that they are compatible with the MLflow <strong class="source-inline">log_metrics</strong> API:</p>
			<p class="source-code">    metrics = dict(map(lambda x: (x[0], float(x[1])), cur_metrics.items()))</p>
			<ol>
				<li value="3">Now, we can call MLflow's <strong class="source-inline">log_metrics</strong> to log all the metrics in the tracking server:<p class="source-code">    mlflow.log_metrics(metrics)</p></li>
			</ol>
			<p>You will <a id="_idIndexMarker162"/>see the following metrics after using the F1 score as the classifier's metric, which will be logged in MLflow's tracking server:</p>
			<p class="source-code">{'train_f1': 0.5838666558265686, </p>
			<p class="source-code">'train_f1_step': 0.75, </p>
			<p class="source-code">'train_cross_entropy': 0.7465656399726868, </p>
			<p class="source-code">'train_cross_entropy_step': 0.30964696407318115, </p>
			<p class="source-code">'val_f1': 0.5203999876976013, </p>
			<p class="source-code">'val_cross_entropy': 0.8168156743049622, </p>
			<p class="source-code">'train_f1_epoch': 0.5838666558265686, </p>
			<p class="source-code">'train_cross_entropy_epoch': 0.7465656399726868, </p>
			<p class="source-code">'test_f1': 0.5343999862670898, </p>
			<p class="source-code">'test_cross_entropy': 0.785443127155304}</p>
			<p>Using MLflow's <strong class="source-inline">log_metrics</strong> API gives us more control with additional lines of code, but if we are satisfied with its auto-logging capabilities, then the only thing we need to change is what metric we want to use for the model training and testing processes. In this case, we only need to define a new metric to use when declaring a new DL model (that is, use the F1 score instead of the default accuracy metric).</p>
			<ol>
				<li value="4">If you want to track multiple model metrics simultaneously, such as the F1 score, accuracy, precision, and recall, then the only thing you need to do is define a Python list of metrics you want to compute and track, as follows:<p class="source-code">list_of_metrics = [torchmetrics.Accuracy(),</p><p class="source-code">   torchmetrics.F1(num_classes=datamodule.num_classes),</p><p class="source-code">   torchmetrics.Precision(num_classes=datamodule.num_classes),</p><p class="source-code">   torchmetrics.Recall(num_classes=datamodule.num_classes)]</p></li>
			</ol>
			<p>Then, in the model initialization statement, instead of passing a single metric to the <strong class="source-inline">metrics</strong> parameter, you can just pass the <strong class="source-inline">list_of_metrics</strong> Python list that we just defined, above the <strong class="source-inline">metrics</strong> parameter, as follows:</p>
			<p class="source-code">classifier_model = TextClassifier(backbone=<strong class="bold">"prajjwal1/bert-tiny"</strong>, num_classes=datamodule.num_classes, metrics=<strong class="bold">list_of_metrics</strong>)</p>
			<p>No more <a id="_idIndexMarker163"/>changes need to be made to the rest of the code. So, in the <strong class="source-inline">dl_model-non-auto-tracking.ipynb</strong> notebook (https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb), you will notice that the preceding line is commented out by default. However, you can uncomment it and then comment out the previous one:</p>
			<p class="source-code">classifier_model = TextClassifier(backbone=<strong class="bold">"prajjwal1/bert-tiny"</strong>, num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))</p>
			<p>Then, when you run the rest of the notebook, you will get the model testing reports, along with the following metrics, in the notebook's output:</p>
			<p class="source-code">{'test_accuracy': 0.6424000263214111, 'test_cross_entropy': 0.6315688490867615, 'test_f1': 0.6424000263214111, 'test_precision': 0.6424000263214111, 'test_recall': 0.6424000263214111}</p>
			<p>You may notice that the numbers for accuracy, F1, precision, and recall are the same. This is <a id="_idIndexMarker164"/>because, by default, <strong class="source-inline">torchmetrics</strong> uses a <strong class="bold">micro-average</strong> method, which computes a single scalar average score for all the classes by counting total true positives, false negatives, and false positives. Scikit-learn <a id="_idIndexMarker165"/>has an average option called <strong class="bold">binary</strong> that outputs only the score for the positive label when it is a binary classification model (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#</a>). However, <strong class="source-inline">torchmetrics</strong> does not support a <strong class="bold">binary</strong> average method for a binary classification model. The only alternative is to use a <strong class="source-inline">none</strong> method, which computes the metric for each class and returns the metric for each class, even for a binary <a id="_idIndexMarker166"/>classification model. So, this does not produce a single scalar number. However, you can always call scikit-learn's metrics API to compute an F1-score or other metrics based on the binary average method by passing two lists of values. Here, we can use <strong class="source-inline">y_true</strong> and <strong class="source-inline">y_predict</strong>, where <strong class="source-inline">y_true</strong> is the list of ground truth label values and <strong class="source-inline">y_predict</strong> is the list of model predicted label values. This can be a good exercise for you to try out as this is a common practice for all ML models, not special treatment for a DL model.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/> Tracking model parameters</h1>
			<p>As we have already seen, there are lots of benefits of using auto-logging in MLflow, but if we want <a id="_idIndexMarker167"/>to track additional model parameters, we can either use MLflow to log additional parameters on top of what auto-logging records, or directly use MLflow to log all the parameters we want without using auto-logging at all.</p>
			<p>Let's walk through a notebook without using MLflow auto-logging. If we want to have full control <a id="_idIndexMarker168"/>of what parameters will be logged by MLflow, we can <a id="_idIndexMarker169"/>use two APIs: <strong class="source-inline">mlflow.log_param</strong> and <strong class="source-inline">mlflow.log_params</strong>. The first one logs a single pair of key-value parameters, while the second logs an entire dictionary of key-value parameters. So, what kind of parameters might we be interested in tracking? The following answers this:</p>
			<ul>
				<li><strong class="bold">Model hyperparameters</strong>: <strong class="bold">Hyperparameters</strong> are defined before the learning <a id="_idIndexMarker170"/>process begins, which means they control how <a id="_idIndexMarker171"/>the learning process learns. These parameters can be turned and can directly affect how well a model trains. In a DL model, the list of hyperparameters includes the backbone language model, learning rate, loss function, the optimizer to be used, and many more. MLflow's auto-logging does not automatically log all the hyperparameters, so this is an opportunity for us to directly use MLflow's <strong class="source-inline">log_params</strong> API to record them in the experiment.</li>
				<li><strong class="bold">Model parameters</strong>: These parameters are learned during the model training process. For a DL model, these usually refer to the neural network weights that are learned <a id="_idIndexMarker172"/>during training. We don't need to log these weight parameters individually since they are already in the logged DL model.</li>
			</ul>
			<p>Let's log these hyperparameters using MLflow's <strong class="source-inline">log_params</strong> API, as follows:</p>
			<pre class="source-code">    params = {"epochs": trainer.max_epochs}</pre>
			<pre class="source-code">    if hasattr(trainer, "optimizers"):</pre>
			<pre class="source-code">        optimizer = trainer.optimizers[0]</pre>
			<pre class="source-code">        params["optimizer_name"] = optimizer.__class__.__name__</pre>
			<pre class="source-code">    if hasattr(optimizer, "defaults"):</pre>
			<pre class="source-code">        params.update(optimizer.defaults)</pre>
			<pre class="source-code">    params.update(classifier_model.hparams)</pre>
			<pre class="source-code">    mlflow.log_params(params)</pre>
			<p>Note that here, we log the maximal number of epochs, the trainer's first optimizer's name, the optimizer's default parameters, and the overall classifier's hyperparameters (<strong class="source-inline">classifier_model.hparams</strong>). The one-line piece of code <strong class="source-inline">mlflow.log_params(params)</strong> logs all the key-value parameters in the <strong class="source-inline">params</strong> dictionary to the MLflow tracking server. If you see the following hyperparameters in the MLflow tracking server, then it means it works!</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="Images/B18120_03_010.jpg" alt="Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters &#13;&#10;" width="840" height="1543"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters </p>
			<p>Notice that this list of parameters is more than what the auto-logger logs as we added additional hyperparameters to log in the experiment. If you want to log any other customized parameters, you can follow the same pattern in your experiment. The complete <a id="_idIndexMarker173"/>notebook, without the use of auto-logging, can be checked out in this chapter's GitHub repository at <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb</a>.</p>
			<p>If you have reached this point in this chapter, then you have successfully implemented an MLflow tracking model and its metrics and parameters!</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we set up a local MLflow development environment that has full support for backend storage and artifact storage using MySQL and the MinIO object store. This will be very useful for us when we develop MLflow-supported DL models in this book. We started by presenting the open provenance tracking framework and asked model provenance tracking questions that are of interest. We worked on addressing the issues of auto-logging and successfully registered a trained model by loading a trained model from a logged model in MLflow for prediction using the <strong class="source-inline">mlflow.pytorch.load_model</strong> API. We also experimented on how to directly use MLflow's <strong class="source-inline">log_metrics</strong>, <strong class="source-inline">log_params</strong>, and <strong class="source-inline">log_model</strong> APIs without auto-logging, which gives us more control and flexibility over how we can log additional or customized metrics and parameters. We were able to answer many of the provenance questions by performing model provenance tracking, as well as by providing a couple of the questions that require further study of using MLflow to track multi-step model pipelines and their deployment. </p>
			<p>We will continue our learning journey in the next chapter and learn how to perform code and data tracking using MLflow, which will give us additional power to answer data and code-related provenance questions.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li>MLflow Docker setup reference: <a href="https://github.com/sachua/mlflow-docker-compose">https://github.com/sachua/mlflow-docker-compose</a></li>
				<li>MLflow PyTorch autologging implementation: <a href="https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py">https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py</a></li>
				<li>MLflow PyTorch model logging, loading, and registry documentation: <a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html">https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html</a></li>
				<li>MLflow parameters and metrics logging documentation: <a href="https://www.mlflow.org/docs/latest/python_api/mlflow.html">https://www.mlflow.org/docs/latest/python_api/mlflow.html</a></li>
				<li>MLflow model registry documentation: <a href="https://www.mlflow.org/docs/latest/model-registry.html">https://www.mlflow.org/docs/latest/model-registry.html</a></li>
				<li>Digging into big provenance (with SPADE): <a href="https://queue.acm.org/detail.cfm?id=3476885">https://queue.acm.org/detail.cfm?id=3476885</a></li>
				<li>How to utilize <strong class="source-inline">torchmetrics</strong> and <strong class="source-inline">lightning-flash</strong>: <a href="https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash">https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash</a></li>
				<li>Why are precision, recall, and F1 score equal when using micro averaging in a multi-class problem? <a href="https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/">https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/</a></li>
			</ul>
		</div>
	</div></body></html>