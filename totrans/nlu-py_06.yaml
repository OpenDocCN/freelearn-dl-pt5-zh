- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring and Visualizing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring and visualizing data are essential steps in the process of developing
    a **natural language understanding** (**NLU**) application. In this chapter, we
    will explore techniques for **data exploration**, such as visualizing word frequencies,
    and techniques for visualizing document similarity. We will also introduce several
    important visualization tools, such as Matplotlib, Seaborn, and WordCloud, that
    enable us to graphically represent data and identify patterns and relationships
    within our datasets. By combining these techniques, we can gain valuable perspectives
    into our data, make informed decisions about the next steps in our NLU processing,
    and ultimately, improve the accuracy and effectiveness of our analyses. Whether
    you’re a data scientist or a developer, data exploration and visualization are
    essential skills for extracting actionable insights from text data in preparation
    for further NLU processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover several topics related to the initial exploration
    of data, especially visual exploration, or **visualization**. We will start by
    reviewing a few reasons why getting a visual perspective of our data can be helpful.
    This will be followed by an introduction to a sample dataset of movie reviews
    that we will be using to illustrate our techniques. Then, we will look at techniques
    for data exploration, including summary statistics, visualizations of word frequencies
    in a dataset, and measuring document similarity. We will follow with a few general
    tips on developing visualizations and conclude with some ideas about using information
    from visualizations to make decisions about further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Why visualize?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General considerations for developing visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using information from visualization to make decisions about processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why visualize?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing data means displaying data in a graphical format such as a chart
    or graph. This is almost always a useful precursor to training a **natural language
    processing** (**NLP**) system to perform a specific task because it is typically
    very difficult to see patterns in large amounts of text data. It is often much
    easier to see overall patterns in data visually. These patterns might be very
    helpful in making decisions about the most applicable text-processing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization can also be useful in understanding the results of NLP analysis
    and deciding what the next steps might be. Because looking at the results of NLP
    analysis is not an initial exploratory step, we will postpone this topic until
    [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226) and [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248).
  prefs: []
  type: TYPE_NORMAL
- en: In order to explore visualization, in this chapter, we will be working with
    a dataset of text documents. The text documents will illustrate a binary classification
    problem, which will be described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Text document dataset – Sentence Polarity Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Sentence Polarity Dataset is a commonly used dataset that consists of movie
    reviews originally from the **Internet Movie Database** (**IMDb**). The reviews
    have been categorized in terms of positive and negative reviews. The task that
    is most often used with these reviews is classifying the reviews into positive
    and negative. The data was collected by a team at Cornell University. More information
    about the dataset and the data itself can be found at [https://www.cs.cornell.edu/people/pabo/movie-review-data/](https://www.cs.cornell.edu/people/pabo/movie-review-data/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Toolkit** (**NLTK**) comes with this dataset as one of its
    built-in corpora. There are 1,000 positive reviews and 1,000 negative reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a positive review from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s an example of a negative review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can look at the thousands of examples in our dataset, but it will be hard
    to see any large-scale patterns by looking at individual examples; there is just
    too much data for us to separate the big picture from the details. The next sections
    will cover tools that we can use to identify these large-scale patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data exploration**, which is sometimes also called **exploratory data analysis**
    (**EDA**), is the process of taking a first look at your data to see what kinds
    of patterns there are to get an overall perspective on the full dataset. These
    patterns and overall perspective will help us identify the most appropriate processing
    approaches. Because some NLU techniques are very computationally intensive, we
    want to ensure that we don’t waste a lot of time applying a technique that is
    inappropriate for a particular dataset. Data exploration can help us narrow down
    the options for techniques at the very beginning of our project. Visualization
    is a great help in data exploration because it is a quick way to get the big picture
    of patterns in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: The most basic kind of information about a corpus that we would want to explore
    includes information such as the number of words, the number of distinct words,
    the average length of documents, and the number of documents in each category.
    We can start by looking at the frequency distributions of words. We will cover
    several different ways of visualizing word frequencies in our datasets and then
    look at some measurements of document similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Frequency distributions show how many items of a specific type occur in some
    context. In our case, the context is a dataset, and we will be looking at the
    frequencies of *words* and then *ngrams* (sequences of words) that occur in our
    dataset and subsets of the dataset. We’ll start by defining word frequency distributions
    and doing some preprocessing. We’ll then visualize the word frequencies with Matplotlib
    tools and WordCloud, and then apply the same techniques to ngrams. Finally, we’ll
    cover some techniques for visualizing document similarity: bag of words (**BoW**)
    and k-means clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: Word frequency distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The words and their frequencies that occur in a corpus are often very informative.
    It’s a good idea to take a look at this information before getting too far into
    an NLP project. Let’s take a look at computing this information using NLTK. We
    start by importing NLTK and the movie reviews dataset, as shown in *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.1 – Importing \uFEFFNLTK and the movie reviews dataset](img/B19005_06_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Importing NLTK and the movie reviews dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.2* shows code that we can use to collect the words in the `movie_reviews`
    dataset into a list and take a look at some of the words in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To collect the words in the corpus into a Python list, NLTK provides a helpful
    function, `words()`, which we use here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find out the total number of words in the dataset by using the normal Python
    `len()` function for finding the size of a list, with the list of words as a parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print the list of words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 6.2\uFEFF –  Making a list of the words in the corpus, counting them,\
    \ and displaying the first few](img/B19005_06_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Making a list of the words in the corpus, counting them, and displaying
    the first few
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of counting the words and printing the first few words is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We won’t see more words than the first few (`'plot'`, `':'`, `'two'`, and so
    on) because there are too many to show. By printing the length of the list, we
    can see that there are 1,583,820 individual words in the dataset, which is certainly
    too many to list. What we can also notice is that this list of words includes
    punctuation, specifically `:`. Since punctuation is so common, looking at it is
    unlikely to yield any insights into differences between documents. In addition,
    including punctuation will also make it harder to see the words that actually
    distinguish categories of documents. So, let’s remove punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.3* shows a way to remove punctuation with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an empty list where we will store the new list of words after removing
    punctuation (`words_no_punct`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through the list of words (`corpus_words`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep only the alphanumeric words. This code uses the Python string function,
    `string.isalnum()`, at line 4 to detect alphanumeric words, which are added to
    the `words_no_punct` list that we previously initialized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we remove the punctuation, we might be interested in the frequencies of
    occurrence of the non-punctuation words. Which are the most common words in our
    corpus? NLTK provides a useful `FreqDist()` function (frequency distribution)
    that computes word frequencies. This function is applied at line 6 in *Figure
    6**.3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then see the most common words in the frequency distribution using the
    NLTK `most_common()` method, which takes as a parameter how many words we want
    to see, shown in line 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we print the 50 most common words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 6.3\uFEFF –  The most common 50 words in the movie review database\
    \ and their frequencies](img/B19005_06_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – The most common 50 words in the movie review database and their
    frequencies
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.3*, we can easily see that `the` is the most common word, with
    76,529 occurrences in the dataset, which is not surprising. However, it’s harder
    to see the frequencies of the less common words. It’s not easy to see, for example,
    the tenth most common word, and how much more common it is than the eleventh most
    common word. This is where we bring in our visualization tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The frequencies that we computed in *Figure 6**.3* can be plotted with the
    `plot()` function for frequency distributions. This function takes two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter is the number of words we want to see – in this case, it
    is `50`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `cumulative` parameter determines whether we just want to see the frequency
    of each of our 50 words (`cumulative=False`). If we want to see the cumulative
    frequencies for all the words, we would set this parameter to `True`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To plot the frequency distribution, we can simply add a call to `plot()` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in *Figure 6**.4*, a plot of the frequencies of each word,
    with the most frequent words first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Frequency distribution of words in the movie review corpus](img/B19005_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Frequency distribution of words in the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: Try `freq.plot()` with different numbers of words and with `cumulative = True`.
    What happens when you ask for more and more words? You will see that looking at
    rarer and rarer words (by increasing the value of the first argument to larger
    numbers) does not provide much insight.
  prefs: []
  type: TYPE_NORMAL
- en: While the graph in *Figure 6**.4* gives us a much clearer idea of the frequencies
    of the different words in the dataset, it also tells us something more important
    about the frequent words. Most of them, such as `the`, `a`, and `and`, are not
    very informative. Just like punctuation, they are not likely to help us distinguish
    the categories (positive and negative reviews) that we’re interested in because
    they occur very frequently in most documents. These kinds of words are called
    **stopwords** and are usually removed from natural language data before NLP precisely
    because they don’t add much information.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLTK provides a list of common stopwords for different languages. Let’s take
    a look at some of the English stopwords:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Examples of 50 English stopwords](img/B19005_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Examples of 50 English stopwords
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.5* shows the code we can use to examine English stopwords, using
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `nltk` and the `stopwords` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the set of stopwords into a Python list so that we can do list operations
    on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find out how long the list is with the Python `len()` function and print the
    value (179 items).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the first `50` stopwords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stopwords are available for other languages in NLTK as well. We can see the
    list by running the code in *Figure 6**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – NLTK languages with stopwords](img/B19005_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – NLTK languages with stopwords
  prefs: []
  type: TYPE_NORMAL
- en: 'The code has the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect the names of the languages with NLTK stopwords. The stopwords are contained
    in files, one file per language, so getting the files is a way to get the names
    of the languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the length of the list of languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the names of the languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have the list of stopwords, it is easy to remove them from a corpus.
    After removing the punctuation, as we saw in *Figure 6**.3*, we can just iterate
    through the list of words in the corpus, removing the words that are in the stopwords
    list, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As an aside, it would clearly be more efficient to combine removing punctuation
    and stopwords with one iteration through the entire list of words, checking each
    word to see whether it is either punctuation or a stopword, and removing it if
    it is either one. We show these steps separately for clarity, but you may want
    to combine these two steps in your own projects. Just as we did in the case of
    removing punctuation, we can use the NLTK frequency distribution function to see
    which words other than stopwords are most common, and display them, as shown in
    *Figure 6**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Most common words, after removing punctuation and stopwords](img/B19005_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Most common words, after removing punctuation and stopwords
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 6**.7* that removing stopwords gives us a much clearer
    picture of the words in the movie review corpus. The most common word is now `film`,
    and the third most common word is `movie`. This is what we would expect from a
    corpus of movie reviews. A similar review corpus (for example, a corpus of product
    reviews) would be expected to show a different distribution of frequent words.
    Even looking at the simple information provided by frequency distributions can
    sometimes be enough to do some NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in authorship studies, we’re interested in trying to attribute
    a document whose author is unknown to a known author. The word frequencies in
    the document whose author we don’t know can be compared to those in the documents
    with a known author. Another example would be domain classification, where we
    want to know whether the document is a movie review or a product review.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data with Matplotlib, Seaborn, and pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While NLTK has some basic plotting functions, there are some other, more powerful
    Python plotting packages that are used very often for plotting all kinds of data,
    including NLP data. We’ll look at some of these in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Matplotlib ([https://matplotlib.org/](https://matplotlib.org/)) is very popular.
    Matplotlib can create a variety of visualizations, including animations and interactive
    visualizations. We’ll use it here to create another visualization of the data
    that we plotted in *Figure 6**.4*. Seaborn ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    is built on Matplotlib and is a higher-level interface for producing attractive
    visualizations. Both of these packages frequently make use of another Python data
    package, pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)), which
    is very useful for handling tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we’ll use the same data that we plotted in *Figure 6**.7*,
    but we’ll use Matplotlib, Seaborn, and pandas to create our visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.8* shows the code for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Python code for collecting the 25 most common words, after removing
    punctuation and stopwords](img/B19005_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Python code for collecting the 25 most common words, after removing
    punctuation and stopwords
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.8* shows the preparatory code for plotting the data, with the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the NLTK, pandas, Seaborn, and Matplotlib libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the frequency cutoff to `25` (this can be whatever number we’re interested
    in).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute a frequency distribution of the corpus words without stopwords (line
    7).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We start by importing NLTK, pandas, Seaborn, and Matplotlib. We set the frequency
    cutoff to `25`, so that we will plot only the 25 most common words, and we get
    the frequency distribution of our data at line 7.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting plot is shown in *Figure 6**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Most common words, after removing punctuation and stopwords,
    displayed using Matplotlib, Seaborn, and pandas libraries](img/B19005_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Most common words, after removing punctuation and stopwords, displayed
    using Matplotlib, Seaborn, and pandas libraries
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.7* and *Figure 6**.9* show the same data. For example, the most
    common word in each figure is `film`, followed by `one`, and then `movie`. Why
    would you pick one visualization tool over the other? One difference is that it
    seems to be easier to see the information for a specific word in *Figure 6**.9*,
    which is probably because of the bar graph format, where every word is associated
    with a specific bar. In contrast, in *Figure 6**.7*, it is a little harder to
    see the frequencies of individual words because this isn’t very clear from the
    line.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the overall frequency distribution is easier to see in *Figure
    6**.7* because of the continuous line. So, choosing one type of visualization
    over the other is really a matter of the kind of information that you want to
    emphasize. Of course, there is also no reason to limit yourself to one visualization
    if you want to see your data in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, the pattern in both figures, where we have very high frequencies
    for the most common words, falling off rapidly for less common words, is very
    common in natural language data. This pattern illustrates a concept called **Zipf’s
    law** (for more information about this concept, see [https://en.wikipedia.org/wiki/Zipf%27s_law](https://en.wikipedia.org/wiki/Zipf%27s_law)).
  prefs: []
  type: TYPE_NORMAL
- en: Looking at another frequency visualization technique – word clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **word cloud** is another way of visualizing the relative frequencies of words.
    A word cloud displays the words in a dataset in different font sizes, with the
    more frequent words shown in larger fonts. Word clouds are a good way to make
    frequent words pop out visually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in *Figure 6**.10* shows how to import the `WordCloud` package and
    create a word cloud from the movie review data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Python code for most common words, shown as a word cloud](img/B19005_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Python code for most common words, shown as a word cloud
  prefs: []
  type: TYPE_NORMAL
- en: 'The code shows the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We import a new library, `WordCloud`, at line 2\. For this display, we will
    select only the most *common 200 words* (with a frequency cutoff of `200` at line
    6).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a frequency distribution at line 7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The frequency distribution is converted to a pandas Series at line 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To reduce the number of very short words, we include only words that are longer
    than two letters at line 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code in line 11 generates the word cloud. The `colormap` parameter specifies
    one of the many Matplotlib color maps (color maps are documented at [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html)).
    You can experiment with different color schemes to find the ones that you prefer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lines 12-14 format the plot area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the plot is displayed at line 15.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 6**.11* shows the word cloud that results from the code in *Figure
    6**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Word cloud for word frequencies in movie reviews](img/B19005_06_11New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Word cloud for word frequencies in movie reviews
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in *Figure 6**.7* and *Figure 6**.9*, the most common words are `film`,
    `one`, and `movie`. However, the word cloud visualization makes the most common
    words stand out in a way that the graphs do not. On the other hand, the less frequent
    words are difficult to differentiate in the word cloud. For example, it’s hard
    to tell whether `good` is more frequent than `story` from the word cloud. This
    is another example showing that you should consider what information you want
    to get from a visualization before selecting one.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will show you how to dig into the data a little more deeply
    to get some insight into subsets of the data such as positive and negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Positive versus negative movie reviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We might want to look at differences between the different classes (positive
    and negative) of movie reviews, or, more generally, any different categories.
    For example, are the frequent words in positive and negative reviews different?
    This preliminary look at the properties of the positive and negative reviews can
    give us some insight into how the classes differ, which, in turn, might help us
    select the approaches that will enable a trained system to differentiate the categories
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We can use any of the visualizations based on frequency distributions for this,
    including the line graph in *Figure 6**.7*, the bar graph in *Figure 6**.9*, or
    the word cloud visualization in *Figure 6**.11*. We’ll use the word cloud in this
    example because it is a good way of seeing the differences in word frequencies
    between the two frequency distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the code in *Figure 6**.12*, we start by importing the libraries
    we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Importing libraries for computing word clouds](img/B19005_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Importing libraries for computing word clouds
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we’ll do is create two functions that will make it easier to
    perform similar operations on different parts of the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Functions for computing word clouds](img/B19005_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Functions for computing word clouds
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function in *Figure 6**.13*, `clean_corpus()`, removes the punctuation
    and stopwords from a corpus. The second one, `plot_freq_dist()`, plots the word
    cloud from the frequency distribution. Now we’re ready to create the word clouds:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.14 – The code for displaying \uFEFFword frequencies for positive\
    \ and negative reviews in a word cloud](img/B19005_06_14New.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The code for displaying word frequencies for positive and negative
    reviews in a word cloud
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the word cloud with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined the functions, we separate the corpus into positive and
    negative reviews. This is done by the code shown in lines 28 and 29.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of asking for all the words in the corpus, as we saw in *Figure 6**.2*,
    we ask for words in specific categories. In this example, the categories are positive
    (`pos`) and negative (`neg`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We remove the stopwords and punctuation in each set of words in lines 28 and
    29.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then create frequency distributions for the positive and negative words in
    lines 30 and 31\. Finally, we plot the word cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 6**.15* shows the word cloud for the word frequencies in the positive
    reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Displaying word frequencies for positive reviews in a word
    cloud](img/B19005_06_15New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Displaying word frequencies for positive reviews in a word cloud
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.16* shows the word frequencies for negative reviews in a word cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Displaying word frequencies for negative reviews in a word
    cloud](img/B19005_06_16New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Displaying word frequencies for negative reviews in a word cloud
  prefs: []
  type: TYPE_NORMAL
- en: Comparing *Figures 6.15* and *6.16* with the original word cloud in *Figure
    6**.11*, we can see that the words `film`, `one`, and `movie` are the most frequent
    words in positive and negative reviews, as well as being the most frequent overall
    words, so they would not be very useful in distinguishing positive and negative
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: The word `good` is larger (and therefore more frequent) in the positive review
    word cloud than in the negative review word cloud, which is just what we would
    expect. However, `good` does occur fairly frequently in the negative reviews,
    so it’s not a definite indication of a positive review by any means. Other differences
    are less expected – for example, `story` is more common in the positive reviews,
    although it does occur in the negative reviews. This comparison shows that the
    problem of distinguishing between positive and negative reviews is unlikely to
    be solved by simple keyword-spotting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques we’ll be looking at in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173)
    and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), will be more suitable for
    addressing the problem of classifying texts into different categories. However,
    we can see that this initial exploration with word clouds was very useful in eliminating
    a simple keyword-based approach from the set of ideas we are considering.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look at other frequencies in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at other frequency measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far we’ve only looked at word frequencies, but we can look at the frequency
    of any other text property we can measure. For example, we can look at the frequencies
    of different characters or different parts of speech. You can try extending the
    code that was presented earlier in this chapter, in the previous section, *Frequency
    distributions*, to count some of these other properties of the movie review texts.
  prefs: []
  type: TYPE_NORMAL
- en: One important property of language is that words don’t just occur in isolation
    – they occur in specific combinations and orders. The meanings of words can change
    dramatically depending on their contexts. For example, *not a good movie* has
    a very different meaning (in fact, the opposite meaning) from *a good movie*.
    There are a number of techniques for taking word context into account, which we
    will explore in *Chapters 8* to *11*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, here we’ll just describe one very simple technique – looking at words
    that occur next to each other. Where two words occur together, this is called
    a `ngrams()`, which takes the desired value of `n` as an argument. *Figure 6**.17*
    shows code for counting and displaying ngrams in the movie review corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Computing bigrams in the movie review data](img/B19005_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Computing bigrams in the movie review data
  prefs: []
  type: TYPE_NORMAL
- en: 'The code shows the following initialization steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by importing `nltk`, the `ngrams` function, and the movie review corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the frequency cutoff to `25`, but as in previous examples, this can be
    any number that we think will be interesting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We collect all the words in the corpus at line 8 (if we wanted just the words
    from the positive reviews or just the words from the negative reviews, we could
    get those by setting `categories = 'neg'` or `categories = 'pos'`, as we saw in
    *Figure 6**.14*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we remove punctuation and stopwords at line 11, using the `clean_corpus`
    function defined in *Figure 6**.13*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In *Figure 6**.18*, we collect the bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Computing bigrams in the movie review data](img/B19005_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Computing bigrams in the movie review data
  prefs: []
  type: TYPE_NORMAL
- en: 'The bigrams are collected using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The `ngrams()` function is used at line 14, with an argument, `2`, indicating
    that we want bigrams (or pairs of two adjacent words). Any number can be used
    here, but very large numbers are not likely to be very useful. This is because
    as the value of `n` increases, there will be fewer and fewer ngrams with that
    value. At some point, there won’t be enough examples of a particular ngram to
    provide any information about patterns in the data. Bigrams or trigrams are usually
    common enough in the corpus to be helpful in identifying patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In lines 21-23, we loop through the list of bigrams, creating a string from
    each pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure 6.19*, we make a frequency distribution of the bigrams and display
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Displaying bigrams in the movie review data](img/B19005_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Displaying bigrams in the movie review data
  prefs: []
  type: TYPE_NORMAL
- en: We use the familiar NLTK `FreqDist()` function on our list of bigrams and convert
    it to a pandas Series in line 29.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest of the code sets up a bar graph plot in Seaborn and Matplotlib, and
    finally, displays it at line 39.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we’re using a frequency cutoff of `25`, we’ll only be looking at the most
    common 25 bigrams. You may want to experiment with larger and smaller frequency
    cutoffs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.20* shows the results of the plot that we computed in *Figure 6**.19*.
    Because bigrams, in general, will be longer than single words and take more room
    to display, the *x* and *y* axes have been swapped around so that the counts of
    bigrams are displayed on the *x* axis and the individual bigrams are displayed
    on the *y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Bigrams in the movie review data, ordered by frequency](img/B19005_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Bigrams in the movie review data, ordered by frequency
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.20* reveals several interesting facts about the corpus. For example,
    the most common bigram, `special effects`, occurs about 400 times in the corpus,
    compared to the most common single word, `film`, which occurs nearly 8,000 times.
    This is to be expected because two words have to occur together to count as a
    bigram. We also see that many of the bigrams are `New York` and `Star Trek` are
    examples in this list. Other bigrams are not idioms but just common phrases, such
    as `real life` and `one day`. In this list, all of the bigrams are very reasonable,
    and it is not surprising to see any of them in a corpus of movie reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, try comparing the bigrams in the positive and negative movie
    reviews. In looking at single-word frequencies, we saw that the most common words
    were the same in both positive and negative reviews. Is that also the case with
    the most common bigrams?
  prefs: []
  type: TYPE_NORMAL
- en: This section covered some ways to get insight into our datasets by simple measurements
    such as counting words and bigrams. There are also some useful exploratory techniques
    for measuring and visualizing the similarities among documents in a dataset. We
    will cover these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the similarities among documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve been looking at visualizing the frequencies of various properties
    of the corpus, such as words and bigrams, with tools such as line charts, bar
    graphs, and word clouds. It is also very informative to visualize document similarity
    – that is, how similar documents in a dataset are to each other. There are many
    ways to measure document similarity, which we will discuss in detail in later
    chapters, especially [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), [*Chapter
    10*](B19005_10.xhtml#_idTextAnchor184), [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    and [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217). We will start here with
    an introductory look at two basic techniques.
  prefs: []
  type: TYPE_NORMAL
- en: BoW and k-means clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For now, we will use a very simple idea, called **bag of words** (**BoW**).
    The idea behind BoW is that documents that are more similar to each other will
    contain more of the same words. For each document in a corpus and for each word
    in the corpus, we look at whether or not that word occurs in that document. The
    more words that any two documents have in common, the more similar to each other
    they are. This is a very simple measurement, but it will give us a basic document
    similarity metric that we can use to illustrate visualizations of document similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in *Figure 6**.21* computes the BoW for the movie review corpus. You
    do not need to be concerned with the details of this code at this time, since
    it’s just a way of getting a similarity measure for the corpus. We will use this
    similarity metric (that is, BoW) to see how similar any two documents are to each
    other. The BoW metric has the advantage that it is easy to understand and compute.
    Although it is not the state-of-the-art way of measuring document similarity,
    it can be useful as a quick first step:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.21 – Setting up the B\uFEFFoW computation](img/B19005_06_21.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Setting up the BoW computation
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.21* shows the process of getting the most frequent 1,000 words
    from the corpus and making them into a list. The number of words we want to keep
    in the list is somewhat arbitrary – a very long list will slow down later processing
    and will also start to include rare words that don’t provide much information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next steps, shown in *Figure 6**.22*, are to define a function to collect
    the words in a document and then make a list of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Collecting the words that occur in a document](img/B19005_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Collecting the words that occur in a document
  prefs: []
  type: TYPE_NORMAL
- en: 'The `document_features()` function in *Figure 6**.22* iterates through the
    given document creating a Python dictionary with the words as keys and 1s and
    0s as the values, depending on whether or not the word occurred in the document.
    Then, we create a list of the features for each document and display the result
    in *Figure 6**.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.23 – Computing the full feature set for all documents and displaying\
    \ the resulting B\uFEFFoW](img/B19005_06_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Computing the full feature set for all documents and displaying
    the resulting BoW
  prefs: []
  type: TYPE_NORMAL
- en: Although the list of features for each document includes its category, we don’t
    need the category in order to display the BoW, so we remove it at lines 34-39.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the first 10 documents in the resulting BoW itself in *Figure 6**.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.24 – B\uFEFFoW for the movie review corpus](img/B19005_06_24.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – BoW for the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.24*, each of the 10 rows of 0s and 1s represents one document.
    There is one column for each word in the corpus. There are 1,000 columns, but
    this is too many to display, so we only see the first few and last few columns.
    The words are sorted in order of frequency, and we can see that the most frequent
    words (`film`, `one`, and `movie`) are the same words that we found to be the
    most frequent (except for stopwords) in our earlier explorations of word frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Each document is represented by a row in the BoW. This list of numbers in these
    rows is a **vector**, which is a very important concept in NLP, which we will
    be seeing much more of in later chapters. Vectors are used to represent text documents
    or other text-based information as numbers. Representing words as numbers opens
    up many opportunities for analyzing and comparing documents, which are difficult
    to do when the documents are in text form. Clearly, BoW loses a lot of information
    compared to the original text representation – we no longer can tell which words
    are even close to each other in the text, for example – but in many cases, the
    simplicity of using BoW outweighs the disadvantage of losing some of the information
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: One very interesting way that we can use vectors in general, including BoW vectors,
    is to try to capture document similarities, which can be a very helpful first
    step in exploring a dataset. If we can tell which documents are similar to which
    other documents, we can put them into categories based on their similarity. However,
    just looking at the document vectors in the rows of *Figure 6**.24* does not provide
    much insight, because it’s not easy to see any patterns. We need some tools for
    visualizing document similarity.
  prefs: []
  type: TYPE_NORMAL
- en: A good technique for visualizing document similarities is k-means clustering.
    `k` value refers to the number of clusters we want to have and is selected by
    the developer. In our case, we will start with `2` as the value of `k` since we
    have 2 known classes, corresponding to the sets of positive and negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.25* shows the code for computing and displaying the results of
    k-means clustering on the BoW we computed in *Figure 6**.23*. We do not have to
    go into detail about the code in *Figure 6**.25* because it will be covered in
    detail in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217). However, we can note
    that this code uses another important Python machine learning library, `sklearn`,
    which is used to compute the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Setting up for k-means clustering](img/B19005_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Setting up for k-means clustering
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to import the libraries we’ll need and set the number of clusters
    we want (`true_k`).
  prefs: []
  type: TYPE_NORMAL
- en: The next part of the code, shown in *Figure 6**.26*, computes the k-means clusters.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.26 – K-means clustering for visualizing document similarities based\
    \ on the B\uFEFFoW](img/B19005_06_26.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – K-means clustering for visualizing document similarities based
    on the BoW
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for computing the clusters as shown in *Figure 6**.26* are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the dimensions to `2` for display.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a `kmeans` object (line 13).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the result (line 18).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the labels from the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final step is plotting the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Plotting k-means clustering](img/B19005_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Plotting k-means clustering
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.27* plots the clusters by iterating through the clusters and printing
    the items in each cluster in the same color.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of using k-means clustering on the movie review BoW can be seen
    in *Figure 6**.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – K-means clustering for the movie corpus with two classes](img/B19005_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – K-means clustering for the movie corpus with two classes
  prefs: []
  type: TYPE_NORMAL
- en: There are two major clusters in *Figure 6**.28*, one above and one below the
    0 point on the *y* axis. The colors are defined based on the Matplotlib `Accent`
    color map at line 21\. (More information about the many color maps available in
    Matplotlib can be found at [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each dot in *Figure 6**.28* represents one document. Because the clusters are
    clearly separated, we can have some confidence that the similarity metric (BoW)
    reflects some real difference between the two classes of documents. However, that
    does not mean that the most insightful number of classes for this data is necessarily
    two. It is always worth checking out other numbers of classes; that is, other
    values of `k`. This can easily be done by changing the value of `true_k` at line
    7 in *Figure 6**.25*. For example, if we change the value of `true_k` to `3`,
    and thereby specify that the data should be divided into three classes, we’ll
    get a chart like the one in *Figure 6**.29*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.29\uFEFF – K-means clustering for the movie corpus with three classes](img/B19005_06_29.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – K-means clustering for the movie corpus with three classes
  prefs: []
  type: TYPE_NORMAL
- en: There are definitely three clear classes in *Figure 6**.29*, although they aren’t
    as nicely separated as the classes in *Figure 6**.28*. This could mean that the
    two classes of positive and negative reviews don’t tell the whole story. Perhaps
    there actually should be a third class of *neutral* reviews? We could investigate
    this by looking at the documents in the three clusters, although we won’t go through
    that exercise here.
  prefs: []
  type: TYPE_NORMAL
- en: By comparing the results in *Figures 6.28* and *6.29*, we can see that initial
    data exploration can be very useful in deciding how many classes in which to divide
    the dataset at the outset. We will take up this topic in much greater detail in
    [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217).
  prefs: []
  type: TYPE_NORMAL
- en: We have so far seen a number of ways of visualizing the information in a dataset,
    including word and bigram frequencies, as well as some introductory visualizations
    of document similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now consider some points about the overall process of visualization.
  prefs: []
  type: TYPE_NORMAL
- en: General considerations for developing visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stepping back a bit from the specific techniques we have reviewed so far, we
    will next discuss some general considerations about visualizations. Specifically,
    in the next sections, we will talk about what to measure, followed by how to represent
    these measurements and the relationships among the measurements. Because the most
    common visualizations are based on representing information in the *XY* plane
    in two dimensions, we will mainly focus on visualizations in this format, starting
    with selecting among measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting among measurements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nearly all NLP begins with measuring some property or properties of texts we
    are analyzing. The goal of this section is to help you understand the different
    kinds of text measurements that are available in NLP projects.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve primarily focused on measurements involving words. Words are a
    natural property to measure because they are easy to count accurately – in other
    words, counting words is a **robust** measurement. In addition, words intuitively
    represent a natural aspect of language. However, just looking at words can lead
    to missing important properties of the meanings of texts, such as those that depend
    on considering the order of words and their relationship to other words in the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: To try to capture richer information that does take into account the orders
    of words and their relationships, we can measure other properties of texts. We
    can, for example, count characters, syntactic constructions, parts of speech,
    ngrams (sequences of words), mentions of named entities, and word lemmas.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we could look at whether pronouns are more common in positive
    movie reviews than in negative movie reviews. However, the downside of looking
    at these richer properties is that, unlike counting words, measurements of richer
    properties are less robust. That means they are more likely to include errors
    that would make the measurements less accurate. For example, if we’re counting
    verbs by using the results of part of speech tagging, an incorrect verb tag that
    should be a noun tag would make the verb count incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, deciding what to measure is not always going to be a cut-and-dry
    decision. Some measurements, such as those that are based on words or characters,
    are very robust but exclude information that could be important. Other measurements,
    such as counting parts of speech, are less robust but more informative. Consequently,
    we can’t provide hard and fast rules for deciding what to measure. However, one
    rule of thumb is to start with the simplest and most robust approaches, such as
    counting words, to see whether your application is working well with those techniques.
    If it is, you can stick with the simplest approaches. If not, you can try using
    richer information. You should also keep in mind that you aren’t limited to only
    one measurement.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have selected what you want to measure, there are other general considerations
    that have to do with visualizing your measurements. In the next sections, we will
    talk about representing variables on the axes of the *XY* plane, different kinds
    of scales, and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Insights from representing independent and dependent variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most measurement involves measuring one `good` and we can tell that it’s negative
    if it contains `bad`. We can test this hypothesis by looking at the counts for
    `good` and `bad` (the dependent variable) for positive and negative reviews (the
    independent variable), as shown in *Figure 6**.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews](img/B19005_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews
  prefs: []
  type: TYPE_NORMAL
- en: The columns on the *x* axis represent positive reviews containing the word `good`,
    negative reviews containing the word `good`, positive reviews containing the word
    `bad`, and negative reviews containing the word `bad`, respectively. Clearly,
    the hypothesis that `good` signals a positive review and `bad` signals a negative
    review is wrong. In fact, `good` occurs more often than `bad` in negative reviews.
    This kind of exploration and visualization can help us rule out lines of investigation
    at the outset of our project that are unlikely to be fruitful. In general, bar
    charts such as the one in *Figure 6**.30* are a good way to represent categorical
    independent variables or data that occurs in distinct classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After looking at displaying the values of dependent and independent variables
    in graphs like *Figure 6**.30*, we can turn to another general consideration:
    linear versus log scales.'
  prefs: []
  type: TYPE_NORMAL
- en: Log scales and linear scales
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes you will see a pattern, as in *Figures 6.4* and *6.7*, where the
    first few items on the *x* axis have extremely high values and where the values
    of the other items drop off quickly. This makes it hard to see what’s going on
    in the part of the graph on the right where the items have lower values. If we
    see this pattern, it suggests that a **log scale** may be a better choice for
    the *y* axis than the **linear scale** in *Figure 6**.2*. We can see a log scale
    for the data from *Figure 6**.7* in *Figure 6**.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.31 – Log scale visualization for the data in \uFEFFFigure 6.4](img/B19005_06_31.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Log scale visualization for the data in Figure 6.4
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.31* shows a log scale plot of the data in *Figure 6**.7*. The *y-a*xis
    values are the counts of the 50 most common words in equally spaced powers of
    10, so each value on the *y* axis is 10 times greater than the previous one. Compared
    with *Figure 6**.7*, we can see that the line representing the counts of frequencies
    is much flatter, especially toward the right. In the log display, it is easier
    to compare the frequencies of these less common, but still very frequent, words.
    Because even the least frequent word in this chart, `she`, still occurs over 3,000
    times in the dataset, the line does not go below 10 3.'
  prefs: []
  type: TYPE_NORMAL
- en: You should keep the option of displaying your data in a log scale in mind if
    you see a pattern like the one in *Figure 6**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve been looking at data in two dimensions that can easily be displayed
    on paper or a flat screen. What happens if we have more than two dimensions? Let’s
    now consider higher dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions and dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The examples we’ve seen up to this point have been largely two-dimensional diagrams
    with *x* and *y* axes. However, many NLP techniques result in data that is of
    much higher dimensionality. While two dimensions are easy to plot and visualize,
    higher dimensional data can be harder to understand. Three-dimensional data can
    be displayed by adding a *z* axis and rotating the graph so that the *x* and *z*
    axes are at 45-degree angles on the display. A fourth or time dimension can be
    added by animating an on-screen graph and showing the changes over time. However,
    dimensions beyond four are really impossible for humans to visualize, so there
    is no way to display them in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: However, in many cases, it turns out that some of the higher dimensions in NLP
    results can be removed without seriously impacting either the visualization or
    the information produced by NLP. This is called **dimensionality reduction**.
    Dimensionality reduction techniques are used to remove the less important dimensions
    from the data so that it can be displayed more meaningfully. We will look at dimensionality
    reduction in more detail in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217).
  prefs: []
  type: TYPE_NORMAL
- en: Note that dimensionality reduction was done in *Figures 6.28* and *6.29* in
    order to display the results on a two-dimensional page.
  prefs: []
  type: TYPE_NORMAL
- en: The final topic in this chapter addresses some things we can learn from visualizations
    and includes some suggestions for using visualization to decide how to proceed
    with the next phases of an NLP project.
  prefs: []
  type: TYPE_NORMAL
- en: Using information from visualization to make decisions about processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section includes guidance about how visualization can help us make decisions
    about processing. For example, in making a decision about whether to remove punctuation
    and stopwords, exploring word frequency visualizations such as frequency distribution
    and word clouds can tell us whether very common words are obscuring patterns in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at frequency distributions of words for different categories of data
    can help rule out simple keyword-based classification techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Frequencies of different kinds of items, such as words and bigrams, can yield
    different insights. It can also be worth exploring the frequencies of other kinds
    of items, such as parts of speech or syntactic categories such as noun phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying document similarities with clustering can provide insight into the
    most meaningful number of classes that you would want to use in dividing a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The final section summarizes the information that we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about some techniques for the initial exploration
    of text datasets. We started out by exploring data by looking at the frequency
    distributions of words and bigrams. We then discussed different visualization
    approaches, including word clouds, bar graphs, line graphs, and clusters. In addition
    to visualizations based on words, we also learned about clustering techniques
    for visualizing similarities among documents. Finally, we concluded with some
    general considerations for developing visualizations and summarized what can be
    learned from visualizing text data in various ways. The next chapter will cover
    how to select approaches for analyzing NLU data and two kinds of representations
    for text data – symbolic representations and numerical representations.
  prefs: []
  type: TYPE_NORMAL
