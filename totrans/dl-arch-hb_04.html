<html><head></head><body>
<div id="_idContainer059">
<h1 class="chapter-number" id="_idParaDest-68"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.2.1">Understanding Recurrent Neural Networks</span></h1>
<p><span class="koboSpan" id="kobo.3.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">RNN</span></strong><span class="koboSpan" id="kobo.7.1">) is a neural network that is made to process sequential data while being aware of the sequence of the data. </span><span class="koboSpan" id="kobo.7.2">Sequential data can involve time </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.8.1">series based data and data that has a sequence but does not have a time component, such as text data. </span><span class="koboSpan" id="kobo.8.2">The applications of such a neural network are built upon the nature of the data itself. </span><span class="koboSpan" id="kobo.8.3">For time-series data, this can be either for nowcasting (predictions made for the current time with both past and present data) or forecasting targets. </span><span class="koboSpan" id="kobo.8.4">For text data, applications such as speech recognition and machine translation can utilize these </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">neural networks.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">Research in recurrent neural networks has slowed in the past few years with the advent of neural networks that can capture sequential data while removing recursive connections completely and achieving better performance, such as transformers. </span><span class="koboSpan" id="kobo.10.2">However, RNNs are still used extensively in the real world today to serve as a good baseline or just an alternative model for faster computations due to their lower number of computations and low memory requirements with reasonable </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">metric performance.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">The two most </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.13.1">prominent RNN layers are </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.16.1">LSTM</span></strong><span class="koboSpan" id="kobo.17.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">Gated Recurrent Units</span></strong><span class="koboSpan" id="kobo.19.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.20.1">GRU</span></strong><span class="koboSpan" id="kobo.21.1">). </span><span class="koboSpan" id="kobo.21.2">We will not be going through </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.22.1">the vanilla and the original recurrent neural networks in this book, but we will show LSTM and GRU as a refresher. </span><span class="koboSpan" id="kobo.22.2">The main operations of LSTM and GRU provide a mechanism to keep only relevant memory and ignore data that is not useful, which is a key inductive bias crafted for time series or </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">sequential data.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">In this chapter, we will dive deeper into these two RNN networks more extensively. </span><span class="koboSpan" id="kobo.24.2">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">following topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.26.1">Understanding LSTM</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.27.1">Understanding GRU</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Understanding advancements over the standard GRU and </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">LSTM layers</span></span></li>
</ul>
<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.30.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.31.1">This chapter is short and sweet but still covers some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">Python</span></strong><span class="koboSpan" id="kobo.33.1"> programming </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.34.1">language to realize the RNN architecture. </span><span class="koboSpan" id="kobo.34.2">To complete it, you will need to have a computer with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">Pytorch</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.36.1">library installed.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">at </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.41.1">Understanding LSTM</span></h1>
<p><span class="koboSpan" id="kobo.42.1">LSTM was invented in 1997 but remains a widely adopted neural network. </span><span class="koboSpan" id="kobo.42.2">LSTM uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.43.1">tanh</span></strong><span class="koboSpan" id="kobo.44.1"> activation function as it provides nonlinearities while providing second derivatives that </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.45.1">can be preserved for a longer sequence. </span><span class="koboSpan" id="kobo.45.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.46.1">tanh</span></strong><span class="koboSpan" id="kobo.47.1"> function helps to prevent exploding and vanishing gradients. </span><span class="koboSpan" id="kobo.47.2">An LSTM layer uses a sequence of LSTM cells sequentially connected. </span><span class="koboSpan" id="kobo.47.3">Let’s take an in-depth look at what the LSTM cell looks like in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.48.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.49.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.51.1"><img alt="Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM cells that forms an LSTM layer" src="image/B18187_04_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.52.1">Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM cells that forms an LSTM layer</span></p>
<p><span class="koboSpan" id="kobo.53.1">The first LSTM cell on the left depicts the high-level structure of an LSTM cell and the second LSTM cell on the left depicts the medium-level operations, connections, and structure of an LSTM cell, while the third cell on the right is just another LSTM cell to emphasize that LSTM layers are made of multiple LSTM cells sequentially connected to each other. </span><span class="koboSpan" id="kobo.53.2">Think of an LSTM cell as containing four gating mechanisms that provide a way to </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">forget</span></strong><span class="koboSpan" id="kobo.55.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">learn</span></strong><span class="koboSpan" id="kobo.57.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">remember</span></strong><span class="koboSpan" id="kobo.59.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">use</span></strong><span class="koboSpan" id="kobo.61.1"> the sequential data. </span><span class="koboSpan" id="kobo.61.2">One notable thing you might wonder about is why the sigmoid is shown as a separate process in three paths from the input and past the hidden state to the forget gate, the remember gate, and the use gate. </span><span class="koboSpan" id="kobo.61.3">The structure shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.63.1">.1</span></em><span class="koboSpan" id="kobo.64.1"> is a famous depiction of the LSTM cell but does not contain information about the weights of the connections. </span><span class="koboSpan" id="kobo.64.2">This is due to the fact that the </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.65.1">inputs go through a weighted addition process to combine the previous cell’s hidden state and the current sequence data with different sets of weights for each of the four connections. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.66.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.67.1">.2</span></em><span class="koboSpan" id="kobo.68.1"> shows the final low-level structure of a single LSTM cell with </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">weights considered:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.70.1"><img alt="Figure 4.2 – Low-level structure of the LSTM cell" src="image/B18187_04_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.71.1">Figure 4.2 – Low-level structure of the LSTM cell</span></p>
<p><span class="koboSpan" id="kobo.72.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.73.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.74.1">.2</span></em><span class="koboSpan" id="kobo.75.1">, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.76.1">W</span></span><span class="koboSpan" id="kobo.77.1"> and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.78.1">B</span></span><span class="koboSpan" id="kobo.79.1"> represent the weights and biases, respectively. </span><span class="koboSpan" id="kobo.79.2">The two small letters represent the data type and gate mechanisms, respectively. </span><span class="koboSpan" id="kobo.79.3">The data type is split into two – the hidden state represented by </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.80.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.81.1">and the input data represented by </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.82.1">i</span></span><span class="koboSpan" id="kobo.83.1">. </span><span class="koboSpan" id="kobo.83.2">The gate mechanisms involved are the forget mechanism represented by </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.84.1">F</span></span><span class="koboSpan" id="kobo.85.1">, the learning mechanism represented by </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.86.1">L</span></span><span class="koboSpan" id="kobo.87.1">, where there are two weights and biases associated with learning, and the use mechanism represented by </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.88.1">U</span></span><span class="koboSpan" id="kobo.89.1">. </span><span class="koboSpan" id="kobo.89.2">To properly perceive how many parameters an LSTM cell has we still have to decode the dimensions of the hidden state and input state weight vectors. </span><span class="koboSpan" id="kobo.89.3">For ease of reference, let’s take the input vector size as </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.90.1">n</span></span><span class="koboSpan" id="kobo.91.1"> and the hidden size </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">as </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.93.1">m</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.95.1">The hidden state weights have a </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">dimension of:</span></span></p>
<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.97.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.98.1">m</span></span></span></p>
<p><span class="koboSpan" id="kobo.99.1">While the input state weights have a </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">dimension of:</span></span></p>
<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.101.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.102.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.103.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.104.1">Bias, on the </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.105.1">other hand, is the size of the input vector size. </span><span class="koboSpan" id="kobo.105.2">For Tensorflow and Keras with Tensorflow, bias is only added once per mechanism. </span><span class="koboSpan" id="kobo.105.3">For PyTorch, the bias is added for each hidden state and input state weight. </span><span class="koboSpan" id="kobo.105.4">For PyTorch, the number of parameters for bias can be </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">defined as:</span></span></p>
<p><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.107.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.108.1">n</span></span></span></p>
<p><span class="koboSpan" id="kobo.109.1">As there are four mechanisms, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.111.1">.2</span></em><span class="koboSpan" id="kobo.112.1">, this means that the number of parameters for an LSTM in the PyTorch implementation can then be computed according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.116.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.118.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.120.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.121.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.122.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.123.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.124.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.126.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.127.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.131.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.132.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.133.1">4</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.134.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.135.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">m</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.137.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.138.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.139.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.140.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.141.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.142.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.143.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.144.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.145.1">Now that we understand where the actual parameters live in the cell, let’s dive into each of these </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">gating mechanisms.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.147.1">Decoding the forget mechanism of LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.148.1">The forget mechanism is accomplished by using a sigmoid activation function multiplied against </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.149.1">the previous cell state. </span><span class="koboSpan" id="kobo.149.2">The name of the gating mechanism implies that it determines the information to be removed based on a combination of the current input sequence and the previous cell output. </span><span class="koboSpan" id="kobo.149.3">A way to think of it is, on a scale of </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">0</span></strong><span class="koboSpan" id="kobo.151.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">1</span></strong><span class="koboSpan" id="kobo.153.1">, how relevant is the information from the past? </span><span class="koboSpan" id="kobo.153.2">The sigmoid mechanism forces the scale to be between </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">0</span></strong><span class="koboSpan" id="kobo.155.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">1</span></strong><span class="koboSpan" id="kobo.157.1">. </span><span class="koboSpan" id="kobo.157.2">Values closer to </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">0</span></strong><span class="koboSpan" id="kobo.159.1"> forget more of the previous cell state (long-term memory) and values closer to </span><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">1</span></strong><span class="koboSpan" id="kobo.161.1"> forget less of the previous cell </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">state memory.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.163.1">Decoding the learn mechanism of LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.164.1">The learn mechanism employs a combination of sigmoid activation of previous cell output and </span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">tanh</span></strong><span class="koboSpan" id="kobo.166.1"> activation of previous cell output added on the outputs of the forget gate </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.167.1">and multiplied by the outputs of the use gate. </span><span class="koboSpan" id="kobo.167.2">This mechanism is also known as the input gate. </span><span class="koboSpan" id="kobo.167.3">This mechanism allows information learning from the current input sequence. </span><span class="koboSpan" id="kobo.167.4">The information learned then gets passed into the remembering mechanism. </span><span class="koboSpan" id="kobo.167.5">Additionally, the information learned will also get passed into the mechanism that allows information usage for the next LSTM cell. </span><span class="koboSpan" id="kobo.167.6">Both of these mechanisms will be introduced </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">sequentially next.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.169.1">Decoding the remember mechanism of LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.170.1">The remember mechanism is achieved by simply adding up information from what’s left of the </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.171.1">forget process and what has been learned, which is the output of the learning gate. </span><span class="koboSpan" id="kobo.171.2">The output of this gate will then be considered the current cell state of the LSTM cell. </span><span class="koboSpan" id="kobo.171.3">The cell state contains what is known as the long-term memory of the LSTM sequence. </span><span class="koboSpan" id="kobo.171.4">Take this mechanism simply as an operation that allows the network to selectively choose which part of the input to maintain </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">and remember.</span></span></p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.173.1">Decoding the “information-using” mechanism of LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.174.1">The information-using mechanism is achieved by applying nonlinearities using the tanh activation </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.175.1">function on the current cell state and again using the current input sequence and previous cell output as a weighting mechanism to determine how much relevant information from the past and present should be used. </span><span class="koboSpan" id="kobo.175.2">The output of applying the use gate gives us the hidden state that will also be used as the previous cell output for the next </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">LSTM cell.</span></span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.177.1">Building a full LSTM network</span></h2>
<p><span class="koboSpan" id="kobo.178.1">Usually, to create </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.179.1">a full LSTM network, multiple LSTM layers are concatenated to each other using the sequence of hidden states from multiple LSTM cells as the subsequent sequence data to apply to the next LSTM layer. </span><span class="koboSpan" id="kobo.179.2">After a couple of LSTM layers, the hidden state sequence of the previous layer will usually then be passed into a fully connected layer to form the basis of supervised learning based simple LSTM architecture. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.181.1">.3</span></em><span class="koboSpan" id="kobo.182.1"> shows a visual structure of how this </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">is done:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.184.1"><img alt="Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully connected layer" src="image/B18187_04_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.185.1">Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully connected layer</span></p>
<p><span class="koboSpan" id="kobo.186.1">Based on the </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.187.1">network depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.188.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.189.1">.3</span></em><span class="koboSpan" id="kobo.190.1">, the implementation in PyTorch will look like </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.192.1">Let’s first import the PyTorch library’s handy </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">nn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.194.1"> module:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.195.1">
import torch.nn as nn</span></pre></li> <li><span class="koboSpan" id="kobo.196.1">Now, we will define the network architecture based on </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.197.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.198.1">.3</span></em><span class="koboSpan" id="kobo.199.1">, using the sequential API this time instead of the </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">class method:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.201.1">
RNN = nn.Sequential(
  nn.LSTM(
     input_size=10, hidden_size=20,
     num_layers=2, dropout=0,
  ),
  nn.Linear(in_features=10, out_features=10),
  nn.Softmax(),
)</span></pre></li> <li><span class="koboSpan" id="kobo.202.1">The input size, hidden sizes, and layer number of the LSTM, along with the output feature size of the linear layer, can be configured according to your input dataset and desire. </span><span class="koboSpan" id="kobo.202.2">Note that each timestep or sequential step of the input data can have a </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.203.1">size greater than one. </span><span class="koboSpan" id="kobo.203.2">This allows us to easily map original features into more representative feature embeddings and leverage their descriptive power. </span><span class="koboSpan" id="kobo.203.3">Additionally, the dropout regularizer can be added easily by setting the </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">dropout</span></strong><span class="koboSpan" id="kobo.205.1"> parameter to a value between 0 and 1, which will introduce the dropout layer at each layer except the last layer at the specified probability. </span><span class="koboSpan" id="kobo.205.2">The RNN defined in </span><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">pytorch</span></strong><span class="koboSpan" id="kobo.207.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.208.1">step 2</span></em><span class="koboSpan" id="kobo.209.1"> can now be trained as usual, like any PyTorch models defined in </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">a class.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.211.1">As usual, PyTorch has made building RNNs so much simpler and faster. </span><span class="koboSpan" id="kobo.211.2">Next, we will step into the next type of RNN, called </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">Gated </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.213.1">Recurrent Units</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.215.1">Understanding GRU</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.216.1">Gated recurrent </span></strong><strong class="bold"><a id="_idIndexMarker331"/></strong><strong class="bold"><span class="koboSpan" id="kobo.217.1">units</span></strong><span class="koboSpan" id="kobo.218.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.219.1">GRU</span></strong><span class="koboSpan" id="kobo.220.1">) was invented in 2014 and based on the ideas implemented in LSTM. </span><span class="koboSpan" id="kobo.220.2">GRU was made to simplify LSTM and provide a faster and more efficient way of achieving the same goals as LSTM to adaptively remember and forget based on past and present data. </span><span class="koboSpan" id="kobo.220.3">In terms of the learning capacity and metric performance achievable, there isn’t a clear silver-bullet winner among the two and often in the industry, the two RNN units are benchmarked against each other to figure out which method provides a better performance level. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.221.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.222.1">.4</span></em><span class="koboSpan" id="kobo.223.1"> shows the structure </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">of GRU.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.225.1"><img alt="Figure 4.4 – A low-level depiction of GRU" src="image/B18187_04_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.226.1">Figure 4.4 – A low-level depiction of GRU</span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.227.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.228.1">.4</span></em><span class="koboSpan" id="kobo.229.1"> adopts the same weights and bias notations as the LSTM depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.230.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.231.1">.2</span></em><span class="koboSpan" id="kobo.232.1">. </span><span class="koboSpan" id="kobo.232.2">There are three different names here for the final small letter notation. </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.233.1">R</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.234.1">being the reset gate, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.235.1">z</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.236.1">representing the update gate, and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.237.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.238.1">representing weights used to obtain the next hidden states. </span><span class="koboSpan" id="kobo.238.2">This means a GRU cell has fewer parameters than an LSTM cell, with three sets of weights and biases instead of four. </span><span class="koboSpan" id="kobo.238.3">This allows GRU networks to be slightly faster than </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">LSTM networks.</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">Although depicted </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.241.1">as a single cell, the same theory that required multiple LSTM cells to be sequentially connected also applies to GRU: a GRU network layer will have multiple GRU cells connected sequentially together. </span><span class="koboSpan" id="kobo.241.2">GRU contains only two </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.242.1">mechanisms, called the </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">reset gate</span></strong><span class="koboSpan" id="kobo.244.1"> and the </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">update gate</span></strong><span class="koboSpan" id="kobo.246.1">, and only has </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.247.1">one input from the previous GRU cell, and one output to the next GRU cell. </span><span class="koboSpan" id="kobo.247.2">The one input-output itself is obviously more efficient than LSTM, as we require fewer operations to be carried out. </span><span class="koboSpan" id="kobo.247.3">Now, let’s dive into these </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">two mechanisms.</span></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.249.1">Decoding the reset gate of GRU</span></h2>
<p><span class="koboSpan" id="kobo.250.1">The reset gate of GRU serves as a mechanism to forget the long-term information, also called the hidden state, of the previous cell. </span><span class="koboSpan" id="kobo.250.2">The goal of this mechanism is similar to the forget gate </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.251.1">in the LSTM cell. </span><span class="koboSpan" id="kobo.251.2">Similarly, this will exist on a scale of 0 to 1, based on the current input sequence and the previous cell state, which decides how much we should reduce and remove the previously gained </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">long-term information.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">However, the reset gates of GRU are functionally different from the forget gate of LSTM. </span><span class="koboSpan" id="kobo.253.2">While the forget gate of LSTM decides what information to forget from the long-term memory, the reset gate of GRU decides how much of the previous hidden state </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">to forget.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.255.1">Decoding the update gate of GRU</span></h2>
<p><span class="koboSpan" id="kobo.256.1">The update gate of GRU controls the amount of information from the long-term memory to </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.257.1">be transferred to the currently maintained memory. </span><span class="koboSpan" id="kobo.257.2">This is similar to the remember gate in LSTMs and helps the network to remember long-term information. </span><span class="koboSpan" id="kobo.257.3">Each weight associated with the previous cell’s hidden unit will learn to capture both short-term dependencies and long-term dependencies. </span><span class="koboSpan" id="kobo.257.4">The short-term dependencies usually have reset gates output values that are closer to </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">0</span></strong><span class="koboSpan" id="kobo.259.1"> to forget former information more frequently, and vice versa with weights and hidden state positions that learn </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">long-term dependencies.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">In terms of the difference from the LSTM remember gate, while the remember gate of LSTM decides what information to remember from the current input and previous hidden state, the update gate of GRU decides how much of the previous hidden state </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">to remember.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">GRU is a simple RNN to consider with more efficient operations compared to LSTMs. </span><span class="koboSpan" id="kobo.263.2">Now that we have decoded both LSTMs and GRU, instead of repeating another GRU-only full network similar to LSTM, let’s discover improvements that can be made using these two methods as </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">a base.</span></span></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.265.1">Understanding advancements over the standard GRU and LSTM layers</span></h1>
<p><span class="koboSpan" id="kobo.266.1">GRU and LSTM are the most widely used RNN methods today, but one might wonder how to push the boundaries achievable by a standard GRU or a standard LSTM. </span><span class="koboSpan" id="kobo.266.2">One good start </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.267.1">to building this intuition is to understand that both of the layer types are capable of accepting sequential data, and to build a network you </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.268.1">need multiple RNN layers. </span><span class="koboSpan" id="kobo.268.2">This means that it is entirely possible to combine GRU and LSTM layers in the same network. </span><span class="koboSpan" id="kobo.268.3">This, however, is not credible enough to be considered an advancement as a fully LSTM network or a fully GRU network can exceed the performance of a combined LSTM and GRU network at any time. </span><span class="koboSpan" id="kobo.268.4">Let’s dive into another simple improvement you can make on top of these </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.269.1">standard RNN layers, called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.270.1">bidirectional RNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">.</span></span></p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.272.1">Decoding bidirectional RNN</span></h2>
<p><span class="koboSpan" id="kobo.273.1">Both GRU and LSTM rely on the sequential nature of the data. </span><span class="koboSpan" id="kobo.273.2">This order of the sequence </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.274.1">can be forward in increasing time steps and also can be backward in decreasing time steps. </span><span class="koboSpan" id="kobo.274.2">Which direction to use usually comes down to an act of trial and error, and more often than not, the natural direction to use will be the forward </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">time order.</span></span></p>
<p><span class="koboSpan" id="kobo.276.1">In the year 1997, an improvement was made called bidirectional RNNs, which combine both forward-ordered RNNs with backward-ordered RNNs in an effort to maximize the input data that can be processed by an RNN model. </span><span class="koboSpan" id="kobo.276.2">The original idea was to estimate the value at a current timestep using both future information and historical information, given that both future and historical information was available with two of the RNNs taking in different sets of data. </span><span class="koboSpan" id="kobo.276.3">This naturally allowed for the capacity to achieve better prediction performance on such data setups. </span><span class="koboSpan" id="kobo.276.4">Today, this idea has extended to be a general layer applied to the same sequential data estimating and is also proven to provide prediction performance improvements. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.277.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.278.1">.5</span></em><span class="koboSpan" id="kobo.279.1"> shows an example of bidirectional RNNs using GRU where the hidden states from the forward and backward GRU </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">are concatenated:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.281.1"><img alt="Figure 4.5 – Bidirectional GRU" src="image/B18187_04_005.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.282.1">Figure 4.5 – Bidirectional GRU</span></p>
<p><span class="koboSpan" id="kobo.283.1">The concatenated </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.284.1">hidden states can then be passed into fully connected layers for standard supervised learning objectives. </span><span class="koboSpan" id="kobo.284.2">An example implementation in PyTorch of a bidirectional GRU </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">is shown:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.286.1">
RNN = nn.Sequential(
  nn.GRU(
     input_size=10, hidden_size=20,
     num_layers=2, bidirectional=True
  ),
  nn.Linear(in_features=10, out_features=10),
  nn.Softmax(),
)</span></pre> <p><span class="koboSpan" id="kobo.287.1">Next, let’s discover an improvement that is made based </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">on LSTMs.</span></span></p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.289.1">Adding peepholes to LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.290.1">Introduced in 2000, peepholes enable the cell states (from the previous and current cell), which hold </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.291.1">the long-term memory of LSTMs, to influence the sigmoid gating mechanisms in the LSTM cell. </span><span class="koboSpan" id="kobo.291.2">The intuition is that the long-term memory has information about the past time steps that are not available in the short-term memory held in the hidden state of the previous cell. </span><span class="koboSpan" id="kobo.291.3">This allowed for improved predictive performance when compared to a vanilla LSTM. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.292.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.293.1">.6</span></em><span class="koboSpan" id="kobo.294.1"> shows the extra peephole connections from the </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">cell state:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.296.1"><img alt="Figure 4.6 – LSTM peephole connections" src="image/B18187_04_006.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.297.1">Figure 4.6 – LSTM peephole connections</span></p>
<p><span class="koboSpan" id="kobo.298.1">However, one pitfall of this method is that the cell states can grow to large values over time due to the long-term memory nature of the states, as it is unbounded. </span><span class="koboSpan" id="kobo.298.2">This may saturate the gates to always be in an open state and render the gate useless sometimes. </span><span class="koboSpan" id="kobo.298.3">This brings us to the last improvement that we will discuss in the </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">next subsection.</span></span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.300.1">Adding working memory to exceed the peephole connection limitations for LSTM</span></h2>
<p><span class="koboSpan" id="kobo.301.1">In 2021, an improvement was made on top of peephole connections for LSTMs to enforce </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.302.1">a bound to the cell states by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">tanh</span></strong><span class="koboSpan" id="kobo.304.1"> activation. </span><span class="koboSpan" id="kobo.304.2">The simple addition proved itself to be better in performance than the LSTM with peepholes unbounded version in experimental benchmarks </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.305.1">and was called </span><strong class="bold"><span class="koboSpan" id="kobo.306.1">Working Memory Connections</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.307.1">for LSTM</span></strong><span class="koboSpan" id="kobo.308.1">. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.309.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.310.1">.7</span></em><span class="koboSpan" id="kobo.311.1"> shows the Working Memory Connection for </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">LSTM structure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.313.1"><img alt="Figure 4.7 – Working Memory Connection for LSTM structure" src="image/B18187_04_007.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.314.1">Figure 4.7 – Working Memory Connection for LSTM structure</span></p>
<p><span class="koboSpan" id="kobo.315.1">RNNs are similar to the standard MLP in the sense that there isn’t a single generic dataset that is used as a reference across different research initiatives. </span><span class="koboSpan" id="kobo.315.2">Even if the same dataset is used, the results might not be a definitive source of truth as, again, the dataset is not extensive enough to generalize across other datasets. </span><span class="koboSpan" id="kobo.315.3">In other words, there is no </span><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">ImageNet</span></strong><span class="koboSpan" id="kobo.317.1"> equivalent in sequence data. </span><span class="koboSpan" id="kobo.317.2">Text data, video data, and other time-series data are all wildly different from each other but essentially are all considered </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.318.1">sequence data and can be fed into RNNs. </span><span class="koboSpan" id="kobo.318.2">Take benchmark results from anywhere on RNNs and MLPs with a pinch of salt as results can vary widely from dataset to dataset. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.319.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.320.1">.8</span></em><span class="koboSpan" id="kobo.321.1">, however, shows one </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.322.1">version of benchmarks done with GRU, LSTM, </span><strong class="bold"><span class="koboSpan" id="kobo.323.1">LSTM peepholes</span></strong><span class="koboSpan" id="kobo.324.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.325.1">LSTM-PH</span></strong><span class="koboSpan" id="kobo.326.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.327.1">LSTM Working Memory</span></strong><span class="koboSpan" id="kobo.328.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.329.1">LSTM-WM</span></strong><span class="koboSpan" id="kobo.330.1">) on two different </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.331.1">model settings out of RNNs, done on an image captioning task using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.332.1">COCO</span></strong><span class="koboSpan" id="kobo.333.1"> dataset using a metric that considers the naturalism of the produced text – the higher </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">the better.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.335.1"><img alt="Figure 4.8 – RNN benchmark on image captioning task on COCO dataset" src="image/B18187_04_008.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 4.8 – RNN benchmark on image captioning task on COCO dataset</span></p>
<p><span class="koboSpan" id="kobo.337.1">The figure shows that LSTM-WM dominates over the other methods in a single experiment </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.338.1">across different evaluation scores. </span><span class="koboSpan" id="kobo.338.2">Again, take the results with a pinch of salt as the COCO dataset is by no means a representative dataset of sequential or </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">time-series data.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">Try it out on your own dataset to know for sure! </span><span class="koboSpan" id="kobo.340.2">With that, we have gone through important concepts of RNN, from basic to advanced levels. </span><span class="koboSpan" id="kobo.340.3">Let’s summarize the </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">chapter next.</span></span></p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.342.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.343.1">Recurrent neural networks are a type of neural network that explicitly includes inductive biases of sequential data in </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">its structure.</span></span></p>
<p><span class="koboSpan" id="kobo.345.1">A couple of variations of RNNs exist but all of them maintain the same high-level concept for their overall structure. </span><span class="koboSpan" id="kobo.345.2">Mainly, they provide varying ways to decide which data to learn from and remember along with which data to forget from the memory from the </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">remembering stage.</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">However, do note that a more recent architecture called transformers, which will be introduced in </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.348.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.349.1">, </span><em class="italic"><span class="koboSpan" id="kobo.350.1">Understanding Neural Network Transformers</span></em><span class="koboSpan" id="kobo.351.1">, demonstrated that recurrence is not needed to achieve a good performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">sequential data.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">With that, we are done with RNNs and will dive briefly into the world of autoencoders in the </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">next chapter.</span></span></p>
</div>
</body></html>