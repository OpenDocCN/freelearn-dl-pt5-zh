<html><head></head><body>
		<div id="_idContainer096">
			<p> </p>
			<h1 id="_idParaDest-152" class="chapter-number"><a id="_idTextAnchor173"/>9</h1>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor174"/>Machine Learning Part 1 –  Statistical Machine Learning</h1>
			<p>In this chapter, we will discuss how to apply classical statistical machine learning techniques <a id="_idIndexMarker701"/>such as <strong class="bold">Naïve Bayes</strong>, <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>), <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), and <strong class="bold">conditional random fields</strong> (<strong class="bold">CRFs</strong>) to <a id="_idIndexMarker702"/>common <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks <a id="_idIndexMarker703"/>such as <a id="_idIndexMarker704"/>classification (or intent recognition) and <a id="_idIndexMarker705"/><span class="No-Break">slot filling.</span></p>
			<p>There are two aspects of these classical techniques that we need to consider: representations and models. <strong class="bold">Representation</strong> refers to the format of the data that we are going to analyze. You <a id="_idIndexMarker706"/>will recall from <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, that it is standard to represent NLP data in formats other than lists of words. Numeric data representation formats such as vectors make it possible to use widely available numeric processing techniques, and consequently open up many possibilities for processing. In <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we also explored data representations such as the <strong class="bold">count bag of words</strong>(<strong class="bold">BoW</strong>), TF-IDF, and Word2Vec. We will primarily be using TF-IDF in <span class="No-Break">this chapter.</span></p>
			<p>Once the data is in a format that is ready for further processing, that is, once it has been <em class="italic">vectorized</em>, we can use it to train, or build, a model that can be used to analyze similar data that the system may encounter in the future. This is the <strong class="bold">training</strong> phase. The future data can be test data; that is, previously unseen data similar to the training data that is used to evaluate the model. In addition, if the model is being used in a practical application, the future data could be new examples of queries from users or customers addressed to the runtime system. When the system is used to process data after the training phase, this is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">inference</strong></span><span class="No-Break">.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A quick overview <span class="No-Break">of evaluation</span></li>
				<li>Representing documents with TF-IDF and classifying with <span class="No-Break">naïve Bayes</span></li>
				<li>Classifying documents <span class="No-Break">with SVMs</span></li>
				<li>Slot filling with conditional <span class="No-Break">random fields</span></li>
			</ul>
			<p>We will start this chapter with a very practical and basic set of techniques that should be in everyone’s toolbox, and that frequently end up being practical solutions to <span class="No-Break">classification problems.</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor175"/>A quick overview of evaluation</h1>
			<p>Before we look at how different statistical techniques work, we have to have a way to measure their <a id="_idIndexMarker707"/>performance, and there are a couple of important considerations that <a id="_idIndexMarker708"/>we should review first. The first consideration is the <em class="italic">metric</em> or score that we assign to the system’s processing. The most common and simple metric is <strong class="bold">accuracy</strong>, which is <a id="_idIndexMarker709"/>the number of correct responses divided by the overall number of attempts. For example, if we’re attempting to measure the performance of a movie review classifier, and we attempt to classify 100 reviews as positive or negative, if the system classifies 75 reviews correctly, the accuracy is 75%. A closely related metric is <strong class="bold">error rate</strong>, which is, in a sense, the opposite of accuracy because it measures <a id="_idIndexMarker710"/>how often the system made a mistake. In this example, the error rate would <span class="No-Break">be 25%.</span></p>
			<p>We will only make use of accuracy in this chapter, although there are more precise and informative metrics <a id="_idIndexMarker711"/>that are actually more commonly used, for example, <strong class="bold">precision</strong>, <strong class="bold">recall</strong>, <strong class="bold">F1</strong>, and <strong class="bold">area under the curve (AUC)</strong>. We will discuss these in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>. For the purposes of this chapter, we just need a basic metric so that we can compare results, and accuracy will be adequate <span class="No-Break">for that.</span></p>
			<p>The second important consideration that we need to keep in mind in evaluation is how to treat the data that we will be using for evaluation. Machine learning approaches are trained and evaluated with a standard approach that involves splitting the dataset into <em class="italic">training</em>, <em class="italic">development</em>, also often called <em class="italic">validation</em> data, and <em class="italic">testing</em> subsets. The training set is the data that is used to build the model, and is typically about 60-80 percent of the available data, although the exact percentage is not critical. Typically, you would want to use as much data as possible for training, while reserving a reasonable amount of data for evaluation purposes. Once the model is built, it can be tested with development data, normally about 10-20 percent of the overall dataset. Problems with the training algorithm are generally uncovered by trying to use the model on the development set. A final evaluation is done once, on the remaining data – the test data, which is usually about 10 percent of the <span class="No-Break">total data.</span></p>
			<p>Again, the exact breakdown of training, development, and test data is not critical. Your goal is to use the training data to build a good model that will enable your system to accurately predict the interpretation of new, previously unseen data. To meet that goal, you need as much <a id="_idIndexMarker712"/>training data as possible. The goal of the test data is to get an accurate measure of how your model performs on new data. To meet that goal, you need as much test data as possible. So, the data split will always involve a trade-off between <span class="No-Break">these goals.</span></p>
			<p>It is very important to keep the training data separate from the development data, and especially from the test data. Performance on the training data is not a good indicator of how the system will really perform on new, unseen data, so performance on the training data is not used <span class="No-Break">for evaluation.</span></p>
			<p>Following this brief introduction to evaluation, We will now move on to the main topics of this chapter. We will cover some of the most well-established machine learning approaches for important NLP applications such as classification and <span class="No-Break">slot filling.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor176"/>Representing documents with TF-IDF and classifying with Naïve Bayes</h1>
			<p>In addition to evaluation, two important topics in the general paradigm of machine learning are <a id="_idIndexMarker713"/>representation and processing algorithms. Representation <a id="_idIndexMarker714"/>involves converting a text, such <a id="_idIndexMarker715"/>as a document, into a numerical format that preserves relevant <a id="_idIndexMarker716"/>information about the text. This information is then analyzed by the processing algorithm to perform the NLP application. You’ve already seen a common approach to representation, TF-IDF, in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. In this section, we will cover using TF-IDF with a common classification approach, Naïve Bayes. We will explain both techniques and show <span class="No-Break">an example.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor177"/>Summary of TF-IDF</h2>
			<p>You will recall the discussion of TF-IDF from <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. TF-IDF is based on the intuitive goal of trying to find <a id="_idIndexMarker717"/>words in documents that are particularly diagnostic of their classification topic. Words that are relatively infrequent in the whole corpus, but which are relatively common in a specific document, seem to be helpful in finding the document’s class. TF-IDF was defined in the equations presented in the <em class="italic">term frequency-inverse document frequency (TF-IDF)</em> section in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. In addition, we saw partial TF-IDF vectors for some of the documents in the movie review corpus in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em>. Here, we will take the TF-IDF vectors and use them to classify documents using the Naïve Bayes <span class="No-Break">classification approach.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor178"/>Classifying texts with Naïve Bayes</h2>
			<p>Bayesian classification techniques have been used for many years, and, despite their long history, are still very common <a id="_idIndexMarker718"/>and widely used. Bayesian classification is simple and fast, and <a id="_idIndexMarker719"/>can lead to acceptable results in <span class="No-Break">many applications.</span></p>
			<p>The formula for Bayesian classification is shown in the following equation. For each category in the set of possible categories, and for each document, we want to compute the probability that that is the correct category for that document. This computation is based on some representation of the document; in our case, the representation will be a vector like one of the ones we’ve previously discussed – BoW, TF-IDF, <span class="No-Break">or Word2Vec.</span></p>
			<p>To compute this probability, we take into account the probability of the vector, given a category, multiplied by the probability of the category, and divided by the probability of the document vector, as shown in the <span class="No-Break">following equation:</span></p>
			<p>                         <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∣</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∣</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">   </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">____________________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>The training process will determine the probabilities of the document vectors in each category and the overall probabilities of <span class="No-Break">the categories.</span></p>
			<p>The formula is called <em class="italic">naïve</em> because it makes the assumption that the features in the vectors are independent. This is clearly not correct for text because the words in sentences are not <a id="_idIndexMarker720"/>at all independent. However, this assumption <a id="_idIndexMarker721"/>makes the processing much simpler, and in practice does not usually make a significant difference in <span class="No-Break">the results.</span></p>
			<p>There are both binary and multi-class versions of Bayesian classification. We will work with binary classification with the movie review corpus since we have only two categories <span class="No-Break">of reviews.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor179"/>TF-IDF/Bayes classification example</h2>
			<p>Using TF-IDF and <a id="_idIndexMarker722"/>Naïve Bayes to classify the movie reviews, we can start by reading the reviews and splitting the data into training and test sets, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import sklearn
import os
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from sklearn.datasets import load_files
path = './movie_reviews/'
# we will consider only the most 1000 common words
max_tokens = 1000
# load files -- there are 2000 files
movie_reviews = load_files(path)
# the names of the categories (the labels) are automatically generated from the names of the folders in path
# 'pos' and 'neg'
labels = movie_reviews.target_names
# Split data into training and test sets
# since this is just an example, we will omit the dev test set
# 'movie_reviews.data' is the movie reviews
# 'movie_reviews.target' is the categories assigned to each review
# 'test_size = .20' is the proportion of the data that should be reserved for testing
# 'random_state = 42' is an integer that controls the randomization of the
# data so that the results are reproducible
from sklearn.model_selection import train_test_split
movies_train, movies_test, sentiment_train, sentiment_test = train_test _split(movie_reviews.data,                                                                          movie_reviews.target,                                                                                                                                                    test_size  = 0.20,                                                                                                                                                      random_state = 42)</pre>
			<p>Once we have our training and test data, the next step is creating TF-IDF vectors from the reviews, as <a id="_idIndexMarker723"/>shown in the following code snippet. We will primarily be using the scikit-learn library, although we’ll use NLTK <span class="No-Break">for tokenization:</span></p>
			<pre class="source-code">
# initialize TfidfVectorizer to create the tfIdf representation of the corpus
# the parameters are: min_df -- the percentage of documents that the word has
# to occur in to be considered, the tokenizer to use, and the maximum
# number of words to consider (max_features)
vectorizer = TfidfVectorizer(min_df = .1,
                             tokenizer = nltk.word_tokenize,
                             max_features = max_tokens)
# fit and transform the text into tfidf format, using training text
# here is where we build the tfidf representation of the training data
movies_train_tfidf = vectorizer.fit_transform(movies_train)</pre>
			<p>The main steps in the preceding code are creating the vectorizer and then using the vectorizer to convert the movie reviews to TF-IDF format. This is the same process that we followed in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. The resulting TF-IDF vectors were shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em>, so we won’t repeat <span class="No-Break">that here.</span></p>
			<p>Classifying the <a id="_idIndexMarker724"/>documents into positive and negative reviews is then done with the multinomial Naïve Bayes function from scikit-learn, which is one of scikit-learn’s Naïve Bayes packages, suitable for working with TF-IDF vector data. You <a id="_idIndexMarker725"/>can take a look at scikit-learn’s other Naïve Bayes packages at <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes">https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes</a> for <span class="No-Break">more information.</span></p>
			<p>Now that we have the TF-IDF vectors, we can initialize the naïve Bayes classifier and train it on the training data, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
from sklearn.naive_bayes import MultinomialNB
# Initialize the classifier and train it
classifier = MultinomialNB()
classifier.fit(movies_train_tfidf, sentiment_train)</pre>
			<p>Finally, we can compute the accuracy of the classifier by vectorizing the test set (<strong class="source-inline">movies_test_tfidf</strong>), and using the classifier that was created from the training data to predict the <a id="_idIndexMarker726"/>classes of the test data, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# find accuracy based on test set
movies_test_tfidf = vectorizer.fit_transform(movies_test)
# for each document in the test data, use the classifier to predict whether its sentiment is positive or negative
sentiment_pred = classifier.predict(movies_test_tfidf)
sklearn.metrics.accuracy_score(sentiment_test,
    sentiment_pred)
0.64
# View the results as a confusion matrix
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(sentiment_test,
    sentiment_pred,normalize=None)
print(conf_matrix)
[[132  58]
 [ 86 124]]</pre>
			<p>We can see from the preceding code that the accuracy of the classifier is <strong class="source-inline">0.64</strong>. That is, 64% of the test data reviews were assigned the correct category (positive or negative). We can also get some more information about how well the classification worked by looking at the <em class="italic">confusion matrix</em>, which is shown in the last two lines of <span class="No-Break">the code.</span></p>
			<p>In general, a confusion matrix shows which categories were confused with which other categories. We have a total of <strong class="source-inline">400</strong> test items (20% of the 2,000 reviews that were reserved as test examples). <strong class="source-inline">132</strong> of the <strong class="source-inline">190</strong> negative reviews were correctly classified as negative, and <strong class="source-inline">58</strong> were incorrectly classified as positive. Similarly, <strong class="source-inline">124</strong> of the <strong class="source-inline">210</strong> positive reviews were correctly classified as positive, but <strong class="source-inline">86</strong> were misclassified as negative. That means 69% of the negative reviews were correctly classified, and 59% of the positive reviews <a id="_idIndexMarker727"/>were correctly classified. From this, we can see that our model does slightly better in classifying negative reviews as negative. The reasons for this difference are not clear. To understand this result better, we can look more carefully at the reviews that were misclassified. We won’t do this right now, but we will look at analyzing results more carefully in <a href="B19005_14.xhtml#_idTextAnchor248"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
			<p>We will now consider a more modern and generally more accurate approach <span class="No-Break">to classification.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor180"/>Classifying documents with Support Vector Machines (SVMs)</h1>
			<p>SVMs are a popular and robust tool for text classification in applications such as intent recognition and <a id="_idIndexMarker728"/>chatbots. Unlike neural networks, which we will <a id="_idIndexMarker729"/>discuss in the next chapter, the training process is usually relatively quick and normally doesn’t require enormous amounts of data. That means that SVMs are good for applications that have to be quickly deployed, perhaps as a preliminary step in the development of a <span class="No-Break">larger-scale application.</span></p>
			<p>The basic idea behind SVMs is that if we represent documents as <strong class="bold">n</strong>-dimensional vectors (for example, the TF-IDF vectors that we discussed in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we want to be able to identify a hyperplane that provides a boundary that separates the documents into two categories with as large a boundary (or <em class="italic">margin</em>) <span class="No-Break">as possible.</span></p>
			<p>An illustration <a id="_idIndexMarker730"/>of using SVMs on the movie review <a id="_idIndexMarker731"/>data is shown here. We start, as usual, by importing the data and creating a <span class="No-Break">train/test split:</span></p>
			<pre class="source-code">
import numpy as np
from sklearn.datasets import load_files
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
# the directory root will be wherever the movie review data is located
directory_root = "./lab/movie_reviews/"
movie_reviews = load_files(directory_root,
    encoding='utf-8',decode_error="replace")
# count the number of reviews in each category
labels, counts = np.unique(movie_reviews.target,
    return_counts=True)
# convert review_data.target_names to np array
labels_str = np.array(movie_reviews.target_names)[labels]
print(dict(zip(labels_str, counts)))
{'neg': 1000, 'pos': 1000}
from sklearn.model_selection import train_test_split
movies_train, movies_test, sentiment_train, sentiment_test
    = train_test_split(movie_reviews.data,
        movie_reviews.target, test_size = 0.20,
        random_state = 42)</pre>
			<p>Now that we <a id="_idIndexMarker732"/>have the data set up and we have generated <a id="_idIndexMarker733"/>the train/test split, we will create the TF-IDF vectors and perform the SVC classification in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# We will work with a TF_IDF representation, as before
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
# Use the Pipeline function to construct a list of two processes
# to run, one after the other -- the vectorizer and the classifier
svc_tfidf = Pipeline([
        ("tfidf_vectorizer", TfidfVectorizer(
        stop_words = "english", max_features=1000)),
        ("linear svc", SVC(kernel="linear"))
    ])
model = svc_tfidf
model.fit(movies_train, sentiment_train)
sentiment_pred = model.predict(movies_test)
accuracy_result = accuracy_score( sentiment_test,
    sentiment_pred)
print(accuracy_result)
0.8125
# View the results as a confusion matrix
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(sentiment_test,
    sentiment_pred,normalize=None)
print(conf_matrix)
[[153  37]
 [ 38 172]]</pre>
			<p>The process shown here is very similar to the Naïve Bayes classification example shown in the code snippets in the previous section. However, in this case, we use an SVM instead of Naïve Bayes for classification, although we still use TF-IDF to vectorize the data. In the preceding code, we can see that the accuracy result for the classification is <strong class="source-inline">0.82</strong>, which is considerably better than the Bayes accuracy shown in the <span class="No-Break">previous section.</span></p>
			<p>The resulting confusion matrix is also better, in that <strong class="source-inline">153</strong> of the <strong class="source-inline">190</strong> negative reviews were correctly classified as negative, and <strong class="source-inline">37</strong> were incorrectly classified as positive. Similarly, <strong class="source-inline">172</strong> of the <strong class="source-inline">210</strong> positive reviews were correctly classified as positive, but <strong class="source-inline">38</strong> were misclassified as negative. That means 80% of the negative reviews were correctly classified, and 81% of the positive reviews were <span class="No-Break">correctly classified.</span></p>
			<p>SVMs were originally <a id="_idIndexMarker734"/>designed for binary classification, as we <a id="_idIndexMarker735"/>have just seen with the movie review data, where we only have two categories (positive and negative, in this case). However, they can be extended to multi-class problems (which includes, most cases of intent recognition) by splitting the problem into multiple <span class="No-Break">binary problems.</span></p>
			<p>Consider a multi-class problem that might occur with a generic personal assistant application. Suppose the application includes several intents, <span class="No-Break">for example:</span></p>
			<ul>
				<li>Find out <span class="No-Break">the weather</span></li>
				<li><span class="No-Break">Play music</span></li>
				<li>Read the <span class="No-Break">latest headlines</span></li>
				<li>Tell me the latest sports scores for my <span class="No-Break">favorite teams</span></li>
				<li>Find a nearby restaurant offering a <span class="No-Break">particular cuisine</span></li>
			</ul>
			<p>The application needs to classify the user’s query into one of these intents in order to process it and answer the user’s question. To use SVMs for this, it is necessary to recast the problem as a set of binary problems. There are two ways to <span class="No-Break">do this.</span></p>
			<p>One is to create multiple models, one for each pair of classes, and split the data so that each class is compared to every other class. With the personal assistant example, the classification would need to decide about questions such as “<em class="italic">Is the category weather or sports?</em>” and <em class="italic">Is the category weather or news?</em>. This is the <em class="italic">one versus one</em> approach, and you can see that this could result in a very large number of classifications if the number of intents <span class="No-Break">is large.</span></p>
			<p>The other approach is called <em class="italic">one versus rest</em> or <em class="italic">one versus all</em>. Here, the idea is to ask questions such as <em class="italic">Is the category </em>“<em class="italic">weather</em>”<em class="italic"> or is it something else?</em> This is the more popular approach, which we will <span class="No-Break">show here.</span></p>
			<p>The way to use the <a id="_idIndexMarker736"/>multiclass SVM from scikit-learn is very similar <a id="_idIndexMarker737"/>to what was shown previously. The difference is that we’re importing the <strong class="source-inline">OneVsRestClassifier</strong> and using it to create the classification model, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from sklearn.multiclass import OneVsRestClassifier
model = OneVsRestClassifier(SVC())</pre>
			<p><strong class="bold">Classification</strong> is widely used in many natural language applications, both for categorizing documents <a id="_idIndexMarker738"/>such as movie reviews and for categorizing what a user’s overall goal (or <em class="italic">intent</em>) in asking their question is in applications such as chatbots. However, often the application will require more fine-grained information from the utterance or document <a id="_idIndexMarker739"/>in addition to its overall classification. This process is often called <strong class="bold">slot filling</strong>. We discussed slot-filling in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> and showed how to write slot-filling rules. In the following section, we will show another approach to slot-filling, based on statistical techniques, specifically <strong class="bold">conditional random </strong><span class="No-Break"><strong class="bold">fields</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CRFs</strong></span><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor181"/>Slot-filling with CRFs</h1>
			<p>In <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, we discussed the popular application of slot-filling, and we <a id="_idIndexMarker740"/>used the spaCy rule engine to find slots for the restaurant search application shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em>. This required writing rules for finding the fillers of each slot in the application. This approach can work fairly well if the potential slot fillers are known in advance, but if they aren’t known in advance, it won’t be possible to write rules. For example, with the rules in the code following <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em>, if a user asked for a new cuisine, say, <em class="italic">Thai</em>, the rules wouldn’t be able to recognize <em class="italic">Thai</em> as a new filler for the <strong class="source-inline">CUISINE</strong> slot, and wouldn’t be able to recognize <em class="italic">not too far away</em> as a filler for the <strong class="source-inline">LOCATION</strong> slot. Statistical methods, which we will discuss in this section, can help with <span class="No-Break">this problem.</span></p>
			<p>With <strong class="bold">statistical methods</strong>, the system does not use rules but looks for patterns in its training data that can be <a id="_idIndexMarker741"/>applied to new examples. Statistical methods depend on having enough training data for the system to be able to learn accurate patterns, but if there is enough training data, statistical methods will generally provide a more robust solution to an NLP problem than <span class="No-Break">rule-based methods.</span></p>
			<p>In this section, we will look at a popular approach that can be applied to statistical slot filling – CRF. CRFs are a way <a id="_idIndexMarker742"/>of taking the context of textual items into account when trying to find the label for a span of text. Recall that the rules we discussed in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> did not look at any nearby words or other context – they <a id="_idIndexMarker743"/>only looked at the item itself. In contrast, CRFs attempt to model the probability of a label for a particular section of text, that is, given an input <em class="italic">x</em>, they model the probability of that input being an example category <em class="italic">y (P(y|x))</em>. CRFs use the word (or token) sequence to estimate the conditional probabilities of slot labels in that context. We will not review the mathematics of CRF here, but you can find many detailed descriptions of the underlying mathematics on the web, for <span class="No-Break">example, </span><a href="https://arxiv.org/abs/1011.4088"><span class="No-Break">https://arxiv.org/abs/1011.4088</span></a><span class="No-Break">.</span></p>
			<p>To train a system for slot-tagging, the data has to be annotated so that the system can tell what slots it’s looking for. There are at least four different formats used in the NLP technical literature for representing the annotations for slot-tagged data, and we’ll briefly illustrate these. These formats can be used both for training data and for representing processed NLP results, which can, in turn, be used for further processing stages such as retrieving information from <span class="No-Break">a database.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor182"/>Representing slot-tagged data</h2>
			<p>Training data <a id="_idIndexMarker744"/>for slot-filling applications can be found in several formats. Let’s look at how the sentence <strong class="source-inline">show me science fiction films directed by steven spielberg</strong>, a query from the MIT movie query corpus (<a href="https://groups.csail.mit.edu/sls/downloads/">https://groups.csail.mit.edu/sls/downloads/</a>), can be represented in four <span class="No-Break">different formats.</span></p>
			<p>One commonly <a id="_idIndexMarker745"/>used notation is <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
{tokens": "show me science fiction films directed by steven spielberg"
"entities": [
  {"entity": {
    "tokens": "science fiction films",
    "name": "GENRE"
    }},
    {
    "entity": {
     "tokens": "steven spielberg",
     "name": "DIRECTOR"
     }}
     ]
     }</pre>
			<p>Here, we see that the input sentence is shown as <strong class="source-inline">tokens</strong>, which is followed by a list of slots (called <strong class="source-inline">entities</strong> here). Each entity is associated with a name, such as <strong class="source-inline">GENRE</strong> or <strong class="source-inline">DIRECTOR</strong> and the <a id="_idIndexMarker746"/>tokens that it applies to. The example shows two slots, <strong class="source-inline">GENRE</strong> and <strong class="source-inline">DIRECTOR</strong>, which are filled by <strong class="source-inline">science fiction films</strong> and <strong class="source-inline">steven </strong><span class="No-Break"><strong class="source-inline">spielberg</strong></span><span class="No-Break">, respectively:</span></p>
			<p>The second <a id="_idIndexMarker747"/>format uses <strong class="bold">Extensible Markup Language</strong> (<strong class="bold">XML</strong>) tags to label the slots, as in <strong class="source-inline">show me &lt;GENRE&gt;science fiction films&lt;/GENRE&gt; directed by &lt;</strong><span class="No-Break"><strong class="source-inline">DIRECTOR&gt;steven Spielberg&lt;/DIRECTOR&gt;</strong></span><span class="No-Break">.</span></p>
			<p>The third format is called <strong class="bold">Beginning Inside Outside</strong> (<strong class="bold">BIO</strong>), which is a textual format that labels the <a id="_idIndexMarker748"/>beginning, middle, and end of each slot filler in a sentence, as in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B19005_09_01.jpg" alt="Figure 9.1 – BIO tagging for “show me science fiction films directed by steven spielberg”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – BIO tagging for “show me science fiction films directed by steven spielberg”</p>
			<p>In the BIO format, words outside of any slot are labeled <strong class="source-inline">O</strong> (that is <strong class="source-inline">Outside</strong>), the beginning of a slot (<strong class="source-inline">science</strong> and <strong class="source-inline">steven</strong> are labeled <strong class="source-inline">B</strong>, and the inside of a slot is <span class="No-Break">labeled </span><span class="No-Break"><strong class="source-inline">I</strong></span><span class="No-Break">).</span></p>
			<p>Finally, another very simple format <a id="_idIndexMarker749"/>for representing tagged slots is <strong class="bold">Markdown</strong>, a simplified way of <a id="_idIndexMarker750"/>marking up text for rendering. We’ve seen Markdown before, in Jupyter notebooks, as a way to display comment blocks. In <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, we can see an example of some training data for a restaurant search application similar to the one we looked at in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> (which was shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em>). Slot values are shown in square brackets, and the slot names are shown <span class="No-Break">in parentheses:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B19005_09_02.jpg" alt="Figure 9.2 – Markdown tagging for a restaurant search application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Markdown tagging for a restaurant search application</p>
			<p>All four formats show <a id="_idIndexMarker751"/>basically the same information about the slots and values; the information is just represented in slightly different ways. For your own projects, if you are using a public dataset, it would probably be most convenient to use the format that the dataset already uses. However, if you are using your own data, you can choose whatever format is most appropriate or easiest to use for your application. All other considerations being equal, the XML and JSON formats are generally more flexible than BIO or markdown, since they can represent nested slots, that is, slots that contain additional values as <span class="No-Break">slot fillers.</span></p>
			<p>For our example, we will be using the spaCy CRF suite library, located at <a href="https://github.com/talmago/spacy_crfsuite">https://github.com/talmago/spacy_crfsuite</a>, and we will use restaurant search as an example application. This dataset is annotated using the <span class="No-Break">Markdown format.</span></p>
			<p>The following code sets up the application by importing display and Markdown functionality and then reading in the Markdown file from the <strong class="source-inline">examples</strong> directory. Reading in the <a id="_idIndexMarker752"/>Markdown file will reproduce the list of utterances shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>. Note that the training data in the Markdown file is not large enough for a real application, but it works as an <span class="No-Break">example here:</span></p>
			<pre class="source-code">
from IPython.display import display, Markdown
with open("examples/restaurant_search.md", "r") as f:
    display(Markdown(f.read()))</pre>
			<p>The next steps, shown below, will be to import <strong class="source-inline">crfsuite</strong> and <strong class="source-inline">spacy</strong> and convert the Markdown-formatted training dataset to a CRF format. (The code in GitHub shows some additional steps that are omitted here <span class="No-Break">for simplicity):</span></p>
			<pre class="source-code">
import sklearn_crfsuite
from spacy_crfsuite import read_file
train_data = read_file("examples/restaurant_search.md")
train_data
In [ ]:
import spacy
from spacy_crfsuite.tokenizer import SpacyTokenizer
from spacy_crfsuite.train import gold_example_to_crf_tokens
nlp = spacy.load("en_core_web_sm", disable=["ner"])
tokenizer = SpacyTokenizer(nlp)
train_dataset = [
    gold_example_to_crf_tokens(ex, tokenizer=tokenizer)
    for ex in train_data
]
train_dataset[0]</pre>
			<p>At this point, we can <a id="_idIndexMarker753"/>do the actual training of the CRF with the <strong class="source-inline">CRFExtractor</strong> object, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
from spacy_crfsuite import CRFExtractor
crf_extractor = CRFExtractor(
    component_config=component_config)
crf_extractor
rs = crf_extractor.fine_tune(train_dataset, cv=5,
    n_iter=50, random_state=42)
print("best_params:", rs.best_params_, ", score:",
    rs.best_score_)
crf_extractor.train(train_dataset)
classification_report = crf_extractor.eval(train_dataset)
print(classification_report[1])</pre>
			<p>The classification report, which is produced in the second to last step, and is shown next, is based on the training dataset (<strong class="source-inline">train_dataset</strong>). Since the CRF was trained on this dataset, the classification report will show perfect performance on each slot. Obviously, this is not realistic, but it’s shown here in order to illustrate the classification report. Remember that we will return to the topics of precision, recall, and F1 score in <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
              precision    recall  f1-score   support
U-atmosphere      1.000     1.000     1.000         1
   U-cuisine      1.000     1.000     1.000         9
  U-location      1.000     1.000     1.000         6
      U-meal      1.000     1.000     1.000         2
     B-price      1.000     1.000     1.000         1
     I-price      1.000     1.000     1.000         1
     L-price      1.000     1.000     1.000         1
   U-quality      1.000     1.000     1.000         1
   micro avg      1.000     1.000     1.000        22
   macro avg      1.000     1.000     1.000        22
weighted avg      1.000     1.000     1.000        22</pre>
			<p>At this point, the CRF model has been trained and is ready to test with new data. If we test this model <a id="_idIndexMarker754"/>with the sentence <em class="italic">show me some good chinese restaurants near me</em>, we can see the result in JSON format in the following code. The CRF model found two slots, <strong class="source-inline">CUISINE</strong> and <strong class="source-inline">QUALITY</strong>, but missed the <strong class="source-inline">LOCATION</strong> slot, which should have been filled by <strong class="source-inline">near me</strong>. The results also show the model’s confidence in the slots, which was quite high, well over <strong class="source-inline">0.9</strong>. The result also includes the zero-based positions of the characters in the input that begin and end the slot value (<strong class="source-inline">good</strong> starts at position <strong class="source-inline">10</strong> and ends at <span class="No-Break">position </span><span class="No-Break"><strong class="source-inline">14</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
example = {"text": "show some good chinese restaurants near me"}
tokenizer.tokenize(example, attribute="text")
crf_extractor.process(example)
[{'start': 10,
  'end': 14,
  'value': 'good',
  'entity': 'quality',
  'confidence': 0.9468721304898786},
 {'start': 15,
  'end': 22,
  'value': 'chinese',
  'entity': 'cuisine',
  'confidence': 0.9591743424660175}]</pre>
			<p>Finally, we can illustrate the robustness of this approach by testing our application with a cuisine that was not <a id="_idIndexMarker755"/>seen in the training data, <strong class="source-inline">Japanese</strong>. Let’s see if the system can label <strong class="source-inline">Japanese</strong> as a cuisine in a new utterance. We can try an utterance such as <strong class="source-inline">show some good Japanese restaurants near here</strong> and see the result in the <span class="No-Break">following JSON:</span></p>
			<pre class="source-code">
[{'start': 10,
  'end': 14,
  'value': 'good',
  'entity': 'quality',
  'confidence': 0.6853277275481114},
 {'start': 15,
  'end': 23,
  'value': 'japanese',
  'entity': 'cuisine',
  'confidence': 0.537198793062902}]</pre>
			<p>The system did identify <strong class="source-inline">Japanese</strong> as a cuisine in this example, but the confidence was much lower than we saw in the previous example, only <strong class="source-inline">0.537</strong> this time, compared with <strong class="source-inline">0.96</strong> for the same sentence with a known cuisine. This relatively low confidence is typical for slot fillers that didn’t occur in the training data. Even the confidence of the <strong class="source-inline">QUALITY</strong> slot (which did occur in the training data) was lower, probably because it was affected by the low probability of the unknown <strong class="source-inline">CUISINE</strong> <span class="No-Break">slot filler.</span></p>
			<p>A final observation <a id="_idIndexMarker756"/>worth pointing out is that while it would have been possible to develop a rule-based slot tagger for this task, as we saw in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, the resulting system would not have been able to even tentatively identify <strong class="source-inline">Japanese</strong> as a slot filler unless <strong class="source-inline">Japanese</strong> had been included in one of the rules. This is a general illustration of how the statistical approach can provide results that are not all or none, compared to <span class="No-Break">rule-based approaches.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor183"/>Summary</h1>
			<p>This chapter has explored some of the basic and most useful classical statistical techniques for NLP. They are especially valuable for small projects that start out without a large amount of training data, and for the exploratory work that often precedes a <span class="No-Break">large-scale project.</span></p>
			<p>We started out by learning about some basic evaluation concepts. We learned particularly about accuracy, but we also looked at some confusion matrices. We also learned how to apply Naïve Bayes classification to texts represented in TF-IDF format, and then we worked through the same classification task using a more modern technique, SVMs. Comparing the results produced by Naïve Bayes and SVMs, we saw that we got better performance from the SVMs. We then turned our attention to a related NLP task, slot-filling. We learned about different ways to represent slot-tagged data and finally illustrated CRFs with a restaurant recommendation task. These are all standard approaches that are good to have in your NLP toolbox, especially for the initial exploration of applications with <span class="No-Break">limited data.</span></p>
			<p>In <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we will continue working on topics in machine learning, but we will move on to a very different type of machine learning, neural networks. There are many varieties of neural networks, but overall, neural networks and their variants have become the standard technologies for NLP in the last decade or so. The next chapter will introduce this <span class="No-Break">important topic.</span></p>
		</div>
	</body></html>