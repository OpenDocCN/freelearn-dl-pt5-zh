- en: Training a Single Neuron
  prefs: []
  type: TYPE_NORMAL
- en: 'After revising the concepts around learning from data, we will now pay close
    attention to an algorithm that trains one of the most fundamental neural-based
    models: the **perceptron**. We will look at the steps required for the algorithm
    to function, and the stopping conditions. This chapter will present the perceptron
    model as the first model that represents a neuron, which aims to learn from data
    in a simple manner. The perceptron model is key to understanding basic and advanced
    neural models that learn from data. In this chapter, we will also cover the problems
    and considerations associated with non-linearly separable data.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon completion of the chapter, you should feel comfortable discussing the perceptron
    model, and applying its learning algorithm. You will be able to implement the
    algorithm over both linearly and non-linearly separable data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perceptron learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A perceptron over non-linearly separable data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perceptron model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in [Chapter 1](e3181710-1bb7-4069-825a-a235355bc116.xhtml), *Introduction
    to Machine Learning*, we briefly introduced the basic model of a neuron and the
    **perceptron learning algorithm** (**PLA**). Here, in this chapter, we will now
    revisit and expand the concept and show how that is coded in Python. We will begin
    with the basic definition.
  prefs: []
  type: TYPE_NORMAL
- en: The visual concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is an analogy of a human-inspired information processing unit,
    originally conceived by F. Rosenblatt and depicted in *Figure 5.1* (Rosenblatt,
    F. (1958)). In the model, the input is represented with the vector ![](img/dd2553bf-c673-4ade-b77f-c9edb436ed12.png),
    the activation of the neuron is given by the function ![](img/018df1a9-86c5-40ef-afd0-12da768026e0.png),
    and the output is ![](img/e9f481bc-0639-4987-8be9-75dd4d377193.png). The parameters
    of the neuron are ![](img/d16c70d8-9f05-4c25-91f9-61fa60a89974.png) and ![](img/430e33f2-b6a0-44d0-b21a-c16e32da02b8.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bac392e3-087b-485d-aa22-287a161238ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The basic model of a perceptron
  prefs: []
  type: TYPE_NORMAL
- en: 'The *trainable* parameters of a perceptron are ![](img/b7dce5c4-c06a-4c49-99ac-44a00c3dde4a.png),
    and they are unknown. Thus, we can use input training data ![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png) to
    determine these parameters using the PLA. From *Figure 5.1*, ![](img/5ebb08ed-23c4-4a97-ad97-192adaf9fa16.png)multiplies ![](img/590e73b6-a479-4c5c-9285-4551930c7b91.png),
    then ![](img/8b81f157-95ad-4b92-9977-3c2f3500a544.png)multiplies ![](img/67beef13-3471-40ef-869e-0c5009fa2a91.png),
    and ![](img/d0b9f56d-d74f-40fe-9160-7c65b95b390b.png) is multiplied by 1; all
    these products are added and then passed into the *sign* activation function,
    which in the perceptron operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25c4e4bc-29d9-4ee4-8ff0-0d8ada66ccb1.png)'
  prefs: []
  type: TYPE_IMG
- en: The main purpose of the activation sign is to map any response of the model
    to a binary output: [![](img/c2b8debd-da54-4f91-82aa-f673eacf1584.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about tensors in a general sense.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Python, the implementation of the perceptron requires a few simple tensor
    (vector) operations that can be performed through standard NumPy functionalities.
    First, we can assume that we are given data [![](img/0d85ebf4-d57f-4fd1-88ee-e155345e4c6f.png)] in
    the form of a vector containing multiple vectors ![](img/59afade2-2430-4eba-b0c8-28c69956ec56.png)
    (a matrix), represented as [![](img/585f7ba2-8841-4a53-8431-2d1be8d31af2.png)],
    and multiple individual targets represented as a vector [![](img/b7e3a027-88db-4c9b-ad29-639468ea172f.png)].
    However, notice that for easier implementation of the perceptron it will be necessary
    to include ![](img/3b0fa764-dbef-4a77-a092-47b93447c9ed.png) in ![](img/7d70cc37-fb85-4d34-99b5-37b13204746e.png),
    as suggested in *Figure 5.1*, so that the products and additions in [![](img/55992dcb-f61a-4702-9339-fb13e815fdec.png)] can
    be simplified if we modify ![](img/5c464c60-6998-4884-9f1d-38fbca822376.png) to
    be ![](img/5fc1e66a-27b1-4b71-a1f9-e19233e0ffd6.png), and ![](img/71f4dc57-8177-4f50-b003-612501ff1d00.png) to
    be [![](img/c6fb2cba-f4dc-4519-8728-4d7bfb626352.png)]. In this way, the perceptron
    response for an input [![](img/e8377d7f-1b36-43b3-882d-751ea830c783.png)] could
    be simplified to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e56c3f72-ce08-4071-9aac-2104e2d42362.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that **![](img/6d483edb-3362-4650-a2ef-4ad86675a0ae.png) **is now implicit
    in ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Say that we want to have training data `X`, which we need to prepare for the
    perceptron; we can do that with a simple linearly separable dataset that can be
    generated through scikit-learn''s dataset method called `make_classification`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we use the `make_classification` constructor to produce 100 data points
    (`n_samples`) for two classes (`n_classes`) and with enough separation (`class_sep`)
    to make data that is linearly separable. However, the dataset produced binary
    values in `y` in the set [![](img/5fbae48a-781e-4d49-8899-6cefba685f19.png)],
    and we need to convert it to the values in the set [![](img/db866a8e-aee2-419a-9953-74d93d142c8d.png)].
    This can be easily achieved by replacing the zero targets with the negative targets
    by simply doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset produced looks as depicted in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fe6eb0e-e3ec-42a1-a4f3-7e9aa99ab418.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2– Sample two-dimensional data for perceptron testing
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can add the number 1 to each input vector by adding a vector of ones
    to `X` with length `N=100` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The new data in `X` now contains a vector of ones. This will allow easier calculation
    of the tensor operation [![](img/f401a345-a15c-485c-8636-1fefe31ec8d7.png)] for
    all ![](img/f58f01a4-07d4-4577-aabe-6f1b9e92ce78.png). This common tensor operation
    can be performed in one single step considering the matrix ![](img/3964917c-2a69-4948-b032-f55e473de016.png) simply
    as ![](img/caa93965-f1c5-4721-8829-197f116857b1.png). We can even combine this
    operation and the sign activation function in one single step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is the equivalent of the mathematical tensor operation [![](img/3ac75eb8-972d-46c9-9647-b8cda8237618.png)].
    With this in mind, let's review the PLA in more detail using the dataset introduced
    previously, and the operations just described.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **perceptron learning algorithm** (**PLA**) is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: Binary class dataset [![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png)]'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) to zeros, and iteration counter ![](img/467437b4-17a7-42a3-af49-cd3cc8b8816e.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While there are any incorrectly classified examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pick an incorrectly classified example, call it ![](img/01a87dc1-7f59-4c55-9c52-a1803ae95db7.png),
    whose true label is ![](img/16eb7743-58d1-4037-ae7f-67757849756f.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) as follows: ![](img/cfa7a1f4-17cd-4f28-b07e-f17fbac38691.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase iteration counter, [![](img/99f0af99-2238-4552-9903-2580096b8378.png)], and
    repeat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return**: ![](img/8ca08b66-f0e6-466a-bd50-d4d584c2cf06.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how this takes form in Python.
  prefs: []
  type: TYPE_NORMAL
- en: PLA in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is an implementation in Python that we will discuss part by part, while
    some of it has already been discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first few lines have been discussed previously in the *Tensor operations*
    section of this chapter. The initialization of ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) to
    zeros is done with `w = np.zeros(X_train.shape[1])`. The size of this vector depends
    on the dimensionality of the input. Then, `it` is merely an iteration counter
    to keep track of the number of iterations that are performed until the PLA converges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `classification_error()` method is a helper method that takes as arguments
    the current vector of parameters `w`, the input data `X_train`, and corresponding
    target data `y`. The purpose of this method is to determine the number of misclassified
    points at the present state ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png),
    if there are any, and return the total count of errors. The method can be defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This method could be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: However, while this is a nice optimization for small datasets, for large datasets
    it may not be necessary to calculate all points for error. Thus, the first (and
    longer) method can be used and modified according to the type of data that is
    expected, and if we know that we will be dealing with large datasets, we could
    break out of the method at the first sign of error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second helper method in our code is `choose_miscl_point()`. The main purpose
    of this method is to select, at random, one of the misclassified points, if there
    are any. It takes as arguments the current vector of parameters `w`, the input
    data `X_train`, and corresponding target data `y`. It returns a misclassified
    point, `x`, and what the corresponding target sign should be, `s`. The method
    could be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, this could be optimized for speed by randomizing a list of indices,
    iterating over them, and returning the first one found, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, the first implementation can be useful for absolute beginners or for
    those who would like to do some additional analysis of the misclassified points,
    which could be conveniently available in the list `mispts`.
  prefs: []
  type: TYPE_NORMAL
- en: The crucial point, no matter the implementation, is to randomize the selection
    of the misclassified point.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the update happens using the current parameters, the misclassified
    point, and the corresponding target on the line that executes `w = w + s*x`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the complete program, it should output something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of iterations may vary depending on the type of data and the
    random nature of the selection of the misclassified points. For the particular
    dataset we are using, the decision boundary could look as shown in *Figure 5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61a2413b-086d-4458-bab7-0433588aa938.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Decision boundary found with PLA
  prefs: []
  type: TYPE_NORMAL
- en: The number of iterations will also depend on the separation or gap between data
    points in the feature space. The larger the gap is, the easier it is to find a
    solution, and the converse is also true. The worst-case scenario is when the data
    is non-linearly separable, which we'll address next.
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron over non-linearly separable data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed before, a perceptron will find a solution in finite time
    if the data is separable. However, how many iterations it will take to find a
    solution depends on how close the groups are to each other in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convergence** is when the learning algorithm finds a solution or reaches
    a steady state that is acceptable to the designer of the learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: The following paragraphs will deal with convergence on different types of data: linearly separable
    and non-linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: Convergence on linearly separable data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the particular dataset that we have been studying in this chapter, the
    separation between the two groups of data is a parameter that can be varied (this
    is usually a problem with real data). The parameter is `class_sep` and can take
    on a real number; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to study how many iterations it takes, on average, for the perceptron
    algorithm to converge if we vary the separation parameter. The experiment can
    be designed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will vary the separation coefficient from large to small, recording the
    number of iterations it takes to converge: 2.0, 1.9, ..., 1.2, 1.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will repeat this 1,000 times and record the average number of iterations
    and the corresponding standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that we decided to run this experiment down to 1.1, since 1.0 already
    produces a non-linearly separable dataset. If we perform the experiment, we can
    record the results in a table and it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Run** | **2.0** | **1.9** | **1.8** | **1.7** | **1.6** | **1.5** | **1.4**
    | **1.3** | **1.2** | **1.1** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 2 | 2 | 2 | 7 | 10 | 4 | 15 | 13 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 5 | 1 | 2 | 2 | 4 | 8 | 6 | 26 | 62 | 169 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4 | 4 | 5 | 6 | 6 | 10 | 11 | 29 | 27 | 293 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 998 | 2 | 5 | 3 | 1 | 9 | 3 | 11 | 9 | 35 | 198 |'
  prefs: []
  type: TYPE_TB
- en: '| 999 | 2 | 2 | 4 | 7 | 6 | 8 | 2 | 4 | 14 | 135 |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 2 | 1 | 2 | 2 | 2 | 8 | 13 | 25 | 27 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| **Avg.** | **2.79** | **3.05** | **3.34** | **3.67** | **4.13** | **4.90**
    | **6.67** | **10.32** | **24.22** | **184.41** |'
  prefs: []
  type: TYPE_TB
- en: '| **Std.** | **1.2** | **1.3** | **1.6** | **1.9** | **2.4** | **3.0** | **4.7**
    | **7.8** | **15.9** | **75.5** |'
  prefs: []
  type: TYPE_TB
- en: 'This table shows that the average number of iterations taken is fairly stable
    when the data is nicely separated; however, as the separation gap is reduced,
    the number of iterations increases dramatically. To put this in a visual perspective,
    the same data from the table is now shown in *Figure 5.4* on a logarithmic scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f74dcd4-9806-4fd8-b92b-035c1de18d94.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – The growth of the number of PLA iterations as the data groups are
    closer together
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very clear that the number of iterations can grow exponentially as the
    separation gap is closing. *Figure 5.5* depicts the largest separation gap, 2.0,
    and indicates that the PLA found a solution after four iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5364997e-8957-4e62-9ba5-d183b96c0dec.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The perceptron found a solution in four iterations for a separation
    gap of 2.0
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, *Figure 5.6* shows that for the largest gap, 1.1, the PLA takes
    183 iterations; a close inspection of the figure reveals that the solution for
    the latter case is difficult to find because the data groups are too close to
    each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd93c7a8-7a43-491c-b7d6-f61bcbc8b8ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The perceptron found a solution in 183 iterations for a separation
    gap of 1.1
  prefs: []
  type: TYPE_NORMAL
- en: As noted before, data that is not linearly separable can be produced with a
    gap of 1.0 and the PLA will run in an infinite loop since there will always be
    a data point that will be incorrectly classified and the `classification_error()`
    method will never return a zero value. For those cases, we can modify the PLA
    to allow finding solutions on non-linearly separable data, as we'll cover in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Convergence on non-linearly separable data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The modifications to the original PLA are rather simple, but are good enough
    to allow finding an acceptable solution in most cases. The main two things that
    we need to add to the PLA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A mechanism to prevent the algorithm from running forever
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mechanism to store the best solution ever found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With respect to the first point, we can simply specify a number of iterations
    at which the algorithm can stop. With respect to the second point, we can simply
    keep a solution in storage, and compare it to the one in the current iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relevant portion of the PLA is shown here and the new changes have been
    marked with bold font and will be discussed in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this code, `bestW` is a dictionary for keeping track of the best results
    so far, and it is initialized to reasonable values. Notice first that the loop
    is now bounded by the number 1,000, which is the maximum number of iterations
    you currently allow and you can change it to anything you desire to be the maximum
    number of allowed iterations. It would be reasonable to reduce this number for
    large datasets or high-dimensional datasets where every iteration is costly.
  prefs: []
  type: TYPE_NORMAL
- en: The next changes are the inclusion of the conditional statement, `if err < bestW['err']`,
    which determines whether we should store a new set of parameters. Every time the
    error, as determined by the total number of misclassified examples, is lower than
    the error of the stored parameters, then an update is made. And just for completion,
    we have to still check that there are no errors, which indicates that the data
    is linearly separable, a solution has been found, and the loop needs to terminate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last few `print` statements will simply inform the iteration and error
    obtained when the best solution was recorded. The output may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This output was produced by running the updated PLA over the dataset with a
    separation of 1.0, which is depicted in *Figure 5.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e1b5cd4-6720-4c62-bd35-ec5bdf23c214.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The updated PLA finds a solution with only one misclassified point
    after 95 iterations
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, it can be seen that there is one sample from the positive class
    that is incorrectly classified. Knowing that in this example there is a total
    of 100 data points, we can determine that the accuracy is 99/100.
  prefs: []
  type: TYPE_NORMAL
- en: This type of algorithm, which stores the *best solution so far*, is usually
    known as a **pocket algorithm** (Muselli, M. 1997). And the idea of the early
    termination of a learning algorithm is inspired by well-known numerical optimization
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: One of the general limitations is that the perceptron can only produce solutions
    that are based on a line in two dimensions, or a linear hyperplane in multiple
    dimensions. However, this limitation can be easily solved by putting several perceptrons
    together and in multiple layers to produce highly complex non-linear solutions
    for separable and non-separable problems. This will be the topic of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter presented an overview of the classic perceptron model. We covered
    the theoretical model and its implementation in Python for both linearly and non-linearly
    separable datasets. At this point, you should feel confident that you know enough
    about the perceptron that you can implement it yourself. You should be able to
    recognize the perceptron model in the context of a neuron. Also, you should now
    be able to implement a pocket algorithm and early termination strategies in a
    perceptron, or any other learning algorithm in general.
  prefs: []
  type: TYPE_NORMAL
- en: Since the perceptron is the most essential element that paved the way for deep
    neural networks, after we have covered it here, the next step is to go to [Chapter
    6](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml), *Training Multiple Layers of Neurons.* In
    that chapter, you will be exposed to the challenges of deep learning using the
    multi-layer perceptron algorithm, such as gradient descent techniques for error
    minimization, and hyperparameter optimization to achieve generalization. But before
    you go there, please try to quiz yourself with the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the relationship between the separability of the data and the number
    of iterations of the PLA?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of iterations can grow exponentially as the data groups get close
    to one another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Will the PLA always converge? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not always, only for linearly separable data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can the PLA converge on non-linearly separable data?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. However, you can find an acceptable solution by modifying it with the pocket
    algorithm, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is the perceptron important?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because it is one of the most fundamental learning strategies that has helped
    conceive the possibility of learning. Without the perceptron, it could have taken
    longer for the scientific community to realize the potential of computer-based
    automatic learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, 65(6), 386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muselli, M. (1997). On convergence properties of the pocket algorithm. *IEEE
    Transactions on Neural Networks*, 8(3), 623-629.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
