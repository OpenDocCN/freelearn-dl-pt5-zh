- en: ­
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10:* Deploying a Deep Learning Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections of this book, we covered the training of deep neural
    networks for many different use cases, starting with an autoencoder for fraud
    detection, through **Long Short-Term Memory** (**LSTM**) networks for energy consumption
    prediction and free text generation, all the way to cancer cell classification.
    But training the network is not the only part of a project. Once a deep learning
    network is trained, the next step is to deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: During the exploration of some of the use cases, a second workflow has already
    been introduced, to deploy the network to work on real-world data. So, you have
    already seen some deployment examples. In this last section of the book, however,
    we focus on the many deployment options for machine learning models in general,
    and for trained deep learning networks in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a second workflow is built and dedicated to deployment. This workflow
    reads the trained model and the new real-world data, it preprocesses this data
    in exactly the same way as for the training data, then it applies the trained
    deep learning network on is transformed data and produces the results according
    to the project's requirements.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on the reading, writing, and preprocessing of the data
    in a deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with a review of the features for saving, reading, and converting
    a trained network. This is followed by two examples of how the preprocessing for
    our sentiment analysis use case can also be implemented in a deployment workflow.
    Finally, the chapter shows how to improve execution speed by enabling GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter consists of the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversion of the Network Structure
  prefs: []
  type: TYPE_NORMAL
- en: Building a Simple Deployment Workflow
  prefs: []
  type: TYPE_NORMAL
- en: Improving Scalability – GPU Execution
  prefs: []
  type: TYPE_NORMAL
- en: Conversion of the Network Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of a deployment workflow is to apply a trained network to new real-world
    data. Therefore, the last step of the training workflow must be to save the trained
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a Trained Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All networks described in this book have been trained using the Keras libraries,
    relying on TensorFlow as the backend. So, the most natural way to save a network
    is to continue using the Keras libraries and therefore to use the `.h5` file.
  prefs: []
  type: TYPE_NORMAL
- en: However, Keras-formatted networks can only be interpreted and executed via the
    Keras libraries. This is already one level on top of the TensorFlow libraries.
    Executing the network application on the TensorFlow Java API directly, rather
    than on a Python kernel via the Keras Python API, makes execution faster. The
    good news is that KNIME Analytics Platform also has nodes for TensorFlow execution
    in addition to the nodes based on Keras libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if faster execution is needed, the Keras network should be converted into
    a TensorFlow network using the `SavedModel` file, a compressed `zip` file. A `SavedModel`
    file contains a complete TensorFlow program, including weights and computation.
    It does not require the original model building code to run, which makes it useful
    for sharing or deploying.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in a deployment network is to read a trained network.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a Trained Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'KNIME Analytics Platform provides many nodes for reading a trained neural network,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Keras Network Reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Network Reader (and TensorFlow 2 Network Reader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL Python Network Creator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX Network Reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `.h5` file) or just a network architecture definition without weights (a
    `.json` or `.yaml` file). You can use the node to read networks trained with KNIME
    Analytics Platform or networks trained directly with Keras, such as pretrained
    Keras networks.
  prefs: []
  type: TYPE_NORMAL
- en: The `zip` file. If reading from a directory, it has to be a valid `SavedModel`
    folder. If reading from a `zip` file, it must contain a valid `SavedModel` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Network Reader node allows us to select a tag and a signature
    in its configuration window. Tags are used to identify the meta graph definition
    to load. Signatures are `SavedModel` can have multiple tags as well as multiple
    signatures per tag. A network saved with KNIME Analytics Platform has only one
    tag and one signature. In the **Advanced** tab of the configuration window, you
    can define your own signature by defining the input and output of the model by
    selecting one of the hidden layers as output, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Another node, which allows you to read pretrained networks without writing a
    single line of code, is the **ONNX Network Reader** node. **ONNX** stands for
    **Open Neural Network Exchange** and is a standard format for neural networks
    developed by Microsoft and Facebook. Since it is a standard format, it is portable
    across machine learning frameworks such as PyTorch, Caffe2, TensorFlow, and more.
    You can download pretrained networks from the ONNX Model Zoo ([https://github.com/onnx/models#vision](https://github.com/onnx/models#vision))
    and read them with the ONNX Network Reader node. The ONNX networks can also be
    converted into TensorFlow networks using the **ONNX to TensorFlow Network Converter**
    node, and then executed with the TensorFlow Network Executor node.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To use the ONNX nodes, you need to install the **KNIME Deep Learning – ONNX
    Integration** extension.
  prefs: []
  type: TYPE_NORMAL
- en: Another option for reading a network using Python code is the **DL Python Network
    Creator** node, which can be used to read pretrained neural networks using a few
    lines of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The DL Python Network Creator node can also be used in training workflows to
    define the network architecture using Python code instead of layer nodes.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used Keras-based nodes with TensorFlow 1 as the backend. There
    are also nodes that use TensorFlow 2 as the backend to implement similar operations.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For all the examples in this book, we have used Keras-based nodes that run TensorFlow
    1 as the backend. TensorFlow 2 is also supported since the release of KNIME Analytics
    Platform 4.2\. On the KNIME Hub, you can find many examples of how to use TensorFlow
    2 integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TensorFlow 2 integration comes with three nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The TensorFlow 2 Network Executor** node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The TensorFlow 2 Network Reader** node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The TensorFlow 2 Network Writer** node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To train a deep learning model using TensorFlow 2 you can use the **DL Python
    Network Learner** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed the many options to save and read neural networks,
    let's focus on building a simple deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Simple Deployment Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in all the case studies we have explored, we have always performed some
    kind of preprocessing of the input data, such as encoding categorical features,
    encoding text, or normalizing data, to name just some of the adopted preprocessing
    steps. During deployment, the new incoming data must be prepared with the exact
    same preprocessing as the training data in order to be consistent with the task
    and with the input that the network expects.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we use the sentiment analysis case study shown in [*Chapter
    7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing NLP Applications*,
    as an example, and we build two deployment workflows for it. The goal of both
    workflows is to read new movie reviews from a database, predict the sentiment,
    and write the prediction into the database.
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, the preprocessing steps are implemented manually into
    the deployment workflow. In the second example, the **Integrated Deployment**
    feature is used.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow Manually, without Integrated Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deployment workflow should access new reviews from a table in a database,
    apply the trained network, write the reviews with the corresponding predictions
    into another table in the database, and delete the reviews from the first table.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps are performed by the workflow in *Figure 10.1*, which you can download
    from the KNIME Hub at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Deployment workflow for the sentiment analysis case study from
    Chapter 7, Implementing NLP Applications](img/B16391_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Deployment workflow for the sentiment analysis case study from
    [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), Implementing NLP
    Applications
  prefs: []
  type: TYPE_NORMAL
- en: The workflow first connects to a SQLite database, where the new movie reviews
    are stored, using the **SQLite Connector** node.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the **SELECT** SQL statement to read the new reviews from the table named
    **new_reviews** is implemented by the **DB Table Selector** node.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL statement is then executed through the **DB Reader** node. As a result,
    we have the new reviews in a data table at the output port of the node.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16391_02_Final_SK_ePUB.xhtml#_idTextAnchor051), *Data Access
    and Preprocessing with KNIME Analytics Platform*, the database extension was introduced
    in detail. Remember that the database nodes create a SQL statement at their output
    brown-squared port.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying the network to these new reviews, we need to perform the same
    transformations as in the training workflow. In the training workflow, reported
    in [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing
    NLP Applications*, there was a metanode named **Preprocess test set** where all
    the required preprocessing steps were applied to the test data. We used this metanode
    as the basis for creating the preprocessing steps for the incoming data in the
    deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.2* shows the content of this metanode, which is dedicated to the
    preprocessing of the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Preprocessing of the test data in the training workflow of
    the sentiment analysis case study from Chapter 7, Implementing NLP Applications](img/B16391_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Preprocessing of the test data in the training workflow of the
    sentiment analysis case study from [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    Implementing NLP Applications
  prefs: []
  type: TYPE_NORMAL
- en: In the deployment workflow in *Figure 10.1*, the dictionary, created during
    training is read first; then the preprocessing steps are implemented in the **Preprocessing**
    metanode.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.3* shows you the workflow snippet inside this metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Workflow snippet inside the Preprocessing metanode of the deployment
    workflow](img/B16391_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Workflow snippet inside the Preprocessing metanode of the deployment
    workflow
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the workflow snippets in *Figure 10.2* and *Figure 10.3*, you
    can see that they contain the same preprocessing steps, as was expected.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the same preprocessing as for the training data has been applied to
    the deployment data, the trained network can be introduced through the **Keras
    Network Reader** node (*Figure 10.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the trained network runs on the preprocessed deployment reviews using
    the **Keras Network Executor** node. The output of the network is the probability
    of the sentiment being equal to 1, where 1 encodes a positive movie review. The
    same threshold as during training is also applied here through the **Rule Engine**
    node: a threshold of ![](img/Formula_B16391_10_001.png).'
  prefs: []
  type: TYPE_NORMAL
- en: In the last step, the tables in the database are updated. First, the **DB Delete**
    node deletes the reviews we just analyzed from the **new_reviews** table. Then,
    the **DB Writer** node appends the new movie reviews with their predictions to
    another table in the database, named **review-with-sentiment**.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first example of the deployment of a neural network using KNIME
    Analytics Platform. This workflow should be executed on a regular basis to predict
    the sentiment for all new incoming movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Server can schedule the execution of workflows, so you can trigger their
    execution automatically on a regular schedule.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has one disadvantage. If the model is retrained on more data or
    with different settings (for example, if more or fewer terms are taken into account
    during training or the threshold for the Rule Engine node is changed) we need
    to remember to also update the preprocessing steps in the deployment workflow.
    And since we are forgetful humans, we might forget or make mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to overcome this issue is the concept of **Integrated Deployment**.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow Automatically with Integrated Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until KNIME Analytics Platform 4.2, as well as in other tools, a common approach
    was to implement data blending, data transformation, and network execution manually
    in the deployment workflow. This means that you need to copy the different preprocessing
    snippets, parameters, and network executor nodes from the training workflow to
    the deployment workflow, making sure that all settings remain unaltered.
  prefs: []
  type: TYPE_NORMAL
- en: This manual step slows down the process and can easily lead to mistakes. Automating
    the construction of parts of the deployment workflow can be a safer option, especially
    if the models are changed often, for example, every day or even every hour.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Other common names for the training process are data science creation or modeling
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The nodes from the Integrated Deployment extension close the gap between creating
    and deploying data science.
  prefs: []
  type: TYPE_NORMAL
- en: The Integrated Deployment Extension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Integrated Deployment extension allows data scientists to combine the model
    training and deployment into one single workflow. The idea is to capture parts
    of the training workflow and to automatically write them into the deployment workflow
    during the execution of the training workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of copying the preprocessing parts manually, one by one, the required
    parts from the training workflow are captured in between the **Capture Workflow
    Start** and **Capture Workflow End** nodes. The captured workflow part in the
    middle can then be written into a new workflow with a **Workflow Writer** node.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Integrated Deployment Extension in the Training Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's consider again the deployment workflow for the sentiment analysis case
    study described in [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*. In the training workflow, we have introduced
    the **Capture Workflow Start** node and the **Capture Workflow End** node to isolate
    the workflow snippet that we want to reproduce exactly in the deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'This includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The metanode named **Preprocessing test set**, including all required preprocessing
    steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Keras Network Executor** node to apply the trained network on the deployment
    transformed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Rule Engine** node, which decides on the positive or the negative class
    based on a threshold applied to the output class' probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The workflow in *Figure 10.4* shows you this example based on the sentiment
    analysis case study. You can download the workflow from the KNIME Hub at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Training workflow that automatically creates a deployment workflow
    using Integrated Deployment](img/B16391_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Training workflow that automatically creates a deployment workflow
    using Integrated Deployment
  prefs: []
  type: TYPE_NORMAL
- en: The part in the thick box is the captured workflow snippet. The **Capture Workflow
    Start** node defines the beginning and the **Capture Workflow End** node defines
    the end of the workflow snippet to capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The start node doesn''t need any configuration. *Figure 10.5* shows the configuration
    window of the **Capture Workflow End** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Configuration window of the Capture Workflow End node](img/B16391_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Configuration window of the Capture Workflow End node
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window, you can set the name of the captured workflow snippet.
    You can also set whether the captured snippet should be stored with the data and,
    if yes, the maximum number of data rows to include. We will see in a second why
    it can be helpful to store some data in the captured workflow snippet.
  prefs: []
  type: TYPE_NORMAL
- en: The captured workflow snippet, with or without data, is then exported via the
    output port (the black square) of the **Capture Workflow End** node. In the workflow
    in *Figure 10.4*, the workflow snippet is then collected by the **Workflow Writer**
    node and written into the deployment workflow, with unaltered settings and configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.6* shows the configuration window of the **Workflow Writer** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The Workflow Writer node and its configuration window](img/B16391_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – The Workflow Writer node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: At the top, you can set the location of the folder of the destination workflow
    (**Output location**).
  prefs: []
  type: TYPE_NORMAL
- en: Next, you need to set the name of the destination workflow. The node automatically
    proposes a default name, which you can customize via the **Use custom workflow
    name** option. If the name you choose refers to a workflow that already exists,
    you can let the writer node fail or overwrite.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the bottom, you can select the deployment option for the destination workflow:
    just create it, create it and open it, or save it as a `.knwf` file to export.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure, *Figure 10.7*, shows you the automatically generated deployment
    workflow by the **Workflow Writer** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Automatically created deployment workflow from the workflow
    snippet captured via Integrated Deployment](img/B16391_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Automatically created deployment workflow from the workflow snippet
    captured via Integrated Deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'In the captured workflow you can see the **Preprocessing test set** metanode,
    as well as the **Keras Network Executor**, **Rule Engine**, and **Column Filter**
    nodes. Additionally, the whole Integrated Deployment process has added the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two **Reference Reader** nodes. They are generic reader nodes, loading the connection
    information of static parameters not found in the captured workflow snippet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Container Input (Table)** and a **Container Output (Table)** node in order
    to accept input data and to send output data respectively from and to other applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The execution of this deployment workflow can be triggered either by another
    workflow using the **Call Workflow (Table)** node or via a REST service if the
    workflow has been deployed on a KNIMEs Server. In the next chapter, we will talk
    about the REST calls and REST services in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 10.7*, the example deployment workflow reads two entities at the
    top of the workflow using the two reader nodes without an icon inside them. The
    left one provides the dictionary table based on the training data, and the right
    one provides the trained neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you can see two more new nodes, which are the **Container Input
    (Table)** and **Container Output (Table)** nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The **Container Input (Table)** node receives a data table from an external
    caller (that is, the **Call Workflow (Table Based)** node) and makes it available
    on the output port. A configuration parameter enables the external caller to send
    a data table to the **Container Input (Table)** node.
  prefs: []
  type: TYPE_NORMAL
- en: The **Container Input (Table)** node also has an optional input port (represented
    by an unfilled input port). If a data table is connected to the optional input,
    the node will simply forward this table to the next node; if a table is supplied
    via a REST API, then the supplied table will be available on the output port.
  prefs: []
  type: TYPE_NORMAL
- en: If no input is given, a default template table will be provided on the output
    of the node. Here, the **Store input tables** setting from the **Capture Workflow
    End** node comes in. If you select to store some data rows, they are used to define
    this default template table.
  prefs: []
  type: TYPE_NORMAL
- en: The **Container Output (Table)** node sends a KNIME data table to an external
    caller.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now find out how the automatically created workflow can be used to predict
    the sentiment of new reviews during deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Automatically Created Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's have a look now at how the deployment workflow can be consumed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.8* shows you an example of how the automatically created deployment
    workflow can be consumed to classify the sentiment of new movie reviews, and you
    can download it from the KNIME Hub to try it out, at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: ': Figure 10.8 – Workflow calling the automatically created deployment workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Workflow calling the automatically created deployment workflow](img/B16391_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The workflow connects to the database and reads the incoming new movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the **Call Workflow (Table Based)** node calls the deployment workflow
    (*Figure 10.7*), the one that was automatically built. The **Call Workflow (Table
    Based)** node indeed calls other workflows residing on your local workspace or
    on a mounted KNIME server. The called workflow must contain at least one Container
    Input node and one Container Output node to define the interface between the two
    workflows: the called and the caller workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: Via the **Call Workflow (Table Based)** node, we send the new movie reviews
    to the deployment workflow to feed the **Container Input (Table)** node. The deployment
    workflow is then executed, and the predictions are sent back to the caller workflow
    and made available via the output port of the **Call Workflow (Table Based)**
    node.
  prefs: []
  type: TYPE_NORMAL
- en: A great advantage of this strategy is the ensured consistency between the data
    operations in the training workflow and the data operations in the deployment
    workflow. If we now change any settings in the data operations in the training
    workflow, for example, the value of the threshold in the **Rule Engine** node
    (*Figure 10.4*), and we re-execute the training workflow, these changes are automatically
    imported into the new version of the deployment workflow (*Figure 10.7*) and used
    by any workflow relying on it (*Figure 10.8*).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Another great node of the **Integrated Deployment** extension is the **Workflow
    Combiner** node, which allows us to combine workflow snippets from different original
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: We have reached the last section of this chapter, which is on scalability and
    GPU execution.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Scalability – GPU Execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the case studies described in this book, we have used relatively small datasets
    and small networks. This allowed us to train the networks within hours using only
    CPU-based execution. However, training tasks that take minutes or hours on small
    datasets can easily take days or weeks on larger datasets; small network architectures
    can quickly increase in size and execution times can quickly become prohibitive.
    In general, when working with deep neural networks, the training phase is the
    most resource-intensive task.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs have been designed to handle multiple computations simultaneously. This
    paradigm suits the intensive computations required to train a deep learning network.
    Hence, GPUs are an alternative option to train large deep learning networks efficiently
    and effectively on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Some Keras libraries can exploit the computational power of NVIDIA®-compatible
    GPUs via the TensorFlow paradigms. As a consequence, **KNIME Keras integration**
    can also exploit the computational power of GPUs to train deep learning networks
    more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B16391_01_Final_NM_ePUB.xhtml#_idTextAnchor016), *Introduction
    to Deep Learning with KNIME Analytics Platform*, we introduced how to set up Python
    for KNIME Keras integration and KNIME TensorFlow integration. In order to run
    the KNIME Keras integration on the GPU rather than on the CPU, you do not need
    to take many extra steps.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you need a GPU-enabled computer. TensorFlow 1.12 requires an NVIDIA
    GPU card with a CUDA compute capability of 3.5 or higher.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, most of the required dependencies (that is, CUDA® and cuDNN) will
    be automatically installed by Anaconda when installing the conda `tensorflow=1.12`
    and `keras-gpu=2.2.4`. packages
  prefs: []
  type: TYPE_NORMAL
- en: The only extra step at installation is the latest version of the NVIDIA® GPU
    driver, to be installed manually.
  prefs: []
  type: TYPE_NORMAL
- en: At installation time, by selecting `keras-gpu=2.2.4` is created.
  prefs: []
  type: TYPE_NORMAL
- en: When using the TensorFlow integration, it is also possible to execute on the
    GPU to read and execute TensorFlow's SavedModel.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The GPU support for the **KNIME TensorFlow integration** (which uses the TensorFlow
    Java API) is generally independent of the GPU support for the **KNIME Keras integration**
    (which uses Python). Hence, the two GPU supports must be set up individually.
    Due to the limitations of TensorFlow, the GPU support for the KNIME TensorFlow
    integration can only run on Windows and Linux, and not on Mac.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the following GPU configuration is recommended by KNIME.
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNIME TensorFlow integration uses TensorFlow version 1.13.1, which requires
    the following NVIDIA® software to be installed on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NVIDIA® GPU drivers: CUDA® 10.0 requires 410.x or higher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CUDA® Toolkit: TensorFlow (≥ 1.13.0) supports CUDA® 10.0\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'cuDNN (version ≥ 7.4.1): Select cuDNN v7.6.0 (May 20, 2019) for CUDA® 10.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For detailed instructions and the most recent updates, please check the KNIME
    documentation ([https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration](https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered three different topics. We started with a summary
    of the many options for reading, converting, and writing neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then moved on to the deployment of neural networks, using the sentiment
    analysis case study from [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*, as an example. The goal here was to build a workflow
    that uses the trained neural network to predict the sentiment of new reviews stored
    in the database. We have shown that a deployment workflow can be assembled in
    two ways: manually or automatically with Integrated Deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: The last section of the chapter dealt with the scalability of network training
    and execution. In particular, it showed how to exploit the computational power
    of GPUs when training a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and last chapter of this book, we will explore further deployment
    options and best practices when working with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which network conversions are available in KNIME Analytics Platform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Keras to TensorFlow network conversion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) TensorFlow to Keras network conversion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) ONNX to Keras network conversion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Keras to ONNX network conversion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which statements regarding Integrated Deployment are true (two statements are
    correct)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Integrated Deployment allows us to retrain a model during execution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The execution of the automatically generated workflow can be triggered by
    another workflow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The execution of the training workflow is triggered by the deployment workflow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Integrated Deployment closes the gap between training and deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
