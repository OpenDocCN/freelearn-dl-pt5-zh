<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer018">
<h1 class="chapter-number" id="_idParaDest-44"><a id="_idTextAnchor041"/>2</h1>
<h1 id="_idParaDest-45"><a id="_idTextAnchor042"/>Deep Learning Frameworks and Containers on SageMaker</h1>
<p>Amazon SageMaker supports many popular ML and DL frameworks. Framework support in SageMaker is achieved using prebuilt Docker containers for inference and training tasks. Prebuilt SageMaker containers provide a great deal of functionality, and they allow you to implement a wide range of use cases with minimal coding. There are also real-life scenarios where you need to have a custom, runtime environment for training and/or inference tasks. To address these cases, SageMaker provides a flexible <strong class="bold">Bring-Your-Own</strong> (<strong class="bold">BYO</strong>) container feature.</p>
<p>In this chapter, we will review key supported DL frameworks and corresponding container images. Then, we will focus our attention on the two most popular DL frameworks, TensorFlow and PyTorch, and learn how to use them in Amazon SageMaker. Additionally, we will review a higher-level, state-of-the-art framework, Hugging Face, for NLP tasks, and its implementation for Amazon SageMaker.</p>
<p>Then, we will understand how to use and extend prebuilt SageMaker containers based on your use case requirements, as well as learning about the SageMaker SDK and toolkits, which simplify writing training and inference scripts that are compatible with Amazon SageMaker.</p>
<p>In later sections, we will dive deeper into how to decide whether to use prebuilt SageMaker containers or BYO containers. Then, we will develop a SageMaker-compatible BYO container.</p>
<p>These topics will be covered in the following sections:</p>
<ul>
<li>Exploring DL frameworks on SageMaker</li>
<li>Using SageMaker DL containers</li>
<li>Developing BYO containers</li>
</ul>
<p>By the end of this chapter, you will be able to decide which container strategy to choose based on your specific problem requirements and chosen DL framework. Additionally, you will understand the key aspects of training and inference script development, which are compatible with Amazon SageMaker. </p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor043"/>Technical requirements</h1>
<p>In the <em class="italic">Using SageMaker DL containers</em> and <em class="italic">Developing BYO containers</em> sections, we will provide walk-through code samples, so you can develop practical skills. Full code examples are available at <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/</a>.</p>
<p>To follow along with this code, you will need the following:</p>
<ul>
<li>An AWS account and IAM user with the permissions to manage Amazon SageMaker resources.</li>
<li>Python 3 and the SageMaker SDK (<a href="https://pypi.org/project/sagemaker/">https://pypi.org/project/sagemaker/</a>) installed on your development machine.</li>
<li>Docker installed on your development machine.</li>
<li>To use SageMaker P2 instances for training purposes, you will likely need to request a service limit increase on your AWS account. For more details, please view <a href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml">https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.xhtml</a>.</li>
</ul>
<h1 id="_idParaDest-47"><a id="_idTextAnchor044"/>Exploring DL frameworks on SageMaker</h1>
<p>At the time of <a id="_idIndexMarker116"/>writing this book, Amazon SageMaker supports the following frameworks, where DL frameworks are marked with an asterisk:</p>
<ul>
<li>scikit-learn</li>
<li>SparkML Serving</li>
<li>Chainer*</li>
<li>Apache MXNet*</li>
<li>Hugging Face*</li>
<li>PyTorch*</li>
<li>TensorFlow*</li>
<li>Reinforcement <a id="_idIndexMarker117"/>learning containers – including TensorFlow- and PyTorch-enabled containers</li>
<li>XGBoost</li>
</ul>
<p>The preceding list of supported frameworks could change in the future. Be sure to check the official SageMaker documentation<a id="_idIndexMarker118"/> at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.xhtml</a>.</p>
<p>In this book, we will primarily focus on the two most popular choices: <strong class="bold">TensorFlow</strong> and <strong class="bold">PyTorch</strong>. Both are open source<a id="_idIndexMarker119"/> frameworks with a large and vibrant communities. Depending <a id="_idIndexMarker120"/>on the specific use case or model architecture, one or the other framework might have a slight advantage. However, it’s safe to assume that both frameworks are comparable in terms of features and performance. In many practical scenarios, the choice between TensorFlow or PyTorch is made based on historical precedents or individual preferences. </p>
<p>Another framework that we will discuss in this<a id="_idIndexMarker121"/> book is <strong class="bold">Hugging Face</strong>. This is a high-level framework that provides access to SOTA models, training, and inference facilities for NLP tasks (such as text classification, translation, and more). Hugging Face is a set of several libraries (transformers, datasets, tokenizers, and accelerate) designed to simplify building SOTA NLP models. Under the hood, Hugging Face libraries use TensorFlow and PyTorch primitives (collectively known as “backends”) to perform computations. Users can choose which backend to use based on specific runtime requirements. Given its popularity, Amazon SageMaker has recently added support for the Hugging Face libraries in separate prebuilt containers for training and inference tasks.</p>
<p class="callout-heading">Container sources</p>
<p class="callout">Sources of SageMaker DL containers<a id="_idIndexMarker122"/> are available on the public GitHub repository at <a href="https://github.com/aws/deep-learning-containers">https://github.com/aws/deep-learning-containers</a>. In certain cases, it can be helpful to review relevant Dockerfiles to understand the runtime configuration of prebuilt containers. Container images are available in AWS public registries<a id="_idIndexMarker123"/> at <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>.</p>
<p>For each of the supported frameworks, SageMaker provides separate training and inference containers. We have separate containers for these two tasks because of the following considerations:</p>
<ul>
<li>Training and<a id="_idIndexMarker124"/> inference tasks might have different runtime requirements. For example, you might choose to run your training and inference tasks on different compute platforms. This will result in different sets of accelerators and performance optimization tweaks in your container, depending on your specific task.</li>
<li>Training and inference<a id="_idIndexMarker125"/> tasks require different sets of auxiliary scripts; for instance, standing up a model server in the case of inference tasks. Not separating training and inference containers could result in bloated container sizes and intricate APIs.</li>
</ul>
<p>For this reason, we will always explicitly identify the container we are using depending on the specific task.</p>
<p>Specific to DL containers, AWS also defines separate GPU-based and CPU-based containers. GPU-based containers require the installation of additional accelerators to be able to run computations on GPU devices (such as the CUDA toolkit). </p>
<p class="callout-heading">Model requirements</p>
<p class="callout">When choosing a SageMaker DL container, always consider the model requirements for compute resources. For the majority of SOTA models, it’s recommended that you use GPU-based compute instances to achieve acceptable performance. Choose your DL container accordingly.</p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor045"/>TensorFlow containers </h2>
<p>A TensorFlow container<a id="_idIndexMarker126"/> has two major versions: 1.x (maintenance mode) and 2.x (the latest version). Amazon SageMaker supports both versions and provides inference and training containers. In this book, all of the code examples and general commentary are done assuming TensorFlow v2.x.</p>
<p>AWS updates with frequently <a id="_idIndexMarker127"/>supported minor TensorFlow versions. The latest supported major version is 2.10.0.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor046"/>PyTorch containers </h2>
<p>Amazon SageMaker provides <a id="_idIndexMarker128"/>inference and training containers for PyTorch. The latest version is 1.12.1.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor047"/>Hugging Face containers </h2>
<p>AWS provides <a id="_idIndexMarker129"/>Hugging Face containers in two flavors: PyTorch and TensorFlow backends. Each backend has separate training and inference containers.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor048"/>Using SageMaker Python SDK</h2>
<p>AWS provides a convenient Python SDK<a id="_idIndexMarker130"/> that simplifies interactions with supported DL frameworks via the Estimator, Model, and Predictor classes. Each supported framework has a separate module with the implementation of respective classes. For example, here is how you import Predict, Estimator, and Model classes for the PyTorch framework:</p>
<pre class="source-code">
from sagemaker.pytorch.estimator import PyTorch
from sagemaker.pytorch.model import PyTorchModel, PyTorchPredictor</pre>
<p>The following diagram shows the <a id="_idIndexMarker131"/>SageMaker Python SDK workflow:</p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 2.1 – How SageMaker Python SDK works with image URIs  " height="491" src="image/B17519_02_01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – How SageMaker Python SDK works with image URIs </p>
<p>To build a <a id="_idIndexMarker132"/>better intuition, let’s do a quick example of how to run a training job using a PyTorch container with a specific version using SageMaker Python SDK. For a visual overview, please refer to <em class="italic">Figure 2.1</em>:</p>
<ol>
<li>First, we decide which framework to use and import the respective <strong class="source-inline">Pytorch</strong> <strong class="source-inline">estimator</strong> class: <p class="source-code">from sagemaker.pytorch.estimator import PyTorch</p></li>
</ol>
<p>When instantiating the PyTorch <strong class="source-inline">estimator</strong> object, we need to provide several more parameters including the framework version and the Python version:</p>
<p class="source-code">estimator = PyTorch(</p>
<p class="source-code">    entry_point="training_script.py",</p>
<p class="source-code">    framework_version="1.8",</p>
<p class="source-code">    py_version="py3",</p>
<p class="source-code">    role=role,</p>
<p class="source-code">    instance_count=1,</p>
<p class="source-code">    instance_type="ml.p2.xlarge"</p>
<p class="source-code">)</p>
<ol>
<li value="2">When executing this code, SageMaker Python SDK automatically validates user input, including the framework version and the Python version. If the requested container exists, then SageMaker Python SDK retrieves the appropriate container image URI. If there is no container with the requested parameters, SageMaker Python SDK will throw an exception.</li>
<li>During the <strong class="source-inline">fit()</strong> call, a correct container image URI will be provided to the SageMaker API, so the training job will be running inside the SageMaker container with PyTorch v1.8 and Python v3.7 installed. Since we are requesting a GPU-based instance, a<a id="_idIndexMarker133"/> training container with the CUDA toolkit installed will be used:<p class="source-code">estimator.fit()</p></li>
</ol>
<p class="callout-heading">Using custom images</p>
<p class="callout">Please note that if, for some reason, you would prefer to provide a direct URI to your container image, you can do it using the <strong class="source-inline">image_uri</strong> parameter that is supported by the <strong class="source-inline">model</strong> and <strong class="source-inline">estimator</strong> classes. </p>
<p>Now, let’s take a deep dive into SageMaker DL containers, starting with the available prebuilt containers for the TensorFlow, PyTorch, and Hugging Face frameworks.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor049"/>Using SageMaker DL containers</h1>
<p>Amazon SageMaker supports<a id="_idIndexMarker134"/> several container usage patterns. Also, it provides you with Training and Inference Toolkits that simplify using prebuilt containers and developing BYO containers.</p>
<p>In this section, we will learn how to choose the most efficient container usage pattern for your use case and how to use the available SageMaker toolkits to implement it.</p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor050"/>Container usage patterns</h2>
<p>Amazon <a id="_idIndexMarker135"/>SageMaker provides you with the flexibility to choose whether to use prebuilt containers “as is” (known as <strong class="bold">Script Mode</strong>), <strong class="bold">BYO containers</strong>, or modify prebuilt containers.</p>
<p>Typically, the choice of approach is driven by specific model runtime requirements, available resources, and engineering expertise. In the next few subsections, we will discuss when to choose one approach over another.</p>
<h3>Script Mode</h3>
<p>In <a id="_idIndexMarker136"/>script mode, you define which prebuilt container you’d like to use and then provide one or more scripts with the implementation of your training or inference logic. Additionally, you can provide any other dependencies (proprietary or public) that will be exported to the containers.</p>
<p>Both training and inference containers in script mode come with preinstalled toolkits that provide common<a id="_idIndexMarker137"/> functionality such as downloading data to containers and model artifacts, starting jobs, and<a id="_idIndexMarker138"/> others. We will look at further details of the SageMaker <strong class="bold">Inference Toolkit</strong> and <strong class="bold">Training Toolkit</strong> later in this chapter.</p>
<p>Script Mode is suitable for the following scenarios:</p>
<ul>
<li>Prebuilt containers satisfy your runtime requirements, or you can install any dependencies without needing to rebuild the container</li>
<li>You want to minimize the time spent on developing and testing your containers or you don’t <a id="_idIndexMarker139"/>have the required expertise to do so</li>
</ul>
<p>In the following sections, we will review how to prepare your first training and inference scripts and run them on SageMaker in script mode.</p>
<h3>Modifying prebuilt containers</h3>
<p>Another way to <a id="_idIndexMarker140"/>use SageMaker’s prebuilt containers is to modify them. In this case, you will use one of the prebuilt containers as a base image for your custom container.</p>
<p>Modifying prebuilt containers can be beneficial in the following scenarios:</p>
<ul>
<li>You need to add additional dependencies (for instance, ones that need to be compiled from sources) or reconfigure the runtime environment</li>
<li>You want to minimize the development and testing efforts of your container and rely for the most part on the functionality of the base container tested by AWS</li>
</ul>
<p>Please note that when you extend a prebuilt container, you will be responsible for the following aspects:</p>
<ul>
<li>Creating the Dockerfile with the implementation of your runtime environment</li>
<li>Building and storing your container in a Container registry such as <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) or <a id="_idIndexMarker141"/>private Docker<a id="_idIndexMarker142"/> registries</li>
</ul>
<p>Later in this chapter, we see an example of how to extend a prebuilt PyTorch container for a training task.</p>
<h3>BYO containers</h3>
<p>There are many <a id="_idIndexMarker143"/>scenarios in which you might need to create a custom container, such as the following:</p>
<ul>
<li>You have unique runtime requirements that cannot be addressed by extending the prebuilt container</li>
<li>You want to compile frameworks and libraries from sources for specific hardware platforms</li>
<li>You are using DL frameworks that are not supported natively by SageMaker (for instance, JAX)</li>
</ul>
<p>Building a custom container compatible with SageMaker inference and training resources requires development efforts, an understanding of Docker containers, and specific SageMaker requirements. Therefore, it’s usually recommended that you consider script mode or extending a prebuilt container first and choose to use a BYO container only if the first options do not work for your particular use case.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor051"/>SageMaker toolkits</h2>
<p>To simplify the<a id="_idIndexMarker144"/> development of custom scripts and containers that are compatible with Amazon SageMaker, AWS created Python toolkits for training and inference tasks. </p>
<p>Toolkits provide the following benefits:</p>
<ul>
<li>Establish consistent<a id="_idIndexMarker145"/> runtime environments and locations for storing code assets</li>
<li><strong class="source-inline">ENTRYPOINT</strong> scripts to run tasks when the container is started</li>
</ul>
<p>Understanding these toolkits helps to simplify and speed up the development of SageMaker-compatible containers, so let’s review them in detail.</p>
<h3>The Training Toolkit</h3>
<p>The SageMaker Training <a id="_idIndexMarker146"/>Toolkit has several key functions:</p>
<ul>
<li>It establishes a consistent runtime environment, setting environment variables and a directory structure to store the input and output artifacts of model training:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.2 – The directory structure in SageMaker-compatible containers " height="328" src="image/B17519_02_02.jpg" width="742"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – The directory structure in SageMaker-compatible containers</p>
<p>The Training <a id="_idIndexMarker147"/>Toolkit sets up the following directories in the training container:</p>
<ul>
<li>The <strong class="source-inline">/opt/ml/input/config</strong> directory with the model hyperparameters and the network layout used for distributed training as JSON files.</li>
<li>The <strong class="source-inline">/opt/ml/input/data</strong> directory with input data when S3 is used as data storage.</li>
<li>The <strong class="source-inline">/opt/ml/code/</strong> directory, containing code assets to run training job.</li>
<li>The <strong class="source-inline">/opt/ml/model/</strong> directory, containing the resulting model; SageMaker automatically copies it to Amazon S3 after training completion.</li>
</ul>
<ul>
<li>It executes the entrypoint script and handles success and failure statuses. In the case of a training job failure, the output will be stored in <strong class="source-inline">/opt/ml/output/failure</strong>. For successful executions, the toolkit will write output to the <strong class="source-inline">/opt/ml/success</strong> directory.</li>
</ul>
<p>By default, all prebuilt training containers already have a training toolkit installed. If you wish to use it, you will need to install it on your container by running the following:</p>
<pre class="source-code">
RUN pip install sagemaker-training</pre>
<p>Also, you will need to copy all of the code dependencies into your container and define a special environmental variable in your main training script, as follows:</p>
<pre class="source-code">
COPY train_scipt.py /opt/ml/code/train_script.py
ENV SAGEMAKER_PROGRAM train_scipt.py</pre>
<p>The training toolkit <a id="_idIndexMarker148"/>package is available in the PyPI (<a href="http://pypi.org">pypi.org</a>) package <a id="_idIndexMarker149"/>and the SageMaker GitHub repository (<a href="https://github.com/aws/sagemaker-training-toolkit">https://github.com/aws/sagemaker-training-toolkit</a>). </p>
<h3>Inference Toolkit</h3>
<p>The <strong class="bold">Inference Toolkit</strong> implements<a id="_idIndexMarker150"/> a model serving stack that is compatible with SageMaker inference services. It comes together with an <a id="_idIndexMarker151"/>open source <strong class="bold">Multi-Model Server</strong> (<strong class="bold">MMS</strong>) to serve models. It has the following key functions:</p>
<ul>
<li>To establish runtime environments, such as directories to store input and output artifacts of inference and environmental variables. The directory structure follows the layout of the training container.</li>
<li>To implement a handler service that is called from the model server to load the model into memory, and handle model inputs and outputs.</li>
<li>To implement default serializers and deserializers to handle inference requests.</li>
</ul>
<p>The Inference <a id="_idIndexMarker152"/>Toolkit package is available in the PyPi (<a href="http://pypi.org">pypi.org</a>) package and the GitHub repository (<a href="https://github.com/aws/sagemaker-inference-toolkit">https://github.com/aws/sagemaker-inference-toolkit</a>). </p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor052"/>Developing for script mode</h2>
<p>Now that we have an <a id="_idIndexMarker153"/>understanding of SageMaker’s container ecosystem, let’s implement several learning projects to build practical skills. In this first example, we will use SageMaker script mode to train our custom NLP model and deploy it for inference.</p>
<h3>Problem overview</h3>
<p>In this example, we will learn how to<a id="_idIndexMarker154"/> develop training and inference scripts using the Hugging Face framework. We will leverage prebuilt SageMaker containers for Hugging Face (with the PyTorch backend).</p>
<p>We chose to solve a typical NLP task: text classification. We will use the <strong class="source-inline">20 Newsgroups</strong> dataset, which assembles ~20,000 newsgroup documents across 20 different newsgroups (categories). There are a number of model architectures that can address this task. Usually, current SOTA models are based on Transformer architecture. Autoregressive models such as <strong class="bold">BERT</strong> and its <a id="_idIndexMarker155"/>various derivatives are suitable for this task. We will use a concept known<a id="_idIndexMarker156"/> as <strong class="bold">transfer learning</strong>, where a model that is pretrained for one task is used for a new task with minimal modifications.</p>
<p>As a baseline model, we will use model architecture known<a id="_idIndexMarker157"/> as <strong class="bold">DistilBERT</strong>, which provides high accuracy on a wide variety of tasks and is considerably smaller than other models (for instance, the original BERT model). To adapt the model for a classification task, we would need to add a classification layer, which will be trained during our training to recognize articles:</p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 2.3 – The model architecture for the text classification task " height="979" src="image/B17519_02_03.jpg" width="1309"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The model architecture for the text classification task</p>
<p>The Hugging Face Transformers library <a id="_idIndexMarker158"/>simplifies model selection and modification for fine-tuning in the following ways:</p>
<ul>
<li>It provides a rich model zoo with a number of pretrained models and tokenizers</li>
<li>It has a simple model API to modify the baseline model for fine-tuning a specific task</li>
<li>It implements inference pipelines, combining data preprocessing and actual inference together</li>
</ul>
<p>The full source code of this learning project is available at <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter2/1_Using_SageMaker_Script_Mode.ipynb</a>.</p>
<h3>Developing a training script</h3>
<p>When running <a id="_idIndexMarker159"/>SageMaker training jobs, we need to provide a training script. Additionally, we might provide any other dependencies. We can also install or modify Python packages that are installed on prebuilt containers via the <strong class="source-inline">requirements.txt</strong> file.</p>
<p>In this example, we will use a new feature of the Hugging Face framework to fine-tune a multicategory classifier using the Hugging Face Trainer API. Let’s make sure that the training container has the newer Hugging Face Transformer library installed. For this, we create the <strong class="source-inline">requirements.txt</strong> file and specify a minimal compatible version. Later, we will provide this file to our SageMaker training job:</p>
<pre class="source-code">
transformers &gt;= 4.10</pre>
<p>Next, we need to develop the training script. Let’s review some key components of it.</p>
<p>At training time, SageMaker starts training by calling <strong class="source-inline">user_training_script --arg1 value1 --arg2 value2 ...</strong>. Here, <strong class="source-inline">arg1..N</strong> are training hyperparameters and other miscellaneous parameters provided by users as part of training job configuration. To correctly kick off the training process in our script, we need to include <strong class="source-inline">main guard</strong> within our script:</p>
<ol>
<li value="1">To correctly capture the parameters, the training script needs to be able to parse command-line arguments. We use the Python <strong class="source-inline">argparse</strong> library to do this:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">     parser = argparse.ArgumentParser()</p><p class="source-code">     parser.add_argument("--epochs", type=int, default=1)</p><p class="source-code">     parser.add_argument("--per-device-train-batch-size", type=int, default=16)</p><p class="source-code">     parser.add_argument("--per-device-eval-batch-size", type=int, default=64)</p><p class="source-code">     parser.add_argument("--warmup-steps", type=int, default=100)</p><p class="source-code">     parser.add_argument("--logging-steps", type=float, default=100)</p><p class="source-code">     parser.add_argument("--weight-decay", type=float, default=0.01)</p><p class="source-code">     args, _ = parser.parse_known_args()</p><p class="source-code">     train(args)</p></li>
<li>The <strong class="source-inline">train()</strong> method<a id="_idIndexMarker160"/> is responsible for running end-to-end training jobs. It includes the following components:<ul><li>Calling <strong class="source-inline">_get_tokenized_dataset</strong> to load and tokenize datasets using a pretrained DistilBERT tokenizer from the Hugging Face library.</li><li>Loading and configuring the DistilBERT model from the Hugging Face model zoo. Please note that we update the default configuration for classification tasks to adjust for our chosen number of categories.</li><li>Configuring Hugging Face Trainer and starting the training process.</li><li>Once the training is done, we save the trained model:<p class="source-code">def train(args):</p><p class="source-code">    train_enc_dataset, test_enc_dataset = _get_tokenized_data()</p><p class="source-code">    training_args = TrainingArguments(</p><p class="source-code">        output_dir=os.getenv(</p><p class="source-code">            "SM_OUTPUT_DIR", "./"</p><p class="source-code">        ),  # output directory, if runtime is not</p><p class="source-code">        num_train_epochs=args.epochs,</p><p class="source-code">        per_device_train_batch_size=args.per_device_train_batch_size,</p><p class="source-code">        per_device_eval_batch_size=args.per_device_eval_batch_size,</p><p class="source-code">        warmup_steps=args.warmup_steps,</p><p class="source-code">        weight_decay=args.weight_decay,</p><p class="source-code">        logging_steps=args.logging_steps,</p><p class="source-code">    )</p><p class="source-code">    config = DistilBertConfig()</p><p class="source-code">    config.num_labels = NUM_LABELS</p><p class="source-code">    model = DistilBertForSequenceClassification.from_pretrained(</p><p class="source-code">        MODEL_NAME, config=config</p><p class="source-code">    )</p><p class="source-code">    trainer = Trainer(</p><p class="source-code">        model=model,  # model to be trained</p><p class="source-code">        args=training_args,  # training arguments, defined above</p><p class="source-code">        train_dataset=train_enc_dataset,  # training dataset</p><p class="source-code">        eval_dataset=test_enc_dataset,  # evaluation dataset</p><p class="source-code">    )</p><p class="source-code">    trainer.train()</p><p class="source-code">    model.save_pretrained(os.environ["SM_MODEL_DIR"])</p></li></ul></li>
</ol>
<p>So far in our script, we<a id="_idIndexMarker161"/> have covered key aspects: handling configuration settings and model hyperparameters, loading pretrained models, and starting training using the Hugging Face Trainer API.</p>
<h3>Starting the training job</h3>
<p>Once we have<a id="_idIndexMarker162"/> our training script and dependencies ready, we can proceed with the training and schedule a training job via SageMaker Python SDK. We start with the import of the Hugging Face Estimator object and get the IAM execution role for our training job:</p>
<pre class="source-code">
from sagemaker.huggingface.estimator import HuggingFace
from sagemaker import get_execution_role
role=get_execution_role()</pre>
<p>Next, we need to define the hyperparameters of our model and training processes. These variables will be passed to our script at training time:</p>
<pre class="source-code">
hyperparameters = {
    "epochs":1,
    "per-device-train-batch-size":16, 
    "per-device-eval-batch-size":64,
    "warmup-steps":100,
    "logging-steps":100,
    "weight-decay":0.01    
}
estimator = HuggingFace(
    py_version="py36",
    entry_point="train.py",
    source_dir="1_sources",
    pytorch_version="1.7.1",
    transformers_version="4.6.1",
    hyperparameters=hyperparameters,
    instance_type="ml.p2.xlarge",
    instance_count=1,
    role=role
)
estimator.fit({
    "train":train_dataset_uri,
    "test":test_dataset_uri
})</pre>
<p>After that, the<a id="_idIndexMarker163"/> training job will be scheduled and executed. It will take 10–15 minutes for it to complete, then the trained model and other output artifacts will be added to Amazon S3.</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor053"/>Developing an inference script for script mode</h2>
<p>Now that we <a id="_idIndexMarker164"/>have a trained model, let’s deploy it as a <a id="_idIndexMarker165"/>SageMaker real-time endpoint. We will use the prebuilt SageMaker Hugging Face container and will only provide our inference script. The inference requests will be handled by the <strong class="bold">AWS MMS</strong>, which exposes the HTTP endpoint.</p>
<p>When using prebuilt inference containers, SageMaker automatically recognizes our inference script. According to SageMaker convention, the inference script has to contain the following methods:</p>
<ul>
<li><strong class="source-inline">model_fn(model_dir)</strong> is executed at the container start time to load the model into memory. This method takes the model directory as an input argument. You can use <strong class="source-inline">model_fn()</strong> to initialize other components of your inference pipeline, such as the tokenizer in our case. Note, Hugging Face Transformers have a convenient Pipeline API that allows us to combine data preprocessing (in our case, text tokenization) and actual inference in a single object. Hence, instead of a loaded model, we return an inference pipeline:<p class="source-code">MODEL_NAME = "distilbert-base-uncased"</p><p class="source-code">NUM_LABELS = 6 # number of categories</p><p class="source-code">MAX_LENGTH = 512 # max number of tokens model can handle</p><p class="source-code">def model_fn(model_dir):</p><p class="source-code">    device_id = 0 if torch.cuda.is_available() else -1</p><p class="source-code">    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)</p><p class="source-code">    config = DistilBertConfig()</p><p class="source-code">    config.num_labels = NUM_LABELS</p><p class="source-code">    model = DistilBertForSequenceClassification.from_pretrained(</p><p class="source-code">        model_dir, config=config</p><p class="source-code">    )</p><p class="source-code">    inference_pipeline = pipeline(</p><p class="source-code">        model=model,</p><p class="source-code">        task="text-classification",</p><p class="source-code">        tokenizer=tokenizer,</p><p class="source-code">        framework="pt",</p><p class="source-code">        device=device_id,</p><p class="source-code">        max_length=MAX_LENGTH,</p><p class="source-code">        truncation=True</p><p class="source-code">    )</p><p class="source-code">    return inference_pipeline</p></li>
<li><strong class="source-inline">transform_fn(inference_pipeline, data, content_type, accept_type)</strong> is responsible <a id="_idIndexMarker166"/>for running the actual<a id="_idIndexMarker167"/> inference. Since we are communicating with an end client via HTTP, we also need to do payload deserialization and response serialization. In our sample example, we expect a JSON payload and return a JSON payload; however, this can be extended to any other formats based on the requirements (for example, CSV and Protobuf):<p class="source-code">def transform_fn(inference_pipeline, data, content_type, accept_type):</p><p class="source-code">    # Deserialize payload</p><p class="source-code">    if "json" in content_type:</p><p class="source-code">        deser_data = json.loads(data)</p><p class="source-code">    else:</p><p class="source-code">        raise NotImplemented("Only 'application/json' content type is implemented.")</p><p class="source-code">    </p><p class="source-code">    # Run inference</p><p class="source-code">    predictions = inference_pipeline(deser_data)</p><p class="source-code">    </p><p class="source-code">    # Serialize response</p><p class="source-code">    if "json" in accept_type:</p><p class="source-code">        return json.dumps(predictions)</p><p class="source-code">    else:</p><p class="source-code">        raise NotImplemented("Only 'application/json' accept type is implemented.")</p></li>
</ul>
<p>Sometimes, combining <a id="_idIndexMarker168"/>deserialization, inference, and<a id="_idIndexMarker169"/> serialization in a single method can be inconvenient. Alternatively, SageMaker supports a more granular API:</p>
<ul>
<li><strong class="source-inline">input_fn(request_body, request_content_type)</strong> runs deserialization</li>
<li><strong class="source-inline">predict_fn(deser_input, model)</strong> performs predictions</li>
<li><strong class="source-inline">output_fn(prediction, response_content_type)</strong> runs the serialization <a id="_idIndexMarker170"/>of <a id="_idIndexMarker171"/>predictions</li>
</ul>
<p>Note that the <strong class="source-inline">transform_fn()</strong> method is mutually exclusive with the <strong class="source-inline">input_fn()</strong>, <strong class="source-inline">predict_fn()</strong>, and <strong class="source-inline">output_fn()</strong> methods.</p>
<h3>Deploying a Text Classification endpoint</h3>
<p>Now we are ready to deploy and test our <a id="_idIndexMarker172"/>Newsgroup Classification endpoint. We can use the <strong class="source-inline">estimator.create_model()</strong> method to configure our model deployment parameters, specifically the following:</p>
<ol>
<li value="1">Define the inference script and other dependencies that will be uploaded by SageMaker to an endpoint.</li>
<li>Identify the inference container. If you provide the <strong class="source-inline">transformers_version</strong>, <strong class="source-inline">pytorch_version</strong>, and <strong class="source-inline">py_version</strong> parameters, SageMaker will automatically find an appropriate prebuilt inference container (if it exists). Alternatively, you can provide <strong class="source-inline">image_uri</strong> to directly specify the container image you wish to use:<p class="source-code">from sagemaker.huggingface.estimator import HuggingFaceModel</p><p class="source-code">model = estimator.create_model(role=role, </p><p class="source-code">                               entry_point="inference.py", </p><p class="source-code">                               source_dir="1_sources",</p><p class="source-code">                               py_version="py36",</p><p class="source-code">                               transformers_version="4.6.1",</p><p class="source-code">                               pytorch_version="1.7.1"</p><p class="source-code">                              )</p></li>
<li>Next, we define the parameters of our endpoint such as the number and type of instances behind it. The <strong class="source-inline">model.deploy()</strong> method starts the inference deployment (which, usually, takes several minutes) and returns a <strong class="source-inline">Predictor</strong> object to<a id="_idIndexMarker173"/> run inference requests:<p class="source-code">predictor = model.deploy(</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type="ml.m5.xlarge"</p><p class="source-code">)</p></li>
</ol>
<p>Next, let’s explore how to extend pre-built DL containers.</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor054"/>Extending the prebuilt containers</h2>
<p>We will reuse code<a id="_idIndexMarker174"/> assets from the script mode example. However, unlike the previous container, we will modify our runtime environment and install the latest stable Hugging Face Transformer from the GitHub master branch. This modification will be implemented in our custom container image.</p>
<p>First off, we need to identify which base image we will use. AWS has published all of the available DL containers at <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>.</p>
<p>Since we plan to use reinstall from scratch HugggingFace Transformer library anyway, we might choose the PyTorch base image. At the time of writing, the latest PyTorch SageMaker container was <strong class="source-inline">763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04</strong>. Note that this container URI is for the AWS East-1 region and will be different for other AWS regions. Please consult the preceding referenced AWS article on the correct URI for your region.</p>
<p>To build a new container, we will need to perform the following steps:</p>
<ul>
<li>Create a Dockerfile with runtime instructions.</li>
<li>Build the container image locally.</li>
<li>Push the new container<a id="_idIndexMarker175"/> image to the <strong class="bold">container registry</strong>. In this example, we will use ECR as a container registry: a managed service from AWS, which is well integrated into the<a id="_idIndexMarker176"/> SageMaker ecosystem.</li>
</ul>
<p>First, let’s create a Dockerfile for our extended container.</p>
<h3>Developing a Dockerfile for our extended container</h3>
<p>To extend the prebuilt <a id="_idIndexMarker177"/>SageMaker container, we need to <a id="_idIndexMarker178"/>have at least the following components:</p>
<ul>
<li>A SageMaker PyTorch image to use as a base.</li>
<li>The required dependencies installed, such as the latest PyTorch and Hugging Face Transformers from the latest Git master branch.</li>
<li>Copy our training script from the previous example into the container.</li>
<li>Define the <strong class="source-inline">SAGEMAKER_SUBMIT_DIRECTORY</strong> and <strong class="source-inline">SAGEMAKER_PROGRAM</strong> environmental variables, so SageMaker knows which training script to execute when the container starts:<p class="source-code">FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04</p><p class="source-code">RUN pip3 install git+https://github.com/huggingface/transformers </p><p class="source-code">ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code </p><p class="source-code">ENV SAGEMAKER_PROGRAM train.py </p><p class="source-code">COPY 1_sources/train.py $SAGEMAKER_SUBMIT_DIRECTORY/$SAGEMAKER_PROGRAM</p></li>
</ul>
<p>Now we are ready to build and push this container image to ECR. You can find the <strong class="source-inline">bash</strong> script to do this in the chapter repository.</p>
<h3>Scheduling a training job</h3>
<p>Once we have<a id="_idIndexMarker179"/> our extended PyTorch container in ECR, we are ready to execute a SageMaker training job. The training job configuration will be similar to the script mode example with one notable difference: instead of the <strong class="source-inline">HuggingFaceEstimator</strong> object, we will use a generic SageMaker <strong class="source-inline">Estimator</strong> object that allows us to work with custom images. Note that you need to update the <strong class="source-inline">image_uri</strong> parameter with reference to the image URI in your ECR instance. You can find it by navigating to the ECR service on your AWS Console and finding the extended container there:</p>
<pre class="source-code">
from sagemaker.estimator import Estimator
estimator = Estimator(
    image_uri="&lt;UPDATE WITH YOUR IMAGE URI FROM ECR&gt;",
    hyperparameters=hyperparameters,
    instance_type="ml.p2.xlarge",
    instance_count=1,
    role=role
)
estimator.fit({
    "train":train_dataset_uri,
    "test":test_dataset_uri
})</pre>
<p>After completing the training job, we should expect similar training outcomes as those shown in the script mode example.</p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor055"/>Developing a BYO container for inference</h1>
<p>In this section, we<a id="_idIndexMarker180"/> will learn how to build a SageMaker-compatible inference container using an official TensorFlow image, prepare an inference script and model server, and deploy it for inference on SageMaker Hosting.</p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor056"/>Problem overview</h2>
<p>We will develop a <a id="_idIndexMarker181"/>SageMaker-compatible container for inference. We will use the latest official TensorFlow container as a base image and use AWS MMS as a model server. Please note that MMS is one of many ML model serving options that can be used. SageMaker doesn’t have any restrictions on a model server other than that it should serve models on port <strong class="source-inline">8080</strong>.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor057"/>Developing the serving container</h2>
<p>When deploying a serving container<a id="_idIndexMarker182"/> to the endpoint, SageMaker runs the following command:</p>
<p class="source-code">docker run &lt;YOUR BYO IMAGE&gt; serve</p>
<p>To comply with this requirement, it’s recommended that you use the exec format of the <strong class="source-inline">ENTRYPOINT</strong> instruction in your Dockerfile.</p>
<p>Let’s review our BYO Dockerfile:</p>
<ul>
<li>We use the latest TensorFlow container as a base</li>
<li>We install general and SageMaker-specific dependencies</li>
<li>We copy our model serving scripts to the container</li>
<li>We specify <strong class="source-inline">ENTRYPOINT</strong> and the CMD instructions to comply with the SageMaker requirements</li>
</ul>
<p>Now, let’s put it into action:</p>
<ol>
<li value="1">Use the latest official TensorFlow container:<p class="source-code">FROM tensorflow/tensorflow:latest</p></li>
<li>Install Java, as required by MMS and any other common dependencies. </li>
<li>Copy the<a id="_idIndexMarker183"/> entrypoint script to the image:<p class="source-code">COPY 3_sources/src/dockerd_entrypoint.py /usr/local/bin/dockerd-entrypoint.py</p><p class="source-code">RUN chmod +x /usr/local/bin/dockerd-entrypoint.py</p></li>
<li>Copy the default custom service file to handle incoming data and inference requests:<p class="source-code">COPY 3_sources/src/model_handler.py /opt/ml/model/model_handler.py</p><p class="source-code">COPY 3_sources/src/keras_model_loader.py /opt/ml/model/keras_model_loader.py</p></li>
<li>Define an entrypoint script and its default parameters:<p class="source-code">ENTRYPOINT ["python3", "/usr/local/bin/dockerd-entrypoint.py"]</p><p class="source-code">CMD ["serve"]</p></li>
</ol>
<p>In this example, we don’t intend to cover MMS and the development of inference scripts in detail. However, it’s worth highlighting some key script aspects:</p>
<ul>
<li><strong class="source-inline">dockerd_entrypoint.py</strong> is an executable that starts the MMS server when the <strong class="source-inline">serve</strong> argument is passed to it.</li>
<li><strong class="source-inline">model_handler.py</strong> implements model-loading and model-serving logics. Note that the <strong class="source-inline">handle()</strong> method checks whether the model is already loaded into memory. If it’s not, it will load a model into memory once and then proceed to the handling serving request, which includes the following:<ul><li>Deserializing the request payload</li><li>Running predictions</li><li>Serializing<a id="_idIndexMarker184"/> predictions</li></ul></li>
</ul>
<h3>Deploying the SageMaker endpoint</h3>
<p>To schedule the deployment of the<a id="_idIndexMarker185"/> inference endpoint, we use the generic <strong class="source-inline">Model</strong> class from SageMaker Python SDK. Note that since we downloaded the model from a public model zoo, we don’t need to provide a <strong class="source-inline">model_data</strong> parameter (hence, its value is <strong class="source-inline">None</strong>): </p>
<pre class="source-code">
from sagemaker import Model
mms_model = Model(
    image_uri=image_uri,
    model_data=None,
    role=role,
    name=model_name,
    sagemaker_session=session
)
mms_model.deploy(
    initial_instance_count=1,
    instance_type="ml.m5.xlarge", 
    endpoint_name=endpoint_name
)</pre>
<p>It might take several minutes to fully deploy the endpoint and start the model server. Once it’s ready, we can call the endpoint using the <strong class="source-inline">boto3.sagemaker-runtime</strong> client, which allows you to construct the HTTP request and send the inference payload (or image, in our case) to a specific SageMaker endpoint:</p>
<pre class="source-code">
import boto3
client = boto3.client('sagemaker-runtime')
accept_type = "application/json"
content_type = 'image/jpeg'
headers = {'content-type': content_type}
payload = open(test_image, 'rb')
response = client.invoke_endpoint(
    EndpointName=endpoint_name,
    Body=payload,
    ContentType=content_type,
    Accept = accept_type
)
most_likely_label = response['Body'].read()
print(most_likely_label)</pre>
<p>This code will, most likely, return<a id="_idIndexMarker186"/> an object in the image based on model predictions. </p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor058"/>Summary</h1>
<p>In this chapter, we reviewed how SageMaker provides support for the ML and DL frameworks using Docker containers. After reading this chapter, you should now know how to select the most appropriate DL container usage pattern according to your specific use case requirements. We learned about SageMaker toolkits, which simplifies developing SageMaker-compatible containers. In later sections, you gained practical knowledge of how to develop custom containers and scripts for training and inference tasks on Amazon SageMaker.</p>
<p>In the next chapter, we will learn about the SageMaker development environment and how to efficiently develop and troubleshoot your DL code. Additionally, we will learn about DL-specific tools and interfaces that the SageMaker development environment provides to simplify the building, deploying, and monitoring of your DL models.</p>
</div>
</div></body></html>