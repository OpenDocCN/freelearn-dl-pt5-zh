["```py\n# Perceptron Model\ndef perceptron(weights, bias, features):\nreturn mx.nd.dot(features, weights) + bias\n```", "```py\nWeights:\n [[ 0.96975976]\n [-0.52853745]\n [-1.88909 ]\n [ 0.65479124]\n [-0.45481315]\n [ 0.32510808]\n [-1.3002341 ]\n [ 0.3679345 ]\n [ 1.4534262 ]\n [ 0.24154152]\n [ 0.47898006]\n [ 0.96885103]\n [-1.0218245 ]\n [-0.06812762]]\n <NDArray 14x1 @cpu(0)>\n Bias:\n[-0.31868345]\n <NDArray 1 @cpu(0)>\n```", "```py\n2610.2383\n```", "```py\nerror_abs = abs(expected_output - model_output)\n error_perc = error_abs / expected_output * 100\n print(\"Absolute Error:\", error_abs)\n print(\"Relative Error (%):\", error_perc)\n```", "```py\nAbsolute Error: 219289.76171875\nRelative Error (%): 98.82368711976115\n```", "```py\ndef create_regression_network():\n    # MultiLayer Perceptron Model (this time using Gluon)\n    net = mx.gluon.nn.Sequential()\n    net.add(mx.gluon.nn.Dense(128))\n    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n    net.add(mx.gluon.nn.Activation('relu'))\n    net.add(mx.gluon.nn.Dropout(.5))\n    net.add(mx.gluon.nn.Dense(1024))\n    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n    net.add(mx.gluon.nn.Activation('relu'))\n    net.add(mx.gluon.nn.Dropout(.4))\n    net.add(mx.gluon.nn.Dense(128))\n    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n    net.add(mx.gluon.nn.Activation('relu'))\n    net.add(mx.gluon.nn.Dropout(.3))\n    net.add(mx.gluon.nn.Dense(1))\n     return net\n```", "```py\n[...]\nParameters in forward computation graph, duplicate included\n   Total params: 272513\n   Trainable params: 269953\n   Non-trainable params: 2560\nShared params in forward computation graph: 0\nUnique parameters in model: 272513\n```", "```py\nnet.collect_params().initialize(mx.init.MSRAPrelu(), ctx=ctx, force_reinit=True)\n```", "```py\n# Define Loss Function\nloss_fn = mx.gluon.loss.HuberLoss()\n# Define Optimizer and Hyper Parameters\ntrainer = mx.gluon.Trainer(net.collect_params(), \"adam\", {\"learning_rate\": 0.01})\n```", "```py\n# Dataset Split 80/10/10\nfrom sklearn.model_selection import train_test_split\nfull_train_df, test_df = train_test_split(house_df, test_size=0.2, random_state=42)\n# To match correctly 10% size, we use previous size as reference\ntrain_df, val_df = train_test_split(full_train_df, test_size=len(test_df), random_state=42)\n```", "```py\nscaled_input = mx.nd.array([scaled_X_train_onehot_df.values[random_index]])\n# Unscaled Expected Output\nexpected_output = y_test[random_index]\n print(\"Unscaled Expected Output:\", expected_output)\n# Scaled Expected Output\nscaled_expected_output = scaled_y_test[random_index]\n print(\"Scaled Expected Output:\", scaled_expected_output)\n# Model Output (scaled)\n output = net(scaled_input.as_in_context(ctx)).asnumpy()[0]\n print(\"Model Output (scaled):\", output)\n# Unscaled Output\nunscaled_output = sc_y.inverse_transform(output)\n print(\"Unscaled Output:\", unscaled_output)\n# Absolute Error\nabs_error = abs(expected_output - unscaled_output)\n print(\"Absolute error: \", abs_error)\n# Percentage Error\nperc_error = abs_error / expected_output * 100.0\nprint(\"Percentage Error: \", perc_error)\n```", "```py\nUnscaled Expected Output: [380000.]\n Scaled Expected Output: [-0.4304741]\n Model Output (scaled): [-0.45450553]\n Unscaled Output: [370690.]\n Absolute error:  [9310.]\n Percentage Error:  [2.45]\n```", "```py\nMean Absolute Error (MAE): [81103.97]\n```", "```py\nMean Absolute Percentage Error (MAPE): [16.008343]\n```", "```py\nHouses with a predicted price error below 25.0 %: [81.23987971]\n```"]