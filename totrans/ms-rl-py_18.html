<html><head></head><body>
		<div id="_idContainer1639">
			<h1 id="_idParaDest-286"><em class="italic"><a id="_idTextAnchor306"/>Chapter 14</em>: Autonomous Systems</h1>
			<p>So far in the book, we have covered many state-of-the-art algorithms and approaches in reinforcement learning. Now, starting with this chapter, we will see them in action to take on real-world problems! We'll start with robot learning, an important application area for reinforcement learning. To this end, we will train a KUKA robot to grasp objects on a tray using PyBullet physics simulation. We will discuss several ways of solving this hard-exploration problem and solve it both using a manually crafted curriculum as well as using the ALP-GMM algorithm. At the end of the chapter, we will present other simulation libraries for robotics and autonomous driving, which are commonly used to train reinforcement learning agents. </p>
			<p>So, this chapter covers the following:</p>
			<ul>
				<li>Introducing PyBullet</li>
				<li>Getting familiar with the KUKA environment</li>
				<li>Developing strategies to solve the KUKA environment</li>
				<li>Using curriculum learning to train the KUKA robot</li>
				<li>Going beyond PyBullet, into autonomous driving</li>
			</ul>
			<p>This is one of the most challenging and fun areas for reinforcement learning. Let's dive right in!</p>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor307"/>Introducing PyBullet</h1>
			<p>PyBullet is a popular <a id="_idIndexMarker1238"/>high-fidelity physics simulation module for robotics, machine learning, games, and more. It is one of the most commonly used libraries for robot learning using RL, especially in sim-to-real transfer research and applications:</p>
			<div>
				<div id="_idContainer1608" class="IMG---Figure">
					<img src="image/B14160_14_1.jpg" alt="Figure 14.1 – PyBullet environments and visualizations (source: PyBullet GitHub repo)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"><a id="_idTextAnchor308"/>Figure 14.1 – PyBullet environments and visualizations (source: PyBullet GitHub repo)</p>
			<p>PyBullet allows developers to create their own physics simulations. In addition, it has prebuilt environments<a id="_idIndexMarker1239"/> using the OpenAI Gym interface. Some of those environments are shown in <em class="italic">Figure 14.1</em>.</p>
			<p>In the next section, we will set up a virtual environment for PyBullet.</p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor309"/>Setting up PyBullet</h2>
			<p>It is almost always a <a id="_idIndexMarker1240"/>good idea to work in virtual environments for Python projects, which is also what we will do for our robot learning experiments in this chapter. So, let's go ahead and execute the following commands to install the libraries we will use:</p>
			<p class="source-code">$ virtualenv pybenv</p>
			<p class="source-code">$ source pybenv/bin/activate</p>
			<p class="source-code">$ pip install pybullet --upgrade</p>
			<p class="source-code">$ pip install gym</p>
			<p class="source-code">$ pip install tensorflow==2.3.1</p>
			<p class="source-code">$ pip install ray[rllib]==1.0.0</p>
			<p class="source-code">$ pip install scikit-learn==0.23.2</p>
			<p>You can test whether your installation is working by running this:</p>
			<p class="source-code">$ python -m pybullet_envs.examples.enjoy_TF_AntBulletEnv_v0_2017may</p>
			<p>And if everything is working fine, you will see a cool Ant robot wandering around as in <em class="italic">Figure 14.2</em>:</p>
			<div>
				<div id="_idContainer1609" class="IMG---Figure">
					<img src="image/B14160_14_2.jpg" alt="Figure 14.2 – Ant robot walking in PyBullet&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"><a id="_idTextAnchor310"/></p>
			<p class="figure-caption">Figure 14.2 – Ant robot walking in PyBullet</p>
			<p>Great! We are now<a id="_idIndexMarker1241"/> ready to proceed to the KUKA environment that we will use.</p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor311"/>Getting familiar with the KUKA environment</h1>
			<p>KUKA is a company that<a id="_idIndexMarker1242"/> offers industrial robotics solutions, which are widely used in manufacturing and assembly environments. PyBullet includes a simulation of a KUKA robot, used for object-grasping simulations (<em class="italic">Figure 14.3</em>):</p>
			<p class="figure-caption"><a id="_idTextAnchor312"/></p>
			<div>
				<div id="_idContainer1610" class="IMG---Figure">
					<img src="image/B14160_14_3.jpg" alt="Figure 14.3 – KUKA robots are widely used in industry. (a) A real KUKA robot &#13;&#10;(image source CNC Robotics website), (b) a PyBullet simulation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – KUKA robots are widely used in industry. (a) A real KUKA robot (image source CNC Robotics website), (b) a PyBullet simulation</p>
			<p>There are multiple KUKA <a id="_idIndexMarker1243"/>environments in PyBullet, for the following:</p>
			<ul>
				<li>Grasping a rectangular block using robot and object positions and angles</li>
				<li>Grasping a rectangular block using camera inputs</li>
				<li>Grasping random objects using camera/position inputs</li>
			</ul>
			<p>In this chapter, we'll focus on the first one, which we'll look into next in more detail.</p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor313"/>Grasping a rectangular block using a KUKA robot</h2>
			<p>In this <a id="_idIndexMarker1244"/>environment, the goal of the robot is to reach a <a id="_idIndexMarker1245"/>rectangular object, grasp it, and raise it up to a certain height. An example scene from the environment, along with the robot coordinate system, is shown in <em class="italic">Figure 14.4<a id="_idTextAnchor314"/></em>:</p>
			<div>
				<div id="_idContainer1611" class="IMG---Figure">
					<img src="image/B14160_14_4.jpg" alt="Figure 14.4 – Object-grasping scene and the robot coordinate system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Object-grasping scene and the robot coordinate system</p>
			<p>The dynamics and initial <a id="_idIndexMarker1246"/>position of the robot joints are <a id="_idIndexMarker1247"/>defined in the <strong class="source-inline">Kuka</strong> class of the <strong class="source-inline">pybullet_envs</strong> package. We will talk about these details only as much as we need to, but you should feel free to dive into the class definition to better understand the dynamics.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">To better understand the PyBullet environment and how the <strong class="source-inline">Kuka</strong> class is constructed, you can check out the <em class="italic">PyBullet Quickstart Guide</em> at <a href="https://bit.ly/323PjmO">https://bit.ly/323PjmO</a>.</p>
			<p>Let's now dive into the Gym environment created to control this robot inside PyBullet.</p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor315"/>The KUKA Gym environment</h2>
			<p><strong class="source-inline">KukaGymEnv</strong> wraps<a id="_idIndexMarker1248"/> the <strong class="source-inline">Kuka</strong> robot class and turns it into a Gym environment. The action, observation, reward, and terminal conditions are defined as follows.</p>
			<h3>Actions</h3>
			<p>There are three actions<a id="_idIndexMarker1249"/> the agent takes in the environment, which are all about moving the gripper. These actions are as follows:</p>
			<ul>
				<li>Velocity along the <img src="image/Formula_14_001.png" alt=""/> axis</li>
				<li>Velocity along the <img src="image/Formula_14_002.png" alt=""/> axis</li>
				<li>Angular velocity to rotate the gripper (yaw)</li>
			</ul>
			<p>The environment itself moves the gripper along the <img src="image/Formula_14_003.png" alt=""/> axis towards the tray, where the object is located. When it gets sufficiently close to the tray, it closes the fingers of the gripper to try grasping the object.</p>
			<p>The environment can be configured to accept discrete or continuous actions. We will use the latter in our case.</p>
			<h3>Observations</h3>
			<p>The agent receives nine<a id="_idIndexMarker1250"/> observations from the environment:</p>
			<ul>
				<li>Three observations for the <img src="image/Formula_14_004.png" alt=""/>, <img src="image/Formula_14_005.png" alt=""/>, and <img src="image/Formula_14_006.png" alt=""/> positions of the gripper</li>
				<li>Three observations for the Euler angles of the gripper with respect to the <img src="image/Formula_14_007.png" alt=""/>, <img src="image/Formula_14_008.png" alt=""/>, and <img src="image/Formula_14_009.png" alt=""/> axes</li>
				<li>Two observations for the <img src="image/Formula_14_010.png" alt=""/> and <img src="image/Formula_14_011.png" alt=""/> positions of the object <strong class="bold">relative</strong> to the gripper</li>
				<li>One observation for the Euler angle of the object <strong class="bold">relative</strong> to the gripper's Euler angle along<a id="_idIndexMarker1251"/> the <img src="image/Formula_14_012.png" alt=""/> axis</li>
			</ul>
			<h3>Reward</h3>
			<p>The reward for grasping<a id="_idIndexMarker1252"/> the object successfully and lifting it up to a certain height is 10,000 points. Other than that, there is a slight cost that penalizes the distance between the gripper and the object. Additionally, there is also some energy cost from rotating the gripper.</p>
			<h3>Terminal conditions</h3>
			<p>An episode terminates<a id="_idIndexMarker1253"/> after 1,000 steps or after the gripper closes, whichever occurs first.</p>
			<p>The best way to wrap your mind around how the environment works is to actually experiment with it, which is what you will do next. </p>
			<p>This can be done with the following code file: <strong class="source-inline">Chapter14/manual_control_kuka.py.</strong></p>
			<p>This script allows you to control the robot manually. You can use the "gym-like" control mode, where the vertical speed and the gripper finger angles are controlled by the environment. Alternatively, you can choose the non-gym-like mode to exert more control.</p>
			<p>One thing you will notice is that even if you keep the speeds along the <img src="image/Formula_14_013.png" alt=""/> and <img src="image/Formula_14_014.png" alt=""/> axes at zero, in the gym-like control mode, the robot will change its <img src="image/Formula_14_015.png" alt=""/> and <img src="image/Formula_14_016.png" alt=""/> positions while going down. This is because the default speed of the gripper along the <img src="image/Formula_14_017.png" alt=""/> axis is too high. You can actually verify that, in the non-gym-like mode, values below <img src="image/Formula_14_018.png" alt=""/> for <img src="image/Formula_14_019.png" alt=""/> alter the positions on the other axes too much. We will reduce the speed when we customize the environment to alleviate that.</p>
			<p>Now that you are <a id="_idIndexMarker1254"/>familiar with the KUKA environment, let's discuss some alternative strategies to solve it.</p>
			<h1 id="_idParaDest-292"><a id="_idTextAnchor316"/>Developing strategies to solve the KUKA environment </h1>
			<p>The object-grasping <a id="_idIndexMarker1255"/>problem in the environment is a <strong class="bold">hard-exploration</strong> problem, meaning that it is unlikely to stumble upon<a id="_idIndexMarker1256"/> the sparse reward that the agent receives at the end upon grasping the object. Reducing the vertical speed as we will do will make it a bit easier. Still, let's refresh our minds about what strategies we have covered to address these kinds of problems:</p>
			<ul>
				<li><strong class="bold">Reward shaping</strong> is one of the most common <strong class="bold">machine teaching</strong> strategies that we discussed earlier. In some problems, incentivizing the agent towards the goal is very straightforward. In many problems, though, it can be quite painful. So, unless there is an obvious way of doing so, crafting the reward function may just take too much time (and expertise about the problem). Also notice that the original reward function has a component to penalize the distance between the gripper and the object, so the reward is already shaped to some extent. We will not go beyond that in our solution.</li>
				<li><strong class="bold">Curiosity-driven learning</strong> incentivizes the agent to discover new parts of the state space. For this problem, though, we don't need the agent to randomly explore the state space too much as we already have some idea about what it should do. So, we will skip this technique as well.</li>
				<li><strong class="bold">Increasing the entropy of the policy</strong> incentivizes the agent to diversify its actions. The coefficient for this can be set using the <strong class="source-inline">"entropy_coeff"</strong> config inside the PPO trainer of RLlib, which is what we will use. However, our hyperparameter<a id="_idIndexMarker1257"/> search (we will come to it soon) ended up picking this value as zero.</li>
				<li><strong class="bold">Curriculum learning</strong> is perhaps the most suitable approach here. We can identify what makes the problem challenging for the agent, start training it at easy levels, and gradually increase the difficulty.</li>
			</ul>
			<p>So, curriculum learning is what we will leverage to solve this problem. But first, let's identify the dimensions to parametrize the environment to create a curriculum.</p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor317"/>Parametrizing the difficulty of the problem</h2>
			<p>When you experimented <a id="_idIndexMarker1258"/>with the environment, you may have noticed the factors that make the problem difficult:</p>
			<ul>
				<li>The gripper starts too high to discover the correct sequences of actions to grasp the object. So, the robot joint that adjusts the height will be one dimension we will parameterize. It turns out that this is set in the second element of the <strong class="source-inline">jointPositions</strong> array of the <strong class="source-inline">Kuka</strong> class.</li>
				<li>When the gripper is not at its original height, it may get misaligned with the location of the object along the <img src="image/Formula_14_020.png" alt=""/> axis. We will also parametrize the position of the joint that controls this, which is the fourth element of the <strong class="source-inline">jointPositions</strong> array of the <strong class="source-inline">Kuka</strong> class.</li>
				<li>Randomizing the object position is another source of difficulty for the agent, which takes place for the <img src="image/Formula_14_021.png" alt=""/> and <img src="image/Formula_14_022.png" alt=""/> positions as well as the object angle. We will parametrize the degree of randomization between 0 and 100% for each of these components.</li>
				<li>Even when the object is not randomly positioned, its center is not aligned with the default position of the robot on the <img src="image/Formula_14_023.png" alt=""/> axis. We will add some bias to the <img src="image/Formula_14_024.png" alt=""/> position of the object, again parametrized.</li>
			</ul>
			<p>This is great! We <a id="_idIndexMarker1259"/>know what to do, which is a big first step. Now, we can go into curriculum learning!</p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor318"/>Using curriculum learning to train the KUKA robot</h1>
			<p>The first step before <a id="_idIndexMarker1260"/>actually kicking off <a id="_idIndexMarker1261"/>some training is to customize the <strong class="source-inline">Kuka</strong> class as well as <strong class="source-inline">KukaGymEnv</strong> to make them work with the curriculum learning parameters we described above. So, let's do that next.</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor319"/>Customizing the environment for curriculum learning</h2>
			<p>First, we start by<a id="_idIndexMarker1262"/> creating a <strong class="source-inline">CustomKuka</strong> class that inherits the original <strong class="source-inline">Kuka</strong> class of PyBullet. Here is how we do it:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter14/custom_kuka.py</p>
			<ol>
				<li>We first need to create the new class, and accept an additional argument, the <strong class="source-inline">jp_override</strong> dictionary, which stands for <strong class="bold">joint position override</strong>:<p class="source-code">class CustomKuka(Kuka):</p><p class="source-code">    def __init__(self, *args, jp_override=None, **kwargs):</p><p class="source-code">        self.jp_override = jp_override</p><p class="source-code">        super(CustomKuka, self).__init__(*args, **kwargs)</p></li>
				<li>We need this to change the <strong class="source-inline">jointPositions</strong> array set in the <strong class="source-inline">reset</strong> method, which we override:<p class="source-code">    def reset(self):</p><p class="source-code">    ...</p><p class="source-code">        if self.jp_override:</p><p class="source-code">            for j, v in self.jp_override.items():</p><p class="source-code">                j_ix = int(j) - 1</p><p class="source-code">                if j_ix &gt;= 0 and j_ix &lt;= 13:</p><p class="source-code">                    self.jointPositions[j_ix] = v</p><p>Now, it's time <a id="_idIndexMarker1263"/>to create <strong class="source-inline">CustomKukaEnv</strong>.</p></li>
				<li>Create the custom environment that accepts all these parametrization inputs for curriculum learning: <p class="source-code">class CustomKukaEnv(KukaGymEnv):</p><p class="source-code">    def __init__(self, env_config={}):</p><p class="source-code">        renders = env_config.get("renders", False)</p><p class="source-code">        isDiscrete = env_config.get("isDiscrete", False)</p><p class="source-code">        maxSteps = env_config.get("maxSteps", 2000)</p><p class="source-code">        self.rnd_obj_x = env_config.get("rnd_obj_x", 1)</p><p class="source-code">        self.rnd_obj_y = env_config.get("rnd_obj_y", 1)</p><p class="source-code">        self.rnd_obj_ang = env_config.get("rnd_obj_ang", 1)</p><p class="source-code">        self.bias_obj_x = env_config.get("bias_obj_x", 0)</p><p class="source-code">        self.bias_obj_y = env_config.get("bias_obj_y", 0)</p><p class="source-code">        self.bias_obj_ang = env_config.get("bias_obj_ang", 0)</p><p class="source-code">        self.jp_override = env_config.get("jp_override")</p><p class="source-code">        super(CustomKukaEnv, self).__init__(</p><p class="source-code">            renders=renders, isDiscrete=isDiscrete, maxSteps=maxSteps</p><p class="source-code">        )</p><p>Note that we are<a id="_idIndexMarker1264"/> also making it RLlib compatible by accepting <strong class="source-inline">env_config</strong>.</p></li>
				<li>We use the randomization parameters in the <strong class="source-inline">reset</strong> method to override the default amount of randomization in the object position:<p class="source-code">    def reset(self):</p><p class="source-code">        ...</p><p class="source-code">        xpos = 0.55 + self.bias_obj_x + 0.12 * random.random() * self.rnd_obj_x</p><p class="source-code">        ypos = 0 + self.bias_obj_y + 0.2 * random.random() * self.rnd_obj_y</p><p class="source-code">        ang = (</p><p class="source-code">            3.14 * 0.5</p><p class="source-code">            + self.bias_obj_ang</p><p class="source-code">            + 3.1415925438 * random.random() * self.rnd_obj_ang</p><p class="source-code">        )</p></li>
				<li>Also, we should now replace the old <strong class="source-inline">Kuka</strong> class with <strong class="source-inline">CustomKuka</strong> and pass the joint position override input to it:<p class="source-code">        ...</p><p class="source-code">        self._kuka = CustomKuka(</p><p class="source-code">            jp_override=self.jp_override,</p><p class="source-code">            urdfRootPath=self._urdfRoot,</p><p class="source-code">            timeStep=self._timeStep,</p><p class="source-code">        )</p></li>
				<li>Finally, we override <a id="_idIndexMarker1265"/>the <strong class="source-inline">step</strong> method of the environment to decrease the default speed on the <img src="image/Formula_14_025.png" alt=""/> axis:<p class="source-code">    def step(self, action):</p><p class="source-code">        dz = -0.0005</p><p class="source-code">        ...</p><p class="source-code">		...</p><p class="source-code">            realAction = [dx, dy, dz, da, f]</p><p class="source-code">        obs, reward, done, info = self.step2(realAction)</p><p class="source-code">        return obs, reward / 1000, done, info</p><p>Also notice that we rescaled the reward (it will end up between -10 and 10) to make the training easy.</p></li>
			</ol>
			<p>Great job! Next, let's discuss what kind of curriculum to use.</p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor320"/>Designing the lessons in the curriculum </h2>
			<p>It is one thing to<a id="_idIndexMarker1266"/> determine the dimensions to parametrize the difficulty of the problem, and another thing to decide how to expose this parametrization to the agent. We know that the agent should start with easy lessons and move to more difficult ones gradually. This, though, raises some important questions:</p>
			<ul>
				<li>Which parts of the parametrized space are easy?</li>
				<li>What should the step sizes be to change the parameters between lesson transitions? In other words, how should we slice the space into lessons?</li>
				<li>What are the criteria of success for the agent to transition to the next lesson?</li>
				<li>What if the agent fails in a lesson, meaning that its performance is unexpectedly bad? Should it go back to the previous lesson? What is the bar for failure?</li>
				<li>What if the agent cannot transition into the next lesson for a long time? Does it mean that we set the bar for success for the lesson too high? Should we divide that lesson into sub-lessons?</li>
			</ul>
			<p>As you can see, these are non-trivial questions to answer when we are designing a curriculum manually. But <a id="_idIndexMarker1267"/>also remember that in <a href="B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239"><em class="italic">Chapter 11</em></a>, <em class="italic">Generalization and Partial Observability</em>, we introduced the <strong class="bold">Absolute Learning Progress with Gaussian Mixture Models (ALP-GMM)</strong> method, which handles all these decisions for us. Here, we will implement both, starting with a manual curriculum first.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor321"/>Training the agent using a manually designed curriculum</h2>
			<p>We will design a <a id="_idIndexMarker1268"/>rather simple curriculum for this problem. It will transition the agent to subsequent lessons when it meets the success criteria, and fall back to a previous lesson in the event of low performance. The curriculum will be implemented as a method inside the <strong class="source-inline">CustomKukaEnv</strong> class, with the <strong class="source-inline">increase_difficulty</strong> method:</p>
			<ol>
				<li value="1">We start by defining the delta changes in the parameter values during lesson transitions. For the joint values, we will decrease the joint positions from what is entered by the user (easy) to the original values in the environment (difficult):<p class="source-code">    def increase_difficulty(self):</p><p class="source-code">        deltas = {"2": 0.1, "4": 0.1}</p><p class="source-code">        original_values = {"2": 0.413184, "4": -1.589317}</p><p class="source-code">        all_at_original_values = True</p><p class="source-code">        for j in deltas:</p><p class="source-code">            if j in self.jp_override:</p><p class="source-code">                d = deltas[j]</p><p class="source-code">                self.jp_override[j] = max(self.jp_override[j] - d, original_values[j])</p><p class="source-code">                if self.jp_override[j] != original_values[j]:</p><p class="source-code">                    all_at_original_values = False</p></li>
				<li>During each<a id="_idIndexMarker1269"/> lesson transition, we also make sure to increase the randomization of the object position:<p class="source-code">        self.rnd_obj_x = min(self.rnd_obj_x + 0.05, 1)</p><p class="source-code">        self.rnd_obj_y = min(self.rnd_obj_y + 0.05, 1)</p><p class="source-code">        self.rnd_obj_ang = min(self.rnd_obj_ang + 0.05, 1)</p></li>
				<li>Finally, we remember to set the biases to zeros when the object position becomes fully randomized:<p class="source-code">        if self.rnd_obj_x == self.rnd_obj_y == self.rnd_obj_ang == 1:</p><p class="source-code">            if all_at_original_values:</p><p class="source-code">                self.bias_obj_x = 0</p><p class="source-code">                self.bias_obj_y = 0</p><p class="source-code">                self.bias_obj_ang = 0</p></li>
			</ol>
			<p>So far, so good. We<a id="_idIndexMarker1270"/> have almost everything ready to train our agent. One last thing before doing so: let's discuss how to pick the hyperparameters. </p>
			<h3>Hyperparameter selection</h3>
			<p>In order to tune the <a id="_idIndexMarker1271"/>hyperparameters in RLlib, we can use the Ray Tune library. In <a href="B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329"><em class="italic">Chapter 15</em></a>, <em class="italic">Supply Chain Management</em>, we will provide you with an example of how it is done. For now, you can just use the hyperparameters we have picked in <strong class="source-inline">Chapter14/configs.py</strong>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In hard-exploration problems, it may make more sense to tune the hyperparameters for a simple version of the problem. This is because without observing some reasonable rewards, the tuning may not pick a good set of hyperparameter values. After we do an initial tuning in an easy environment setting, the chosen values can be adjusted later in the process if the learning stalls.</p>
			<p>Finally, let's see how we can use the environment we have just created during training with the curriculum we have defined.</p>
			<h4>Training the agent on the curriculum using RLlib</h4>
			<p>To proceed<a id="_idIndexMarker1272"/> with the training, we<a id="_idIndexMarker1273"/> need the following ingredients:</p>
			<ul>
				<li>Initial parameters for the curriculum</li>
				<li>Some criteria to define the success (and failure, if needed)</li>
				<li>A callback function that will execute the lesson transitions</li>
			</ul>
			<p>In the following code snippet, we use the PPO algorithm in RLlib, set the initial parameters, and set the reward threshold (empirically) to <em class="italic">5.5</em> in the callback function that executes<a id="_idIndexMarker1274"/> the<a id="_idIndexMarker1275"/> lesson transitions:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter14/train_ppo_manual_curriculum.py</p>
			<p class="source-code">config["env_config"] = {</p>
			<p class="source-code">    "jp_override": {"2": 1.3, "4": -1}, "rnd_obj_x": 0, </p>
			<p class="source-code">    "rnd_obj_y": 0, "rnd_obj_ang": 0, "bias_obj_y": 0.04}</p>
			<p class="source-code">def on_train_result(info):</p>
			<p class="source-code">    result = info["result"]</p>
			<p class="source-code">    if result["episode_reward_mean"] &gt; 5.5:</p>
			<p class="source-code">        trainer = info["trainer"]</p>
			<p class="source-code">        trainer.workers.foreach_worker(</p>
			<p class="source-code">            lambda ev: ev.foreach_env(lambda env: env.increase_difficulty()))</p>
			<p class="source-code">ray.init()</p>
			<p class="source-code">tune.run("PPO", config=dict(config,</p>
			<p class="source-code">                            **{"env": CustomKukaEnv,</p>
			<p class="source-code">                               "callbacks": {</p>
			<p class="source-code">                          "on_train_result": on_train_result}}</p>
			<p class="source-code">                            ),</p>
			<p class="source-code">          checkpoint_freq=10)</p>
			<p>This should kick off the training and you will see the curriculum learning in action! You will notice that as the agent transitions to the next lesson, its performance will usually drop as the<a id="_idIndexMarker1276"/> environment gets more<a id="_idIndexMarker1277"/> difficult.</p>
			<p>We will look into the results of this training later. Let's now also implement the ALP-GMM algorithm.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor322"/>Curriculum learning using absolute learning progress</h2>
			<p>The ALP-GMM <a id="_idIndexMarker1278"/>method focuses on where the biggest performance change (absolute learning progress) in the parameter space is and generates parameters around that gap. This idea is illustrated in <em class="italic">Figure 14.5<a id="_idTextAnchor323"/></em>:</p>
			<div>
				<div id="_idContainer1637" class="IMG---Figure">
					<img src="image/B14160_14_5.jpg" alt="Figure 14.5 – ALP-GMM generates parameters (tasks) around points, between which the biggest episode reward change is observed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – ALP-GMM generates parameters (tasks) around points, between which the biggest episode reward change is observed</p>
			<p>This way, the learning budget is not spent on the parts of the state space that have already been learned, or on the parts that are too difficult to learn for the current agent.</p>
			<p>After this recap, let's go ahead and implement it. We start by creating a custom environment in which the ALP-GMM algorithm will run.</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter14/custom_kuka.py</p>
			<p>We get the ALP-GMM implementation directly from the source repo accompanying the paper (Portelas et al. 2019) and put it under <strong class="source-inline">Chapter14/alp</strong>. We can then plug that into the new environment we create, <strong class="source-inline">ALPKukaEnv</strong>, the key pieces of which are here:</p>
			<ol>
				<li value="1">We create the class and define all the minimum and maximum values of the parameter space <a id="_idIndexMarker1279"/>we are trying to teach the agent:<p class="source-code">class ALPKukaEnv(CustomKukaEnv):</p><p class="source-code">    def __init__(self, env_config={}):</p><p class="source-code">        ...</p><p class="source-code">        self.mins = [...]</p><p class="source-code">        self.maxs =  [...]</p><p class="source-code">        self.alp = ALPGMM(mins=self.mins, </p><p class="source-code">                     maxs=self.maxs, </p><p class="source-code">                           params={"fit_rate": 20})</p><p class="source-code">        self.task = None</p><p class="source-code">        self.last_episode_reward = None</p><p class="source-code">        self.episode_reward = 0</p><p class="source-code">        super(ALPKukaEnv, self).__init__(env_config)</p><p>Here, the task is the latest sample from the parameter space generated by the ALP-GMM algorithm to configure the environment.</p></li>
				<li>A task is sampled at the beginning of each episode. Once an episode finishes, the task (environment parameters used in the episode) and the episode reward are used to update the GMM model:<p class="source-code">    def reset(self):</p><p class="source-code">        if self.task is not None and self.last_episode_reward is not None:</p><p class="source-code">            self.alp.update(self.task, </p><p class="source-code">                            self.last_episode_reward)</p><p class="source-code">        self.task = self.alp.sample_task()</p><p class="source-code">        self.rnd_obj_x = self.task[0]</p><p class="source-code">        self.rnd_obj_y = self.task[1]</p><p class="source-code">        self.rnd_obj_ang = self.task[2]</p><p class="source-code">        self.jp_override = {"2": self.task[3], </p><p class="source-code">                            "4": self.task[4]}</p><p class="source-code">        self.bias_obj_y = self.task[5]</p><p class="source-code">        return super(ALPKukaEnv, self).reset()</p></li>
				<li>And finally, we <a id="_idIndexMarker1280"/>make sure to keep track of the episode reward:<p class="source-code">    def step(self, action):</p><p class="source-code">        obs, reward, done, info = super(ALPKukaEnv,  self).step(action)</p><p class="source-code">        self.episode_reward += reward</p><p class="source-code">        if done:</p><p class="source-code">            self.last_episode_reward = self.episode_reward</p><p class="source-code">            self.episode_reward = 0</p><p class="source-code">        return obs, reward, done, info</p></li>
			</ol>
			<p>One thing to note here is that ALP-GMM is normally implemented in a centralized fashion: a central process generates all the tasks for the rollout workers and collects the episode rewards associated with those tasks to process. Here, since we are working in RLlib, it is easier to implement it inside the environment instances. In order to account for the reduced amount of data collected in a single rollout, we used <strong class="source-inline">"fit_rate": 20</strong>, lower than the original level of 250, so that a rollout worker doesn't wait too long before it fits a GMM to the task-reward data it collects.</p>
			<p>After creating <strong class="source-inline">ALPKukaEnv</strong>, the rest is just a simple call of the Ray <strong class="source-inline">tune.run()</strong> function, which is available in <strong class="source-inline">Chapter14/train_ppo_alp.py</strong>. Note that, unlike in a manual curriculum, we don't specify the initial values of the parameters. Instead, we have passed<a id="_idIndexMarker1281"/> the bounds of the ALP-GMM processes, and they guide the curriculum within those bounds.</p>
			<p>Now, we are ready to do a curriculum learning bake-off!</p>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor324"/>Comparing the experiment results</h2>
			<p>We kick off three training <a id="_idIndexMarker1282"/>sessions using the manual curriculum we described, the ALP-GMM one, and one without any curriculum implemented. The TensorBoard view of the training progress is shown in <em class="italic">Figure 14<a id="_idTextAnchor325"/>.6</em>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer1638" class="IMG---Figure">
					<img src="image/B14160_14_6.jpg" alt="Figure 14.6 – Training progress on TensorBoard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.6 – Training progress on TensorBoard</p>
			<p>A first glance might tell you that the manual curriculum and ALP-GMM are close, while not using a curriculum is a distant third. Actually, this is not the case. Let's unpack this plot:</p>
			<ul>
				<li>The manual curriculum goes from easy to difficult. That is why it is at the top most of the time. In our run, it could not even get to the latest lesson within the time budget. Therefore, the performance shown in the figure is inflated for the manual curriculum.</li>
				<li>The no-curriculum training is always competing at the most difficult level. That is why it is at the bottom most of the time: The other agents are not running against the hardest parameter configurations so they slowly get there.</li>
				<li>ALP-GMM is in the middle for the most part, because it is experimenting with difficult and hard configurations at the same time while focusing on somewhere in between.<p>Since this plot is inconclusive, we evaluate the agents on the original (most difficult) configuration. The results are the following after 100 test episodes for each:</p><p class="source-code"><strong class="bold">Agent ALP-GMM score: 3.71</strong></p><p class="source-code"><strong class="bold">Agent Manual score: -2.79</strong></p><p class="source-code"><strong class="bold">Agent No Curriculum score: 2.81</strong></p></li>
				<li>The manual <a id="_idIndexMarker1283"/>curriculum performed the worst as it could not get to the latest lesson as of the end of the training. </li>
				<li>The no-curriculum training had some success, but starting with the most difficult setting seems to have set it back. Also, the evaluation performance is in line with what is shown on TensorBoard, since the evaluation settings are no different from the training settings in this case.</li>
				<li>ALP-GMM seems to have benefited from gradually increasing the difficulty and performs the best. </li>
				<li>The no-curriculum training's peak point on the TensorBoard graph is similar to ALP-GMM's latest performance. So, our modification with respect to the vertical speed of the robot diminished the difference between the two. Not using a curriculum, however, causes the agent to not learn at all in many hard-exploration scenarios.</li>
			</ul>
			<p>You can find the code for the evaluation in <strong class="source-inline">Chapter14/evaluate_ppo.py</strong>. Also, you can use the script <strong class="source-inline">Chapter14/visualize_policy.py</strong> to watch your trained agents in action, see where they fall short, and come up with ideas to improve the performance!</p>
			<p>This concludes our <a id="_idIndexMarker1284"/>discussion on the KUKA example of robot learning. In the next section, we will wrap up this chapter with a list of some popular simulation environments used to train autonomous robots and vehicles.</p>
			<h1 id="_idParaDest-300"><a id="_idTextAnchor326"/>Going beyond PyBullet into autonomous driving</h1>
			<p>PyBullet is a great <a id="_idIndexMarker1285"/>environment to test the capabilities of reinforcement learning algorithms in a high-fidelity physics simulation. Some of the other libraries you will come across at the<a id="_idIndexMarker1286"/> intersection<a id="_idIndexMarker1287"/> of<a id="_idIndexMarker1288"/> robotics and reinforcement learning are as follows:</p>
			<ul>
				<li>Gazebo: <a href="http://gazebosim.org/">http://gazebosim.org/</a>.</li>
				<li>MuJoCo (requires a license): <a href="http://www.mujoco.org/">http://www.mujoco.org/</a>.</li>
				<li>Adroit: <a href="https://github.com/vikashplus/Adroit">https://github.com/vikashplus/Adroit</a>.</li>
			</ul>
			<p>In addition, you will see Unity and Unreal Engine-based environments used to train reinforcement learning agents.</p>
			<p>The next and more popular level of autonomy is of course autonomous vehicles. RL is increasingly experimented with in realistic autonomous vehicle simulations as well. The most popular <a id="_idIndexMarker1289"/>libraries<a id="_idIndexMarker1290"/> in this area are these:</p>
			<ul>
				<li>CARLA: <a href="https://github.com/carla-simulator/carla">https://github.com/carla-simulator/carla</a>.</li>
				<li>AirSim: <a href="https://github.com/microsoft/AirSim">https://github.com/microsoft/AirSim</a>. (Disclaimer: The author is a Microsoft employee at the time of authoring this book and part of the organization developing AirSim.)</li>
			</ul>
			<p>With this, we conclude this chapter on robot learning. This is a very hot application area in RL, and there are many environments I hope you enjoyed and are inspired to start tinkering with robots.</p>
			<h1 id="_idParaDest-301"><a id="_idTextAnchor327"/>Summary</h1>
			<p>Autonomous robots and vehicles are going to play a huge role in our world in the future, and reinforcement learning is one of the primary approaches to create such autonomous systems. In this chapter, we have taken a peek at what it looks like to train a robot to accomplish an object-grasping task, a major challenge in robotics with many applications in manufacturing and material handling in warehouses. We used the PyBullet physics simulator to train a KUKA robot in a hard-exploration setting, for which we used both manual and ALP-GMM-based curriculum learning. Now that you have a fairly good grasp of how to utilize these techniques, you can take on other similar problems.</p>
			<p>In the next chapter, we will look into another major area for reinforcement learning applications: supply chain management. Stay tuned for another exciting journey! </p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor328"/>References </h1>
			<ul>
				<li>Coumans, E., Bai, Y. (2016-2019). PyBullet, a Python module for physics simulation for games, robotics, and machine learning. URL: <a href="http://pybullet.org">http://pybullet.org</a>.</li>
				<li>Bulletphysics/Bullet3. (2020). Bullet Physics SDK, GitHub. URL: <a href="https://github.com/bulletphysics/bullet3">https://github.com/bulletphysics/bullet3</a>.</li>
				<li>CNC Robotics. (2018). KUKA Industrial Robots, Robotic Specialists. URL: <a href="https://www.cncrobotics.co.uk/news/kuka-robots/">https://www.cncrobotics.co.uk/news/kuka-robots/</a>.</li>
				<li>KUKA AG. (2020). URL: <a href="https://www.kuka.com/en-us">https://www.kuka.com/en-us</a>.</li>
				<li>Portelas, Rémy, et al. (2019). Teacher Algorithms for Curriculum Learning of Deep RL in Continuously Parameterized Environments. ArXiv:1910.07224 [Cs, Stat], Oct. 2019. arXiv.org, <a href="http://arxiv.org/abs/1910.07224">http://arxiv.org/abs/1910.07224</a>.</li>
			</ul>
		</div>
	</body></html>