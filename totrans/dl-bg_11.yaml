- en: Variational Autoencoders
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can be really powerful in finding rich latent spaces. They are
    almost magical, right? What if we told you that **variational autoencoders** (**VAEs**)
    are even more impressive? Well, they are. They have inherited all the nice things
    about traditional autoencoders and added the ability to generate data from a parametric
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the philosophy behind generative models in
    the unsupervised deep learning field and their importance in the production of
    new data. We will present the VAE as a better alternative to a deep autoencoder.
    At the end of this chapter, you will know where VAEs come from and what their
    purpose is. You will be able to see the difference between deep and shallow VAE
    models and you will be able to appreciate the generative property of VAEs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing deep generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the VAE model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing deep and shallow VAEs on MNIST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking about the ethical implications of generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing deep generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning has very interesting contributions to the general machine learning
    community, particularly when it comes to deep discriminative and generative models.
    We are familiar with what a discriminative model is—for example, a **Multilayer
    Perceptron** (**MLP**) is one. In a discriminative model, we are tasked with guessing,
    predicting, or approximating a desired target, [![](img/2c8c7283-b742-4625-b194-d682a0dae077.png)],
    given input data *![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png)*. In statistical
    theory terms, we are modeling the conditional probability density function, ![](img/4af3a90c-1525-4445-a4ec-ddade5cbafce.png).
    On the other hand, by a generative model, this is what most people mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A model that can generate data ![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png) that
    follows a particular distribution based on an input or stimulus ![](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png).*'
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, we can build a neural network that can model this generative
    process very well. In statistical terms, the neural model approximates the conditional
    probability density function, ![](img/8c63f596-488c-4e73-a7aa-7e08c2fa3fac.png). While
    there are several generative models today, in this book, we will talk about three
    in particular.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will talk about VAEs, which are discussed in the next section. Second, [Chapter
    10](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml), *Restricted Boltzmann Machines*,
    will introduce a graphical approach and its properties (Salakhutdinov, R., et
    al. (2007)). The last approach will be covered in [Chapter 14](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml), *Generative
    Adversarial Networks.* These networks are changing the way we think about model
    robustness and data generation (Goodfellow, I., *et al.* (2014)).
  prefs: []
  type: TYPE_NORMAL
- en: Examining the VAE model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VAE is a particular type of autoencoder (Kingma, D. P., & Welling, M. (2013)).
    It learns specific statistical properties of the dataset derived from a Bayesian
    approach. First, let''s define [![](img/83389b96-e277-4cf3-8778-7c1450d525ac.png)] as
    the prior probability density function of a random latent variable, [![](img/52cbb884-6c5d-4cfc-9d26-f0eb85c61622.png)].
    Then, we can describe a conditional probability density function, [![](img/a04acbe7-4f47-4fd3-9d94-2a97e8e478b8.png)],
    which can be interpreted as a model that can produce data—say, [![](img/7b2a4e2f-4402-4293-b890-0ab54f1f0694.png)].
    It follows that we can approximate the posterior probability density function
    in terms of the conditional and prior distributions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab9cec77-1f80-41d4-96d0-67a90fd91a22.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that an exact posterior is intractable, but this problem can be
    solved, approximately, by making a few assumptions and using an interesting idea
    to compute gradients. To begin with, the prior can be assumed to follow an isotropic
    Gaussian distribution, ![](img/8732332a-8166-4cbc-b4cf-fca9b6f5b0c9.png). We can
    also assume that the conditional distribution, ![](img/dc0906a5-b6fc-4f7e-b632-939b6c8585ca.png), can
    be parametrized and modeled using a neural network; that is, given a latent vector
    ![](img/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png), we use a neural network to
    *generate* ![](img/de19cd96-7891-4682-a488-d8f832e9aeee.png). The weights of the
    network, in this case, are denoted as  [![](img/29d99b0a-5807-4f74-8a55-16b1a1745334.png)],
    and the network would be the equivalent of the *decoder* network. The choice of
    the parametric distribution could be Gaussian, for outputs where ![](img/cac94549-d340-466d-b3d0-6e71f6c823f4.png) can
    take on a wide variety of values, or Bernoulli, if the output ![](img/36b66f81-9b91-46e6-9cc5-c91817249c39.png) is
    likely to be binary (or Boolean) values. Next, we must go back again to another
    neural network to approximate the posterior, using [![](img/80b72202-e128-44ce-ad05-ace625e90357.png)] with
    separate parameters, [![](img/160f2e57-8c5d-4a98-9a12-faf7e05313f1.png)]. This
    network can be interpreted as the *encoder* network that takes ![](img/913f83a9-0a47-4629-b4ea-7d16124233b3.png) as
    input and generates the latent variable ![](img/f8da3963-6d86-4faf-90ad-0ce4598b25f8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under this assumption, we can define a loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
  prefs: []
  type: TYPE_IMG
- en: The full derivation can be followed in Kingma, D. P., & Welling, M. (2013).
    However, in short, we can say that in the first term, ![](img/6c8220c0-ab8a-49f3-8984-c004e5782ac5.png) is
    the Kullback–Leibler divergence function, which aims to measure how different
    the distribution of the prior is, [![](img/110e5ad7-4d3b-42f3-bd5a-c048eefa31b3.png)],
    with respect to the distribution of the posterior, [![](img/68e048a8-5c70-4a2a-b51e-9ded415b10bd.png)].
    This happens in the *encoder*, and we want to make sure that the prior and posterior
    of ![](img/f1af68c2-2332-4405-bd6f-16ec408d80c3.png) are a close match. The second
    term is related to the decoder network, which aims to minimize the reconstruction
    loss based on the negative log likelihood of the conditional distribution, ![](img/84214a05-6ba7-43ae-9981-1c3469e9407a.png),
    with respect to the expectation of the posterior, ![](img/4e736730-4af1-4524-b200-f64746e7e823.png).
  prefs: []
  type: TYPE_NORMAL
- en: One last trick to make the VAE learn through gradient descent is to use an idea
    called **re-parameterization***.* This trick is necessary because it is impossible
    to encode one sample, ![](img/82260ac8-99f8-4fb4-9f7d-509ac5aefc7c.png), to approximate
    an isotropic Gaussian with 0 mean and some variance, and draw a sample ![](img/79de787c-640f-439e-80f7-bbb776fdf0b1.png)
    from that distribution, pause there, and then go on to decode that and calculate
    the gradients, and then go back and make updates. The re-parametrization trick
    is simply a method for generating samples from [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)],
    while at the same time, it allows gradient calculation. If we say that [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)],
    we can express the random variable ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png) in
    terms of an auxiliary variable, ![](img/d20928b8-47fc-495e-8597-0c476e80bb92.png),
    with marginal probability density function, [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)],
    such that, [![](img/04fa784b-ebe5-4b9c-9b22-2506e12654a9.png)] and [![](img/c07036e2-7876-4882-a7df-c93fcdd8a6b2.png)] is
    a function parameterized by [![](img/dbeee19b-aca5-4a8f-b802-64dcd61ec13e.png)] and
    returns a vector. This allows gradient calculations on parameters [![](img/6dc7faab-b474-45db-87c2-62cc0e4a321b.png)] (generator
    or decoder) and updates on both [![](img/5635e398-101a-4836-9b24-2b94084c779c.png)] and [![](img/456dbd6d-36b0-42c9-8457-3124c7245a53.png)] using
    any available gradient descent method.
  prefs: []
  type: TYPE_NORMAL
- en: The *tilde* sign (~) in the [![](img/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png)] equation
    can be interpreted as *follows the distribution of*. Thus, the equation can be
    read as ![](img/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png) follows the distribution
    of the posterior, [![](img/a5b1d977-263d-4bda-ba53-00039ad43c93.png)].
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.1* depicts the VAE architecture, explicitly showing the pieces involved
    in the *bottlenecks* and how the pieces of the network are interpreted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6f80791-6557-4162-a6b7-642b28784a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – VAE architecture
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows that in an ideal VAE, the parameters of the distributions
    are learned precisely and perfectly to achieve exact reconstruction. However,
    this is just an illustration, and in practice, perfect reconstruction can be difficult
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bottlenecks **are the latent representations or parameters that are found
    in neural networks that go from layers with large numbers of neural units to layers
    with a decreasing number of neural units. These bottlenecks are known to produce
    interesting feature spaces (Zhang, Y., *et al.* (2014)).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's prepare to make our first VAE piece by piece. We will begin by describing
    the dataset we will use.
  prefs: []
  type: TYPE_NORMAL
- en: The heart disease dataset revisited
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*,
    we described in full the properties of a dataset called the **Cleveland Heart
    Disease** dataset. A screenshot of two columns from this dataset is depicted in
    *Figure 9.2*. Here, we will revisit this dataset with the purpose of reducing
    the original 13 dimensions of the data down to only two dimensions. Not only that,
    but we will also try to produce new data from the generator—that is, the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e25e8f1-e950-4fd6-a7ec-9ac75ecdd0cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The Cleveland Heart Disease dataset samples on two columns
  prefs: []
  type: TYPE_NORMAL
- en: Our attempts to perform dimensionality reduction can be easily justified by
    looking at *Figures 3.8* and *3.9* in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*, and noticing that the data can possibly be processed so as to
    see whether a neural network can cause the data associated with hearts with no
    disease to cluster separately from the rest. Similarly, we can justify the generation
    of new data given that the dataset itself only contains 303 samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the data, we can simply run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to load the data into a data frame and separate the training data and
    the targets, we can execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The next thing we will need to do is to code the re-parametrization trick so
    that we can sample random noise during training.
  prefs: []
  type: TYPE_NORMAL
- en: The re-parametrization trick and sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that the re-parametrization trick aims to sample from [![](img/800b5609-ccd9-447d-8da9-add870f8b803.png)] instead
    of [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)]. Also, recall the distribution
    [![](img/65ff4f78-8d25-4b17-a877-3775474be6a7.png)]. This will allow us to make
    the learning algorithm learn the parameters of [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)]—that
    is, [![](img/3ab31204-0e3f-44a5-97b3-e0829a9a29c0.png)]—and we simply produce
    a sample from [![](img/57985506-28ec-4d9a-9bdb-499ecfa47d83.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we can generate the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `sampling()` method receives the mean and log variance of ![](img/c8dc8061-6987-435e-a863-5b8b836b864d.png) (which
    are to be learned), and returns a vector that is sampled from this parametrized
    distribution; ![](img/e1c06fa7-5aa1-40cb-8903-2ba1bd71dcac.png) is just random
    noise from a Gaussian (`random_normal`) distribution with 0 mean and unit variance.
    To make this method fully compatible with mini-batch training, the samples are
    generated according to the size of the mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the posterior's parameters in the encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The posterior distribution, [![](img/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png)],
    is intractable by itself, but since we are using the re-parametrization trick,
    we can actually perform sampling based on ![](img/18a8f5d8-cb9e-484d-8786-64bc62d3bb16.png).
    We will now make a simple encoder to learn these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For numerical stability, we need to scale the input to make it have 0 mean
    and unit variance. For this, we can invoke the methodology learned in [Chapter
    3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `x_train` matrix contains the scaled training data. The following variables
    will also be useful for designing the encoder of the VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These variables are straightforward, except that the batch size is in terms
    of the square root of the number of samples. This is an empirical value found
    to be a good start, but on larger datasets, it is not guaranteed to be the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can build the encoder portion, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach of encoder utilizes the `Lambda` class, which is part of the `tensorflow.keras.layers`
    collection. This allows us to use the previously defined `sampling()` method (or
    any arbitrary expression, really) as a layer object. *Figure 9.3* illustrates
    the architecture of the full VAE, including the layers of the encoder described
    in the preceding code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5540457-c546-4297-8aa3-3166a1987073.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – VAE architecture for the Cleveland Heart Disease dataset
  prefs: []
  type: TYPE_NORMAL
- en: The encoder uses batch normalization, followed by **dropout** at the input layers,
    followed by a dense layer that has **Tanh activation** and **dropout**. From the
    **dropout**, two dense layers are in charge of modeling the parameters of the
    distribution of the latent variable, and a sample is drawn from this parametrized
    distribution. The decoder network is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The decoder portion of the VAE is very much standard with respect to what you
    already know about an autoencoder. The decoder takes the latent variable, which
    in the VAE was produced by a parametric distribution, and then it should reconstruct
    the input exactly. The decoder can be specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we simply connect two dense layers—the first contains
    a ReLU activation, while the second has linear activations in order to map back
    to the input space. Finally, we can define the complete VAE in terms of inputs
    and outputs as defined in the encoder and decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The VAE model is completed as described here and depicted in *Figure 9.3*. The
    next steps leading to the training of this model include the definition of a loss
    function, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We explained earlier that the loss function needs to be in terms of the encoder
    and decoder; this is the equation we discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to code this loss, we will need to code it in terms of something
    more practical. Applying all the previous assumptions made on the problem, including
    the re-parametrization trick, allows us to rewrite an approximation of the loss
    in simpler terms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bddf69c-11f1-415d-be2a-c1766f4a885e.png)'
  prefs: []
  type: TYPE_IMG
- en: This is for all samples of ![](img/71d43944-64da-4574-b03d-25570e0ecc86.png) where [![](img/445b73e4-f1f8-49d6-953f-301b73f27395.png)] and
    [![](img/ce34c93c-41c6-4549-99f3-1c8eb8a3152c.png)]. Furthermore, the decoder
    loss portion can be approximated using any of your favorite reconstruction losses—for
    example, the **mean squared error** (**MSE**) loss or the binary cross-entropy
    loss. It has been proven that minimizing any of these losses will also minimize
    the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the reconstruction loss in terms of the MSE, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we could do so with the binary cross-entropy loss, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'One additional thing we could do, which is optional, is to monitor how important
    the reconstruction loss is in comparison to the KL-divergence loss (the term related
    to the encoder). One typical thing to do is to make the reconstruction loss be
    multiplied by either the latent dimension or by the input dimension. This effectively
    makes the loss larger by that factor. If we do the latter, we can penalize the
    reconstruction loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The KL-divergence loss for the encoder term can now be expressed in terms of
    the mean and variance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we can simply add the overall loss to the model, which becomes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With this, we are good to go ahead and compile the model and train it, as explained
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Training a VAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The finishing touch is to compile the VAE model, which will put all the pieces
    together. During the compilation, we will choose an optimizer (the gradient descent
    method). In this case, we will choose *Adam* (Kingma, D. P., *et al.* (2014)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun fact: the creator of the VAE is the same person who soon after created
    Adam. His name is **Diederik P. Kingma** and he is currently a research scientist
    at Google Brain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile the model and choose the optimizer, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we train with training data for 500 epochs, using a batch size of
    18, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we are using the training set as the validation set. This is not
    recommended in most settings, but here, it works because the chance of selecting
    identical mini-batches for training and validation is very low. Furthermore, it
    would usually be considered cheating to do that; however, the latent representation
    used in the reconstruction does not directly come from the input; rather, it comes
    from a distribution similar to the input data. To demonstrate that the training
    and validation sets yield different results, we plot the training progress across
    epochs, as shown in *Figure 9.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bced4db-4f13-4517-8210-33d5982e7a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – VAE training performance across epochs
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure not only indicates that the model converges quickly, but
    it also shows that the model does not overfit on the input data. This is a nice
    property to have, usually.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.4* can be produced with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: However, note that the results may vary due to the unsupervised nature of the
    VAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, if we take a look at the latent representation that is produced by sampling
    from a random distribution using the parameters that were learned during training,
    we can see what the data looks like. *Figure 9.5* depicts the latent representations
    obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e59a5b95-6a1a-4f10-8cc4-6f568df25c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – VAE's sampled latent representation in two dimensions
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure clearly suggests that the data corresponding to no indication of
    heart disease is clustered on the left quadrants, while the samples corresponding
    to heart disease are clustered on the right quadrant of the latent space. The
    histogram shown on the top of the figure suggests the presence of two well-defined
    clusters. This is great! Also, recall that the VAE does not know anything about
    labels: we can''t stress this enough! Compare *Figure 9.5* here with *Figure 3.9*
    in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data*,
    and you will notice that the performance of the VAE is superior to KPCA. Furthermore,
    compare this figure with *Figure 3.8* in [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data,*
    and notice that the performance of the VAE is comparable (if not better) than
    **Linear Discriminant Analysis** (**LDA**), which uses label information to produce
    low-dimensional representations. In other words, LDA cheats a little bit.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most interesting properties of the VAE is that we can generate data;
    let's go ahead and see how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Generating data from the VAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the VAE learns the parameters of a parametric distribution over the latent
    space, which is sampled to reconstruct the input data back, we can use those parameters
    to draw more samples and reconstruct them. The idea is to generate data for whatever
    purposes we have in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by encoding the original dataset and see how close the reconstruction is to
    the original. Then, generating data should be straightforward. To encode the input
    data into the latent space and decode it, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Recall that `x_train` is ![](img/b0c5b964-8583-4007-8807-e18c4b9522ef.png) and
    `x_hat` is the reconstruction, ![](img/6476b534-a425-4523-a577-baeeb7b34df3.png).
    Notice that we are using `encdd[0]` as the input for the decoder. The reason for
    this is that the encoder yields a list of three vectors, `[z_mean, z_log_var,
    z]`. Therefore, to use the 0 element in the list is to refer to the mean of the
    distribution corresponding to the samples. In fact, `encdd[0][10]` would yield
    a two-dimensional vector corresponding to the mean parameter of the distribution
    that can produce the 10^(th) sample in the dataset—that is, `x_train[10]`. If
    you think about it, the mean could be the best latent representation that we can
    find since it would be the most likely to reconstruct the input in the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, we can take a look at how good the reconstruction is by
    running something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If the output shows scientific notation that is difficult to read, try disabling
    it temporarily like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.set_printoptions(suppress=True)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`print(np.around(scaler.inverse_transform(x_train[0]), decimals=1))`'
  prefs: []
  type: TYPE_NORMAL
- en: '`print(np.around(scaler.inverse_transform(x_hat[0]), decimals=1))`'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.set_printoptions(suppress=False)`'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are focusing on the first data point in the training set—that
    is, `x_train[0]`—in the top row; its reconstruction is the bottom row. Close examination
    reveals that there are differences between both; however, these differences might
    be relatively small in terms of the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect to point out here is that the data needs to be scaled
    back to its original input space since it was scaled prior to its use in training
    the model. Fortunately, the `StandardScaler()` class has an `inverse_transform()`
    method that can help in mapping any reconstruction back to the range of values
    of each dimension in input space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate more data at will, we can define a method to do so. The
    following method produces random noise, uniformly, in the range `[-2, +2]`, which
    comes from an examination of *Figure 9.5*, which shows the range of the latent
    space to be within such range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This function would need to be adjusted according to the range of values in
    the latent space; also, it can be adjusted by looking at the distribution of the
    data in the latent space. For example, if the latent space seems to be normally
    distributed, then a normal distribution can be used like this: `noise = np.random.normal(0.0,
    1.0, (N,latent_dim))`, assuming 0 mean and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call the function to generate *fake* data by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Recall that this data is generated from random noise. You can see how this is
    a major breakthrough in the deep learning community. You can use this data to
    augment your dataset and produce as many samples as you wish. We can look at the
    quality of the generated samples and decide for ourselves whether the quality
    is good enough for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, granted that since you and I may not be medical doctors specializing in
    heart disease, we might not be qualified to determine with certainty that the
    data generated makes sense; but if we did this correctly, it generally does make
    sense. To make this clear, the next section uses MNIST images to prove that the
    generated samples are good since we can all make a visual assessment of numeral
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing a deep and shallow VAE on MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Comparing shallow and deep models is part of the experimentation process that
    leads to finding the best models. In this comparison over MNIST images, we will
    be implementing the architecture shown in *Figure 9.6* as the shallow model, while
    the deep model architecture is shown in *Figure 9.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/066f2ead-8aeb-434c-b6fc-e6b5081790be.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – VAE shallow architecture over MNIST
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can appreciate, both models are substantially different when it comes
    to the number of layers involved in each one. The quality of the reconstruction
    will be different as a consequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b345ff5-b67c-4f2d-977d-1dd9834d5b0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – VAE deep architecture over MNIST
  prefs: []
  type: TYPE_NORMAL
- en: These models will be trained using a small number of epochs for the shallow
    VAE and a much larger number of epochs for the deeper model.
  prefs: []
  type: TYPE_NORMAL
- en: The code to reproduce the shallow encoder can be easily inferred from the example
    used in the Cleveland Heart Disease dataset; however, the code for the deep VAE
    will be discussed in the sections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Shallow VAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the first things that we can use to compare the VAE is its learned representations.
    *Figure 9.8* depicts the latent space projections of the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5234c729-7ee3-4c7c-82d2-2a97581675a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Shallow VAE latent space projections of the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding figure, we can observe clusters of data points that are spreading
    out from the center coordinates. What we would like to see are well-defined clusters
    that are ideally separated enough so as to facilitate classification, for example.
    In this case, we see a little bit of overlap among certain groups, particularly
    number 4 and number 9, which makes a lot of sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing to look into is the reconstruction ability of the models. *Figure
    9.9* shows a sample of the input, and *Figure 9.10* shows the corresponding reconstructions
    after the model has been trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a20024e-4abf-44aa-ac42-9314e2d61afb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Sample input to the VAE
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation for the shallow model is to perform in a manner that is directly
    related to the size of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9de3f378-6e48-4e41-b4e2-7d575ea4d98f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Shallow VAE reconstructions with respect to the input in Figure
    9.9
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, there seem to be some issues with the reconstruction of the number
    2 and number 8, which is confirmed by observing the great deal of overlap shown
    between these two numerals in *Figure 9.8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing we can do is to visualize the data generated by the VAE if we
    draw numbers from the range of the latent space. *Figure 9.11* shows the latent
    space as it changes across the two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b394a3ad-b012-4f4e-bd26-5ee5448e1c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Shallow VAE latent space exploration in the range [-4,4] in both
    dimensions
  prefs: []
  type: TYPE_NORMAL
- en: What we find really interesting in *Figure 9.11* is that we can see how numerals
    are transformed into others progressively as the latent space is traversed. If
    we take the center line going from top to bottom, we can see how we can go from
    the number 0 to the number 6, then to the number 2, then to the number 8, down
    to the number 1\. We could do the same by tracing a diagonal path, or other directions.
    Making this type of visualization also allows us to see some artifacts that were
    not seen in the training dataset that could cause potential problems if we generate
    data without care.
  prefs: []
  type: TYPE_NORMAL
- en: To see whether the deeper model is any better than this, we will implement it
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deep VAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 9.7* depicts a deep VAE architecture that can be implemented in parts—first
    the encoder, and then the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The encoder can be implemented using the functional paradigm, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `inpt_dim` corresponds to the 784 dimensions of a 28*28 MNIST image.
    Continuing with the rest, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the encoder model uses dropout layers with a 10% dropout rate. The
    rest of the layers are all things we have seen before, including batch normalization.
    The only new thing here is the `Lambda` function, which is exactly as defined
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will define the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder is a few layers shorter than the encoder. This choice of layers
    is simply to show that as long as the number of dense layers is almost equivalent
    in the encoder and decoder, some of the other layers can be omitted as part of
    the experiment to look for performance boosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the design of the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, there is nothing new here that we have not seen before. It is all
    layers after more layers. Then, we can put all this together in the model, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: That is it! After this, we can compile the model, choose our optimizer, and
    train the model in the exact same way that we did in earlier sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to visualize the latent space of the deep VAE in order to compare
    it with *Figure 9.8*, we could look at the space shown in *Figure 9.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/243634d2-d778-484e-8dac-ae717e021a0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Deep VAE latent space projections of the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the latent space appears to be different in the way the geometry
    of it looks. This is most likely the effect of activation functions delimiting
    the latent space range for specific manifolds. One of the most interesting things
    to observe is the separation of groups of samples even if there still exists some
    overlap–for example, with numerals 9 and 4\. However, the overlaps in this case
    are less severe in comparison to *Figure 9.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.13* shows the reconstruction of the same input shown in *Figure 9.9*,
    but now using the deeper VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3c397cf-bc79-42f8-b882-7ffecf4360b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Deep VAE reconstructions with respect to the input in Figure 9.9\.
    Compare to Figure 9.10
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, the reconstruction is much more better and robust in comparison to
    the shallow VAE. To make things even clearer, we can also explore the generator
    by traversing the latent space by producing random noise in the same range as
    the latent space. This is shown in *Figure 9.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f32052b8-c121-446c-9efe-212cf3ccc400.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Deep VAE latent space exploration in the range [-4,4] in both
    dimensions. Compare to Figure 9.11
  prefs: []
  type: TYPE_NORMAL
- en: The latent space of the deeper VAE clearly seems to be richer in diversity and
    more interesting from a visual perspective. If we pick the rightmost line and
    traverse the space from bottom to top, we can see how numeral 1 becomes a 7 and
    then a 4 with smaller and progressive changes.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising VAEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VAEs are also known to be good in image denoising applications (Im, D. I. J.,
    *et al.* (2017)). This property is achieved by injecting noise as part of the
    learning process. To find out more about this, you can search the web for denoising
    VAEs and you will find resources on that particular topic. We just want you to
    be aware of them and know that they exist if you need them.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about the ethical implications of generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative models are one of the most exciting topics in deep learning nowadays.
    But with great power comes great responsibility. We can use the power of generative
    models for many good things, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting your dataset to make it more complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your model with unseen data to make it more stable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding adversarial examples to re-train your model and make it more robust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new images of things that look like other things, such as images of
    art or vehicles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new sequences of sounds that sound like other sounds, such as people
    speaking or birds singing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating new security codes for data encryption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can go on as our imagination permits. What we must always remember is that
    these generative models, if not modeled properly, can lead to many problems, such
    as bias, causing trustworthiness issues on your models. It can be easy to use
    these models to generate a fake sequence of audio of a person saying something
    that they did not really say, or producing an image of the face of a person doing
    something they did not really do, or with a body that does not belong to the face.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most notable wrongdoings include deepfakes. It is not worth our
    time going through the ways of achieving such a thing, but suffice to say, our
    generative models should not be used for malicious purposes. Soon enough, international
    law will be established to punish those who commit crimes through malicious generative
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'But until international laws are set and countries adopt new policies, you
    must follow the best practices when developing your models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test your models for the most common types of bias: historical, societal, algorithmic,
    and so on (Mehrabi, N., *et al.* (2019)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train your models using reasonable training and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be mindful of data preprocessing techniques; see [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*, for more details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure your models produce output that always respects the dignity and worth
    of all human beings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have your model architectures validated by a peer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this in mind, go on and be as responsible and creative as you can with
    this new tool that you now have at your disposal: VAEs.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This advanced chapter has shown you one of the most interesting and simpler
    models that is able to generate data from a learned distribution using the configuration
    of an autoencoder and by applying variational Bayes principles leading to a VAE.
    We looked at the pieces of the model itself and explained them in terms of input
    data from the Cleveland dataset. Then, we generated data from the learned parametric
    distribution, showing that VAEs can easily be used for this purpose. To prove
    the robustness of VAEs on shallow and deep configurations, we implemented a model
    over the MNIST dataset. The experiment proved that deeper architectures produce
    well-defined regions of data distributions as opposed to fuzzy groups in shallow
    architectures; however, both shallow and deep models are particularly good for
    the task of learning representations.
  prefs: []
  type: TYPE_NORMAL
- en: By this point, you should feel confident in identifying the pieces of a VAE
    and being able to tell the main differences between a traditional autoencoder
    and a VAE in terms of its motivation, architecture, and capabilities. You should
    appreciate the generative power of VAEs and feel ready to implement them. After
    reading this chapter, you should be able to code both basic and deep VAE models
    and be able to use them for dimensionality reduction and data visualization, as
    well as to generate data while being mindful of the potential risks. Finally,
    you should now be familiarized with the usage of the `Lambda` functions for general-purpose
    use in TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: If you have liked learning about unsupervised models so far, stay with me and
    continue to [Chapter 10](6ec46669-c8d3-4003-ba28-47114f1515df.xhtml), *Restricted
    Boltzmann Machines,* which will present a unique model that is rooted in what
    is known as graphical models. Graphical models use graph theory mixed with learning
    theory to perform machine learning. An interesting aspect of restricted Boltzmann
    machines is that the algorithm can go forward and backward during learning to
    satisfy connection constraints. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**How is data generation possible from random noise?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the VAE learns the parameters of a parametric random distribution, we
    can simply use those parameters to sample from such a distribution. Since random
    noise usually follows a normal distribution with certain parameters, we can say
    that we are sampling random noise. The nice thing is that the decoder knows what
    to do with the noise that follows a particular distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the advantage of having a deeper VAE? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is hard to say what the advantage is (if there is any) without having the
    data or knowing the application. For the Cleveland Heart Disease dataset, for
    example, a deeper VAE might not be necessary; while for MNIST or CIFAR, a moderately
    large model might be beneficial. It depends.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is there a way to make changes to the loss function?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, you can change the loss function, but be careful to preserve the
    principles on which it is constructed. Let's say that a year from now we found
    a simpler way of minimizing the negative log likelihood function, then we could
    (and should) come back and edit the loss to adopt the new ideas.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kingma, D. P., & Welling, M. (2013). Auto-encoding variational Bayes. *arXiv
    preprint* arXiv:1312.6114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salakhutdinov, R., Mnih, A., & Hinton, G. (2007, June). Restricted Boltzmann
    machines for collaborative filtering. In *Proceedings of the 24th International
    Conference on Machine Learning* (pp. 791-798).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A. and Bengio, Y. (2014). Generative adversarial nets. In *Advances
    in Neural Information Processing Systems* (pp. 2672-2680).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Y., Chuangsuwanich, E., & Glass, J. (2014, May). Extracting deep neural
    network bottleneck features using low-rank matrix factorization. In *2014 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*
    (pp. 185-189). IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
    *arXiv preprint* arXiv:1412.6980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019).
    A survey on bias and fairness in machine learning. *arXiv preprint* arXiv:1908.09635.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Im, D. I. J., Ahn, S., Memisevic, R., & Bengio, Y. (2017, February). Denoising
    criterion for variational auto-encoding framework. In *Thirty-First AAAI Conference
    on Artificial Intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
