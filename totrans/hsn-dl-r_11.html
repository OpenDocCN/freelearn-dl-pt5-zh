<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generative Adversarial Networks for Faces</h1>
                </header>
            
            <article>
                
<p>In the last chapter, we used a <strong>long short-term memory</strong> (<strong>LSTM</strong>) model on a time-series forecasting task. In this chapter, we will create a generator model, which means the model will not output predictions but rather files (in this case, images). We created a generator model in <span><a href="a000fdb8-7d3e-436b-abd9-9fbc9b3c33f0.xhtml">Chapter 7</a>, </span><span><span><em>Deep Learning for Natural Language Processing</em></span></span>; however, in that case, we just generated latent features. Here, we will describe the main components and applications of <strong>generative adversarial networks</strong> (<strong>GANs</strong>). You will learn about the common applications of GANs and how to build a face generation model using a GAN.</p>
<p>Over the course of this chapter, we will investigate the architecture of a GAN. A GAN is composed of two competing neural networks, one of which is known as the <strong>generator model</strong>. It takes random data and creates synthetic target data. The other part of a GAN is the <strong>discriminator model</strong>. This model takes two pieces of input—the synthetic target data and the real target data—and it determines which is the real target data. After understanding this process, we will code our own GAN model for face recognition and generation using the Keras package and images from the <em>labeled faces in the wild</em> dataset.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>An overview of GANs</li>
<li>Defining the generator model</li>
<li>Defining the discriminator model</li>
<li>Preparing and preprocessing a dataset</li>
<li>Training and evaluating a model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You can find the code files used in this chapter at the f</span>ollowing GitHub link:</p>
<p><a href="https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R">https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of GANs</h1>
                </header>
            
            <article>
                
<p>A GAN is a modeling algorithm that pits two neural networks against each other. One of them uses random data to create output. The other evaluates the real target data and the generated output and determines which is real. Over time, the first neural network creates better fake target data and the second neural network continues to try and determine which is the real target data. The two neural networks continue to compete and the models both improve to create increasingly realistic synthetic data.</p>
<p>Breaking down the term, we can see how this modeling technique differs from others. First, it is generative, which means that the goal is to generate data. This is in contrast to other models, such as classification or regression, that predict probabilities or values. Next, it is adversarial. That is, there are two models that are set to compete against each other. Generally, we have one model and it trains on data that can be evaluated and improved using a variety of metrics. However, in this case, we have one model that seeks to improve prediction performance and that is the discriminator model. In addition, we have another model that creates fake images to try to reduce the performance of the discriminator model.</p>
<p>We generally think of two main categories for machine learning:</p>
<ul>
<li><strong>Supervised learning</strong>: Where the model uses labeled target data to make predictions</li>
<li><strong>Unsupervised learning</strong>: Where the model identifies patterns without any labeled target data</li>
</ul>
<p>However, we can get even more granular with unsupervised learning. GANs belong to a special subset of unsupervised learning that uses learned patterns from unlabeled data to generate synthetic data, rather than just classifying the data. However, this introduces a problem for us. Since the goal is to generate data, there are no direct metrics we can use to evaluate the performance. The relative success or failure of a GAN model is based largely on the subjective interpretation of the output.</p>
<p>For this GAN, we will use images as our input. All images can be represented as a matrix of values for grayscale images, or three matrices of values for color images. The values of the matrix range from <kbd>0</kbd> to <kbd>255</kbd> and correspond with the intensity of the pixel value at that location. For example, a pixel value of <kbd>255</kbd> means high intensity or black for a grayscale image and a value of <kbd>0</kbd> means low intensity or white. The dimensions of the matrix correspond with the pixel width and height of the image. Color images are represented as a three-dimensional array. This can be thought of as three matrices overlapping, with each corresponding to the intensity of the red, green, and blue pixel values for the image. Let's take a look at a sample image and see how it is represented as a matrix of values. To do this, we will read in the following shape:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/87a19f79-255f-4257-8689-28a682e5f076.png" style="width:11.67em;height:11.08em;"/></p>
<p>To read in this shape, we will use the <kbd>OpenImageR</kbd> package. This package reads the file in as a four-dimensional array. In this case, we only want the fourth dimension, which contains the grayscale values. To read in this file and then look at a small segment, we run the following code:</p>
<pre>library(OpenImageR)<br/><br/>clock &lt;- readImage("Alarms_&amp;_Clock_icon.png")<br/><br/>clock[1:10,46:56,4]</pre>
<p>After running this code, we will see the following printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5f0f973f-de91-4e56-b295-b0d5e639b42b.png" style="width:40.67em;height:12.08em;"/></p>
<p>We can see how these values represent the top of the left bell on the alarm clock. The zeroes are the white space and we see a gradient in the second rows and the top rows, showing the curve. In this way, we can see how images can be expressed as a matrix of values between <kbd>0</kbd> and <kbd>1</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the generator model</h1>
                </header>
            
            <article>
                
<p>The generator model is the neural network that creates synthetic target data out of random inputs. In this case, we will use a <strong>convolutional neural network</strong> (<strong>CNN</strong>) in reverse. What this means is that we will start with a vector of data points and create a fully connected layer, then reshape the data into the size that we want it to be. As a middle step, we will make the target shape only half the size and then we will upsample using a transposed convolution layer. In the end, we have an array of normalized pixel values that is the same shape as our target array. This then becomes the data object that will be used to try to fool the discriminator model. This array of synthetic values will, over time, be trained to resemble the values in the target data object so that the discriminator model cannot predict,<span> </span><span>with a high probability,</span><span> which is the true data image. We will define the discriminator model using the following steps: </span></p>
<ol>
<li>First, we will define that our entry point will be a 100-dimensional vector. Everything we do to define our models will be done with Keras. So, we load the Keras model at this step. We then define our input shape as a vector with 100 values.</li>
<li>Then, we will pass a vector to this model. In this step, we tell the model what we will be passing in the later step. Using the following code, we declare that the input to this model will be a vector with 100 values that we will later populate with random values:</li>
</ol>
<pre style="padding-left: 60px">library(keras)<br/>generator_in &lt;- layer_input(shape = c(100))  </pre>
<ol start="3">
<li>After running this step, we can see that we have a special data object, called <kbd>Tensor</kbd>, in our data environment. The object contains the type, name, shape, and data type of the layer. Your data environment will look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1b1edff-a48c-4d71-9b2c-596f14425005.png" style="width:45.17em;height:8.50em;"/></p>
<ol start="4">
<li>After this, we define how our random values will be processed, transformed, and reshaped to create a synthetic array that matches our target arrays. The code to do this is long, but many of the parts are repeated. There are a few lines that are required while others can be modified. The <kbd>layer_dense</kbd> layer needs to contain the number of units that will appear later in the <kbd>layer_reshape</kbd> layer. In this case, we will create a shape that has a width and height of <kbd>25</kbd> and a depth of <kbd>128</kbd>. The depth is modifiable; however, the width and height must be set at half the size of the final image's dimensions when using one transposed convolution layer, as follows:</li>
</ol>
<pre style="padding-left: 60px">generator_out &lt;- generator_in %&gt;%<br/>   layer_dense(units = 128 * 25 * 25) %&gt;%<br/>   layer_reshape(target_shape = c(25, 25, 128))</pre>
<ol start="5">
<li><span>The </span><kbd>layer_conv_2d_transpose</kbd><span><span> layer uses a 2 x 2 stride to upsample and double the shape of the layer. In this step, the shape changes from 25 x 25 to 50 x 50:<br/></span></span></li>
</ol>
<pre style="padding-left: 60px">generator_out &lt;- generator_in %&gt;%<br/>   layer_dense(units = 128 * 25 * 25) %&gt;%<br/>   layer_reshape(target_shape = c(25, 25, 128)) %&gt;%<br/>   layer_conv_2d(filters = 512, kernel_size = 5,<br/>                 padding = "same")</pre>
<ol start="6">
<li><span><span><span>The convolution applies filters that look for patterns and the normalization takes the results of the convolution step and normalizes the values. So the mean is close to 0 and the standard deviation is close to 1, and ReLU is used as our activation function. We will add these layers after our dense layer and our convolution layer using the following code:<br/></span></span></span></li>
</ol>
<pre style="padding-left: 60px">generator_out &lt;- generator_in %&gt;%<br/>   layer_dense(units = 128 * 25 * 25) %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_reshape(target_shape = c(25, 25, 128)) %&gt;%<br/>   layer_conv_2d(filters = 512, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() </pre>
<ol start="7">
<li><span><span>After this, we can continue to add additional convolution layers using the same pattern of convolution, normalization, and activation. Here, we will add four additional series of layers using the pattern we just described:<br/></span></span></li>
</ol>
<pre style="padding-left: 60px">generator_out &lt;- generator_in %&gt;%<br/>   layer_dense(units = 128 * 25 * 25) %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_reshape(target_shape = c(25, 25, 128)) %&gt;%<br/>   layer_conv_2d(filters = 512, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d_transpose(filters = 256, kernel_size = 4,<br/>                           strides = 2, padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 128, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 64, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu()</pre>
<ol start="8">
<li>In the very last step, the <kbd>filters</kbd> argument needs to be set to the number of channels for the image—in this case, three for the red, green, and blue channels of a color image. This completes the definition of our generator model. The entire generator model<span> is defined </span>using the following code<span><span>:<br/></span></span></li>
</ol>
<pre style="padding-left: 60px">generator_out &lt;- generator_in %&gt;%<br/>   layer_dense(units = 128 * 25 * 25) %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_reshape(target_shape = c(25, 25, 128)) %&gt;%<br/>   layer_conv_2d(filters = 512, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d_transpose(filters = 256, kernel_size = 4,<br/>                           strides = 2, padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 128, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 64, kernel_size = 5,<br/>                 padding = "same") %&gt;%<br/>   layer_batch_normalization(momentum = 0.5) %&gt;%<br/>   layer_activation_relu() %&gt;%<br/>   layer_conv_2d(filters = 3, kernel_size = 7,<br/>                 activation = "tanh", padding = "same")</pre>
<ol start="9">
<li>After running this code, we will now see two objects in our environment. We have defined the connected tensors for input and the connected input for the output. Setting up our tensors in this way allows data to be input in batches using the <kbd>keras_model</kbd> function. Your data environment should look like the following now:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e3087f68-2aa2-43f0-a2f2-8165b8996a54.png" style="width:34.42em;height:7.58em;"/></p>
<ol start="10">
<li>After, we define that the input will be 100 random values and the output will be random values mapped to a data object with the same dimensions as our target image. </li>
<li>We can then define <kbd>keras_model</kbd>, which takes the input and output as arguments, specifically. We pass in these defined tensor layers, at this point, to complete the definition of our model.</li>
<li>After defining the model, we can run the <kbd>summary</kbd> function on the generator model to helpfully see what is happening to the data at each layer. We define our generator and view the summary using the following code:</li>
</ol>
<pre style="padding-left: 60px">generator &lt;- keras_model(generator_in, generator_out)<br/>summary(generator)</pre>
<ol start="13">
<li><span>After running the <kbd>summary</kbd> function, we will see details about our model printed to our console, which looks like this:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/dbfce84e-5c01-4adc-a852-90182cc49caf.png" style="width:35.50em;height:45.08em;"/><br/></span></p>
<ol start="14">
<li class="CDPAlignLeft CDPAlign">From the console output, we can see that we start with one fully connected layer and after numerous intermediate layers, we end up with a final layer that matches the shape of our target image data.</li>
</ol>
<p>We now have our generator completely defined. We have seen how we can insert random values and how those random values are then transformed to produce a synthetic image. The process of passing data to this model occurs later in the process. With a system in place to produce fake images, we now move on to defining the discriminator model, which will determine whether a given array of pixel data is a real or fake image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the discriminator model</h1>
                </header>
            
            <article>
                
<p>The discriminator model is the neural network that evaluates the synthetic target data and the real target data to determine which is the real one.</p>
<p>The discriminator, in this case, is a CNN model; it takes a three-dimensional array as input. Often with CNNs, convolving layers and pooling layers are used to reshape the dimensions of the input—ultimately, to a fully connected layer. However, when using these layers to define a discriminator model in the context of creating a GAN, we instead use 2 x 2 strides in the convolving layers to reshape the input dimensions. In the end, a fully connected layer with one unit is passed through the sigmoid activation function to calculate the probability that a given input is real or fake. <span>Let's follow the following lines of code to define the discriminator model:</span></p>
<ol>
<li>As <span>we did in the generator model, let's start by defining the input shape. While the generator model started with a vector of 100 random values, our discriminator starts with input in the shape of our image data as that is what will be passed to the model. The dimensions for the image are used as arguments to define the input shape using the following code:</span></li>
</ol>
<pre style="padding-left: 60px">discriminator_in &lt;- layer_input(shape = c(50, 50, 3))  </pre>
<ol start="2">
<li>Running this code adds another object to our data environment. Your <span class="packt_screen">Environment</span> pane will now look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3f436752-c3cf-4c52-9257-e6e721ded803.png" style="width:35.25em;height:11.75em;"/></p>
<ol start="3">
<li>Next, we process and transform our data. While the generator took a one-dimensional vector and created a three-dimensional array in the size of our image data, here we will do the opposite. We start with data that is in the shape of our image data as the first layer, as in the following code:</li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3)</pre>
<ol start="4">
<li>The next layer that we will add to the discriminator is an activation layer. For the discriminator, we will use a Leaky ReLU activation function. The activation layer is added after our first convolution layer so that our code now looks like the following:</li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/>   layer_activation_leaky_relu()</pre>
<ol start="5">
<li>In our next convolution layer, we use strides of <kbd>2</kbd> to halve the height and width while doubling the depth:</li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/>   layer_activation_leaky_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) </pre>
<ol start="6">
<li>We can now continue to add more layers, using the same sequence of convolution layers, to the Leaky ReLU activation layer. <span><span>The constraint is that—as mentioned—at each layer, the height and width are halved and the depth is doubled, so the height and width dimensions need to be such that they can be halved, with the output containing whole numbers, or you will receive an error. In our case, we will add three more sequences of layers so that our code now looks like this:<br/></span></span></li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/>   layer_activation_leaky_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;%   <br/>   layer_activation_leaky_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;%<br/>   layer_activation_leaky_relu() %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3, strides = 2) %&gt;%<br/>   layer_activation_leaky_relu()</pre>
<ol start="7">
<li>We now need to add a layer to flatten our values to one dimension in preparation for our final output layer. When we add this layer, our code will look like the following:</li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;% <br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 3, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_flatten()</pre>
<ol start="8">
<li>After this, we add a <kbd>dropout</kbd> layer that removes some data at random, which forces the model to work harder and slows down training, which produces a better generalizer. Adding this layer results in the following code:</li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;% <br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 3, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_flatten() %&gt;%<br/>   layer_dropout(rate = 0.5)</pre>
<ol start="9">
<li>Lastly, we add a <kbd>dense</kbd> layer with just <kbd>1</kbd> unit, representing the probability that an image is real or fake. Adding this last layer will complete our discriminator model. The final discriminator model is defined with the following code<span><span>:<br/></span></span></li>
</ol>
<pre style="padding-left: 60px">discriminator_out &lt;- discriminator_in %&gt;%<br/>   layer_conv_2d(filters = 256, kernel_size = 3) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;% <br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 5, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_conv_2d(filters = 256, kernel_size = 3, strides = 2) %&gt;%<br/> layer_activation_leaky_relu() %&gt;%<br/> layer_flatten() %&gt;%<br/>   layer_dropout(rate = 0.5) %&gt;%     <br/>   layer_dense(units = 1, activation = "sigmoid")</pre>
<p style="padding-left: 60px">After running the code, there are now four defined tensors in our data environment. Your data environment will look as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/15ca4d53-c985-4a9a-86ce-ea978a3f6b8f.png" style="width:47.42em;height:17.42em;"/></p>
<ol start="10">
<li>After defining the input and output, both objects are passed as arguments to the <kbd>keras_model</kbd> function, as before, with the generator model. We define the discriminator model using the input and output definitions from the previous steps and then run the <kbd>summary</kbd> function to see the details of the model, using the following code:</li>
</ol>
<pre style="padding-left: 60px">discriminator &lt;- keras_model(discriminator_in, discriminator_out)<br/>summary(discriminator)</pre>
<p style="padding-left: 60px">After running the preceding code, you will have details about the model printed to your console. The output to your console will look as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bdeb4ba8-60d2-492d-9077-403ea27f4b69.png" style="width:39.25em;height:40.58em;"/></p>
<ol start="11">
<li class="CDPAlignLeft CDPAlign">To view the details of our model, we can see the dimensions shifting as the input passes through convolution layers. We start with the input in the shape of our image data and at each layer, two dimensions are reduced while the third dimension is increased. In the end, we have one fully connected layer. We see that if we had added a few more convolution layers, we would have gotten to a point where we could no longer halve our data and still have a whole unit remaining.</li>
<li class="CDPAlignLeft CDPAlign">During this step, we will also define our optimizer, which is how the model will pass data back to improve future iterations of the model. We will calculate performance using <kbd>binary_crossentropy</kbd> and then use the <kbd>adam</kbd> optimizer to feed data back to the model from the error rate gradients. We define how we evaluate and incrementally improve our discriminator model using the following code:</li>
</ol>
<pre style="padding-left: 60px">discriminator_optimizer &lt;- optimizer_adam(<br/>   lr = 0.0008<br/> )<br/> discriminator %&gt;% compile(<br/>   optimizer = discriminator_optimizer,<br/>   loss = "binary_crossentropy"<br/> )</pre>
<p>We now have our generator model and discriminator defined. These are the two main building blocks for our GAN. In the next step, we will load in the real images and show you how to convert the images to numeric data. This is the third and final piece that we need before we assemble everything together to train our GAN and begin generating synthetic images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing and preprocessing a dataset</h1>
                </header>
            
            <article>
                
<p>For this chapter, we will use a small subset of images from the labeled faces in the wild dataset. Specifically, we will use images of former United States president George W. Bush, since this is the image object that occurs most often in the dataset. Using the following code, we will bring in the image data and convert it to a format that can be inputted into our model. We start by loading the libraries and data files required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the libraries and data files</h1>
                </header>
            
            <article>
                
<p>We will begin by using the following steps:</p>
<ol>
<li>First, we load all the libraries that we will use. We will use just one function from each of these libraries but we need each one to get our data in the proper format. The <kbd>jpeg</kbd> library will be used to read in the image data and store it as a matrix. The <kbd>purrr</kbd> package will be used to apply a function to our list of arrays. The <kbd>abind</kbd> package will be used to convert the list of arrays into one array. Finally, <kbd>OpenImageR</kbd> will be used to resize our data. We load all the libraries needed to bring in images and convert them to the proper format using the following code:</li>
</ol>
<pre style="padding-left: 60px"> library(jpeg)<br/> library(purrr)<br/> library(abind)<br/> library(OpenImageR)</pre>
<ol start="3">
<li>After loading the libraries, the next step is to bring over all the image files. The first step in this process is to change the working directory, for convenience, to the folder containing all the image files.</li>
<li>Once you have navigated to this folder, use the <kbd>list.files</kbd> function to bring over a vector of all the filenames. Lastly, we use the <kbd>map()</kbd> function from the <kbd>purrr</kbd> package to perform functions on every element in our vector and pass the results to a list.</li>
<li>In this case, every element in our vector is a file path. We pass each file path as an argument to the <kbd>readJPEG</kbd> function from the <kbd>jpeg</kbd> package. This function returns an array for every image, with all the pixel values represented as normalized values between <kbd>0</kbd> and <kbd>1</kbd>. This is convenient because this is the format we want for our neural networks. As noted before, the pixel values are ordinarily stored as integers between <kbd>0</kbd> and <kbd>255</kbd>; however, values bound between <kbd>0</kbd> and <kbd>1</kbd> work better when planning to pass data through a neural network. We import our images, convert all the pixel values to normalized values between <kbd>0</kbd> and <kbd>1</kbd>, and store all the formatted image data in a list of arrays using the following code:</li>
</ol>
<pre style="padding-left: 60px"> setwd('data/faces')<br/> filename_vector = list.files(pattern="*.jpg")<br/> image_list &lt;- purrr::map(filename_vector, jpeg::readJPEG)</pre>
<ol start="6">
<li>After running the code, we now have the list of arrays in our data environment. If we expand the object, we can see a sample of pixel values for the images in this set. After expanding the data object in your environment, it will look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/691d240f-f9c3-4e69-9a66-26e846a541a6.png" style="width:38.33em;height:21.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resizing our images</h1>
                </header>
            
            <article>
                
<p>We will resize the images using the following steps:</p>
<ol start="1">
<li>This step is done for the purposes of this book to speed up the model execution time. In a real-world example, this step may not be necessary or desirable. However, knowing how to resize images is helpful in any case. We can resize every image by once again using the <kbd>map</kbd> function from the <kbd>purrr</kbd> package and also the <kbd>resizeImage</kbd> function from the <kbd>OpenImageR</kbd> package. In this case, <kbd>map</kbd> takes every element from the <kbd>image_list</kbd> object and passes it as an argument through the <kbd>resizeImage</kbd> function. So, every array will change from having dimensions of 250 x 250 to 50 x 50. We resize every image by running the following code:</li>
</ol>
<pre style="padding-left: 60px">image_list &lt;- purrr::map(image_list, ~OpenImageR::resizeImage(., width=50, height=50, method = "nearest"))</pre>
<ol start="2">
<li>After running this code, we can see the dimensions of our images have changed. If <kbd>image_list</kbd> is still expanded in the data <span class="packt_screen">Environment</span> pane, then it will now look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f5a164a7-3a67-43fe-ad90-4410f8363095.png" style="width:39.25em;height:22.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Merging arrays</h1>
                </header>
            
            <article>
                
<p>Now that we are done resizing, we will start merging the arrays:</p>
<ol start="1">
<li>After our images are resized, there is just one last step to get the data in the proper format. The data is currently stored in a list of arrays; however, we need the data to all be in one four-dimensional array. The following code takes all our three-dimensional arrays and combines them along a new fourth dimension. We can combine all of our three-dimensional arrays into one four-dimensional array and then view the dimensions using the following code:</li>
</ol>
<pre style="padding-left: 60px">image_array &lt;- abind::abind( image_list, along = 0)<br/>dim(image_array)</pre>
<ol start="2">
<li>This code will print details about our image dimensions to the console, so we can now see the new shape of our four-dimensional array and how the fourth dimension corresponds with the number of image objects we have. You will see the following printed to your console:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fde30a9a-28fa-4888-8a89-90c90ba806ca.png" style="width:13.25em;height:5.17em;"/></p>
<ol start="3">
<li>Our data is now in the proper format and we can just do two last clean-up steps. We remove the list of arrays and the vector of file path names, since we no longer need these, and reset our working directory back to the root for our project:</li>
</ol>
<pre style="padding-left: 60px">rm(image_list,filename_vector)<br/> <br/>setwd('../..')</pre>
<ol start="4">
<li>After running this code, we have all the objects that we need to begin assembling our GAN model. Your data environment will look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9aba0a85-13f3-4f1e-99bf-5413760b9499.png" style="width:39.00em;height:18.17em;"/></p>
<p>With the data now imported to the environment and converted into the proper format, we are now ready to put everything together to create our GAN model. The data we just loaded is used with the discriminator model, along with the array created by the generator model. We will now write the code that combines the data, generator, and discriminator to create our GAN model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and evaluating the model</h1>
                </header>
            
            <article>
                
<p>Now that the data is in the proper format and we have our discriminator and generator defined, we can put it all together to train our GAN. The final GAN model takes input from our target image dataset and the output is the probability that this is a real image after the real image data and the fake image data have been passed as input to the discriminator. We train our GAN model by running the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the GAN model</h1>
                </header>
            
            <article>
                
<p>We define the GAN model as follows:</p>
<ol>
<li>The first step that we will perform is calling the <kbd>freeze_weights</kbd> function on the discriminator model. This is so that the weights for the discriminator model do not update during the training process. We want the weights to update for the generator and not for the discriminator:</li>
</ol>
<pre style="padding-left: 60px">freeze_weights(discriminator)</pre>
<ol start="2">
<li>The next step is to define the input and output for <kbd>keras_model</kbd>, as we did with the generator and the discriminator. In this case, <kbd>keras_model</kbd> will be our final GAN model. Note here that the input will contain 100 values, which is the same as our generator, since the input to our GAN model will pass through our generator model and then continue through to our discriminator model, which will then produce the output for our model. We define the GAN model using the following code:</li>
</ol>
<pre style="padding-left: 60px">gan_in &lt;- layer_input(shape = c(100))<br/>gan_out &lt;- discriminator(generator(gan_in))<br/>gan &lt;- keras_model(gan_in, gan_out)</pre>
<ol start="3">
<li>After running this code, we now have the following objects in our data environment. We can see in all the details about the different tensor layers the path of the data through the entire GAN model pipeline. Your <span class="packt_screen">Environment</span> pane will look like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c9a7ab3f-43a1-4c3c-af50-c7e686700261.png" style="width:41.33em;height:23.50em;"/></p>
<ol start="4">
<li>Similar to the discriminator model, we need to define the compile step. We set it up in the same way that we did with the discriminator. The error is computed using the <kbd>binary_crossentropy</kbd> loss function and <kbd>adam</kbd> is used to iteratively improve the model. Defining how the final GAN model is compiled is done using the following code:</li>
</ol>
<pre style="padding-left: 60px">gan_optimizer &lt;- optimizer_adam(<br/>   lr = 0.0004<br/> )<br/> gan %&gt;% compile(<br/>   optimizer = gan_optimizer,<br/>   loss = "binary_crossentropy"<br/> )</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Passing data to the GAN model</h1>
                </header>
            
            <article>
                
<p>Now, we will pass data to the model as follows:</p>
<ol start="1">
<li>With this, our model is ready and we can begin to pass data through to generate synthetic images. In order to store these data objects, we will need to create a directory within our project folder. We create a directory to hold our real and fake images by using the following code:</li>
</ol>
<pre style="padding-left: 60px">image_directory &lt;- "gan_images"<br/> dir.create(image_directory)</pre>
<ol start="2">
<li>After running this code, you wi<span>ll see a new folder in your main project folder with the name <kbd>gan_images</kbd>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the GAN model</h1>
                </header>
            
            <article>
                
<p>Now that we have prepared our model, it is time to train it, using the following steps:</p>
<ol start="1">
<li>Training our GAN model is an iterative process and we will need to create a loop that selects and creates image arrays and then passes them through our GAN, which will calculate the probability that each image array belongs to the target class. However, if we start a loop here, then the effects on every line of code will not be seen until the entire code completes. For that reason, we will first walk through every line of code inside the loop and then, we will show the entire code wrapped in the <kbd>for</kbd> loop.</li>
<li>Before entering the loop, we declare one variable that we will need inside our loop. The following code sets a value for the <kbd>first_row</kbd> variable. This will be used later when we subset our array. We start with the first three-dimensional array within our four-dimensional array. Later, when the following code is run in a loop, the value for <kbd>first_row</kbd> will change during every iteration to ensure a different subset of real images is passed to the discriminator model. We set the value of <kbd>first_row</kbd> for the first iteration of the loop by running the following code:</li>
</ol>
<pre style="padding-left: 60px">first_row &lt;- 1</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating random images</h1>
                </header>
            
            <article>
                
<p>After training, we will use the model to create random images, as follows:</p>
<ol start="1">
<li>The next step is to create a matrix of random variables. The number of random variables should be set at the batch size times the size of the input shape for our generator model. The dimensions are then set so that the number of rows is equal to the batch size and the number of columns is equal to the length of the input shape defined here. In this case, <kbd>20</kbd> is used as the batch size and <kbd>100</kbd> is used as the length of the vector to be passed to the generator model. Both of these values are modifiable. Increasing either or both provides more data to the model, which could improve performance but will also increase runtime. We create our matrix of random values from a normal distribution by using the following code:</li>
</ol>
<pre style="padding-left: 60px">random_value_matrix &lt;- matrix(rnorm(20 * 100),<br/>                                   nrow = 20, ncol = 100)</pre>
<ol start="2">
<li>After running the code, a matrix will be created consisting of values selected from a normal (Gaussian) distribution. Every row in the matrix will be used to generate an image. The images are created by using random values. By selecting the object in our data environment, we can view it. After selecting the data object from the environment, you will see something like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/45add777-2b12-486f-94d5-48a4a7cd804a.png" style="width:27.42em;height:27.00em;"/></p>
<ol start="3">
<li>Next, using our matrix of random values, we will generate fake images. These fake images are created using the generator model that we defined earlier. The model takes the random values as input and the output is a four-dimensional array. The first dimension of the array corresponds with the batch size, which in this case is <kbd>20</kbd>, and the other three dimensions correspond to the dimensions of our image data. After generating synthetic data, we will sample a few values to show that the arrays have been created and populated with the random values. We create the array and view a portion of it by running the following code:</li>
</ol>
<pre style="padding-left: 60px">fake_images &lt;- generator %&gt;% predict(random_value_matrix)<br/>fake_images[1,1:5,1:5,1]</pre>
<ol start="4">
<li>After running the preceding code, we see a small section of the array that we created. Since this has a value of <kbd>1</kbd> for the first dimension and a value of <kbd>1</kbd> for the fourth dimension, then we know that the values will be used to represent the intensity of the red values for the first image. The preceding code prints values to the console. You will see something like the following printed to your console:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/94cfbdcc-0b4b-42fc-ae4a-ea688b87d4d6.png" style="width:36.17em;height:10.42em;"/></p>
<ol start="5">
<li>Earlier, we set the <kbd>first_row</kbd> value to indicate where we would like row-wise subsets to begin for every iteration. Next, we need to define the last row, which is equal to the value of the first row plus one less than the batch size. In this case, the batch size is <kbd>20</kbd>, so we use <kbd>19</kbd>. Also, while the <kbd>first_row</kbd> value begins at <kbd>1</kbd>, it will change dynamically throughout the iterations. We set the value of the last row for subsetting our data by running the following code:</li>
</ol>
<pre style="padding-left: 60px">last_row &lt;- first_row + 19</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting real images</h1>
                </header>
            
            <article>
                
<p>Now, we select the real images, as follows:</p>
<ol start="1">
<li>Next, we use the values for <kbd>first_row</kbd> and <kbd>last_row</kbd> to create a subset of the array containing our real target images. We will also run two lines to remove attributes from our data objects. This is not strictly necessary and at times, you may want data that is stored here. However, for demonstration purposes, we will remove it now so we can see the dimensions of all the arrays in the data environment window. The array of real images equal to the batch size to be used in an iteration of the model is created using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">real_images &lt;- image_array[first_row:last_row,,,]<br/><br/>attr(real_images, "dimnames") &lt;- NULL<br/>attr(image_array, "dimnames") &lt;- NULL</pre>
<ol start="2">
<li>After running these lines, we can now see that <kbd>real_images</kbd> and <kbd>fake_images</kbd> are arrays of the same size. Expand both data objects in your <span class="packt_screen">Environment</span> pane and you will see that your environment now looks like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/af9057de-5c7e-4eda-81e8-3256ba26e4f1.png" style="width:40.42em;height:29.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining real and fake images</h1>
                </header>
            
            <article>
                
<p>After differentiating the real and fake images, we will now merge them using the following steps:</p>
<ol start="1">
<li>In our next step, we create an array with all the <kbd>0</kbd> values in the shape of our real images stacked on top of our fake images. That is to say, the first dimension is equal to twice the batch size, which in this case is <kbd>40</kbd>, and the remaining three dimensions are equal to the size of our image arrays. We create this placeholder array of zeroes by running the following code:</li>
</ol>
<pre style="padding-left: 60px">combined_images &lt;- array(0, dim = c(nrow(real_images) * 2, 50,50,3))</pre>
<ol start="2">
<li>After running this code, we will see a new object in our <span class="packt_screen">Environment</span> window and we can see that it does have a first dimension that is twice the size of the other two arrays that we created. Your environment will now look like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0f485eeb-0b22-4568-99cb-cd01482d1e38.png" style="width:41.00em;height:31.83em;"/></p>
<ol start="3">
<li>Next, we will populate our placeholder array. For the top half of this array, we assign the values from the fake images that we generated and for the bottom half, we assign the values for the real images. We populate our array with values from the fake and real image data using the following code:</li>
</ol>
<pre style="padding-left: 60px">combined_images[1:nrow(real_images),,,] &lt;- fake_images<br/>combined_images[(nrow(real_images)+1):(nrow(real_images)*2),,,] &lt;- real_images</pre>
<ol start="4">
<li>After running this code, we will see that even for the small sample of data available in the <span class="packt_screen">Environment</span> window, the values for <kbd>combined_images</kbd> have changed from all the zeroes, as seen earlier, to random values from our <kbd>fake_images</kbd> array. Your <span class="packt_screen">Environment</span> window will now look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aef705b9-a007-49ad-adef-40b8d38aed97.png" style="width:41.58em;height:32.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating target labels</h1>
                </header>
            
            <article>
                
<p>Now, we create target labels for all the images using the following steps:</p>
<ol start="1">
<li>We need to create a matrix of labels; this is simply a matrix of binary values. We add <kbd>20</kbd> rows with values of <kbd>1</kbd> to label the fake images and <kbd>20</kbd> rows with values of <kbd>0</kbd> to label the real images. We create our matrix of label data using the following code:</li>
</ol>
<pre style="padding-left: 60px">labels &lt;- rbind(matrix(1, nrow = 20, ncol = 1),<br/>                matrix(0, nrow = 20, ncol = 1))</pre>
<ol start="2">
<li>After running this code, let's click on the <kbd>labels</kbd> object to view it. We can see that it does contain 20 rows with a value of <kbd>1</kbd> and 20 rows with a value of <kbd>0</kbd>. The following is an image that you will see near the midpoint when viewing this matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0f5da36c-01d5-41d4-92bf-334ae969e1e7.png" style="width:11.25em;height:31.92em;"/></p>
<ol start="3">
<li>The next step is to add some noise to the labels. Just like using the dropout layer earlier, we want to introduce some noise and randomness throughout the modeling process to try to force the discriminator to generalize more and avoid overfitting. We add noise by applying a constant value to an array of random values selected from a uniform distribution that is the same length as our labels object and then adding it to the current values in the labels matrix. We add noise to our labels using the following code:</li>
</ol>
<pre style="padding-left: 60px">labels &lt;- labels + (0.1 * array(runif(prod(dim(labels))),<br/>                                 dim = dim(labels)))</pre>
<ol start="4">
<li>After we do this, we can look at the same subset of rows from the <kbd>labels</kbd> object and see that the values are now all slightly modified. The values in your <kbd>labels</kbd> object will be similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/74a808fc-90be-49a2-b0d5-194c90fca2aa.png" style="width:11.83em;height:32.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Passing input to the discriminator model</h1>
                </header>
            
            <article>
                
<p>Now, we will pass the inputs to the discriminator model using the following steps:</p>
<ol start="1">
<li>Next, we pass our combined images, containing a mix of real and fake images, as input to our discriminator model and we pass along the labels as the target variable for the model. We feed our independent and dependent variables to the input and output layers of our discriminator model using the following code:</li>
</ol>
<pre style="padding-left: 60px">d_loss &lt;- discriminator %&gt;% train_on_batch(combined_images, labels)<br/><br/>d_loss</pre>
<ol start="2">
<li>The result of running this code is the error rate for the discriminator. We can just run the name of the object to have this value printed to our console. After running the preceding code, your console will look as in the following screenshot, though the value may be slightly different:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e2ac21a8-ec92-44d7-b6c9-c716fb2c7642.png" style="width:7.83em;height:4.42em;"/></p>
<ol start="3">
<li>Our next step will be to create another random matrix, which we will use as input to our GAN. It goes to the generator and in turn, the discriminator, as defined in our GAN model definition. We create the input for our GAN model using the following code:</li>
</ol>
<pre style="padding-left: 60px"> random_value_matrix &lt;- matrix(rnorm(20 * 100),<br/>                               nrow = 20, ncol = 100)</pre>
<ol start="4">
<li>After this, we create an array that is the size of our batch. It is set to state that all the data objects are true:</li>
</ol>
<pre style="padding-left: 60px">fake_target_array &lt;- array(0, dim = c(20, 1)) </pre>
<ol start="5">
<li>We pass this matrix of random variables and the array to the GAN model:</li>
</ol>
<pre style="padding-left: 60px">a_loss &lt;- gan %&gt;% train_on_batch(<br/>     random_value_matrix,<br/>     fake_target_array<br/>   )  <br/><br/>a_loss</pre>
<ol start="6">
<li>The result of running this code is a calculation of the error rate for the GAN. If many of the true target images were correctly identified, then the generator will make larger changes and if the generator creates images that are selected as real images, then fewer changes will be made during future iterations. If we run the line that only contains the object name, then we will receive a print out to our console. Your console will look similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6f89a105-6861-47b9-9c2f-3a45d1386007.png" style="width:8.08em;height:4.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the row selector</h1>
                </header>
            
            <article>
                
<p>Next, we will update the row selector with the following steps:</p>
<ol start="1">
<li>Our next step will be to reset the <kbd>first_row</kbd> value to get a different subset of the <kbd>real_image</kbd> data during the subsequent iteration. We reset the <kbd>first_row</kbd> value using the following code:</li>
</ol>
<pre style="padding-left: 60px">  first_row &lt;- first_row + 20<br/>   if (first_row  &gt; (nrow(image_array) - 20))<br/>     first_row &lt;- sample(1:10,1)<br/><br/>first_row</pre>
<ol start="2">
<li>After this code is run, the <kbd>first_row</kbd> value will either be set to the previous value plus the batch size of <kbd>20</kbd>, or if that number would result in a subset that is out of bounds, then the <kbd>first_row</kbd> value is set to a randomly drawn value between <kbd>1</kbd> and <kbd>10</kbd>. In this case, the value will be set to <kbd>21</kbd>. You will see a printout to your console, as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/34464036-eea3-4a6f-92df-a02a058af701.png" style="width:6.75em;height:4.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Finally, we will evaluate the model using the following steps:</p>
<ol start="1">
<li>The last step is to periodically print model diagnostics along with real and generated images for comparison and to track whether the synthetic images are being generated as expected. We print the model iteration and error rates and also save one real and one fake image in the directory that we created earlier using the following code:</li>
</ol>
<pre style="padding-left: 60px">if (i %% 100 == 0) {<br/>     <br/>     cat("step:", i, "\n")<br/>     cat("discriminator loss:", d_loss, "\n")<br/>     cat("adversarial loss:", a_loss, "\n")  <br/>     <br/>     image_array_save(<br/>       fake_images[1,,,] * 255,<br/>       path = file.path(image_directory, paste0("fake_gwb", i, ".png"))<br/>     )<br/>     <br/>     image_array_save(<br/>       real_images[1,,,] * 255,<br/>       path = file.path(image_directory, paste0("real_gwb", i, ".png"))<br/>     )<br/>   }</pre>
<ol start="2">
<li>After running the code, we can see model diagnostics printed to our console. Your console will look as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8e4daa10-1762-4266-8d32-5ebbbf154a2e.png" style="width:16.00em;height:5.25em;"/></p>
<ol start="3">
<li>In addition, we can see generated and real images in the folder we created earlier. It will take quite a large number of rounds or epochs for any images to look like realistic faces, as in our real images set. However, even in earlier rounds, we can see the GAN begin to find features. The following is an original photo from our dataset:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/36581ad9-524e-4454-9e30-065b97f2ee50.png" style="width:13.08em;height:13.50em;"/></p>
<ol start="4">
<li>This is a generated image:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b5ea8b24-34c5-43a9-9b96-ba939b117169.png" style="width:12.33em;height:12.25em;"/></p>
<p><br/>
This early synthetic image has captured a number of features already.</p>
<p>For convenience, the entire <kbd>for</kbd> loop for iteratively training our model is included in the GitHub repository.</p>
<p>Our GAN model is complete. You can continue to make a number of adjustments to see how it affects the synthetic images created. All through the model pipeline creation, we noted the values that need to be present in order to make the model work. However, huge portions can be modified. All modifications will result in different generated images. As noted before, there is no metric for a successful GAN. It will all depend on the end user's interpretation of the generated data. Adding more layers to the generator or discriminator, as well as adjusting the filter size, layer parameters, and learning rate, are all good options for modification as you continue to explore developing this particular type of deep learning model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we created a model that can take images of faces as input and generate faces as output. We used images from the labeled faces in the wild dataset. Using a GAN model, we generated an image with random values and then sampled an actual image. To generate an image, we took random values and reshaped them to the dimensions of the images in our dataset. We then fed this image—composed of random values—along with an actual image, to a model that reshaped the data down to a simple probability score, representing the likelihood that an image is real or fake. Through multiple iterations, the generator was trained to create images that were increasingly likely to be classified as real by the discriminator model.  </p>
<p>In our next chapter, we will learn about another unsupervised deep learning technique called <strong>reinforcement learning</strong>. It is similar to GANs in that an agent performs a task and continually learns from failing until it can perform the task successfully. We will dive into the details of reinforcement learning in the next chapter.</p>


            </article>

            
        </section>
    </body></html>