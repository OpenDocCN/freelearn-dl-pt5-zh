["```py\n    import glob\n    import numpy as np\n    import torch\n    import torch.nn as nn\n    import torchvision.transforms as transforms\n    from PIL import Image\n    from captum.attr import IntegratedGradients\n    from captum.attr import NoiseTunnel\n    from captum.attr import visualization as viz\n    from lucent.optvis import render, param, objectives\n    from lucent.optvis.objectives import diversity\n    ```", "```py\n    class CNN(nn.Module):\n      def __init__(self, num_classes, model='resnet50'):\n        super(CNN, self).__init__()\n        self.num_classes = num_classes\n        self.chosen_model = model\n        if self.chosen_model=='densenet121':\n          self.model = models.densenet121(pretrained=True)\n          self.classifier = nn.Sequential(\n            nn.Dropout(p=0.1),\n            nn.Linear(self.model.classifier.in_features, 256, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Linear(256, 128, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, self.num_classes, bias=False),\n            nn.BatchNorm1d(self.num_classes),\n          )\n          self.model.classifier = self.classifier\n          model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n          params = sum([np.prod(p.size()) for p in model_parameters])\n      def forward(self, x):\n        return self.model(x)\n    ```", "```py\n    checkpoint = torch.load('0.8228 checkpoint.pt', map_location=torch.device('cpu'))\n    model = checkpoint['model']\n    ```", "```py\n    resize_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((224, 224)),\n    ])\n    norm_transform = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225])\n    ```", "```py\n    all_melanoma_images = glob.glob(\"lesions_augmented_data/Data/test/Melanoma/*.jpg\")\n    ```", "```py\n    integrated_gradients = IntegratedGradients(model)\n    noise_tunnel_applyer = NoiseTunnel(integrated_gradients)\n    prediction_label_index = 6\n    ```", "```py\n    for melanoma_image in all_melanoma_images[:6]:\n      pil_image = Image.open(melanoma_image)\n      img = np.array(pil_image)\n      transformed_image = resize_transform(img)\n      input =  norm_transform(transformed_image).unsqueeze(0)\n      attributions_ig_nt =  noise_tunnel_applyer.attribute(\n        input, nt_samples=10, nt_type='smoothgrad_sq',\n        target= prediction_label_index)\n      _ = viz.visualize_image_attr_multiple(   np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n    np.transpose(transformed_image.squeeze().cpu().detach().numpy(), (1,2,0)),\n          [\"original_image\", \"heat_map\"],\n          [\"all\", \"positive\"],\n          cmap='turbo',\n          show_colorbar=True)\n    ```", "```py\n    param_to_optimize = \"classifier_8:6\"\n    ```", "```py\n    cppn_param_f = lambda: param.cppn(224)\n    cppn_opt = lambda params: torch.optim.Adam(params, 4e-3)\n    ```", "```py\n    model.cuda()\n    images = render.render_vis(\n        model, param_to_optimize, cppn_param_f, cppn_opt,\n        thresholds=np.linspace(0, 10000, 100, dtype=int).tolist(),\n        verbose=True,\n        show_inline=True\n    )\n    ```", "```py\n    inference_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((224, 224)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225]),\n    ])\n    ```", "```py\n    def inference_lesion_model(img):\n        transformed_image = inference_transform(img).unsqueeze(0)\n        with torch.no_grad():\n            output = torch.nn.functional.softmax(model(transformed_image), dim=1)\n        return output\n    output = inference_lesion_model(images[-1].squeeze())\n    ```", "```py\n    obj = objectives.channel(\"classifier_8\", 6) - 1e2 * diversity(\"input\")\n    batch_param_f = lambda: param.image(224, fft=True, decorrelate=True, batch=4)\n    batch_images = render.render_vis(\n        model, obj,\n        batch_param_f,\n        thresholds=np.linspace(0, 500, 50, dtype=int).tolist(),\n        show_inline=True,\n        verbose=True,\n    )\n    ```", "```py\noutputs = []\n for img_idx in range(4):\n     img = batch_images[-1][img_idx]\n    output = inference_lesion_model(img)\n    outputs.append((output[0][6], output.argmax()))\n```", "```py\n [(tensor(0.9946, device='cuda:0'), tensor(6, device='cuda:0')),\n (tensor(0.9899, device='cuda:0'), tensor(6, device='cuda:0')),\n (tensor(0.7015, device='cuda:0'), tensor(6, device='cuda:0')),\n (tensor(0.9939, device='cuda:0'), tensor(6, device='cuda:0'))]\n```"]