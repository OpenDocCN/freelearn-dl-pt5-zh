- en: '*Chapter 9*: Multi-Agent Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there is something more exciting than training a **reinforcement** **learning**
    (**RL**) agent to exhibit intelligent behavior, it is to train multiple of them
    to collaborate or compete. **Multi-agent** **RL** (**MARL**) is where you will
    really feel the potential in artificial intelligence. Many famous RL stories,
    such as AlphaGo or OpenAI Five, stemmed from MARL, which we introduce you to in
    this chapter. Of course, there is no free lunch, and MARL comes with lots of challenges
    along with its opportunities, which we will also explore. At the end of the chapter,
    we will train a bunch of tic-tac-toe agents through competitive self-play. So,
    at the end, you will have some companions to play some game against.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will be a fun chapter, and specifically we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing multi-agent reinforcement learning,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the challenges in multi-agent reinforcement learning,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training policies in multi-agent settings,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training tic-tac-toe agents through self-play.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing multi-agent reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the problems and algorithms we have covered in the book so far involved
    a single agent being trained in an environment. On the other hand, in many applications
    from games to autonomous vehicle fleets, there are multiple decision-makers, agents,
    which train concurrently, but execute local policies (i.e., without a central
    decision-maker). This leads us to MARL, which involves a much richer set of problems
    and challenges than single-agent RL does. In this section, we give an overview
    of MARL landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration and competition between MARL agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MARL problems can be classified into three different groups with respect to
    the structure of collaboration and competition between agents. Let's look into
    what those groups are and what types of applications fit into each group.
  prefs: []
  type: TYPE_NORMAL
- en: Fully cooperative environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this setting, all of the agents in the environment work towards a common
    long-term goal. The agents are credited equally for the return the environment
    reveals, so there is no incentive for any of the agents to deviate from the common
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples to fully cooperative environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autonomous vehicle / robot fleets**: There are many applications where a
    fleet of autonomous vehicles / robots could work towards accomplishing a common
    mission. One example is disaster recovery / emergency response / rescue missions,
    where the fleet tries to achieve tasks such as delivering emergency supplies to
    first responders, shutting of gas valves, removing debris from roads etc. Similarly,
    transportation problems as in supply chains or in the form of transporting a big
    object using multiple robots are good examples in this category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manufacturing**: The whole idea behind Industry 4.0 is to have interconnected
    equipment and cyber-physical systems to achieve efficient production and service.
    If you think of a single manufacturing floor, in which there are usually many
    decision-making equipment, MARL is a natural fit to model such control problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart grid**: In the emerging field of smart grid, many problems can be modeled
    in this category. An example is the problem of cooling a data center that involves
    many cooling units. Similarly, control of traffic lights in an intersection is
    another good example in this area. In fact, in [*Chapter 17*](B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365)*,
    Smart City and Cybersecurity*, we will model and solve this problem using MARL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before moving into discussing other types of MARL environments, while we are
    at MARL for autonomous vehicles, let's briefly mention a useful platform, MACAD-Gym,
    for you to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: MACAD-Gym for multi-agent connected autonomous driving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MACAD-Gym is a Gym-based library for connected and autonomous driving applications
    in multi-agent settings, built on top of the famous CARLA simulator.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The platform provides a rich set of scenarios involving cars, pedestrians,
    traffic lights, bikes etc., depicted in Figure 9.1\. In more detail, MACAD-Gym
    environments contain a variety of MARL configurations as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To see what you can do with MACAD-Gym, check out its Github repo, developed
    and maintained by Praveen Palanisamy, at [https://github.com/praveen-palanisamy/macad-gym](https://github.com/praveen-palanisamy/macad-gym).
  prefs: []
  type: TYPE_NORMAL
- en: After this short detour, let's proceed to fully competitive settings in MARL.
  prefs: []
  type: TYPE_NORMAL
- en: Fully competitive environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In fully competitive environments, the success of one of the agents means failure
    for the others. Therefore, such settings are modelled as zero-sum games:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/Formula_09_002.png) is the reward for the ![](img/Formula_09_003.png)
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples to fully competitive environments are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Board games**: This is the classic example for such environments, such as
    chess, Go, and tic-tac-toe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial settings**: In situations where we want to minimize the risk
    of failure for an agent in real-life, we might train it against adversarial agents.
    This creates a fully competitive environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, let's take a look at mixed cooperative-competitive environments.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed cooperative-competitive environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A third type of environments involves both collaboration and cooperation between
    agents. These environments are usually modelled as general-sum games:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/Formula_09_005.png) is the reward for the ![](img/Formula_09_006.png)
    agent and ![](img/Formula_09_007.png) is some fixed total reward that can be collected
    by the agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples to mixed environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Team competitions**: When there are teams of agents competing against each
    other, the agents within a team collaborate to defeat the other teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Economy**: If you think about the economic activities we are involved in,
    it is a mix of competition and cooperation. A nice example to this is how tech
    companies such as Microsoft, Google, Facebook, and Amazon compete against each
    other for certain businesses while collaborating on some open-source projects
    to advance the software technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Info
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, it is worth taking a pause to watch OpenAI's demo on agents playing
    hide-and-seek in teams. The agents develop very cool cooperation and competition
    strategies after playing against each other for a large number of episodes, inspiring
    us for the potential in RL towards artificial general intelligence. See Figure
    9.2 for a quick snapshot and the link to the video.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2: OpenAI''s agents playing hide-and-seek (Source: https://youtu.be/kopoLzvh5jY)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.2: OpenAI''s agents playing hide-and-seek (Source: [https://youtu.be/kopoLzvh5jY](https://youtu.be/kopoLzvh5jY))'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the fundamentals, next, let's look into some of the
    challenges with MARL.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the challenges in multi-agent reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the earlier chapters in this book, we discussed many challenges in reinforcement
    learning. In particular, the dynamic programming methods we initially introduced
    are not able to scale to problems with complex and large observation and action
    spaces. Deep reinforcement learning approaches, on the other hand, although capable
    of handling complex problems, lack theoretical guarantees and therefore required
    many tricks to stabilize and converge. Now that we talk about problems in which
    there are more than one agent learning, interacting with each other, and affecting
    the environment; the challenges and complexities of single-agent RL are multiplied.
    For this reason, many results in MARL are empirical.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discuss what makes MARL specifically complex and challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Non-stationarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mathematical framework behind single-agent RL is the Markov decision process
    (MDP), which establishes that the environment dynamics depend on the state it
    is in and not the history. This suggests that the environment is stationary, on
    which many approaches rely for convergence. Now that there are multiple agents
    in the environment that are learning hence changing their behavior over time,
    that fundamental assumption falls apart and prevents us analyzing MARL the same
    way we analyze single-agent RL.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider off-policy methods such as Q-learning with replay buffer.
    In MARL, using such approaches is especially challenging because the experience
    that was collected a while ago might be wildly different than how the environment
    (in part, the other agents) responds to the actions of a single agent.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One possible solution to non-stationarity is to account for the actions of the
    other agents, such as using a joint action space. As the number of agents increases,
    this becomes increasingly intractable, which makes scalability an issue in MARL.
  prefs: []
  type: TYPE_NORMAL
- en: Having said this, it is somewhat easier to analyze how the agent behavior could
    converge when there are only two agents in the environment. If you are familiar
    with game theory, a common way to look at such systems is to understand equilibrium
    points where neither of the agents benefit from changing their policies.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: If you need a brief introduction to game theory and Nash equilibrium, check
    out this video at [https://www.youtube.com/watch?v=0i7p9DNvtjk](https://www.youtube.com/watch?v=0i7p9DNvtjk)
  prefs: []
  type: TYPE_NORMAL
- en: This analysis gets significantly harder when there are more than two agents
    in the environment, which makes large-scale MARL very difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Unclear reinforcement learning objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In single-agent RL, the objective is clear: To maximize the expected cumulative
    return. On the other hand, there is no such a unique objective defined MARL.'
  prefs: []
  type: TYPE_NORMAL
- en: Think about a chess game in which we try to train a very good chess agent. To
    this end, we train many agents competing with each other using **self-play**.
    How would you set the objective of this problem? First thought could be to maximize
    the reward of the best agent. But this could result in making all the agents terrible
    players but one. This would certainly not what we would want.
  prefs: []
  type: TYPE_NORMAL
- en: A popular objective in MARL is to achieve convergence to Nash equilibrium. This
    often works well, but it also has disadvantages when the agents are not fully
    rational. Moreover, Nash equilibrium naturally implies overfitting to the policies
    of the other agents, which is not necessarily desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Information sharing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important challenge in MARL is to design the information sharing structure
    between the agents. There are three alternative information structures we can
    consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully centralized**: In this structure, all of the information collected
    by the agents are processed by a central mechanism and the local policies would
    leverage this centralized knowledge. The advantage of this structure is the full
    coordination between the agents. On the other hand, this could lead to an optimization
    problem that won''t scale as the number of the agents grow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully decentralized**: In this structure, no information is exchanged between
    the agents and each agent would act based on their local observations. The obvious
    benefit here is that there is no burden of a centralized coordinator. On the flip
    side, the actions of the agents would be suboptimal due to their limited information
    about the environment. In addition, RL algorithms might have a hard time to converge
    when the training is fully independent due to high partial observability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralized but networked agents**: This structure would allow information
    exchange between small groups of (neighbor) agents. In turn, this would help the
    information spread among them. The challenge here would be to create a robust
    communication structure that would work under different conditions of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the objective of the RL problem as well as the availability of
    computational resources, different approaches could be preferred. Consider a cooperative
    environment in which a large swarm of robots are trying to achieve a common goal.
    In this problem, fully centralized or networked control might make sense. In a
    fully competitive environment, such as a strategy video game, a fully decentralized
    structure might be preferred since there would be no common goal between the agents.
  prefs: []
  type: TYPE_NORMAL
- en: After this much theory, it is now time to go into practice! Soon, we will train
    a tic-tac-toe agent that you can play against during your meetings or classes.
    Let's first describe how we will do the training and then go into implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Training policies in multi-agent settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many algorithms and approaches designed for MARL, which can be classified
    in the following two broad categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent learning**: This approach suggests training agents individually
    while treating the other agents in the environment as part of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralized training and decentralized execution**: In this approach, there
    is a centralized controller that uses information from multiple agents during
    training. At the time of execution (inference), the agents locally execute the
    policies, without relying on a central mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, we can take any of the algorithms we covered in one of the
    previous chapters and use it in a multi-agent setting to train policies via independent
    learning, which, as it turns out, is a very competitive alternative to specialized
    MARL algorithms. So rather than dumping more theory and notation on you, in this
    chapter, we will skip discussing the technical details of any specific MARL algorithm
    and refer you to literature for that.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: An easy and highly recommended read on a comparison of deep MARL algorithms
    is by (Papoudakis et al. 2020). Just visit the references section to find the
    link to the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will use independent learning. How does it work, though? Well, it requires
    us to:'
  prefs: []
  type: TYPE_NORMAL
- en: Have an environment with multiple agents,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain policies to support the agents,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriately assign the rewards coming out of the environment to the agents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It would be tricky for us to come up with a proper framework here to handle
    the points above. Fortunately, RLlib has a multi-agent environment to come to
    the rescue. Next, let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: RLlib multi-agent environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RLlib''s multi-agent environment flexibly allows us to hook up with one of
    the algorithms that you already know to use for MARL. In fact, RLlib documentation
    conveniently shows which algorithms are compatible with this environment type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: RLlib''s algorithm list shows multi-agent compatibility (Source:
    https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.3: RLlib''s algorithm list shows multi-agent compatibility (Source:
    [https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html](https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html))'
  prefs: []
  type: TYPE_NORMAL
- en: In that list, you will also see a separate section for MARL-specific algorithms.
    In this chapter, we will use PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the next step is to understand how to use an algorithm we selected
    with a multi-agent environment. At this point, we need to make a key differentiation:
    *Using RLlib, we will train policies, not the agents (at least directly). An agent
    will be mapped to one of the policies that are being trained to retrieve actions.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'RLlib documentation illustrates this with the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: Relationship between agents and policies in RLlib (source: https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.4: Relationship between agents and policies in RLlib (source: [https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical](https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical))'
  prefs: []
  type: TYPE_NORMAL
- en: In case you have not realized, this gives us a very powerful framework to model
    a MARL environment. For example, we can flexibly add agents to the environment,
    remove them, and train multiple policies for the same task. All is fine as far
    as we specify the mapping between the policies and agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let''s look into what the training loop in RLlib requires to be
    used with a multi-agent environment:'
  prefs: []
  type: TYPE_NORMAL
- en: List of policies with corresponding ids. These are what will be trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A function that maps a given agent id to a policy id, so that RLlib knows where
    the actions for a given agent will come from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once this is set, the environment will use the Gym convention communicate with
    RLlib. The difference will be that observations, rewards, and terminal statements
    will be emitted for multiple agents in the environment. For example, a reset function
    will return a dictionary of observations like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the actions by the policies will be passed to the agents from which
    we received an observation, similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This means that if the environment returns an observation for an agent, it is
    asking an action back.
  prefs: []
  type: TYPE_NORMAL
- en: So far so good! This should suffice to give you some idea about how it works.
    Things will be clearer when we go into the implementation!
  prefs: []
  type: TYPE_NORMAL
- en: Soon, we will train tic-tac-toe policies, as we mentioned. The agents that will
    use these policies will compete against each other to learn how to play the game.
    This is called **competitive self-play**, which we discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Competitive self-play
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-play is a great tool to train RL agents for competitive tasks, which include
    things like board games, multi-player video games, adversarial scenarios etc.
    Many of the famous RL agents you have heard about are trained this way, such as
    AlphaGo, OpenAI Five for Dota 2, and the StarCraft II agent of DeepMind.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: The story of OpenAI Five is a very interesting one, showing how the project
    started and evolved to what it is today. The blog posts on the project gives many
    beneficial information, from hyperparameters used in the models to how the OpenAI
    team overcame interesting challenges throughout the work. You can find the project
    page at [https://openai.com/projects/five/](https://openai.com/projects/five/).
  prefs: []
  type: TYPE_NORMAL
- en: One of the drawbacks of vanilla self-play is that the agents, who only see the
    other agents trained the same way, tend to overfit to each other's strategies.
    To overcome this, it makes sense to train multiple policies and pit them against
    each other, which is also what we will do in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is such a challenge in self-play that even training multiple policies
    and simply setting them against each other is not enough. DeepMind created a "League"
    of agents/policies, like a basketball league, to obtain a really competitive training
    environment, which led to the success of their StarCraft II agents. They explain
    their approach in a nice blog at [https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is time to experiment with multi-agent reinforcement learning!
  prefs: []
  type: TYPE_NORMAL
- en: Training tic-tac-toe agents through self-play
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide you with some key explanations of the code
    in our Github repo to get a better grasp of MARL with RLlib while training tic-tac-toe
    agents on a 3x3 board. For the full code, you can refer to [https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is
    played, see https://en.wikipedia.org/wiki/Tic-tac-toe'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is
    played, see [https://en.wikipedia.org/wiki/Tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's started with designing the multi-agent environment.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the multi-agent tic-tac-toe environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the game, we have two agents, X and O, playing the game. We will train four
    policies for the agents to pull their actions from, and each policy can play either
    an X or O. We construct the environment class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter09/tic_tac_toe.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, 9 refers to the number of squares on the board, each of which can be filled
    by either X, O, or none.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reset this environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, we don't pass this state directly to the policy as it is just full
    of characters. We process it so that, for the player that is about to play, its
    own marks are always represented as 1s and the other player's marks are always
    represented as 2s.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This processed observation is what is passed to the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `step` method processes the action for the player and proceeds
    the environment to the next step. For a win, the player gets a ![](img/Formula_09_008.png),
    and ![](img/Formula_09_009.png) for a loss. Notice that the policies may suggest
    putting the mark on an already-occupied square, a behavior that is penalized by
    a ![](img/Formula_09_010.png) points.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the trainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create 4 policies to train, assign them some ids, and specify their observation
    and action spaces. Here is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter09/ttt_train.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: While creating the config dictionary to pass to the trainer, we map the agents
    to the policies. To mitigate overfitting, rather than assigning a specific policy
    to a given agent, we randomly pick the policy to retrieve the action from for
    the agent that is about to play.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: During training, we save the models as they improve. Since there are multiple
    policies involved, as a proxy to measure the progress, we check if the episodes
    are getting longer with valid moves. Our hope is that as the agents get competitive,
    more and more games will result in draws, at which point the board is full of
    marks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Now the fun of watching the training!
  prefs: []
  type: TYPE_NORMAL
- en: Observing the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Initially in the game, there will be lots of invalid moves, resulting in extended
    episode lengths and excessive penalties for the agents. Therefore, the mean agent
    reward plot will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6: Average agent reward'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.6: Average agent reward'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how this starts in deep negatives to converge to zero, indicating draws
    as the common result. In the meantime, you should see the episode length converging
    to 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 9.7: Episode length progress'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_09_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 9.7: Episode length progress'
  prefs: []
  type: TYPE_NORMAL
- en: When you see the competition on fire, you can stop the training! What will be
    even more interesting is to play against the AI by running the script `ttt_human_vs_ai.py`
    or watch them compete by running `ttt_ai_vs_ai.py`.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we conclude this chapter. This was a fun one, wasn't it? Let's summarize
    the learnings from this chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered multi-agent reinforcement learning. This branch
    of RL is more challenging than others due to multiple decision-makers influencing
    the environment and also evolving over time. After introducing some MARL concepts,
    we explored these challenges in detail. We then proceeded to train tic-tac-toe
    agents through competitive self-play using RLlib. And they were so competitive
    that they kept coming to a draw at the end of the training!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we switch gears to discuss an emerging approach in reinforcement
    learning, called Machine Teaching, which brings the subject matter expert, you,
    more actively into the process to guide the training. Hoping to see you there
    soon!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mosterman, P. J. et al. (2014). A heterogeneous fleet of vehicles for automated
    humanitarian missions. Computing in Science & Engineering, vol. 16, issue 3, pg.
    90-95\. URL: [http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf](http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Papoudakis, Georgios, et al. (2020). Comparative Evaluation of Multi-Agent Deep
    Reinforcement Learning Algorithms. arXiv.org, [http://arxiv.org/abs/2006.07869](http://arxiv.org/abs/2006.07869)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Palanisamy, Praveen. (2019). Multi-Agent Connected Autonomous Driving Using
    Deep Reinforcement Learning. arxiv.org, [https://arxiv.org/abs/1911.04175v1](https://arxiv.org/abs/1911.04175v1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
