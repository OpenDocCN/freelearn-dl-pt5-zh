<html><head></head><body>
<div id="_idContainer890">
<h1 class="chapter-number" id="_idParaDest-128" lang="en-GB"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-129" lang="en-GB"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.2.1">The Attention Mechanism and Transformers</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.5.1">, we outlined </span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.6.1">a typical </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">natural language processing</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">NLP</span></strong><span class="koboSpan" id="kobo.10.1">) pipeline, and we introduced </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.13.1">RNNs</span></strong><span class="koboSpan" id="kobo.14.1">) as a candidate </span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.15.1">architecture for NLP tasks. </span><span class="koboSpan" id="kobo.15.2">But we also outlined their drawbacks—they are inherently sequential (that is, not parallelizable) and cannot process longer sequences, because of the limitations of </span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.16.1">their internal sequence representation. </span><span class="koboSpan" id="kobo.16.2">In this chapter, we’ll introduce the </span><strong class="bold"><span class="koboSpan" id="kobo.17.1">attention mechanism</span></strong><span class="koboSpan" id="kobo.18.1">, which allows a </span><strong class="bold"><span class="koboSpan" id="kobo.19.1">neural network</span></strong><span class="koboSpan" id="kobo.20.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.21.1">NN</span></strong><span class="koboSpan" id="kobo.22.1">) to have </span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.23.1">direct access to the whole input sequence. </span><span class="koboSpan" id="kobo.23.2">We’ll briefly discuss the attention mechanism in the context of RNNs since it was first introduced as an RNN extension. </span><span class="koboSpan" id="kobo.23.3">However, the star of this chapter will be the </span><strong class="bold"><span class="koboSpan" id="kobo.24.1">transformer</span></strong><span class="koboSpan" id="kobo.25.1">—a recent </span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.26.1">NN architecture that relies entirely on attention. </span><span class="koboSpan" id="kobo.26.2">Transformers have been one of the most important NN innovations in the past 10 years. </span><span class="koboSpan" id="kobo.26.3">They are </span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.27.1">at the core of all recent </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">large language models</span></strong><span class="koboSpan" id="kobo.29.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.30.1">LLMs</span></strong><span class="koboSpan" id="kobo.31.1">), such </span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.32.1">as ChatGPT (</span><a href="https://chat.openai.com/"><span class="koboSpan" id="kobo.33.1">https://chat.openai.com/</span></a><span class="koboSpan" id="kobo.34.1">), and even </span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.35.1">image generation models such as Stable Diffusion (</span><a href="https://stability.ai/stable-diffusion"><span class="koboSpan" id="kobo.36.1">https://stability.ai/stable-diffusion</span></a><span class="koboSpan" id="kobo.37.1">). </span><span class="koboSpan" id="kobo.37.2">This is the second chapter in our arc dedicated to NLP and the first of three chapters dedicated </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">to transformers.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.39.1">This chapter will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">following topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.41.1">Introducing </span><strong class="bold"><span class="koboSpan" id="kobo.42.1">sequence-to-sequence</span></strong><span class="koboSpan" id="kobo.43.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.44.1">seq2seq</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">) models</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.46.1">Understanding the </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">attention mechanism</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.48.1">Building transformers </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">with attention</span></span></li>
</ul>
<h1 id="_idParaDest-130" lang="en-GB"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.50.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.51.1">We’ll implement the example in this chapter using Python, PyTorch, and the Hugging Face Transformers library (</span><a href="https://github.com/huggingface/transformers"><span class="koboSpan" id="kobo.52.1">https://github.com/huggingface/transformers</span></a><span class="koboSpan" id="kobo.53.1">). </span><span class="koboSpan" id="kobo.53.2">If you don’t have an environment set up with these tools, fret not—the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.53.3">You can find the code examples in the book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07"><span class="No-Break"><span class="koboSpan" id="kobo.55.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter07</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.56.1">.</span></span></p>
<h1 id="_idParaDest-131" lang="en-GB"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.57.1">Introducing seq2seq models</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.58.1">In </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.59.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.60.1">, we outlined several types of recurrent models, depending on the input/output combinations. </span><span class="koboSpan" id="kobo.60.2">One of them is indirect many-to-many, or </span><strong class="bold"><span class="koboSpan" id="kobo.61.1">seq2seq</span></strong><span class="koboSpan" id="kobo.62.1">, where an input </span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.63.1">sequence is transformed into another, different output sequence, not necessarily with the same length as the input. </span><span class="koboSpan" id="kobo.63.2">One type of seq2seq task is machine translation. </span><span class="koboSpan" id="kobo.63.3">The input sequences are the words of a sentence in one language, and the output sequences are the words of the same sentence translated into another language. </span><span class="koboSpan" id="kobo.63.4">For example, we can translate the English sequence </span><em class="italic"><span class="koboSpan" id="kobo.64.1">tourist attraction</span></em><span class="koboSpan" id="kobo.65.1"> to the German </span><em class="italic"><span class="koboSpan" id="kobo.66.1">Touristenattraktion</span></em><span class="koboSpan" id="kobo.67.1">. </span><span class="koboSpan" id="kobo.67.2">Not only is the output of a different length but there is no direct correspondence between the elements of the input and output sequences. </span><span class="koboSpan" id="kobo.67.3">One output element corresponds to a combination of two </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">input elements.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.69.1">Another type of indirect many-to-many task is conversational chatbots such as ChatGPT, where the initial input sequence is the first user query. </span><span class="koboSpan" id="kobo.69.2">After that, the whole conversation so far (including both user queries and bot responses) serves as an input sequence for the newly generated </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">bot responses.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.71.1">In this section, we’ll focus on encoder-decoder seq2seq models (</span><em class="italic"><span class="koboSpan" id="kobo.72.1">Sequence to Sequence Learning with Neural Networks</span></em><span class="koboSpan" id="kobo.73.1">, </span><a href="https://arxiv.org/abs/1409.3215;"><span class="koboSpan" id="kobo.74.1">https://arxiv.org/abs/1409.3215;</span></a> <em class="italic"><span class="koboSpan" id="kobo.75.1">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</span></em><span class="koboSpan" id="kobo.76.1">, </span><a href="https://arxiv.org/abs/1406.1078"><span class="koboSpan" id="kobo.77.1">https://arxiv.org/abs/1406.1078</span></a><span class="koboSpan" id="kobo.78.1">), first introduced in 2014. </span><span class="koboSpan" id="kobo.78.2">They use RNNs in a way that’s especially suited for solving indirect many-to-many tasks such as these. </span><span class="koboSpan" id="kobo.78.3">The following is a diagram of the seq2seq model, where an input sequence </span><strong class="source-inline"><span class="koboSpan" id="kobo.79.1">[A, B, C, &lt;EOS&gt;]</span></strong><span class="koboSpan" id="kobo.80.1"> is decoded into an output sequence </span><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">[W, X, Y, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">Z, &lt;EOS&gt;]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer694">
<span class="koboSpan" id="kobo.84.1"><img alt="Figure 7.1 – A seq2seq model (inspired by https://arxiv.org/abs/1409.3215)" src="image/B19627_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.85.1">Figure 7.1 – A seq2seq model (inspired by </span><a href="https://arxiv.org/abs/1409.3215"><span class="koboSpan" id="kobo.86.1">https://arxiv.org/abs/1409.3215</span></a><span class="koboSpan" id="kobo.87.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.88.1">The model consists of </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">two parts:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.90.1">Encoder</span></strong><span class="koboSpan" id="kobo.91.1">: An RNN such as </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.93.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.94.1">LSTM</span></strong><span class="koboSpan" id="kobo.95.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">Gated Recurrent Unit</span></strong><span class="koboSpan" id="kobo.97.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.98.1">GRU</span></strong><span class="koboSpan" id="kobo.99.1">). </span><span class="koboSpan" id="kobo.99.2">Taken by itself, the encoder works like a regular RNN—it reads the input sequence, one step at a time, and updates its internal state </span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.100.1">after each step. </span><span class="koboSpan" id="kobo.100.2">The encoder </span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.101.1">will stop reading the input sequence once a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.102.1">&lt;EOS&gt;</span></strong><span class="koboSpan" id="kobo.103.1">—end-of-sequence—token is reached. </span><span class="koboSpan" id="kobo.103.2">Let’s assume that the input is a textual sequence using word-level tokenization. </span><span class="koboSpan" id="kobo.103.3">Then, we’ll use word-embedding vectors as the encoder input at each step, and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">&lt;EOS&gt;</span></strong><span class="koboSpan" id="kobo.105.1"> token signals the end of a sentence. </span><span class="koboSpan" id="kobo.105.2">The encoder output is discarded and has no role in the seq2seq model, as we’re only interested in the hidden </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">encoder state.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.107.1">Decoder</span></strong><span class="koboSpan" id="kobo.108.1">: Once </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.109.1">the encoder is </span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.110.1">finished, we’ll signal the decoder so that it can start generating the output sequence with a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.111.1">&lt;GO&gt;</span></strong><span class="koboSpan" id="kobo.112.1"> input signal. </span><span class="koboSpan" id="kobo.112.2">The encoder is also an RNN (LSTM or GRU). </span><span class="koboSpan" id="kobo.112.3">The link between the encoder and the decoder is the most recent encoder’s internal </span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.113.1">state vector, </span><span class="koboSpan" id="kobo.114.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/550.png" style="vertical-align:-0.340em;height:1.042em;width:0.783em"/></span><span class="koboSpan" id="kobo.115.1"> (also known as the </span><strong class="bold"><span class="koboSpan" id="kobo.116.1">thought vector</span></strong><span class="koboSpan" id="kobo.117.1">), which is fed as the recurrence relation at the first decoder step. </span><span class="koboSpan" id="kobo.117.2">The decoder output </span><span class="koboSpan" id="kobo.118.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/558.png" style="vertical-align:-0.340em;height:0.788em;width:1.200em"/></span><span class="koboSpan" id="kobo.119.1"> at step </span><em class="italic"><span class="koboSpan" id="kobo.120.1">t+1</span></em><span class="koboSpan" id="kobo.121.1"> is one element of the output sequence. </span><span class="koboSpan" id="kobo.121.2">Next, we’ll use </span><span class="koboSpan" id="kobo.122.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/559.png" style="vertical-align:-0.340em;height:0.788em;width:1.215em"/></span><span class="koboSpan" id="kobo.123.1"> as a model input at step </span><em class="italic"><span class="koboSpan" id="kobo.124.1">t+2</span></em><span class="koboSpan" id="kobo.125.1"> to generate new output, and so </span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.126.1">on (this type of model is called </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">autoregressive</span></strong><span class="koboSpan" id="kobo.128.1">). </span><span class="koboSpan" id="kobo.128.2">In the case of textual sequences, the decoder output is a softmax operation over all the words in the vocabulary. </span><span class="koboSpan" id="kobo.128.3">At each step, we take the word with the highest probability, and we feed it as input to the next step. </span><span class="koboSpan" id="kobo.128.4">Once </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">&lt;EOS&gt;</span></strong><span class="koboSpan" id="kobo.130.1"> becomes the most probable symbol, the decoding </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">is finished.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.132.1">An example of an autoregressive model</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.133.1">Let’s assume </span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.134.1">that we want to translate the English sentence </span><em class="italic"><span class="koboSpan" id="kobo.135.1">How are you today?</span></em><span class="koboSpan" id="kobo.136.1"> into Spanish. </span><span class="koboSpan" id="kobo.136.2">We’ll tokenize it as </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">[how, are, you, today, ?, &lt;EOS&gt;]</span></strong><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">An autoregressive model will start with an initial sequence </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">[&lt;GO&gt;]</span></strong><span class="koboSpan" id="kobo.140.1">. </span><span class="koboSpan" id="kobo.140.2">Then, it will generate the first word of the translation and will append it to the existing input sequence: </span><strong class="source-inline"><span class="koboSpan" id="kobo.141.1">[&lt;GO&gt;, ¿]</span></strong><span class="koboSpan" id="kobo.142.1">. </span><span class="koboSpan" id="kobo.142.2">The new sequence will serve as new input to the decoder, so it can produce the next element and extend the sequence again: </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">[&lt;GO&gt;, ¿, cómo]</span></strong><span class="koboSpan" id="kobo.144.1">. </span><span class="koboSpan" id="kobo.144.2">We’ll repeat the same steps until the decoder predicts the </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">&lt;EOS&gt;</span></strong><span class="koboSpan" id="kobo.146.1"> token: </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">[&lt;GO&gt;, ¿, cómo, estás, hoy, ?, &lt;</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">EOS&gt;]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.150.1">The training of the model is supervised, as it needs to know both the input sequence and its corresponding target output sequence (for example, the same text in multiple languages). </span><span class="koboSpan" id="kobo.150.2">We feed the input sequence to the encoder, generate the thought vector, </span><span class="koboSpan" id="kobo.151.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/550.png" style="vertical-align:-0.340em;height:1.042em;width:0.780em"/></span><span class="koboSpan" id="kobo.152.1">, and use it to initiate the output sequence generation from the decoder. </span><span class="koboSpan" id="kobo.152.2">Training the decoder uses </span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.153.1">a process called </span><strong class="bold"><span class="koboSpan" id="kobo.154.1">teacher forcing</span></strong><span class="koboSpan" id="kobo.155.1">—its input at step </span><em class="italic"><span class="koboSpan" id="kobo.156.1">t</span></em><span class="koboSpan" id="kobo.157.1"> is always the correct word from the target sequence at step </span><em class="italic"><span class="koboSpan" id="kobo.158.1">t-1</span></em><span class="koboSpan" id="kobo.159.1">, even if the decoder prediction of step </span><em class="italic"><span class="koboSpan" id="kobo.160.1">t-1</span></em><span class="koboSpan" id="kobo.161.1"> is wrong. </span><span class="koboSpan" id="kobo.161.2">For example, let’s say that the correct target sequence until step </span><em class="italic"><span class="koboSpan" id="kobo.162.1">t</span></em><span class="koboSpan" id="kobo.163.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">[W, X, Y]</span></strong><span class="koboSpan" id="kobo.165.1">, but the current decoder-generated output sequence is </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">[W, X, Z]</span></strong><span class="koboSpan" id="kobo.167.1">. </span><span class="koboSpan" id="kobo.167.2">With teacher forcing, the decoder input at step </span><em class="italic"><span class="koboSpan" id="kobo.168.1">t+1</span></em><span class="koboSpan" id="kobo.169.1"> will be </span><em class="italic"><span class="koboSpan" id="kobo.170.1">Y</span></em><span class="koboSpan" id="kobo.171.1"> instead of </span><em class="italic"><span class="koboSpan" id="kobo.172.1">Z</span></em><span class="koboSpan" id="kobo.173.1">. </span><span class="koboSpan" id="kobo.173.2">In other words, the decoder learns to generate target values </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">[t+1,...]</span></strong><span class="koboSpan" id="kobo.175.1"> given target values </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">[...,t]</span></strong><span class="koboSpan" id="kobo.177.1">. </span><span class="koboSpan" id="kobo.177.2">We can think of this in the following way: the decoder input is the target sequence, while its output (target values) is the same sequence but shifted one position to </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">the right.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.179.1">To summarize, the seq2seq model solves the problem of varying input/output sequence lengths by </span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.180.1">encoding the input sequence into a fixed-length state vector, </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">v</span></strong><span class="koboSpan" id="kobo.182.1">, and then using this vector as a base to generate the output sequence. </span><span class="koboSpan" id="kobo.182.2">We can formalize this by saying that it tries to maximize the </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">following probability:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.184.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∏&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/561.png" style="vertical-align:-0.764em;height:2.201em;width:17.983em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.185.1">This is equivalent to </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">the following:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.187.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/562.png" style="vertical-align:-0.383em;height:1.183em;width:25.345em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.188.1">Let’s look at the elements of this formula in </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">more detail:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.190.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/563.png" style="vertical-align:-0.383em;height:1.183em;width:6.796em"/></span><span class="koboSpan" id="kobo.191.1">: The conditional probability where </span><span class="koboSpan" id="kobo.192.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/564.png" style="vertical-align:-0.383em;height:0.880em;width:3.150em"/></span><span class="koboSpan" id="kobo.193.1"> is the input sequence with length </span><em class="italic"><span class="koboSpan" id="kobo.194.1">T</span></em><span class="koboSpan" id="kobo.195.1"> and </span><span class="koboSpan" id="kobo.196.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/565.png" style="vertical-align:-0.383em;height:0.880em;width:3.275em"/></span><span class="koboSpan" id="kobo.197.1"> is the output sequence with </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">length </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.199.1">T’</span></em></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.200.1">v</span></strong><span class="koboSpan" id="kobo.201.1">: The fixed-length encoding of the input sequence (the </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">thought vector)</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.203.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;mml:mi&gt;‘&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/566.png" style="vertical-align:-0.383em;height:1.183em;width:7.245em"/></span><span class="koboSpan" id="kobo.204.1">: The probability of an output word </span><span class="koboSpan" id="kobo.205.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/567.png" style="vertical-align:-0.333em;height:0.781em;width:0.989em"/></span><span class="koboSpan" id="kobo.206.1"> given prior words </span><em class="italic"><span class="koboSpan" id="kobo.207.1">y</span></em><span class="koboSpan" id="kobo.208.1">, as well as the thought </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">vector, </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.210.1">v</span></strong></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.211.1">The original seq2seq paper introduces a few tricks to enhance the training and performance of the model. </span><span class="koboSpan" id="kobo.211.2">For example, the encoder and decoder are two separate LSTMs. </span><span class="koboSpan" id="kobo.211.3">In the </span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.212.1">case of machine translations, this makes it possible to train different decoders for different languages with the </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">same encoder.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.214.1">Another improvement is that the input sequence is fed to the decoder in reverse. </span><span class="koboSpan" id="kobo.214.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">[A,B,C]</span></strong><span class="koboSpan" id="kobo.216.1"> -&gt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">[W,X,Y,Z]</span></strong><span class="koboSpan" id="kobo.218.1"> would become </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">[C,B,A]</span></strong><span class="koboSpan" id="kobo.220.1"> -&gt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">[W,X,Y,Z]</span></strong><span class="koboSpan" id="kobo.222.1">. </span><span class="koboSpan" id="kobo.222.2">There is no clear explanation of why this works, but the authors have shared their intuition: since this is a step-by-step model, if the sequences were in normal order, each source word in the source sentence would be far from its corresponding word in the output sentence. </span><span class="koboSpan" id="kobo.222.3">If we reverse the input sequence, the average distance between input/output words won’t change, but the first input words will be very close to the first output words. </span><span class="koboSpan" id="kobo.222.4">This will help the model to establish better communication between the input and output sequences. </span><span class="koboSpan" id="kobo.222.5">However, this improvement also illustrates the deficiencies of the hidden state of RNNs (even LSTM or GRU)—the more recent sequence elements suppress the available information for the older elements. </span><span class="koboSpan" id="kobo.222.6">In the next section, we’ll introduce an elegant way to solve this issue once and </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">for all.</span></span></p>
<h1 id="_idParaDest-132" lang="en-GB"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.224.1">Understanding the attention mechanism</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.225.1">In this section, we’ll </span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.226.1">discuss several iterations of the attention mechanism in the order that they </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">were introduced.</span></span></p>
<h2 id="_idParaDest-133" lang="en-GB"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.228.1">Bahdanau attention</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.229.1">The first </span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.230.1">attention iteration (</span><em class="italic"><span class="koboSpan" id="kobo.231.1">Neural Machine Translation by Jointly Learning to Align and Translate</span></em><span class="koboSpan" id="kobo.232.1">, </span><a href="https://arxiv.org/abs/1409.0473"><span class="koboSpan" id="kobo.233.1">https://arxiv.org/abs/1409.0473</span></a><span class="koboSpan" id="kobo.234.1">), known </span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.235.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.236.1">Bahdanau</span></strong><span class="koboSpan" id="kobo.237.1"> attention, extends the seq2seq model with the ability for the decoder to work with all encoder hidden states, not just the last one. </span><span class="koboSpan" id="kobo.237.2">It is an addition to the existing seq2seq model, rather than an independent entity. </span><span class="koboSpan" id="kobo.237.3">The following diagram shows how Bahdanau </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">attention works:</span></span></p>
<p class="IMG---Figure" lang="en-GB"><span class="koboSpan" id="kobo.239.1"><img alt="Figure 7.2 – The attention mechanism" src="image/B19627_07_2.png"/></span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.240.1">Figure 7.2 – The attention mechanism</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.241.1">Don’t worry—it looks scarier than it is. </span><span class="koboSpan" id="kobo.241.2">We’ll go through this diagram from top to bottom: the attention mechanism works by </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.242.1">plugging an additional </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">context vector</span></strong><span class="koboSpan" id="kobo.244.1">, </span><span class="koboSpan" id="kobo.245.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/568.png" style="vertical-align:-0.340em;height:0.793em;width:0.635em"/></span><span class="koboSpan" id="kobo.246.1">, between </span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.247.1">the encoder and the decoder. </span><span class="koboSpan" id="kobo.247.2">The hidden decoder state </span><span class="koboSpan" id="kobo.248.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/569.png" style="vertical-align:-0.340em;height:0.793em;width:0.591em"/></span><span class="koboSpan" id="kobo.249.1"> at time </span><em class="italic"><span class="koboSpan" id="kobo.250.1">t</span></em><span class="koboSpan" id="kobo.251.1"> is now a function not only of the hidden state and decoder output at step </span><em class="italic"><span class="koboSpan" id="kobo.252.1">t-1</span></em><span class="koboSpan" id="kobo.253.1"> but also of the context </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">vector </span></span><span class="No-Break"><span class="koboSpan" id="kobo.255.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/570.png" style="vertical-align:-0.340em;height:0.793em;width:0.650em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.257.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/571.png" style="vertical-align:-0.390em;height:1.101em;width:6.932em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.258.1">Each decoder </span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.259.1">step has a unique context vector, and the context vector for one decoder step is just </span><em class="italic"><span class="koboSpan" id="kobo.260.1">a weighted sum of all encoder hidden states</span></em><span class="koboSpan" id="kobo.261.1">. </span><span class="koboSpan" id="kobo.261.2">In this way, the encoder can access all input sequence states at each output step </span><em class="italic"><span class="koboSpan" id="kobo.262.1">t</span></em><span class="koboSpan" id="kobo.263.1">, which removes the necessity to encode all information of the source sequence into a fixed-length thought vector, as the regular seq2seq </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">model does:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.265.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/572.png" style="vertical-align:-0.767em;height:2.201em;width:5.261em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.266.1">Let’s discuss this formula in </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">more detail:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.268.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/570.png" style="vertical-align:-0.340em;height:0.793em;width:0.650em"/></span><span class="koboSpan" id="kobo.269.1">: The context vector for a decoder output step </span><em class="italic"><span class="koboSpan" id="kobo.270.1">t</span></em><span class="koboSpan" id="kobo.271.1"> out of </span><em class="italic"><span class="koboSpan" id="kobo.272.1">T’</span></em><span class="koboSpan" id="kobo.273.1"> total </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">output steps</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.275.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span><span class="koboSpan" id="kobo.276.1">: The hidden state vector of encoder step </span><em class="italic"><span class="koboSpan" id="kobo.277.1">i</span></em><span class="koboSpan" id="kobo.278.1"> out of </span><em class="italic"><span class="koboSpan" id="kobo.279.1">T</span></em><span class="koboSpan" id="kobo.280.1"> total </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">input steps</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.282.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/575.png" style="vertical-align:-0.402em;height:0.850em;width:0.998em"/></span><span class="koboSpan" id="kobo.283.1">: The scalar weight associated with </span><span class="koboSpan" id="kobo.284.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span><span class="koboSpan" id="kobo.285.1"> in the context of the current decoder </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">step </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.287.1">t</span></em></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.288.1">Note that </span><span class="koboSpan" id="kobo.289.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/577.png" style="vertical-align:-0.402em;height:0.850em;width:0.989em"/></span><span class="koboSpan" id="kobo.290.1"> is unique for both the encoder and decoder steps—that is, the input sequence states </span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.291.1">will have different weights depending on the current output step. </span><span class="koboSpan" id="kobo.291.2">For example, if the input and output sequences </span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.292.1">have lengths of 10, then the weights will be represented by a 10×10 matrix for a total of 100 weights. </span><span class="koboSpan" id="kobo.292.2">This means that the attention mechanism will focus the attention (get it?) of the decoder on different parts of the input sequence, depending on the current state of the output sequence. </span><span class="koboSpan" id="kobo.292.3">If </span><span class="koboSpan" id="kobo.293.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/575.png" style="vertical-align:-0.402em;height:0.850em;width:0.998em"/></span><span class="koboSpan" id="kobo.294.1"> is large, then the decoder will pay a lot of attention to </span><span class="koboSpan" id="kobo.295.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span><span class="koboSpan" id="kobo.296.1"> at </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">step </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.298.1">t</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.300.1">But how do we compute the weights </span><span class="koboSpan" id="kobo.301.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/575.png" style="vertical-align:-0.402em;height:0.850em;width:0.998em"/></span><span class="koboSpan" id="kobo.302.1">? </span><span class="koboSpan" id="kobo.302.2">First, we should mention that the sum of all </span><span class="koboSpan" id="kobo.303.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/581.png" style="vertical-align:-0.402em;height:0.850em;width:0.989em"/></span><span class="koboSpan" id="kobo.304.1"> weights for a decoder at step </span><em class="italic"><span class="koboSpan" id="kobo.305.1">t</span></em><span class="koboSpan" id="kobo.306.1"> is 1. </span><span class="koboSpan" id="kobo.306.2">We can implement this with a softmax operation on top of the </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">attention mechanism:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.308.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/582.png" style="vertical-align:-1.001em;height:2.250em;width:14.296em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.309.1">Here, </span><span class="koboSpan" id="kobo.310.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/583.png" style="vertical-align:-0.402em;height:0.850em;width:0.842em"/></span><span class="koboSpan" id="kobo.311.1"> is an alignment score, which indicates how well the input sequence elements around position </span><em class="italic"><span class="koboSpan" id="kobo.312.1">i</span></em><span class="koboSpan" id="kobo.313.1"> match (or align with) the output at position </span><em class="italic"><span class="koboSpan" id="kobo.314.1">t</span></em><span class="koboSpan" id="kobo.315.1">. </span><span class="koboSpan" id="kobo.315.2">This score (represented by the weight </span><span class="koboSpan" id="kobo.316.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/584.png" style="vertical-align:-0.402em;height:0.850em;width:0.997em"/></span><span class="koboSpan" id="kobo.317.1">) is based on the previous decoder state </span><span class="koboSpan" id="kobo.318.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/585.png" style="vertical-align:-0.340em;height:0.793em;width:1.197em"/></span><span class="koboSpan" id="kobo.319.1"> (we use </span><span class="koboSpan" id="kobo.320.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/585.png" style="vertical-align:-0.340em;height:0.793em;width:1.197em"/></span><span class="koboSpan" id="kobo.321.1"> because we have not computed </span><span class="koboSpan" id="kobo.322.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/587.png" style="vertical-align:-0.340em;height:0.793em;width:0.592em"/></span><span class="koboSpan" id="kobo.323.1"> yet), as well as the encoder </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">state </span></span><span class="No-Break"><span class="koboSpan" id="kobo.325.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.327.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/589.png" style="vertical-align:-0.402em;height:1.154em;width:6.049em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.328.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.329.1">a</span></em><span class="koboSpan" id="kobo.330.1"> (not alpha) is a differentiable function, which is trained with backpropagation together with the rest of the system. </span><span class="koboSpan" id="kobo.330.2">Different functions satisfy these requirements, but the </span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.331.1">authors of the paper chose the so-called </span><strong class="bold"><span class="koboSpan" id="kobo.332.1">additive attention</span></strong><span class="koboSpan" id="kobo.333.1">, which combines </span><span class="koboSpan" id="kobo.334.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.235em"/></span><span class="koboSpan" id="kobo.335.1"> and </span><span class="koboSpan" id="kobo.336.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/591.png" style="vertical-align:-0.340em;height:1.042em;width:0.763em"/></span><span class="koboSpan" id="kobo.337.1"> with the help of vector addition. </span><span class="koboSpan" id="kobo.337.2">It exists in </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">two flavors:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.339.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/592.png" style="vertical-align:-0.440em;height:1.242em;width:15.456em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.340.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/593.png" style="vertical-align:-0.402em;height:1.154em;width:17.184em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.341.1">In the first formula, </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">W</span></strong><span class="koboSpan" id="kobo.343.1"> is a weight matrix, applied over the concatenated vectors </span><span class="koboSpan" id="kobo.344.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.345.1"> and </span><span class="koboSpan" id="kobo.346.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span><span class="koboSpan" id="kobo.347.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.348.1">v</span></strong><span class="koboSpan" id="kobo.349.1"> is a weight </span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.350.1">vector. </span><span class="koboSpan" id="kobo.350.2">The second formula is similar, but this time we have separate </span><strong class="bold"><span class="koboSpan" id="kobo.351.1">fully connected</span></strong><span class="koboSpan" id="kobo.352.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.353.1">FC</span></strong><span class="koboSpan" id="kobo.354.1">) layers (the weight matrices </span><span class="koboSpan" id="kobo.355.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/596.png" style="vertical-align:-0.333em;height:0.982em;width:1.308em"/></span><span class="koboSpan" id="kobo.356.1"> and </span><span class="koboSpan" id="kobo.357.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/597.png" style="vertical-align:-0.333em;height:0.982em;width:1.308em"/></span><span class="koboSpan" id="kobo.358.1">) and we sum </span><span class="koboSpan" id="kobo.359.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.360.1"> and </span><span class="koboSpan" id="kobo.361.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/574.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span><span class="koboSpan" id="kobo.362.1">. </span><span class="koboSpan" id="kobo.362.2">In both cases, the alignment model can be represented </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.363.1">as a simple </span><strong class="bold"><span class="koboSpan" id="kobo.364.1">feedforward network</span></strong><span class="koboSpan" id="kobo.365.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.366.1">FFN</span></strong><span class="koboSpan" id="kobo.367.1">) with one </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">hidden layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.369.1">Now that </span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.370.1">we know the formulas for </span><span class="koboSpan" id="kobo.371.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/570.png" style="vertical-align:-0.340em;height:0.793em;width:0.650em"/></span><span class="koboSpan" id="kobo.372.1"> and </span><span class="koboSpan" id="kobo.373.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/575.png" style="vertical-align:-0.402em;height:0.850em;width:0.998em"/></span><span class="koboSpan" id="kobo.374.1">, let’s replace </span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.375.1">the latter with </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">the former:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.377.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/602.png" style="vertical-align:-0.928em;height:2.362em;width:12.957em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.378.1">As a conclusion, let’s summarize the attention algorithm in a step-by-step manner </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">as follows:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.380.1">Feed the encoder with the input sequence and compute the set of hidden states, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.381.1">H</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.382.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.383.1">{</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.384.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.385.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.386.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.387.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.388.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.389.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.390.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.391.1">…</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.392.1">h</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.393.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.394.1">T</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.395.1">}</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.397.1">Compute the alignment scores, </span><span class="koboSpan" id="kobo.398.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/603.png" style="vertical-align:-0.402em;height:1.154em;width:6.028em"/></span><span class="koboSpan" id="kobo.399.1">, that use the decoder state from the preceding step </span><span class="koboSpan" id="kobo.400.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.401.1">. </span><span class="koboSpan" id="kobo.401.2">If </span><em class="italic"><span class="koboSpan" id="kobo.402.1">t=1</span></em><span class="koboSpan" id="kobo.403.1">, we’ll use the last encoder state, </span><span class="koboSpan" id="kobo.404.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/605.png" style="vertical-align:-0.333em;height:1.035em;width:0.951em"/></span><span class="koboSpan" id="kobo.405.1">, as the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">hidden state.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.407.1">Compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">weights </span></span><span class="No-Break"><span class="koboSpan" id="kobo.409.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/606.png" style="vertical-align:-0.452em;height:1.166em;width:8.673em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.411.1">Compute the context </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">vector </span></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/607.png" style="vertical-align:-0.404em;height:1.166em;width:5.864em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.415.1">Compute the hidden state, </span><span class="koboSpan" id="kobo.416.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;RNN&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/608.png" style="vertical-align:-0.440em;height:1.089em;width:11.074em"/></span><span class="koboSpan" id="kobo.417.1">, based on the concatenated vectors </span><span class="koboSpan" id="kobo.418.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/609.png" style="vertical-align:-0.340em;height:0.793em;width:1.212em"/></span><span class="koboSpan" id="kobo.419.1"> and </span><span class="koboSpan" id="kobo.420.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/570.png" style="vertical-align:-0.340em;height:0.793em;width:0.650em"/></span><span class="koboSpan" id="kobo.421.1"> and the previous decoder output </span><span class="koboSpan" id="kobo.422.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/611.png" style="vertical-align:-0.340em;height:0.788em;width:1.231em"/></span><span class="koboSpan" id="kobo.423.1">. </span><span class="koboSpan" id="kobo.423.2">At this point, we can compute the final output </span><span class="koboSpan" id="kobo.424.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/612.png" style="vertical-align:-0.340em;height:0.788em;width:0.609em"/></span><span class="koboSpan" id="kobo.425.1">. </span><span class="koboSpan" id="kobo.425.2">In the case where we need to classify the next word, we’ll use the softmax output, </span><span class="koboSpan" id="kobo.426.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/613.png" style="vertical-align:-0.340em;height:1.051em;width:5.325em"/></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.427.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/614.png" style="vertical-align:-0.533em;height:1.232em;width:2.959em"/></span></span><span class="koboSpan" id="kobo.428.1">, where </span><span class="koboSpan" id="kobo.429.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/615.png" style="vertical-align:-0.483em;height:1.132em;width:1.286em"/></span><span class="koboSpan" id="kobo.430.1"> is a </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">weight matrix.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.432.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.433.1">steps 2</span></em><span class="koboSpan" id="kobo.434.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.435.1">5</span></em><span class="koboSpan" id="kobo.436.1"> until the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">the sequence.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.438.1">Next, we’ll discuss a slightly improved version of </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">Bahdanau attention.</span></span></p>
<h2 id="_idParaDest-134" lang="en-GB"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.440.1">Luong attention</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.441.1">Luong attention</span></strong><span class="koboSpan" id="kobo.442.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.443.1">Effective Approaches to Attention-based Neural Machine Translation</span></em><span class="koboSpan" id="kobo.444.1">, </span><a href="https://arxiv.org/abs/1508.04025"><span class="koboSpan" id="kobo.445.1">https://arxiv.org/abs/1508.04025</span></a><span class="koboSpan" id="kobo.446.1">) introduces several improvements over Bahdanau attention. </span><span class="koboSpan" id="kobo.446.2">Most </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.447.1">notably, the alignment scores depend on the </span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.448.1">decoder’s hidden state </span><span class="koboSpan" id="kobo.449.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.605em"/></span><span class="koboSpan" id="kobo.450.1">, as opposed to </span><span class="koboSpan" id="kobo.451.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/617.png" style="vertical-align:-0.340em;height:0.793em;width:1.218em"/></span><span class="koboSpan" id="kobo.452.1"> in Bahdanau attention. </span><span class="koboSpan" id="kobo.452.2">To better understand this, let’s compare the </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">two algorithms:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer757">
<span class="koboSpan" id="kobo.454.1"><img alt="Figure 7.3 – Left: Bahdanau attention; right: Luong attention" src="image/B19627_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.455.1">Figure 7.3 – Left: Bahdanau attention; right: Luong attention</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.456.1">We’ll go through a step-by-step execution of </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">Luong attention:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.458.1">Feed the encoder with the input sequence and compute the set of encoder hidden </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">states </span></span><span class="No-Break"><span class="koboSpan" id="kobo.460.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/618.png" style="vertical-align:-0.000em;height:0.648em;width:1.801em"/></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.461.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/619.png" style="vertical-align:-0.383em;height:1.135em;width:5.184em"/></span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.463.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;RN&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;N&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;decoder&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/620.png" style="vertical-align:-0.390em;height:1.039em;width:9.700em"/></span><span class="koboSpan" id="kobo.464.1">: Compute the decoder’s hidden state based on the previous decoder’s hidden state </span><span class="koboSpan" id="kobo.465.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.466.1"> and the previous decoder’s output </span><span class="koboSpan" id="kobo.467.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/611.png" style="vertical-align:-0.340em;height:0.788em;width:1.231em"/></span><span class="koboSpan" id="kobo.468.1"> (not the context </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">vector, though).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.470.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/623.png" style="vertical-align:-0.402em;height:1.154em;width:5.509em"/></span><span class="koboSpan" id="kobo.471.1">: Compute the alignment scores, which use the decoder state from the current step, </span><span class="koboSpan" id="kobo.472.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/624.png" style="vertical-align:-0.340em;height:0.793em;width:0.698em"/></span><span class="koboSpan" id="kobo.473.1">. </span><span class="koboSpan" id="kobo.473.2">Besides additive attention, the Luong attention paper also proposes </span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.474.1">two types of </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.475.1">multiplicative attention</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.477.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/625.png" style="vertical-align:-0.402em;height:1.120em;width:3.895em"/></span><span class="koboSpan" id="kobo.478.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.479.1">Dot product</span></strong><span class="koboSpan" id="kobo.480.1"> without </span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.481.1">any parameters. </span><span class="koboSpan" id="kobo.481.2">In this case, the vectors </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">s</span></strong><span class="koboSpan" id="kobo.483.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.484.1">h</span></strong><span class="koboSpan" id="kobo.485.1"> (represented as column and row matrices) need to have the </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">same sizes.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.487.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/626.png" style="vertical-align:-0.402em;height:1.120em;width:5.732em"/></span><span class="koboSpan" id="kobo.488.1">: Here, </span><span class="koboSpan" id="kobo.489.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/627.png" style="vertical-align:-0.340em;height:0.989em;width:1.487em"/></span><span class="koboSpan" id="kobo.490.1"> is a trainable weight matrix of the </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">attention layer.</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.492.1">The multiplication </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.493.1">of the vectors as an alignment </span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.494.1">score measurement has an intuitive explanation—as we mentioned in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.495.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.496.1">, the dot product acts as a similarity measure between vectors. </span><span class="koboSpan" id="kobo.496.2">Therefore, if the vectors are similar (that is, aligned), the result of the multiplication will be a large value and the attention will be focused on the current </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.497.1">t,i</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.498.1"> relationship.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.499.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/606.png" style="vertical-align:-0.452em;height:1.166em;width:8.673em"/></span><span class="koboSpan" id="kobo.500.1">: Compute </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">the weights.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.502.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/607.png" style="vertical-align:-0.404em;height:1.166em;width:5.864em"/></span><span class="koboSpan" id="kobo.503.1">: Compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">context vector.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.505.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;~&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/630.png" style="vertical-align:-0.440em;height:1.151em;width:8.338em"/></span><span class="koboSpan" id="kobo.506.1">: Compute the intermediate vector based on the concatenated vectors </span><span class="koboSpan" id="kobo.507.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/570.png" style="vertical-align:-0.340em;height:0.793em;width:0.655em"/></span><span class="koboSpan" id="kobo.508.1"> and </span><span class="koboSpan" id="kobo.509.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span><span class="koboSpan" id="kobo.510.1">. </span><span class="koboSpan" id="kobo.510.2">At this point, we can compute the final output </span><span class="koboSpan" id="kobo.511.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/612.png" style="vertical-align:-0.340em;height:0.788em;width:0.609em"/></span><span class="koboSpan" id="kobo.512.1">. </span><span class="koboSpan" id="kobo.512.2">In the case of classification, we’ll use softmax, </span><span class="koboSpan" id="kobo.513.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;~&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/634.png" style="vertical-align:-0.533em;height:1.244em;width:8.255em"/></span><span class="koboSpan" id="kobo.514.1">, where </span><span class="koboSpan" id="kobo.515.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/615.png" style="vertical-align:-0.483em;height:1.132em;width:1.286em"/></span><span class="koboSpan" id="kobo.516.1"> is a </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">weight matrix.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.518.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.519.1">steps 2</span></em><span class="koboSpan" id="kobo.520.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.521.1">6</span></em><span class="koboSpan" id="kobo.522.1"> until the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">the sequence.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.524.1">Next, we’ll use Bahdanau and Luong attention as a stepping stone to a generic </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">attention mechanism.</span></span></p>
<h2 id="_idParaDest-135" lang="en-GB"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.526.1">General attention</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.527.1">Although </span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.528.1">we’ve discussed the attention mechanism </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.529.1">in the context of seq2seq with RNNs, it is a general </span><strong class="bold"><span class="koboSpan" id="kobo.530.1">deep learning</span></strong><span class="koboSpan" id="kobo.531.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.532.1">DL</span></strong><span class="koboSpan" id="kobo.533.1">) technique </span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.534.1">in its own right. </span><span class="koboSpan" id="kobo.534.2">To understand it, let’s start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer776">
<span class="koboSpan" id="kobo.536.1"><img alt="Figure 7.4 – General attention" src="image/B19627_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.537.1">Figure 7.4 – General attention</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.538.1">It starts with a query, </span><strong class="bold"><span class="koboSpan" id="kobo.539.1">q</span></strong><span class="koboSpan" id="kobo.540.1">, executed against a database of key-value pairs, </span><strong class="bold"><span class="koboSpan" id="kobo.541.1">k</span></strong><span class="koboSpan" id="kobo.542.1"> and </span><span class="koboSpan" id="kobo.543.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/636.png" style="vertical-align:-0.335em;height:0.776em;width:0.832em"/></span><span class="koboSpan" id="kobo.544.1">, respectively. </span><span class="koboSpan" id="kobo.544.2">Each key, </span><span class="koboSpan" id="kobo.545.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/637.png" style="vertical-align:-0.340em;height:1.042em;width:0.870em"/></span><span class="koboSpan" id="kobo.546.1">, has a single corresponding value, </span><span class="koboSpan" id="kobo.547.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/638.png" style="vertical-align:-0.532em;height:0.973em;width:1.020em"/></span><span class="koboSpan" id="kobo.548.1">. </span><span class="koboSpan" id="kobo.548.2">The query, keys, and values are </span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.549.1">vectors. </span><span class="koboSpan" id="kobo.549.2">Because of this, we can represent </span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.550.1">the key-value store as two matrices, </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">K</span></strong><span class="koboSpan" id="kobo.552.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">V</span></strong><span class="koboSpan" id="kobo.554.1">. </span><span class="koboSpan" id="kobo.554.2">If we have multiple queries, </span><span class="koboSpan" id="kobo.555.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/639.png" style="vertical-align:-0.340em;height:0.793em;width:1.026em"/></span><span class="koboSpan" id="kobo.556.1">, we can also represent them as a matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.557.1">Q</span></strong><span class="koboSpan" id="kobo.558.1">. </span><span class="koboSpan" id="kobo.558.2">Hence, these are often abbreviated as </span><strong class="bold"><span class="koboSpan" id="kobo.559.1">Q</span></strong><span class="koboSpan" id="kobo.560.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.561.1">K</span></strong><span class="koboSpan" id="kobo.562.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.564.1">V</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.566.1">Differences between general attention and Bahdanau/Luong attention</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.567.1">Unlike </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.568.1">general attention, the keys </span><strong class="bold"><span class="koboSpan" id="kobo.569.1">K</span></strong><span class="koboSpan" id="kobo.570.1"> and the </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.571.1">values </span><strong class="bold"><span class="koboSpan" id="kobo.572.1">V</span></strong><span class="koboSpan" id="kobo.573.1"> of Bahdanau and Luong </span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.574.1">attention are the same thing—that is, these </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.575.1">attention models are more like </span><strong class="bold"><span class="koboSpan" id="kobo.576.1">Q</span></strong><span class="koboSpan" id="kobo.577.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.578.1">V</span></strong><span class="koboSpan" id="kobo.579.1">, rather than </span><strong class="bold"><span class="koboSpan" id="kobo.580.1">Q</span></strong><span class="koboSpan" id="kobo.581.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.582.1">K</span></strong><span class="koboSpan" id="kobo.583.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.584.1">V</span></strong><span class="koboSpan" id="kobo.585.1">. </span><span class="koboSpan" id="kobo.585.2">Having separate keys and values provides more flexibility to the general attention—the keys specialize in matching the input queries, and the values carry the actual information. </span><span class="koboSpan" id="kobo.585.3">We can think of the Bahdanau vector </span><span class="koboSpan" id="kobo.586.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/640.png" style="vertical-align:-0.340em;height:0.793em;width:1.175em"/></span><span class="koboSpan" id="kobo.587.1"> (or </span><span class="koboSpan" id="kobo.588.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/641.png" style="vertical-align:-0.340em;height:0.793em;width:0.578em"/></span><span class="koboSpan" id="kobo.589.1"> in Luong attention) as the query, </span><span class="koboSpan" id="kobo.590.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/642.png" style="vertical-align:-0.340em;height:0.793em;width:1.001em"/></span><span class="koboSpan" id="kobo.591.1">, executed against the database of key-value pairs, where the keys/values are the hidden </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">states </span></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/643.png" style="vertical-align:-0.340em;height:1.042em;width:0.759em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.595.1">General attention uses a multiplicative, rather than additive, mechanism (like Luong attention). </span><span class="koboSpan" id="kobo.595.2">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">it works:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.597.1">The starting point is one of the input query </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">vectors, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.599.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/644.png" style="vertical-align:-0.340em;height:0.793em;width:1.044em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.601.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/645.png" style="vertical-align:-0.532em;height:1.250em;width:5.350em"/></span><span class="koboSpan" id="kobo.602.1">: Compute the alignment scores, using the dot product between the query, </span><span class="koboSpan" id="kobo.603.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/646.png" style="vertical-align:-0.340em;height:0.793em;width:1.071em"/></span><span class="koboSpan" id="kobo.604.1">, and each key vector, </span><span class="koboSpan" id="kobo.605.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/647.png" style="vertical-align:-0.340em;height:1.042em;width:0.873em"/></span><span class="koboSpan" id="kobo.606.1">. </span><span class="koboSpan" id="kobo.606.2">As we mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.607.1">Bahdanau attention</span></em><span class="koboSpan" id="kobo.608.1"> section, the dot product acts as a similarity measure, and it makes sense to use it on </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">this occasion.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.610.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/648.png" style="vertical-align:-0.630em;height:1.595em;width:6.776em"/></span><span class="koboSpan" id="kobo.611.1">: Compute the final weights of each value vector against the query with the help </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">of softmax.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.613.1">The final </span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.614.1">attention vector is the weighted </span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.615.1">addition (that is, an element-wise sum) of all value </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">vectors, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.617.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/649.png" style="vertical-align:-0.532em;height:0.973em;width:0.936em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">:</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.619.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Attention&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/650.png" style="vertical-align:-1.040em;height:2.473em;width:22.066em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.620.1">To better understand the attention mechanism, we’ll use the numerical example displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer792">
<span class="koboSpan" id="kobo.622.1"><img alt="Figure 7.5 – An attention example with a four-dimensional query executed against a key-value store with four vectors" src="image/B19627_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.623.1">Figure 7.5 – An attention example with a four-dimensional query executed against a key-value store with four vectors</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.624.1">Let’s track it step </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">by step:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.626.1">Execute a four-dimensional query vector, </span><span class="koboSpan" id="kobo.627.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.6,1.2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1.2,1.8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/651.png" style="vertical-align:-0.333em;height:1.023em;width:9.253em"/></span><span class="koboSpan" id="kobo.628.1">, against a key-value store of four </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">four-dimensional vectors.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.630.1">Compute the alignment scores. </span><span class="koboSpan" id="kobo.630.2">For example, the first score is </span><span class="koboSpan" id="kobo.631.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;q&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;k&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.6&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;0.2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;0.4&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1.8&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;0.8&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;36&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/652.png" style="vertical-align:-0.528em;height:1.211em;width:12.955em"/></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.632.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;q&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;k&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.6&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;0.2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;0.4&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;1.2&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1.8&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;0.8&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mn&gt;36&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/653.png" style="vertical-align:-0.062em;height:0.746em;width:12.885em"/></span></span><span class="koboSpan" id="kobo.633.1">. </span><span class="koboSpan" id="kobo.633.2">The rest of the scores are displayed in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.634.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.635.1">.5</span></em><span class="koboSpan" id="kobo.636.1">. </span><span class="koboSpan" id="kobo.636.2">We have intentionally selected the query, </span><span class="koboSpan" id="kobo.637.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.6,1.2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1.2,1.8&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/654.png" style="vertical-align:-0.333em;height:1.023em;width:9.278em"/></span><span class="koboSpan" id="kobo.638.1">, to be relatively similar to the second key vector, </span><span class="koboSpan" id="kobo.639.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.2,0.4&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;0.6,0.6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/655.png" style="vertical-align:-0.333em;height:1.035em;width:9.220em"/></span><span class="koboSpan" id="kobo.640.1">. </span><span class="koboSpan" id="kobo.640.2">In this way, </span><span class="koboSpan" id="kobo.641.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/656.png" style="vertical-align:-0.333em;height:1.035em;width:0.839em"/></span><span class="koboSpan" id="kobo.642.1"> has the largest alignment score, </span><span class="koboSpan" id="kobo.643.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2.4&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/657.png" style="vertical-align:-0.528em;height:1.162em;width:4.234em"/></span><span class="koboSpan" id="kobo.644.1">, </span><br/><span class="koboSpan" id="kobo.645.1">and it should have the largest influence over the </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">final result.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.647.1">Compute the weights, </span><span class="koboSpan" id="kobo.648.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/658.png" style="vertical-align:-0.532em;height:0.980em;width:1.813em"/></span><span class="koboSpan" id="kobo.649.1">, with the help of softmax—for example, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.650.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.651.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.652.1">q</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.653.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.654.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.655.1">,</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.656.1">k</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.657.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.658.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.659.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.660.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.661.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.662.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.663.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.664.1">2.4</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.665.1">)</span></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.666.1">/</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.667.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.668.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.669.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.670.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.671.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.672.1">0.36</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.673.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.674.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.675.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.676.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.677.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.678.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.679.1">2.4</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.680.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.681.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.682.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.683.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.684.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.685.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.686.1">0.36</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.687.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.688.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.689.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.690.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.691.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.692.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.693.1">0.36</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.694.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.695.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.696.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.697.1">0.756</span></span><span class="koboSpan" id="kobo.698.1">. </span><span class="koboSpan" id="kobo.698.2">The key vector, </span><span class="koboSpan" id="kobo.699.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/659.png" style="vertical-align:-0.333em;height:1.035em;width:0.842em"/></span><span class="koboSpan" id="kobo.700.1">, has the largest weight because of its large alignment score. </span><span class="koboSpan" id="kobo.700.2">The softmax function exaggerates the differences between the inputs, hence the final weight of </span><span class="koboSpan" id="kobo.701.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/660.png" style="vertical-align:-0.333em;height:1.035em;width:0.938em"/></span><span class="koboSpan" id="kobo.702.1"> is even higher compared to the ratio of the input </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">alignment scores.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.704.1">Compute </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.705.1">the final result, </span><span class="koboSpan" id="kobo.706.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;r&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;[&lt;/mml:mo&gt;&lt;mml:mn&gt;1.98,2.98,3.98,4.98&lt;/mml:mn&gt;&lt;mml:mo&gt;]&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/661.png" style="vertical-align:-0.127em;height:0.761em;width:10.024em"/></span><span class="koboSpan" id="kobo.707.1">, which is the weighted element-wise sum of the value vectors, </span><span class="koboSpan" id="kobo.708.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/662.png" style="vertical-align:-0.532em;height:0.973em;width:1.109em"/></span><span class="koboSpan" id="kobo.709.1">. </span><span class="koboSpan" id="kobo.709.2">For example, we can compute the first element of the result as </span><span class="koboSpan" id="kobo.710.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.098&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.756&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.048&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;0.098&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1.98&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/663.png" style="vertical-align:-0.333em;height:0.973em;width:23.465em"/></span><span class="koboSpan" id="kobo.711.1">. </span><span class="koboSpan" id="kobo.711.2">We can see that the values </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.712.1">of the result are closest to the value vector, </span><span class="koboSpan" id="kobo.713.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/664.png" style="vertical-align:-0.528em;height:0.969em;width:0.997em"/></span><span class="koboSpan" id="kobo.714.1">, which, again, reflects the large alignment between the key vector, </span><span class="koboSpan" id="kobo.715.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/659.png" style="vertical-align:-0.333em;height:1.035em;width:0.842em"/></span><span class="koboSpan" id="kobo.716.1">, and the input </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">query, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.718.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/666.png" style="vertical-align:-0.333em;height:0.786em;width:0.865em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.720.1">I hope that this example has helped you understand the attention mechanism, as this is one of the major DL innovations in the past 10 years. </span><span class="koboSpan" id="kobo.720.2">Next, we’ll discuss an even more advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">attention version.</span></span></p>
<h2 id="_idParaDest-136" lang="en-GB"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.722.1">Transformer attention</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.723.1">In this section, we’ll </span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.724.1">discuss the attention mechanism, as it appears in </span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.725.1">the transformer NN architecture (</span><em class="italic"><span class="koboSpan" id="kobo.726.1">Attention Is All You Need</span></em><span class="koboSpan" id="kobo.727.1">, </span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.728.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.729.1">). </span><span class="koboSpan" id="kobo.729.2">Don’t worry—you don’t need to know about transformers yet, as </span><strong class="bold"><span class="koboSpan" id="kobo.730.1">transformer attention</span></strong><span class="koboSpan" id="kobo.731.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.732.1">TA</span></strong><span class="koboSpan" id="kobo.733.1">) is an independent self-sufficient building block of the entire model. </span><span class="koboSpan" id="kobo.733.2">It is displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer809">
<span class="koboSpan" id="kobo.735.1"><img alt="Figure 7.6 – Scaled dot product (multiplicative) TA (inspired by https://arxiv.org/abs/1706.03762)" src="image/B19627_07_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.736.1">Figure 7.6 – Scaled dot product (multiplicative) TA (inspired by </span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.737.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.738.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.739.1">The TA uses dot product (multiplicative) similarity and follows the general attention procedure </span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.740.1">we introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.741.1">General attention</span></em><span class="koboSpan" id="kobo.742.1"> section (as we have already mentioned, it is not restricted to RNN models). </span><span class="koboSpan" id="kobo.742.2">We can define it </span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.743.1">with the </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.745.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/667.png" style="vertical-align:-1.020em;height:2.248em;width:16.156em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.746.1">In practice, we’ll compute the TA function over a set of queries simultaneously, packed in a matrix </span><strong class="bold"><span class="koboSpan" id="kobo.747.1">Q</span></strong><span class="koboSpan" id="kobo.748.1"> (the keys </span><strong class="bold"><span class="koboSpan" id="kobo.749.1">K</span></strong><span class="koboSpan" id="kobo.750.1">, the values </span><strong class="bold"><span class="koboSpan" id="kobo.751.1">V</span></strong><span class="koboSpan" id="kobo.752.1">, and the result are also matrices). </span><span class="koboSpan" id="kobo.752.2">Let’s discuss the steps of the formula in </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">more detail:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.754.1">Match the queries, </span><strong class="bold"><span class="koboSpan" id="kobo.755.1">Q</span></strong><span class="koboSpan" id="kobo.756.1">, against the database (keys </span><strong class="bold"><span class="koboSpan" id="kobo.757.1">K</span></strong><span class="koboSpan" id="kobo.758.1">) with matrix multiplication to produce the alignment scores, </span><span class="koboSpan" id="kobo.759.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/668.png" style="vertical-align:-0.168em;height:0.886em;width:2.024em"/></span><span class="koboSpan" id="kobo.760.1">. </span><span class="koboSpan" id="kobo.760.2">Matrix multiplication is equivalent to applying dot product similarity between each unique pair of query and key vectors. </span><span class="koboSpan" id="kobo.760.3">Let’s assume that we want to match </span><em class="italic"><span class="koboSpan" id="kobo.761.1">m</span></em><span class="koboSpan" id="kobo.762.1"> different queries to a database of </span><em class="italic"><span class="koboSpan" id="kobo.763.1">n</span></em><span class="koboSpan" id="kobo.764.1"> values and the query-key vector length is </span><span class="koboSpan" id="kobo.765.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/669.png" style="vertical-align:-0.340em;height:1.051em;width:0.739em"/></span><span class="koboSpan" id="kobo.766.1">. </span><span class="koboSpan" id="kobo.766.2">Then, we have the query matrix, </span><span class="koboSpan" id="kobo.767.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/670.png" style="vertical-align:-0.168em;height:0.915em;width:4.271em"/></span><span class="koboSpan" id="kobo.768.1">, with one </span><span class="koboSpan" id="kobo.769.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/671.png" style="vertical-align:-0.340em;height:1.051em;width:0.709em"/></span><span class="koboSpan" id="kobo.770.1">-dimensional query per row for </span><em class="italic"><span class="koboSpan" id="kobo.771.1">m</span></em><span class="koboSpan" id="kobo.772.1"> total rows. </span><span class="koboSpan" id="kobo.772.2">Similarly, we have the key matrix, </span><span class="koboSpan" id="kobo.773.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/672.png" style="vertical-align:-0.027em;height:0.775em;width:4.344em"/></span><span class="koboSpan" id="kobo.774.1">, with one </span><span class="koboSpan" id="kobo.775.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/673.png" style="vertical-align:-0.340em;height:1.051em;width:0.783em"/></span><span class="koboSpan" id="kobo.776.1">-dimensional key vector per row for </span><em class="italic"><span class="koboSpan" id="kobo.777.1">n</span></em><span class="koboSpan" id="kobo.778.1"> total rows (its transpose is </span><span class="koboSpan" id="kobo.779.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/674.png" style="vertical-align:-0.027em;height:0.775em;width:4.676em"/></span><span class="koboSpan" id="kobo.780.1">). </span><span class="koboSpan" id="kobo.780.2">Then, the output matrix will be </span><span class="koboSpan" id="kobo.781.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/675.png" style="vertical-align:-0.168em;height:0.886em;width:5.633em"/></span><span class="koboSpan" id="kobo.782.1">, where one row contains the alignment scores of a single query against all keys of </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">the database:</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.784.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo stretchy=&quot;true&quot;&gt;⏟&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mi&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo stretchy=&quot;true&quot;&gt;⏟&lt;/mo&gt;&lt;/munder&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo stretchy=&quot;true&quot;&gt;⏟&lt;/mo&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/676.png" style="vertical-align:-2.678em;height:4.921em;width:24.261em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.785.1">In other words, we can match multiple queries against multiple database keys in a single matrix-matrix multiplication. </span><span class="koboSpan" id="kobo.785.2">For example, in the context of translation, we can compute the alignment scores of all words of the target sentence over all words of the source sentence in the </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">same way.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.787.1">2.	</span><span class="koboSpan" id="kobo.787.2">Scale the alignment scores with </span><span class="koboSpan" id="kobo.788.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/677.png" style="vertical-align:-0.380em;height:1.261em;width:2.337em"/></span><span class="koboSpan" id="kobo.789.1"> , where </span><span class="koboSpan" id="kobo.790.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/678.png" style="vertical-align:-0.340em;height:1.051em;width:0.756em"/></span><span class="koboSpan" id="kobo.791.1"> is the same vector size as the key vectors in the matrix </span><strong class="bold"><span class="koboSpan" id="kobo.792.1">K</span></strong><span class="koboSpan" id="kobo.793.1">, which is also equal to the size of the query vectors in </span><strong class="bold"><span class="koboSpan" id="kobo.794.1">Q</span></strong><span class="koboSpan" id="kobo.795.1"> (analogously, </span><span class="koboSpan" id="kobo.796.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/679.png" style="vertical-align:-0.340em;height:1.051em;width:0.731em"/></span><span class="koboSpan" id="kobo.797.1"> is the vector size of the value vectors </span><strong class="bold"><span class="koboSpan" id="kobo.798.1">V</span></strong><span class="koboSpan" id="kobo.799.1">). </span><span class="koboSpan" id="kobo.799.2">The authors of the paper </span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.800.1">suspect that for large values of </span><span class="koboSpan" id="kobo.801.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/680.png" style="vertical-align:-0.340em;height:1.051em;width:0.743em"/></span><span class="koboSpan" id="kobo.802.1">, the dot product grows large in magnitude and pushes the softmax in </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.803.1">regions with extremely small gradients. </span><span class="koboSpan" id="kobo.803.2">This, in turn, leads to the vanishing gradients problem, hence the need to scale </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">the results.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.805.1">3.	</span><span class="koboSpan" id="kobo.805.2">Compute the attention scores with the softmax operation along the rows of the matrix (we’ll talk </span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.806.1">about the </span><strong class="bold"><span class="koboSpan" id="kobo.807.1">mask</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.808.1">operation later):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.809.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtable columnspacing=&quot;0.8000em&quot; columnwidth=&quot;auto auto&quot; columnalign=&quot;center center&quot; rowspacing=&quot;1.0000ex&quot; rowalign=&quot;baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtable columnwidth=&quot;auto&quot; columnalign=&quot;center&quot; rowspacing=&quot;1.0000ex&quot; rowalign=&quot;baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtable columnspacing=&quot;0.8000em&quot; columnwidth=&quot;auto auto&quot; columnalign=&quot;center center&quot; rowalign=&quot;baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/681.png" style="vertical-align:-2.710em;height:5.927em;width:27.712em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.810.1">4.	</span><span class="koboSpan" id="kobo.810.2">Compute the final attention vector by multiplying the attention scores with the </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">values </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.812.1">V</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.814.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;V&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtable columnspacing=&quot;0.8000em&quot; columnwidth=&quot;auto auto&quot; columnalign=&quot;center center&quot; rowspacing=&quot;1.0000ex&quot; rowalign=&quot;baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mtable columnwidth=&quot;auto&quot; columnalign=&quot;center&quot; rowspacing=&quot;1.0000ex&quot; rowalign=&quot;baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtable columnspacing=&quot;0.8000em&quot; columnwidth=&quot;auto auto&quot; columnalign=&quot;center center&quot; rowalign=&quot;baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;s&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;a&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mi&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/682.png" style="vertical-align:-2.710em;height:5.927em;width:39.199em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.815.1">The full TA uses </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.816.1">a collection of such attention blocks and is known as </span><strong class="bold"><span class="koboSpan" id="kobo.817.1">multi-head attention</span></strong><span class="koboSpan" id="kobo.818.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.819.1">MHA</span></strong><span class="koboSpan" id="kobo.820.1">), as displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer826">
<span class="koboSpan" id="kobo.822.1"><img alt="Figure 7.7 – MHA (inspired by https://arxiv.org/abs/1706.03762)" src="image/B19627_07_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.823.1">Figure 7.7 – MHA (inspired by https://arxiv.org/abs/1706.03762)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.824.1">Instead of </span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.825.1">a single attention function with </span><span class="koboSpan" id="kobo.826.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/683.png" style="vertical-align:-0.340em;height:1.051em;width:1.805em"/></span><span class="koboSpan" id="kobo.827.1">-dimensional keys, we linearly project the keys, queries, and values </span><em class="italic"><span class="koboSpan" id="kobo.828.1">h</span></em><span class="koboSpan" id="kobo.829.1"> times to produce </span><em class="italic"><span class="koboSpan" id="kobo.830.1">h</span></em><span class="koboSpan" id="kobo.831.1"> different </span><span class="koboSpan" id="kobo.832.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/684.png" style="vertical-align:-0.340em;height:1.051em;width:0.721em"/></span><span class="koboSpan" id="kobo.833.1">-, </span><span class="koboSpan" id="kobo.834.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/684.png" style="vertical-align:-0.340em;height:1.051em;width:0.721em"/></span><span class="koboSpan" id="kobo.835.1">-, and </span><span class="koboSpan" id="kobo.836.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/686.png" style="vertical-align:-0.340em;height:1.051em;width:0.686em"/></span><span class="koboSpan" id="kobo.837.1">-dimensional projections of these values. </span><span class="koboSpan" id="kobo.837.2">Then, we apply </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.838.1">separate parallel attention blocks (or </span><strong class="bold"><span class="koboSpan" id="kobo.839.1">heads</span></strong><span class="koboSpan" id="kobo.840.1">) over the newly created vectors, which yield a single </span><span class="koboSpan" id="kobo.841.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/687.png" style="vertical-align:-0.340em;height:1.051em;width:0.684em"/></span><span class="koboSpan" id="kobo.842.1">-dimensional output for each head. </span><span class="koboSpan" id="kobo.842.2">Next, we concatenate the head outputs and linearly project them to produce the final </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">attention result.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.844.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.845.1">By linear projection, we mean applying an FC layer. </span><span class="koboSpan" id="kobo.845.2">That is, initially we branch the </span><strong class="bold"><span class="koboSpan" id="kobo.846.1">Q</span></strong><span class="koboSpan" id="kobo.847.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.848.1">K</span></strong><span class="koboSpan" id="kobo.849.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.850.1">V</span></strong><span class="koboSpan" id="kobo.851.1"> matrices with the help of separate FC operations. </span><span class="koboSpan" id="kobo.851.2">In the end, we use an FC layer to combine and compress the concatenated head outputs. </span><span class="koboSpan" id="kobo.851.3">In this case, we follow the terminology used in the </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">original paper.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.853.1">MHA allows each head to attend to different elements of the sequence. </span><span class="koboSpan" id="kobo.853.2">At the same time, the model combines the outputs of the heads in a single cohesive representation. </span><span class="koboSpan" id="kobo.853.3">More precisely, we can define this with the </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.855.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;M&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;l&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;H&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;d&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;C&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;c&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/688.png" style="vertical-align:-0.390em;height:1.142em;width:23.305em"/></span></p>
<p lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.856.1">Here, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.857.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/689.png" style="vertical-align:-0.390em;height:1.160em;width:16.165em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.859.1">Let’s look at this in more detail, starting with </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">the heads:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.861.1">Each head receives the linearly projected versions of the initial </span><strong class="bold"><span class="koboSpan" id="kobo.862.1">Q</span></strong><span class="koboSpan" id="kobo.863.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.864.1">K</span></strong><span class="koboSpan" id="kobo.865.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.866.1">V</span></strong><span class="koboSpan" id="kobo.867.1"> matrices. </span><span class="koboSpan" id="kobo.867.2">The projections are computed with the learnable weight matrices </span><span class="koboSpan" id="kobo.868.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/690.png" style="vertical-align:-0.340em;height:1.060em;width:1.479em"/></span><span class="koboSpan" id="kobo.869.1">, </span><span class="koboSpan" id="kobo.870.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/691.png" style="vertical-align:-0.340em;height:1.051em;width:1.448em"/></span><span class="koboSpan" id="kobo.871.1">, and </span><span class="koboSpan" id="kobo.872.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/692.png" style="vertical-align:-0.340em;height:1.051em;width:1.462em"/></span><span class="koboSpan" id="kobo.873.1"> respectively (again, the projections are FC layers). </span><span class="koboSpan" id="kobo.873.2">Note that we have a separate set of weights for each component (</span><strong class="bold"><span class="koboSpan" id="kobo.874.1">Q</span></strong><span class="koboSpan" id="kobo.875.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.876.1">K</span></strong><span class="koboSpan" id="kobo.877.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.878.1">V</span></strong><span class="koboSpan" id="kobo.879.1">) and for each head, </span><em class="italic"><span class="koboSpan" id="kobo.880.1">i</span></em><span class="koboSpan" id="kobo.881.1">. </span><span class="koboSpan" id="kobo.881.2">To satisfy the transformation from </span><span class="koboSpan" id="kobo.882.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/693.png" style="vertical-align:-0.340em;height:1.051em;width:1.866em"/></span><span class="koboSpan" id="kobo.883.1"> to </span><span class="koboSpan" id="kobo.884.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/680.png" style="vertical-align:-0.340em;height:1.051em;width:0.748em"/></span><span class="koboSpan" id="kobo.885.1"> and </span><span class="koboSpan" id="kobo.886.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/695.png" style="vertical-align:-0.340em;height:1.051em;width:0.708em"/></span><span class="koboSpan" id="kobo.887.1">, the dimensions of these matrices are </span><span class="koboSpan" id="kobo.888.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/696.png" style="vertical-align:-0.340em;height:1.088em;width:5.739em"/></span><span class="koboSpan" id="kobo.889.1">, </span><span class="koboSpan" id="kobo.890.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/697.png" style="vertical-align:-0.340em;height:1.088em;width:5.706em"/></span><span class="koboSpan" id="kobo.891.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.892.1">and </span></span><span class="No-Break"><span class="koboSpan" id="kobo.893.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/698.png" style="vertical-align:-0.340em;height:1.088em;width:5.699em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.894.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.895.1">Once </span><strong class="bold"><span class="koboSpan" id="kobo.896.1">Q</span></strong><span class="koboSpan" id="kobo.897.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.898.1">K</span></strong><span class="koboSpan" id="kobo.899.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.900.1">V</span></strong><span class="koboSpan" id="kobo.901.1"> are transformed, we can compute the attention of each head using the regular attention block we described at the beginning of </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">this section.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.903.1">The final attention result is the linear projection (FC layer with a weight matrix </span><span class="koboSpan" id="kobo.904.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/699.png" style="vertical-align:-0.011em;height:0.731em;width:1.407em"/></span><span class="koboSpan" id="kobo.905.1"> of learnable weights) over the concatenated head </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">outputs </span></span><span class="No-Break"><span class="koboSpan" id="kobo.907.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/700.png" style="vertical-align:-0.340em;height:1.042em;width:2.342em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.909.1">So far, we’ve assumed </span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.910.1">that the attention works for different </span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.911.1">input and output sequences. </span><span class="koboSpan" id="kobo.911.2">For example, in translation, each word of the translated sentence </span><em class="italic"><span class="koboSpan" id="kobo.912.1">attends</span></em><span class="koboSpan" id="kobo.913.1"> to the words of the source sentence. </span><span class="koboSpan" id="kobo.913.2">However, there is another valid attention use case. </span><span class="koboSpan" id="kobo.913.3">The transformer </span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.914.1">also relies on </span><strong class="bold"><span class="koboSpan" id="kobo.915.1">self-attention</span></strong><span class="koboSpan" id="kobo.916.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.917.1">intra-attention</span></strong><span class="koboSpan" id="kobo.918.1">), where </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.919.1">the queries, </span><strong class="bold"><span class="koboSpan" id="kobo.920.1">Q</span></strong><span class="koboSpan" id="kobo.921.1">, belong to the same dataset as the keys, </span><strong class="bold"><span class="koboSpan" id="kobo.922.1">K</span></strong><span class="koboSpan" id="kobo.923.1">, and values, </span><strong class="bold"><span class="koboSpan" id="kobo.924.1">V</span></strong><span class="koboSpan" id="kobo.925.1">, of the query database. </span><span class="koboSpan" id="kobo.925.2">In other words, in self-attention, the source and the target are the same sequence (in our case, the same sentence). </span><span class="koboSpan" id="kobo.925.3">The benefit of self-attention is not immediately obvious, as there is no direct task to apply it to. </span><span class="koboSpan" id="kobo.925.4">On an intuitive level, it allows us to see the relationship between words of the same sequence. </span><span class="koboSpan" id="kobo.925.5">To understand why this is important, let’s recall the word2vec model (</span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.926.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.927.1">), where we use the context of a word (that is, its surrounding words) to learn an embedding vector of said word. </span><span class="koboSpan" id="kobo.927.2">One of the limitations of word2vec is that the embedding is static (or context independent)—we have a single embedding vector for all contexts of the word in the whole training corpus. </span><span class="koboSpan" id="kobo.927.3">For example, the word </span><em class="italic"><span class="koboSpan" id="kobo.928.1">new</span></em><span class="koboSpan" id="kobo.929.1"> will have the same embedding vector, regardless of whether we use it in the phrase </span><em class="italic"><span class="koboSpan" id="kobo.930.1">new shoes</span></em><span class="koboSpan" id="kobo.931.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.932.1">New York</span></em><span class="koboSpan" id="kobo.933.1">. </span><span class="koboSpan" id="kobo.933.2">Self-attention </span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.934.1">allows us to solve this problem by </span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.935.1">creating a </span><strong class="bold"><span class="koboSpan" id="kobo.936.1">dynamic embedding</span></strong><span class="koboSpan" id="kobo.937.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.938.1">context dependent</span></strong><span class="koboSpan" id="kobo.939.1">) of that word. </span><span class="koboSpan" id="kobo.939.2">We won’t go into too much detail just yet (we’ll do this in the </span><em class="italic"><span class="koboSpan" id="kobo.940.1">Building transformers with attention</span></em><span class="koboSpan" id="kobo.941.1"> section), but the dynamic embedding works in the following way: we feed the current word into the attention block, but also its current immediate surrounding (context). </span><span class="koboSpan" id="kobo.941.2">The word is the query, </span><strong class="bold"><span class="koboSpan" id="kobo.942.1">q</span></strong><span class="koboSpan" id="kobo.943.1">, and the context is the </span><strong class="bold"><span class="koboSpan" id="kobo.944.1">K</span></strong><span class="koboSpan" id="kobo.945.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.946.1">V</span></strong><span class="koboSpan" id="kobo.947.1"> key-value store. </span><span class="koboSpan" id="kobo.947.2">In this way, the self-attention mechanism allows the model to produce a dynamic embedding vector, unique to the current context of the word. </span><span class="koboSpan" id="kobo.947.3">This vector serves as an input for a variety of downstream tasks. </span><span class="koboSpan" id="kobo.947.4">Its purpose is similar to the static word2vec embedding, but it is much more expressive and makes it possible to solve more complex tasks with </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">greater accuracy.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.949.1">We can </span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.950.1">illustrate how self-attention works with the following </span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.951.1">diagram, which shows the multi-head self-attention of the word </span><em class="italic"><span class="koboSpan" id="kobo.952.1">economy</span></em><span class="koboSpan" id="kobo.953.1"> (different colors represent different </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">attention heads):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer845">
<span class="koboSpan" id="kobo.955.1"><img alt="Figure 7.8 – Multi-head self-attention of the word “economy” (generated by https://github.com/jessevig/bertviz)" src="image/B19627_07_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.956.1">Figure 7.8 – Multi-head self-attention of the word “economy” (generated by https://github.com/jessevig/bertviz)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.957.1">We can see that the strongest link to </span><em class="italic"><span class="koboSpan" id="kobo.958.1">economy</span></em><span class="koboSpan" id="kobo.959.1"> comes from the word </span><em class="italic"><span class="koboSpan" id="kobo.960.1">market</span></em><span class="koboSpan" id="kobo.961.1">, which makes sense because the two words form a phrase with a unique meaning. </span><span class="koboSpan" id="kobo.961.2">However, we can also see that different heads attend to different, further, parts of the </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">input sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.963.1">As a </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.964.1">conclusion to this section, let’s outline the advantages </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.965.1">of the attention mechanism compared to the way RNNs </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">process sequences:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.967.1">Direct access to the elements of the sequence</span></strong><span class="koboSpan" id="kobo.968.1">: An RNN encodes the information of the input elements in a single hidden (thought vector). </span><span class="koboSpan" id="kobo.968.2">In theory, it represents a distilled version of all sequence elements so far. </span><span class="koboSpan" id="kobo.968.3">In practice, it has limited representational power—it can only preserve meaningful information for a sequence with a maximum length of around 100 tokens before the newest tokens start erasing the information of the </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">older ones.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.970.1">In contrast, the attention mechanism provides direct access to all input sequence elements. </span><span class="koboSpan" id="kobo.970.2">On one hand, this imposes a strict limit on the maximum sequence length. </span><span class="koboSpan" id="kobo.970.3">On the other hand, it makes it possible, as of the time of writing book, to have transformer-based LLMs, which can process sequences of more than </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">32,000 tokens.</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.972.1">Parallel processing of the input sequence</span></strong><span class="koboSpan" id="kobo.973.1">: An RNN processes the input sequence elements one by one, in the order of their arrival. </span><span class="koboSpan" id="kobo.973.2">Therefore, we cannot parallelize RNNs. </span><span class="koboSpan" id="kobo.973.3">Compare this with the attention mechanism—it consists exclusively of </span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.974.1">matrix multiplication operations, which are </span><strong class="bold"><span class="koboSpan" id="kobo.975.1">embarrassingly parallel</span></strong><span class="koboSpan" id="kobo.976.1">. </span><span class="koboSpan" id="kobo.976.2">This makes it possible to train LLMs with billions of trainable parameters over large </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">training datasets.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.978.1">But these advantages come with one disadvantage—where an RNN preserves the order of the sequence elements, the attention mechanism, with its direct access, does not. </span><span class="koboSpan" id="kobo.978.2">However, we’ll introduce a workaround to that limitation in the </span><em class="italic"><span class="koboSpan" id="kobo.979.1">Transformer </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.980.1">encoder</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.981.1"> section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.982.1">This concludes our theoretical introduction to TA. </span><span class="koboSpan" id="kobo.982.2">Next, let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">implement it.</span></span></p>
<h2 id="_idParaDest-137" lang="en-GB"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.984.1">Implementing TA</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.985.1">In this section, we’ll </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.986.1">implement MHA, following the definitions from the </span><em class="italic"><span class="koboSpan" id="kobo.987.1">Transformer attention</span></em><span class="koboSpan" id="kobo.988.1"> section. </span><span class="koboSpan" id="kobo.988.2">The code in this section is part of the larger transformer implementation, which we’ll discuss throughout the chapter. </span><span class="koboSpan" id="kobo.988.3">We won’t include the full source code, but you can find it in the book’s </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">GitHub repo.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.990.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.991.1">This example is based on </span><a href="https://github.com/harvardnlp/annotated-transformer"><span class="koboSpan" id="kobo.992.1">https://github.com/harvardnlp/annotated-transformer</span></a><span class="koboSpan" id="kobo.993.1">. </span><span class="koboSpan" id="kobo.993.2">Let’s also note </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.994.1">that PyTorch has native transformer modules (the documentation is available at </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"><span class="koboSpan" id="kobo.995.1">https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html</span></a><span class="koboSpan" id="kobo.996.1">). </span><span class="koboSpan" id="kobo.996.2">Still, in this section, we’ll implement TA from scratch to understand </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">it better.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.998.1">We’ll start with the implementation of regular scaled dot product attention. </span><span class="koboSpan" id="kobo.998.2">As a reminder, it implements the formula </span><span class="koboSpan" id="kobo.999.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/701.png" style="vertical-align:-0.430em;height:1.361em;width:16.906em"/></span><span class="koboSpan" id="kobo.1000.1">, where </span><strong class="bold"><span class="koboSpan" id="kobo.1001.1">Q</span></strong><span class="koboSpan" id="kobo.1002.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.1003.1">query</span></strong><span class="koboSpan" id="kobo.1004.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1005.1">K</span></strong><span class="koboSpan" id="kobo.1006.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.1007.1">key</span></strong><span class="koboSpan" id="kobo.1008.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1009.1">V</span></strong><span class="koboSpan" id="kobo.1010.1"> = </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1011.1">value</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1013.1">
def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    # 1) and 2) Compute the alignment scores with scaling
    scores = (query @ key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    # 3) Compute the attention scores (softmax)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    # 4) Apply the attention scores over the values
    return p_attn @ value, p_attn</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1014.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1015.1">attention</span></strong><span class="koboSpan" id="kobo.1016.1"> function includes </span><strong class="source-inline"><span class="koboSpan" id="kobo.1017.1">dropout</span></strong><span class="koboSpan" id="kobo.1018.1">, as it is part of the full transformer implementation. </span><span class="koboSpan" id="kobo.1018.2">Once again, we’ll leave the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">mask</span></strong><span class="koboSpan" id="kobo.1020.1"> parameter and its purpose for later. </span><span class="koboSpan" id="kobo.1020.2">Let’s also note a novel detail—the use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">@</span></strong><span class="koboSpan" id="kobo.1022.1"> operator (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">query @ key.transpose(-2, -1)</span></strong><span class="koboSpan" id="kobo.1024.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">p_attn @ value</span></strong><span class="koboSpan" id="kobo.1026.1">), which, as of Python 3.5, is reserved for </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">matrix multiplication.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1028.1">Next, let’s continue </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.1029.1">with the MHA implementation. </span><span class="koboSpan" id="kobo.1029.2">As a reminder, the implementation follows the formula: </span><span class="koboSpan" id="kobo.1030.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;M&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;l&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;H&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;d&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;C&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;c&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/702.png" style="vertical-align:-0.390em;height:1.142em;width:22.672em"/></span><span class="koboSpan" id="kobo.1031.1"> . </span><span class="koboSpan" id="kobo.1031.2">Here, </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1032.1">h</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1033.1">e</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1034.1">a</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1035.1">d</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1036.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1037.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1038.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1039.1">A</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1040.1">t</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1041.1">t</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1042.1">e</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1043.1">n</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1044.1">t</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1045.1">i</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1046.1">o</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1047.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1048.1">(</span></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1049.1">Q</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1050.1">W</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1051.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1052.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1053.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1054.1">Q</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1055.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><br/></span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1056.1">K</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1057.1">W</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1058.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1059.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1060.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1061.1">K</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1062.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1063.1">V</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.1064.1">W</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1065.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1066.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1067.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1068.1">V</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1069.1">)</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1070.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1071.1">We’ll implement it as a subclass of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1072.1">torch.nn.Module</span></strong><span class="koboSpan" id="kobo.1073.1">, called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1074.1">MultiHeadedAttention</span></strong><span class="koboSpan" id="kobo.1075.1">. </span><span class="koboSpan" id="kobo.1075.2">We’ll start with </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">the constructor:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1077.1">
class MultiHeadedAttention(torch.nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        """
        :param h: number of heads
        :param d_model: query/key/value vector length
        """
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        # Create 4 fully connected layers
        # 3 for the query/key/value projections
        # 1 to concatenate the outputs of all heads
        self.fc_layers = clones(
            torch.nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = torch.nn.Dropout(p=dropout)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1078.1">Note that we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1079.1">clones</span></strong><span class="koboSpan" id="kobo.1080.1"> function (implemented on GitHub) to create four identical FC </span><strong class="source-inline"><span class="koboSpan" id="kobo.1081.1">self.fc_layers</span></strong><span class="koboSpan" id="kobo.1082.1"> instances. </span><span class="koboSpan" id="kobo.1082.2">We’ll use three of them for the </span><strong class="bold"><span class="koboSpan" id="kobo.1083.1">Q</span></strong><span class="koboSpan" id="kobo.1084.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.1085.1">K</span></strong><span class="koboSpan" id="kobo.1086.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.1087.1">V</span></strong><span class="koboSpan" id="kobo.1088.1"> multi-head linear projections—</span><span class="koboSpan" id="kobo.1089.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/703.png" style="vertical-align:-0.340em;height:1.060em;width:1.516em"/></span><span class="koboSpan" id="kobo.1090.1">, </span><span class="koboSpan" id="kobo.1091.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/704.png" style="vertical-align:-0.340em;height:1.051em;width:1.495em"/></span><span class="koboSpan" id="kobo.1092.1">, and </span><span class="koboSpan" id="kobo.1093.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/705.png" style="vertical-align:-0.340em;height:1.051em;width:1.510em"/></span><span class="koboSpan" id="kobo.1094.1">. </span><span class="koboSpan" id="kobo.1094.2">The fourth FC layer is to merge the concatenated results of the outputs of the different heads, </span><span class="koboSpan" id="kobo.1095.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow /&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/706.png" style="vertical-align:-0.333em;height:1.053em;width:1.441em"/></span><span class="koboSpan" id="kobo.1096.1">. </span><span class="koboSpan" id="kobo.1096.2">We’ll store the current attention results in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1097.1">self.attn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1"> property.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1099.1">Next, let’s implement the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1100.1">MultiHeadedAttention.forward</span></strong><span class="koboSpan" id="kobo.1101.1"> method. </span><span class="koboSpan" id="kobo.1101.2">Please bear in mind </span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.1102.1">that the declaration should be indented, as it is a property of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1103.1">MultiHeadedAttention</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1"> class:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1105.1">
def forward(self, query, key, value, mask=None):
    if mask is not None:
        # Same mask applied to all h heads.
</span><span class="koboSpan" id="kobo.1105.2">        mask = mask.unsqueeze(1)
    batch_samples = query.size(0)
    # 1) Do all the linear projections in batch from d_model =&gt; h x d_k
    projections = [
        l(x).view(batch_samples, -1, self.h, self.d_k)
        .transpose(1, 2)
        for l, x in zip(self.fc_layers, (query, key, value))
    ]
    query, key, value = projections
    # 2) Apply attention on all the projected vectors in batch.
</span><span class="koboSpan" id="kobo.1105.3">    x, self.attn = attention(
        query, key, value,
        mask=mask,
        dropout=self.dropout)
    # 3) "Concat" using a view and apply a final linear.
</span><span class="koboSpan" id="kobo.1105.4">    x = x.transpose(1, 2).contiguous() \
        .view(batch_samples, -1, self.h * self.d_k)
    return self.fc_layers[-1](x)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1106.1">We iterate </span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.1107.1">over the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1108.1">query</span></strong><span class="koboSpan" id="kobo.1109.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1110.1">key</span></strong><span class="koboSpan" id="kobo.1111.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1112.1">value</span></strong><span class="koboSpan" id="kobo.1113.1"> tensors and their reference projection, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1114.1">self.fc_layers</span></strong><span class="koboSpan" id="kobo.1115.1">, and produce </span><strong class="source-inline"><span class="koboSpan" id="kobo.1116.1">query</span></strong><span class="koboSpan" id="kobo.1117.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1118.1">key</span></strong><span class="koboSpan" id="kobo.1119.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1120.1">value</span></strong><span class="koboSpan" id="kobo.1121.1"> projections with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1122.1">following snippet:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1123.1">
l(x).view(batch_samples, -1, self.h, self.d_k).transpose(1, 2)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1124.1">Then, we apply regular attention over the projections using the attention function we first defined. </span><span class="koboSpan" id="kobo.1124.2">Next, we concatenate the outputs of the multiple heads, and finally, we feed them to the last FC layer (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1125.1">self.fc_layers[-1]</span></strong><span class="koboSpan" id="kobo.1126.1">) and return </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">the results.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1128.1">Now that we’ve discussed the TA, let’s continue with the transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">model itself.</span></span></p>
<h1 id="_idParaDest-138" lang="en-GB"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.1130.1">Building transformers with attention</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1131.1">We’ve spent </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.1132.1">the better part of this chapter touting the </span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.1133.1">advantages of the attention mechanism. </span><span class="koboSpan" id="kobo.1133.2">It’s time to reveal the full </span><strong class="bold"><span class="koboSpan" id="kobo.1134.1">transformer</span></strong><span class="koboSpan" id="kobo.1135.1"> architecture, which, unlike RNNs, relies solely on the </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.1136.1">attention mechanism (</span><em class="italic"><span class="koboSpan" id="kobo.1137.1">Attention Is All You Need</span></em><span class="koboSpan" id="kobo.1138.1">, </span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.1139.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.1140.1">). </span><span class="koboSpan" id="kobo.1140.2">The following diagram shows two of the most popular </span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.1141.1">transformer flavors, </span><strong class="bold"><span class="koboSpan" id="kobo.1142.1">post-ln</span></strong><span class="koboSpan" id="kobo.1143.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1144.1">pre-ln</span></strong><span class="koboSpan" id="kobo.1145.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1146.1">post-normalization</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1147.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1148.1">pre-normalization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1149.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer852">
<span class="koboSpan" id="kobo.1150.1"><img alt="Figure 7.9 – Left: the original (post-normalization, post-ln) transformer; right: pre-normalization (pre-ln) transformer (inspired by https://arxiv.org/abs/1706.03762)" src="image/B19627_07_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1151.1">Figure 7.9 – Left: the original (post-normalization, post-ln) transformer; right: pre-normalization (pre-ln) transformer (inspired by https://arxiv.org/abs/1706.03762)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1152.1">It looks scary, but fret not—it’s easier than it seems. </span><span class="koboSpan" id="kobo.1152.2">In this section, we’ll discuss the transformer </span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.1153.1">in the context of the seq2seq task, which we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1154.1">Introducing seq2seq models</span></em><span class="koboSpan" id="kobo.1155.1"> section. </span><span class="koboSpan" id="kobo.1155.2">That is, it will take a sequence of tokens as input, and it will output another, different, token sequence. </span><span class="koboSpan" id="kobo.1155.3">As with the seq2seq model, it has </span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.1156.1">two components—an </span><strong class="bold"><span class="koboSpan" id="kobo.1157.1">encoder</span></strong><span class="koboSpan" id="kobo.1158.1"> and a </span><strong class="bold"><span class="koboSpan" id="kobo.1159.1">decoder</span></strong><span class="koboSpan" id="kobo.1160.1">. </span><span class="koboSpan" id="kobo.1160.2">We’ll start </span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.1161.1">with the encoder (the left-hand component of both sections of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">preceding diagram).</span></span></p>
<h2 id="_idParaDest-139" lang="en-GB"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.1163.1">Transformer encoder</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1164.1">The encoder begins with an input sequence of one-hot-encoded tokens. </span><span class="koboSpan" id="kobo.1164.2">The most popular </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.1165.1">tokenization algorithms are </span><strong class="bold"><span class="koboSpan" id="kobo.1166.1">byte-pair encoding</span></strong><span class="koboSpan" id="kobo.1167.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1168.1">BPE</span></strong><span class="koboSpan" id="kobo.1169.1">), WordPiece, and </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.1170.1">Unigram (</span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1171.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1172.1">). </span><span class="koboSpan" id="kobo.1172.2">The tokens are transformed into </span><span class="koboSpan" id="kobo.1173.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/707.png" style="vertical-align:-0.340em;height:1.051em;width:1.930em"/></span><span class="koboSpan" id="kobo.1174.1">-dimensional embedding vectors. </span><span class="koboSpan" id="kobo.1174.2">The transformation works in the way we described in </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1175.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1176.1">. </span><span class="koboSpan" id="kobo.1176.2">We have a lookup table (matrix)—the index </span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.1177.1">of the one-hot-encoded token indicates the matrix row, which represents the embedding vector. </span><span class="koboSpan" id="kobo.1177.2">The embedding vectors are further multiplied by </span><span class="koboSpan" id="kobo.1178.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/708.png" style="vertical-align:-0.380em;height:1.261em;width:2.618em"/></span><span class="koboSpan" id="kobo.1179.1">. </span><span class="koboSpan" id="kobo.1179.2">They are initialized randomly and are trained with the whole model (this is opposed to initializing them with an algorithm such </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">as word2vec).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1181.1">The next step adds positional information to the existing embedding vector. </span><span class="koboSpan" id="kobo.1181.2">This is necessary because the attention mechanism doesn’t preserve the order of sequence elements. </span><span class="koboSpan" id="kobo.1181.3">This step modifies the embedding vectors in a way that implicitly encodes that information </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">within them.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1183.1">The original </span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.1184.1">transformer implementation uses </span><strong class="bold"><span class="koboSpan" id="kobo.1185.1">static positional encoding</span></strong><span class="koboSpan" id="kobo.1186.1">, represented by special positional encoding vectors with the same size as the token embeddings. </span><span class="koboSpan" id="kobo.1186.2">We add these vectors, using element-wise addition, to all embedding vectors of the sequence, depending on their position. </span><span class="koboSpan" id="kobo.1186.3">The static encoding is unique for each position of the sequence but is constant with regard to the elements of the sequence. </span><span class="koboSpan" id="kobo.1186.4">Because of this, we can precompute the positional encodings only once and use </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">them subsequently.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1188.1">An alternative way to encode positional information is with relative position representations </span><br/><span class="koboSpan" id="kobo.1189.1">(</span><em class="italic"><span class="koboSpan" id="kobo.1190.1">Self-Attention with Relative Position Representations</span></em><span class="koboSpan" id="kobo.1191.1">, </span><a href="https://arxiv.org/abs/1803.02155"><span class="koboSpan" id="kobo.1192.1">https://arxiv.org/abs/1803.02155</span></a><span class="koboSpan" id="kobo.1193.1">). </span><span class="koboSpan" id="kobo.1193.2">Here, the positional information is dynamically encoded in the key-value matrices, </span><strong class="bold"><span class="koboSpan" id="kobo.1194.1">K</span></strong><span class="koboSpan" id="kobo.1195.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.1196.1">V</span></strong><span class="koboSpan" id="kobo.1197.1">, of the attention blocks. </span><span class="koboSpan" id="kobo.1197.2">Each element of the input sequence has a different position in relation to the rest of the elements. </span><span class="koboSpan" id="kobo.1197.3">Therefore, the relative position encoding is computed dynamically for each token. </span><span class="koboSpan" id="kobo.1197.4">This encoding is applied to the </span><strong class="bold"><span class="koboSpan" id="kobo.1198.1">K</span></strong><span class="koboSpan" id="kobo.1199.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1200.1">V</span></strong><span class="koboSpan" id="kobo.1201.1"> matrices as an additional part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1202.1">attention formula.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1203.1">The rest of the encoder is composed of a stack of </span><em class="italic"><span class="koboSpan" id="kobo.1204.1">N=6</span></em><span class="koboSpan" id="kobo.1205.1"> identical blocks that come in two flavors: post-ln and pre-ln. </span><span class="koboSpan" id="kobo.1205.2">Both types of blocks share the </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">following sublayers:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1207.1">A multi-head self-attention mechanism, like the one we described in the </span><em class="italic"><span class="koboSpan" id="kobo.1208.1">Transformer attention</span></em><span class="koboSpan" id="kobo.1209.1"> section. </span><span class="koboSpan" id="kobo.1209.2">Since the self-attention mechanism works across the whole input sequence, the encoder is </span><strong class="bold"><span class="koboSpan" id="kobo.1210.1">bidirectional</span></strong><span class="koboSpan" id="kobo.1211.1"> by design. </span><span class="koboSpan" id="kobo.1211.2">That is, the context of the current token includes both the tokens that come before and the ones that come after it in the sequence. </span><span class="koboSpan" id="kobo.1211.3">This is opposed to a regular RNN, which only has access to the tokens that came before the current one. </span><span class="koboSpan" id="kobo.1211.4">Each position in an encoder block can attend to all positions in the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">encoder block.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1213.1">We feed the </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.1214.1">embedding of each token as a query, </span><strong class="bold"><span class="koboSpan" id="kobo.1215.1">q</span></strong><span class="koboSpan" id="kobo.1216.1">, to the </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.1217.1">multi-head self-attention (we can feed the full input sequence in one pass as an input matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.1218.1">Q</span></strong><span class="koboSpan" id="kobo.1219.1">). </span><span class="koboSpan" id="kobo.1219.2">At the same time, the embeddings of its context act as the key-value store </span><strong class="bold"><span class="koboSpan" id="kobo.1220.1">K</span></strong><span class="koboSpan" id="kobo.1221.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.1222.1">V</span></strong><span class="koboSpan" id="kobo.1223.1">. </span><span class="koboSpan" id="kobo.1223.2">The output vector of the multi-head self-attention operation serves as input for the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1224.1">the model.</span></span></p></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1225.1">MHA and activation functions</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1226.1">The MHA produces </span><em class="italic"><span class="koboSpan" id="kobo.1227.1">h</span></em><span class="koboSpan" id="kobo.1228.1"> attention vectors for each of the </span><em class="italic"><span class="koboSpan" id="kobo.1229.1">h</span></em><span class="koboSpan" id="kobo.1230.1"> attention heads for each input token. </span><span class="koboSpan" id="kobo.1230.2">Then, they are linearly projected with an FC layer that combines them. </span><span class="koboSpan" id="kobo.1230.3">The whole attention block doesn’t have an explicit activation function. </span><span class="koboSpan" id="kobo.1230.4">But let’s recall that the attention block ends with a non-linearity, softmax. </span><span class="koboSpan" id="kobo.1230.5">The dot product of the key-value vectors is an additional </span><br/><span class="koboSpan" id="kobo.1231.1">non-linearity. </span><span class="koboSpan" id="kobo.1231.2">In that strict sense, the attention block doesn’t need </span><span class="No-Break"><span class="koboSpan" id="kobo.1232.1">additional activation.</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1233.1">A simple FC FFN, which is defined by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">following formula:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1235.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;FFN&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;ActivationFunc&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/709.png" style="vertical-align:-0.383em;height:1.144em;width:18.687em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1236.1">The network is applied to each sequence element, </span><strong class="bold"><span class="koboSpan" id="kobo.1237.1">x</span></strong><span class="koboSpan" id="kobo.1238.1">, separately. </span><span class="koboSpan" id="kobo.1238.2">It uses the same set of parameters (</span><span class="koboSpan" id="kobo.1239.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/710.png" style="vertical-align:-0.333em;height:0.982em;width:1.365em"/></span><span class="koboSpan" id="kobo.1240.1">, </span><span class="koboSpan" id="kobo.1241.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/711.png" style="vertical-align:-0.333em;height:0.982em;width:1.367em"/></span><span class="koboSpan" id="kobo.1242.1">, </span><span class="koboSpan" id="kobo.1243.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/712.png" style="vertical-align:-0.333em;height:1.044em;width:0.758em"/></span><span class="koboSpan" id="kobo.1244.1">, and </span><span class="koboSpan" id="kobo.1245.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/713.png" style="vertical-align:-0.333em;height:1.044em;width:0.758em"/></span><span class="koboSpan" id="kobo.1246.1">) across different positions, but different parameters across the different encoder blocks. </span><span class="koboSpan" id="kobo.1246.2">The original transformer </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.1247.1">uses </span><strong class="bold"><span class="koboSpan" id="kobo.1248.1">rectified linear unit</span></strong><span class="koboSpan" id="kobo.1249.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1250.1">ReLU</span></strong><span class="koboSpan" id="kobo.1251.1">) activations. </span><span class="koboSpan" id="kobo.1251.2">However, more recent models use one of its variations, such </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.1252.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.1253.1">sigmoid linear units</span></strong><span class="koboSpan" id="kobo.1254.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1255.1">SiLUs</span></strong><span class="koboSpan" id="kobo.1256.1">). </span><span class="koboSpan" id="kobo.1256.2">The role of the FFN is to process the MHA output in a way that better fits the input for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">next block.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1258.1">The difference between pre-ln and post-ln blocks lies in the position of the normalization layer. </span><span class="koboSpan" id="kobo.1258.2">Each post-ln sublayer (both the MHA and FFN) has a residual connection around itself and ends with normalization and dropout over the sum of that connection and its own output. </span><span class="koboSpan" id="kobo.1258.3">The normalization layers in the post-ln transformer lie after the attention and the FFN, respectively. </span><span class="koboSpan" id="kobo.1258.4">Therefore, the output of each post-ln sublayer is </span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1260.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;LayerNorm&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mtext&gt;SubLayer&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/714.png" style="vertical-align:-0.307em;height:1.067em;width:11.776em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1261.1">In contrast, the pre-ln blocks (the right section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1262.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1263.1">.9</span></em><span class="koboSpan" id="kobo.1264.1">), in the two encoder normalization </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.1265.1">layers lie before the attention and the FFN, respectively. </span><span class="koboSpan" id="kobo.1265.2">Therefore, the output of each pre-ln sublayer </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">is this:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1267.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mtext&gt;SubLayer&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;LayerNorm&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/715.png" style="vertical-align:-0.307em;height:1.018em;width:11.776em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1268.1">The difference </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.1269.1">between the two flavors manifests itself during training. </span><span class="koboSpan" id="kobo.1269.2">Without going into too many details, the aptly named paper </span><em class="italic"><span class="koboSpan" id="kobo.1270.1">Understanding the Difficulty of Training Transformers</span></em><span class="koboSpan" id="kobo.1271.1"> (</span><a href="https://arxiv.org/abs/2004.08249"><span class="koboSpan" id="kobo.1272.1">https://arxiv.org/abs/2004.08249</span></a><span class="koboSpan" id="kobo.1273.1">) suggests that the post-ln transformer’s strong dependency on residual connections amplifies the fluctuation caused by parameter changes (for example, adaptive learning rate) and destabilizes the training. </span><span class="koboSpan" id="kobo.1273.2">Because of this, the post-ln training starts with a warmup phase with a low learning rate, before ultimately increasing it. </span><span class="koboSpan" id="kobo.1273.3">This is opposed to the usual learning rate schedule that starts with a large value, which only decreases with the progression of the training. </span><span class="koboSpan" id="kobo.1273.4">The pre-ln blocks don’t </span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.1274.1">have such a problem and don’t need a warmup phase. </span><span class="koboSpan" id="kobo.1274.2">However, they could suffer from </span><strong class="bold"><span class="koboSpan" id="kobo.1275.1">representation collapse</span></strong><span class="koboSpan" id="kobo.1276.1">, where the hidden representation in deeper blocks (those closer to the end of the NN) will be similar and thus contribute little to model capacity. </span><span class="koboSpan" id="kobo.1276.2">In practice, both types of blocks are </span><span class="No-Break"><span class="koboSpan" id="kobo.1277.1">in use.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1278.1">So far, so good with the encoder. </span><span class="koboSpan" id="kobo.1278.2">Next, let’s build upon our attention implementation by building the encoder </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">as well.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1280.1">Implementing the encoder</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1281.1">In this </span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.1282.1">section, we’ll implement the post-ln encoder, which is composed of several different submodules. </span><span class="koboSpan" id="kobo.1282.2">Let’s start with the main </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">class, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1284.1">Encoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1285.1">:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1286.1">
class Encoder(torch.nn.Module):
    def __init__(self, block: EncoderBlock, N: int):
        super(Encoder, self).__init__()
        self.blocks = clones(block, N)
        self.norm = torch.nn.LayerNorm(block.size)
    def forward(self, x, mask):
        """Iterate over all blocks and normalize"""
        for layer in self.blocks:
            x = layer(x, mask)
        return self.norm(x)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1287.1">It stacks </span><strong class="source-inline"><span class="koboSpan" id="kobo.1288.1">N</span></strong><span class="koboSpan" id="kobo.1289.1"> instances of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1290.1">EncoderBlock</span></strong><span class="koboSpan" id="kobo.1291.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1292.1">self.blocks</span></strong><span class="koboSpan" id="kobo.1293.1">), followed by a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1294.1">LayerNorm</span></strong><span class="koboSpan" id="kobo.1295.1"> normalization, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1296.1">self.norm</span></strong><span class="koboSpan" id="kobo.1297.1">. </span><span class="koboSpan" id="kobo.1297.2">Each instance serves as input to the next, as the definition of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1298.1">forward</span></strong><span class="koboSpan" id="kobo.1299.1"> method shows. </span><span class="koboSpan" id="kobo.1299.2">In addition to the regular input, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1300.1">x</span></strong><span class="koboSpan" id="kobo.1301.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1302.1">forward</span></strong><span class="koboSpan" id="kobo.1303.1"> also takes as input a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1304.1">mask</span></strong><span class="koboSpan" id="kobo.1305.1"> parameter. </span><span class="koboSpan" id="kobo.1305.2">However, it is only relevant to the decoder part, so we won’t focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.1306.1">it here.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1307.1">Next, let’s see </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.1308.1">the implementation of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1309.1">EncoderBlock</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1"> class:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1311.1">
class EncoderBlock(torch.nn.Module):
    def __init__(self,
                 size: int,
                 self_attn: MultiHeadedAttention,
                 ffn: PositionwiseFFN,
                 dropout=0.1):
        super(EncoderBlock, self).__init__()
        self.self_attn = self_attn
        self.ffn = ffn
        # Create 2 sub-layer connections
        # 1 for the self-attention
        # 1 for the FFN
        self.sublayers = clones(SublayerConnection(size, dropout), 2)
        self.size = size
    def forward(self, x, mask):
        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayers[1](x, self.ffn)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1312.1">Each encoder </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.1313.1">block consists of multi-head self-attention (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1314.1">self.self_attn</span></strong><span class="koboSpan" id="kobo.1315.1">) and FFN (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1316.1">self.ffn</span></strong><span class="koboSpan" id="kobo.1317.1">) sublayers (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1318.1">self.sublayers</span></strong><span class="koboSpan" id="kobo.1319.1">). </span><span class="koboSpan" id="kobo.1319.2">Each </span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.1320.1">sublayer is wrapped by its residual connection, </span><strong class="bold"><span class="koboSpan" id="kobo.1321.1">layer normalization</span></strong><span class="koboSpan" id="kobo.1322.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1323.1">LN</span></strong><span class="koboSpan" id="kobo.1324.1">), and dropout, implemented by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1325.1">SublayerConnection</span></strong><span class="koboSpan" id="kobo.1326.1"> class and instantiated with the familiar </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1327.1">clone</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1328.1"> function:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1329.1">
class SublayerConnection(torch.nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = torch.nn.LayerNorm(size)
        self.dropout = torch.nn.Dropout(dropout)
    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1330.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1331.1">SublayerConnection.forward</span></strong><span class="koboSpan" id="kobo.1332.1"> method takes as input the data tensor, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1333.1">x</span></strong><span class="koboSpan" id="kobo.1334.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1335.1">sublayer</span></strong><span class="koboSpan" id="kobo.1336.1">, which is an instance of either </span><strong class="source-inline"><span class="koboSpan" id="kobo.1337.1">MultiHeadedAttention</span></strong><span class="koboSpan" id="kobo.1338.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.1339.1">PositionwiseFFN</span></strong><span class="koboSpan" id="kobo.1340.1"> (it matches the sublayer definition </span><span class="koboSpan" id="kobo.1341.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;LayerNorm&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mtext&gt;SubLayer&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/716.png" style="vertical-align:-0.307em;height:1.067em;width:11.756em"/></span><span class="koboSpan" id="kobo.1342.1"> from the </span><em class="italic"><span class="koboSpan" id="kobo.1343.1">Transformer </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1344.1">encoder</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1345.1"> section).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1346.1">The only component we haven’t defined yet is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1347.1">PositionwiseFFN</span></strong><span class="koboSpan" id="kobo.1348.1">, which implements the formula </span><span class="koboSpan" id="kobo.1349.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;FFN&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;x&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;ActivationFunc&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/717.png" style="vertical-align:-0.383em;height:1.144em;width:18.236em"/></span><span class="koboSpan" id="kobo.1350.1">. </span><span class="koboSpan" id="kobo.1350.2">We’ll use SiLU activation. </span><span class="koboSpan" id="kobo.1350.3">Let’s add this </span><span class="No-Break"><span class="koboSpan" id="kobo.1351.1">missing piece:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1352.1">
class PositionwiseFFN(torch.nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout=0.1):
        super(PositionwiseFFN, self).__init__()
        self.w_1 = torch.nn.Linear(d_model, d_ff)
        self.w_2 = torch.nn.Linear(d_ff, d_model)
        self.dropout = torch.nn.Dropout(dropout)
    def forward(self, x):
        return self.w_2(
            self.dropout(
                torch.nn.functional.silu(
                    self.w_1(x)
                )))</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1353.1">This concludes </span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.1354.1">our implementation of the encoder. </span><span class="koboSpan" id="kobo.1354.2">Next, let’s focus our attention on </span><span class="No-Break"><span class="koboSpan" id="kobo.1355.1">the decoder.</span></span></p>
<h2 id="_idParaDest-140" lang="en-GB"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.1356.1">Transformer decoder</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1357.1">The decoder generates the output sequence, based on a combination of the encoder output and </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.1358.1">its own previously generated </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.1359.1">sequence of tokens (we can see the decoder on the right side of both sections in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1360.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1361.1">.9</span></em><span class="koboSpan" id="kobo.1362.1"> at the beginning of the </span><em class="italic"><span class="koboSpan" id="kobo.1363.1">Building transformers with attention</span></em><span class="koboSpan" id="kobo.1364.1"> section). </span><span class="koboSpan" id="kobo.1364.2">In the context of a seq2seq task, the full encoder-decoder transformer is an autoregressive model. </span><span class="koboSpan" id="kobo.1364.3">First, we feed the initial sequence—for example, a sentence to translate or a question to answer—to the encoder. </span><span class="koboSpan" id="kobo.1364.4">This can happen in a single pass, if the sequence is short enough to fit the maximum size of the query matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.1365.1">Q</span></strong><span class="koboSpan" id="kobo.1366.1">. </span><span class="koboSpan" id="kobo.1366.2">Once the encoder processes all sequence elements, the decoder will take the encoder output and start generating the output sequence one token at a time. </span><span class="koboSpan" id="kobo.1366.3">It will append each generated token to the initial input sequence. </span><span class="koboSpan" id="kobo.1366.4">We’ll feed the new, extended sequence to the encoder once again. </span><span class="koboSpan" id="kobo.1366.5">The new output of the encoder will initiate the next token generation step of the decoder, and so on. </span><span class="koboSpan" id="kobo.1366.6">In effect, the target token sequence is the same as the input token sequence, shifted by one (similar to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">seq2seq decoder).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1368.1">The decoder uses the same embedding vectors and positional encoding as the encoder. </span><span class="koboSpan" id="kobo.1368.2">It continues </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.1369.1">with a stack of </span><em class="italic"><span class="koboSpan" id="kobo.1370.1">N=6</span></em><span class="koboSpan" id="kobo.1371.1"> identical </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.1372.1">decoder blocks. </span><span class="koboSpan" id="kobo.1372.2">Each block consists of three sublayers and each sublayer employs residual connections, dropout, and normalization. </span><span class="koboSpan" id="kobo.1372.3">As with the encoder, the blocks come in post-ln and pre-ln flavors. </span><span class="koboSpan" id="kobo.1372.4">The sublayers are </span><span class="No-Break"><span class="koboSpan" id="kobo.1373.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1374.1">A masked multi-head self-attention mechanism. </span><span class="koboSpan" id="kobo.1374.2">The encoder’s self-attention is bidirectional—it can attend to all elements of the sequence, regardless of whether they come before or after the current element. </span><span class="koboSpan" id="kobo.1374.3">However, the decoder only has a partially generated target sequence. </span><span class="koboSpan" id="kobo.1374.4">Therefore, the decoder is </span><strong class="bold"><span class="koboSpan" id="kobo.1375.1">unidirectional</span></strong><span class="koboSpan" id="kobo.1376.1">—the self-attention can only attend to the preceding sequence elements. </span><span class="koboSpan" id="kobo.1376.2">During inference, we have no choice but to run the transformer in a sequential way so that it can produce each token of the output sequence one by one. </span><span class="koboSpan" id="kobo.1376.3">However, during training, we can feed the whole target sequence simultaneously, as it’s known in advance. </span><span class="koboSpan" id="kobo.1376.4">To avoid illegal forward attention, we can </span><strong class="bold"><span class="koboSpan" id="kobo.1377.1">mask out</span></strong><span class="koboSpan" id="kobo.1378.1"> illegal connections by setting −∞ on all such values in the input of the attention softmax. </span><span class="koboSpan" id="kobo.1378.2">We can see the mask component in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1379.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1380.1">.6</span></em><span class="koboSpan" id="kobo.1381.1"> of the </span><em class="italic"><span class="koboSpan" id="kobo.1382.1">Transformer attention</span></em><span class="koboSpan" id="kobo.1383.1"> section and the result of the mask </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">operation here:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1385.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;k&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Q&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;s&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;k&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋱&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;33&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;∞&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋱&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/718.png" style="vertical-align:-2.696em;height:5.899em;width:27.906em"/></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1386.1">A regular attention (not self-attention) mechanism, where the queries come from the previous decoder layer, and the keys and values come from the encoder output. </span><span class="koboSpan" id="kobo.1386.2">This allows every position in the decoder to attend all positions in the original input sequence. </span><span class="koboSpan" id="kobo.1386.3">This mimics the typical encoder-decoder attention mechanisms, which we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1387.1">Introducing seq2seq </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1388.1">models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1389.1"> section.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1390.1">FFN, which is similar to the one in </span><span class="No-Break"><span class="koboSpan" id="kobo.1391.1">the encoder.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1392.1">The decoder ends with an FC layer, followed by a softmax operation, which produces the most probable next word of </span><span class="No-Break"><span class="koboSpan" id="kobo.1393.1">the sentence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1394.1">We can train the full encoder-decoder model using the teacher-forcing process we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1395.1">Introducing seq2seq </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1396.1">models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1"> section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1398.1">Next, let’s implement </span><span class="No-Break"><span class="koboSpan" id="kobo.1399.1">the decoder.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1400.1">Implementing the decoder</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1401.1">In this section, we’ll implement the decoder in a similar pattern to the encoder. </span><span class="koboSpan" id="kobo.1401.2">We’ll start with </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.1402.1">the implementation of the main </span><span class="No-Break"><span class="koboSpan" id="kobo.1403.1">module, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1404.1">Decoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1406.1">
class Decoder(torch.nn.Module):
    def __init__(self, block: DecoderBlock, N: int, vocab_size: int):
        super(Decoder, self).__init__()
        self.blocks = clones(block, N)
        self.norm = torch.nn.LayerNorm(block.size)
        self.projection = torch.nn.Linear(block.size, vocab_size)
    def forward(self, x, encoder_states, source_mask, target_mask):
        for layer in self.blocks:
            x = layer(x, encoder_states, source_mask, target_mask)
        x = self.norm(x)
        return torch.nn.functional.log_softmax(self.projection(x), dim=-1)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1407.1">It consists of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1408.1">N</span></strong><span class="koboSpan" id="kobo.1409.1"> instances of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1410.1">DecoderBlock</span></strong><span class="koboSpan" id="kobo.1411.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1412.1">self.blocks</span></strong><span class="koboSpan" id="kobo.1413.1">). </span><span class="koboSpan" id="kobo.1413.2">As we can see in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1414.1">forward</span></strong><span class="koboSpan" id="kobo.1415.1"> method, the output of each </span><strong class="source-inline"><span class="koboSpan" id="kobo.1416.1">DecoderBlock</span></strong><span class="koboSpan" id="kobo.1417.1"> instance serves as input to the next. </span><span class="koboSpan" id="kobo.1417.2">These are followed by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1418.1">self.norm</span></strong><span class="koboSpan" id="kobo.1419.1"> normalization (an instance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1420.1">LayerNorm</span></strong><span class="koboSpan" id="kobo.1421.1">). </span><span class="koboSpan" id="kobo.1421.2">The decoder ends with an FC layer (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1422.1">self.projection</span></strong><span class="koboSpan" id="kobo.1423.1">), followed by a softmax to produce the most probable next word. </span><span class="koboSpan" id="kobo.1423.2">Note that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1424.1">Decoder.forward</span></strong><span class="koboSpan" id="kobo.1425.1"> method takes an additional parameter, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1426.1">encoder_states</span></strong><span class="koboSpan" id="kobo.1427.1">, which is passed to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1428.1">DecoderBlock</span></strong><span class="koboSpan" id="kobo.1429.1"> instances. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1430.1">encoder_states</span></strong><span class="koboSpan" id="kobo.1431.1"> represents the encoder output and is the link between the encoder and the decoder. </span><span class="koboSpan" id="kobo.1431.2">In addition, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1432.1">source_mask</span></strong><span class="koboSpan" id="kobo.1433.1"> parameter provides the mask of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">decoder self-attention.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1435.1">Next, let’s </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.1436.1">implement the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1437.1">DecoderBlock</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1"> class:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1439.1">
class DecoderBlock(torch.nn.Module):
    def __init__(self,
                 size: int,
                 self_attn: MultiHeadedAttention,
                 encoder_attn: MultiHeadedAttention,
                 ffn: PositionwiseFFN,
                 dropout=0.1):
        super(DecoderBlock, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.encoder_attn = encoder_attn
        self.ffn = ffn
        self.sublayers = clones(SublayerConnection(size,
                dropout), 3)
    def forward(self, x, encoder_states, source_mask, target_mask):
        x = self.sublayers[0](x, lambda x: \
                self.self_attn(x, x, x, target_mask))
        x = self.sublayers[1](x, lambda x: \
                self.encoder_attn(x, encoder_states,
                encoder_states, source_mask))
        return self.sublayers[2](x, self.ffn)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1440.1">This implementation follows the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1441.1">EncoderBlock</span></strong><span class="koboSpan" id="kobo.1442.1"> pattern but is adapted to the decoder: in addition to self-attention (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1443.1">self_attn</span></strong><span class="koboSpan" id="kobo.1444.1">), we also have encoder attention (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1445.1">encoder_attn</span></strong><span class="koboSpan" id="kobo.1446.1">). </span><span class="koboSpan" id="kobo.1446.2">Because of this, we instantiate three </span><strong class="source-inline"><span class="koboSpan" id="kobo.1447.1">sublayers</span></strong><span class="koboSpan" id="kobo.1448.1"> instances (instances of the familiar </span><strong class="source-inline"><span class="koboSpan" id="kobo.1449.1">SublayerConnection</span></strong><span class="koboSpan" id="kobo.1450.1"> class): for self-attention, encoder attention, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1451.1">the FFN.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1452.1">We can see </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.1453.1">the combination of multiple attention mechanisms in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1454.1">DecoderBlock.forward</span></strong><span class="koboSpan" id="kobo.1455.1"> method. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1456.1">encoder_attn</span></strong><span class="koboSpan" id="kobo.1457.1"> takes as a query the output of the preceding decoder block (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1458.1">x</span></strong><span class="koboSpan" id="kobo.1459.1">) and key-value combination from the encoder output (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1460.1">encoder_states</span></strong><span class="koboSpan" id="kobo.1461.1">). </span><span class="koboSpan" id="kobo.1461.2">In this way, regular attention establishes the link between the encoder and the decoder. </span><span class="koboSpan" id="kobo.1461.3">On the other hand, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1462.1">self_attn</span></strong><span class="koboSpan" id="kobo.1463.1"> uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.1464.1">x</span></strong><span class="koboSpan" id="kobo.1465.1"> for the query, key, </span><span class="No-Break"><span class="koboSpan" id="kobo.1466.1">and value.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1467.1">This concludes the decoder implementation. </span><span class="koboSpan" id="kobo.1467.2">We’ll proceed with building the full transformer model in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1468.1">next section.</span></span></p>
<h2 id="_idParaDest-141" lang="en-GB"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.1469.1">Putting it all together</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1470.1">We now </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.1471.1">have implementations of the encoder and the decoder. </span><span class="koboSpan" id="kobo.1471.2">Let’s combine them in the full </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1472.1">EncoderDecoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1473.1"> class:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1474.1">
class EncoderDecoder(torch.nn.Module):
    def __init__(self,
                 encoder: Encoder,
                 decoder: Decoder,
                 source_embeddings: torch.nn.Sequential,
                 target_embeddings: torch.nn.Sequential):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.source_embeddings = source_embeddings
        self.target_embeddings = target_embeddings
    def forward(self, source, target, source_mask, target_mask):
        encoder_output = self.encoder(
            x=self.source_embeddings(source),
            mask=source_mask)
        return self.decoder(
            x=self.target_embeddings(target),
            encoder_states=encoder_output,
            source_mask=source_mask,
            target_mask=target_mask)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1475.1">It combines </span><strong class="source-inline"><span class="koboSpan" id="kobo.1476.1">encoder</span></strong><span class="koboSpan" id="kobo.1477.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1478.1">decoder</span></strong><span class="koboSpan" id="kobo.1479.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1480.1">source_embeddings</span></strong><span class="koboSpan" id="kobo.1481.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1482.1">target_embeddings</span></strong><span class="koboSpan" id="kobo.1483.1">. </span><span class="koboSpan" id="kobo.1483.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1484.1">forward</span></strong><span class="koboSpan" id="kobo.1485.1"> method takes the source sequence and feeds it to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1486.1">encoder</span></strong><span class="koboSpan" id="kobo.1487.1">. </span><span class="koboSpan" id="kobo.1487.2">Then, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1488.1">decoder</span></strong><span class="koboSpan" id="kobo.1489.1"> takes its input from the preceding output step (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1490.1">x=self.target_embeddings(target)</span></strong><span class="koboSpan" id="kobo.1491.1">), the encoder states (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1492.1">encoder_states=encoder_output)</span></strong><span class="koboSpan" id="kobo.1493.1">, and the source and target masks. </span><span class="koboSpan" id="kobo.1493.2">With these inputs, it produces the predicted next token (word) of the sequence, which is also the return value of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1494.1">forward</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1495.1"> method.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1496.1">Next, we’ll </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.1497.1">implement the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1498.1">build_model</span></strong><span class="koboSpan" id="kobo.1499.1"> function, which instantiates all the classes we implemented to produce a single </span><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">transformer instance:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1501.1">
def build_model(source_vocabulary: int,
                target_vocabulary: int,
                N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFFN(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
      encoder=Encoder(
        EncoderBlock(d_model, c(attn), c(ff), dropout), N),
        decoder=Decoder(
            DecoderBlock(d_model, c(attn), c(attn),
                    c(ff), dropout), N, target_vocabulary),
        source_embeddings=torch.nn.Sequential(
            Embeddings(d_model, source_vocabulary), c(position)),
        target_embeddings=torch.nn.Sequential(
            Embeddings(d_model, target_vocabulary), c(position)))
    # Initialize parameters with random weights
    for p in model.parameters():
        if p.dim() &gt; 1:
            torch.nn.init.xavier_uniform_(p)
    return model</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1502.1">Besides </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.1503.1">the familiar </span><strong class="source-inline"><span class="koboSpan" id="kobo.1504.1">MultiHeadedAttention</span></strong><span class="koboSpan" id="kobo.1505.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1506.1">PositionwiseFFN</span></strong><span class="koboSpan" id="kobo.1507.1">, we also create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1508.1">position</span></strong><span class="koboSpan" id="kobo.1509.1"> variable (an instance of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1510.1">PositionalEncoding</span></strong><span class="koboSpan" id="kobo.1511.1"> class). </span><span class="koboSpan" id="kobo.1511.2">This class implements the static positional encoding we described in the </span><em class="italic"><span class="koboSpan" id="kobo.1512.1">Transformer encoder</span></em><span class="koboSpan" id="kobo.1513.1"> section (we won’t include the full </span><span class="No-Break"><span class="koboSpan" id="kobo.1514.1">implementation here).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1515.1">Now, let’s focus on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1516.1">EncoderDecoder</span></strong><span class="koboSpan" id="kobo.1517.1"> instantiation: we are already familiar with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1518.1">encoder</span></strong><span class="koboSpan" id="kobo.1519.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1520.1">decoder</span></strong><span class="koboSpan" id="kobo.1521.1">, so there are no surprises there. </span><span class="koboSpan" id="kobo.1521.2">But the embeddings are a tad more interesting. </span><span class="koboSpan" id="kobo.1521.3">The following code instantiates the source embeddings (but this is also valid for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1522.1">target ones):</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1523.1">
source_embeddings=torch.nn.Sequential(Embeddings(d_model, source_vocabulary), c(position))</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1524.1">We can see that they are a sequential list of </span><span class="No-Break"><span class="koboSpan" id="kobo.1525.1">two components:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1526.1">An instance of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1527.1">Embeddings</span></strong><span class="koboSpan" id="kobo.1528.1"> class, which is simply a combination of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1529.1">torch.nn.Embedding</span></strong><span class="koboSpan" id="kobo.1530.1"> further multiplied by </span><span class="koboSpan" id="kobo.1531.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/719.png" style="vertical-align:-0.380em;height:1.261em;width:2.503em"/></span><span class="koboSpan" id="kobo.1532.1"> (we’ll omit the class </span><span class="No-Break"><span class="koboSpan" id="kobo.1533.1">definition here)</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1534.1">Positional encoding </span><strong class="source-inline"><span class="koboSpan" id="kobo.1535.1">c(position)</span></strong><span class="koboSpan" id="kobo.1536.1">, which adds the static positional data to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1537.1">embedding vector</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1538.1">Once we </span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.1539.1">have the input data preprocessed in this way, it can serve as input to the core part of </span><span class="No-Break"><span class="koboSpan" id="kobo.1540.1">the encoder-decoder.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1541.1">In the next section, we’ll discuss the major variants of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">transformer architecture.</span></span></p>
<h2 id="_idParaDest-142" lang="en-GB"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.1543.1">Decoder-only and encoder-only models</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1544.1">So far, we’ve </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.1545.1">discussed the full encoder-decoder </span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.1546.1">variant of the transformer architecture. </span><span class="koboSpan" id="kobo.1546.2">But in practice, we are going to mostly use two of </span><span class="No-Break"><span class="koboSpan" id="kobo.1547.1">its variations:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1548.1">Encoder-only</span></strong><span class="koboSpan" id="kobo.1549.1">: These </span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.1550.1">models use only the encoder part of the full transformer. </span><span class="koboSpan" id="kobo.1550.2">Encoder-only models are bidirectional, following the properties of </span><span class="No-Break"><span class="koboSpan" id="kobo.1551.1">encoder self-attention.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1552.1">Decoder-only</span></strong><span class="koboSpan" id="kobo.1553.1">: These </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.1554.1">models use only the decoder part of the transformer. </span><span class="koboSpan" id="kobo.1554.2">Decoder-only models are unidirectional, following the properties of the decoder’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1555.1">masked self-attention.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1556.1">I know that these dry definitions sound vague, but don’t worry—in the next two sections we’ll discuss one example of each type to make </span><span class="No-Break"><span class="koboSpan" id="kobo.1557.1">it clear.</span></span></p>
<h2 id="_idParaDest-143" lang="en-GB"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.1558.1">Bidirectional Encoder Representations from Transformers</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1559.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.1560.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1561.1">BERT</span></strong><span class="koboSpan" id="kobo.1562.1">; see </span><a href="https://arxiv.org/abs/1810.04805"><span class="koboSpan" id="kobo.1563.1">https://arxiv.org/abs/1810.04805</span></a><span class="koboSpan" id="kobo.1564.1">), as the name gives away, is an encoder-only (hence bidirectional) model </span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.1565.1">that learns representations. </span><span class="koboSpan" id="kobo.1565.2">These representations serve </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.1566.1">as a base for solving various downstream tasks (the pure BERT model doesn’t solve any specific problem). </span><span class="koboSpan" id="kobo.1566.2">The following diagram shows generic pre-ln and post-ln encoder-only models with softmax outputs (which also apply </span><span class="No-Break"><span class="koboSpan" id="kobo.1567.1">to BERT):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer866">
<span class="koboSpan" id="kobo.1568.1"><img alt="Figure 7.10 – Left: post-ln encoder-only model; right: pre-ln encoder-only model" src="image/B19627_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1569.1">Figure 7.10 – Left: post-ln encoder-only model; right: pre-ln encoder-only model</span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1570.1">BERT model sizes</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1571.1">BERT comes in two variations</span><span class="koboSpan" id="kobo.1572.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;—&lt;/mml:mo&gt;&lt;mml:mtext&gt;BER&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;T&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;BASE&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/720.png" style="vertical-align:-0.342em;height:1.016em;width:4.817em"/></span><span class="koboSpan" id="kobo.1573.1"> and </span><span class="koboSpan" id="kobo.1574.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;BER&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;T&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;LARGE&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/721.png" style="vertical-align:-0.342em;height:1.016em;width:4.392em"/></span><span class="koboSpan" id="kobo.1575.1">. </span><span class="koboSpan" id="kobo.1576.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;BER&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;T&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;BASE&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/722.png" style="vertical-align:-0.342em;height:1.016em;width:3.895em"/></span><span class="koboSpan" id="kobo.1577.1"> has 12 encoder blocks, each with 12 attention heads, 768-dimensional attention vectors (the </span><span class="koboSpan" id="kobo.1578.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/723.png" style="vertical-align:-0.340em;height:1.051em;width:1.866em"/></span><span class="koboSpan" id="kobo.1579.1"> parameter), and a total of 110M parameters. </span><span class="koboSpan" id="kobo.1580.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;BER&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;T&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;LARGE&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/724.png" style="vertical-align:-0.342em;height:1.016em;width:4.501em"/></span><span class="koboSpan" id="kobo.1581.1"> has 24 encoder blocks, each with 16 attention heads, 1,024-dimensional attention vectors, and a total of 340M parameters. </span><span class="koboSpan" id="kobo.1581.2">The models use WordPiece tokenization and have a </span><span class="No-Break"><span class="koboSpan" id="kobo.1582.1">30,000-token vocabulary.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1583.1">Let’s </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.1584.1">start with the way BERT represents its input data, which is an important part of its architecture. </span><span class="koboSpan" id="kobo.1584.2">We can see an input data representation in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1585.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer872">
<span class="koboSpan" id="kobo.1586.1"><img alt="Figure 7.11 – BERT input embeddings as the sum of the token embeddings, the segmentation embeddings, and the position embeddings (source: https://arxiv.org/abs/1810.04805)" src="image/B19627_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1587.1">Figure 7.11 – BERT input embeddings as the sum of the token embeddings, the segmentation embeddings, and the position embeddings (source: https://arxiv.org/abs/1810.04805)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1588.1">Because BERT is encoder-only, it has two special modes of input data representation so that it can handle a variety of </span><span class="No-Break"><span class="koboSpan" id="kobo.1589.1">downstream tasks:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1590.1">A single </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.1591.1">sequence (for example, in classification tasks, such as </span><strong class="bold"><span class="koboSpan" id="kobo.1592.1">sentiment analysis</span></strong><span class="koboSpan" id="kobo.1593.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1594.1">or </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1595.1">SA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1596.1">)</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1597.1">A pair of </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.1598.1">sequences (for example, machine translation or </span><strong class="bold"><span class="koboSpan" id="kobo.1599.1">question-answering</span></strong><span class="koboSpan" id="kobo.1600.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1601.1">QA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1602.1">) problems)</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1603.1">The first token of every sequence is always a special classification token, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1604.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1605.1">. </span><span class="koboSpan" id="kobo.1605.2">The encoder output, corresponding to this token, is used as the aggregate sequence representation for classification tasks. </span><span class="koboSpan" id="kobo.1605.3">For example, if we want to apply SA over the sequence, the output corresponding to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1606.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1607.1"> input token will represent the sentiment (positive/negative) output of the model (this example is relevant when the input data is a single sequence). </span><span class="koboSpan" id="kobo.1607.2">This is necessary because the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1608.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1609.1"> token acts as a query, while all other elements of the input sequence act as the key/value store. </span><span class="koboSpan" id="kobo.1609.2">In this way, all tokens of the sequence participate in the weighted attention vector, which serves as input to the rest of the model. </span><span class="koboSpan" id="kobo.1609.3">Selecting another token besides </span><strong class="source-inline"><span class="koboSpan" id="kobo.1610.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1611.1"> excludes this token from the attention formula, which introduces unfair bias against it and results in an </span><span class="No-Break"><span class="koboSpan" id="kobo.1612.1">incomplete sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1613.1">If the </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.1614.1">input data is a pair of sequences, we pack them together in a single sequence, separated by a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.1615.1">[SEP]</span></strong><span class="koboSpan" id="kobo.1616.1"> token. </span><span class="koboSpan" id="kobo.1616.2">On top of that, we have additional learned segmentation embedding for every token, which indicates whether it belongs to sequence </span><em class="italic"><span class="koboSpan" id="kobo.1617.1">A</span></em><span class="koboSpan" id="kobo.1618.1"> or sequence </span><em class="italic"><span class="koboSpan" id="kobo.1619.1">B</span></em><span class="koboSpan" id="kobo.1620.1">. </span><span class="koboSpan" id="kobo.1620.2">Therefore, the input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings. </span><span class="koboSpan" id="kobo.1620.3">Here, the token and position embeddings serve the same purpose as they do in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1621.1">regular transformer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1622.1">Now that we are familiar with the input data representation, let’s continue with </span><span class="No-Break"><span class="koboSpan" id="kobo.1623.1">the training.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1624.1">BERT training</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1625.1">BERT </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.1626.1">training is a two-step process (this is also valid for other </span><span class="No-Break"><span class="koboSpan" id="kobo.1627.1">transformer-based models):</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1628.1">Pre-training</span></strong><span class="koboSpan" id="kobo.1629.1">: Train the model with unlabeled data over different </span><span class="No-Break"><span class="koboSpan" id="kobo.1630.1">pre-training tasks</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1631.1">Fine-tuning</span></strong><span class="koboSpan" id="kobo.1632.1">: A form of </span><strong class="bold"><span class="koboSpan" id="kobo.1633.1">transfer learning</span></strong><span class="koboSpan" id="kobo.1634.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1635.1">TL</span></strong><span class="koboSpan" id="kobo.1636.1">), where we initialize the model with the </span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.1637.1">pre-trained parameters and fine-tune them over a labeled dataset of a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.1638.1">downstream task</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1639.1">We can see the pre-training on the left side and the fine-tuning on the right side of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1640.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer873">
<span class="koboSpan" id="kobo.1641.1"><img alt="Figure 7.12 – Left: pre-training; right: fine-tuning (source: https://arxiv.org/abs/1810.04805)" src="image/B19627_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1642.1">Figure 7.12 – Left: pre-training; right: fine-tuning (source: https://arxiv.org/abs/1810.04805)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1643.1">Here, </span><strong class="bold"><span class="koboSpan" id="kobo.1644.1">Tok N</span></strong><span class="koboSpan" id="kobo.1645.1"> represents the one-hot-encoded input tokens, </span><strong class="bold"><span class="koboSpan" id="kobo.1646.1">E</span></strong><span class="koboSpan" id="kobo.1647.1"> represents the token embeddings, and </span><strong class="bold"><span class="koboSpan" id="kobo.1648.1">T</span></strong><span class="koboSpan" id="kobo.1649.1"> represents the model output vector. </span><span class="koboSpan" id="kobo.1649.2">The topmost labels represent the different tasks we can use the model for in each of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1650.1">training modes.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1651.1">The authors </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.1652.1">of the paper pre-trained the model </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.1653.1">using two unsupervised training tasks: </span><strong class="bold"><span class="koboSpan" id="kobo.1654.1">masked language modeling</span></strong><span class="koboSpan" id="kobo.1655.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1656.1">MLM</span></strong><span class="koboSpan" id="kobo.1657.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.1658.1">next sentence </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1659.1">prediction</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1660.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1661.1">NSP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1662.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1663.1">We’ll start with MLM, where the model is presented with an input sequence and its goal is to predict </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.1664.1">a missing word in that sequence. </span><span class="koboSpan" id="kobo.1664.2">MLM is similar in nature to the </span><strong class="bold"><span class="koboSpan" id="kobo.1665.1">continuous bag-of-words</span></strong><span class="koboSpan" id="kobo.1666.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1667.1">CBOW</span></strong><span class="koboSpan" id="kobo.1668.1">) objective of the word2vec model (see </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1669.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1670.1">). </span><span class="koboSpan" id="kobo.1670.2">To solve this task, the BERT encoder output is extended with an FC layer with softmax activation, which outputs the most probable word </span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.1671.1">given the input sequence. </span><span class="koboSpan" id="kobo.1671.2">Each input sequence is modified by randomly masking 15% (according to the paper) of the WordPiece tokens. </span><span class="koboSpan" id="kobo.1671.3">Within these 15%, we can replace the target token with either a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.1672.1">[MASK]</span></strong><span class="koboSpan" id="kobo.1673.1"> token (80% of the time), a random word (10% of the time), or leave the word as is (10% of the time). </span><span class="koboSpan" id="kobo.1673.2">This is necessary because the vocabulary of the downstream tasks doesn’t have the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1674.1">[MASK]</span></strong><span class="koboSpan" id="kobo.1675.1"> token. </span><span class="koboSpan" id="kobo.1675.2">On the other hand, the pre-trained model might expect it, which could lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.1676.1">unpredictable behavior.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1677.1">Next, let’s continue with NSP. </span><span class="koboSpan" id="kobo.1677.2">The authors argue that many important downstream tasks, such as </span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.1678.1">question answering or </span><strong class="bold"><span class="koboSpan" id="kobo.1679.1">natural language inference</span></strong><span class="koboSpan" id="kobo.1680.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1681.1">NLI</span></strong><span class="koboSpan" id="kobo.1682.1">), are based on understanding the relationship between two sentences, which is not directly captured by </span><span class="No-Break"><span class="koboSpan" id="kobo.1683.1">language modeling.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1684.1">NLI</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1685.1">NLI determines whether a sentence, which represents a </span><strong class="bold"><span class="koboSpan" id="kobo.1686.1">hypothesis</span></strong><span class="koboSpan" id="kobo.1687.1">, is either true (</span><strong class="bold"><span class="koboSpan" id="kobo.1688.1">entailment</span></strong><span class="koboSpan" id="kobo.1689.1">), false (</span><strong class="bold"><span class="koboSpan" id="kobo.1690.1">contradiction</span></strong><span class="koboSpan" id="kobo.1691.1">), or undetermined (</span><strong class="bold"><span class="koboSpan" id="kobo.1692.1">neutral</span></strong><span class="koboSpan" id="kobo.1693.1">) given another sentence, called a </span><strong class="bold"><span class="koboSpan" id="kobo.1694.1">premise</span></strong><span class="koboSpan" id="kobo.1695.1">. </span><span class="koboSpan" id="kobo.1695.2">For example, given the premise </span><em class="italic"><span class="koboSpan" id="kobo.1696.1">I am running</span></em><span class="koboSpan" id="kobo.1697.1">, we have the following hypothesis: </span><em class="italic"><span class="koboSpan" id="kobo.1698.1">I am sleeping</span></em><span class="koboSpan" id="kobo.1699.1"> is false; </span><em class="italic"><span class="koboSpan" id="kobo.1700.1">I am listening to music</span></em><span class="koboSpan" id="kobo.1701.1"> is undetermined; </span><em class="italic"><span class="koboSpan" id="kobo.1702.1">I am training</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1703.1">is true.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1704.1">The authors of BERT propose a simple and elegant unsupervised solution to pre-train the model to understand sentence relationships (displayed on the left side of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1705.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1706.1">.12</span></em><span class="koboSpan" id="kobo.1707.1">). </span><span class="koboSpan" id="kobo.1707.2">We’ll train the model on binary classification, where each input sample starts with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1708.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1709.1"> token and consists of two sequences (let’s use sentences for simplicity), </span><em class="italic"><span class="koboSpan" id="kobo.1710.1">A</span></em><span class="koboSpan" id="kobo.1711.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1712.1">B</span></em><span class="koboSpan" id="kobo.1713.1">, separated by a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1714.1">[SEP]</span></strong><span class="koboSpan" id="kobo.1715.1"> token. </span><span class="koboSpan" id="kobo.1715.2">We’ll extract sentences </span><em class="italic"><span class="koboSpan" id="kobo.1716.1">A</span></em><span class="koboSpan" id="kobo.1717.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1718.1">B</span></em><span class="koboSpan" id="kobo.1719.1"> from the training corpus. </span><span class="koboSpan" id="kobo.1719.2">In 50% of the training samples, </span><em class="italic"><span class="koboSpan" id="kobo.1720.1">B</span></em><span class="koboSpan" id="kobo.1721.1"> is the actual next sentence that follows </span><em class="italic"><span class="koboSpan" id="kobo.1722.1">A</span></em><span class="koboSpan" id="kobo.1723.1"> (labeled as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1724.1">is_next</span></strong><span class="koboSpan" id="kobo.1725.1">). </span><span class="koboSpan" id="kobo.1725.2">In the other 50%, </span><em class="italic"><span class="koboSpan" id="kobo.1726.1">B</span></em><span class="koboSpan" id="kobo.1727.1"> is a random sentence from the corpus (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1728.1">not_next</span></strong><span class="koboSpan" id="kobo.1729.1">). </span><span class="koboSpan" id="kobo.1729.2">As we mentioned, the model outputs </span><strong class="source-inline"><span class="koboSpan" id="kobo.1730.1">is_next</span></strong><span class="koboSpan" id="kobo.1731.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1732.1">not_next</span></strong><span class="koboSpan" id="kobo.1733.1"> labels on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1734.1">[CLS]</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1735.1">corresponding input.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1736.1">Next, let’s focus on the fine-tuning task, which follows the pre-training task (the right side of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1737.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1738.1">.12</span></em><span class="koboSpan" id="kobo.1739.1">). </span><span class="koboSpan" id="kobo.1739.2">The two steps are very similar, but instead of creating a masked sequence, we simply feed the BERT model with the task-specific unmodified input and output </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.1740.1">and fine-tune all the parameters in an end-to-end fashion. </span><span class="koboSpan" id="kobo.1740.2">Therefore, the model that we use in the fine-tuning phase is the same model that we’ll use in the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.1741.1">production environment.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1742.1">Let’s continue with some of the downstream tasks we can solve </span><span class="No-Break"><span class="koboSpan" id="kobo.1743.1">with BERT.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1744.1">BERT downstream tasks</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1745.1">The </span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.1746.1">following diagram shows how to solve several different types of tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.1747.1">with BERT:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer874">
<span class="koboSpan" id="kobo.1748.1"><img alt="Figure 7.13 – BERT applications for diﬀerent tasks (source: https://arxiv.org/abs/1810.04805)" src="image/B19627_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1749.1">Figure 7.13 – BERT applications for diﬀerent tasks (source: </span><a href="https://arxiv.org/abs/1810.04805"><span class="koboSpan" id="kobo.1750.1">https://arxiv.org/abs/1810.04805</span></a><span class="koboSpan" id="kobo.1751.1">)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1752.1">Let’s </span><a id="_idIndexMarker1107"/><span class="No-Break"><span class="koboSpan" id="kobo.1753.1">discuss them:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1754.1">The top-left scenario illustrates how to use BERT for sentence-pair classification tasks, such as NLI. </span><span class="koboSpan" id="kobo.1754.2">In short, we feed the model with two concatenated sentences and only look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1755.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1756.1"> token output classification, which will output the model result. </span><span class="koboSpan" id="kobo.1756.2">For example, in an NLI task, the goal is to predict whether the second sentence is an entailment, a contradiction, or neutral with respect to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1757.1">first one.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1758.1">The top-right scenario illustrates how to use BERT for single-sentence classification tasks, such as SA. </span><span class="koboSpan" id="kobo.1758.2">This is very similar to sentence-pair classification. </span><span class="koboSpan" id="kobo.1758.3">In both cases, we’ll extend the encoder with an FC layer and a binary softmax, with </span><em class="italic"><span class="koboSpan" id="kobo.1759.1">N</span></em><span class="koboSpan" id="kobo.1760.1"> possible classes (</span><em class="italic"><span class="koboSpan" id="kobo.1761.1">N</span></em><span class="koboSpan" id="kobo.1762.1"> is the number of classes for </span><span class="No-Break"><span class="koboSpan" id="kobo.1763.1">each task).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1764.1">The bottom-left scenario illustrates how to use BERT on a QA dataset. </span><span class="koboSpan" id="kobo.1764.2">Given that sequence </span><em class="italic"><span class="koboSpan" id="kobo.1765.1">A</span></em><span class="koboSpan" id="kobo.1766.1"> is a question and sequence </span><em class="italic"><span class="koboSpan" id="kobo.1767.1">B</span></em><span class="koboSpan" id="kobo.1768.1"> is a passage from </span><em class="italic"><span class="koboSpan" id="kobo.1769.1">Wikipedia</span></em><span class="koboSpan" id="kobo.1770.1">, which contains the answer, the goal is to predict the text span (start and end) of the answer within this passage. </span><span class="koboSpan" id="kobo.1770.2">The model outputs the probability for each token of sequence </span><em class="italic"><span class="koboSpan" id="kobo.1771.1">B</span></em><span class="koboSpan" id="kobo.1772.1"> to be either the start or the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.1773.1">the answer.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1774.1">The </span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.1775.1">bottom-right scenario illustrates how to use BERT for </span><strong class="bold"><span class="koboSpan" id="kobo.1776.1">named entity recognition</span></strong><span class="koboSpan" id="kobo.1777.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1778.1">NER</span></strong><span class="koboSpan" id="kobo.1779.1">), where each input token is classified as some type </span><span class="No-Break"><span class="koboSpan" id="kobo.1780.1">of entity.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1781.1">This </span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.1782.1">concludes our section dedicated to the BERT model. </span><span class="koboSpan" id="kobo.1782.2">Next, let’s focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.1783.1">decoder-only models.</span></span></p>
<h2 id="_idParaDest-144" lang="en-GB"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.1784.1">Generative Pre-trained Transformer</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1785.1">In this </span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.1786.1">section, we’ll discuss a decoder-only </span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.1787.1">model, known as </span><strong class="bold"><span class="koboSpan" id="kobo.1788.1">Generative Pre-trained Transformer</span></strong><span class="koboSpan" id="kobo.1789.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1790.1">GPT</span></strong><span class="koboSpan" id="kobo.1791.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.1792.1">Improving Language Understanding by Generative Pre-Training</span></em><span class="koboSpan" id="kobo.1793.1">, </span><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"><span class="koboSpan" id="kobo.1794.1">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</span></a><span class="koboSpan" id="kobo.1795.1">). </span><span class="koboSpan" id="kobo.1795.2">This is the first of a series of GPT models, released by OpenAI, which led to the now-famous GPT-3 </span><span class="No-Break"><span class="koboSpan" id="kobo.1796.1">and GPT-4.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1797.1">GPT model size</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1798.1">GPT has 12 decoder layers, each with 12 attention heads, and 768-dimensional attention vectors. </span><span class="koboSpan" id="kobo.1798.2">The FFN is 3,072-dimensional. </span><span class="koboSpan" id="kobo.1798.3">The model has a total of 117 million parameters (weights). </span><span class="koboSpan" id="kobo.1798.4">GPT uses BPE tokenization and has a token vocabulary size </span><span class="No-Break"><span class="koboSpan" id="kobo.1799.1">of 40,000.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1800.1">We can see the GPT decoder-only architecture in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1801.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer875">
<span class="koboSpan" id="kobo.1802.1"><img alt="Figure 7.14 – Left: post-ln decoder-only model; right: pre-ln decoder-only model; different outputs for the pre-training and fine-tuning training steps" src="image/B19627_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1803.1">Figure 7.14 – Left: post-ln decoder-only model; right: pre-ln decoder-only model; different outputs for the pre-training and fine-tuning training steps</span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1804.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1805.1">We </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.1806.1">discuss the decoder-only architecture </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.1807.1">in the context of the original GPT paper, but it applies to the broad class of </span><span class="No-Break"><span class="koboSpan" id="kobo.1808.1">decoder-only models.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1809.1">It is derived from the decoder we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1810.1">Transformer decoder</span></em><span class="koboSpan" id="kobo.1811.1"> section. </span><span class="koboSpan" id="kobo.1811.2">The model takes as input token embeddings and adds static positional encoding. </span><span class="koboSpan" id="kobo.1811.3">This is followed by a stack of </span><em class="italic"><span class="koboSpan" id="kobo.1812.1">N</span></em><span class="koboSpan" id="kobo.1813.1"> decoder blocks. </span><span class="koboSpan" id="kobo.1813.2">Each block has </span><span class="No-Break"><span class="koboSpan" id="kobo.1814.1">two sublayers:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1815.1">Masked multi-head self-attention</span></strong><span class="koboSpan" id="kobo.1816.1">: Let’s put emphasis on the masked part. </span><span class="koboSpan" id="kobo.1816.2">It </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.1817.1">determines the main properties of the decoder-only model—it is unidirectional and autoregressive. </span><span class="koboSpan" id="kobo.1817.2">This is opposed to bidirectional </span><span class="No-Break"><span class="koboSpan" id="kobo.1818.1">encoder-only models.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1819.1">FFN</span></strong><span class="koboSpan" id="kobo.1820.1">: This </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.1821.1">sublayer has the same purpose as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1822.1">encoder-decoder model.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1823.1">The sublayers contain the relevant residual links, normalization, and dropout. </span><span class="koboSpan" id="kobo.1823.2">The decoder comes in pre-ln and </span><span class="No-Break"><span class="koboSpan" id="kobo.1824.1">post-ln flavors.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1825.1">The model ends with an FC layer, followed by a softmax operation, which can be adapted to suit the specific task </span><span class="No-Break"><span class="koboSpan" id="kobo.1826.1">at hand.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1827.1">The </span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.1828.1">main difference between this decoder and the one in the full encoder-decoder transformer is the lack of an attention sublayer, which links the encoder and decoder blocks in the full model. </span><span class="koboSpan" id="kobo.1828.2">Since the current architecture doesn’t have an encoder part, the sublayer is obsolete. </span><span class="koboSpan" id="kobo.1828.3">This makes the decoder </span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.1829.1">very similar to the encoder, except for masked self-attention. </span><span class="koboSpan" id="kobo.1829.2">Hence, the main difference between the encoder-only and decoder-only models is that they are bidirectional and </span><span class="No-Break"><span class="koboSpan" id="kobo.1830.1">unidirectional, respectively.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1831.1">As with BERT, the training of GPT is a two-step process, which consists of unsupervised pre-training and supervised fine-tuning. </span><span class="koboSpan" id="kobo.1831.2">Let’s start with the pre-training, which resembles the seq2seq training algorithm (the decoder part) we described in the </span><em class="italic"><span class="koboSpan" id="kobo.1832.1">Introducing seq2seq models</span></em><span class="koboSpan" id="kobo.1833.1"> section. </span><span class="koboSpan" id="kobo.1833.2">As a reminder, we train the original seq2seq model to transform an input sequence of tokens into another, different output sequence of tokens. </span><span class="koboSpan" id="kobo.1833.3">Examples of such tasks include machine translation and question answering. </span><span class="koboSpan" id="kobo.1833.4">The original seq2seq training is supervised because matching the input and output sequences counts as labeling. </span><span class="koboSpan" id="kobo.1833.5">Once the full input sequence is fed into the seq2seq encoder, the decoder starts generating the output sequence one token at a time. </span><span class="koboSpan" id="kobo.1833.6">In effect, the seq2seq decoder learns to predict the next word in the sequence (as opposed to predicting any masked word in the full sequence, as with BERT). </span><span class="koboSpan" id="kobo.1833.7">Here, we have a similar algorithm, but the output sequence is the same as the input sequence. </span><span class="koboSpan" id="kobo.1833.8">From a language modeling point of view, the pre-training learns to approximate the conditional probability of the next token, </span><span class="koboSpan" id="kobo.1834.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/725.png" style="vertical-align:-0.340em;height:0.929em;width:0.556em"/></span><span class="koboSpan" id="kobo.1835.1">, given an input sequence of tokens, </span><span class="koboSpan" id="kobo.1836.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/726.png" style="vertical-align:-0.340em;height:0.929em;width:3.471em"/></span><span class="koboSpan" id="kobo.1837.1">, and the model parameters, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1838.1">θ</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1839.1">: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1840.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/727.png" style="vertical-align:-0.390em;height:1.189em;width:6.198em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1841.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1842.1">Let’s illustrate the pre-training with an example. </span><span class="koboSpan" id="kobo.1842.2">We’ll assume that our input sequence is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1843.1">[[START], </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1844.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/728.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1845.1">,  </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1846.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/729.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1847.1">, ..., </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1848.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/730.png" style="vertical-align:-0.338em;height:0.927em;width:1.309em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1849.1">]</span></strong><span class="koboSpan" id="kobo.1850.1"> and that we’ll denote the training pairs as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1851.1">{input: label}</span></strong><span class="koboSpan" id="kobo.1852.1">. </span><span class="koboSpan" id="kobo.1852.2">Our training pairs are going to be </span><strong class="source-inline"><span class="koboSpan" id="kobo.1853.1">{[[START]]: </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1854.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/728.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1855.1">}</span></strong><span class="koboSpan" id="kobo.1856.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1857.1">{[[START], </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1858.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/728.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1859.1">]: </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1860.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/729.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1861.1">}</span></strong><span class="koboSpan" id="kobo.1862.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1863.1">{[[START], </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1864.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/728.png" style="vertical-align:-0.333em;height:0.922em;width:0.559em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1865.1">,..., </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1866.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/730.png" style="vertical-align:-0.338em;height:0.927em;width:1.309em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1867.1">]: </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1868.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/736.png" style="vertical-align:-0.338em;height:0.927em;width:0.691em"/></span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1869.1">}</span></strong><span class="koboSpan" id="kobo.1870.1">. </span><span class="koboSpan" id="kobo.1870.2">We can see the same scenario displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1871.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer888">
<span class="koboSpan" id="kobo.1872.1"><img alt="Figure 7.15 – GPT pre-training to predict the next word of the same input/output sequence" src="image/B19627_07_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1873.1">Figure 7.15 – GPT pre-training to predict the next word of the same input/output sequence</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1874.1">Next, let’s discuss the supervised fine-tuning step, which is similar to BERT fine-tuning. </span><span class="koboSpan" id="kobo.1874.2">The </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.1875.1">following diagram illustrates how the </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.1876.1">tasks of sequence classification and NLI work </span><span class="No-Break"><span class="koboSpan" id="kobo.1877.1">in GPT:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer889">
<span class="koboSpan" id="kobo.1878.1"><img alt="Figure 7.16 – GPT fine-tuning; top: text classification; bottom: NLI" src="image/B19627_07_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1879.1">Figure 7.16 – GPT fine-tuning; top: text classification; bottom: NLI</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1880.1">In both cases, we have special </span><strong class="source-inline"><span class="koboSpan" id="kobo.1881.1">[START]</span></strong><span class="koboSpan" id="kobo.1882.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1883.1">[EXTRACT]</span></strong><span class="koboSpan" id="kobo.1884.1"> tokens. </span><span class="koboSpan" id="kobo.1884.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1885.1">[EXTRACT]</span></strong><span class="koboSpan" id="kobo.1886.1"> token plays the same role as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1887.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1888.1"> in BERT—we take the output of that token as the result of the classification. </span><span class="koboSpan" id="kobo.1888.2">But here, it’s at the end of the sequence, rather than the start. </span><span class="koboSpan" id="kobo.1888.3">Again, the reason for this is that the decoder is unidirectional and only has full access to the input sequence at its end. </span><span class="koboSpan" id="kobo.1888.4">The NLI task concatenates the premise and the entailment, separated by a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.1889.1">[</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1890.1">DELIM]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1891.1"> token.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1892.1">This concludes our introduction to GPT—the prototypical example of a decoder-only model. </span><span class="koboSpan" id="kobo.1892.2">With this, we’ve introduced the three major transformer architectures—encoder-decoder, encoder-only, and decoder-only. </span><span class="koboSpan" id="kobo.1892.3">This is also a good place to conclude </span><span class="No-Break"><span class="koboSpan" id="kobo.1893.1">the chapter.</span></span></p>
<h1 id="_idParaDest-145" lang="en-GB"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.1894.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1895.1">Our focus in this chapter was the attention mechanism and transformers. </span><span class="koboSpan" id="kobo.1895.2">We started with the seq2seq model, and we discussed Bahdanau and Luong attention in its context. </span><span class="koboSpan" id="kobo.1895.3">Next, we gradually introduced the TA mechanism, before discussing the full encoder-decoder transformer architecture. </span><span class="koboSpan" id="kobo.1895.4">Finally, we focused on encoder-only and decoder-only </span><span class="No-Break"><span class="koboSpan" id="kobo.1896.1">transformer variants.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1897.1">In the next chapter, we’ll focus on LLMs, and we’ll explore the Hugging Face </span><span class="No-Break"><span class="koboSpan" id="kobo.1898.1">transformers library.</span></span></p>
</div>
</body></html>