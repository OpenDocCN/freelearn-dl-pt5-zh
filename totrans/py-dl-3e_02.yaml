- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: In [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016), we introduced a number of
    basic **machine learning** (**ML**) concepts and techniques. We went through the
    main ML paradigms, as well as some popular classic ML algorithms, and we finished
    on **neural networks** (**NN**). In this chapter, we will formally introduce what
    NNs are, discuss their mathematical foundations, describe in detail how their
    building blocks work, see how we can stack many layers to create a deep feedforward
    NN, and then learn how to train them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中，我们介绍了许多基本的**机器学习**（**ML**）概念和技术。我们讲解了主要的机器学习范式以及一些经典的流行机器学习算法，最后介绍了**神经网络**（**NN**）。在本章中，我们将正式介绍神经网络是什么，讨论它们的数学基础，详细描述其构建模块如何工作，看看我们如何堆叠多个层来创建深度前馈神经网络，并学习如何训练它们。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The need for NNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的需求
- en: The math of NNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的数学
- en: An introduction to NNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Training NNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The link between NNs and the human brain
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络与人类大脑的联系
- en: Initially, NNs were inspired by the biological brain (hence the name). Over
    time, however, we’ve stopped trying to emulate how the brain works, and instead,
    we’re focused on finding the correct configurations for specific tasks, including
    computer vision, natural language processing, and speech recognition. You can
    think of it this way – for a long time, we were inspired by the flight of birds,
    but, in the end, we created airplanes, which are quite different. We are still
    far from matching the potential of the brain. Perhaps the machine learning algorithms
    of the future will resemble the brain more, but that’s not the case now. Hence,
    for the rest of this book, we won’t try to create analogies between the brain
    and NNs. To follow this train of thought, we’ll call the smallest building NN
    building blocks **units**, instead of neurons, as they were originally known.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，神经网络（NN）是受生物大脑的启发（因此得名）。然而，随着时间的推移，我们已不再尝试模仿大脑的工作方式，而是集中精力寻找适用于特定任务的正确配置，包括计算机视觉、自然语言处理和语音识别。你可以这样理解——长时间以来，我们受到了鸟类飞行的启发，但最终我们创造了飞机，这与鸟类飞行是截然不同的。我们仍然远未能与大脑的潜力相匹配。也许未来的机器学习算法会更像大脑，但现在情况并非如此。因此，在本书的其余部分，我们将不再试图在大脑和神经网络之间做类比。为了沿着这个思路进行，我们将把最小的神经网络构建模块称为**单元**，而不是最初的“神经元”。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python. If you don’t have
    an environment set up with these tools, fret not – the example is available as
    a Jupyter notebook on Google Colab. You can find the code examples in the book’s
    GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用 Python 实现示例。如果你尚未设置好相关工具的环境，不用担心——该示例可以在 Google Colab 上作为 Jupyter
    notebook 使用。你可以在本书的 GitHub 仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02)。
- en: The need for NNs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的需求
- en: NNs have been around for many years, and they’ve gone through several periods,
    during which they’ve fallen in and out of favor. However, recently, they have
    steadily gained ground over many other competing machine learning algorithms.
    This resurgence is due to computers getting faster, the use of **graphical processing
    units** (**GPUs**) versus the most traditional use of **central processing units**
    (**CPUs**), better algorithms and NN design, and increasingly larger datasets,
    which we’ll look at in this book. To get an idea of their success, let’s look
    at the ImageNet Large Scale Visual Recognition Challenge ([http://image-net.org/challenges/LSVRC/](http://image-net.org/challenges/LSVRC/),
    or just **ImageNet**). The participants train their algorithms using the ImageNet
    database. It contains more than 1 million high-resolution color images in over
    1,000 categories (one category may be images of cars, another of people, trees,
    and so on). One of the tasks in the challenge is to classify unknown images into
    these categories. In 2011, the winner achieved a top-five accuracy of 74.2%. In
    2012, Alex Krizhevsky and his team entered the competition with a convolutional
    network (a special type of deep network). That year, they won with a top-five
    accuracy of 84.7%. Since then, the winning algorithms have always been NNs, and
    the current top-five accuracy is around 99%. However, deep learning algorithms
    have excelled in other areas – for example, both Google Now and Apple’s Siri assistants
    rely on deep networks for speech recognition and Google’s use of deep learning
    for their translation engines. Recent image and text generation systems such as
    Stability AI’s Stable Diffusion and OpenAI’s DALL-E and ChatGPT are implemented
    with NNs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经存在了许多年，并经历了多个阶段，期间它们的受欢迎程度时高时低。然而，最近，它们在许多其他竞争的机器学习算法中稳步占据了主导地位。这一复兴归因于计算机速度的提升，**图形处理单元**（**GPU**）的使用取代了最传统的**中央处理单元**（**CPU**）使用，算法和神经网络设计的改进，以及日益增大的数据集，这些我们将在本书中探讨。为了让你了解它们的成功，让我们来看看ImageNet大规模视觉识别挑战赛（[http://image-net.org/challenges/LSVRC/](http://image-net.org/challenges/LSVRC/)，或者简称**ImageNet**）。参与者通过使用ImageNet数据库来训练他们的算法。该数据库包含了超过100万张高分辨率彩色图像，涵盖了1,000多个类别（一个类别可能是汽车的图像，另一个类别是人的图像、树木的图像等等）。该挑战赛中的一个任务是将未知的图像分类到这些类别中。2011年，获胜者实现了74.2%的前五名准确率。2012年，Alex
    Krizhevsky及其团队带着卷积网络（深度网络的一种特殊类型）参加了比赛。那年，他们以84.7%的前五名准确率赢得了比赛。从那时起，获胜的算法总是神经网络，而当前的前五名准确率大约为99%。然而，深度学习算法在其他领域也表现出色——例如，谷歌的Google
    Now和苹果的Siri助手依赖深度网络进行语音识别，谷歌也利用深度学习进行翻译引擎的开发。最近的图像和文本生成系统，如Stability AI的Stable
    Diffusion和OpenAI的DALL-E与ChatGPT，都是使用神经网络实现的。
- en: 'We’ll talk about these exciting advances in the following chapters, but for
    now, we’ll focus on the mathematical foundations of NNs. To help us with this
    task, we’ll use simple networks with one or two layers. You can think of these
    as toy examples that are not deep networks, but understanding how they work is
    important. Here’s why:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中讨论这些激动人心的进展，但现在，我们将专注于神经网络的数学基础。为了帮助我们完成这项任务，我们将使用具有一两层的简单网络。你可以将这些视为玩具示例，它们不是深度网络，但理解它们的工作原理非常重要。原因如下：
- en: Knowing the theory of NNs will help you understand the rest of the book because
    a large majority of NNs in use today share common principles. Understanding simple
    networks means that you’ll understand deep networks too.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解神经网络的理论将帮助你理解本书的其余部分，因为如今大多数使用中的神经网络都共享一些共同的原理。理解简单的网络意味着你也能理解深度网络。
- en: Having some fundamental knowledge is always good. It will help you a lot when
    you face some new material (even material not included in this book).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有一些基础知识总是有益的。当你面对一些新的材料时（即使是本书未涉及的材料），它会对你大有帮助。
- en: I hope these arguments will convince you of the importance of this chapter.
    As a small consolation, we’ll talk about deep learning in depth (pun intended)
    in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这些论点能说服你本章的重要性。作为一个小小的安慰，我们将在[*第3章*](B19627_03.xhtml#_idTextAnchor079)中深入讨论深度学习（这个双关语用得很恰当）。
- en: The math of NNs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的数学
- en: In the following few sections, we’ll discuss the mathematical principles of
    NNs. This way, we’ll be able to explain NNs through these very principles in a
    fundamental and structured way.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分中，我们将讨论神经网络的数学原理。通过这种方式，我们将能够以一种基础且结构化的方式，通过这些原理来解释神经网络。
- en: Linear algebra
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性代数
- en: Linear algebra deals with objects such as vectors and matrices, linear transformations,
    and linear equations such as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/32.png).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数处理的对象包括向量、矩阵、线性变换和线性方程，如 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/32.png)。
- en: 'Linear algebra identifies the following mathematical objects:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数识别以下数学对象：
- en: '**Scalar**: A single number.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：一个单独的数字。'
- en: '**Vector**: A one-dimensional array of numbers (also known as components or
    **scalars**), where each element has an index. We can denote vectors either with
    a superscript arrow (![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>x</mi><mo
    stretchy="true">→</mo></mover></mrow></math>](img/33.png)) or in bold (**x**),
    but we’ll mostly use the bold notation throughout the book. The following is an
    example of a vector:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：一个一维的数字数组（也称为分量或**标量**），每个元素都有一个索引。我们可以用上标箭头 (![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>x</mi><mo
    stretchy="true">→</mo></mover></mrow></math>](img/33.png)) 或加粗字体 (**x**) 来表示向量，但在本书中我们将主要使用加粗符号。以下是一个向量的示例：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/34.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/34.png)'
- en: 'We can represent a *n*-dimensional vector as the coordinates of a point in
    an *n*-dimensional Euclidean space, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/35.png).
    Think of Euclidean space as a coordinate system – the vector starts at the center
    of that system, and each of the vector’s elements represents the coordinate of
    the point along its corresponding coordinate axis. The following figure shows
    a vector in a three-dimensional coordinate system, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/36.png):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个 *n* 维向量表示为一个点在 *n* 维欧几里得空间中的坐标，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/35.png)。可以将欧几里得空间视为一个坐标系——向量从该坐标系的中心开始，每个向量的元素表示沿其相应坐标轴的点的坐标。下图显示了三维坐标系中的一个向量，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/36.png)：
- en: '![Figure 2.1 – Vector representation in a three-dimensional space](img/B19627_02_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 三维空间中的向量表示](img/B19627_02_01.jpg)'
- en: Figure 2.1 – Vector representation in a three-dimensional space
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 三维空间中的向量表示
- en: 'The figure can also help us define two additional properties of the vector:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该图还帮助我们定义了向量的两个额外属性：
- en: '**Magnitude** (or **length**):'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小**（或**长度**）：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfenced
    open="|" close="|"><mi mathvariant="bold">x</mi></mfenced><mo>=</mo><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mo>…</mo><mo>+</mo><msubsup><mi>x</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mrow></math>](img/37.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfenced
    open="|" close="|"><mi mathvariant="bold">x</mi></mfenced><mo>=</mo><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mo>…</mo><mo>+</mo><msubsup><mi>x</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mrow></math>](img/37.png)'
- en: Think of the magnitude as a generalization of the Pythagorean theorem for an
    n-dimensional space.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将大小视为勾股定理在 n 维空间中的推广。
- en: '**Direction**: The angle between the vector and each axis of the vector space.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方向**：向量与向量空间中各轴之间的角度。'
- en: '**Matrix**: A two-dimensional array of scalars, where each element is identified
    by a row and a column. We’ll denote a matrix with a bold capital letter – for
    example, **A**. Conversely, we’ll denote the matrix elements with the small matrix
    letter, with the row and column as subscript indices – for example, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/38.png).
    We can see an example of the matrix notation in the following formula:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：一个二维标量数组，每个元素通过行和列来标识。我们用粗体大写字母表示矩阵—例如，**A**。相反，我们用小写字母表示矩阵元素，并将行和列作为下标—例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/38.png)。我们可以在下面的公式中看到矩阵符号的例子：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/39.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/39.png)'
- en: We can represent a vector as either a single-row *1×n* matrix (**row matrix**)
    or a single-column *n×1* matrix (**column matrix**). Transformed like this, the
    vector can participate in different matrix operations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将向量表示为单行的 *1×n* 矩阵（**行矩阵**）或单列的 *n×1* 矩阵（**列矩阵**）。通过这种转换，向量可以参与不同的矩阵运算。
- en: '**Tensor**: The term *tensor* originates from mathematics and physics, where
    it existed long before we started using it in ML. Its definition in these fields
    differs from the one in ML. Fortunately, the tensor in the context of ML is just
    a multi-dimensional array with the following properties:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**：*张量* 这一术语来源于数学和物理学，在我们开始在机器学习中使用它之前，它就已经存在。这些领域中的定义与机器学习中的定义不同。幸运的是，在机器学习的背景下，张量仅仅是一个具有以下属性的多维数组：'
- en: '**Rank**: The number of array dimensions. Vectors and matrices are special
    cases of tensors. A tensor of rank 0 is a scalar, a tensor of rank 1 is a vector,
    and a tensor of rank 2 is a matrix. There is no limit on the number of dimensions,
    and some types of NNs can use tensors of rank 4 or more.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：数组的维度数量。向量和矩阵是张量的特殊情况。秩为 0 的张量是标量，秩为 1 的张量是向量，秩为 2 的张量是矩阵。维度数量没有限制，某些类型的神经网络可以使用秩为
    4 或更高的张量。'
- en: '**Shape**: The size of each of the tensor’s dimensions.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状**：张量每个维度的大小。'
- en: '**Data type** of the tensor values. In practice, the data types include 16-,
    32-, and 64-bit floats and 8-, 16-, 32-, and 64-bit integers.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量值的 **数据类型**。在实际应用中，数据类型包括 16 位、32 位和 64 位浮动数，以及 8 位、16 位、32 位和 64 位整数。
- en: The tensor is the main data structure of libraries such as PyTorch, Keras, and
    TensorFlow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是像 PyTorch、Keras 和 TensorFlow 等库的主要数据结构。
- en: The nature of tensors
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的性质
- en: 'You can find a detailed discussion on the nature of tensors here: [https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors](https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors).
    You can also compare this with the PyTorch ([https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html))
    and TensorFlow ([https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor))
    and tensor definitions.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到关于张量性质的详细讨论：[https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors](https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors)。你还可以将此与
    PyTorch ([https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html))
    和 TensorFlow ([https://www.tensorflow.org/guide/tensor](https://www.tensorflow.org/guide/tensor))
    的张量定义进行对比。
- en: Now that we’ve introduced vectors, matrices, and tensors, let’s continue with
    some of the linear operations they can participate in.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经介绍了向量、矩阵和张量，让我们继续讨论它们能参与的某些线性运算。
- en: Vector and matrix operations
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量和矩阵运算
- en: 'We’ll focus on the operations that relate to NNs, starting with vectors:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点讨论与神经网络（NNs）相关的运算，从向量开始：
- en: '**Vector addition**: Adds two or more *n*-dimensional vectors, **a** and **b**
    (and so on) to a new vector:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量加法**：将两个或更多 *n* 维向量 **a** 和 **b**（等等）加在一起，得到一个新的向量：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/40.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/40.png)'
- en: '**Dot (or scalar) product**: Combines two *n*-dimensional vectors, **a** and
    **b**, into a **scalar value**:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点积（或标量积）**：将两个 *n* 维向量 **a** 和 **b** 合并为一个 **标量值**：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"
    separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mfenced><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/41.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"
    separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mfenced><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/41.png)'
- en: 'Here, the angle between the two vectors is *θ*, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/42.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/43.png)
    are their magnitudes. For example, if the vectors are *two*-dimensional and their
    components are ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/44.png),
    the preceding formula becomes the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/45.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram illustrates the dot product of **a** and **b**:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – The dot product of vectors – top: vector components, and bottom:
    the dot product of the two vectors](img/B19627_02_02.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2 – The dot product of vectors – top: vector components, and bottom:
    the dot product of the two vectors'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of the dot product as a similarity measure between the two vectors,
    where the angle *θ* indicates how similar they are. If *θ* is small (the vectors
    have similar directions), then their dot product will be higher because ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/46.png)
    will converge toward 1\. In this context, we can define a **cosine similarity**
    between two vectors as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将点积视为两个向量之间的相似度度量，其中角度*θ*表示它们的相似程度。如果*θ*很小（即向量方向相似），那么它们的点积将更高，因为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/46.png)会趋近于
    1。在这种情况下，我们可以定义两个向量之间的**余弦相似度**如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold">a</mi><mo>⋅</mo><mi mathvariant="bold">b</mi></mrow><mrow><mfenced
    open="|" close="|"><mi mathvariant="bold">a</mi></mfenced><mfenced open="|" close="|"><mi
    mathvariant="bold">b</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/47.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold">a</mi><mo>⋅</mo><mi mathvariant="bold">b</mi></mrow><mrow><mfenced
    open="|" close="|"><mi mathvariant="bold">a</mi></mfenced><mfenced open="|" close="|"><mi
    mathvariant="bold">b</mi></mfenced></mrow></mfrac></mrow></mrow></math>](img/47.png)'
- en: '**Cross (or vector) product**: A combination of two vectors, **a** and **b**,
    in a new vector, which is perpendicular to both initial vectors. The magnitude
    of the output vector is equal to the following:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叉积（或向量积）**：两个向量**a**和**b**的组合，得到一个新的向量，该向量垂直于两个初始向量。输出向量的大小等于以下公式：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>×</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"
    separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mfenced><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/48.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">a</mml:mi><mml:mo>×</mml:mo><mml:mi
    mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"
    separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mfenced><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mfenced><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:math>](img/48.png)'
- en: 'We can see an example of a cross product of two-dimensional vectors in the
    following diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到二维向量叉积的例子：
- en: '![Figure 2.3 – A cross product of two two-dimensional vectors](img/B19627_02_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 两个二维向量的叉积](img/B19627_02_03.jpg)'
- en: Figure 2.3 – A cross product of two two-dimensional vectors
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 两个二维向量的叉积
- en: The output vector is perpendicular (or **normal**) to the plane, which contains
    the input vectors. The output vector magnitude is equal to the area of a parallelogram
    with sides, the **a** and **b** vectors (denoted with light blue in the preceding
    diagram).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出向量垂直（或**法向**）于包含输入向量的平面。输出向量的大小等于由**a**和**b**向量（在前面的图中以浅蓝色表示）构成的平行四边形的面积。
- en: 'Now, let’s focus on the matrix operations:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注矩阵运算：
- en: '**Matrix transpose**: Flip the matrix along its main diagonal, represented
    by the set of all matrix elements, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/49.png),
    where *i=j*. We’ll denote the transpose operation with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>⊤</mml:mi></mml:math>](img/50.png)
    in superscript form. The cell ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/51.png)
    of **A** is equal to the cell ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/49.png)
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/53.png):'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/54.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'The transpose of an *m×n* matrix is an *n×m* matrix, as we can see with the
    following examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>⇒</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>33</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/55.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold">A</mi><mo>=</mo><mfenced open="[" close="]"><mtable columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" columnalign="center center center" rowspacing="1.0000ex"
    rowalign="baseline baseline"><mtr><mtd><msub><mi>a</mi><mn>11</mn></msub></mtd><mtd><msub><mi>a</mi><mn>12</mn></msub></mtd><mtd><msub><mi>a</mi><mn>13</mn></msub></mtd></mtr><mtr><mtd><msub><mi>a</mi><mn>21</mn></msub></mtd><mtd><msub><mi>a</mi><mn>22</mn></msub></mtd><mtd><msub><mi>a</mi><mn>23</mn></msub></mtd></mtr></mtable></mfenced><mo>⇒</mo><msup><mi
    mathvariant="bold">A</mi><mi mathvariant="normal">⊤</mi></msup><mo>=</mo><mfenced
    open="[" close="]"><mtable columnspacing="0.8000em" columnwidth="auto auto" columnalign="center
    center" rowspacing="1.0000ex 1.0000ex" rowalign="baseline baseline baseline"><mtr><mtd><msub><mi>a</mi><mn>11</mn></msub></mtd><mtd><msub><mi>a</mi><mn>21</mn></msub></mtd></mtr><mtr><mtd><msub><mi>a</mi><mn>12</mn></msub></mtd><mtd><msub><mi>a</mi><mn>22</mn></msub></mtd></mtr><mtr><mtd><msub><mi>a</mi><mn>13</mn></msub></mtd><mtd><msub><mi>a</mi><mn>23</mn></msub></mtd></mtr></mtable></mfenced></mrow></mrow></math>](img/56.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold">A</mi><mo>=</mo><mfenced open="[" close="]"><mtable columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" columnalign="center center center" rowalign="baseline"><mtr><mtd><msub><mi>a</mi><mn>11</mn></msub></mtd><mtd><msub><mi>a</mi><mn>12</mn></msub></mtd><mtd><msub><mi>a</mi><mn>13</mn></msub></mtd></mtr></mtable></mfenced><mo>⇒</mo><msup><mi
    mathvariant="bold">A</mi><mi mathvariant="normal">⊤</mi></msup><mo>=</mo><mfenced
    open="[" close="]"><mtable columnwidth="auto" columnalign="center" rowspacing="1.0000ex
    1.0000ex" rowalign="baseline baseline baseline"><mtr><mtd><msub><mi>a</mi><mn>11</mn></msub></mtd></mtr><mtr><mtd><msub><mi>a</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd><msub><mi>a</mi><mn>13</mn></msub></mtd></mtr></mtable></mfenced></mrow></mrow></math>](img/57.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: '**Matrix-scalar multiplication**: Multiplication of a matrix, **A**, by a scalar,
    *y*, into a new matrix with the same size as the original:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/58.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: '**Matrix-matrix addition**: Element-wise addition of two or more matrices,
    **A** and **B** (and so on), into a new matrix. All input matrices must be the
    same size:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"
    separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/59.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: '**Matrix-vector multiplication**: Multiplication of a matrix, **A**, by a vector,
    **x**, into a new vector:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/60.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/61.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>17</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>39</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/62.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: The number of matrix columns must be equal to the vector size. The result of
    an *m×n* matrix, multiplied by an *n*-dimensional vector, is an *m*-dimensional
    vector. We can assume that the rows of the matrix are *n*-dimensional vectors.
    Then, each value of the output vector is the dot product between the corresponding
    matrix row vector and **x**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix multiplication**: A binary operation, which represents the multiplication
    of two matrices, **A** and **B**, into a single output matrix. We can think of
    it as multiple matrix-vector multiplications, where each column of the second
    matrix is a vector:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>31</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>23</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/63.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd><mml:mn>5</mml:mn></mml:mtd><mml:mtd><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>5</mml:mn></mml:mtd><mml:mtd><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>6</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mn>8</mml:mn><mml:mo>+</mml:mo><mml:mn>18</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>4</mml:mn><mml:mo>+</mml:mo><mml:mn>15</mml:mn><mml:mo>+</mml:mo><mml:mn>30</mml:mn></mml:mtd><mml:mtd><mml:mn>8</mml:mn><mml:mo>+</mml:mo><mml:mn>20</mml:mn><mml:mo>+</mml:mo><mml:mn>36</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>22</mml:mn></mml:mtd><mml:mtd><mml:mn>28</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>49</mml:mn></mml:mtd><mml:mtd><mml:mn>64</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math>](img/64.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: If we represent the two vectors as matrices, their dot product, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">a</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold">a</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/65.png),
    is equivalent to matrix-matrix multiplication.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: You can now breathe a sigh of relief because we’ve concluded our introduction
    to linear algebra. Not all is well though, as we’ll focus on probability theory
    next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to probability
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll introduce some basic concepts of probability theory.
    They will help us later in the book when we discuss NN training algorithms and
    natural language processing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the concept of a **statistical experiment**, which has the
    following properties:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: It is composed of multiple independent trials
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outcome of each trial is determined by chance (it is **non-deterministic**)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has multiple possible outcomes, known as **events**
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know in advance all possible experiment outcomes
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of statistical experiments include a coin toss with two possible outcomes
    (heads or tails) and a dice roll with six possible outcomes (1 to 6).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood that some event, *е*, would occur is known as **probability**
    P(*е*). It is a value in the range of [0, 1]. P(*e*) = 0.5 indicates a 50–50 chance
    that the event will occur, P(*e*) = 0 indicates that the event cannot occur, and
    P(*e*) = 1 indicates that it will always occur.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'We can approach probability in two ways:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '**Theoretical**: All events are equally likely to occur and the probability
    of the event (outcome) we’re interested in is as follows:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>e</mtext></mfenced><mo>=</mo><mfrac><mrow><mtext>N</mtext><mtext>umber of successful outcomes</mtext></mrow><mrow><mtext>T</mtext><mtext>otal number of outcomes</mtext></mrow></mfrac></mrow></mrow></math>](img/66.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: The theoretical probability of the two possible outcomes in the coin toss example
    is P(heads) = P(tails) = 1/2\. In the dice roll example, we have P(each side of
    the dice) = 1/6.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**Empirical**: This is the number of times an event, *e*, occurs in relation
    to all trials:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mi>e</mi></mfenced><mo>=</mo><mfrac><mrow><mtext>N</mtext><mtext>umber of times</mtext><mi>e</mi><mtext>occurs</mtext></mrow><mrow><mtext>T</mtext><mtext>otal number of trials</mtext></mrow></mfrac></mrow></mrow></math>](img/67.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: The empirical result of the experiment may show that the events aren’t equally
    likely. For example, if we toss a coin 100 times and observe heads 47 times, the
    empirical probability for heads is P(heads) = 47 / 100 = 0.47\. The law of large
    numbers tells us that we’ll calculate the probability more accurately with a higher
    number of trials.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll discuss probability in the context of sets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Probability and sets
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll introduce sets and their properties. We’ll also see
    how to apply these properties in probability theory. Let’s start with some definitions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample space**: The **set** (get it?) of all possible events (outcomes) of
    an experiment. We’ll denote it with a capital letter. Like Python, we’ll list
    all events in the sample space with {}. For example, the sample spaces of coin
    toss and dice roll events are ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi
    mathvariant="normal">S</mi><mi>c</mi></msub><mo>=</mo><mfenced open="{" close="}"><mrow><mtext>heads,</mtext><mtext>tails</mtext></mrow></mfenced></mrow></mrow></math>](img/68.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo><mml:mn>5,6</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/69.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo><mml:mn>5,6</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/70.png)
    respectively.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample point**: A single event (for example, tails) of the sample space.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event**: A single sample point or a combination (**subset**) of sample points
    of the sample space. For example, a combined event is for the dice to land on
    an odd number, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1,3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/71.png).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s assume that we have a set (sample space), S = {1, 2, 3, 4, 5}, and two
    subsets (combined events), A = {1, 2, 3} and B = {3, 4, 5}. We’ll use them to
    define the following set operations:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '**Intersection**: A set of elements that exist in both A and B:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>A</mml:mtext><mml:mo>∩</mml:mo><mml:mtext>B</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/72.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: If the intersection of A and B is an empty set {}, they are **disjoint**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**Complement**: A set of all elements that aren’t included in A or B:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mtext>A</mtext><mo>′</mo></mrow><mo>=</mo><mfenced
    open="{" close="}"><mn>4,5</mn></mfenced><mrow><mtext>B</mtext><mo>′</mo></mrow><mo>=</mo><mfenced
    open="{" close="}"><mn>1,2</mn></mfenced></mrow></mrow></math>](img/73.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: '**Union**: A set of all elements that exist in either A or B:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>A</mml:mtext><mml:mo>∪</mml:mo><mml:mtext>B</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/74.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'The following Venn diagrams illustrate these operations:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Venn diagrams of the possible set relationships](img/B19627_02_04.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Venn diagrams of the possible set relationships
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how to transfer the set operations in the probability domain.
    We’ll work with independent events – that is, the occurrence of one event doesn’t
    affect the probability of the occurrence of another. For example, the outcomes
    of the different coin tosses are independent of one another. With that, let’s
    define the set operations in terms of probability and events:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**The intersection of two events**: A set of sample points that exist in both
    events. The probability of the intersection is called **joint probability**:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P(A∩B)=P(A)</mml:mtext><mml:mtext>×</mml:mtext><mml:mtext>P(B)</mml:mtext></mml:math>](img/75.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Let’s say that we want to compute the probability of a card being simultaneously
    spades and an ace (more poetically, the ace of spades). The probability for spades
    is P(spades) = 13/52 = ¼, and the probability of an ace is P(ace) = 4/52 = 1/13\.
    The joint probability of the two is P(ace, spades) = (1/13) ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>×</mml:mo></mml:math>](img/76.png)
    (1/4) = 1/52\. We can intuitively validate this result because the ace of spades
    is a unique card, and its probability would be 1/52\. Since we draw a single card,
    the two events occur at the same time and are independent. Had they occurred successively
    – for example, two card draws, where one is a black and the other is an ace –
    we would fall in the domain of conditional probability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the occurrence of a single event, P(A), is also known as
    **marginal probability** (as opposed to joint probability).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Disjoint (or mutually exclusive) events**: Two or more events that don’t
    share any outcomes. In other words, their respective sample space subsets are
    disjoint. For example, the events of odd or even dice rolls are disjoint. The
    following is true for disjoint events:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The joint probability for these events to occur together is P(A![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∩</mo></mrow></math>](img/77.png)B)
    = 0
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the probabilities of disjoint events is ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mrow
    /><mrow /></msubsup><mrow><mtext>P</mtext><mfenced open="(" close=")"><mrow><mtext>disjoint</mtext><mtext>events</mtext></mrow></mfenced></mrow></mrow><mo>≤</mo><mn>1</mn></mrow></mrow></math>](img/78.png)
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jointly exhaustive events**: The subsets of such events contain the whole
    sample space between themselves. For example, events A = {1, 2, 3} and B = {4,
    5, 6} are jointly exhaustive because, together, they cover the whole sample space
    S = {1, 2, 3, 4, 5, 6}. The following is true for the probability of jointly exhaustive
    events:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><munderover><mo>∑</mo><mrow
    /><mrow /></munderover><mrow><mtext>P</mtext><mfenced open="(" close=")"><mtext>jointly exhaustive events</mtext></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/79.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: '**Complement events**: Two or more events that are disjoint and jointly exhaustive
    at the same time. For example, odd and even dice roll events are complementary.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Union of events**: A set of events coming from either A or B (not necessarily
    in both). The probability of this union is:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P(A</mml:mtext><mml:mtext>∪</mml:mtext><mml:mtext>B)=P(A)+P(B)-P(A∩B)</mml:mtext></mml:math>](img/80.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: So far, we have discussed independent events. Now, let’s see what happens if
    the events are not independent.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability and the Bayes rule
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If event A occurs before B and the occurrence of A changes the probability of
    the occurrence of B, then the two events are dependent. To understand this, let’s
    imagine that we draw consecutive cards from the deck. When the deck is full, the
    probability of drawing a spade is P(spade) = 13/52 = 0.25\. However, once we’ve
    drawn the first spade, the probability of picking a spade on the second turn changes.
    Now, we only have 51 cards and 1 less spade. The probability of the second draw
    is called **conditional probability**, P(B|A). This is the probability of event
    B (the second draw of a spade) to occur, given that event A (the first draw of
    a spade) has occurred. The probability of picking a spade on the second draw is
    P(spade2|spade1) = 12/51 = 0.235.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend the joint probability formula (introduced in the preceding section)
    for dependent events:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P(A∩B)=P(A)</mml:mtext><mml:mo>×</mml:mo><mml:mtext>P(B|A)</mml:mtext></mml:math>](img/81.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'However, this formula is just a special case for two events. We can extend
    it even further for multiple events, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/82.png).
    This new generic formula is known as the **chain rule** **of probability**:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>n</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:mo>…</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>n</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>n-1</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:mo>…</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>n-1</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:mo>…</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/83.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'For example, the chain rule for three events is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/84.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'We can use this property to derive the formula for the conditional probability
    itself:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P(B|A)</mtext><mo>=</mo><mfrac><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>A∩B</mtext></mfenced></mrow><mtext>P(A)</mtext></mfrac></mrow></mrow></math>](img/85.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Let’s discuss the intuition behind this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**P(A** **∩** **B)** indicates that we’re only interested in the occurrences
    of B, if A has already occurred – that is, we’re interested in the joint occurrence
    of the events, hence the joint probability.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P(A)** indicates that we’re interested only in the subset of outcomes when
    event A has occurred. We already know that A has occurred, and therefore, we restrict
    our observations to these outcomes.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is true for dependent events:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>A</mml:mtext><mml:mo>∩</mml:mo><mml:mtext>B</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow></mml:mfenced></mml:math>](img/86.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>A</mml:mtext><mml:mo>∩</mml:mo><mml:mtext>B</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>A</mml:mtext></mml:mrow><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow></mml:mfenced></mml:math>](img/87.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'We can use this rule to replace the value of P(A∩B) in the conditional probability
    formula to derive what is known as the **Bayes rule**:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>A∩B</mtext></mfenced><mtext>=P</mtext><mfenced open="("
    close=")"><mtext>A</mtext></mfenced><mo>×</mo><mtext>P</mtext><mfenced open="("
    close=")"><mrow><mtext>B</mtext><mo>|</mo><mtext>A</mtext></mrow></mfenced><mtext>=P</mtext><mfenced
    open="(" close=")"><mtext>B</mtext></mfenced><mo>×</mo><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>A</mtext><mo>|</mo><mtext>B</mtext></mrow></mfenced><mtext>⇔</mtext><mtext>  </mtext><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>B</mtext><mo>|</mo><mtext>A</mtext></mrow></mfenced><mtext>=</mtext><mfrac><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>A∩B</mtext></mfenced></mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>A</mtext></mfenced></mrow></mfrac><mtext>=</mtext><mfrac><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>B</mtext></mfenced><mo>×</mo><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>A</mtext><mo>|</mo><mtext>B</mtext></mrow></mfenced></mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>A</mtext></mfenced></mrow></mfrac></mrow></mrow></math>](img/88.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: The Bayes rule makes it possible to compute the conditional probability, P(B|A),
    if we know the opposite conditional probability, P(A|B). P(A) and P(B|A) are known
    as **prior probability and posterior** **probability**, respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We can illustrate the Bayes rule with a classic example from the realm of medical
    testing. A patient is administered a medical test for a disease, which comes out
    positive. Most tests have a sensitivity value, which is the percentage chance
    of the test being positive when administered to people with a particular disease.
    Using this information, we’ll apply the Bayes rule to compute the actual probability
    of the patient having the disease, given that the test is positive. We get the
    following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>has disease</mtext><mo>|</mo><mtext>test</mtext><mo>=</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mtext>has disease</mtext></mfenced><mo>×</mo><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>test</mtext><mo>=</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>|</mo><mtext>has disease</mtext></mrow></mfenced></mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><mtext>test</mtext><mo>=</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/89.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: We can think of P(has disease) as the probability of the disease in the general
    population.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’ll make some assumptions about the disease and the sensitivity of the
    test:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The test is 98% sensitive – that is, it will detect only 98% of all positive
    cases: P(test=positive|has disease) = 0.98'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two per cent of the people under 50 have this kind of disease: P(has disease)
    = 0.02'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test is positive for 3.9% of the population when administered to people
    under 50: P(test=positive) = 0.039'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can ask the following question: if a test is 98% sensitive and a 45-year-old
    person is administered the test, which turns out to be positive, what is the probability
    that they have the disease? We can calculate it with the Bayes rule:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>P</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>has disease</mml:mtext></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>has disease</mml:mtext></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>test</mml:mtext><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mtext>has disease</mml:mtext></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>P</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>test</mml:mtext><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>0.02</mml:mn><mml:mo>×</mml:mo><mml:mn>0.98</mml:mn></mml:mrow><mml:mrow><mml:mn>0.039</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/90.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: This example can serve as an introduction to the next section, where we’ll introduce
    the confusion matrix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **confusion matrix** is used to evaluate the performance of a binary classification
    algorithm, similar to the medical test we introduced in the *Conditional probability
    and the Bayes* *rule* section:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Confusion matrix](img/B19627_02_05.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Confusion matrix
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between the actual condition (**P** and **N**) and the predicted
    outcome (**PP** and **PN**) allows us to place the prediction in one of four categories:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive (TP)**: The actual and predicted values are both true'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative (TN)**: The actual and predicted values are both false'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive (FP)**: The actual value is negative, but the classification
    algorithm has predicted a positive value'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative (FN)**: The actual value is positive, but the algorithm has
    predicted a negative value'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these categories, we’ll introduce some measures that evaluate different
    aspects of the performance of the algorithm:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** = ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle
    scriptlevel="+1"><mfrac><mrow><mtext>TP</mtext><mtext>+</mtext><mtext>TN</mtext></mrow><mtext>TP + FP + FN+TN</mtext></mfrac></mstyle></mrow></math>](img/91.png):
    The fraction of correct predictions among all cases.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision** = ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP + FP</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/92.png):
    The fraction of positive predictions that were actually correct.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** (or **sensitivity**) = ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP + FN</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/93.png):
    The fraction of actual positive cases that were predicted correctly.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity** = ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TN + FP</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/94.png):
    The fraction of actual negative cases that were predicted correctly.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score** = ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mtext>precision</mml:mtext><mml:mo>×</mml:mo><mml:mtext>recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>precision+recall</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/95.png):
    Represents the balance between precision and recall. Because of the multiplication
    of the two measures, the F1 score will have a high value when both measures have
    high values.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we’ll discuss the field of differential calculus,
    which will help us to train NNs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Differential calculus
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can think of an ML algorithm as a mathematical function with inputs and
    parameters (which is the case for NNs). Our goal is to adjust these parameters
    in a way that will allow the ML function to closely approximate some other target
    function. To do this, we need to know how the output of the ML function changes
    when we change some of its parameters (called weights). Fortunately, differential
    calculus can help us here – it deals with the rate of change of a function with
    respect to a variable (parameter) that the function depends on. To understand
    how this works, we’ll start with a function, *f(x)*, with a single parameter,
    *x*, which has the following graph:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – A sample graph of a function, f(x), with a single parameter,
    x. The function graph is denoted with an uninterrupted blue line; the slope is
    denoted with an interrupted red line](img/B19627_02_06.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – A sample graph of a function, f(x), with a single parameter, x.
    The function graph is denoted with an uninterrupted blue line; the slope is denoted
    with an interrupted red line
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We can approximate how *f(x)* changes with respect to *x* for any value of
    *x* by calculating the slope of the function for that value. If the slope is positive,
    the function increases, and it decreases if the slope is negative. The steepness
    of the slope indicates the rate of change of the function for that value. We can
    calculate the slope with the following formula:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>slope</mtext><mo>=</mo><mfrac><mrow><mtext>Δ</mtext><mi>y</mi></mrow><mrow><mtext>Δ</mtext><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>+</mo><mtext>Δ</mtext><mi>x</mi></mrow></mfenced><mo>−</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow><mrow><mtext>Δ</mtext><mi>x</mi></mrow></mfrac></mrow></mrow></math>](img/96.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'The idea here is simple – we calculate the difference between the two values
    of *f* at *x* and *x+Δx* (*Δx* is a very small value) – *Δy = f(x + Δx) - f(x).*
    The ratio between *Δy* and *Δx* gives us the slope. But why is *Δx* required to
    be small? If *Δx* is too big, the part of the function graph between *x* and *x+Δx*
    may change significantly, and the slope measurement would be inaccurate. When
    *Δx* converges to 0, we’ll assume that our slope approximates the actual slope
    at a single point of the graph. In this case, we call the slope the **first derivative**
    of *f(x)*. We can express this in mathematical terms via the following equation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>′</mo></mrow><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mfrac><mrow><mi>d</mi><mi>y</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><munder><mi>lim</mi><mrow><mtext>Δ</mtext><mi>x</mi><mo>→</mo><mn>0</mn></mrow></munder><mfrac><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>+</mo><mtext>Δ</mtext><mi>x</mi></mrow></mfenced><mo>−</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow><mrow><mtext>Δ</mtext><mi>x</mi></mrow></mfrac></mrow></mrow></math>](img/97.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mi>lim</mi><mrow><mtext>Δ</mtext><mi>x</mi><mo>→</mo><mn>0</mn></mrow></munder></mrow></math>](img/98.png)is
    the mathematical concept of the limit (*Δx* approaches 0), and *f’(x)* and *dy/dx*
    are Lagrange’s and Leibniz’s notations for derivatives, respectively. The process
    of finding the derivative of *f* is called **differentiation**. The following
    diagram shows slopes at different values of *x*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Slopes at different values of x](img/B19627_02_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Slopes at different values of x
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The points, where *f* neither increases nor decreases as we change *x*, are
    called **saddle points**. The values of *f* at the saddle points are called the
    **local minimum** and the **local maximum**. Conversely, the slopes of *f* at
    the saddle points are 0.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed a function with a single parameter, *x*. Now, let’s
    focus on a function with multiple parameters, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/99.png).
    The derivative of *f* with respect to any of the parameters, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/100.png),
    is called a **partial derivative** and is denoted by ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>f</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:mi>x</mml:mi></mml:math>](img/101.png).
    To compute the partial derivative, we will assume that all the other parameters,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/102.png),
    are constants. We’ll denote the partial derivatives of the components of a vector
    with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∇</mml:mo><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/103.png).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s discuss some useful differentiation rules:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '**Chain rule**: *f* and *g* are functions and *h(x)= f(g(x))*. The derivative
    of *f* with respect to *x* for any *x* is as follows:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mtext>or</mml:mtext><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/104.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: '**Sum rule**: *f* and *g* are some functions and *h(x) = f(x) + g(x)*. The
    sum rule states the following:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>⟹</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/105.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: '**Common functions**:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/106.png)'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:math>](img/107.png),
    where *a* is a scalar'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/108.png),
    where *a* is a scalar'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi></mml:math>](img/109.png)'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:math>](img/110.png)'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: The mathematical foundations of NNs and NNs themselves form a kind of knowledge
    hierarchy. Think of the topics we discussed in *The math of NNs* section as the
    building blocks of NNs. They represent an important step toward a comprehensive
    understanding of NNs, which will help us throughout this book and beyond. Now,
    we have the necessary preparation to learn full-fledged NNs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to NNs
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can describe NNs as a mathematical model for information processing. As
    discussed in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016), this is a good way
    to describe any ML algorithm, but in this chapter, it has a specific meaning in
    the context of NNs. An NN is not a fixed program but rather a model, a system
    that processes information, or inputs. The characteristics of an NN are as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Information processing occurs in its simplest form, over simple elements called
    **units**
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Units are connected, and they exchange signals between them through connection
    links
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection links between units can be stronger or weaker, and this determines
    how information is processed
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each unit has an internal state that is determined by all the incoming connections
    from other units
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each unit has a different **activation function** that is calculated on its
    state and determines its output signal
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more general description of an NN would be as a computational graph of mathematical
    operations, but we will learn more about that later.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify two main characteristics of an NN:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural net architecture**: This describes the set of connections – namely,
    feedforward, recurrent, multi- or single-layered, and so on – between the units,
    the number of layers, and the number of units in each layer.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning**: This describes what is commonly defined as training. The most
    common but not exclusive way to train an NN is with **gradient descent** (**GD**)
    and **backpropagation** (**BP**).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll start our discussion from the smallest building block of the NN – the
    unit.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Units – the smallest NN building block
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Units are mathematical functions that can be defined as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/111.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Here, we do the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the weighted sum ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mrow
    /><mrow /></msubsup><msub><mi>x</mi><mi>i</mi></msub></mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/112.png)
    (also known as an activation value). Let’s focus on the components of this sum:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inputs ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png)
    are numerical values that represent either the outputs of other units of the network,
    or the values of the input data itself
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/114.png)
    are numerical values that represent either the strength of the inputs or the strength
    of the connections between the units
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The weight *b* is a special weight called **bias**, which represents an always-on
    input unit with a value of 1
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, we can substitute ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/15.png)
    with their vector representations, where x =  → x  = [x 1, x 2, … , x n] and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/117.png).
    Here, the formula will use the dot product of the two vectors:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>⋅</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/118.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: '2. The sum ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mrow
    /><mrow /></msubsup><msub><mi>x</mi><mi>i</mi></msub></mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/119.png)
    serves as input to the **activation function** *f* (also known as **transfer function**).
    The output of *f* is a single **numerical value**, which represents the output
    of the unit itself. The activation function has the following properties:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-linear**: *f* is the source of non-linearity in an NN – if the NN was
    entirely linear, it would only be able to approximate other linear functions'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differentiable**: This makes it possible to train the network with GD and
    BP'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t worry if you don’t understand everything – we’ll discuss activation functions
    in detail later in the chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram (left) shows a unit:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Left: a unit and its equivalent formula, and right: a geometric
    representation of a perceptron](img/B19627_02_08.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8 – Left: a unit and its equivalent formula, and right: a geometric
    representation of a perceptron'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The input vector **x** will be perpendicular to the weight vector **w** if ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/120.png).
    Therefore, all **x** vectors, where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⋅</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/121.png),
    define a hyperplane in vector space ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/122.png),
    where *n* is the dimension of **x**. In the case of two-dimensional input ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/123.png),
    we can represent the hyperplane as a line. This could be illustrated with the
    perceptron (or **binary classifier**) – a unit with a **threshold activation function**,
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mi>a</mi></mfenced><mo>=</mo><mn>1</mn><mtext>if</mtext><mi>a</mi><mo>≥</mo><mn>0</mn><mtext>else</mtext><mn>0</mn></mrow></mrow></math>](img/124.png)![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced open="("
    close=")"><mi>a</mi></mfenced><mo>=</mo><mn>1</mn><mtext>if</mtext><mi>a</mi><mo>≥</mo><mn>0</mn><mtext>else</mtext><mn>0</mn></mrow></mrow></math>](img/125.png),
    that classifies its input in one of the two classes. The geometric representation
    of the perceptron with two inputs ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/126.png)
    is a line (or **decision boundary**) separating the two classes (to the right
    in the preceding diagram).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can also see that the role of the bias, *b*, is
    to allow the hyperplane to shift away from the center of the coordinate system.
    If we don’t use bias, the unit will have limited representation power.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The unit is a generalization of several algorithms we introduced in [*Chapter
    1*](B19627_01.xhtml#_idTextAnchor016):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: A unit with an identity activation function *f(x) = x* is equivalent to **multiple**
    **linear regression**
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unit with a **sigmoid activation** function is equivalent to logistic regression
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unit with a threshold activation function is equivalent to a perceptron
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We already know from [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016) that the
    perceptron (hence the unit) only works with linearly separable classes, and now
    we know that is because it defines a hyperplane. This imposes a serious limitation
    on the unit because it cannot classify linearly inseparable problems – even simple
    ones such as **exclusive or** (**XOR**). To overcome this limitation, we’ll need
    to organize the units in an NN. However, before we discuss full-fledged NNs, we’ll
    focus on the next NN building block – the layers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Layers as operations
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An NN can have an indefinite number of units, which are organized in interconnected
    layers. A layer has the following properties:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: It combines the scalar outputs of multiple units in a single output vector.
    A unit can convey limited information because its output is a scalar. By combining
    the unit outputs, instead of a single activation, we can now consider the vector
    in its entirety. This way, we can convey a lot more information, not only because
    the vector has multiple values but also because the relative ratios between them
    carry additional meaning.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The units of one layer can be connected to the units of other layers, but not
    to other units of the same layer. Because of this, we can parallelize the computation
    of the outputs of all units in a single layer (thereby increasing the computational
    speed). This ability is one of the major reasons for the success of DL in recent
    years.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can generalize multivariate regression to a layer, as opposed to only linear
    or logistic regression to a single unit. In other words, we can approximate multiple
    values with a layer as opposed to a single value with a unit. This happens in
    the case of classification output, where each output unit represents the probability
    the input belongs to a certain class.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In classical NNs (that is, NNs before DL, when they were just one of many ML
    algorithms), the primary type of layer is the **fully connected** (**FC**) layer.
    In this layer, every unit receives weighted input from all the components of the
    input vector, **x**. This can represent either the output of another layer in
    the network or a sample of the input dataset. Let’s assume that the size of the
    input vector is *m*, and that the FC layer has *n* units and an activation function,
    *f*, which is the same for all the units. Each of the *n* units will have *m*
    weights – one for each of the *m* inputs. The following is a formula we can use
    for the output of a single unit, *j*, of an FC layer. It’s the same as the formula
    we defined in the *Units – the smallest NN building block* section, but we’ll
    include the unit index here:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/127.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/128.png)
    is the weight between the *j*-th layer unit and the *i*-th value of the input
    vector, **x**. We can represent the weights connecting the elements of **x** to
    the units as an *m×n* matrix, **W**. Each column of **W** represents the weight
    vector of all the inputs to a single unit of the layer. In this case, the output
    vector of the layer, **y**, is the result of matrix-vector multiplication.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: We can also combine multiple input sample vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/129.png),
    in an input matrix, **X**, where each input data vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/130.png),
    is represented by a row in **X**. The matrix itself is referred to as a **batch**.
    Then, we’ll simultaneously compute all output vectors, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/131.png),
    corresponding to the input samples, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/132.png).
    In this case, we will have matrix-matrix multiplication, **XW**, and the layer
    output is also a matrix, **Y**.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of an FC layer, as well as its equivalent
    formulas in the batch and single sample scenarios:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – An FC layer with vector/matrix inputs and outputs and its equivalent
    formulas](img/B19627_02_09.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – An FC layer with vector/matrix inputs and outputs and its equivalent
    formulas
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We have explicitly separated the bias and input weight matrices, but in practice,
    the underlying implementation may use a shared weight matrix and append an additional
    row of 1s to the input data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: So far, we represented the input data samples as vectors, which we can combine
    in a matrix. However, the input data can have more dimensions. For example, we
    can represent an RGB image with three dimensions – three two-dimensional channels
    (one channel for each color). To combine multiple images in a batch, we’ll need
    a fourth dimension. In such cases, we can use input/output **tensors** instead
    of matrices.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also use different types of layers to process multidimensional data.
    One such type is the convolutional layer, which we’ll discuss in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).
    We have many other layer types, such as attention, pooling, and so on. Some of
    the layers have trainable weights (FC, attention, convolutional), while others
    don’t (pooling). We can also use the terms functions or operations interchangeably
    with the layer. For example, in TensorFlow and PyTorch, the FC layer we just described
    is a combination of two sequential operations. First, we perform the weighted
    sum of the weights and inputs, and then we feed the result as an input to the
    activation function operation. In practice (that is, when working with DL libraries),
    the basic building block of an NN is not the unit but an operation that takes
    one or more tensors as input and outputs one or more tensors:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – A function (or operation) with input and output tensors](img/B19627_02_10.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – A function (or operation) with input and output tensors
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have all the necessary information to discuss NNs in their full
    glory.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer NNs
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have mentioned several times, single-layer neural nets can only classify
    linearly separable classes. However, there is nothing that prevents us from introducing
    more layers between the input and the output. These extra layers are called hidden
    layers. The following diagram demonstrates a three-layer fully connected NN with
    two hidden layers:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Multi-layer feed-forward network](img/B19627_02_11.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Multi-layer feed-forward network
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The input layer has *k* input units, the first hidden layer has *n* hidden units,
    and the second hidden layer has *m* hidden units. On top is the always-on bias
    unit. The output, in this example, is the two units, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/133.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/134.png),
    where each unit represents one of two possible classes. The output unit with the
    highest activation value will determine the NN’s class prediction for the given
    input sample. Each of the hidden units has a non-linear activation function, and
    the outputs have a special activation function called **softmax**, which we’ll
    discuss in the *Activation functions* section. A unit from one layer is connected
    to all units from the previous and following layers (hence fully connected). Each
    connection has its own weight, *w*, which is not depicted for reasons of simplicity.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in [*Chapter 1*](B19627_01.xhtml#_idTextAnchor016), we can think
    of the hidden layers as the NN’s internal representation of the input data. This
    is the way the NN understands the input sample with its own internal logic. However,
    this internal representation is non-interpretable by humans. To bridge the gap
    between the network’s representation and the actual data we’re interested in,
    we need the output layer. You can think of this as a translator; we use it to
    understand the network’s logic, and at the same time, we can convert it to the
    actual target values that we are interested in.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we are not limited to single-path networks with sequential layers,
    as shown in the preceding diagram. The layers (or operations in general) form
    **directed acyclic graphs**. In such a graph, the information cannot pass twice
    through the same layer (no loops) and it flows in only one direction, from the
    input to the output. The network in the preceding diagram is just a special case
    of a graph whose layers are connected sequentially. The following diagram also
    depicts a valid NN with two input layers, a single output layer, and randomly
    interconnected hidden layers. The layers are represented as operations ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/135.png)
    (*i* is an index that helps us differentiate between multiple operations):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – NN as a graph of operations](img/B19627_02_12.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – NN as a graph of operations
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: There is a special class of NNs called **recurrent networks**, which represent
    a **directed cyclic graph** (they can have loops). We’ll discuss them in detail
    in [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we introduced the most basic type of NN – that is, the unit
    – gradually expanded it to layers, and then generalized it as a graph of operations.
    We can also think of it in another way. The operations have precise mathematical
    definitions. Therefore, the NN, as a composition of functions is also a mathematical
    function, where the input data represents the function arguments, and the set
    of network weights, *θ* (a set of all weight matrices, **W**), are its parameters.
    We’ll denote it with either ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/136.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/137.png).
    Let’s assume that, when an operation receives input from more than one source
    (input data or other operations), we use the elementwise sum to combine the multiple
    input tensors. Then, we can represent the NN as a series of nested functions/operations.
    The equivalent formula for the feed-forward network on the left is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/138.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: Now that we’re familiar with the full NN architecture, let’s discuss the different
    types of activation functions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now know that multi-layer networks can classify linearly inseparable classes,
    but to do this, they need to satisfy one more condition. If the units don’t have
    activation functions, their output would be the weighted sum of the inputs, ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msubsup><mo>∑</mo><mrow
    /><mrow /></msubsup><msub><mi>x</mi><mi>i</mi></msub></mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/139.png),
    which is a linear function. Then, the entire NN – that is, a composition of units
    – becomes a composition of linear functions, which is also a linear function.
    This means that even if we add hidden layers, the network will still be equivalent
    to a simple linear regression model, with all its limitations. To turn the network
    into a non-linear function, we’ll use non-linear activation functions for the
    units. Usually, all units in the same layer have the same activation function,
    but different layers may have different activation functions. We’ll start with
    three popular activation functions. The first two are from the *classic* period
    of NNs, while the third is contemporary:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid**: Its output is bounded between 0 and 1 and can be interpreted stochastically
    as the probability of the unit being active. Because of these properties, the
    sigmoid was the most popular activation function for a long time. However, it
    also has some less desirable properties (more on that later), which led to its
    decline in use. The following diagram shows the sigmoid function, its derivative,
    and their graphs (the derivative will be useful when we discuss BP):'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.13 – The sigmoid activation function](img/B19627_02_13.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – The sigmoid activation function
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperbolic tangent (tanh)**: The name speaks for itself. The principal difference
    with the sigmoid is that the tanh is in the (-1, 1) range. The following diagram
    shows the tanh function, its derivative, and their graphs:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.14 – The hyperbolic tangent activation function](img/B19627_02_14.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – The hyperbolic tangent activation function
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit (ReLU)**: This is the new kid on the block (that is,
    compared to the *veterans*). ReLU was first successfully used in 2011 (see *Deep
    Sparse Rectifier Neural Networks* at [http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)).
    The following diagram shows the ReLU function, its derivative, and their graphs:'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – The ReLU activation function](img/B19627_02_15.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – The ReLU activation function
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the ReLU repeats its input when x > 0 and stays at 0 otherwise.
    This activation has several important advantages over sigmoid and tanh, which
    make it possible to train NNs with more hidden layers (that is, deeper networks).
    We’ll discuss these advantages, as well as other types of activation functions,
    in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll demonstrate how NNs can approximate any function.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The universal approximation theorem
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *Multi-layer NNs* section, we defined the NN as a function, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/140.png),
    where **x** is the input data (most often a vector or a tensor) and *θ* is the
    NN weights. Conversely, the training dataset is a collection of input samples
    and labels, which represents another, real-world, function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/141.png).
    The NN function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/142.png)
    **approximates** the function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/141.png):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/144.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: The universal approximation theorem states that any continuous function on compact
    subsets of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math>](img/122.png)
    can be approximated to an arbitrary degree of accuracy by a feedforward NN, with
    at least one hidden layer with a finite number of units and a non-linear activation.
    This is significant because it tells us that there are no theoretical insurmountable
    limitations in terms of NNs. In practice, an NN with a single hidden layer will
    perform poorly in many tasks, but at least we can aspire to a bright future with
    all-powerful NNs. We can understand the universal approximation theorem with an
    intuitive example.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: The idea for the following example was inspired by Michael A. Nielsen’s book
    *Neural Networks and Deep* *Learning* ([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll design an NN with one hidden layer that approximates the **boxcar function**
    (shown on the right in the following diagram). This is a type of step function,
    which is 0 across all input values, except in a narrow range, where it is equal
    to a constant value, *A*. A series of **translated** step functions can approximate
    any continuous function on a compact subset of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="double-struck">R</mml:mi></mml:math>](img/146.png),
    as shown in the left figure of the following diagram:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Left: a continuous function approximation with a series of
    step functions, and right: a single boxcar step function](img/B19627_02_16.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16 – Left: a continuous function approximation with a series of step
    functions, and right: a single boxcar step function'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start building our boxcar NN with a single unit, with a single scalar
    input, *x*, and sigmoid activation. The following figure shows the unit, as well
    as its equivalent formula:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – A unit with single input and sigmoid activation](img/B19627_02_17.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – A unit with single input and sigmoid activation
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagrams, we can see the unit output for different values
    of *b* and *w* with inputs, *x*, in the range [-10: 10]:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – The unit output based on diﬀerent values of w and b. The network
    input, x, is represented on the x axis](img/B19627_02_18.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – The unit output based on diﬀerent values of w and b. The network
    input, x, is represented on the x axis
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the weight, *w*, determines the steepness of the sigmoid function.
    We can also see that the formula, *t = -b/w*, determines the translation of the
    function along the *x* axis. Let’s cover the different graphs in the preceding
    diagram:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '**Top-left**: Regular sigmoid'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-right**: A large weight, *w*, amplifies the input, *x*, to a point where
    the sigmoid output resembles threshold activation'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bottom-left**: The bias, *b*, translates the unit activation along the *x*
    axis'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bottom-right**: We can simultaneously translate the activation along the
    *x* axis with the bias, *b*, and reverse the activation with a negative value
    of the weight, *w*'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can intuitively see that the unit can implement all the pieces of the box
    function. However, to create a full box function, we’ll have to combine two such
    units in an NN with one hidden layer. The following diagram shows the NN architecture,
    along with the weights and biases of the units, as well as the box function that’s
    produced by the network:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – A box function approximation NN](img/B19627_02_19.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – A box function approximation NN
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how it works:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>x</mi><mo><</mo><mo>-</mo><mn>5</mn></mrow></mrow></math>](img/147.png):
    the NN output is 0.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>≥</mml:mo></mml:math>](img/148.png)5:
    The top unit activates for the upper step of the function and stays active for
    all values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>5</mml:mn></mml:math>](img/149.png).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:math>](img/150.png)0:
    The bottom unit activates for the bottom step of the function and stays active
    for all ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>10</mml:mn></mml:math>](img/151.png).
    The outputs of the two hidden units cancel each other out because of the weights
    in the output layer, which are the same but with opposite signs.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of the output layer determine the constant value, *A = 5*, of the
    boxcar function.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of this network is 5 in the [-5:5] interval and 0 for all other inputs.
    We can approximate additional boxes by adding more units to the hidden layer in
    a similar manner.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re familiar with the structure of an NN, let’s focus on their training
    process.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Training NNs
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NN function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/152.png)
    **approximates** the function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/153.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/154.png).
    The goal of the training is to find parameters, *θ*, such that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/155.png)
    will best approximate ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/156.png).
    First, we’ll see how to do that for a'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: single-layer network, using an optimization algorithm called GD. Then, we’ll
    extend it to a deep feedforward network with the help of BP.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: We should note that an NN and its training algorithm are two separate things.
    This means we can adjust the weights of a network in some way other than GD and
    BP, but this is the most popular and efficient way to do so and is, ostensibly,
    the only way that is currently used in practice.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: GD
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the purposes of this section, we’ll train a simple NN using the **mean
    square error** (**MSE**) cost function. It measures the difference (known as **error**)
    between the network output and the training data labels of all training samples:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/157.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'At first, this might look scary, but fear not! Behind the scenes, it’s very
    simple and straightforward mathematics (I know that sounds even scarier!). Let’s
    discuss its components:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/158.png):
    The output of the NN, where *θ* is the set of all network weights. For the remainder
    of this section, we’ll denote the individual weights with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/159.png)
    (unlike the *w* notation in the rest of the sections).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n*: The total number of samples in the training set.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/132.png):
    The vector representation of a training sample, where the superscript *i* indicates
    the *i*-th sample of the dataset. We use superscript because ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/132.png)
    is a vector, and the subscript is reserved for each of the vector components.
    For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/162.png)
    is the *j*-th component of the *i*-th training sample.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/163.png):
    The label associated with the training sample, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/132.png).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will use MSE, but in practice, there are different types
    of cost functions. We’ll discuss them in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'First, GD computes the derivative (gradient) of *J(θ)* with respect to all
    the network weights. The gradient gives us an indication of how *J(θ)* changes
    with respect to each weight. Then, the algorithm uses this information to update
    the weights in a way that will minimize *J(θ)* in future occurrences of the same
    input/target pairs. The goal is to gradually reach the **global minimum** of the
    cost function, where the gradient is 0\. The following is a visualization of GD
    for MSE with respect to a single NN weight:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – An MSE diagram](img/B19627_02_20.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – An MSE diagram
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over the execution of GD step by step:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network weights, *θ*, with random values.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until the cost function, *J(θ)*, falls below a certain threshold:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forward pass**: Compute the MSE *J(θ)* cost function for all the samples
    of the training set using the preceding formula'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward pass**: Compute the partial derivatives (gradients) of *J(θ)* with
    respect to all the network weights ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png),
    using the chain rule:'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/166.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: 'Let’s analyze the partial derivative ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/167.png).
    *J* is a function of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/168.png)
    by being a function of the network output. Therefore, it is also a function of
    the NN function itself – that is, *J(f(θ))*. Then, by following the chain rule,
    we get the following: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mstyle
    scriptlevel="+1"><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mstyle><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced open="(" close=")"><mrow><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mstyle><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced open="(" close=")"><mrow><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow></mfenced></mrow><mrow><mo>∂</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow></mfrac></mstyle><mstyle scriptlevel="+1"><mfrac><mrow><mo>∂</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mstyle></mrow></mrow></math>](img/169.png)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Use these gradient values to update each of the network weights:'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/170.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: Here, *η* is the **learning rate**, which determines the step size at which
    the optimizer makes updates to the weights during training. Let’s note that as
    we move closer to the global minimum, the gradient will get smaller and we’ll
    update the weights in finer steps.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand how GD works, we’ll use linear regression as an example.
    Let’s recall that linear regression is equivalent to a single NN unit with an
    identity activation function, *f(x) =* *x*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression is represented by the function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/171.png),
    where *m* is the dimension of the input vector (equal to the number of weights)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we have the MSE cost function – ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/172.png)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we compute the partial derivative ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/173.png)
    with respect to a single network weight ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/174.png)
    using the chain rule and the sum rule:'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mrow><mn>2</mn><mi>n</mi></mrow></mfrac></mstyle><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mfenced
    open="(" close=")"><mrow><mfenced open="(" close=")"><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mi>x</mi><mi>j</mi><mfenced
    open="(" close=")"><mi>i</mi></mfenced></msubsup></mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfenced><mo>−</mo><msup><mi>t</mi><mfenced
    open="(" close=")"><mi>i</mi></mfenced></msup></mrow></mfenced><mn>2</mn></msup></mrow></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></math>](img/175.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>n</mi></mrow></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><mrow><mo>∂</mo><msup><mfenced
    open="(" close=")"><mrow><mfenced open="(" close=")"><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mi>x</mi><mi>j</mi><mfenced
    open="(" close=")"><mi>i</mi></mfenced></msubsup></mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfenced><mo>−</mo><msup><mi>t</mi><mfenced
    open="(" close=")"><mi>i</mi></mfenced></msup></mrow></mfenced><mn>2</mn></msup></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></mrow></math>](img/176.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/177.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math>](img/178.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Now that we have the gradient, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/179.png),
    we can update the weight ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/180.png)
    using the learning rate *η*
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we’ve discussed a GD that works with NNs with multiple weights. However,
    for the sake of simplicity, the preceding diagram illustrates the relationship
    between the cost function and a single-weight NN. Let’s remedy this by introducing
    a more complex cost function for an NN with two weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/181.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/182.png):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – The cost function J with respect to two weights](img/B19627_02_21.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – The cost function J with respect to two weights
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The function has a local and a global minimum. Nothing prevents GD from converging
    to the local minimum, instead of the global one, thus finding a sub-optimal approximation
    to the target function. We can try to mitigate this by increasing the learning
    rate *η*. The idea is that even if GD converges toward the local minimum, the
    larger *η* will help us *jump* over the saddle and converge toward the global
    maximum. The risk is that the opposite could happen – if GD correctly converges
    toward the global maximum, the larger learning rate could make it *jump* over
    to the local minimum instead.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: A more elegant way to prevent this issue is to use **momentum**. This extends
    vanilla GD by adjusting the current weight update with the values of the previous
    weight updates – that is, if the weight update at step t-1 was big, it will also
    increase the weight update of step t. We can explain momentum with an analogy.
    Think of the loss function surface as the surface of a hill. Now, imagine that
    we are holding a ball at the top of the hill (maximum). If we drop the ball, thanks
    to the Earth’s gravity, it will start rolling toward the bottom of the hill (minimum).
    The more distance it travels, the more its speed will increase. In other words,
    it will gain momentum (hence the name of the optimization).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at how to implement momentum in the weight update rule ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/183.png).
    We’ll assume that we are at step t of the training process:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll calculate the current weight update value ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/184.png)
    by also including the velocity of the previous update ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/185.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/186.png).
    Here, μ is a hyperparameter in the [0:1] range called the momentum rate. ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png)
    is initialized as 0 during the first iteration.'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we perform the actual weight update – ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/188.png).
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finding the best values of the learning rate *η* and the momentum rate *μ* is
    an empirical task. They can depend on the NN architecture, the type and size of
    a dataset, and other factors. In addition, we might have to adjust them during
    training. Since the NN weights are randomly initialized, we usually start with
    a larger *η* so that GD can quickly advance while the initial value of the cost
    function (error) is large. Once the decrease in the cost function starts plateauing,
    we can decrease the learning rate. In this way, GD can find minimums that would
    have been *jumped over* with the larger learning rate.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use an adaptive learning rate algorithm such as **Adam**
    (see *Adam: A Method for Stochastic Optimization* at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)).
    It calculates individual and adaptive learning rates for every weight, based on
    previous weight updates (momentum).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'The GD we just described is called **batch gradient descent** because it accumulates
    the error across *all* training samples and then performs a single weight update.
    This is fine for small datasets but could become impractical for large ones, as
    the training would take a long time with such sporadic updates. In practice, we
    would use two modifications:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic (or online) gradient descent** (**SGD**): Updates the weights
    after every training sample.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch gradient descent**: Accumulates the error over batches of *k*
    samples (called **mini-batches**) and performs one weight update after each mini-batch.
    It is a hybrid between online and batch GD. In practice, we’ll almost always use
    mini-batch GD over the other modifications.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step in our learning journey is to understand how to apply GD to train
    networks with more than one layer.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss how to combine GD with the BP algorithm to update
    the weights of networks with more than one layer. As we demonstrated in the *GD*
    section, this means finding the derivative of the cost function *J(θ)* with respect
    to each network weight. We already took a step in this direction with the help
    of the chain rule:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mi>θ</mi></mfenced></mrow></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mi>θ</mi></mfenced></mrow></mfenced></mrow><mrow><mo>∂</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>f</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></math>](img/189.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f(θ)* is the network output and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/190.png)
    is the *j*-th network weight. In this section, we’ll push the envelope further,
    and we’ll learn how to derive the NN function itself for all the network weights
    (hint – the chain rule). We’ll do this by propagating the error gradient backward
    through the network (hence the name). Let’s start with a few assumptions:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, we’ll work with a sequential feedforward NN. *Sequential*
    means that each layer takes input from the preceding layer and sends its output
    to the following layer.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll define ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/191.png)
    as the weight between the *i*-th unit of layer *l* and the *j*-th unit of the
    subsequent layer *l+1*. In a multi-layer network, *l* and *l+1* can be any two
    consecutive layers, including input, hidden, and output layers.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll denote the output of the *i*-th unit of layer *l* with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/192.png)
    and the output of the *j*-th unit of layer *l+1* with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/193.png).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll denote the input to the activation function (that is, the weighted sum
    of the inputs before activation) of unit *j* of layer *l*, with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/194.png).
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows all the notations we introduced:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Layer l represents the input, layer l+1 represents the output,
    and w connects the y activation in layer l to the inputs of the j-th unit of layer
    l+1](img/B19627_02_22.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – Layer l represents the input, layer l+1 represents the output,
    and w connects the y activation in layer l to the inputs of the j-th unit of layer
    l+1
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with this useful knowledge, let’s get down to business:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll assume that *l* and *l+1* are the second-to-last and the last
    (output) network layers, respectively. Knowing this, the derivative of *J* with
    respect to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/195.png)
    is as follows:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow></mrow></math>](img/196.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: 2. Let’s focus on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/197.png).
    Here, we compute the partial derivative of the weighted sum of the output of layer
    *l* with respect to one of the weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/198.png).
    As we discussed in the *Differential calculus* section, in partial derivatives,
    we’ll consider all the function parameters except ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/128.png)
    constants. When we derive ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/200.png),
    they all become 0, and we’re only left with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/201.png).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we get the following:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/202.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: '3. The formula from point 1 holds for any two consecutive hidden layers, *l*
    and *l+1*, of the network. We know that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/203.png),
    and we also know that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/204.png)
    is the derivative of the activation function, which we can calculate (see the
    *Activation functions* section). All we need to do is calculate the derivative
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/205.png)
    (recall that, here, *l+1* is a hidden layer). Let’s note that this is the derivative
    of the error with respect to the activation function in layer *l+1*. We can now
    calculate all the derivatives, starting from the last layer and moving backward,
    because the following apply:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate this derivative for the last layer
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a formula that allows us to calculate the derivative for one layer,
    assuming that we can calculate the derivative for the next
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4. With these points in mind, we get the following equation by applying the
    chain rule:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow></mrow></mrow></math>](img/206.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: The sum over j reflects the fact that, in the feedforward part of the network,
    the output ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/207.png)
    is fed to all the units in layer *l+1*. Therefore, they all contribute to ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/208.png)
    when the error is propagated backward.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: In the *Layers as operations* section, we discussed that in the forward pass,
    we can simultaneously compute all outputs of layer *l+1* as a matrix-matrix multiplication
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/209.png).
    Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/210.png)
    is the layer output of layer *l*, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/211.png)
    is the weight matrix between layers *l* and *l+1*. In the forward pass, one column
    of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/212.png)
    represents the weights from all units of the input layer *l* to a single unit
    of the output layer *l+1*. We can also represent the backward pass as a matrix-matrix
    multiplication by using the transpose weight matrix ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/213.png).
    A column of the transposed matrix represents the weights of all units of *l*,
    which contributed to a particular unit of *l+1* during the forward phase.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we can calculate ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/214.png).
    Using the same logic that we followed in *step 3*, we can compute that ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/215.png).
    Therefore, once we know ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/216.png),
    we can calculate ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/217.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/218.png).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Since we can calculate ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/219.png)
    for the last layer, we can move backward and calculate ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/220.png)
    for any layer and, therefore, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/221.png)
    for any layer.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '5. To summarize, let’s say we have a sequence of layers where the following
    applies:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/222.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following fundamental equations:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow></mrow></math>](img/196.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow></mrow></mrow></math>](img/206.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: By using these two equations, we can calculate the derivatives for the cost
    with respect to each layer.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '6. If we set ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math>](img/225.png),
    then ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/226.png)
    represents the variation in cost with respect to the activation value, and we
    can think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/227.png)
    as the error at unit ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/228.png).
    We can rewrite these equations as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow></mfrac><mfrac><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>j</mi><mfenced
    open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><msubsup><mi>δ</mi><mi>j</mi><mfenced open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mrow></math>](img/229.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: 'Following this, we can write the following equation:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msubsup><mi>δ</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup><mo>=</mo><mfenced open="(" close=")"><mrow><mrow><munderover><mo>∑</mo><mi>j</mi><mrow
    /></munderover><msubsup><mi>δ</mi><mi>j</mi><mfenced open="(" close=")"><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mfenced></msubsup></mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfenced><mfrac><mrow><mo>∂</mo><msubsup><mi>y</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow><mrow><mo>∂</mo><msubsup><mi>a</mi><mi>i</mi><mfenced
    open="(" close=")"><mi>l</mi></mfenced></msubsup></mrow></mfrac></mrow></mrow></math>](img/230.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: These two equations provide us with an alternative view of BP, since there is
    a variation in cost with respect to the activation value. They provide us with
    a way to calculate the variation for any layer *l* once we know the variation
    for the following layer, *l+1*.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '7. We can combine these equations to show the following:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/231.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: '8. The updated rule for the weights of each layer is given by the following
    equation:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math>](img/232.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
- en: Now that we’re familiar with GD and BP, let’s implement them in Python.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: A code example of an NN for the XOR function
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll create a simple network with one hidden layer, which
    solves the XOR function. Let’s recall that XOR is a linearly inseparable problem,
    hence the need for a hidden layer. The source code will allow you to easily modify
    the number of layers and the number of units per layer, so you can try a number
    of different scenarios. We won’t use any ML libraries. Instead, we’ll implement
    them from scratch, with only the help of `numpy`. We’ll also use `matplotlib`
    to visualize the results:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing these libraries:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we will define the activation function and its derivative. In this example,
    we will use `tanh(x)`:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we will start the definition of the `NeuralNetwork` class and its constructor
    (note that all its methods and properties have to be properly indented):'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `net_arch` is a one-dimensional array containing the number of units for
    each layer. For example `[2, 4, 1]` means an input layer with two units, a hidden
    layer with four units, and an output layer with one unit. Since we are studying
    the XOR function, the input layer will have two units, and the output layer will
    only have one unit. However, the number of hidden units can vary.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To conclude the constructor, we will initialize the network weights with random
    values in the range (-1, 1).
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we need to define the fit function, which will train our network:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will start the implementation by concatenating `bias` to the training `data`
    in a new variable, `input_data` (the source code is indented within the method
    definition):'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we’ll run the training for a number of `epochs`:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Within the loop, we’ll visualize the epoch number and the prediction output
    of the NN at the start of each epoch:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Within the loop, we select a random sample from the training set and propagate
    it forward through the hidden network layers:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Outside the loop, we will calculate the last layer output and the error:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we will propagate the error back (backward pass):'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we will update the weights based on the errors we just computed. We
    will multiply its output delta and input activation to get the gradient of the
    weight. Then, we will update the weight using the learning rate:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This concludes the implementation of the `fit` method.
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We’ll now write a `predict` function to check the results, which returns the
    network output for the given input:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we’ll write the `plot_decision_regions` method, which plots the hyperplane
    separating the classes (in our case, represented as lines), based on the input
    variables. We will create a two-dimensional grid with one axis for each input
    variable. We will plot the NN predictions for all input value combinations across
    the whole grid. We will take the network output larger than 0.5 to be `true` and
    `false` otherwise (we’ll see the plots at the end of the section):'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This concludes the implementation of the `NeuralNetwork` class.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can run the program with the following code:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We will build the default network, `nn = NeuralNetwork([2,2,1])`. The first
    and last values (`2` and `1`) represent the input and output layers and cannot
    be modified, but we can add different numbers of hidden layers with different
    numbers of units. For example, `([2,4,3,1])` will represent a three-layer NN,
    with four units in the first hidden layer and three units in the second hidden
    layer.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will use `numpy.random.seed(0)` to ensure that the weight initialization
    is consistent across runs, so we can compare their results. This is a common practice
    when training NNs.
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will define the training XOR data and labels in `x` and `y` respectively.
    We will run the training for 10 epochs. Finally, we will plot the result.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagrams, you can see how the `nn.plot_decision_regions` function
    method plots the hyperplanes, which separate the classes. The circles represent
    the network output for the `(true, true)` and `(false, false)` inputs, while the
    triangles represent the `(true, false)` and `(false, true)` inputs for the XOR
    function. To the left, we can see the hyperplane of an NN with two hidden units,
    and to the right, we can see an NN with four hidden units:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 – Left: the hyperplane learned by an NN with two hidden units,
    and right: the hyperplane of an NN with four hidden units](img/B19627_02_23.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23 – Left: the hyperplane learned by an NN with two hidden units,
    and right: the hyperplane of an NN with four hidden units'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Networks with different architectures can produce different separating regions.
    In the preceding figure, we can see that while the network finds the right solution,
    the curves separating the regions will be different, depending on the chosen architecture.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to start looking more closely at what deep neural nets are
    and their applications.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced NNs in detail, and we mentioned their success
    vis-à-vis other competing algorithms. NNs are comprised of interconnected units,
    where the weights of the connections characterize the strength of the communication
    between different units. We discussed different network architectures, how an
    NN can have many layers, and why inner (hidden) layers are important. We explained
    how information flows from the input to the output by passing from one layer to
    the next, based on weights and the activation function. Finally, we showed how
    to train NNs – that is, how to adjust their weights using GD and BP.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we’ll continue discussing deep NNs. We’ll explain
    in particular the meaning of *deep* in deep learning, and that it not only refers
    to the number of hidden layers in a network but to the quality of the learning
    of the network. For this purpose, we’ll show how NNs learn to recognize features
    and compile them as representations of larger objects. We’ll also describe a few
    important deep learning libraries and, finally, provide a concrete example of
    applying NNs to handwritten digit recognition.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
