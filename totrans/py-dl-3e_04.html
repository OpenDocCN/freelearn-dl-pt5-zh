<html><head></head><body>
<div id="_idContainer415">
<h1 class="chapter-number" id="_idParaDest-63" lang="en-GB"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-64" lang="en-GB"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.2.1">Computer Vision with Convolutional Networks</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.5.1"> and </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.7.1">, we set high expectations for </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">deep learning</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">DL</span></strong><span class="koboSpan" id="kobo.11.1">) and computer vision. </span><span class="koboSpan" id="kobo.11.2">First, we mentioned the ImageNet competition, and then we talked about some of its exciting real-world applications, such as semi-autonomous cars. </span><span class="koboSpan" id="kobo.11.3">In this chapter, and the next two chapters, we’ll deliver on </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">those expectations.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.13.1">Vision is arguably the most important human sense. </span><span class="koboSpan" id="kobo.13.2">We rely on it for almost any action we take. </span><span class="koboSpan" id="kobo.13.3">But image recognition has (and in some ways still is), for the longest time, been one of the most difficult problems in computer science. </span><span class="koboSpan" id="kobo.13.4">Historically, it’s been very difficult to explain to a machine what features make up a specified object, and how to detect them. </span><span class="koboSpan" id="kobo.13.5">But, as we’ve seen, in DL, a </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">neural network</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.16.1">NN</span></strong><span class="koboSpan" id="kobo.17.1">) can learn those features </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">by itself.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.19.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">following topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.21.1">Intuition and justification for </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">convolutional neural </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.23.1">networks</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.24.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.25.1">CNNs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">)</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.27.1">Convolutional layers</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">Pooling layers</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.29.1">The structure of a </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">convolutional network</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.31.1">Classifying images with PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">and Keras</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.33.1">Advanced types </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">of convolutions</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.35.1">Advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">CNN models</span></span></li>
</ul>
<h1 id="_idParaDest-65" lang="en-GB"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.37.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.38.1">We’ll implement the example in this chapter using Python, PyTorch, and Keras. </span><span class="koboSpan" id="kobo.38.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter Notebook on Google Colab. </span><span class="koboSpan" id="kobo.38.3">You can find the code examples in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04"><span class="No-Break"><span class="koboSpan" id="kobo.40.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.41.1">.</span></span></p>
<h1 id="_idParaDest-66" lang="en-GB"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.42.1">Intuition and justification for CNNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.43.1">The information we extract</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.44.1"> from sensory inputs is often determined by their context. </span><span class="koboSpan" id="kobo.44.2">With images, we can assume that nearby pixels are closely related, and their collective information is more relevant when taken as a unit. </span><span class="koboSpan" id="kobo.44.3">Conversely, we can assume that individual pixels don’t convey information related to each other. </span><span class="koboSpan" id="kobo.44.4">For example, to recognize letters or digits, we need to analyze the dependency of pixels close by because they determine the shape of the element. </span><span class="koboSpan" id="kobo.44.5">In this way, we could figure out the difference between, say, a 0 or a 1. </span><span class="koboSpan" id="kobo.44.6">The pixels in an image are organized in a two-dimensional grid, and if the image isn’t grayscale, we’ll have a third dimension for the </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">color channels.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.46.1">Alternatively, a </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">magnetic resonance image</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">MRI</span></strong><span class="koboSpan" id="kobo.50.1">) also uses three-dimensional space. </span><span class="koboSpan" id="kobo.50.2">You might recall</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.51.1"> that, until now, if we wanted to feed an image to an NN, we had to reshape it from a two-dimensional array into a one-dimensional array. </span><span class="koboSpan" id="kobo.51.2">CNNs are built to address this issue: how to make information about units that are closer more relevant than information coming from units that are further apart. </span><span class="koboSpan" id="kobo.51.3">In visual problems, this translates into making units process information coming from pixels that are near to one another. </span><span class="koboSpan" id="kobo.51.4">With CNNs, we’ll be able to feed one-, two-, or three-dimensional inputs and the network will produce an output of the same dimensionality. </span><span class="koboSpan" id="kobo.51.5">As we’ll see later, this will give us </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">several advantages.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.53.1">You may recall</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.54.1"> that at the end of the previous chapter, we successfully classified the MNIST images (with around 98% accuracy) using an NN of </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">fully connected</span></strong><span class="koboSpan" id="kobo.56.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.57.1">FC</span></strong><span class="koboSpan" id="kobo.58.1">) layers. </span><span class="koboSpan" id="kobo.58.2">Historically, MNIST classification has been an important benchmark for measuring the performance of new computer vision algorithms. </span><span class="koboSpan" id="kobo.58.3">But nowadays, it’s a toy dataset that we use for educational purposes, as in this book. </span><span class="koboSpan" id="kobo.58.4">Another such dataset is CIFAR-10 (</span><em class="italic"><span class="koboSpan" id="kobo.59.1">Canadian Institu</span><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.60.1">te For Advanced Research</span></em><span class="koboSpan" id="kobo.61.1">, </span><a href="https://www.cs.toronto.edu/~kriz/cifar.html"><span class="koboSpan" id="kobo.62.1">https://www.cs.toronto.edu/~kriz/cifar.html</span></a><span class="koboSpan" id="kobo.63.1">). </span><span class="koboSpan" id="kobo.63.2">It consists of 60,000 32×32 </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">red, green, blue</span></strong><span class="koboSpan" id="kobo.65.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.66.1">RGB</span></strong><span class="koboSpan" id="kobo.67.1">) images, divided into 10 classes of objects, namely </span><strong class="source-inline"><span class="koboSpan" id="kobo.68.1">airplane</span></strong><span class="koboSpan" id="kobo.69.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.70.1">automobile</span></strong><span class="koboSpan" id="kobo.71.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.72.1">bird</span></strong><span class="koboSpan" id="kobo.73.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">cat</span></strong><span class="koboSpan" id="kobo.75.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">deer</span></strong><span class="koboSpan" id="kobo.77.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.78.1">dog</span></strong><span class="koboSpan" id="kobo.79.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">frog</span></strong><span class="koboSpan" id="kobo.81.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">horse</span></strong><span class="koboSpan" id="kobo.83.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">ship</span></strong><span class="koboSpan" id="kobo.85.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.86.1">truck</span></strong><span class="koboSpan" id="kobo.87.1">. </span><span class="koboSpan" id="kobo.87.2">Had we tried to classify CIFAR-10 with</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.88.1"> an FC NN with one or more hidden layers, its validation accuracy would have been just around 50% (trust me, we did just that in the previous edition of this book). </span><span class="koboSpan" id="kobo.88.2">Compared to the MNIST result of nearly 98% accuracy, this is a dramatic difference, even though CIFAR-10 is also a toy problem. </span><span class="koboSpan" id="kobo.88.3">Therefore, FC NNs are of little practical use for computer</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.89.1"> vision problems. </span><span class="koboSpan" id="kobo.89.2">To understand why, let’s analyze the first hidden layer of our hypothetical CIFAR-10 network, which has 1,000 units. </span><span class="koboSpan" id="kobo.89.3">The input size of the image is </span><br/><span class="koboSpan" id="kobo.90.1">32 * 32 * 3 = 3,072. </span><span class="koboSpan" id="kobo.90.2">Therefore, the first hidden layer had a total of 2,072 * 1,000 = 2,072,000 weights. </span><span class="koboSpan" id="kobo.90.3">That’s no small number! </span><span class="koboSpan" id="kobo.90.4">Not only is it easy to overfit such a large network, but it’s also memory inefficient. </span><br/><span class="koboSpan" id="kobo.91.1">Even more important, each input unit (or pixel) is connected to every unit in the hidden layer. </span><span class="koboSpan" id="kobo.91.2">Because of this, the network cannot take advantage of the spatial proximity of the pixels since it doesn’t have a way of knowing which pixels are close to each other. </span><span class="koboSpan" id="kobo.91.3">In contrast, CNNs have properties that provide an effective solution to </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">these problems:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.93.1">They connect units that only correspond to neighboring pixels of the image. </span><span class="koboSpan" id="kobo.93.2">In this way, the units are “forced” to only take input from other units that are spatially close. </span><span class="koboSpan" id="kobo.93.3">This also reduces the number of weights since not all units </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">are interconnected.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.95.1">CNNs use parameter</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.96.1"> sharing. </span><span class="koboSpan" id="kobo.96.2">In other words, a limited number of weights are shared among all units in a layer. </span><span class="koboSpan" id="kobo.96.3">This further reduces the number of weights and helps fight overfitting. </span><span class="koboSpan" id="kobo.96.4">It might sound confusing, but it will become clear in the </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">next section.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.98.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.99.1">In this chapter, we’ll discuss CNNs in the context of computer vision, because computer vision is their most common application. </span><span class="koboSpan" id="kobo.99.2">However, CNNs are successfully applied in areas such as speech recognition and </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">natural language processing</span></strong><span class="koboSpan" id="kobo.101.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.102.1">NLP</span></strong><span class="koboSpan" id="kobo.103.1">). </span><span class="koboSpan" id="kobo.103.2">Many of the explanations we’ll describe here are also valid for those areas – that is, the principles of CNNs are the same regardless of the field </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">of use.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.105.1">To understand CNNs, we’ll first discuss their basic building blocks. </span><span class="koboSpan" id="kobo.105.2">Once we’ve done this, we’ll show you how to assemble them in a full-fledged NN. </span><span class="koboSpan" id="kobo.105.3">Then, we’ll demonstrate that such a network is good enough to classify the CIFAR-10 with high accuracy. </span><span class="koboSpan" id="kobo.105.4">Finally, we’ll discuss advanced CNN models, which can be applied to real-world computer </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">vision tasks.</span></span></p>
<h1 id="_idParaDest-67" lang="en-GB"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.107.1">Convolutional layers</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.108.1">The convolutional layer</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.109.1"> is the most important building block of a CNN. </span><span class="koboSpan" id="kobo.109.2">It consists of a set of </span><strong class="bold"><span class="koboSpan" id="kobo.110.1">filters</span></strong><span class="koboSpan" id="kobo.111.1"> (also known as </span><strong class="bold"><span class="koboSpan" id="kobo.112.1">kernels</span></strong><span class="koboSpan" id="kobo.113.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.114.1">feature detectors</span></strong><span class="koboSpan" id="kobo.115.1">), where each filter</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.116.1"> is applied across</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.117.1"> all areas</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.118.1"> of the input data. </span><span class="koboSpan" id="kobo.118.2">A filter is defined by a </span><strong class="bold"><span class="koboSpan" id="kobo.119.1">set of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.120.1">learnable weights</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.122.1">To add some meaning to this laconic definition, we’ll start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer353">
<span class="koboSpan" id="kobo.124.1"><img alt="Figure 4.1 – Convolution operation start" src="image/B19627_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.125.1">Figure 4.1 – Convolution operation start</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.126.1">The preceding figure shows a two-dimensional input layer of a CNN. </span><span class="koboSpan" id="kobo.126.2">For the sake of simplicity, we’ll assume that this is the input layer, but it can be any layer of the network. </span><span class="koboSpan" id="kobo.126.3">We’ll also assume that the input is a grayscale image, and each input unit represents the color intensity of a pixel. </span><span class="koboSpan" id="kobo.126.4">This image is represented by a </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">two-dimensional tensor.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.128.1">We’ll start the convolution by applying a 3×3 filter of weights (again, a two-dimensional tensor) in the top-left corner of the image. </span><span class="koboSpan" id="kobo.128.2">Each input unit is associated with a single weight of the filter. </span><span class="koboSpan" id="kobo.128.3">It has nine weights, because of the nine input units, but, in general, the size is arbitrary (2×2, 4×4, 5×5, and so on). </span><span class="koboSpan" id="kobo.128.4">The convolution operation is defined as the following </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">weighted sum:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.130.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/289.png" style="vertical-align:-0.841em;height:2.532em;width:14.059em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.131.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.132.1">row</span></em><span class="koboSpan" id="kobo.133.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.134.1">col</span></em><span class="koboSpan" id="kobo.135.1"> represent the input layer position, where we apply the filter (</span><em class="italic"><span class="koboSpan" id="kobo.136.1">row=1</span></em><span class="koboSpan" id="kobo.137.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.138.1">col=1</span></em><span class="koboSpan" id="kobo.139.1"> in the preceding figure); </span><span class="koboSpan" id="kobo.140.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/290.png" style="vertical-align:-0.340em;height:0.988em;width:0.775em"/></span><span class="koboSpan" id="kobo.141.1"> and </span><span class="koboSpan" id="kobo.142.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/291.png" style="vertical-align:-0.340em;height:0.988em;width:0.845em"/></span><span class="koboSpan" id="kobo.143.1"> are the height and width of the filter size (3×3); </span><em class="italic"><span class="koboSpan" id="kobo.144.1">i</span></em><span class="koboSpan" id="kobo.145.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.146.1">j</span></em><span class="koboSpan" id="kobo.147.1"> are the filter indices</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.148.1"> of each filter weight, </span><span class="koboSpan" id="kobo.149.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/292.png" style="vertical-align:-0.483em;height:0.931em;width:0.962em"/></span><span class="koboSpan" id="kobo.150.1">; </span><em class="italic"><span class="koboSpan" id="kobo.151.1">b</span></em><span class="koboSpan" id="kobo.152.1"> is the bias weight. </span><span class="koboSpan" id="kobo.152.2">The group</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.153.1"> of units, </span><span class="koboSpan" id="kobo.154.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/293.png" style="vertical-align:-0.483em;height:0.931em;width:4.234em"/></span><span class="koboSpan" id="kobo.155.1">, which participates in the input, is called the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.156.1">receptive field</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.158.1">We can see that</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.159.1"> in a convolutional layer, the unit activation value is defined in the same way as the activation value of the unit we defined in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.160.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.161.1"> – that is, a weighted sum of its inputs. </span><span class="koboSpan" id="kobo.161.2">But here, the unit takes input only from a limited number of input units in its immediate surroundings (the receptive field). </span><span class="koboSpan" id="kobo.161.3">This is opposed to an FC layer, where the input comes from all input units. </span><span class="koboSpan" id="kobo.161.4">The difference matters because the purpose of the filter is to highlight a specific feature in the input, for example, an edge or a line in an image. </span><span class="koboSpan" id="kobo.161.5">In the context of the NN, the filter output represents the activation value of a unit in the next layer. </span><span class="koboSpan" id="kobo.161.6">The unit will be active if the feature is present at this spatial location. </span><span class="koboSpan" id="kobo.161.7">In hierarchically structured data, such as images, neighboring pixels form meaningful shapes and objects such as an edge or a line. </span><span class="koboSpan" id="kobo.161.8">However, a pixel at one end of the image is unlikely to have a relationship with a pixel at another end. </span><span class="koboSpan" id="kobo.161.9">Because of this, using an FC layer to connect all of the input pixels with each output unit is like asking the network to find a needle in a haystack. </span><span class="koboSpan" id="kobo.161.10">It has no way of knowing whether an input pixel is relevant (in the immediate surroundings) to the output unit or not (the other end of the image). </span><span class="koboSpan" id="kobo.161.11">Therefore, the limited receptive field of the convolutional layer is better suited to highlight meaningful features in the </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">input data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.163.1">We’ve calculated the activation of a single unit, but what about the others? </span><span class="koboSpan" id="kobo.163.2">It’s simple! </span><span class="koboSpan" id="kobo.163.3">For each new unit, we’ll slide the filter across the input image, and we’ll compute its output (the weighted sum) with each new set of input units. </span><span class="koboSpan" id="kobo.163.4">The following diagram shows how to compute the activations of the next two positions (one pixel to </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the right):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer359">
<span class="koboSpan" id="kobo.165.1"><img alt="Figure 4.2 – The first three steps of a convolution operation" src="image/B19627_04_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.166.1">Figure 4.2 – The first three steps of a convolution operation</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.167.1">By “slide,” we mean that the weights</span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.168.1"> of the filter don’t change across the image. </span><span class="koboSpan" id="kobo.168.2">In effect, we’ll use the same nine filter weights and the single bias weight to compute</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.169.1"> the activations of all output units, each time with a different set of inputs. </span><span class="koboSpan" id="kobo.169.2">We call this </span><strong class="bold"><span class="koboSpan" id="kobo.170.1">parameter sharing</span></strong><span class="koboSpan" id="kobo.171.1">, and we do it for </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">two reasons:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.173.1">By reducing the number of weights, we reduce the memory footprint and </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">prevent overfitting.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.175.1">The filter highlights a specific visual feature in the image. </span><span class="koboSpan" id="kobo.175.2">We can assume that this feature is useful, regardless of its position on the image. </span><span class="koboSpan" id="kobo.175.3">Since we apply the same filter throughout the image, the convolution is translation invariant; that is, it can detect the same feature, regardless of its location on the image. </span><span class="koboSpan" id="kobo.175.4">However, the convolution is neither rotation-invariant (it is not guaranteed to detect a feature if it’s rotated) nor scale-invariant (it is not guaranteed to detect the same artifact in </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">different scales).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.177.1">To compute all output activations, we’ll repeat the sliding process until we’ve covered the whole</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.178.1"> input. </span><span class="koboSpan" id="kobo.178.2">The spatially arranged input and output units are called </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">depth slices</span></strong><span class="koboSpan" id="kobo.180.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.181.1">feature maps</span></strong><span class="koboSpan" id="kobo.182.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.183.1">channels</span></strong><span class="koboSpan" id="kobo.184.1">), implying that there is more than one slice. </span><span class="koboSpan" id="kobo.184.2">The slices, like</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.185.1"> the image, are represented</span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.186.1"> by tensors. </span><span class="koboSpan" id="kobo.186.2">A slice tensor can serve as an input to other layers in the network. </span><span class="koboSpan" id="kobo.186.3">Finally, just as with</span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.187.1"> regular layers, we can use an activation function, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.188.1">rectified linear unit</span></strong><span class="koboSpan" id="kobo.189.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.190.1">ReLU</span></strong><span class="koboSpan" id="kobo.191.1">), after </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">each unit.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.193.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.194.1">It’s interesting to note that each input unit is part of the input of multiple output units. </span><span class="koboSpan" id="kobo.194.2">For example, as we slide the filter, the green unit in the preceding diagram will form the input of nine </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">output units.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.196.1">We can illustrate what we’ve learned so far with a simple example. </span><span class="koboSpan" id="kobo.196.2">The following diagram illustrates a 2D convolution with a 2×2 filter applied over a single </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">3×3 slice:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer360">
<span class="koboSpan" id="kobo.198.1"><img alt="Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3 slice for a 2×2 output slice" src="image/B19627_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.199.1">Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3 slice for a 2×2 output slice</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.200.1">This example also shows us that the input and output feature maps have different dimensions. </span><span class="koboSpan" id="kobo.200.2">Let’s say we have an input layer with a size of </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">(width_i, height_i)</span></strong><span class="koboSpan" id="kobo.202.1"> and a filter with dimensions, </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">(filter_w, filter_h)</span></strong><span class="koboSpan" id="kobo.204.1">. </span><span class="koboSpan" id="kobo.204.2">After applying the convolution, the dimensions</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.205.1"> of the output layer are  </span><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">width_o = width_i - filter_w + 1</span></strong><span class="koboSpan" id="kobo.207.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">height_o = height_i - filter_h + 1</span></strong><span class="koboSpan" id="kobo.209.1">. </span><br/><span class="koboSpan" id="kobo.210.1">In this example, we have </span><strong class="source-inline"><span class="koboSpan" id="kobo.211.1">width_o = height_o = 3 – 2 + 1 = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.214.1">In the next section, we’ll illustrate convolutions with a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">coding example.</span></span></p>
<h2 id="_idParaDest-68" lang="en-GB"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.216.1">A coding example of the convolution operation</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.217.1">We’ve now described</span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.218.1"> how convolutional layers work, but we’ll gain better intuition with a visual example. </span><span class="koboSpan" id="kobo.218.2">Let’s implement a convolution operation by applying a couple of filters across an image. </span><span class="koboSpan" id="kobo.218.3">For the sake of clarity, we’ll implement the sliding of the filters across the image manually and we won’t use any DL libraries. </span><span class="koboSpan" id="kobo.218.4">We’ll only include the relevant parts and not the full program, but you can find the full example in this book’s GitHub repository. </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.220.1">Import </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">numpy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.223.1">
import numpy as np</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.224.1">Define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">conv</span></strong><span class="koboSpan" id="kobo.226.1"> function, which applies the convolution across the image. </span><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">conv</span></strong><span class="koboSpan" id="kobo.228.1"> takes two parameters, both two-dimensional numpy arrays: </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">image</span></strong><span class="koboSpan" id="kobo.230.1">, for the pixel intensities of the grayscale image itself, and the hardcoded </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">im_filter</span></strong><span class="koboSpan" id="kobo.232.1">, for </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">the filter:</span></span><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.234.1">First, we’ll compute the output image size, which depends on the input </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">image</span></strong><span class="koboSpan" id="kobo.236.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">im_filter</span></strong><span class="koboSpan" id="kobo.238.1"> sizes. </span><span class="koboSpan" id="kobo.238.2">We’ll use it to instantiate the output </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">image, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">im_c</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.242.1">Then, we’ll iterate over all pixels of </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">image</span></strong><span class="koboSpan" id="kobo.244.1">, applying </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">im_filter</span></strong><span class="koboSpan" id="kobo.246.1"> at each location. </span><span class="koboSpan" id="kobo.246.2">This operation requires four nested loops: the first two for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">image</span></strong><span class="koboSpan" id="kobo.248.1"> dimensions and the second two for iterating over the </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">two-dimensional filter.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.250.1">We’ll check if any value is out of the [0, 255] interval and fix it, </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">if necessary.</span></span></li></ol><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.252.1">This is shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">following example:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.254.1">
def conv(image, im_filter):
    # input dimensions
    height = image.shape[0]
    width = image.shape[1]
    # output image with reduced dimensions
    im_c = np.zeros((height - len(im_filter) + 1,
        width - len(im_filter) + 1))
    # iterate over all rows and columns
    for row in range(len(im_c)):
        for col in range(len(im_c[0])):
            # apply the filter
            for i in range(len(im_filter)):
                for j in range(len(im_filter[0])):
                    im_c[row, col] += image[row + i, /
                        col + j] * im_fi</span><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.255.1">lter[i][j]
    # fix out-of-bounds values
    im_c[im_c &gt; 255] = 255
    im_c[im_c &lt; 0] = 0
    return im_c</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.256.1">Apply different filters</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.257.1"> across the image. </span><span class="koboSpan" id="kobo.257.2">To better illustrate our point, we’ll use a 10×10 blur filter, as well as Sobel edge detectors, as shown in the following example (</span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">image_grayscale</span></strong><span class="koboSpan" id="kobo.259.1"> is the two-dimensional </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">numpy</span></strong><span class="koboSpan" id="kobo.261.1"> array, which represents the pixel intensities of a </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">grayscale image):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.263.1">
# blur filter
blur = np.full([10, 10], 1. </span><span class="koboSpan" id="kobo.263.2">/ 100)
conv(image_grayscale, blur)
# sobel filters
sobel_x = [[-1, -2, -1],
           [0, 0, 0],
           [1, 2, 1]]
conv(image_grayscale, sobel_x)
sobel_y = [[-1, 0, 1],
           [-2, 0, 2],
           [-1, 0, 1]]
conv(image_grayscale, sobel_y)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.264.1">The full program will produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">following output:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer361">
<span class="koboSpan" id="kobo.266.1"><img alt="Figure 4.4 – The ﬁrst image is the grayscale input. The second image is the result of a 10×10 blur ﬁlter. The third and fourth images use detectors and vertical Sobel edge detectors" src="image/B19627_04_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.267.1">Figure 4.4 – The ﬁrst image is the grayscale input. </span><span class="koboSpan" id="kobo.267.2">The second image is the result of a 10×10 blur ﬁlter. </span><span class="koboSpan" id="kobo.267.3">The third and fourth images use detectors and vertical Sobel edge detectors</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.268.1">In this example, we used filters with hardcoded weights to visualize how the convolution operation works in NNs. </span><span class="koboSpan" id="kobo.268.2">In reality, the weights of the filter will be set during the network’s training. </span><span class="koboSpan" id="kobo.268.3">All we’ll need to do is define the network architecture, such as the number of convolutional layers, the depth of the output volume, and the size of the filters. </span><span class="koboSpan" id="kobo.268.4">The network will figure out the features highlighted</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.269.1"> by each filter </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">during training.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.271.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.272.1">As we saw in this example, we had to implement four nested loops to implement the convolution. </span><span class="koboSpan" id="kobo.272.2">However, with some clever transformations, the convolution operation can be implemented with matrix-matrix multiplication. </span><span class="koboSpan" id="kobo.272.3">In this way, it can take full advantage of </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">GPU parallelization.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.274.1">In the next few sections, we’ll discuss some of the finer details of the </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">convolutional layers.</span></span></p>
<h2 id="_idParaDest-69" lang="en-GB"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.276.1">Cross-channel and depthwise convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.277.1">So far, we have described the one-to-one slice relation, where we apply a single filter over a single input slice to produce a single output slice. </span><span class="koboSpan" id="kobo.277.2">But this arrangement is limiting for the </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">following reasons:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.279.1">A single input slice</span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.280.1"> works well for a grayscale image, but it doesn’t work for color images with multiple channels or any other </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">multi-dimensional input</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.282.1">A single filter </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.283.1">can detect a single feature in the slice, but we are interested in detecting many </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">different features</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.285.1">How do we solve these limitations? </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">It’s simple:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.287.1">For the input, we’ll split the image into color channels. </span><span class="koboSpan" id="kobo.287.2">In the case of an RGB image, that would be three. </span><span class="koboSpan" id="kobo.287.3">We can think of each color channel as a depth slice, where the values are the pixel intensities for the given color (R, G, or B), as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">following example:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer362">
<span class="koboSpan" id="kobo.289.1"><img alt="Figure 4.5 – An example of an input slice with a depth of 3" src="image/B19627_04_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.290.1">Figure 4.5 – An example of an input slice with a depth of 3</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.291.1">The combination of input slices is called </span><strong class="bold"><span class="koboSpan" id="kobo.292.1">input volume</span></strong><span class="koboSpan" id="kobo.293.1"> with a </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">depth</span></strong><span class="koboSpan" id="kobo.295.1"> of 3. </span><span class="koboSpan" id="kobo.295.2">An RGB image is represented</span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.296.1"> by a 3D tensor of three 2D slices (one slice per </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">color channel).</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.298.1">The CNN convolution can have multiple filters, highlighting different features, which results in multiple</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.299.1"> output feature maps (one for each filter), combined in an </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.300.1">output volume</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.302.1">Let’s say we have </span><span class="koboSpan" id="kobo.303.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/294.png" style="vertical-align:-0.340em;height:1.004em;width:1.058em"/></span><span class="koboSpan" id="kobo.304.1"> input (uppercase </span><em class="italic"><span class="koboSpan" id="kobo.305.1">C</span></em><span class="koboSpan" id="kobo.306.1">) and </span><span class="koboSpan" id="kobo.307.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/295.png" style="vertical-align:-0.340em;height:1.004em;width:1.409em"/></span><span class="koboSpan" id="kobo.308.1"> output slices. </span><span class="koboSpan" id="kobo.308.2">Depending on the relationship of the input and output slice, we get cross-channel and depthwise convolutions, as illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer365">
<span class="koboSpan" id="kobo.310.1"><img alt="Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)" src="image/B19627_04_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.311.1">Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.312.1">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">their properties:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.314.1">Cross-channel convolutions</span></strong><span class="koboSpan" id="kobo.315.1">: One output slice receives input</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.316.1"> from all input slices (</span><span class="koboSpan" id="kobo.317.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/296.png" style="vertical-align:-0.340em;height:1.004em;width:5.173em"/></span><span class="koboSpan" id="kobo.318.1"> relationship). </span><span class="koboSpan" id="kobo.318.2">With multiple</span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.319.1"> output slices, the relationship becomes </span><span class="koboSpan" id="kobo.320.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/297.png" style="vertical-align:-0.340em;height:1.004em;width:5.475em"/></span><span class="koboSpan" id="kobo.321.1">. </span><span class="koboSpan" id="kobo.321.2">In other words, each input slice contributes to the output of each output slice. </span><span class="koboSpan" id="kobo.321.3">Each pair of input/output slices uses a separate filter slice that’s unique to that pair. </span><span class="koboSpan" id="kobo.321.4">Let’s denote the index of the input slice with </span><span class="koboSpan" id="kobo.322.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/298.png" style="vertical-align:-0.340em;height:0.788em;width:0.856em"/></span><span class="koboSpan" id="kobo.323.1"> (lowercase </span><em class="italic"><span class="koboSpan" id="kobo.324.1">c</span></em><span class="koboSpan" id="kobo.325.1">); the index of the output slice with </span><span class="koboSpan" id="kobo.326.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/299.png" style="vertical-align:-0.340em;height:0.788em;width:1.143em"/></span><span class="koboSpan" id="kobo.327.1">; the dimensions of the filter with </span><span class="koboSpan" id="kobo.328.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/300.png" style="vertical-align:-0.340em;height:0.988em;width:0.829em"/></span><span class="koboSpan" id="kobo.329.1"> and </span><span class="koboSpan" id="kobo.330.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/301.png" style="vertical-align:-0.340em;height:0.988em;width:0.866em"/></span><span class="koboSpan" id="kobo.331.1">. </span><span class="koboSpan" id="kobo.331.2">Then, the cross-channel 2D convolution of a single output cell in one of the output slices is defined as the following </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">weighted sum:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.333.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/302.png" style="vertical-align:-0.956em;height:2.719em;width:18.644em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.334.1">Note that we have a unique bias, </span><span class="koboSpan" id="kobo.335.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/303.png" style="vertical-align:-0.532em;height:1.243em;width:1.306em"/></span><span class="koboSpan" id="kobo.336.1"> for each </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">output slice.</span></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.338.1">We can also compute the total number of weights, </span><em class="italic"><span class="koboSpan" id="kobo.339.1">W</span></em><span class="koboSpan" id="kobo.340.1">, in a cross-channel 2D convolution with the </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">following equation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.342.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/304.png" style="vertical-align:-0.390em;height:1.102em;width:12.044em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.343.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.344.1">+1</span></em><span class="koboSpan" id="kobo.345.1"> represents the bias weight for each filter. </span><span class="koboSpan" id="kobo.345.2">Let’s say we have three input slices and want to apply four 5×5 filters to them. </span><span class="koboSpan" id="kobo.345.3">If we did this, the convolution filter would have a total of (3 * 5 * 5 + 1) * 4 = 304 weights, four output slices (an output volume with a depth of 4), and one bias per slice. </span><span class="koboSpan" id="kobo.345.4">The filter for each output slice will have three 5×5 filter patches</span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.346.1"> for each of the three input slices</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.347.1"> and one bias for a total of 3 * 5 * 5 + 1 = </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">76 weights.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.349.1">Depthwise convolutions</span></strong><span class="koboSpan" id="kobo.350.1">: One output slice receives input from a single input slice. </span><span class="koboSpan" id="kobo.350.2">It’s a kind of reversal</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.351.1"> of the previous case. </span><span class="koboSpan" id="kobo.351.2">In its simplest form, we apply a filter</span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.352.1"> over a single input slice to produce a single output slice. </span><span class="koboSpan" id="kobo.352.2">In this case, the input and output volumes have the same depth – that is, </span><em class="italic"><span class="koboSpan" id="kobo.353.1">C</span></em><span class="koboSpan" id="kobo.354.1">. </span><span class="koboSpan" id="kobo.354.2">We can also specify a channel multiplier (an integer, </span><em class="italic"><span class="koboSpan" id="kobo.355.1">M</span></em><span class="koboSpan" id="kobo.356.1">), where we apply </span><em class="italic"><span class="koboSpan" id="kobo.357.1">M</span></em><span class="koboSpan" id="kobo.358.1"> filters over a single output slice to produce </span><em class="italic"><span class="koboSpan" id="kobo.359.1">M</span></em><span class="koboSpan" id="kobo.360.1"> output slices per input slice. </span><span class="koboSpan" id="kobo.360.2">In this case, the total number of output slices is </span><span class="koboSpan" id="kobo.361.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/305.png" style="vertical-align:-0.340em;height:1.004em;width:3.015em"/></span><span class="koboSpan" id="kobo.362.1">. </span><span class="koboSpan" id="kobo.362.2">The depthwise 2D convolution is defined as the following </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">weighted sum:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.364.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/306.png" style="vertical-align:-0.768em;height:2.532em;width:18.252em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.365.1">We can compute the number of weights, </span><em class="italic"><span class="koboSpan" id="kobo.366.1">W</span></em><span class="koboSpan" id="kobo.367.1">, in a 2D depthwise convolution with the </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.369.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/307.png" style="vertical-align:-0.390em;height:1.087em;width:11.549em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.370.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.371.1">+M</span></em><span class="koboSpan" id="kobo.372.1"> represents</span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.373.1"> the biases of each </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">output</span></span><span class="No-Break"><a id="_idIndexMarker476"/></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1"> slice.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.376.1">Next, we’ll discuss some more properties of the </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">convolution operation.</span></span></p>
<h2 id="_idParaDest-70" lang="en-GB"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.378.1">Stride and padding in convolutional layers</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.379.1">So far, we’ve assumed that sliding of the filter happens one pixel at a time, but that’s not always the case. </span><span class="koboSpan" id="kobo.379.2">We can slide the</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.380.1"> filter over multiple positions. </span><span class="koboSpan" id="kobo.380.2">This parameter</span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.381.1"> of the convolutional layers is called </span><strong class="bold"><span class="koboSpan" id="kobo.382.1">stride</span></strong><span class="koboSpan" id="kobo.383.1">. </span><span class="koboSpan" id="kobo.383.2">Usually, the stride is the same across all dimensions of the input. </span><span class="koboSpan" id="kobo.383.3">In the following diagram, we can see a convolutional layer with </span><em class="italic"><span class="koboSpan" id="kobo.384.1">stride = 2</span></em><span class="koboSpan" id="kobo.385.1"> (also called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.386.1">stride convolution</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer378">
<span class="koboSpan" id="kobo.388.1"><img alt="Figure 4.7 – With stride = 2, the ﬁlter is translated by two pixels at a time" src="image/B19627_04_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.389.1">Figure 4.7 – With </span><em class="italic"><span class="koboSpan" id="kobo.390.1">stride = 2</span></em><span class="koboSpan" id="kobo.391.1">, the ﬁlter is translated by two pixels at a time</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.392.1">The main effect of the larger stride is an increase in the receptive field of the output units at the expense of the size of the output slice itself. </span><span class="koboSpan" id="kobo.392.2">To understand this, let’s recall that in the previous section, we introduced a simple formula for the output size, which included the sizes of the input and the kernel. </span><span class="koboSpan" id="kobo.392.3">Now, we’ll extend it to also include the stride: </span><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">width_o = (width_i - filter_w) / str</span><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.394.1">ide_w + 1</span></strong><span class="koboSpan" id="kobo.395.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">height_o = 1 + (height_i - filter_h) / stride_h</span></strong><span class="koboSpan" id="kobo.397.1">. </span><span class="koboSpan" id="kobo.397.2">For example, the output size of a square slice generated by a 28×28 input image, convolved with a 3×3 filter with </span><em class="italic"><span class="koboSpan" id="kobo.398.1">stride = 1</span></em><span class="koboSpan" id="kobo.399.1">, would be 1 + 28 - 3 = 26. </span><span class="koboSpan" id="kobo.399.2">But with </span><em class="italic"><span class="koboSpan" id="kobo.400.1">stride = 2</span></em><span class="koboSpan" id="kobo.401.1">, we get 1 + (28 - 3) / 2 = 13. </span><span class="koboSpan" id="kobo.401.2">Therefore, if we use </span><em class="italic"><span class="koboSpan" id="kobo.402.1">stride = 2</span></em><span class="koboSpan" id="kobo.403.1">, the size of the output slice will be roughly four times smaller than the input. </span><span class="koboSpan" id="kobo.403.2">In other words, one output unit will “cover” an area, which is four times larger compared to the input units. </span><span class="koboSpan" id="kobo.403.3">The units in the following layers will gradually capture input from larger regions from the input image. </span><span class="koboSpan" id="kobo.403.4">This is important because it would allow them</span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.404.1"> to detect larger and more complex features</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.405.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">the input.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.407.1">The convolution operations </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.408.1">we have discussed so far have produced smaller output than the input (even with </span><em class="italic"><span class="koboSpan" id="kobo.409.1">stride = 1</span></em><span class="koboSpan" id="kobo.410.1">). </span><span class="koboSpan" id="kobo.410.2">But, in practice, it’s often desirable to control the size of the output. </span><span class="koboSpan" id="kobo.410.3">We can solve this by </span><strong class="bold"><span class="koboSpan" id="kobo.411.1">padding</span></strong><span class="koboSpan" id="kobo.412.1"> the edges of the input slice</span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.413.1"> with rows and columns of zeros before the convolution operation. </span><span class="koboSpan" id="kobo.413.2">The most common way to use padding is to produce output with the same dimensions as the input. </span><span class="koboSpan" id="kobo.413.3">In the following diagram, we can see a convolutional layer with </span><em class="italic"><span class="koboSpan" id="kobo.414.1">padding = 1</span></em><span class="koboSpan" id="kobo.415.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.416.1">stride = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.417.1">1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">:</span></span></p>
<p class="IMG---Figure" lang="en-GB"><span class="koboSpan" id="kobo.419.1"><img alt="Figure 4.8 – A convolutional layer with padding = 1" src="image/B19627_04_08.png"/></span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.420.1">Figure 4.8 – A convolutional layer with padding = 1</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.421.1">The white units represent the padding. </span><span class="koboSpan" id="kobo.421.2">The input and the output slices have the same dimensions (dark units). </span><span class="koboSpan" id="kobo.421.3">The newly padded zeros will participate in the convolution operation with the slice, but they won’t affect the result. </span><span class="koboSpan" id="kobo.421.4">The reason is that, even though the padded areas are connected with weights to the following layer, we’ll always multiply those weights by the</span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.422.1"> padded value, which is 0. </span><span class="koboSpan" id="kobo.422.2">At the same time, sliding the filter across the padded input slice will produce an output slice with the same dimensions as the </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">unpadded input.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.424.1">Now that we know about stride</span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.425.1"> and padding, we can introduce</span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.426.1"> the full formula</span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.427.1"> for the size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">output</span></span><span class="No-Break"><a id="_idIndexMarker487"/></span><span class="No-Break"><span class="koboSpan" id="kobo.429.1"> slice:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.430.1">
height_o = 1 + (height_i + 2*padding_h – filter_h) / stride
width_o = 1 + (width_i + 2*padding_w – filter_w) / stride</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.431.1">We now have a basic knowledge of convolutions, and we can continue to the next building block of CNNs – the pooling layer. </span><span class="koboSpan" id="kobo.431.2">Once we know all about pooling layers, we’ll introduce our first full CNN, and we’ll implement a simple task to solidify our knowledge. </span><span class="koboSpan" id="kobo.431.3">Then, we’ll focus on more advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">CNN topics.</span></span></p>
<h1 id="_idParaDest-71" lang="en-GB"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.433.1">Pooling layers</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.434.1">In the previous section, we explained</span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.435.1"> how to increase the receptive field of the units by using </span><em class="italic"><span class="koboSpan" id="kobo.436.1">stride &gt; 1</span></em><span class="koboSpan" id="kobo.437.1">. </span><span class="koboSpan" id="kobo.437.2">But we can also do this with the help of pooling layers. </span><span class="koboSpan" id="kobo.437.3">A pooling layer splits the input slice into a grid, where each grid cell represents a receptive field of several units (just as a convolutional layer does). </span><span class="koboSpan" id="kobo.437.4">Then, a pooling operation is applied over each cell of the grid. </span><span class="koboSpan" id="kobo.437.5">Pooling layers don’t change the volume depth because the pooling operation is performed independently on each slice. </span><span class="koboSpan" id="kobo.437.6">They are defined by two parameters: stride and receptive field size, just like convolutional layers (pooling layers usually don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">use padding).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.439.1">In this section, we’ll discuss three types</span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.440.1"> of pooling layers – max pooling, average pooling, and </span><strong class="bold"><span class="koboSpan" id="kobo.441.1">global average pooling</span></strong><span class="koboSpan" id="kobo.442.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.443.1">GAP</span></strong><span class="koboSpan" id="kobo.444.1">). </span><span class="koboSpan" id="kobo.444.2">These three types of pooling are displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer380">
<span class="koboSpan" id="kobo.446.1"><img alt="Figure 4.9 – Max, average, and global average pooling" src="image/B19627_04_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.447.1">Figure 4.9 – Max, average, and global average pooling</span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.448.1">Max pooling</span></strong><span class="koboSpan" id="kobo.449.1"> is the most common way of pooling. </span><span class="koboSpan" id="kobo.449.2">The max pooling operation takes the unit with the highest activation</span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.450.1"> value in each local receptive field (grid cell) and propagates only that value forward. </span><span class="koboSpan" id="kobo.450.2">In the preceding figure (left), we can see an example of max pooling with a receptive field of 2×2 and </span><em class="italic"><span class="koboSpan" id="kobo.451.1">stride = 2</span></em><span class="koboSpan" id="kobo.452.1">. </span><span class="koboSpan" id="kobo.452.2">This operation discards 3/4 of the input units. </span><span class="koboSpan" id="kobo.452.3">Pooling layers don’t have any weights. </span><span class="koboSpan" id="kobo.452.4">In the backward pass of max pooling, the gradient is routed only to the unit with the highest activation during the forward pass. </span><span class="koboSpan" id="kobo.452.5">The other units in the receptive field </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">backpropagate zeros.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.454.1">Average pooling</span></strong><span class="koboSpan" id="kobo.455.1"> is another type of pooling, where the output</span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.456.1"> of each receptive field is the mean value of all activations within the field. </span><span class="koboSpan" id="kobo.456.2">In the preceding figure (middle), we can see an example of average pooling with a receptive field of 2×2 and </span><em class="italic"><span class="koboSpan" id="kobo.457.1">stride = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.458.1">2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.460.1">GAP is similar to average pooling, but a single pooling region covers the whole input slice. </span><span class="koboSpan" id="kobo.460.2">We can think of GAP as an extreme type of dimensionality reduction because it outputs a single value that represents the average of the whole slice. </span><span class="koboSpan" id="kobo.460.3">This type of pooling is usually</span><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.461.1"> applied at the end of the convolutional portion of a CNN. </span><span class="koboSpan" id="kobo.461.2">In the preceding figure (right), we can see an example of a GAP operation. </span><span class="koboSpan" id="kobo.461.3">Stride and receptive field size don’t apply to the </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">GAP operation.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.463.1">In practice, only two combinations of stride and receptive field size are used. </span><span class="koboSpan" id="kobo.463.2">The first is a 2×2 receptive field with </span><em class="italic"><span class="koboSpan" id="kobo.464.1">stride = 2</span></em><span class="koboSpan" id="kobo.465.1">, and the second is a 3×3 receptive field with </span><em class="italic"><span class="koboSpan" id="kobo.466.1">stride = 2</span></em><span class="koboSpan" id="kobo.467.1"> (overlapping). </span><span class="koboSpan" id="kobo.467.2">If we use a larger value for either parameter, the network loses too much information. </span><span class="koboSpan" id="kobo.467.3">Alternatively, if the stride is 1, the size of the layer wouldn’t be smaller, and nor will the receptive </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">field increase.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.469.1">Based on these parameters, we can compute the output size of a </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">pooling layer:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.471.1">
height_o = 1 + (height_i – filter_h) / stride
width_o = 1 + (width_i – filter_w) / stride</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.472.1">Pooling layers</span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.473.1"> are still very much used, but often, we can achieve similar or better results by simply using convolutional layers with larger strides. </span><span class="koboSpan" id="kobo.473.2">(See, for example, </span><em class="italic"><span class="koboSpan" id="kobo.474.1">J. </span><span class="koboSpan" id="kobo.474.2">Springerberg, A. </span><span class="koboSpan" id="kobo.474.3">Dosovitskiy, T. </span><span class="koboSpan" id="kobo.474.4">Brox, and M. </span><span class="koboSpan" id="kobo.474.5">Riedmiller, Striving for Simplicity: The All Convolutional Net, (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.475.1">2015)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">, </span></span><a href="https://arxiv.org/abs/1412.6806"><span class="No-Break"><span class="koboSpan" id="kobo.477.1">https://arxiv.org/abs/1412.6806</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.478.1">.)</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.479.1">We now have sufficient knowledge to introduce our first </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">full CNN.</span></span></p>
<h1 id="_idParaDest-72" lang="en-GB"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.481.1">The structure of a convolutional network</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.482.1">The following figure</span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.483.1"> shows the structure of a basic </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">classification CNN:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer381">
<span class="koboSpan" id="kobo.485.1"><img alt="Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling layers" src="image/B19627_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.486.1">Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling layers</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.487.1">Most CNNs share basic properties. </span><span class="koboSpan" id="kobo.487.2">Here are some </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">of them:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.489.1">We would typically alternate</span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.490.1"> one or more convolutional layers with one pooling layer (or a stride convolution). </span><span class="koboSpan" id="kobo.490.2">In this way, the convolutional layers can detect features at every level of the receptive field size. </span><span class="koboSpan" id="kobo.490.3">The aggregated receptive field size of deeper layers is larger than the ones at the beginning of the network. </span><span class="koboSpan" id="kobo.490.4">This allows them to capture more complex features from larger input regions. </span><span class="koboSpan" id="kobo.490.5">Let’s illustrate this with an example. </span><span class="koboSpan" id="kobo.490.6">Imagine that the network uses 3×3 convolutions with </span><em class="italic"><span class="koboSpan" id="kobo.491.1">stride = 1</span></em><span class="koboSpan" id="kobo.492.1"> and 2×2 pooling with </span><em class="italic"><span class="koboSpan" id="kobo.493.1">stride = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.494.1">2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.496.1">The units of the first convolutional layer will receive input from 3×3 pixels of </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">the image.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.498.1">A group of 2×2 output units of the first layer will have a combined receptive field size of 4×4 (because of </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">the stride).</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.500.1">After the first pooling operation, this group will be combined in a single unit of the </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">pooling layer.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.502.1">The second convolution operation takes input from 3×3 pooling units. </span><span class="koboSpan" id="kobo.502.2">Therefore, it will receive input from a square with side 3×4 = 12 (or a total of 12×12 = 144) pixels from the </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">input image.</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.504.1">We use the convolutional layers to extract features from the input. </span><span class="koboSpan" id="kobo.504.2">The features detected by the deepest layers are highly abstract, but they are also not readable by humans. </span><span class="koboSpan" id="kobo.504.3">To solve this problem, we usually add one or more FC layers after the last convolutional/pooling layer. </span><span class="koboSpan" id="kobo.504.4">In this example, the last FC layer (output) will use softmax to estimate the class probabilities of the input. </span><span class="koboSpan" id="kobo.504.5">You can think of the FC layers as translators between the network’s language (which we don’t understand) </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">and ours.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.506.1">The deeper convolutional layers usually have more filters (hence higher volume depth), compared to the initial ones. </span><span class="koboSpan" id="kobo.506.2">A feature detector at the beginning of the network works on a small receptive field. </span><span class="koboSpan" id="kobo.506.3">It can only detect a limited number of features, such as edges or lines, shared among all classes. </span><span class="koboSpan" id="kobo.506.4">On the other hand, a deeper layer would detect more complex and numerous features. </span><span class="koboSpan" id="kobo.506.5">For example, if we have multiple classes such as cars, trees, or people, each will have its own set of features, such as tires, doors, leaves and faces, and so on. </span><span class="koboSpan" id="kobo.506.6">This would require more </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">feature</span></span><span class="No-Break"><a id="_idIndexMarker495"/></span><span class="No-Break"><span class="koboSpan" id="kobo.508.1"> detectors.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.509.1">Now that we know the structure of a CNN, let’s implement one with PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">and Keras.</span></span></p>
<h1 id="_idParaDest-73" lang="en-GB"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.511.1">Classifying images with PyTorch and Keras</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.512.1">In this section, we’ll try to classify</span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.513.1"> the images of the CIFAR-10 dataset</span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.514.1"> with both PyTorch and Keras. </span><span class="koboSpan" id="kobo.514.2">It consists of 60,000 32x32 RGB images, divided</span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.515.1"> into 10 classes of objects. </span><span class="koboSpan" id="kobo.515.2">To understand</span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.516.1"> these examples, we’ll first focus on two prerequisites that we haven’t covered until now: how images are represented in DL libraries and data augmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">training techniques.</span></span></p>
<h2 id="_idParaDest-74" lang="en-GB"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.518.1">Convolutional layers in deep learning libraries</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.519.1">PyTorch, Keras, and </span><strong class="bold"><span class="koboSpan" id="kobo.520.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.521.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.522.1">TF</span></strong><span class="koboSpan" id="kobo.523.1">) have out-of-the-gate support</span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.524.1"> for 1D, 2D, and 3D convolutions. </span><span class="koboSpan" id="kobo.524.2">The inputs</span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.525.1"> and outputs </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.526.1">of the convolution operation are tensors. </span><span class="koboSpan" id="kobo.526.2">A 1D convolution with multiple input/output slices would have 3D input and output tensors. </span><span class="koboSpan" id="kobo.526.3">Their axes can be in either </span><em class="italic"><span class="koboSpan" id="kobo.527.1">SCW</span></em><span class="koboSpan" id="kobo.528.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.529.1">SWC</span></em><span class="koboSpan" id="kobo.530.1"> order, where we have </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">the following:</span></span></p>
<ul>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.532.1">S</span></em><span class="koboSpan" id="kobo.533.1">: The index of the sample in </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">the mini-batch</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.535.1">C</span></em><span class="koboSpan" id="kobo.536.1">: The index of the depth slice in </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">the volume</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.538.1">W</span></em><span class="koboSpan" id="kobo.539.1">: The content of </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">the slice</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.541.1">In the same way, a 2D convolution will be represented by </span><em class="italic"><span class="koboSpan" id="kobo.542.1">SCHW</span></em><span class="koboSpan" id="kobo.543.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.544.1">SHWC</span></em><span class="koboSpan" id="kobo.545.1"> ordered tensors, where </span><em class="italic"><span class="koboSpan" id="kobo.546.1">H</span></em><span class="koboSpan" id="kobo.547.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.548.1">W</span></em><span class="koboSpan" id="kobo.549.1"> are the height</span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.550.1"> and width of the slices. </span><span class="koboSpan" id="kobo.550.2">A 3D convolution</span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.551.1"> will have </span><em class="italic"><span class="koboSpan" id="kobo.552.1">SCDHW</span></em><span class="koboSpan" id="kobo.553.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.554.1">SDHWC</span></em><span class="koboSpan" id="kobo.555.1"> order, where </span><em class="italic"><span class="koboSpan" id="kobo.556.1">D</span></em><span class="koboSpan" id="kobo.557.1"> stands for the depth of </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">the slice.</span></span></p>
<h2 id="_idParaDest-75" lang="en-GB"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.559.1">Data augmentation</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.560.1">One of the most efficient regularization</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.561.1"> techniques is data augmentation. </span><span class="koboSpan" id="kobo.561.2">If the training data is too small, the network might start overfitting. </span><span class="koboSpan" id="kobo.561.3">Data augmentation helps counter this by artificially increasing the size of the training set. </span><span class="koboSpan" id="kobo.561.4">In the CIFAR-10 examples, we’ll train a CNN over multiple epochs. </span><span class="koboSpan" id="kobo.561.5">The network will “see” every sample of the dataset once per epoch. </span><span class="koboSpan" id="kobo.561.6">To prevent this, we can apply random augmentations to the images, before feeding them to train the CNN. </span><span class="koboSpan" id="kobo.561.7">The labels will stay the same. </span><span class="koboSpan" id="kobo.561.8">Some of the most popular image augmentations are </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.563.1">Rotation</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.564.1">Horizontal and </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">vertical flip</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.566.1">Zoom in/out</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.567.1">Crop</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.568.1">Skew</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.569.1">Contrast and </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">brightness adjustment</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.571.1">The emboldened augmentations are shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">following example:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer382">
<span class="koboSpan" id="kobo.573.1"><img alt="Figure 4.11 – Examples of diﬀerent image augmentations" src="image/B19627_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.574.1">Figure 4.11 – Examples of diﬀerent image augmentations</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.575.1">With that, we’re ready</span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.576.1"> to proceed with </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">the examples.</span></span></p>
<h2 id="_idParaDest-76" lang="en-GB"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.578.1">Classifying images with PyTorch</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.579.1">We’ll start</span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.580.1"> with </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">PyTorch</span></span><span class="No-Break"><a id="_idIndexMarker508"/></span><span class="No-Break"><span class="koboSpan" id="kobo.582.1"> first:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.583.1">Select the device, preferably a GPU. </span><span class="koboSpan" id="kobo.583.2">This NN is larger than the MNIST ones and the CPU training would be </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">very slow:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.585.1">
import torch
from torchsummary import summary
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.586.1">Load the training dataset (followed by </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">the validation):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.588.1">
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
# Training dataset
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225])
])
train_data = datasets.CIFAR10(
    root='data',
    train=True,
    download=True,
    transform=train_transform)
batch_size = 50
train_loader = DataLoader(
    dataset=train_data,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2)</span></pre><p class="list-inset" lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">train_transform</span></strong><span class="koboSpan" id="kobo.590.1"> is of particular</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.591.1"> interest. </span><span class="koboSpan" id="kobo.591.2">It performs random horizontal</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.592.1"> and vertical flips, and it normalizes the dataset with </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">transforms.Normalize</span></strong><span class="koboSpan" id="kobo.594.1"> using z-score normalization. </span><span class="koboSpan" id="kobo.594.2">The hardcoded numerical values represent the manually computed channel-wise mean and </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">std</span></strong><span class="koboSpan" id="kobo.596.1"> values for the CIFAR-10 dataset. </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">train_loader</span></strong><span class="koboSpan" id="kobo.598.1"> takes care of providing </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">training minibatches.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.600.1">Load the validation</span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.601.1"> dataset. </span><span class="koboSpan" id="kobo.601.2">Note that we normalize the validation</span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.602.1"> set with the mean and </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">std</span></strong><span class="koboSpan" id="kobo.604.1"> values of the </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">training dataset:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.606.1">
validation_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.40</span><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.607.1">6],
        [0.229, 0.224, 0.225])
])
validation_data = datasets.CIFAR10(
    root='data',
    train=False,
    download=True,
    transform=validation_transform)
validation_loader = DataLoader(
    dataset=validation_data,
    batch_size=100,
    shuffle=True)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.608.1">Define our CNN using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.609.1">Sequential</span></strong><span class="koboSpan" id="kobo.610.1"> class. </span><span class="koboSpan" id="kobo.610.2">It has the </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">following properties:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.612.1">Three blocks of two convolutional layers (3×3 filters) and one max </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">pooling layer.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.614.1">Batch normalization after each </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">convolutional layer.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.616.1">The first two blocks apply </span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">padding=1</span></strong><span class="koboSpan" id="kobo.618.1"> to the convolutions, so they don’t decrease</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.619.1"> the size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">feature maps.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.621.1">Gaussian Error Linear Unit</span></strong><span class="koboSpan" id="kobo.622.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.623.1">GELU</span></strong><span class="koboSpan" id="kobo.624.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">activation functions.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.626.1">The feature maps after the last pooling are flattened</span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.627.1"> to a tensor of size 512 and serve as input to a </span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">Linear</span></strong><span class="koboSpan" id="kobo.629.1"> (FC) layer with 10 outputs (one of each class). </span><span class="koboSpan" id="kobo.629.2">The final activation</span><a id="_idIndexMarker515"/> <span class="No-Break"><span class="koboSpan" id="kobo.630.1">is softmax.</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.631.1">Let’s see </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">the definition:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.633.1">
from torch.nn import Sequential, Conv2d, BatchNorm2d, GELU, MaxPool2d, Dropout2d, Linear, Flatten
model = Sequential(
    Conv2d(in_channels=3, out_channels=32,
        kernel_size=3, padding=1),
    BatchNorm2d(32),
    GELU(),
    Conv2d(in_channels=32, out_channels=32,
        kernel_size=3, padding=1),
    BatchNorm2d(32),
    GELU(),
    MaxPool2d(kernel_size=2, stride=2),
    Dropout2d(0.2),
    Conv2d(in_channels=32, out_channels=64,
        kernel_size=3, padding=1),
    BatchNorm2d(64),
    GELU(),
    Conv2d(in_channels=64, out_channels=64,
        kernel_size=3, padding=1),
    BatchNorm2d(64),
    GELU(),
    MaxPool2d(kernel_size=2, stride=2),
    Dropout2d(p=0.3),
    Conv2d(in_channels=64, out_channels=128,
        kernel_size=3),
    BatchNorm2d(128),
    GELU(),
    Conv2d(in_channels=128, out_channels=128,
        kernel_size=3),
    BatchNorm2d(128),
    GELU(),
    MaxPool2d(kernel_size=2, stride=2),
    Dropout2d(p=0.5),
    Flatten(),
    Linear(512, 10),
)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.634.1">Run the training</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.635.1"> and</span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.636.1"> validation. </span><span class="koboSpan" id="kobo.636.2">We’ll use the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">train_model</span></strong><span class="koboSpan" id="kobo.638.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">test_model</span></strong><span class="koboSpan" id="kobo.640.1"> functions that we implemented in the MNIST PyTorch example in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.641.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.642.1">. </span><span class="koboSpan" id="kobo.642.2">Because of this, we won’t implement them here, but the full source code is available in this chapter’s GitHub repository (including a Jupyter Notebook). </span><span class="koboSpan" id="kobo.642.3">We can expect the following</span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.643.1"> results: 51% accuracy in 1 epoch, 70% accuracy</span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.644.1"> in 5 epochs, and around 82% accuracy in </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">75 epochs.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.646.1">This concludes our </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">PyTorch example.</span></span></p>
<h2 id="_idParaDest-77" lang="en-GB"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.648.1">Classifying images with Keras</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.649.1">Our second example</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.650.1"> is the same</span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.651.1"> task, but this time implemented </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">with Keras:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.653.1">Start by downloading the dataset. </span><span class="koboSpan" id="kobo.653.2">We’ll also convert the numerical labels into </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">one-hot-encoded tensors:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.655.1">
import tensorflow as tf
(X_train, Y_train), (X_validation, Y_validation) = \
    tf.keras.datasets.cifar10.load_data()
Y_train = tf.keras.utils.to_categorical(Y_train, 10)
Y_validation = \
    tf.keras.utils.to_categorical(Y_validation, 10)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.656.1">Create an instance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.657.1">ImageDataGenerator</span></strong><span class="koboSpan" id="kobo.658.1">, which applies z-normalization over each channel of the training set images. </span><span class="koboSpan" id="kobo.658.2">It also provides data augmentation (random horizontal and vertical flips) during training. </span><span class="koboSpan" id="kobo.658.3">Also, note that we apply the mean and standard variation of the training over the test set for the </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">best performance:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.660.1">
from tensorflow.keras.preprocessing.image import ImageDataGenerator
data_generator = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    horizontal_flip=True,
    vertical_flip=True)
# Apply z-normalization on the training set
data_generator.fit(X_train)
# Standardize the validation set
X_validation = \
    data_generator.standardize( \
    X_validation.astype('float32'))</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.661.1">Then, we can define our CNN</span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.662.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">Sequential</span></strong><span class="koboSpan" id="kobo.664.1"> class. </span><span class="koboSpan" id="kobo.664.2">We’ll use the same architecture we</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.665.1"> defined in the </span><em class="italic"><span class="koboSpan" id="kobo.666.1">Classifying images with PyTorch</span></em><span class="koboSpan" id="kobo.667.1"> section. </span><span class="koboSpan" id="kobo.667.2">The following is the Keras definition of </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">that model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.669.1">
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, M</span><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.670.1">axPooling2D, Dropout, BatchNormalization, Activation, Flatten
model = Sequential(layers=[
    Conv2D(32, (3, 3),
        padding='same',
        input_shape=X_train.shape[1:]),
    BatchNormalization(),
    Activation('gelu'),
    Conv2D(32, (3, 3), padding='same'),
    BatchNormalization(),
    Activation('gelu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.2),
    Conv2D(64, (3, 3), padding='same'),
    BatchNormalization(),
    Activation('gelu'),
    Conv2D(64, (3, 3), padding='same'),
    BatchNormalization(),
    Activation('gelu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.3),
    Conv2D(128, (3, 3)),
    BatchNormalization(),
    Activation('gelu'),
    Conv2D(128, (3, 3)),
    BatchNormalization(),
    Activation('gelu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.5),
    Flatten(),
    Dense(10, activation='softmax')
])</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.671.1">Define the training parameters (we’ll also print the model summary </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">for clarity):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.673.1">
model.compile(loss='categorical_crossentropy',
    optimizer='adam', metrics=['accuracy'])
print(model.summary())</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.674.1">Run the training for </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">50 epochs:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.676.1">
batch_size = 50
model.fit(
    x=data_generator.flow(x=X_train,
        y=Y_train,
        batch_size=batch_size),
    steps_per_epoch=len(X_trai</span><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.677.1">n) // batch_size,
    epochs=100,
    verbose=1,
    validation_data=(X_validation, Y_validation),
    workers=4)</span></pre></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.678.1">Depending on the number of epochs, this model will produce the following results: 50% accuracy in 1 epoch, 72% accuracy in 5 epochs, and around 85% accuracy in 45 epochs. </span><span class="koboSpan" id="kobo.678.2">Our Keras example has slightly higher accuracy compared to the one in PyTorch, although they should be identical. </span><span class="koboSpan" id="kobo.678.3">Maybe we’ve got a bug somewhere. </span><span class="koboSpan" id="kobo.678.4">We might never know, but we can learn a lesson, nevertheless: ML models aren’t easy to debug because they can fail with slightly degraded</span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.679.1"> performance, instead of outright error. </span><span class="koboSpan" id="kobo.679.2">Finding the exact reason for this performance penalty</span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.680.1"> can </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">be hard.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.682.1">Now that we’ve implemented our first full CNN twice, we’ll focus on some more advanced types </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">of convolutions.</span></span></p>
<h1 id="_idParaDest-78" lang="en-GB"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.684.1">Advanced types of convolutions</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.685.1">So far, we’ve discussed the “classic” convolutional operation. </span><span class="koboSpan" id="kobo.685.2">In this section, we’ll introduce several new variations and </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">their properties.</span></span></p>
<h2 id="_idParaDest-79" lang="en-GB"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.687.1">1D, 2D, and 3D convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.688.1">In this chapter, we’ve used </span><strong class="bold"><span class="koboSpan" id="kobo.689.1">2D convolutions</span></strong><span class="koboSpan" id="kobo.690.1"> because computer vision with two-dimensional images</span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.691.1"> is the most common CNN application. </span><span class="koboSpan" id="kobo.691.2">But we can also have</span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.692.1"> 1D and 3D convolutions, where the units are arranged in one-dimensional</span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.693.1"> or three-dimensional space, respectively. </span><span class="koboSpan" id="kobo.693.2">In all cases, the filter has the same number of dimensions as the input, and the weights are shared across the input. </span><span class="koboSpan" id="kobo.693.3">For example, we would use 1D convolution with time series data because the values are arranged across a single time axis. </span><span class="koboSpan" id="kobo.693.4">In the following diagram, on the left, we can see an example of </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">1D convolution:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer383">
<span class="koboSpan" id="kobo.695.1"><img alt="Figure 4.12 –1D convolution (left); 3D convolution (right)" src="image/B19627_04_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.696.1">Figure 4.12 –1D convolution (left); 3D convolution (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.697.1">The weights with the same dashed lines share the same value. </span><span class="koboSpan" id="kobo.697.2">The output of the 1D convolution is also 1D. </span><span class="koboSpan" id="kobo.697.3">If the input is 3D, such as a 3D MRI, we could use 3D convolution, which will also produce 3D output. </span><span class="koboSpan" id="kobo.697.4">In this way, we’ll maintain the spatial arrangement of the input data. </span><span class="koboSpan" id="kobo.697.5">We can see an example of 3D convolution in the preceding diagram, on the right. </span><span class="koboSpan" id="kobo.697.6">The input has dimensions of H/W/L, and the filter has a single size, </span><em class="italic"><span class="koboSpan" id="kobo.698.1">F</span></em><span class="koboSpan" id="kobo.699.1">, for all dimensions. </span><span class="koboSpan" id="kobo.699.2">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">also 3D.</span></span></p>
<h2 id="_idParaDest-80" lang="en-GB"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.701.1">1×1 convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.702.1">A 1×1 (pointwise) convolution is a special case</span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.703.1"> of convolution where each dimension of the convolution filter is of size 1 (1×1 in 2D convolutions and 1×1×1 in 3D). </span><span class="koboSpan" id="kobo.703.2">At first, this doesn’t make sense – a 1×1 filter doesn’t increase the receptive field size of the output units. </span><span class="koboSpan" id="kobo.703.3">The result of such a convolution would be pointwise scaling. </span><span class="koboSpan" id="kobo.703.4">But it can be useful in another way – we can use them to change the depth between the input and output volumes. </span><span class="koboSpan" id="kobo.703.5">To understand this, let’s recall that, in general, we have an input volume with a depth of </span><span class="koboSpan" id="kobo.704.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/308.png" style="vertical-align:-0.340em;height:1.004em;width:1.015em"/></span><span class="koboSpan" id="kobo.705.1"> slices and </span><span class="koboSpan" id="kobo.706.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/309.png" style="vertical-align:-0.340em;height:1.004em;width:1.290em"/></span><span class="koboSpan" id="kobo.707.1"> filters for </span><span class="koboSpan" id="kobo.708.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/309.png" style="vertical-align:-0.340em;height:1.004em;width:1.289em"/></span><span class="koboSpan" id="kobo.709.1"> output slices. </span><span class="koboSpan" id="kobo.709.2">Each output slice is generated by applying a unique filter over all the input slices. </span><span class="koboSpan" id="kobo.709.3">If we use a 1×1 filter and </span><span class="koboSpan" id="kobo.710.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;≠&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/311.png" style="vertical-align:-0.340em;height:1.004em;width:4.073em"/></span><span class="koboSpan" id="kobo.711.1">, we’ll have output slices of the same size, but with different volume depths. </span><span class="koboSpan" id="kobo.711.2">At the same time, we won’t change the receptive field size between the input and output. </span><span class="koboSpan" id="kobo.711.3">The most common</span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.712.1"> use case is to reduce the output</span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.713.1"> volume, or </span><span class="koboSpan" id="kobo.714.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/312.png" style="vertical-align:-0.340em;height:1.004em;width:3.913em"/></span><span class="koboSpan" id="kobo.715.1"> (dimension reduction), nicknamed the </span><strong class="bold"><span class="koboSpan" id="kobo.716.1">“</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.717.1">bottleneck” layer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">.</span></span></p>
<h2 id="_idParaDest-81" lang="en-GB"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.719.1">Depthwise separable convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.720.1">An output slice in a cross-channel convolution</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.721.1"> receives input from all of the input slices using a single filter. </span><span class="koboSpan" id="kobo.721.2">The filter tries to learn features in a 3D space, where two of the dimensions are spatial (the height and width of the slice) and the third is the channel. </span><span class="koboSpan" id="kobo.721.3">Therefore, the filter maps both spatial and </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">cross-channel correlations.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.723.1">Depthwise separable convolutions</span></strong><span class="koboSpan" id="kobo.724.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.725.1">DSCs</span></strong><span class="koboSpan" id="kobo.726.1">, </span><em class="italic"><span class="koboSpan" id="kobo.727.1">Xception: Deep Learning with Depthwise Separable Convolutions</span></em><span class="koboSpan" id="kobo.728.1">, </span><a href="https://arxiv.org/abs/1610.02357"><span class="koboSpan" id="kobo.729.1">https://arxiv.org/abs/1610.02357</span></a><span class="koboSpan" id="kobo.730.1">) can completely decouple </span><br/><span class="koboSpan" id="kobo.731.1">cross-channel and spatial correlations. </span><span class="koboSpan" id="kobo.731.2">A DSC combines two operations: a depthwise convolution and a 1×1 convolution. </span><span class="koboSpan" id="kobo.731.3">In a depthwise convolution, a single input slice produces a single output slice, so it only maps spatial (and not cross-channel) correlations. </span><span class="koboSpan" id="kobo.731.4">With 1×1 convolutions, we have the opposite. </span><span class="koboSpan" id="kobo.731.5">The following diagram represents </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">the DSC:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer389">
<span class="koboSpan" id="kobo.733.1"><img alt="Figure 4.13 – A depth-wise separable convolution" src="image/B19627_04_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.734.1">Figure 4.13 – A depth-wise separable convolution</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.735.1">The DSC is usually</span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.736.1"> implemented without non-linearity after the first (</span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">depthwise) operation.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.738.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.739.1">Let’s compare the standard and depthwise separable convolutions. </span><span class="koboSpan" id="kobo.739.2">Imagine that we have 32 input and output channels and a filter with a size of 3×3. </span><span class="koboSpan" id="kobo.739.3">In a standard convolution, one output slice is the result of applying one filter for each of the 32 input slices for a total of 32 * 3 * 3 = 288 </span><br/><span class="koboSpan" id="kobo.740.1">weights (excluding bias). </span><span class="koboSpan" id="kobo.740.2">In a comparable depthwise convolution, the filter has only 3 * 3 = 9 weights and the filter for the 1×1 convolution has 32 * 1 * 1 = 32 weights. </span><span class="koboSpan" id="kobo.740.3">The total number of weights is 32 + 9 = 41. </span><span class="koboSpan" id="kobo.740.4">Therefore, the depthwise separable convolution</span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.741.1"> is faster and more memory-efficient compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">standard one.</span></span></p>
<h2 id="_idParaDest-82" lang="en-GB"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.743.1">Dilated convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.744.1">The regular convolution</span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.745.1"> applies an </span><em class="italic"><span class="koboSpan" id="kobo.746.1">n×n</span></em><span class="koboSpan" id="kobo.747.1"> filter over an </span><em class="italic"><span class="koboSpan" id="kobo.748.1">n×n</span></em><span class="koboSpan" id="kobo.749.1"> receptive field. </span><span class="koboSpan" id="kobo.749.2">With dilated convolutions, we apply the same filter sparsely over a receptive field of size </span><em class="italic"><span class="koboSpan" id="kobo.750.1">(n * l - 1) × (n * l - 1)</span></em><span class="koboSpan" id="kobo.751.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.752.1">l</span></em><span class="koboSpan" id="kobo.753.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.754.1">dilation factor</span></strong><span class="koboSpan" id="kobo.755.1">. </span><span class="koboSpan" id="kobo.755.2">We still multiply each filter weight</span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.756.1"> by one input slice cell, but these cells are at a distance of </span><em class="italic"><span class="koboSpan" id="kobo.757.1">l</span></em><span class="koboSpan" id="kobo.758.1"> away from each other. </span><span class="koboSpan" id="kobo.758.2">The regular convolution is a special case of dilated convolution with </span><em class="italic"><span class="koboSpan" id="kobo.759.1">l = 1</span></em><span class="koboSpan" id="kobo.760.1">. </span><span class="koboSpan" id="kobo.760.2">This is best illustrated with the </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer390">
<span class="koboSpan" id="kobo.762.1"><img alt="Figure 4.14 – A dilated convolution with a dilation factor of l=2. Here, the ﬁrst two steps of the operation are displayed. The bottom layer is the input while the top layer is the output. Source: https://github.com/vdumoulin/conv_arithmetic" src="image/B19627_04_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.763.1">Figure 4.14 – A dilated convolution with a dilation factor of l=2. </span><span class="koboSpan" id="kobo.763.2">Here, the ﬁrst two steps of the operation are displayed. </span><span class="koboSpan" id="kobo.763.3">The bottom layer is the input while the top layer is the output. </span><span class="koboSpan" id="kobo.763.4">Source: </span><a href="https://github.com/vdumoulin/conv_arithmetic"><span class="koboSpan" id="kobo.764.1">https://github.com/vdumoulin/conv_arithmetic</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.765.1">Dilated convolutions can increase the receptive field’s size exponentially without losing resolution or coverage. </span><span class="koboSpan" id="kobo.765.2">We can also increase the receptive field with stride convolutions or pooling but at the cost of resolution and/or coverage. </span><span class="koboSpan" id="kobo.765.3">To understand this, let’s imagine that we have a stride convolution with stride </span><em class="italic"><span class="koboSpan" id="kobo.766.1">s &gt; 1</span></em><span class="koboSpan" id="kobo.767.1">. </span><span class="koboSpan" id="kobo.767.2">In this case, the output slice is </span><em class="italic"><span class="koboSpan" id="kobo.768.1">s</span></em><span class="koboSpan" id="kobo.769.1"> times smaller than the input (loss of resolution). </span><span class="koboSpan" id="kobo.769.2">If we increase </span><em class="italic"><span class="koboSpan" id="kobo.770.1">s &gt; F</span></em><span class="koboSpan" id="kobo.771.1"> further (</span><em class="italic"><span class="koboSpan" id="kobo.772.1">F</span></em><span class="koboSpan" id="kobo.773.1"> is the size of either the pooling or convolutional kernel), we get a loss of coverage because some of the areas of the input slice will not participate in the output at all. </span><span class="koboSpan" id="kobo.773.2">Additionally, dilated convolutions don’t increase the computation and memory costs because the filter uses the same number</span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.774.1"> of weights as the </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">regular convolution.</span></span></p>
<h2 id="_idParaDest-83" lang="en-GB"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.776.1">Transposed convolutions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.777.1">In the convolutional operations</span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.778.1"> we’ve discussed so far, the output dimensions are either equal or smaller than the input dimensions. </span><span class="koboSpan" id="kobo.778.2">In contrast, transposed convolutions (first proposed in </span><em class="italic"><span class="koboSpan" id="kobo.779.1">Deconvolutional Networks by Matthew D. </span><span class="koboSpan" id="kobo.779.2">Zeiler, Dilip Krishnan, Graham W. </span><span class="koboSpan" id="kobo.779.3">Taylor, and Rob Fergus</span></em><span class="koboSpan" id="kobo.780.1">: </span><a href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf"><span class="koboSpan" id="kobo.781.1">https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf</span></a><span class="koboSpan" id="kobo.782.1">) allow us to upsample the input data (their output is larger than the input). </span><span class="koboSpan" id="kobo.782.2">This operation is also known as </span><strong class="bold"><span class="koboSpan" id="kobo.783.1">deconvolution</span></strong><span class="koboSpan" id="kobo.784.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.785.1">fractionally strided convolution</span></strong><span class="koboSpan" id="kobo.786.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">sub-pixel convolution</span></strong><span class="koboSpan" id="kobo.788.1">. </span><span class="koboSpan" id="kobo.788.2">These names can sometimes lead to confusion. </span><span class="koboSpan" id="kobo.788.3">To clarify things, note that the transposed</span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.789.1"> convolution is, in fact, a regular convolution with a slightly modified</span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.790.1"> input slice or </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">convolutional</span></span><span class="No-Break"><a id="_idIndexMarker541"/></span><span class="No-Break"><span class="koboSpan" id="kobo.792.1"> filter.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.793.1">For the longer explanation, we’ll start with a 1D regular convolution over a single input and </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">output slice:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer391">
<span class="koboSpan" id="kobo.795.1"><img alt="Figure 4.15 – 1D regular convolution" src="image/B19627_04_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.796.1">Figure 4.15 – 1D regular convolution</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.797.1">It uses a filter with </span><em class="italic"><span class="koboSpan" id="kobo.798.1">size = 4</span></em><span class="koboSpan" id="kobo.799.1">, </span><em class="italic"><span class="koboSpan" id="kobo.800.1">stride = 2</span></em><span class="koboSpan" id="kobo.801.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.802.1">padding = 2</span></em><span class="koboSpan" id="kobo.803.1"> (denoted with gray in the preceding diagram). </span><span class="koboSpan" id="kobo.803.2">The input is a vector of size 6 and the output is a vector of size 4. </span><span class="koboSpan" id="kobo.803.3">The filter, a vector, </span><span class="koboSpan" id="kobo.804.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;3,4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/313.png" style="vertical-align:-0.167em;height:0.869em;width:5.289em"/></span><span class="koboSpan" id="kobo.805.1">, is always the same, but it’s denoted with different colors for each position we apply it to. </span><span class="koboSpan" id="kobo.805.2">The respective output cells are denoted with the same color. </span><span class="koboSpan" id="kobo.805.3">The arrows show which input cells contribute to one </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">output cell.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.807.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.808.1">The example that is being discussed in this section is inspired by the paper </span><em class="italic"><span class="koboSpan" id="kobo.809.1">Is the deconvolution layer the same as a convolutional </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.810.1">layer?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.811.1"> (</span></span><a href="https://arxiv.org/abs/1609.07009"><span class="No-Break"><span class="koboSpan" id="kobo.812.1">https://arxiv.org/abs/1609.07009</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.813.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.814.1">Next, we’ll discuss the same</span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.815.1"> example (1D, single input and output slices, and a filter with </span><em class="italic"><span class="koboSpan" id="kobo.816.1">size = 4</span></em><span class="koboSpan" id="kobo.817.1">, </span><em class="italic"><span class="koboSpan" id="kobo.818.1">padding = 2</span></em><span class="koboSpan" id="kobo.819.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.820.1">stride = 2</span></em><span class="koboSpan" id="kobo.821.1">), but for transposed convolution. </span><span class="koboSpan" id="kobo.821.2">The following diagram shows two ways we can </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">implement it:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer393">
<span class="koboSpan" id="kobo.823.1"><img alt="Figure 4.16 – A convolution with stride = 2, applied with the transposed ﬁlter f. The 2 pixels at the beginning and the end of the output are cropped (left); a convolution with stride 0.5, applied over input data, padded with subpixels. The input is ﬁlled with 0-valued pixels (gray) (right)" src="image/B19627_04_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.824.1">Figure 4.16 – A convolution with stride = 2, applied with the transposed ﬁlter f. </span><span class="koboSpan" id="kobo.824.2">The 2 pixels at the beginning and the end of the output are cropped (left); a convolution with stride 0.5, applied over input data, padded with subpixels. </span><span class="koboSpan" id="kobo.824.3">The input is ﬁlled with 0-valued pixels (gray) (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.825.1">Let’s discuss them </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">in detail:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.827.1">In the first case, we have a regular convolution with </span><em class="italic"><span class="koboSpan" id="kobo.828.1">stride = 2</span></em><span class="koboSpan" id="kobo.829.1"> and a filter represented as a transposed row matrix (equivalent to a column matrix) with size 4: </span><span class="koboSpan" id="kobo.830.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;3,4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/314.png" style="vertical-align:-0.167em;height:0.885em;width:6.140em"/></span><span class="koboSpan" id="kobo.831.1"> (shown in the preceding diagram, left). </span><span class="koboSpan" id="kobo.831.2">Note that the stride is applied over the output layer as opposed to the regular convolution, where we stride over the input. </span><span class="koboSpan" id="kobo.831.3">By setting the stride larger than 1, we can increase the output size, compared to the input. </span><span class="koboSpan" id="kobo.831.4">Here, the size of the input slice is </span><em class="italic"><span class="koboSpan" id="kobo.832.1">I</span></em><span class="koboSpan" id="kobo.833.1">, the size of the filter is </span><em class="italic"><span class="koboSpan" id="kobo.834.1">F</span></em><span class="koboSpan" id="kobo.835.1">, the stride is </span><em class="italic"><span class="koboSpan" id="kobo.836.1">S</span></em><span class="koboSpan" id="kobo.837.1">, and the input padding is </span><em class="italic"><span class="koboSpan" id="kobo.838.1">P</span></em><span class="koboSpan" id="kobo.839.1">. </span><span class="koboSpan" id="kobo.839.2">Due to this, the size, </span><em class="italic"><span class="koboSpan" id="kobo.840.1">O</span></em><span class="koboSpan" id="kobo.841.1">, of the output slice of a transposed convolution is given by the following formula: </span><span class="koboSpan" id="kobo.842.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/315.png" style="vertical-align:-0.050em;height:0.748em;width:8.990em"/></span><span class="koboSpan" id="kobo.843.1">. </span><span class="koboSpan" id="kobo.843.2">In this scenario, an input of size 4 produces an output of size 2 * (4 - 1) + 4 - 2 * 2 = 6. </span><span class="koboSpan" id="kobo.843.3">We also crop the two cells at the beginning and the end of the output vector because they only gather input from a single </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">input cell.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.845.1">In the second case, the input</span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.846.1"> is filled with imaginary 0-valued subpixels between the existing ones (shown in the preceding diagram, right). </span><span class="koboSpan" id="kobo.846.2">This is where the name subpixel convolution comes from. </span><span class="koboSpan" id="kobo.846.3">Think of it as padding but within the image itself and not only along the borders. </span><span class="koboSpan" id="kobo.846.4">Once the input has been transformed in this way, a regular convolution </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">is applied.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.848.1">Let’s compare the two output</span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.849.1"> cells, </span><span class="koboSpan" id="kobo.850.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/316.png" style="vertical-align:-0.333em;height:0.781em;width:0.741em"/></span><span class="koboSpan" id="kobo.851.1"> and </span><span class="koboSpan" id="kobo.852.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/317.png" style="vertical-align:-0.340em;height:0.788em;width:0.741em"/></span><span class="koboSpan" id="kobo.853.1">, in both scenarios. </span><span class="koboSpan" id="kobo.853.2">As shown in the preceding diagram, in either case, </span><span class="koboSpan" id="kobo.854.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/318.png" style="vertical-align:-0.333em;height:0.781em;width:0.729em"/></span><span class="koboSpan" id="kobo.855.1"> receives input from the first and the second input cells and </span><span class="koboSpan" id="kobo.856.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/319.png" style="vertical-align:-0.340em;height:0.788em;width:0.729em"/></span><span class="koboSpan" id="kobo.857.1"> receives input from the second and third cells. </span><span class="koboSpan" id="kobo.857.2">The only difference between these two cases is the index of the weight, which participates in the computation. </span><span class="koboSpan" id="kobo.857.3">However, the weights are learned during training, and, because of this, the index is not important. </span><span class="koboSpan" id="kobo.857.4">Therefore, the two operations </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">are equivalent.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.859.1">Next, let’s take a look at a 2D transposed convolution from a subpixel point of view. </span><span class="koboSpan" id="kobo.859.2">As with the 1D case, we insert 0-valued pixels and padding in the input slice to achieve upsampling (the input is at </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">the bottom):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer400">
<span class="koboSpan" id="kobo.861.1"><img alt="Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding = 1 and stride = 2. Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285" src="image/B19627_04_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.862.1">Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding = 1 and stride = 2. </span><span class="koboSpan" id="kobo.862.2">Source: </span><a href="https://github.com/vdumoulin/conv_arithmetic"><span class="koboSpan" id="kobo.863.1">https://github.com/vdumoulin/conv_arithmetic</span></a><span class="koboSpan" id="kobo.864.1">, https://arxiv.org/abs/1603.07285</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.865.1">The backpropagation</span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.866.1"> operation of a regular convolution is a </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">transposed convolution.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.868.1">This concludes our extended introduction to the various types of convolutions. </span><span class="koboSpan" id="kobo.868.2">In the next section, we’ll learn how to build some advanced CNN architectures with the advanced convolutions we’ve learned about </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">so far.</span></span></p>
<h1 id="_idParaDest-84" lang="en-GB"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.870.1">Advanced CNN models</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.871.1">In this section, we’ll discuss</span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.872.1"> some complex CNN models. </span><span class="koboSpan" id="kobo.872.2">They are available in both PyTorch and Keras, with pre-trained weights on the ImageNet dataset. </span><span class="koboSpan" id="kobo.872.3">You can import and use them directly, instead of building them from scratch. </span><span class="koboSpan" id="kobo.872.4">Still, it’s worth discussing their central ideas as an alternative to using them as </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">black boxes.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.874.1">Most of these models share a few </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">architectural principles:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.876.1">They start with an “entry” phase, which</span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.877.1"> uses a combination of stride convolutions and/or pooling to reduce the input image size at least two to eight times, before propagating it to the rest of the network. </span><span class="koboSpan" id="kobo.877.2">This makes a CNN more computationally- and memory-efficient because the deeper layers work with </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">smaller slices.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.879.1">The main network body comes after the entry phase. </span><span class="koboSpan" id="kobo.879.2">It is composed of multiple repeated composite modules. </span><span class="koboSpan" id="kobo.879.3">Each of these modules utilizes padded convolutions in such a way that its input and output slices are the same size. </span><span class="koboSpan" id="kobo.879.4">This makes it possible to stack as many modules as necessary to reach the desired depth. </span><span class="koboSpan" id="kobo.879.5">The deeper modules utilize a higher number of filters (output slices) per convolution, compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">earlier ones.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.881.1">The downsampling in the main body is handled by special modules with stride convolutions and/or </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">pooling operations.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.883.1">The convolutional phase usually ends with GAP over </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">all slices.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.885.1">The output of the GAP operation can serve as input for various tasks. </span><span class="koboSpan" id="kobo.885.2">For example, we can add an FC layer</span><a id="_idIndexMarker548"/> <span class="No-Break"><span class="koboSpan" id="kobo.886.1">for classification.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.887.1">We can see a prototypical CNN</span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.888.1"> built with these principles in the </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer401">
<span class="koboSpan" id="kobo.890.1"><img alt="Figure 4.18 – A prototypical CNN" src="image/B19627_04_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.891.1">Figure 4.18 – A prototypical CNN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.892.1">With that, let’s dig deeper into deep CNNs (</span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">get it?).</span></span></p>
<h2 id="_idParaDest-85" lang="en-GB"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.894.1">Introducing residual networks</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.895.1">Residual networks</span></strong><span class="koboSpan" id="kobo.896.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.897.1">ResNets</span></strong><span class="koboSpan" id="kobo.898.1">, </span><em class="italic"><span class="koboSpan" id="kobo.899.1">Deep Residual Learning for Image Recognition</span></em><span class="koboSpan" id="kobo.900.1">, </span><a href="https://arxiv.org/abs/1512.03385"><span class="koboSpan" id="kobo.901.1">https://arxiv.org/abs/1512.03385</span></a><span class="koboSpan" id="kobo.902.1">) were released in 2015 when they won all five categories</span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.903.1"> of the ImageNet challenge that year. </span><span class="koboSpan" id="kobo.903.2">In </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.904.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.905.1">, we discussed that the layers of an NN are not restricted to sequential order but form a directed graph instead. </span><span class="koboSpan" id="kobo.905.2">This is the first architecture we’ll learn about that takes advantage of this flexibility. </span><span class="koboSpan" id="kobo.905.3">This is also the first network architecture that has successfully trained a network with a depth of more than </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">100 layers.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.907.1">Thanks to better weight initializations, new activation functions, as well as normalization layers, it’s now possible to train deep networks. </span><span class="koboSpan" id="kobo.907.2">However, the authors of the paper conducted some experiments and observed that a network with 56 layers had higher training and testing errors compared to a network with 20 layers. </span><span class="koboSpan" id="kobo.907.3">They argue that this should not be the case. </span><span class="koboSpan" id="kobo.907.4">In theory, we can take a shallow network and stack identity layers (these are layers whose output just repeats the input) on top of it to produce a deeper network that behaves in the same way as the shallow one. </span><span class="koboSpan" id="kobo.907.5">Yet, their experiments have been unable to match the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">shallow network.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.909.1">To solve this problem, they proposed a network constructed of residual blocks. </span><span class="koboSpan" id="kobo.909.2">A residual block consists of two or three sequential convolutional layers and a separate parallel </span><strong class="bold"><span class="koboSpan" id="kobo.910.1">identity</span></strong><span class="koboSpan" id="kobo.911.1"> (repeater) shortcut connection, which connects the input of the first layer and the output of the last one. </span><span class="koboSpan" id="kobo.911.2">We can see three types of residual blocks in the </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer402">
<span class="koboSpan" id="kobo.913.1"><img alt="Figure 4.19 – From left to right – original residual block; original bottleneck residual block; pre-activation residual block; pre-activation bottleneck residual block" src="image/B19627_04_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.914.1">Figure 4.19 – From left to right – original residual block; original bottleneck residual block; pre-activation residual block; pre-activation bottleneck residual block</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.915.1">Each block has two parallel paths. </span><span class="koboSpan" id="kobo.915.2">The left-hand path is similar to the other networks we’ve seen and consists of sequential convolutional layers and batch normalization. </span><span class="koboSpan" id="kobo.915.3">The right path contains</span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.916.1"> the identity shortcut connection (also known as the </span><strong class="bold"><span class="koboSpan" id="kobo.917.1">skip connection</span></strong><span class="koboSpan" id="kobo.918.1">). </span><span class="koboSpan" id="kobo.918.2">The two paths are merged via an element-wise sum – that is, the left and right tensors have the same</span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.919.1"> shape, and an element of the first tensor is added to the element in the same position in the second tensor. </span><span class="koboSpan" id="kobo.919.2">The output is a single tensor with the same shape as the input. </span><span class="koboSpan" id="kobo.919.3">In effect, we propagate the features learned by the block forward, but also the original unmodified signal. </span><span class="koboSpan" id="kobo.919.4">In this way, we can get closer to the original scenario, as described by the authors. </span><span class="koboSpan" id="kobo.919.5">The network can decide to skip some of the convolutional layers thanks to the skip connections, in effect reducing its depth. </span><span class="koboSpan" id="kobo.919.6">The residual blocks use padding in such a way that the input and the output of the block have the same dimensions. </span><span class="koboSpan" id="kobo.919.7">Thanks to this, we can stack any number of blocks for a network with an </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">arbitrary depth.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.921.1">Now, let’s see how the blocks in the </span><span class="No-Break"><span class="koboSpan" id="kobo.922.1">diagram differ:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.923.1">The first block contains two 3×3 convolutional layers. </span><span class="koboSpan" id="kobo.923.2">This is the original residual block, but if the layers are wide, stacking multiple blocks becomes </span><span class="No-Break"><span class="koboSpan" id="kobo.924.1">computationally expensive.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.925.1">The second block is equivalent to the first, but it uses a </span><strong class="bold"><span class="koboSpan" id="kobo.926.1">bottleneck layer</span></strong><span class="koboSpan" id="kobo.927.1">. </span><span class="koboSpan" id="kobo.927.2">First, we use a 1×1 convolution to downsample</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.928.1"> the input volume depth (we discussed this in the </span><em class="italic"><span class="koboSpan" id="kobo.929.1">1×1 convolutions</span></em><span class="koboSpan" id="kobo.930.1"> section). </span><span class="koboSpan" id="kobo.930.2">Then, we apply a 3×3 (bottleneck) convolution to the reduced input. </span><span class="koboSpan" id="kobo.930.3">Finally, we expand</span><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.931.1"> the output back to the desired depth with another 1×1 upsampling convolution. </span><span class="koboSpan" id="kobo.931.2">This layer is less computationally expensive than </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">the first.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.933.1">The third block is the latest revision of the idea, published in 2016 by the same authors (</span><em class="italic"><span class="koboSpan" id="kobo.934.1">Identity Mappings in Deep Residual Networks</span></em><span class="koboSpan" id="kobo.935.1">, </span><a href="https://arxiv.org/abs/1603.05027"><span class="koboSpan" id="kobo.936.1">https://arxiv.org/abs/1603.05027</span></a><span class="koboSpan" id="kobo.937.1">). </span><span class="koboSpan" id="kobo.937.2">It uses pre-activations, and the batch normalization and the activation function come before the convolutional layer. </span><span class="koboSpan" id="kobo.937.3">This may seem strange at first, but thanks to this design, the skip connection path can run uninterrupted throughout the network. </span><span class="koboSpan" id="kobo.937.4">This is contrary to the other residual blocks, where at least one activation function is on the path of the skip connection. </span><span class="koboSpan" id="kobo.937.5">A combination of stacked residual blocks still has the layers in the </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">right order.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.939.1">The fourth block is the bottleneck</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.940.1"> version of the third layer. </span><span class="koboSpan" id="kobo.940.2">It follows the same principle as the bottleneck residual </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">layer v1.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.942.1">In the following table, we can see the family of networks proposed by the authors of </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">the paper:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer403">
<span class="koboSpan" id="kobo.944.1"><img alt="Figure 4.20 – The family of the most popular residual networks. The residual blocks are represented by rounded rectangles. Inspired by https://arxiv. org/abs/1512.03385" src="image/B19627_04_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.945.1">Figure 4.20 – The family of the most popular residual networks. </span><span class="koboSpan" id="kobo.945.2">The residual blocks are represented by rounded rectangles. </span><span class="koboSpan" id="kobo.945.3">Inspired by </span><a href="https://arxiv.org/abs/1512.03385"><span class="koboSpan" id="kobo.946.1">https://arxiv.org/abs/1512.03385</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.947.1">Some of their properties</span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.948.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.950.1">They start with a 7×7 convolutional layer with </span><em class="italic"><span class="koboSpan" id="kobo.951.1">stride = 2</span></em><span class="koboSpan" id="kobo.952.1">, followed by 3×3 max pooling. </span><span class="koboSpan" id="kobo.952.2">This phase serves as a downsampling step, so the rest of the network can work with a much smaller slice of 56×56, compared to 224×224 of </span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">the input.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.954.1">Downsampling in the rest of the network is implemented with a modified residual block with </span><em class="italic"><span class="koboSpan" id="kobo.955.1">stride = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.956.1">2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.958.1">GAP downsamples the output after all residual blocks and before the 1,000-unit FC </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">softmax layer.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.960.1">The number of parameters for the various ResNets range from 25.6 million to 60.4 million and their depth ranges from 18 to </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">152 layers.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.962.1">The ResNet family of networks is popular not only because of their accuracy but also because of their relative simplicity and the versatility of the residual blocks. </span><span class="koboSpan" id="kobo.962.2">As we mentioned previously, the input and output shape of the residual block can be the same due to the padding. </span><span class="koboSpan" id="kobo.962.3">We can stack residual blocks</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.963.1"> in different configurations to solve various problems with wide-ranging training set sizes and </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">input dimensions.</span></span></p>
<h2 id="_idParaDest-86" lang="en-GB"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.965.1">Inception networks</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.966.1">Inception networks</span></strong><span class="koboSpan" id="kobo.967.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.968.1">Going Deeper with Convolutions</span></em><span class="koboSpan" id="kobo.969.1">, </span><a href="https://arxiv.org/abs/1409.4842"><span class="koboSpan" id="kobo.970.1">https://arxiv.org/abs/1409.4842</span></a><span class="koboSpan" id="kobo.971.1">) were introduced in 2014 when they won the ImageNet</span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.972.1"> challenge of that year (there seems to be a pattern here). </span><span class="koboSpan" id="kobo.972.2">Since then, the authors have released multiple improvements (versions) of </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">the architecture.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.974.1">Fun fact</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.975.1">The name </span><em class="italic"><span class="koboSpan" id="kobo.976.1">inception</span></em><span class="koboSpan" id="kobo.977.1"> comes in part from the </span><em class="italic"><span class="koboSpan" id="kobo.978.1">We need to go deeper</span></em><span class="koboSpan" id="kobo.979.1"> internet meme, related to the </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">movie Inception.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.981.1">The idea behind inception networks started from the basic premise that the objects in an image have different scales. </span><span class="koboSpan" id="kobo.981.2">A distant object might take up a small region of the image, but the same object, once nearer, might take up a large part of the image. </span><span class="koboSpan" id="kobo.981.3">This presents a difficulty for standard CNNs, where the units in the different layers have a fixed receptive field size, as imposed on the input image. </span><span class="koboSpan" id="kobo.981.4">A regular network might be a good detector of objects at a certain scale but could miss them otherwise. </span><span class="koboSpan" id="kobo.981.5">To solve this problem, the authors of the paper proposed a novel architecture: one composed of inception blocks. </span><span class="koboSpan" id="kobo.981.6">An inception block starts with a common input and then splits it into different parallel paths (or towers). </span><span class="koboSpan" id="kobo.981.7">Each path contains either convolutional layers with a different-sized filter or a pooling layer. </span><span class="koboSpan" id="kobo.981.8">In this way, we apply different receptive fields to the same input data. </span><span class="koboSpan" id="kobo.981.9">At the end of the Inception block, the outputs of the different paths </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">are concatenated.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.983.1">In the next few sections, we’ll discuss the different variations of </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">Inception networks.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.985.1">Inception v1</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.986.1">The following</span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.987.1"> diagram shows the first version of the inception</span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.988.1"> block, which is part of the </span><strong class="bold"><span class="koboSpan" id="kobo.989.1">GoogLeNet</span></strong><span class="koboSpan" id="kobo.990.1"> network </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">architecture (</span></span><a href="https://arxiv.org/abs/1409.4842"><span class="No-Break"><span class="koboSpan" id="kobo.992.1">https://arxiv.org/abs/1409.4842</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.993.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer404">
<span class="koboSpan" id="kobo.994.1"><img alt="Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842" src="image/B19627_04_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.995.1">Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.996.1">The v1 block has </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">four paths:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.998.1">1×1 convolution, which acts as a kind of repeater to </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">the input</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1000.1">1×1 convolution, followed by a </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">3×3 convolution</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1002.1">1×1 convolution, followed by a </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">5×5 convolution</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1004.1">3×3 max pooling with </span><em class="italic"><span class="koboSpan" id="kobo.1005.1">stride = 1</span></em></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1006.1">The layers in the block use padding in such a way that the input and the output have the same shape (but different depths). </span><span class="koboSpan" id="kobo.1006.2">The padding is also necessary because each path would produce an output with a different shape, depending on the filter size. </span><span class="koboSpan" id="kobo.1006.3">This is valid for all versions of </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">inception blocks.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1008.1">The other major innovation</span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.1009.1"> of this inception block is the use of downsampling 1×1 convolutions. </span><span class="koboSpan" id="kobo.1009.2">They are needed because the output of all paths is concatenated to produce the final output of the block. </span><span class="koboSpan" id="kobo.1009.3">The result of the concatenation is an output with a quadrupled depth. </span><span class="koboSpan" id="kobo.1009.4">If another inception block followed the current one, its output depth would quadruple again. </span><span class="koboSpan" id="kobo.1009.5">To avoid such exponential growth, the block uses 1×1 convolutions to reduce the depth of each path, which, in turn, reduces the output depth of the block. </span><span class="koboSpan" id="kobo.1009.6">This makes it possible to create deeper networks, without running out </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">of resources.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1011.1">The full GoogLeNet has the </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">following</span></span><span class="No-Break"><a id="_idIndexMarker561"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1"> properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1014.1">Like ResNets, it starts with a downsampling phase, which utilizes two convolutional and two max pooling layers to reduce the input size from 224×224 to 56×56, before the inception blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">get involved.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1016.1">The network has nine inception </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">v1 blocks.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1018.1">The convolutional phase ends with global </span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">average pooling.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1020.1">The network utilizes auxiliary classifiers—that is, it has two additional classification outputs (with the same ground truth labels) at various intermediate layers. </span><span class="koboSpan" id="kobo.1020.2">During training, the total value of the loss is a weighted sum of the auxiliary losses and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">real loss.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1022.1">The model has a total</span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.1023.1"> of 6.9 million parameters and a depth of </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">22 layers.</span></span></li>
</ul>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1025.1">Inception v2 and v3</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1026.1">Inception v2 and v3 were released together and proposed</span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.1027.1"> several improved inception blocks over the original v1 (</span><em class="italic"><span class="koboSpan" id="kobo.1028.1">Rethinking the Inception Architecture for Computer Vision</span></em><span class="koboSpan" id="kobo.1029.1">, </span><a href="https://arxiv.org/abs/1512.00567"><span class="koboSpan" id="kobo.1030.1">https://arxiv.org/abs/1512.00567</span></a><span class="koboSpan" id="kobo.1031.1">). </span><span class="koboSpan" id="kobo.1031.2">We can see the first new inception block, A, in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer405">
<span class="koboSpan" id="kobo.1033.1"><img alt="Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567" src="image/B19627_04_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1034.1">Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1035.1">The first new property of block A is the factorization of the 5×5 convolution in two stacked 3×3 convolutions. </span><span class="koboSpan" id="kobo.1035.2">This structure has </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">several advantages.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1037.1">The receptive field of the units of the last stacked layer is equivalent to the receptive field of a single layer with a large convolutional filter. </span><span class="koboSpan" id="kobo.1037.2">The stacked layers achieve the same receptive field size with fewer parameters, compared to a single layer with a large filter. </span><span class="koboSpan" id="kobo.1037.3">For example, let’s replace a single 5×5 layer with two stacked 3×3 layers. </span><span class="koboSpan" id="kobo.1037.4">For the sake of simplicity, we’ll assume that we have single input and output slices. </span><span class="koboSpan" id="kobo.1037.5">The total number of weights (excluding biases) of the 5×5 layer is 5 * 5 = 25. </span><br/><span class="koboSpan" id="kobo.1038.1">On the other hand, the total weights of a single 3×3 layer is 3 * 3 = 9, and simply 2 * (3 * 3) = 18 for two layers, which makes this arrangement 28% more efficient (18/25 = 0.72). </span><span class="koboSpan" id="kobo.1038.2">The efficiency gain is preserved even with multiple</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.1039.1"> input and output slices for the two layers. </span><span class="koboSpan" id="kobo.1039.2">The next improvement is the factorization of an </span><em class="italic"><span class="koboSpan" id="kobo.1040.1">n×n</span></em><span class="koboSpan" id="kobo.1041.1"> convolution in two stacked asymmetrical 1×</span><em class="italic"><span class="koboSpan" id="kobo.1042.1">n</span></em><span class="koboSpan" id="kobo.1043.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1044.1">n</span></em><span class="koboSpan" id="kobo.1045.1">×1 convolutions. </span><span class="koboSpan" id="kobo.1045.2">For example, we can split a single 3×3 convolution into two 1×3 and 3×1 convolutions, where the 3×1 convolution is applied over the output of the 1×3 convolution. </span><span class="koboSpan" id="kobo.1045.3">In the first case, the filter size would be 3 * 3 = 9, while in the second case, we would have a combined size of (3 * 1) + (1 * 3) = 3 + 3 = 6, resulting in 33% efficiency, as seen in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer406">
<span class="koboSpan" id="kobo.1047.1"><img alt="Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions; inspired by https://arxiv.org/abs/1512.00567" src="image/B19627_04_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1048.1">Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions; inspired by https://arxiv.org/abs/1512.00567</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1049.1">The authors introduced two new blocks that utilize factorized convolutions. </span><span class="koboSpan" id="kobo.1049.2">The first of these blocks (and the second in total), inception block B, is equivalent to inception </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">block A:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer407">
<span class="koboSpan" id="kobo.1051.1"><img alt="Figure 4.24 – Inception block B. When n=3, it is equivalent to block A; inspired by https://arxiv.org/abs/1512.00567" src="image/B19627_04_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1052.1">Figure 4.24 – Inception block B. </span><span class="koboSpan" id="kobo.1052.2">When n=3, it is equivalent to block A; inspired by https://arxiv.org/abs/1512.00567</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1053.1">The second (third in total) inception block, C, is similar, but the asymmetrical convolutions are parallel, resulting in a higher output depth (more concatenated paths). </span><span class="koboSpan" id="kobo.1053.2">The hypothesis here is that the more features (different filters) the network has, the faster it learns. </span><span class="koboSpan" id="kobo.1053.3">On the other hand, the wider layers take more memory and computation time. </span><span class="koboSpan" id="kobo.1053.4">As a compromise, this block is only used in the deeper part of the network, after the </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">other blocks:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer408">
<span class="koboSpan" id="kobo.1055.1"><img alt="Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567" src="image/B19627_04_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1056.1">Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1057.1">Another major improvement in this version is the use of batch normalization, which was introduced by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1058.1">same authors.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1059.1">These new blocks create two new inception networks: v2 and v3. </span><span class="koboSpan" id="kobo.1059.2">Inception v3 uses batch normalization and is the more popular of the two. </span><span class="koboSpan" id="kobo.1059.3">It has the </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1061.1">The network starts with a downsampling phase, which utilizes stride convolutions and max pooling to reduce the input size from 299×299 to 35×35 before the inception blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">get involved</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1063.1">The layers are organized into three inception blocks, A, five inception blocks, B, and two inception </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">blocks, C</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1065.1">The convolutional phase</span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.1066.1"> ends with global </span><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">average pooling</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1068.1">It has 23.9 million parameters and a depth of </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">48 layers</span></span></li>
</ul>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1070.1">Inception v4 and Inception-ResNet</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1071.1">The latest revisions</span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.1072.1"> of inception networks introduce three new streamlined inception blocks (</span><strong class="bold"><span class="koboSpan" id="kobo.1073.1">Inception-v4</span></strong><span class="koboSpan" id="kobo.1074.1">,  </span><em class="italic"><span class="koboSpan" id="kobo.1075.1">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</span></em><span class="koboSpan" id="kobo.1076.1">, </span><a href="https://arxiv.org/abs/1602.07261"><span class="koboSpan" id="kobo.1077.1">https://arxiv.org/abs/1602.07261</span></a><span class="koboSpan" id="kobo.1078.1">). </span><span class="koboSpan" id="kobo.1078.2">More specifically, the new versions introduce 7×7 asymmetric factorized convolutions</span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.1079.1"> average pooling instead of max pooling and new Inception-ResNet blocks with residual connections. </span><span class="koboSpan" id="kobo.1079.2">We can see one such block in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer409">
<span class="koboSpan" id="kobo.1081.1"><img alt="Figure 4.26 – An inception block (any kind) with a residual skip connection" src="image/B19627_04_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1082.1">Figure 4.26 – An inception block (any kind) with a residual skip connection</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1083.1">The Inception-ResNet family of models share</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.1084.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1086.1">The networks start with a downsampling phase, which utilizes stride convolutions and max pooling to reduce the input size from 299×299 to 35×35 before the inception blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">get involved.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1088.1">The main body of the model consists of three groups of four residual-inception-A blocks, seven residual-inception-B blocks, three residual inception-B blocks, and special reduction modules between the groups. </span><span class="koboSpan" id="kobo.1088.2">The different models use slightly different variations of </span><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">these blocks.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1090.1">The convolutional phase ends with global </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">average pooling.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1092.1">The models have around 56 </span><span class="No-Break"><span class="koboSpan" id="kobo.1093.1">million weights.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1094.1">In this section, we discussed</span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.1095.1"> different types of inception networks</span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.1096.1"> and the different principles used in the various inception blocks. </span><span class="koboSpan" id="kobo.1096.2">Next, we’ll talk about a newer CNN architecture that takes the inception concept to a new depth (or width, as it </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">should be).</span></span></p>
<h2 id="_idParaDest-87" lang="en-GB"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.1098.1">Introducing Xception</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1099.1">All inception blocks we’ve discussed so far start by splitting</span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.1100.1"> the input into several parallel paths. </span><span class="koboSpan" id="kobo.1100.2">Each path continues with a dimensionality-reduction 1×1 cross-channel convolution, followed by regular cross-channel convolutions. </span><span class="koboSpan" id="kobo.1100.3">On one hand, the 1×1 connection maps cross-channel correlations, but not spatial ones (because of the 1×1 filter size). </span><span class="koboSpan" id="kobo.1100.4">On the other hand, the subsequent cross-channel convolutions map both types of correlations. </span><span class="koboSpan" id="kobo.1100.5">Let’s recall that earlier in this chapter, we introduced DSCs, which combine the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">two operations:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1102.1">A depthwise convolution</span></strong><span class="koboSpan" id="kobo.1103.1">: In a depthwise convolution, a single input slice produces a single</span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.1104.1"> output slice, so it only maps spatial (and not </span><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">cross-channel) correlations</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1106.1">A 1×1 cross-channel convolution</span></strong><span class="koboSpan" id="kobo.1107.1">: With 1×1 convolutions, we have the opposite – that is, they only map</span><a id="_idIndexMarker573"/> <span class="No-Break"><span class="koboSpan" id="kobo.1108.1">cross-channel correlations</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1109.1">The author of Xception (</span><em class="italic"><span class="koboSpan" id="kobo.1110.1">Xception: Deep Learning with Depthwise Separable Convolutions</span></em><span class="koboSpan" id="kobo.1111.1">, </span><a href="https://arxiv.org/abs/1610.02357"><span class="koboSpan" id="kobo.1112.1">https://arxiv.org/abs/1610.02357</span></a><span class="koboSpan" id="kobo.1113.1">) argues that we can think of DSC as an extreme (hence the name) version of an inception block, where each depthwise input/output slice pair represents one parallel path. </span><span class="koboSpan" id="kobo.1113.2">We have as many parallel paths as the number of input slices. </span><span class="koboSpan" id="kobo.1113.3">The following diagram shows a simplified inception block and its transformation to an </span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">Xception block:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer410">
<span class="koboSpan" id="kobo.1115.1"><img alt="Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right); inspired by https://arxiv.org/abs/1610.02357" src="image/B19627_04_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1116.1">Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right); inspired by https://arxiv.org/abs/1610.02357</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1117.1">The Xception block</span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.1118.1"> and the DSC have </span><span class="No-Break"><span class="koboSpan" id="kobo.1119.1">two differences:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1120.1">In Xception, the 1×1 convolution comes first, instead of last as in DSC. </span><span class="koboSpan" id="kobo.1120.2">However, these operations are meant to be stacked anyway, and we can assume that the order is of </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">no significance.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1122.1">The Xception block uses ReLU activations after each convolution, while the DSC doesn’t use non-linearity </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.1123.1">after the cross-channel convolution. </span><span class="koboSpan" id="kobo.1123.2">According to the author’s experiments, networks with absent non-linearity depthwise convolution converged faster and were </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">more accurate.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1125.1">The full Xception network</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.1126.1"> has the </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1128.1">It starts with an entry flow of convolutional and pooling operations, which reduces the input size from 299×299 </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">to 19×19.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1130.1">It has 14 Xception modules, all of which have linear residual connections around them, except for the first and </span><span class="No-Break"><span class="koboSpan" id="kobo.1131.1">last modules.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1132.1">All convolutions and DSCs are followed by batch normalization. </span><span class="koboSpan" id="kobo.1132.2">All DSCs have a depth multiplier of 1 (no </span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1">depth expansion).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1134.1">The convolutional phase ends with global </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">average pooling.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1136.1">A total of 23 million parameters and a depth of 36 </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">convolutional layers.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1138.1">This section concludes</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.1139.1"> the series of inception-based models. </span><span class="koboSpan" id="kobo.1139.2">In the next section, we’ll focus on a novel NN </span><span class="No-Break"><span class="koboSpan" id="kobo.1140.1">architectural element.</span></span></p>
<h2 id="_idParaDest-88" lang="en-GB"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.1141.1">Squeeze-and-Excitation Networks</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1142.1">Squeeze-and-Excitation Networks</span></strong><span class="koboSpan" id="kobo.1143.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1144.1">SENet</span></strong><span class="koboSpan" id="kobo.1145.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1146.1">Squeeze-and-Excitation Networks</span></em><span class="koboSpan" id="kobo.1147.1">, </span><a href="https://arxiv.org/abs/1709.01507"><span class="koboSpan" id="kobo.1148.1">https://arxiv.org/abs/1709.01507</span></a><span class="koboSpan" id="kobo.1149.1">) introduce a new NN architectural unit, which the authors call – you guessed it – the </span><strong class="bold"><span class="koboSpan" id="kobo.1150.1">Squeeze-and-Excitation</span></strong><span class="koboSpan" id="kobo.1151.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1152.1">SE</span></strong><span class="koboSpan" id="kobo.1153.1">) block. </span><span class="koboSpan" id="kobo.1153.2">Let’s recall that the convolutional </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.1154.1">operation applies multiple filters across the input channels to produce multiple output feature maps (or channels). </span><span class="koboSpan" id="kobo.1154.2">The authors of SENet observe that each of these channels has “equal weight” when it serves as input to the next layer. </span><span class="koboSpan" id="kobo.1154.3">However, some channels could be more informative than others. </span><span class="koboSpan" id="kobo.1154.4">To emphasize their importance, the authors propose the content-aware SE block, which weighs each</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.1155.1"> channel adaptively. </span><span class="koboSpan" id="kobo.1155.2">We can also think of the SE block as an </span><strong class="bold"><span class="koboSpan" id="kobo.1156.1">attention mechanism</span></strong><span class="koboSpan" id="kobo.1157.1">. </span><span class="koboSpan" id="kobo.1157.2">To understand how it works, let’s start</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.1158.1"> with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer411">
<span class="koboSpan" id="kobo.1160.1"><img alt="Figure 4.28 – The Squeeze-and-Excitation block" src="image/B19627_04_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1161.1">Figure 4.28 – The Squeeze-and-Excitation block</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1162.1">The block introduces a parallel path to the main NN data flow. </span><span class="koboSpan" id="kobo.1162.2">Let’s see </span><span class="No-Break"><span class="koboSpan" id="kobo.1163.1">its steps:</span></span></p>
<ol>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1164.1">Squeeze phase</span></strong><span class="koboSpan" id="kobo.1165.1">: A GAP operation is applied across the channels. </span><span class="koboSpan" id="kobo.1165.2">The output of the GAP</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.1166.1"> is a single scalar value for each channel. </span><span class="koboSpan" id="kobo.1166.2">For example, if the input is an RGB image, the unique GAP operations across each of the R, G, and B channels will produce a one-dimensional tensor with size 3. </span><span class="koboSpan" id="kobo.1166.3">Think of these scalar values as the distilled state of </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">the channels.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1168.1">Excitement phase</span></strong><span class="koboSpan" id="kobo.1169.1">: The output tensor of the squeeze phase serves as input to a two-layer FC NN. </span><span class="koboSpan" id="kobo.1169.2">This NN is in the shape of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1170.1">FC layer -&gt; ReLU -&gt; FC layer -&gt; sigmoid</span></strong><span class="koboSpan" id="kobo.1171.1">. </span><span class="koboSpan" id="kobo.1171.2">It resembles an autoencoder because the first hidden layer reduces</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.1172.1"> the size of the input tensor and the second hidden layer upscales it to the original size (3 in the case of RGB input). </span><span class="koboSpan" id="kobo.1172.2">The final sigmoid activation ensures that all values of the output are in the (</span><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">0:1) range.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1174.1">Scale</span></strong><span class="koboSpan" id="kobo.1175.1">: The output values of the excitement NN</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.1176.1"> serve as scaling coefficients of the channels of the original input tensor. </span><span class="koboSpan" id="kobo.1176.2">All the values of a channel are scaled (or excited) by its corresponding coefficient produced by the excitement phase. </span><span class="koboSpan" id="kobo.1176.3">In this way, the excitement NN can emphasize the importance of a </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">given</span></span><span class="No-Break"><a id="_idIndexMarker584"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1"> channel.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1179.1">The authors added SE blocks to different existing models, which improved their accuracy. </span><span class="koboSpan" id="kobo.1179.2">In the following figure, we can see how we can add SE blocks to inception and </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">residual modules:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer412">
<span class="koboSpan" id="kobo.1181.1"><img alt="Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)" src="image/B19627_04_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1182.1">Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1183.1">In the next section, we’ll see the SE block</span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.1184.1"> applied to a model, which prioritizes a small footprint and </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">computational efficiency.</span></span></p>
<h2 id="_idParaDest-89" lang="en-GB"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.1186.1">Introducing MobileNet</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1187.1">In this section, we’ll discuss</span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.1188.1"> a lightweight CNN model called </span><strong class="bold"><span class="koboSpan" id="kobo.1189.1">MobileNet</span></strong><span class="koboSpan" id="kobo.1190.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1191.1">MobileNetV3: Searching for MobileNetV3</span></em><span class="koboSpan" id="kobo.1192.1">, </span><a href="https://arxiv.org/abs/1905.02244"><span class="koboSpan" id="kobo.1193.1">https://arxiv.org/abs/1905.02244</span></a><span class="koboSpan" id="kobo.1194.1">). </span><span class="koboSpan" id="kobo.1194.2">We’ll focus on the third revision of this idea (MobileNetV1 was introduced in </span><em class="italic"><span class="koboSpan" id="kobo.1195.1">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</span></em><span class="koboSpan" id="kobo.1196.1">, </span><a href="https://arxiv.org/abs/1704.04861"><span class="koboSpan" id="kobo.1197.1">https://arxiv.org/abs/1704.04861</span></a><span class="koboSpan" id="kobo.1198.1"> and MobileNetV2 was introduced in </span><em class="italic"><span class="koboSpan" id="kobo.1199.1">MobileNetV2: Inverted Residuals and Linear </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1200.1">Bottlenecks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1201.1">, </span></span><a href="https://arxiv.org/abs/1801.04381"><span class="No-Break"><span class="koboSpan" id="kobo.1202.1">https://arxiv.org/abs/1801.04381</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1204.1">MobileNet is aimed at devices</span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.1205.1"> with limited</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.1206.1"> memory and computing</span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.1207.1"> power, such as mobile phones (the name kind of gives it away). </span><span class="koboSpan" id="kobo.1207.2">The NN introduces a new </span><strong class="bold"><span class="koboSpan" id="kobo.1208.1">inverted residual block</span></strong><span class="koboSpan" id="kobo.1209.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1210.1">MBConv</span></strong><span class="koboSpan" id="kobo.1211.1">) with a reduced footprint. </span><span class="koboSpan" id="kobo.1211.2">MBConv uses DSC, </span><strong class="bold"><span class="koboSpan" id="kobo.1212.1">linear bottlenecks</span></strong><span class="koboSpan" id="kobo.1213.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1214.1">inverted residuals</span></strong><span class="koboSpan" id="kobo.1215.1">. </span><span class="koboSpan" id="kobo.1215.2">V3 also introduces SE blocks. </span><span class="koboSpan" id="kobo.1215.3">To understand</span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.1216.1"> all this, here’s the structure of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1217.1">MBConv block:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer413">
<span class="koboSpan" id="kobo.1218.1"><img alt="Figure 4.30 – MobileNetV3 building block. The shortcut connection exists only if the stride s=1" src="image/B19627_04_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1219.1">Figure 4.30 – MobileNetV3 building block. </span><span class="koboSpan" id="kobo.1219.2">The shortcut connection exists only if the stride s=1</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1220.1">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">its properties:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1222.1">Linear bottlenecks</span></strong><span class="koboSpan" id="kobo.1223.1">: We’ll assume that our input is an RGB image. </span><span class="koboSpan" id="kobo.1223.2">As it’s propagated</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.1224.1"> through the NN, each layer produces an activation tensor</span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.1225.1"> with multiple channels. </span><span class="koboSpan" id="kobo.1225.2">It has long been assumed that the information encoded in these tensors can be compressed in the so-called “manifold of interest,” which is represented</span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.1226.1"> by a smaller tensor than the original. </span><span class="koboSpan" id="kobo.1226.2">One way to force the NN to seek such manifolds is with 1×1 bottleneck convolutions. </span><span class="koboSpan" id="kobo.1226.3">However, the authors of the paper argue that if this convolution is followed by non-linearity like ReLU, this might lead to a loss of manifold information because of the dying-ReLUs problem. </span><span class="koboSpan" id="kobo.1226.4">To avoid this, MobileNet uses a 1×1 bottleneck convolution without </span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1">non-linear activation.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1228.1">Inverted residuals</span></strong><span class="koboSpan" id="kobo.1229.1">: In the </span><em class="italic"><span class="koboSpan" id="kobo.1230.1">Residual networks</span></em><span class="koboSpan" id="kobo.1231.1"> section, we introduced the b</span><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.1232.1">ottleneck residual</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.1233.1"> block, where the data flow in the non-shortcut path is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1234.1">input -&gt; 1×1 bottleneck conv -&gt; 3×3 conv -&gt; 1×1 unsampling conv</span></strong><span class="koboSpan" id="kobo.1235.1">. </span><span class="koboSpan" id="kobo.1235.2">In other words, it follows a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1236.1">wide -&gt; narrow -&gt; wide</span></strong><span class="koboSpan" id="kobo.1237.1"> data representation. </span><span class="koboSpan" id="kobo.1237.2">On the other hand, the inverted residual block follows a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1238.1">narrow -&gt; wide -&gt; narrow</span></strong><span class="koboSpan" id="kobo.1239.1"> representation. </span><span class="koboSpan" id="kobo.1239.2">Here, the bottleneck</span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.1240.1"> convolution expands its input with an </span><strong class="bold"><span class="koboSpan" id="kobo.1241.1">expansion </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1242.1">factor</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1244.1">t</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1246.1">The authors argue that the bottlenecks contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor. </span><span class="koboSpan" id="kobo.1246.2">Because of this, they propose having shortcut</span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.1247.1"> connections between the bottleneck </span><span class="No-Break"><span class="koboSpan" id="kobo.1248.1">connections instead.</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1249.1">DSC</span></strong><span class="koboSpan" id="kobo.1250.1">: We already introduced this operation</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.1251.1"> earlier in this chapter. </span><span class="koboSpan" id="kobo.1251.2">MobileNet V3 introduces </span><strong class="bold"><span class="koboSpan" id="kobo.1252.1">H-swish</span></strong><span class="koboSpan" id="kobo.1253.1"> activation in the DSC. </span><span class="koboSpan" id="kobo.1253.2">H-swish resembles the swish function, which</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.1254.1"> we introduced in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1255.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1256.1">. </span><span class="koboSpan" id="kobo.1256.2">The V3 architecture includes alternating ReLU and </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">H-swish activations.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1258.1">SE blocks</span></strong><span class="koboSpan" id="kobo.1259.1">: We’re already familiar with this block. </span><span class="koboSpan" id="kobo.1259.2">The difference here is the </span><strong class="bold"><span class="koboSpan" id="kobo.1260.1">hard sigmoid</span></strong><span class="koboSpan" id="kobo.1261.1"> activation, which approximates the sigmoid but is computationally </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.1262.1">more efficient. </span><span class="koboSpan" id="kobo.1262.2">The module is placed after the expanding depthwise</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.1263.1"> convolution, so the attention can be applied to the largest representation. </span><span class="koboSpan" id="kobo.1263.2">The SE block is a new addition to V3 and was not present </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">in V2.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1265.1">Stride</span></strong> <em class="italic"><span class="koboSpan" id="kobo.1266.1">s</span></em><span class="koboSpan" id="kobo.1267.1">: The block implements downsampling with stride convolutions. </span><span class="koboSpan" id="kobo.1267.2">The shortcut connection</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.1268.1"> exists only </span><span class="No-Break"><span class="koboSpan" id="kobo.1269.1">when </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1270.1">s</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1">=1.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1272.1">MobileNetV3 introduces large and small variations of the network with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1274.1">Both networks start with a stride convolution that downsamples the input from 224×224 </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">to 112×112</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1276.1">The small and large variations have 11 and 15 MBConv </span><span class="No-Break"><span class="koboSpan" id="kobo.1277.1">blocks, respectively</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1278.1">The convolutional phase ends with global average pooling for </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">both networks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1280.1">The small and large networks</span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.1281.1"> have 3 and 5 million </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">parameters, respectively</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1283.1">In the next section, we’ll discuss an improved version of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">MBConv block.</span></span></p>
<h2 id="_idParaDest-90" lang="en-GB"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.1285.1">EfficientNet</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1286.1">EfficientNet</span></strong><span class="koboSpan" id="kobo.1287.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1288.1">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</span></em><span class="koboSpan" id="kobo.1289.1">, </span><a href="https://arxiv.org/abs/1905.11946"><span class="koboSpan" id="kobo.1290.1">https://arxiv.org/abs/1905.11946</span></a><span class="koboSpan" id="kobo.1291.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1292.1">EfficientNetV2: Smaller Models and Faster Training</span></em><span class="koboSpan" id="kobo.1293.1">, </span><a href="https://arxiv.org/abs/2104.00298"><span class="koboSpan" id="kobo.1294.1">https://arxiv.org/abs/2104.00298</span></a><span class="koboSpan" id="kobo.1295.1">) introduces the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.1296.1">compound scaling</span></strong><span class="koboSpan" id="kobo.1297.1">. </span><span class="koboSpan" id="kobo.1297.2">It starts with a small baseline model and then</span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.1298.1"> simultaneously expands</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.1299.1"> it in three directions: depth (more layers), width (more feature maps per layer), and higher input resolution. </span><span class="koboSpan" id="kobo.1299.2">The compound scaling produces a series of new models. </span><span class="koboSpan" id="kobo.1299.3">The EfficientNetV1 baseline model uses the MBConv building block of MobileNetV2. </span><span class="koboSpan" id="kobo.1299.4">EfficientNetV2 introduces the new </span><strong class="bold"><span class="koboSpan" id="kobo.1300.1">fused-MBConv</span></strong><span class="koboSpan" id="kobo.1301.1"> block, which replaces the expanding 1×1 bottleneck</span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.1302.1"> convolution and the 3×3 depthwise convolution of MBConv, with a single expanding 3×3 </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">cross-channel convolution:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer414">
<span class="koboSpan" id="kobo.1304.1"><img alt="Figure 4.31 – Fused-MBConv block" src="image/B19627_04_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1305.1">Figure 4.31 – Fused-MBConv block</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1306.1">The new 3×3 convolution handles both the expanding (with a factor of </span><em class="italic"><span class="koboSpan" id="kobo.1307.1">t</span></em><span class="koboSpan" id="kobo.1308.1">) and the stride (1 </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">or 2).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1310.1">The authors of EfficientNetV2 observed that a CNN, which uses a combination of fused-MBConv and MBConv blocks, trains faster compared to a CNN with MBConv blocks only. </span><span class="koboSpan" id="kobo.1310.2">However, the </span><br/><span class="koboSpan" id="kobo.1311.1">fused-MBConv block is computationally more expensive, compared to the plain MBConv block. </span><span class="koboSpan" id="kobo.1311.2">Because of this, EfficientNetV2 replaces the blocks gradually, starting from the early stages. </span><span class="koboSpan" id="kobo.1311.3">This makes sense because the earlier convolutions use a smaller number of filters (and hence slices), so the memory and computational penalty are less pronounced at this stage. </span><span class="koboSpan" id="kobo.1311.4">Finding the right combination of the two blocks is not trivial, hence the need for compound scaling. </span><span class="koboSpan" id="kobo.1311.5">This process produced multiple models with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">following</span></span><span class="No-Break"><a id="_idIndexMarker606"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1313.1"> properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1314.1">The networks start with a stride convolution that downsamples the </span><span class="No-Break"><span class="koboSpan" id="kobo.1315.1">input twice</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1316.1">The early stages of the main body use fused-MBConv blocks, and the later stages use </span><span class="No-Break"><span class="koboSpan" id="kobo.1317.1">MBConv blocks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1318.1">The convolutional phase ends with global average pooling for </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">all networks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1320.1">The number of parameters ranges between 5.3 million and </span><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">119 million</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1322.1">This concludes our introduction to advanced CNN models. </span><span class="koboSpan" id="kobo.1322.2">We didn’t discuss all the available models, but we focused</span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.1323.1"> on some of the most popular ones. </span><span class="koboSpan" id="kobo.1323.2">I hope that you now have sufficient knowledge to explore new models yourself. </span><span class="koboSpan" id="kobo.1323.3">In the next section, we’ll demonstrate how to use these advanced models in PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">and Keras.</span></span></p>
<h2 id="_idParaDest-91" lang="en-GB"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.1325.1">Using pre-trained models with PyTorch and Keras</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1326.1">Both PyTorch and Keras have a collection of pre-trained ready-to-use models. </span><span class="koboSpan" id="kobo.1326.2">All the models we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1327.1">Advanced network models</span></em><span class="koboSpan" id="kobo.1328.1"> section are available in this way. </span><span class="koboSpan" id="kobo.1328.2">The models are usually pre-trained on classifying the ImageNet dataset and can serve as backbones to various computer vision tasks, as we’ll see in </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1329.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1331.1">We can load a pre-trained</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.1332.1"> model in PyTorch with the</span><a id="_idIndexMarker609"/> <span class="No-Break"><span class="koboSpan" id="kobo.1333.1">following code:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1334.1">
from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights
# With pretrained weights:
model = mobilenet_v3_large(
        weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)
model = mobilenet_v3_large(weights="IMAGENET1K_V1")
# Using no weights:
model = mobilenet_v3_large(weights=None)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1335.1">The weights will be automatically downloaded. </span><span class="koboSpan" id="kobo.1335.2">In addition, we can list all available models and load an arbitrary</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.1336.1"> model using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">following</span></span><span class="No-Break"><a id="_idIndexMarker611"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1"> code:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1339.1">
from torchvision.models import list_models, get_model
# List available models
all_models = list_models()
model = get_model(all_models[0], weights="DEFAULT")</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1340.1">Keras supports</span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.1341.1"> similar functionality. </span><span class="koboSpan" id="kobo.1341.2">We can load a pre-trained model</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.1342.1"> with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1343.1">following code:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1344.1">
from keras.applications.mobilenet_v3 import MobileNetV3Large
model = MobileNetV3Large(weights='imagenet')</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1345.1">These short but very useful code examples conclude </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">this chapter.</span></span></p>
<h1 id="_idParaDest-92" lang="en-GB"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.1347.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1348.1">In this chapter, we introduced CNNs. </span><span class="koboSpan" id="kobo.1348.2">We talked about their main building blocks – convolutional and pooling layers – and we discussed their architecture and features. </span><span class="koboSpan" id="kobo.1348.3">We paid special attention to the different types of convolutions. </span><span class="koboSpan" id="kobo.1348.4">We also demonstrated how to use PyTorch and Keras to implement the CIFAR-10 classification CNN. </span><span class="koboSpan" id="kobo.1348.5">Finally, we discussed some of the most popular CNN models in </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">use today.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1350.1">In the next chapter, we’ll build upon our new-found computer vision knowledge with some exciting additions. </span><span class="koboSpan" id="kobo.1350.2">We’ll discuss how to train networks faster by transferring knowledge from one problem to another. </span><span class="koboSpan" id="kobo.1350.3">We’ll also go beyond simple classification with object detection, or how to find the object’s location on the image. </span><span class="koboSpan" id="kobo.1350.4">We’ll even learn how to segment each pixel of </span><span class="No-Break"><span class="koboSpan" id="kobo.1351.1">an image.</span></span></p>
</div>
</body></html>