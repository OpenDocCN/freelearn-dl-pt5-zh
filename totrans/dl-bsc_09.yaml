- en: 8\. Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a machine learning method based on deep neural networks. You
    can create a deep network by adding layers to the networks we've described so
    far. However, a deep network has problems. This chapter will describe the characteristics,
    problems, and possibilities of deep learning, as well as an overview of current
    deep learning practices.
  prefs: []
  type: TYPE_NORMAL
- en: Making a Network Deeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we have learned a lot about neural networks, including
    the various layers that constitute a neural network, effective techniques used
    in training, CNNs that are especially effective for handling images, and how to
    optimize parameters. These are all important techniques in deep learning. Here,
    we will integrate the techniques we have learned so far to create a deep network.
    Then, we will try our hand at handwritten digit recognition using the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we will create a CNN that has the network architecture shown in *Figure
    8.1*. This network is based on the VGG network, which will be described in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 8.1*, the network is deeper than the networks that we have
    implemented so far. All the convolution layers used here are small 3x3 filters.
    Here, the number of channels becomes larger as the network deepens (as the number
    of channels in a convolution layer increases from 16 in the first layer to 16,
    32, 32, 64, and 64). As you can see, pooling layers are inserted to reduce the
    spatial size of intermediate data gradually, while dropout layers are used for
    the latter fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Deep CNN for handwritten digit recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.1: Deep CNN for handwritten digit recognition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This network uses the "He initializer" to initialize the weights, and Adam
    to update the weight parameters, resulting in the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layers which use small 3×3 filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU as the activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dropout layer used after a fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization is done by Adam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"He initializer" for initial weight values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As these characteristics indicate, the network in *Figure 8.1* uses many neural
    network techniques that we have learned so far. Now, let's use this network for
    training. The result shows that the recognition accuracy of this network is 99.38%
    (final recognition accuracies vary slightly, but this network will generally exceed
    99%).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The source code that implemented the network shown in *Figure 8.1* is located
    at `ch08/deep_convnet.py`. The code for training is provided at `ch08/train_deepnet.py`.
    You can use this code to reproduce the training that will be conducted here. Training
    in a deep network takes a lot of time (probably more than half a day). This book
    provides trained weight parameters in `ch08/deep_conv_net_params.pkl`. The `deep_convnet.py`
    code file provides a feature for loading trained parameters. You can use it as
    required.
  prefs: []
  type: TYPE_NORMAL
- en: 'The error rate of the network shown in *Figure 8.1* is only 0.62%. Here, we
    can see what images were incorrectly recognized. *Figure 8.2* shows the recognition
    error examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: Sample images that were recognized incorrectly – the upper left
    of each image shows the correct label, while the lower right shows the result
    of prediction by this network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.2: Sample images that were recognized incorrectly – the upper left
    of each image shows the correct label, while the lower right shows the result
    of prediction by this network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in *Figure 8.2*, these images are difficult even for us humans to recognize.
    The upper-left image looks like a "0" (the correct answer is "6"), and the one
    next to it certainly seems to be a "5" (the correct answer is "3"). Generally,
    the distinctions between "1" and "7", "0" and "6", and "3" and "5" are difficult.
    These examples explain why they were recognized incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: While this deep CNN is very precise, it recognized images incorrectly in the
    same way as humans would. This also shows us the large potential of a deep CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Recognition Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The website called "What is the class of this image?" *(Rodrigo Benenson''s
    blog* "*Classification datasets results*" ([http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)))
    ranks the recognition accuracies for various datasets by the techniques published
    in the related literature (*Figure 8.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Ranking techniques for the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.3: Ranking techniques for the MNIST dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.3* is cited from reference, *Rodrigo Benenson''s blog* "*Classification
    datasets results*" ([http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html))
    as of June 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ranking shown in *Figure 8.3*, keywords such as "neural networks," "deep,"
    and "convolutional" are noticeable. Many high-ranked techniques are CNN-based.
    As of June 2016, the highest recognition accuracy for the MNIST dataset is 99.79%
    (an error rate of 0.21%), and the technique is also CNN-based (*Li Wan, Matthew
    Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus (2013): Regularization of Neural
    Networks using DropConnect. In Sanjoy Dasgupta & David McAllester, eds. Proceedings
    of the 30th International Conference on Machine Learning (ICML2013). JMLR Workshop
    and Conference Proceedings, 1058 – 1066*). The CNN that is used there is not very
    deep (two convolution layers and two fully connected layers).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the MNIST dataset, the highest accuracy can be obtained immediately, even
    if the network is not very deep. For a relatively simple problem such as handwritten
    digit recognition, the representation of the network does not need to be very
    high. Therefore, adding layers is not very beneficial. In the large-scale general
    object recognition process, adding layers greatly improves recognition accuracy
    because it is a complicated problem.
  prefs: []
  type: TYPE_NORMAL
- en: By examining the aforementioned high-ranked techniques, we can find techniques
    and tips for further improving recognition accuracy. For example, we can see that
    ensemble learning, learning rate decay, and **data augmentation** contribute to
    the improvement of recognition accuracy. Data augmentation is a simple but particularly
    effective method for improving recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data augmentation uses an algorithm to expand input images (training images)
    artificially. As shown in *Figure 8.4*, it adds the images by slightly changing
    the input images with rotation or vertical/horizontal movement. This is especially
    effective when the number of images in the dataset is limited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Sample data augmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.4: Sample data augmentation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can use data augmentation to expand images in various ways, other than the
    modifications shown in *Figure 8.4*. For example, you can cut out part of an image
    (or crop) or reverse an image horizontally (called flipping, though this is only
    effective when the symmetry of the image does not need to be considered). For
    ordinary images, changing their appearance (e.g., by adding brightness and scaling
    them up or down, is also effective. If you can use data augmentation to increase
    the number of training images, you can improve the recognition accuracy by using
    deep learning. This may seem a simple trick, but it often brings good results.
    We will not implement data augmentation here. Since implementing this is easy,
    please try it for yourself if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for a Deeper Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is still much that is not known about the importance of making a network
    deeper. Although theoretical findings are insufficient now, past research and
    experiments can explain some things (rather intuitively). This section will provide
    some data and explanations that support the importance of "making a network deeper."
  prefs: []
  type: TYPE_NORMAL
- en: First, the results from competitions surrounding large-scale image recognition
    such as ILSVRC show the importance of "making a network deeper" (please see the
    next section for details). They indicate that many of the recent high-ranked techniques
    are based on deep learning and that the networks tend to go deeper. The deeper
    the network, the better the recognition performance.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of this is that you can reduce the number of parameters
    in the network. When a network is deeper, it can achieve similar (or higher) representation
    with fewer parameters. This is easy to understand when you consider the filter
    size in a convolution operation. *Figure 8.5* shows a convolution layer with a
    5x5 filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note the area of the input data each node of the output data is calculated
    in. Of course, each output node is based on the 5x5 area of the input data in
    the example shown in *Figure 8.5*. Now, let''s think about a case where 3x3 convolution
    operations are repeated twice, as shown in *Figure 8.6*. In this case, intermediate
    data is based on a 3x3 area for each output node. So, which area of the previous
    input data is the 3x3 area of intermediate data based on? When you look at *Figure
    8.6* carefully, you will notice that it is based on a 5×5 area. Thus, the output
    data of *Figure 8.6* "looks at" a 5×5 area of input data for calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Example of a 5x5 convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.5: Example of a 5x5 convolution operation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 8:6: Example of when 3x3 convolution operations are repeated twice'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8:6: Example of when 3x3 convolution operations are repeated twice'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The area of one 5x5 convolution operation is equivalent to that of two 3x3 convolution
    operations. The former uses 25 parameters (5x5), while the latter uses 18 parameters
    (2x3x3) in total. Thus, multiple convolution layers reduce the number of parameters.
    As the network gets deeper, the reduced number of parameters becomes larger. For
    example, when 3x3 convolution operations are repeated three times, the number
    of parameters is 27 in total. To "look at" the same area with one convolution
    operation, a 7x7 filter is required, which means that the number of parameters
    goes up to 49.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The advantage of making a network deeper by applying a small filter several
    times is that it can reduce the number of parameters and expand the **receptive
    field** (a local space area that changes neurons). When you add layers, an activation
    function, such as ReLU, is placed between convolution layers, resulting in an
    improved network representation. This is because the activation function applies
    a "nonlinear" force to the network. Multiple nonlinear functions enable more complicated
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Training efficiency is another advantage of making a network deeper. A deeper
    network can reduce training data and conduct training quickly. You can understand
    this intuitively by remembering the description provided in *Visualizing a CNN*
    section in *Chapter 7*, *Convolutional Neural Networks*. In that section, you
    learned that the convolution layers in a CNN extract information hierarchically.
    In the front convolution layer, neurons react to simple shapes such as edges.
    As a layer becomes deeper, neurons react to hierarchically more complicated shapes,
    such as textures and object parts.
  prefs: []
  type: TYPE_NORMAL
- en: With such a hierarchical structure of a network in mind, consider the problem
    of recognizing a "dog." To solve this problem in a shallow network, convolution
    layers must "understand" many characteristics of a dog at one time. There are
    various types of dogs, and what they look like varies, depending on the environment
    in which the image was shot. Therefore, understanding the characteristics of a
    dog requires varied training data and a lot of time in training.
  prefs: []
  type: TYPE_NORMAL
- en: However, you can divide the problem to learn hierarchically by making a network
    deeper. Then, the problem for each layer to learn becomes simpler. For example,
    the first layer can concentrate on learning edges. Thus, the network can learn
    efficiently with a small amount of training data. This is because the number of
    images that contain edges is larger than that of images of a dog, and the pattern
    of an edge is simpler than that of a dog.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important that you can pass information hierarchically by making
    a network deeper. For example, the layer next to the one that extracted edges
    can use edge information, so we can expect it to learn more advanced patterns
    efficiently. In short, by making a network deeper, you can divide the problem
    for each layer to learn into "simple problems that are easy to solve" so that
    you can expect efficient training.
  prefs: []
  type: TYPE_NORMAL
- en: This is the explanation that supports the importance of "making a network deeper."
    Please note that deeper networks in recent years have been provided by new techniques
    and environments, such as big data and computer power, which enable correct training
    in a deep network.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief History of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is said that deep learning started to draw a lot of attention in the competition
    of large-scale image recognition due to the **ImageNet Large Scale Visual Recognition
    Challenge** (**ILSRVC**), which was held in 2012\. In the competition, a deep
    learning technique called AlexNet achieved an overwhelming win, overturning the
    traditional approaches to image recognition. Since deep learning launched a counterattack
    in 2012, it has always played the leading role in subsequent competitions. Here,
    we will look at the current trend of deep learning around the competition of large-scale
    image recognition, known as ILSVRC.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ImageNet (*J. Deng, W. Dong, R. Socher, L.J. Li, Kai Li, and Li Fei-Fei (2009):
    ImageNet: A large-scale hierarchical image database*. In IEEE Conference on *Computer
    Vision and Pattern Recognition, 2009\. CVPR 2009\. 248 – 255\. DOI*: ([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)))
    is a dataset that contains more than 1 million images. As shown in *Figure 8.7*,
    it contains various types of images, and each image is associated with a label
    (class name). An image recognition competition called ILSVRC is held every year
    using this huge dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: Sample data in the large-scale ImageNet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.7: Sample data in the large-scale ImageNet dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.7* is cited from reference, *J. Deng, W. Dong, R. Socher, L.J. Li,
    Kai Li, and Li Fei-Fei (2009): ImageNet: A large-scale hierarchical image database*.
    In IEEE Conference on *Computer Vision and Pattern Recognition, 2009\. CVPR 2009\.
    248 – 255\. DOI:* ([http://dx.doi.org/10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ILSVRC competition provides some test items, and one of them is "classification"
    (in the "classification" division, 1,000 classes are classified to compete in
    recognition accuracy). *Figure 8.8* shows the results of the winning teams for
    ILSVRC''s classification division since 2010\. Here, a classification is regarded
    as "correct" if the top 5 predictions contain the correct class. The following
    bar graphs show the error rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: The results of the winning teams in ILSVRC – the vertical axis
    shows error rates,'
  prefs: []
  type: TYPE_NORMAL
- en: while the horizontal axis shows years. Team names or technique names are shown
    in the
  prefs: []
  type: TYPE_NORMAL
- en: parentheses on the horizontal axis.
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.8: The results of the winning teams in ILSVRC – the vertical axis
    shows error rates, while the horizontal axis shows years. Team names or technique
    names are shown in the parentheses on the horizontal axis.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Please note from the preceding graph that deep learning techniques have always
    been on top since 2012\. Actually, we can see that, in 2012, AlexNet significantly
    reduced the error rate. Since then, deep learning techniques have steadily improved
    in terms of accuracy. This was especially apparent with ResNet in 2015, which
    was a deep network with more than 150 layers and had reduced the error rate to
    3.5%. It is even said that this result exceeded the recognition capability of
    ordinary humans.
  prefs: []
  type: TYPE_NORMAL
- en: Among the deep learning networks that have achieved great results for the past
    several years, VGG, GoogLeNet, and ResNet are the most famous. You will come across
    them at various places relevant to deep learning. I will introduce these three
    famous networks briefly next.
  prefs: []
  type: TYPE_NORMAL
- en: VGG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'VGG is a "basic" CNN that consists of convolution layers and pooling layers.
    As shown in *Figure 8.9*, it can have as many as 16 (or 19) layers with weights
    (convolution layers and fully connected layers) to make itself deep and is sometimes
    called "VGG16" or "VGG19" based on the number of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9: VGG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.9: VGG'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.9* is cited from reference, *Karen Simonyan and Andrew Zisserman
    (2014): Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556[cs]
    (September 2014)*.'
  prefs: []
  type: TYPE_NORMAL
- en: VGG contains consecutive convolution layers with a small 3x3 filter. As shown
    in the preceding image, two or four consecutive convolution layers and a pooling
    layer halve the size, and this process is repeated. Finally, the result is provided
    via fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VGG won second prize in the 2014 competition (GoogLeNet, which is described
    next, won in 2014). Its performance was not as good as the first-place network,
    but many engineers prefer to use VGG-based networks because they are very simple
    in structure and versatile.
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Figure 8.10* shows the network architecture for GoogLeNet. The rectangles
    represent the various layers, such as convolution and pooling layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: GoogLeNet'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.10: GoogLeNet'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.10* and *Figure 8.11* are cited from *Christian Szegedy et al. (2015):
    Going Deeper With Convolutions. In The IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Its network architecture seems very complicated when you look at it, but it
    is basically the same as that of a CNN. What is distinctive about GoogLeNet is
    that the network not only has depth in the vertical direction but also in the
    horizontal direction (spread).
  prefs: []
  type: TYPE_NORMAL
- en: 'GoogLeNet has "width" in the horizontal direction. It is called an "inception
    architecture" and is based on the structure shown in *Figure 8.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11: Inception architecture of GoogLeNet'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.11: Inception architecture of GoogLeNet'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in *Figure 8.11*, the inception architecture applies multiple filters
    of different sizes (and pooling) and combines the results. Using this inception
    architecture as one building block (component) is the main characteristic of GoogLeNet.
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet uses convolution layers with a 1x1 filter in many places. This 1x1
    convolution operation reduces the size in the channel direction to reduce the
    number of parameters and accelerate processing.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ResNet (*Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015): Deep
    Residual Learning for Image Recognition. arXiv:1512.03385[cs] (December 2015)*)
    is a network developed by a team at Microsoft. It is characterized by a "mechanism"
    that can make the network deeper than ever.'
  prefs: []
  type: TYPE_NORMAL
- en: Making a network deeper is important to improve its performance. However, when
    a network becomes too deep, deep learning fails and the final performance is often
    poor. To solve this problem, ResNet introduced a "skip architecture" (also called
    "shortcut" or "bypass"). By introducing this skip architecture, performance can
    be improved as the network becomes deeper (though there is a limit to permissible
    depth).
  prefs: []
  type: TYPE_NORMAL
- en: 'The skip architecture skips convolution layers in the input data to add the
    input data to the output, as shown in *Figure 8.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: Components of ResNet – the "weight layer" here indicates a convolution
    layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.12: Components of ResNet – the "weight layer" here indicates a convolution
    layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.12* and *Figure 8.13* are cited from reference, *Kaiming He, Xiangyu
    Zhang, Shaoqing Ren, and Jian Sun (2015): Deep Residual Learning for Image Recognition.
    arXiv:1512.03385[cs] (December 2015)*.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8.12*, the input, *x*, is connected to the output by skipping two
    consecutive convolution layers. The output of two convolution layers is originally
    *F(x)*, while the skip architecture changes it to *F(x) + x*.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting this skip architecture enables efficient learning, even when the network
    is deep. This is because the skip architecture transmits signals without decay
    during backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The skip architecture only passes input data "as it is." In backward propagation,
    it also passes the gradients from the upper stream "as they are" to the lower
    stream without them being changed. Therefore, you don't need to be worried about
    the gradients becoming small (or too large) with the skip architecture. You can
    expect "meaningful gradients" to be transmitted to the front layers. You can also
    expect the skip architecture to alleviate a traditional gradient vanishing problem
    that reduces gradients as the network becomes deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNet is based on the VGG network we described earlier and adopts the skip
    architecture to make the network deeper. *Figure 8.13* shows the result of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: ResNet – blocks support 3x3 convolution layers. Its characteristic
    is the skip'
  prefs: []
  type: TYPE_NORMAL
- en: architecture, which skips layers.
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.13: ResNet – blocks support 3x3 convolution layers. Its characteristic
    is the skip architecture, which skips layers.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in *Figure 8.13*, ResNet skips two convolution layers to make the network
    deeper. Experiments have shown that recognition accuracy continues to improve,
    even when the network contains 150 or more layers. In the ILSVRC competition,
    it achieved an amazing result of 3.5% in terms of error rate (the percentage of
    correct classes that were not included in the top 5 predictions).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Weight data that's trained by using the huge ImageNet dataset is often used
    effectively. This is called **transfer learning**. Part of the trained weights
    is copied to another neural network for fine-tuning. For example, a network that
    has the same structure as VGG is provided. Trained weights are used as initial
    values, and fine-tuning is conducted for a new dataset. Transfer learning is especially
    effective when you have a few datasets at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Big data and large-scale networks require massive operations in deep learning.
    We have used CPUs for calculations so far, but CPUs alone are not sufficient to
    tackle deep learning. In fact, many deep learning frameworks support **Graphics
    Processing Units** (**GPUs**) to process a large number of operations quickly.
    Recent frameworks are starting to support distributed learning by using multiple
    GPUs or machines. This section describes accelerating calculations in deep learning.
    Our implementations of deep learning ended in section 8.1\. We will not implement
    the acceleration (such as support of GPUs) described here.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges to Overcome
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before discussing the acceleration of deep learning, let''s see what processes
    take time in deep learning. The pie charts in *Figure 8.14* show the time spent
    on each class in the forward processing of AlexNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: Percentage of time that each layer spends in the forward processing
    of AlexNet – the left-hand chart shows GPU time, while the right-hand one shows
    CPU time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.14: Percentage of time that each layer spends in the forward processing
    of AlexNet – the left-hand chart shows GPU time, while the right-hand one shows
    CPU time'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, "conv" indicates a convolution layer, "pool" indicates a pooling layer,
    "fc" indicates a fully connected layer, and "norm" indicates a normalization layer
    (cited from *Jia Yangqing (2014): Learning Semantic Image Representations at a
    Large Scale. PhD thesis, EECS Department, University of California, Berkeley,
    May 2014*, ([http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html))).'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, convolution layers spend a lot of time in AlexNet. Actually,
    the total processing time in convolution layers reaches 95% of GPU time and 89%
    of CPU time! Therefore, conducting fast and efficient operations in convolution
    layers is the main challenge of deep learning. *Figure 8.14* shows the results
    in the inference phase, but convolution layers spend a lot of time in the training
    phase as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As explained in the *Convolution Layers* topic in *Chapter 7*, *Convolutional
    Neural Networks*, operations in convolution layers are basically "multiply-accumulate
    operations." Therefore, accelerating deep learning depends on how massive "multiply-accumulate
    operations" are calculated quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Using GPUs for Acceleration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Originally, GPUs were exclusively used for graphics. Recently, they have been
    used for general numerical calculations, as well as graphics processing. Because
    GPUs can conduct parallel arithmetic operations quickly, GPU computing uses its
    overwhelming power for various purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning requires massive multiply-accumulate operations (or products
    of large matrices). GPUs are good at such massive parallel operations, while CPUs
    are good at continuous and complicated calculations. You can use a GPU to accelerate
    deep learning operations surprisingly compared to using only a CPU. *Figure 8.15*
    compares the time that AlexNet took for learning between a CPU and a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15: Comparing the time that AlexNet took for learning between a
    "16-core Xeon CPU"'
  prefs: []
  type: TYPE_NORMAL
- en: and a "Titan series" GPU
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.15: Comparing the time that AlexNet took for learning between a "16-core
    Xeon CPU" and a "Titan series" GPU'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.15* is cited from reference, *NVIDIA blog "NVIDIA Propels Deep Learning
    with TITAN X, New DIGITS Training System and DevBox"* ([https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/](https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/)).'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the CPU took more than 40 days, while the GPU took only 6 days.
    We can also see that using the cuDNN library, which is optimized for deep learning,
    accelerates the training further.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are mainly provided by two companies, NVIDIA and AMD. Although you can
    use both of their GPUs for general arithmetic operations, NVIDIA's GPUs are more
    "familiar" with deep learning. Actually, many deep learning frameworks can benefit
    only from NVIDIA's GPUs. This is because CUDA, which is an integrated development
    environment for GPU computing provided by NVIDIA, is used in deep learning frameworks.
    cuDNN, which can be seen in *Figure 8.15*, is a library that runs on CUDA in which
    the functions optimized for deep learning are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We used `im2col` to convert the operations in a convolution layer into the products
    of large matrices. Implementing this `im2col` method is suitable for GPUs. GPUs
    are good at calculating a large batch at a stretch rather than calculating small
    batches one by one. Using `im2col` to calculate the products of huge matrices
    makes it easy to exhibit a GPU's real power.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can accelerate deep learning operations by using a GPU, but a deep network
    still requires several days or weeks for training. As we have seen so far, deep
    learning involves lots of trial and error. You must try many things to create
    a good network. Naturally, you want to reduce the time required for training as
    much as possible. Then, scaling deep learning out or "distributed training" becomes
    important.
  prefs: []
  type: TYPE_NORMAL
- en: To further accelerate the calculations required for deep learning, you may want
    to distribute them among multiple GPUs or machines. Now, some deep learning frameworks
    support distributed training by multiple GPUs or machines. Among them, Google's
    TensorFlow and Microsoft's **Computational Network Toolkit** (**CNTK**) have been
    developed to focus on distributed training. Based on low-delay and high-throughput
    networks in huge data centers, distributed training by these frameworks achieves
    surprising results.
  prefs: []
  type: TYPE_NORMAL
- en: How much can distributed training accelerate deep learning? The answer is that
    the larger the number of GPUs, the faster the training speed. In fact, 100 GPUs
    (a total of 100 GPUs installed on multiple machines) achieves a 56-fold speedup
    compared with one GPU. This means that training that usually takes 7 days is completed
    in only 3 hours, for example, and indicates the surprising effect of distributed
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '"How to distribute calculations" in distributed training is a very difficult
    problem. It contains many problems that are not easy to solve, such as communication
    and data synchronization between machines. You can leave such difficult problems
    to excellent frameworks such as TensorFlow. Here, we will not discuss the details
    of distributed training. For the technical details of distributed training, please
    see the technical paper (white paper) about TensorFlow (*Mart í n Abadi et al.
    (2016): TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
    Systems. arXiv:1603.04467[cs] (March 2016)*).'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the Bit Number for Arithmetic Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory space and bus bandwidth, as well as computational complexity, can be
    bottlenecks in accelerating deep learning. For memory space, a large number of
    weight parameters and intermediate data must be stored in memory. For bus bandwidth,
    a bottleneck occurs when the data that flows through the GPU (or CPU) bus increases,
    exceeding a limit. In these cases, you want the bit number of the data flowing
    in the network to be as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: A computer mainly uses 64- or 32-bit floating-point numbers to represent real
    numbers. Using many bits to represent a number reduces the influence of the error
    at numerical calculation but increases the processing cost and memory usage, placing
    a load on the bus bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: From what we know about deep learning regarding numerical precision (how many
    bits are used to represent a numeric value), it does not need very high precision.
    This is one of the most important characteristics of a neural network due to its
    robustness. The robustness here means that, for example, the output result will
    not change in a neural network, even if the input images contain a small amount
    of noise. Think of it as a small influence on the output result because of the
    robustness, even if the data flowing in a network is "deteriorated."
  prefs: []
  type: TYPE_NORMAL
- en: 'A computer usually uses 32-bit single-precision floating-point representations
    or 64-bit double-precision floating-point representations to represent a decimal.
    Experiments have shown that 16-bit `float`) are sufficient in deep learning (*Suyog
    Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan (2015): Deep
    learning with limited numerical precision. CoRR, abs/1502.02551 392 (2015)*).
    Actually, the Pascal architecture used for NVIDIA''s generation GPUs supports
    the operation of half-precision floating-point numbers. It is thought that the
    half format will be used as the standard in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NVIDIA's Maxwell generation of GPUs supported the storage of half-accuracy floating-point
    numbers (to maintain data), but it did not conduct 16-bit operations. The next-generation
    Pascal architecture conducts 16-bit operations as well. We can expect that only
    using half-accuracy floating-point numbers for calculations will accelerate processing
    so that it's around twice as fast as a previous-generation GPU.
  prefs: []
  type: TYPE_NORMAL
- en: We haven't covered numerical precision in the preceding implementations of deep
    learning. Python generally uses 64-bit floating-point numbers. NumPy provides
    a 16-bit half-accuracy floating-point data type (however, it is used only for
    storage, not for operations). We can easily show that using NumPy's half-accuracy
    floating-point numbers do not reduce recognition accuracy. If you are interested,
    please see `ch08/half_float_network.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some research has been conducted into reducing the bit number in deep learning.
    In recent research, a technique called a "binarized neural network" was proposed
    (*Matthieu Courbariaux and Yoshua Bengio (2016): Binarized Neural Networks: Training
    Deep Neural Networks with Weights and Activations Constrained to +1 or -1\. arXiv
    preprint arXiv:1602.02830 (2016)*). It represents the weights and intermediate
    data by 1 bit. Reducing the number of bits to accelerate deep learning is a topic
    we should keep our eyes on. It is especially important when we''re thinking of
    using deep learning for embedded devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Uses of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example of using deep learning, we have mainly discussed image classification,
    such as handwritten digit recognition, which is called "object recognition." However,
    we can apply deep learning to many problems other than object recognition. Deep
    learning demonstrates excellent performance for many problems, such as image recognition,
    sound (speech recognition), and natural language processing. This section will
    introduce what deep learning can do (its applications) in the computer vision
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object detection identifies the positions of objects in images and classifies
    them. Object detection is more difficult than object recognition. While object
    recognition targets the entire image, object detection must identify the positions
    of classes in an image, and multiple objects may exist.
  prefs: []
  type: TYPE_NORMAL
- en: Some CNN-based techniques have been proposed for object detection. They demonstrate
    excellent performance, which indicates that deep learning is also effective for
    object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among CNN-based object detection techniques, a technique called R-CNN (*Ross
    Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik (2014): Rich Feature
    Hierarchies for Accurate Object Detection and Semantic Segmentation. In 580 –
    587*) is famous. *Figure 8.16* shows the process flow of R-CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: Process flow of R-CNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.16: Process flow of R-CNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.16* is cited from reference, *Ross Girshick, Jeff Donahue, Trevor
    Darrell, and Jitendra Malik (2014): Rich Feature Hierarchies for Accurate Object
    Detection and Semantic Segmentation. In 580 – 587*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8.16*, note the *2\. Extract region proposals* and *3\. Compute
    CNN features* sections. The first technique detects the areas that seem to be
    objects (in some way) and then applies a CNN to the extracted areas to classify
    them. R-CNN converts an image into squares and uses **support vector machines**
    (**SVMs**) for classification. Its actual process flow is slightly complicated
    but mainly consists of the aforementioned processes: the extraction of candidate
    regions and to compute CNN features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the "Extract region proposals" process of R-CNN, candidates for objects
    are detected, and this is where various techniques that have been developed in
    computer vision can be used. In the paper about R-CNN, a technique called selective
    search is used. Recently, a technique called "Faster R-CNN" (*Shaoqing Ren, Kaiming
    He, Ross Girshick, and Jian Sun (2015): Faster R-CNN: Towards Real-Time Object
    Detection with Region Proposal Networks. In C. Cortes, N. D. Lawrence, D. D. Lee,
    M. Sugiyama, & R. Garnett, eds. Advances in Neural Information Processing Systems
    28\. Curran Associates, Inc., 91 – 99*) has been proposed. It even uses CNNs to
    extract region proposals. Faster R-CNN uses one CNN for the entire process, which
    enables fast processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Segmentation classifies an image on a pixel basis. It learns by using training
    data where objects are colored on a pixel basis and classifies all the pixels
    of an input image during inference. The neural networks we've implemented so far
    classify the entire image. So, how can we classify it on a pixel basis?
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest method of performing segmentation with a neural network is to
    make a prediction for each pixel. For example, you can provide a network that
    classifies a pixel at the center of a rectangular area to make a prediction for
    all the pixels. As you can see, this requires as many forward processes as the
    number of pixels, thus taking a lot of time to complete (the problem being that
    convolution operations re-calculate many areas uselessly). To reduce such useless
    calculations, a technique called a **Fully Convolutional Network** (**FCN**) has
    been proposed (*Jonathan Long, Evan Shelhamer, and Trevor Darrell (2015): Fully
    Convolutional Networks for Semantic Segmentation. In The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*). It classifies all the pixels in one forward
    process (see *Figure 8.20*).'
  prefs: []
  type: TYPE_NORMAL
- en: A FCN is a network that consists only of convolution layers. While an ordinary
    CNN contains fully connected layers, a FCN replaces fully connected layers with
    *convolution layers that play the same role*. In fully connected layers in a network
    that's used in object recognition, the space volume of the intermediate data is
    processed as nodes arranged in a line. On the other hand, in a network that consists
    only of convolution layers, the space volume can be maintained during processing
    until the last output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main characteristic of a FCN is that the space size is expanded at the
    end. This expansion can enlarge shrunk intermediate data so that it''s the same
    size as the input image all at once. The expansion at the end of an FCN is an
    expansion by bi-linear interpolation (bi-linear expansion). An FCN uses deconvolution
    to conduct the bi-linear expansion (for details, see the paper (*Jonathan Long,
    Evan Shelhamer, and Trevor Darrell (2015): Fully Convolutional Networks for Semantic
    Segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)* about FCN).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a fully connected layer, the output is connected to all the inputs. You can
    also create a connection that's the same structure in a convolution layer. For
    example, a fully connected layer whose input data size is 32x10x10 (the number
    of channels is 32, the height is 10, and the width is 10) can be replaced with
    a convolution layer whose filter size is 32x10x10\. If the fully connected layer
    has 100 output nodes, the convolution layer can achieve completely the same processing
    by providing 100 of the 32x10x10 filters. In this way, a fully connected layer
    can be replaced with a convolution layer that conducts equivalent processing.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Image Captions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is some interesting research being conducted that combines natural language
    and computer vision. When an image is provided, the text explaining the image
    (the image caption) is automatically generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an image of a motorcycle from a dirt bike competition could include
    the caption: "A person riding a motorcycle on a dirt road" (this text is automatically
    generated from the image). It is surprising that the system even "understands"
    that it is on a dirt road and that a person is riding a motorcycle.'
  prefs: []
  type: TYPE_NORMAL
- en: A model called **Neural Image Caption** (**NIC**) is typically used to generate
    image captions for deep learning. NIC consists of a deep CNN and a **Recurrent
    Neural Network** (**RNN**) for handling natural language. An RNN has recursive
    connections and is often used for sequential data such as natural language and
    time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'NIC uses CNN to extract the features from an image and passes them to the RNN.
    The RNN uses the features extracted by the CNN as initial values to generate a
    text "recursively." We will not discuss the technical details here. Basically,
    NIC has a simple architecture that combines two neural networks: a CNN and an
    RNN. It can generate surprisingly precise image captions. Handling various types
    of information, such as images and natural language, is called **multi-modal processing**.
    Multi-modal processing has gained a lot of attention in recent years:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The R in RNN stands for recurrent. "Recurrent" indicates a neural network's
    recurrent network architecture. Because of the recurrent architecture, the RNN
    is affected by the information generated before it – in other words, it remembers
    past information. This is the main characteristic of an RNN. For example, after
    generating the word "I," it is affected by the word and generates the next word
    "am." Then, it is affected by the words "I am" that were previously generated
    and generates the word "sleeping." For continuous data such as natural language
    and time-series data, the RNN behaves as if it remembered past information.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is now being used in various fields, as well as in the traditional
    fields. This section describes the possibilities of deep learning and some research
    that shows the future of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Image Styles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is research being conducted that uses deep learning to "draw" a picture
    as an artist would. One popular use case of neural networks is to create a new
    image based on two provided images. One of them is called a "content image," while
    the other is called a "style image." A new image is created based on these two
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In one example, you can specify Van Gogh''s painting style as the style that
    will be applied to the content image, deep learning draws a new picture, as specified.
    This research was published in the paper "A Neural Algorithm of Artistic Style"
    (*Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge (2015): A Neural Algorithm
    of Artistic Style. arXiv:1508.06576[cs, q-bio] (August 2015)*) and received a
    lot of attention all over the world as soon as it was published.'
  prefs: []
  type: TYPE_NORMAL
- en: Roughly speaking, in the technique, the intermediate data in the network learn
    so that it approaches the intermediate data of the "content image." By doing so,
    the input image can be converted so that it is similar in shape to the content
    image. To absorb a style from the "style image," the concept of a style matrix
    is introduced. By training so that the gap of the style matrix is small, the input
    image can approach Van Gogh's style.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding example of image style transfer required two images to generate
    a new image. On the other hand, some research has tried to generate new images
    without requiring any images (the technique trains by using many images beforehand
    but needs no images to "draw" a new image.) For example, you can use deep learning
    to generate the image of a "bedroom" from scratch
  prefs: []
  type: TYPE_NORMAL
- en: They may seem to be real photographs, but they were newly generated by a DCGAN.
    The images that were generated by the DCGAN are images that nobody has ever seen
    (those that do not exist in the training data) and were newly created from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: When a DCGAN generates images that look like real ones, it creates a model of
    the process where the images were generated. The model learns by using many images
    (such as those of bedrooms). After training finishes, you can use the model to
    generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: 'DCGANs use deep learning. The main point of the DCGAN technique is that it
    uses two neural networks: a generator and a discriminator. The generator generates
    an image that seems real, while the discriminator determines whether it is real,
    that is, whether it was generated by the generator or whether it was really photographed.
    In this way, two networks are trained by making them compete against each other.'
  prefs: []
  type: TYPE_NORMAL
- en: The generator learns a more elaborate technique of creating fake images, while
    the discriminator grows like an appraiser who can detect fakes with higher precision.
    What is interesting is that in a technology called a **Generative Adversarial
    Network** (**GAN**), both of them grow through competition. Finally, the generator
    that has grown through competition can draw images that look real (or may grow
    even more).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The machine learning problems that we have seen so far are called **supervised
    learning** problems. They use a dataset that contains image data and labels in
    pairs, such as in handwritten digit recognition. Meanwhile, label data is not
    provided in the problem here. Only images (a set of images) are provided. This
    is called **unsupervised learning**. Unsupervised learning has been studied for
    a relatively long time (**Deep Belief Networks** and **Deep Boltzmann Machines**
    are famous), but it seems that these days, it is not being researched very actively.
    Since techniques using deep learning, such as DCGANs, are attracting more and
    more attention, it is expected that unsupervised learning will be developed further
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Automated Driving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '"Automated driving" technology, in which a computer drives a car instead of
    a human, is likely to be realized soon. IT companies, universities, and research
    institutions, as well as car manufacturers, are competing to realize automated
    driving. This can only happen when various technologies such as path plan technology,
    which determines a traffic route, and sensing technology, including cameras and
    lasers, are combined. It is said that the technology used to recognize the surrounding
    environment properly is the most important. It is very difficult to recognize
    an environment that changes every moment of every day, as well as the cars and
    people that move around freely.'
  prefs: []
  type: TYPE_NORMAL
- en: If the system can properly recognize the travel area robustly and reliably,
    even in various environments, automated driving may be realized in the near future—a
    task for which deep learning should prove invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a CNN-based network called SegNet (*Vijay Badrinarayanan, Kendall,
    and Roberto Cipolla (2015): SegNet: A Deep Convolutional Encoder-Decoder Architecture
    for Image Segmentation. arXiv preprint arXiv:1511.00561 (2015)*) can recognize
    the road environment accurately, as shown in *Figure 8.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: Example of segmenting an image by using deep learning – the
    road, cars, buildings, and sidewalks are recognized accurately'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.17: Example of segmenting an image by using deep learning – the road,
    cars, buildings, and sidewalks are recognized accurately'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.17* is cited from reference, *SegNet Demo page* ([http://mi.eng.cam.ac.uk/projects/segnet/](http://mi.eng.cam.ac.uk/projects/segnet/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation (pixel-level evaluation) is conducted for the input image, as shown
    in *Figure 8.17*. The result indicates that the road, buildings, sidewalks, trees,
    cars, and motorcycles are distinguished somewhat accurately. If deep learning
    improves the accuracy and speed of these recognition technologies from now on,
    automated driving may be put into practical use in the not too distant future.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Networks (Reinforcement Learning)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a research field called **reinforcement learning** in which computers
    learn independently through trial and error, just as humans learn how to ride
    a bicycle, for example. This is different from "supervised learning," where a
    "supervisor" teaches face to face.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic framework of reinforcement learning is that an agent selects actions,
    depending on the situation of the environment, and its actions change the environment.
    After taking an action, the environment offers the agent some reward. The purpose
    of reinforcement learning is to determine the action policy of the agent so that
    it can obtain a better reward, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: Basic framework of reinforcement learning – the agent learns
    independently'
  prefs: []
  type: TYPE_NORMAL
- en: to obtain a better reward
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.18: Basic framework of reinforcement learning – the agent learns independently
    to obtain a better reward'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The diagram in *Figure 8.18* shows the basic framework of reinforcement learning.
    Note that the reward is not labeled data, as it is in supervised learning. For
    example, in the video game "Super Mario Brothers," the exact quantity of rewards
    you gain by moving Mario to the right is not necessarily clear. In that case,
    the "prospective" reward must be determined by clear indicators such as the game
    scores (obtaining coins, defeating enemies, and so on) and game-over logic. In
    supervised learning, each action can be evaluated correctly by the "supervisor."
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Deep Q-Network** (**DQN**) is a reinforcement learning technique (*Volodymyr
    Mnih et al (2015): Human-level control through deep reinforcement learning. Nature
    518, 7540 (2015), 529 – 533*) that uses deep learning. It is based on the algorithm
    of reinforcement learning called Q-learning. Q-learning determines a function
    called the optimal action-value function to determine the optimal action. A DQN
    uses deep learning (CNNs) to approximate the function.'
  prefs: []
  type: TYPE_NORMAL
- en: Some research has shown that DQNs can learn video games automatically to achieve
    more successful play than humans. As shown in *Figure 8.19*, a CNN, when used
    in a DQN, receives four consecutive frames of game images as input and outputs
    the "value" of the motion of the game controller (the movement of the joystick
    and the button operation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, when a video game was learned by the network, the state of the
    game (such as the positions of the characters) was usually extracted and provided
    in advance. Meanwhile, the DQN receives only the images of a video game as input
    data, as shown in *Figure 8.19*. This is what is noteworthy in a DQN and highly
    improves its applicability. This is because you do not need to change the settings
    for each game, and you only need to provide game images to the DQN. In fact, DQNs
    have learned many games, such as "Pac-Man" and "Atari 2600" with the same configuration
    and achieved better results than humans:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19: Using a Deep Q-Network to learn the operations of a video game.
    Here, the'
  prefs: []
  type: TYPE_NORMAL
- en: network receives the images of a video game as an input and learns the operation
    of the game
  prefs: []
  type: TYPE_NORMAL
- en: controller (joystick) through trial and error
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig08_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.19: Using a Deep Q-Network to learn the operations of a video game.
    Here, the network receives the images of a video game as an input and learns the
    operation of the game controller (joystick) through trial and error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 8.17* is cited from reference, *Volodymyr Mnih et al. (2015): Human-level
    control through deep reinforcement learning. Nature 518, 7540 (2015), 529 – 533*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The news that an AI called AlphaGo (*David Silver et al. (2016): Mastering
    the game of Go with deep neural networks and tree search. Nature 529, 7587 (2016),
    484 – 489*) beat the Go champion attracted much attention. Deep learning and reinforcement
    learning are also used in AlphaGo. It learned from 30 million game records created
    by professionals and played against itself many times to accumulate sufficient
    knowledge. Both AlphaGo and DQNs have been researched by Google''s DeepMind. We
    must keep an eye on their activities in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we implemented a deep CNN and achieved an excellent recognition
    result exceeding 99% for handwritten digit recognition. We also discussed the
    motivation for making a network deeper and the current tendency toward deeper
    networks. We also looked at the trends and applications of deep learning, and
    the research is accelerating it, which will advance this technology into the future.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of deep learning, there is much that is still unknown, and new
    research is being published all the time. Researchers and engineers around the
    world continue to research actively and will realize technologies that we cannot
    even imagine yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following points were covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Making a network deeper will improve performance for many deep learning problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In image recognition competitions, techniques using deep learning get a high
    ranking, and current networks are deeper than their predecessors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Famous networks include VGG, GoogLeNet, and ResNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs, distributed training, and the reduction of bit accuracy can accelerate
    deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning (neural networks) can be used for object detection and segmentation,
    as well as for object recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications that use deep learning include the generation of image captions,
    the generation of images, and reinforcement learning. These days, the use of deep
    learning for automated driving is also expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading this book. We hope that you've gained a better understanding
    of deep learning and have found it an interesting journey.
  prefs: []
  type: TYPE_NORMAL
