<html><head></head><body>
		<div id="_idContainer081">
			<h1 id="_idParaDest-113" class="chapter-number"><a id="_idTextAnchor134"/>6</h1>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor135"/>Exploring and Visualizing Data</h1>
			<p>Exploring and visualizing data are essential steps in the process of developing a <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) application. In this chapter, we will explore techniques <a id="_idIndexMarker380"/>for <strong class="bold">data exploration</strong>, such as visualizing word frequencies, and techniques for visualizing document similarity. We will also introduce several important visualization tools, such as Matplotlib, Seaborn, and WordCloud, that enable us to graphically represent data and identify patterns and relationships within our datasets. By combining these techniques, we can gain valuable perspectives into our data, make informed decisions about the next steps in our NLU processing, and ultimately, improve the accuracy and effectiveness of our analyses. Whether you’re a data scientist or a developer, data exploration and visualization are essential skills for extracting actionable insights from text data in preparation for further <span class="No-Break">NLU processing.</span></p>
			<p>In this chapter, we will cover several topics related to the initial exploration of data, especially visual exploration, or <strong class="bold">visualization</strong>. We will start by reviewing a few reasons why getting a visual perspective of our data can be helpful. This will be followed by an introduction to a sample dataset of movie reviews that we will be using to illustrate our techniques. Then, we will look at techniques for data exploration, including summary statistics, visualizations of word frequencies in a dataset, and measuring document similarity. We will follow with a few general tips on developing visualizations and conclude with some ideas about using information from visualizations to make decisions about <span class="No-Break">further processing.</span></p>
			<p>The topics we will cover are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Why visualize?</span></li>
				<li><span class="No-Break">Data exploration</span></li>
				<li>General considerations for <span class="No-Break">developing visualizations</span></li>
				<li>Using information from visualization to make decisions <span class="No-Break">about processing</span></li>
			</ul>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor136"/>Why visualize?</h1>
			<p>Visualizing data <a id="_idIndexMarker381"/>means displaying data in a graphical format such as a chart or graph. This is almost always a useful precursor to training a <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) system<a id="_idIndexMarker382"/> to perform a specific task because it is typically very difficult to see patterns in large amounts of text data. It is often much easier to see overall patterns in data visually. These patterns might be very helpful in making decisions about the most applicable <span class="No-Break">text-processing techniques.</span></p>
			<p>Visualization can also be useful in understanding the results of NLP analysis and deciding what the next steps might be. Because looking at the results of NLP analysis is not an initial exploratory step, we will postpone this topic until <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a> and <a href="B19005_14.xhtml#_idTextAnchor248"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
			<p>In order to explore visualization, in this chapter, we will be working with a dataset of text documents. The text documents will illustrate a binary classification problem, which will be described in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor137"/>Text document dataset – Sentence Polarity Dataset</h2>
			<p>The <a id="_idIndexMarker383"/>Sentence <a id="_idIndexMarker384"/>Polarity Dataset is a commonly <a id="_idIndexMarker385"/>used dataset that consists of movie reviews<a id="_idIndexMarker386"/> originally from<a id="_idIndexMarker387"/> the <strong class="bold">Internet Movie Database</strong> (<strong class="bold">IMDb</strong>). The reviews have been categorized in terms of positive and negative reviews. The task that is most often used with these reviews is classifying the reviews into positive and negative. The data was collected by a team at Cornell University. More information about the dataset and the data itself can be found <span class="No-Break">at </span><a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/"><span class="No-Break">https://www.cs.cornell.edu/people/pabo/movie-review-data/</span></a><span class="No-Break">.</span></p>
			<p><strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) comes<a id="_idIndexMarker388"/> with this dataset as one of its built-in corpora. There are 1,000 positive reviews and 1,000 <span class="No-Break">negative reviews.</span></p>
			<p>Here’s an example of a positive review from <span class="No-Break">this dataset:</span></p>
			<pre class="source-code">
kolya is one of the richest films i've seen in some time .
zdenek sverak plays a confirmed old bachelor ( who's likely to remain so ) , who finds his life as a czech cellist increasingly impacted by the five-year old boy that he's taking care of .
though it ends rather abruptly-- and i'm whining , 'cause i wanted to spend more time with these characters-- the acting , writing , and production values are as high as , if not higher than , comparable american dramas .
this father-and-son delight-- sverak also wrote the script , while his son , jan , directed-- won a golden globe for best foreign language film and , a couple days after i saw it , walked away an oscar .
in czech and russian , with english subtitles .</pre>
			<p>Here’s an example of a <span class="No-Break">negative review:</span></p>
			<pre class="source-code">
claire danes , giovanni ribisi , and omar epps make a likable trio of protagonists , but they're just about the only palatable element of the mod squad , a lame-brained big-screen version of the 70s tv show .
the story has all the originality of a block of wood ( well , it would if you could decipher it ) , the characters are all blank slates , and scott silver's perfunctory action sequences are as cliched as they come .
by sheer force of talent , the three actors wring marginal enjoyment from the proceedings whenever they're on screen , but the mod squad is just a second-rate action picture with a first-rate cast .</pre>
			<p>We can look at the thousands of examples in our dataset, but it will be hard to see any large-scale <a id="_idIndexMarker389"/>patterns by looking at individual <a id="_idIndexMarker390"/>examples; there <a id="_idIndexMarker391"/>is just too much data for us to separate the big picture from the details. The next sections will cover tools that we can use to identify these <span class="No-Break">large-scale patterns.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor138"/>Data exploration</h1>
			<p><strong class="bold">Data exploration</strong>, which is <a id="_idIndexMarker392"/>sometimes also called <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>), is the <a id="_idIndexMarker393"/>process of taking a first look at your data to see what kinds of patterns there are to get an overall perspective on the full dataset. These patterns and overall perspective will help us identify the most appropriate processing approaches. Because some NLU techniques are very computationally intensive, we want to ensure that we don’t waste a lot of time applying a technique that is inappropriate for a particular dataset. Data exploration can help us narrow down the options for techniques at the very beginning of our project. Visualization is a great help in data exploration because it is a quick way to get the big picture of patterns in <span class="No-Break">the data.</span></p>
			<p>The most basic kind of information about a corpus that we would want to explore includes information such as the number of words, the number of distinct words, the average length of documents, and the number of documents in each category. We can start by looking at the frequency distributions of words. We will cover several different ways of visualizing word frequencies in our datasets and then look at some measurements of <span class="No-Break">document similarity.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor139"/>Frequency distributions</h2>
			<p>Frequency<a id="_idIndexMarker394"/> distributions show how many items of a specific type occur in some context. In our case, the context is a dataset, and we will be looking at the frequencies of <em class="italic">words</em> and then <em class="italic">ngrams</em> (sequences of words) that occur in our dataset and subsets of the dataset. We’ll start by defining word frequency distributions and doing some preprocessing. We’ll then visualize the word frequencies with Matplotlib tools and WordCloud, and then apply the same techniques to ngrams. Finally, we’ll cover some techniques for visualizing document similarity: bag of words (<strong class="bold">BoW</strong>) and <span class="No-Break">k-means clustering.</span></p>
			<h3>Word frequency distributions</h3>
			<p>The words and <a id="_idIndexMarker395"/>their<a id="_idIndexMarker396"/> frequencies that occur in a corpus are often very informative. It’s a good idea to take a look at this information before getting too far into an NLP project. Let’s take a look at computing this information using NLTK. We start by importing NLTK and the movie reviews dataset, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B19005_06_01.jpg" alt="Figure 6.1 – Importing ﻿NLTK and the movie reviews dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Importing NLTK and the movie reviews dataset</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em> shows code that we can use to collect the words in the <strong class="source-inline">movie_reviews</strong> dataset into a list and take a look at some of the words in the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>To collect the words in the corpus into a Python list, NLTK provides a helpful function, <strong class="source-inline">words()</strong>, which we <span class="No-Break">use here.</span></li>
				<li>Find out the total number of words in the dataset by using the normal Python <strong class="source-inline">len()</strong> function for finding the size of a list, with the list of words as <span class="No-Break">a parameter.</span></li>
				<li>Print<a id="_idIndexMarker397"/> the<a id="_idIndexMarker398"/> list <span class="No-Break">of words:</span></li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B19005_06_02.jpg" alt="Figure 6.2﻿ –  Making a list of the words in the corpus, counting them, and displaying the first few"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 –  Making a list of the words in the corpus, counting them, and displaying the first few</p>
			<p>The result of counting the words and printing the first few words is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
1583820
['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]</pre>
			<p>We won’t see more words than the first few (<strong class="source-inline">'plot'</strong>, <strong class="source-inline">':'</strong>, <strong class="source-inline">'two'</strong>, and so on) because there are too many to show. By printing the length of the list, we can see that there are 1,583,820 individual words in the dataset, which is certainly too many to list. What we can also notice is that this list of words includes punctuation, specifically <strong class="source-inline">:</strong>. Since punctuation is so common, looking at it is unlikely to yield any insights into differences between documents. In addition, including punctuation will also make it harder to see the words that actually distinguish categories of documents. So, let’s <span class="No-Break">remove punctuation.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> shows a way to remove punctuation with the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Initialize an empty list where we will store the new list of words after removing <span class="No-Break">punctuation (</span><span class="No-Break"><strong class="source-inline">words_no_punct</strong></span><span class="No-Break">).</span></li>
				<li>Iterate through the list of <span class="No-Break">words (</span><span class="No-Break"><strong class="source-inline">corpus_words</strong></span><span class="No-Break">).</span></li>
				<li>Keep only the alphanumeric words. This code uses the Python string function, <strong class="source-inline">string.isalnum()</strong>, at line 4 to detect alphanumeric words, which are added to the <strong class="source-inline">words_no_punct</strong> list that we <span class="No-Break">previously initialized.</span></li>
				<li>Once we<a id="_idIndexMarker399"/> remove <a id="_idIndexMarker400"/>the punctuation, we might be interested in the frequencies of occurrence of the non-punctuation words. Which are the most common words in our corpus? NLTK provides a useful <strong class="source-inline">FreqDist()</strong> function (frequency distribution) that computes word frequencies. This function is applied at line 6 in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></li>
				<li>We can then see the most common words in the frequency distribution using the NLTK <strong class="source-inline">most_common()</strong> method, which takes as a parameter how many words we want to see, shown in <span class="No-Break">line 8.</span></li>
				<li>Finally, we print the 50 most <span class="No-Break">common words:</span></li>
			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B19005_06_03.jpg" alt="Figure 6.3﻿ –  The most common 50 words in the movie review database and their frequencies"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 –  The most common 50 words in the movie review database and their frequencies</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em>, we can easily see that <strong class="source-inline">the</strong> is the most common word, with 76,529 occurrences in the<a id="_idIndexMarker401"/> dataset, which<a id="_idIndexMarker402"/> is not surprising. However, it’s harder to see the frequencies of the less common words. It’s not easy to see, for example, the tenth most common word, and how much more common it is than the eleventh most common word. This is where we bring in our <span class="No-Break">visualization tools.</span></p>
			<p>The frequencies that we computed in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> can be plotted with the <strong class="source-inline">plot()</strong> function for frequency distributions. This function takes <span class="No-Break">two parameters:</span></p>
			<ol>
				<li>The first parameter is the number of words we want to see – in this case, it <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">50</strong></span><span class="No-Break">.</span></li>
				<li>The <strong class="source-inline">cumulative</strong> parameter determines whether we just want to see the frequency of each of our 50 words (<strong class="source-inline">cumulative=False</strong>). If we want to see the cumulative frequencies for all the words, we would set this parameter <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>To plot the frequency distribution, we can simply add a call to <strong class="source-inline">plot()</strong> as follows:</p>
			<pre class="source-code">
<strong class="bold">freq.plot(50, cumulative = False)</strong></pre>
			<p>The result is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em>, a plot of the frequencies of each word, with the most frequent <a id="_idIndexMarker403"/><span class="No-Break">words </span><span class="No-Break"><a id="_idIndexMarker404"/></span><span class="No-Break">first:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B19005_06_04.jpg" alt="Figure 6.4 – Frequency distribution of words in the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Frequency distribution of words in the movie review corpus</p>
			<p>Try <strong class="source-inline">freq.plot()</strong> with different numbers of words and with <strong class="source-inline">cumulative = True</strong>. What happens when you ask for more and more words? You will see that looking at rarer and rarer words (by increasing the value of the first argument to larger numbers) does not provide <span class="No-Break">much insight.</span></p>
			<p>While the graph in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em> gives us a much clearer idea of the frequencies of the different words in the dataset, it also tells us something more important about the frequent words. Most of them, such as <strong class="source-inline">the</strong>, <strong class="source-inline">a</strong>, and <strong class="source-inline">and</strong>, are not very informative. Just like punctuation, they are not likely to help us distinguish the categories (positive and negative reviews) that we’re interested in because they occur very frequently in most documents. These kinds of words are<a id="_idIndexMarker405"/> called <strong class="bold">stopwords</strong> and are usually removed from natural language data before NLP precisely because they don’t add <span class="No-Break">much information.</span></p>
			<p>NLTK provides a list of common stopwords for different languages. Let’s take a look at some of<a id="_idIndexMarker406"/> the <span class="No-Break">English</span><span class="No-Break"><a id="_idIndexMarker407"/></span><span class="No-Break"> stopwords:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B19005_06_05.jpg" alt="Figure 6.5 – Examples of 50 English stopwords"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Examples of 50 English stopwords</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em> shows the code we can use to examine English stopwords, using the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Import <strong class="source-inline">nltk</strong> and the <span class="No-Break"><strong class="source-inline">stopwords</strong></span><span class="No-Break"> package.</span></li>
				<li>Make the set of stopwords into a Python list so that we can do list operations <span class="No-Break">on it.</span></li>
				<li>Find out how long the list is with the Python <strong class="source-inline">len()</strong> function and print the value (<span class="No-Break">179 items).</span></li>
				<li>Print the first <span class="No-Break"><strong class="source-inline">50</strong></span><span class="No-Break"> stopwords.</span></li>
			</ol>
			<p>Stopwords are available for other languages in NLTK as well. We can see the list by running the code in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B19005_06_06.jpg" alt="Figure 6.6 – NLTK languages with stopwords"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – NLTK languages with stopwords</p>
			<p>The code has the following <span class="No-Break">three steps:</span></p>
			<ol>
				<li>Collect the<a id="_idIndexMarker408"/> names<a id="_idIndexMarker409"/> of the languages with NLTK stopwords. The stopwords are contained in files, one file per language, so getting the files is a way to get the names of <span class="No-Break">the languages.</span></li>
				<li>Print the length of the list <span class="No-Break">of languages.</span></li>
				<li>Print the names of <span class="No-Break">the languages.</span></li>
			</ol>
			<p>Once we have the list of stopwords, it is easy to remove them from a corpus. After removing the punctuation, as we saw in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em>, we can just iterate through the list of words in the corpus, removing the words that are in the stopwords list, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
words_stop = [w for w in words_no_punct if not w in stop_words]</pre>
			<p>As an aside, it would clearly be more efficient to combine removing punctuation and stopwords with one iteration through the entire list of words, checking each word to see whether it is either punctuation or a stopword, and removing it if it is either one. We show these steps separately for clarity, but you may want to combine these two steps in your own projects. Just as we did in the case of removing punctuation, we can use the NLTK frequency distribution function to see which words other than stopwords are most common, and display them, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B19005_06_07.jpg" alt="Figure 6.7 – Most common words, after removing punctuation and stopwords"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Most common words, after removing punctuation and stopwords</p>
			<p>We can see<a id="_idIndexMarker410"/> in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> that <a id="_idIndexMarker411"/>removing stopwords gives us a much clearer picture of the words in the movie review corpus. The most common word is now <strong class="source-inline">film</strong>, and the third most common word is <strong class="source-inline">movie</strong>. This is what we would expect from a corpus of movie reviews. A similar review corpus (for example, a corpus of product reviews) would be expected to show a different distribution of frequent words. Even looking at the simple information provided by frequency distributions can sometimes be enough to do some <span class="No-Break">NLP tasks.</span></p>
			<p>For example, in authorship studies, we’re interested in trying to attribute a document whose author is unknown to a known author. The word frequencies in the document whose author we don’t know can be compared to those in the documents with a known author. Another example would be domain classification, where we want to know whether the <a id="_idIndexMarker412"/>document<a id="_idIndexMarker413"/> is a movie review or a <span class="No-Break">product review.</span></p>
			<h3>Visualizing data with Matplotlib, Seaborn, and pandas</h3>
			<p>While<a id="_idIndexMarker414"/> NLTK <a id="_idIndexMarker415"/>has <a id="_idIndexMarker416"/>some <a id="_idIndexMarker417"/>basic<a id="_idIndexMarker418"/> plotting <a id="_idIndexMarker419"/>functions, there<a id="_idIndexMarker420"/> are <a id="_idIndexMarker421"/>some<a id="_idIndexMarker422"/> other, more <a id="_idIndexMarker423"/>powerful Python plotting packages that are used very often for plotting all kinds of data, including NLP data. We’ll look at some of these in <span class="No-Break">this section.</span></p>
			<p>Matplotlib (<a href="https://matplotlib.org/">https://matplotlib.org/</a>) is<a id="_idIndexMarker424"/> very popular. Matplotlib can create a variety of visualizations, including animations and interactive visualizations. We’ll use it here to create another visualization of the data that we plotted in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em>. Seaborn (<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>) is<a id="_idIndexMarker425"/> built on Matplotlib and is a higher-level interface for producing attractive visualizations. Both of these packages frequently make use of another Python data package, pandas (<a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a>), which<a id="_idIndexMarker426"/> is very useful for handling <span class="No-Break">tabular data.</span></p>
			<p>For our example, we’ll use the same data that we plotted in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, but we’ll use Matplotlib, Seaborn, and pandas to create <span class="No-Break">our visualization.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em> shows the code for <span class="No-Break">this example:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B19005_06_08.jpg" alt="Figure 6.8 – Python code for collecting the 25 most common words, after removing punctuation and stopwords"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Python code for collecting the 25 most common words, after removing punctuation and stopwords</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em> shows the preparatory code for plotting the data, with the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Import the NLTK, pandas, Seaborn, and <span class="No-Break">Matplotlib libraries.</span></li>
				<li>Set the frequency cutoff to <strong class="source-inline">25</strong> (this can be whatever number we’re <span class="No-Break">interested in).</span></li>
				<li>Compute a frequency distribution of the corpus words without stopwords (<span class="No-Break">line 7).</span></li>
			</ol>
			<p>We start by importing NLTK, pandas, Seaborn, and Matplotlib. We set the frequency cutoff to <strong class="source-inline">25</strong>, so that we will plot only the 25 most common <a id="_idIndexMarker427"/>words, and we <a id="_idIndexMarker428"/>get<a id="_idIndexMarker429"/> the<a id="_idIndexMarker430"/> frequency <a id="_idIndexMarker431"/>distribution<a id="_idIndexMarker432"/> of <a id="_idIndexMarker433"/>our <a id="_idIndexMarker434"/>data <a id="_idIndexMarker435"/>at <span class="No-Break">line 7.</span></p>
			<p>The resulting plot is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B19005_06_09.jpg" alt="Figure 6.9 – Most common words, after removing punctuation and stopwords, displayed using Matplotlib, Seaborn, and pandas libraries"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Most common words, after removing punctuation and stopwords, displayed using Matplotlib, Seaborn, and pandas libraries</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> and <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em> show the same data. For example, the most common word in each figure is <strong class="source-inline">film</strong>, followed by <strong class="source-inline">one</strong>, and then <strong class="source-inline">movie</strong>. Why would you pick one visualization tool over the other? One difference is that it seems to be easier to see the information<a id="_idIndexMarker436"/> for a <a id="_idIndexMarker437"/>specific <a id="_idIndexMarker438"/>word <a id="_idIndexMarker439"/>in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, which <a id="_idIndexMarker440"/>is <a id="_idIndexMarker441"/>probably<a id="_idIndexMarker442"/> because of the bar graph format, where every word is associated with a specific bar. In contrast, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, it is a little harder to see the frequencies of individual words because this isn’t very clear from <span class="No-Break">the line.</span></p>
			<p>On the other hand, the overall frequency distribution is easier to see in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> because of the continuous line. So, choosing one type of visualization over the other is really a matter of the kind of information that you want to emphasize. Of course, there is also no reason to limit yourself to one visualization if you want to see your data in <span class="No-Break">different ways.</span></p>
			<p>As a side note, the pattern in both figures, where we have very high frequencies for the most common words, falling off rapidly for less common words, is very common <a id="_idIndexMarker443"/>in<a id="_idIndexMarker444"/> natural <a id="_idIndexMarker445"/>language <a id="_idIndexMarker446"/>data. This <a id="_idIndexMarker447"/>pattern <a id="_idIndexMarker448"/>illustrates <a id="_idIndexMarker449"/>a <a id="_idIndexMarker450"/>concept<a id="_idIndexMarker451"/> called <strong class="bold">Zipf’s law</strong> (for more <a id="_idIndexMarker452"/>information about this concept, <span class="No-Break">see </span><a href="https://en.wikipedia.org/wiki/Zipf%27s_law"><span class="No-Break">https://en.wikipedia.org/wiki/Zipf%27s_law</span></a><span class="No-Break">).</span></p>
			<h3>Looking at another frequency visualization technique – word clouds</h3>
			<p>A <strong class="bold">word cloud</strong> is <a id="_idIndexMarker453"/>another <a id="_idIndexMarker454"/>way of visualizing the relative frequencies of words. A word cloud displays the words in a dataset in different font sizes, with the more frequent words shown in larger fonts. Word clouds are a good way to make frequent words pop <span class="No-Break">out visually.</span></p>
			<p>The code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.10</em> shows how to import the <strong class="source-inline">WordCloud</strong> package and create a word cloud from the movie <span class="No-Break">review data:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B19005_06_10.jpg" alt="Figure 6.10 – Python code for most common words, shown as a word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Python code for most common words, shown as a word cloud</p>
			<p>The code shows the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>We import a new library, <strong class="source-inline">WordCloud</strong>, at line 2. For this display, we will select only the most <em class="italic">common 200 words</em> (with a frequency cutoff of <strong class="source-inline">200</strong> at <span class="No-Break">line 6).</span></li>
				<li>We create a frequency distribution at <span class="No-Break">line 7.</span></li>
				<li>The frequency distribution is converted to a pandas Series at <span class="No-Break">line 8.</span></li>
				<li>To reduce the number of very short words, we include only words that are longer than two letters at <span class="No-Break">line 10.</span></li>
				<li>The code in line 11 generates the word cloud. The <strong class="source-inline">colormap</strong> parameter specifies <a id="_idIndexMarker455"/>one<a id="_idIndexMarker456"/> of the many Matplotlib color maps (color maps are documented at <a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html">https://matplotlib.org/stable/tutorials/colors/colormaps.html</a>). You can experiment with different color schemes to find the ones that <span class="No-Break">you prefer.</span></li>
				<li>Lines 12-14 format the <span class="No-Break">plot area.</span></li>
				<li>Finally, the plot is displayed at <span class="No-Break">line 15.</span></li>
			</ol>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em> shows the word cloud that results from the code in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B19005_06_11New.jpg" alt="Figure 6.11 – Word cloud for word frequencies in movie reviews"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Word cloud for word frequencies in movie reviews</p>
			<p>As we saw in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> and <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, the most common words are <strong class="source-inline">film</strong>, <strong class="source-inline">one</strong>, and <strong class="source-inline">movie</strong>. However, the word cloud visualization makes the most common words stand out in a way that the graphs do not. On the other hand, the less frequent words are difficult to differentiate in the word cloud. For example, it’s hard to tell whether <strong class="source-inline">good</strong> is more frequent than <strong class="source-inline">story</strong> from the word cloud. This is another example showing that you should consider<a id="_idIndexMarker457"/> what information you <a id="_idIndexMarker458"/>want to get from a visualization before <span class="No-Break">selecting one.</span></p>
			<p>The next section will show you how to dig into the data a little more deeply to get some insight into subsets of the data such as positive and <span class="No-Break">negative reviews.</span></p>
			<h3>Positive versus negative movie reviews</h3>
			<p>We might<a id="_idIndexMarker459"/> want<a id="_idIndexMarker460"/> to look at <a id="_idIndexMarker461"/>differences between the different classes (positive and negative) of movie reviews, or, more generally, any different categories. For example, are the frequent words in positive and negative reviews different? This preliminary look at the properties of the positive and negative reviews can give us some insight into how the classes differ, which, in turn, might help us select the approaches that will enable a trained system to differentiate the <span class="No-Break">categories automatically.</span></p>
			<p>We can use any of the visualizations based on frequency distributions for this, including the line graph in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, the bar graph in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, or the word cloud visualization in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em>. We’ll use the word cloud in this example because it is a good way of seeing the differences in word frequencies between the two <span class="No-Break">frequency distributions.</span></p>
			<p>Looking at<a id="_idIndexMarker462"/> the code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.12</em>, we start by importing the libraries <span class="No-Break">we’ll need:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B19005_06_12.jpg" alt="Figure 6.12 – Importing libraries for computing word clouds"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Importing libraries for computing word clouds</p>
			<p>The next thing we’ll do is create two functions that will make it easier to perform similar operations on different parts of <span class="No-Break">the corpus:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B19005_06_13.jpg" alt="Figure 6.13 – Functions for computing word clouds"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Functions for computing word clouds</p>
			<p>The first function in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.13</em>, <strong class="source-inline">clean_corpus()</strong>, removes the punctuation and stopwords <a id="_idIndexMarker463"/>from <a id="_idIndexMarker464"/>a<a id="_idIndexMarker465"/> corpus. The second one, <strong class="source-inline">plot_freq_dist()</strong>, plots the word cloud from the frequency distribution. Now we’re ready to create the <span class="No-Break">word clouds:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B19005_06_14New.jpg" alt="Figure 6.14 – The code for displaying ﻿word frequencies for positive and negative reviews in a word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – The code for displaying word frequencies for positive and negative reviews in a word cloud</p>
			<p>We create the word cloud with the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Now that we’ve defined the functions, we separate the corpus into positive and negative reviews. This is done by the code shown in lines 28 <span class="No-Break">and 29.</span></li>
				<li>Instead of asking for all the words in the corpus, as we saw in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em>, we ask for words in specific categories. In this example, the categories are positive (<strong class="source-inline">pos</strong>) and <span class="No-Break">negative (</span><span class="No-Break"><strong class="source-inline">neg</strong></span><span class="No-Break">).</span></li>
				<li>We remove the stopwords and punctuation in each set of words in lines 28 <span class="No-Break">and 29.</span></li>
				<li>We then create frequency distributions for the positive and negative words in lines 30 and 31. Finally, we plot the <span class="No-Break">word cloud.</span></li>
			</ol>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em> shows <a id="_idIndexMarker466"/>the <a id="_idIndexMarker467"/>word<a id="_idIndexMarker468"/> cloud for the word frequencies in the <span class="No-Break">positive reviews:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B19005_06_15New.jpg" alt="Figure 6.15 – Displaying word frequencies for positive reviews in a word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Displaying word frequencies for positive reviews in a word cloud</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.16</em> shows the word frequencies for negative reviews in a <span class="No-Break">word cloud:</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B19005_06_16New.jpg" alt="Figure 6.16 – Displaying word frequencies for negative reviews in a word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Displaying word frequencies for negative reviews in a word cloud</p>
			<p>Comparing <em class="italic">Figures 6.15</em> and <em class="italic">6.16</em> with the original word cloud in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em>, we can see that the<a id="_idIndexMarker469"/> words <strong class="source-inline">film</strong>, <strong class="source-inline">one</strong>, and <strong class="source-inline">movie</strong> are the most frequent words in<a id="_idIndexMarker470"/> positive<a id="_idIndexMarker471"/> and negative reviews, as well as being the most frequent overall words, so they would not be very useful in distinguishing positive and <span class="No-Break">negative reviews.</span></p>
			<p>The word <strong class="source-inline">good</strong> is larger (and therefore more frequent) in the positive review word cloud than in the negative review word cloud, which is just what we would expect. However, <strong class="source-inline">good</strong> does occur fairly frequently in the negative reviews, so it’s not a definite indication of a positive review by any means. Other differences are less expected – for example, <strong class="source-inline">story</strong> is more common in the positive reviews, although it does occur in the negative reviews. This comparison shows that the problem of distinguishing between positive and negative reviews is unlikely to be solved by simple <span class="No-Break">keyword-spotting techniques.</span></p>
			<p>The techniques we’ll be looking at in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> and <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, will be more suitable for addressing the problem of classifying texts into different categories. However, we can see that this initial exploration with word clouds was very useful in eliminating a simple keyword-based approach from the set of <a id="_idIndexMarker472"/>ideas we<a id="_idIndexMarker473"/> <span class="No-Break">are </span><span class="No-Break"><a id="_idIndexMarker474"/></span><span class="No-Break">considering.</span></p>
			<p>In the next section, we’ll look at other frequencies in <span class="No-Break">the corpus.</span></p>
			<h3>Looking at other frequency measures</h3>
			<p>So far we’ve<a id="_idIndexMarker475"/> only looked at word frequencies, but we can look at the frequency of any other text property we can measure. For example, we can look at the frequencies of different characters or different parts of speech. You can try extending the code that was presented earlier in this chapter, in the previous section, <em class="italic">Frequency distributions</em>, to count some of these other properties of the movie <span class="No-Break">review texts.</span></p>
			<p>One important property of language is that words don’t just occur in isolation – they occur in specific combinations and orders. The meanings of words can change dramatically depending on their contexts. For example, <em class="italic">not a good movie</em> has a very different meaning (in fact, the opposite meaning) from <em class="italic">a good movie</em>. There are a number of techniques for taking word context into account, which we will explore in <em class="italic">Chapters 8</em> <span class="No-Break">to </span><span class="No-Break"><em class="italic">11</em></span><span class="No-Break">.</span></p>
			<p>However, here we’ll just describe one very simple technique – looking at words that occur next to each other. Where two words occur together, this is<a id="_idIndexMarker476"/> called a <strong class="bold">bigram</strong>. An example is <em class="italic">good movie</em>. A set of three words in sequence is called a <strong class="bold">trigram</strong>, and in <a id="_idIndexMarker477"/>general, any <a id="_idIndexMarker478"/>number of words in a sequence is called an <strong class="bold">ngram</strong>. NLTK provides a function for counting ngrams, <strong class="source-inline">ngrams()</strong>, which takes the desired value of <strong class="source-inline">n</strong> as an argument. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.17</em> shows code for counting and displaying ngrams in the movie <span class="No-Break">review corpus:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B19005_06_17.jpg" alt="Figure 6.17 – Computing bigrams in the movie review data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – Computing bigrams in the movie review data</p>
			<p>The code shows the<a id="_idIndexMarker479"/> following <span class="No-Break">initialization steps:</span></p>
			<ol>
				<li>We start by importing <strong class="source-inline">nltk</strong>, the <strong class="source-inline">ngrams</strong> function, and the movie <span class="No-Break">review corpus.</span></li>
				<li>We set the frequency cutoff to <strong class="source-inline">25</strong>, but as in previous examples, this can be any number that we think will <span class="No-Break">be interesting.</span></li>
				<li>We collect all the words in the corpus at line 8 (if we wanted just the words from the positive reviews or just the words from the negative reviews, we could get those by setting <strong class="source-inline">categories = 'neg'</strong> or <strong class="source-inline">categories = 'pos'</strong>, as we saw in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">).</span></li>
				<li>Finally, we remove punctuation and stopwords at line 11, using the <strong class="source-inline">clean_corpus</strong> function defined in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></li>
			</ol>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.18</em>, we collect <span class="No-Break">the bigrams:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B19005_06_18.jpg" alt="Figure 6.18 – Computing bigrams in the movie review data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – Computing bigrams in the movie review data</p>
			<p>The bigrams are<a id="_idIndexMarker480"/> collected using the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>The <strong class="source-inline">ngrams()</strong> function is used at line 14, with an argument, <strong class="source-inline">2</strong>, indicating that we want bigrams (or pairs of two adjacent words). Any number can be used here, but very large numbers are not likely to be very useful. This is because as the value of <strong class="source-inline">n</strong> increases, there will be fewer and fewer ngrams with that value. At some point, there won’t be enough examples of a particular ngram to provide any information about patterns in the data. Bigrams or trigrams are usually common enough in the corpus to be helpful in <span class="No-Break">identifying patterns.</span></li>
				<li>In lines 21-23, we loop through the list of bigrams, creating a string from <span class="No-Break">each pair.</span></li>
			</ol>
			<p>In <em class="italic">Figure 6.19</em>, we make a frequency distribution of the bigrams and display it.</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B19005_06_19.jpg" alt="Figure 6.19 – Displaying bigrams in the movie review data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.19 – Displaying bigrams in the movie review data</p>
			<ol>
				<li value="3">We use the<a id="_idIndexMarker481"/> familiar NLTK <strong class="source-inline">FreqDist()</strong> function on our list of bigrams and convert it to a pandas Series in <span class="No-Break">line 29.</span></li>
				<li>The rest of the code sets up a bar graph plot in Seaborn and Matplotlib, and finally, displays it at <span class="No-Break">line 39.</span></li>
			</ol>
			<p>Since we’re using a frequency cutoff of <strong class="source-inline">25</strong>, we’ll only be looking at the most common 25 bigrams. You may want to experiment with larger and smaller <span class="No-Break">frequency cutoffs.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.20</em> shows the results of the plot that we computed in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.19</em>. Because bigrams, in general, will be longer than single words and take more room to display, the <em class="italic">x</em> and <em class="italic">y</em> axes have been swapped around so that the counts of bigrams are displayed on the <em class="italic">x</em> axis and the individual bigrams are displayed on the <span class="No-Break"><em class="italic">y</em></span><span class="No-Break"> axis:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B19005_06_20.jpg" alt="Figure 6.20 – Bigrams in the movie review data, ordered by frequency"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.20 – Bigrams in the movie review data, ordered by frequency</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.20</em> reveals<a id="_idIndexMarker482"/> several interesting facts about the corpus. For example, the most common bigram, <strong class="source-inline">special effects</strong>, occurs about 400 times in the corpus, compared to the most common single word, <strong class="source-inline">film</strong>, which occurs nearly 8,000 times. This is to be expected because two words have to occur together to count as a bigram. We also see that many of the bigrams<a id="_idIndexMarker483"/> are <strong class="bold">idioms</strong>. An idiom is a combination of two or more words together that have a meaning that is not simply the combination of the meanings of the individual words. <strong class="source-inline">New York</strong> and <strong class="source-inline">Star Trek</strong> are examples in this list. Other bigrams are not idioms but just common phrases, such as <strong class="source-inline">real life</strong> and <strong class="source-inline">one day</strong>. In this list, all of the bigrams are very reasonable, and it is not surprising to see any of them in a corpus of <span class="No-Break">movie reviews.</span></p>
			<p>As an exercise, try comparing the bigrams in the positive and negative movie reviews. In looking at single-word frequencies, we saw that the most common words were the same in both positive and negative reviews. Is that also the case with the most <span class="No-Break">common bigrams?</span></p>
			<p>This section covered some ways to get insight into our datasets by simple measurements such as counting words and bigrams. There are also some useful exploratory<a id="_idIndexMarker484"/> techniques for measuring and visualizing the similarities among documents in a dataset. We will cover these in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor140"/>Measuring the similarities among documents</h2>
			<p>So far, we’ve been <a id="_idIndexMarker485"/>looking at visualizing the frequencies of various properties of the corpus, such as words and bigrams, with tools such as line charts, bar graphs, and word clouds. It is also very informative to visualize document similarity – that is, how similar documents in a dataset are to each other. There are many ways to measure document similarity, which we will discuss in detail in later chapters, especially <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, and <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>. We will start here with an introductory look at two <span class="No-Break">basic techniques.</span></p>
			<h4>BoW and k-means clustering</h4>
			<p>For now, we <a id="_idIndexMarker486"/>will <a id="_idIndexMarker487"/>use a <a id="_idIndexMarker488"/>very<a id="_idIndexMarker489"/> simple idea, called <strong class="bold">bag of words </strong>(<strong class="bold">BoW</strong>). The idea behind BoW is that documents that are more similar to each other will contain more of the same words. For each document in a corpus and for each word in the corpus, we look at whether or not that word occurs in that document. The more words that any two documents have in common, the more similar to each other they are. This is a very simple measurement, but it will give us a basic document similarity metric that we can use to illustrate visualizations of <span class="No-Break">document similarity.</span></p>
			<p>The code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.21</em> computes the BoW for the movie review corpus. You do not need to be concerned with the details of this code at this time, since it’s just a way of getting a similarity measure for the corpus. We will use this similarity metric (that is, BoW) to see how similar any two documents are to each other. The BoW metric has the advantage that it is easy to understand and compute. Although it is not the state-of-the-art way of measuring document similarity, it can be useful as a quick <span class="No-Break">first step:</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B19005_06_21.jpg" alt="Figure 6.21 – Setting up the B﻿oW computation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.21 – Setting up the BoW computation</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.21</em> shows<a id="_idIndexMarker490"/> the <a id="_idIndexMarker491"/>process <a id="_idIndexMarker492"/>of getting the most frequent 1,000 words<a id="_idIndexMarker493"/> from the corpus and making them into a list. The number of words we want to keep in the list is somewhat arbitrary – a very long list will slow down later processing and will also start to include rare words that don’t provide <span class="No-Break">much information.</span></p>
			<p>The next steps, shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.22</em>, are to define a function to collect the words in a document and then make a list of <span class="No-Break">the documents:</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B19005_06_22.jpg" alt="Figure 6.22 – Collecting the words that occur in a document"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.22 – Collecting the words that occur in a document</p>
			<p>The <strong class="source-inline">document_features()</strong> function in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.22</em> iterates through the given document creating a Python dictionary with the words as keys and 1s and 0s as the values, depending<a id="_idIndexMarker494"/> on<a id="_idIndexMarker495"/> whether <a id="_idIndexMarker496"/>or <a id="_idIndexMarker497"/>not the word occurred in the document. Then, we create a list of the features for each document and display the result in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.23</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B19005_06_23.jpg" alt="Figure 6.23 – Computing the full feature set for all documents and displaying the resulting B﻿oW"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.23 – Computing the full feature set for all documents and displaying the resulting BoW</p>
			<p>Although the list of features for each document includes its category, we don’t need the category in order to display the BoW, so we remove it at <span class="No-Break">lines 34-39.</span></p>
			<p>We can see the first 10 documents in the resulting BoW itself in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.24</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B19005_06_24.jpg" alt="Figure 6.24 – B﻿oW for the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.24 – BoW for the movie review corpus</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.24</em>, each <a id="_idIndexMarker498"/>of <a id="_idIndexMarker499"/>the 10 rows <a id="_idIndexMarker500"/>of 0s and 1s represents one <a id="_idIndexMarker501"/>document. There is one column for each word in the corpus. There are 1,000 columns, but this is too many to display, so we only see the first few and last few columns. The words are sorted in order of frequency, and we can see that the most frequent words (<strong class="source-inline">film</strong>, <strong class="source-inline">one</strong>, and <strong class="source-inline">movie</strong>) are the same words that we found to be the most frequent (except for stopwords) in our earlier explorations of <span class="No-Break">word frequencies.</span></p>
			<p>Each document is represented by a row in the BoW. This list of numbers in these rows is a <strong class="bold">vector</strong>, which is a very important concept in NLP, which we will be seeing much more of in later chapters. Vectors are used to represent text documents or other text-based information as numbers. Representing words as numbers opens up many opportunities for analyzing and comparing documents, which are difficult to do when the documents are in text form. Clearly, BoW loses a lot of information compared to the original text representation – we no longer can tell which words are even close to each other in the text, for example – but in many cases, the simplicity of using BoW outweighs the disadvantage of losing some of the information in <span class="No-Break">the text.</span></p>
			<p>One very interesting way that we can use vectors in general, including BoW vectors, is to try to capture document similarities, which can be a very helpful first step in exploring a dataset. If we can tell which documents are similar to which other documents, we can put them into categories based on their similarity. However, just looking at the document vectors in the rows of <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.24</em> does not provide much insight, because it’s not easy to see any patterns. We need some tools for visualizing <span class="No-Break">document similarity.</span></p>
			<p>A good technique for visualizing document similarities is k-means clustering. <strong class="bold">K-means clustering</strong> tries to fit documents into visual clusters based on their similarities. In our case, the similarity metric that we will be using is BoW, where the assumption is that the more words two documents have in common, the more similar they are. K-means clustering is an iterative algorithm that represents similarities visually by the distance between items – more similar items are closer together in space. The <strong class="source-inline">k</strong> value refers to the number of clusters we want to have and is selected by the developer. In our case, we will start with <strong class="source-inline">2</strong> as the value of <strong class="source-inline">k</strong> since we have 2 known classes, corresponding<a id="_idIndexMarker502"/> to<a id="_idIndexMarker503"/> the<a id="_idIndexMarker504"/> sets<a id="_idIndexMarker505"/> of positive and <span class="No-Break">negative reviews.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.25</em> shows the code for computing and displaying the results of k-means clustering on the BoW we computed in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.23</em>. We do not have to go into detail about the code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.25</em> because it will be covered in detail in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>. However, we can note that this code uses another important Python machine learning library, <strong class="source-inline">sklearn</strong>, which is used to compute <span class="No-Break">the clusters:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B19005_06_25.jpg" alt="Figure 6.25 – Setting up for k-means clustering"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.25 – Setting up for k-means clustering</p>
			<p>The first step is to import the libraries we’ll need and set the number of clusters we <span class="No-Break">want (</span><span class="No-Break"><strong class="source-inline">true_k</strong></span><span class="No-Break">).</span></p>
			<p>The next part of the code, shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.26</em>, computes the <span class="No-Break">k-means clusters.</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B19005_06_26.jpg" alt="Figure 6.26 – K-means clustering for visualizing document similarities based on the B﻿oW"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.26 – K-means clustering for visualizing document similarities based on the BoW</p>
			<p>The steps for <a id="_idIndexMarker506"/>computing <a id="_idIndexMarker507"/>the <a id="_idIndexMarker508"/>clusters <a id="_idIndexMarker509"/>as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.26</em> are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Reduce the dimensions to <strong class="source-inline">2</strong> <span class="No-Break">for display.</span></li>
				<li>Initialize a <strong class="source-inline">kmeans</strong> object (<span class="No-Break">line 13).</span></li>
				<li>Compute the result (<span class="No-Break">line 18).</span></li>
				<li>Get the labels from <span class="No-Break">the result.</span></li>
			</ol>
			<p>The final step is plotting <span class="No-Break">the clusters:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B19005_06_27.jpg" alt="Figure 6.27 – Plotting k-means clustering"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.27 – Plotting k-means clustering</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.27</em> plots the<a id="_idIndexMarker510"/> clusters<a id="_idIndexMarker511"/> by<a id="_idIndexMarker512"/> iterating <a id="_idIndexMarker513"/>through the clusters and printing the items in each cluster in the <span class="No-Break">same color.</span></p>
			<p>The results of using k-means clustering on the movie review BoW can be seen in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.28</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B19005_06_28.jpg" alt="Figure 6.28 – K-means clustering for the movie corpus with two classes"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.28 – K-means clustering for the movie corpus with two classes</p>
			<p>There are <a id="_idIndexMarker514"/>two <a id="_idIndexMarker515"/>major<a id="_idIndexMarker516"/> clusters <a id="_idIndexMarker517"/>in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.28</em>, one above and one below the 0 point on the <em class="italic">y</em> axis. The colors are defined based on the Matplotlib <strong class="source-inline">Accent</strong> color map at line 21. (More information about the many color maps available in Matplotlib can be found <span class="No-Break">at </span><a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html"><span class="No-Break">https://matplotlib.org/stable/tutorials/colors/colormaps.html</span></a><span class="No-Break">.)</span></p>
			<p>Each dot in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.28</em> represents one document. Because the clusters are clearly separated, we can have some confidence that the similarity metric (BoW) reflects some real difference between the two classes of documents. However, that does not mean that the most insightful number of classes for this data is necessarily two. It is always worth checking out other numbers of classes; that is, other values of <strong class="source-inline">k</strong>. This can easily be done by changing the value of <strong class="source-inline">true_k</strong> at line 7 in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.25</em>. For example, if we change the value of <strong class="source-inline">true_k</strong> to <strong class="source-inline">3</strong>, and thereby specify that the data should be divided into three <a id="_idIndexMarker518"/>classes, we’ll<a id="_idIndexMarker519"/> get a<a id="_idIndexMarker520"/> chart<a id="_idIndexMarker521"/> like the one in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.29</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B19005_06_29.jpg" alt="Figure 6.29﻿ – K-means clustering for the movie corpus with three classes"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.29 – K-means clustering for the movie corpus with three classes</p>
			<p>There are definitely three clear classes in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.29</em>, although they aren’t as nicely separated as the classes in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.28</em>. This could mean that the two classes of positive and negative reviews don’t tell the whole story. Perhaps there actually should be a third class of <em class="italic">neutral</em> reviews? We could investigate this by looking at the documents in the three clusters, although we won’t go through that <span class="No-Break">exercise here.</span></p>
			<p>By comparing the results in <em class="italic">Figures 6.28</em> and <em class="italic">6.29</em>, we can see that initial data exploration can be very useful in deciding how many classes in which to divide the dataset at the outset. We will take up this topic in much greater detail in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">.</span></p>
			<p>We have so far seen<a id="_idIndexMarker522"/> a <a id="_idIndexMarker523"/>number of<a id="_idIndexMarker524"/> ways <a id="_idIndexMarker525"/>of visualizing the information in a dataset, including word and bigram frequencies, as well as some introductory visualizations of <span class="No-Break">document similarity.</span></p>
			<p>Let’s now consider some points about the overall process <span class="No-Break">of visualization.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor141"/>General considerations for developing visualizations</h1>
			<p>Stepping back a <a id="_idIndexMarker526"/>bit from the specific techniques we have reviewed so far, we will next discuss some general considerations about visualizations. Specifically, in the next sections, we will talk about what to measure, followed by how to represent these measurements and the relationships among the measurements. Because the most common visualizations are based on representing information in the <em class="italic">XY</em> plane in two dimensions, we will mainly focus on visualizations in this format, starting with selecting <span class="No-Break">among measurements.</span></p>
			<h3>Selecting among measurements</h3>
			<p>Nearly all NLP begins with measuring some property or properties of texts we are analyzing. The goal of this section is to help you understand the different kinds of text measurements that are available in <span class="No-Break">NLP projects.</span></p>
			<p>So far, we’ve primarily focused on measurements involving words. Words are a natural property to measure because they are easy to count accurately – in other words, counting words <a id="_idIndexMarker527"/>is a <strong class="bold">robust</strong> measurement. In addition, words intuitively represent a natural aspect of language. However, just looking at words can lead to missing important properties of the meanings of texts, such as those that depend on considering the order of words and their relationship to other words in <span class="No-Break">the text.</span></p>
			<p>To try to capture richer information that does take into account the orders of words and their relationships, we can measure other properties of texts. We can, for example, count characters, syntactic constructions, parts of speech, ngrams (sequences of words), mentions of named entities, and <span class="No-Break">word lemmas.</span></p>
			<p>As an example, we could look at whether pronouns are more common in positive movie reviews than in negative movie reviews. However, the downside of looking at these richer properties is that, unlike counting words, measurements of richer properties are less robust. That means they are more likely to include errors that would make the measurements less accurate. For example, if we’re counting verbs by using the results of part of speech tagging, an incorrect verb tag that should be a noun tag would make the verb <span class="No-Break">count incorrect.</span></p>
			<p>For these reasons, deciding what to measure is not always going to be a cut-and-dry decision. Some measurements, such as those that are based on words or characters, are very robust but exclude information that could be important. Other measurements, such as counting parts of speech, are less robust but more informative. Consequently, we can’t provide hard and fast rules for deciding what to measure. However, one rule of thumb is to start with the simplest and most robust approaches, such as counting words, to see whether your application is working well with those techniques. If it is, you can stick with the simplest approaches. If not, you can try using richer information. You should also keep in mind that you aren’t limited to only <span class="No-Break">one measurement.</span></p>
			<p>Once you have<a id="_idIndexMarker528"/> selected what you want to measure, there are other general considerations that have to do with visualizing your measurements. In the next sections, we will talk about representing variables on the axes of the <em class="italic">XY</em> plane, different kinds of scales, and <span class="No-Break">dimensionality reduction.</span></p>
			<h3>Insights from representing independent and dependent variables</h3>
			<p>Most measurement involves<a id="_idIndexMarker529"/> measuring one <strong class="bold">independent variable</strong> that we have control over and measuring another variable that we do not control (a <strong class="bold">dependent variable</strong>) and <a id="_idIndexMarker530"/>seeing what the relationship is. For example, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em>, the words in the dataset are the independent variable, on the <em class="italic">x</em> axis, and their counts are the dependent variable, on the <em class="italic">y</em> axis. Going further, suppose we want to evaluate a hypothesis that we can tell that a review is positive if it contains the word <strong class="source-inline">good</strong> and we can tell that it’s negative if it contains <strong class="source-inline">bad</strong>. We can test this hypothesis by looking at the counts for <strong class="source-inline">good</strong> and <strong class="source-inline">bad</strong> (the dependent variable) for positive and negative reviews (the independent variable), as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.30</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B19005_06_30.jpg" alt="Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews</p>
			<p>The columns on <a id="_idIndexMarker531"/>the <em class="italic">x</em> axis represent positive reviews containing the word <strong class="source-inline">good</strong>, negative reviews containing the word <strong class="source-inline">good</strong>, positive reviews containing the word <strong class="source-inline">bad</strong>, and negative reviews containing the word <strong class="source-inline">bad</strong>, respectively. Clearly, the hypothesis that <strong class="source-inline">good</strong> signals a positive review and <strong class="source-inline">bad</strong> signals a negative review is wrong. In fact, <strong class="source-inline">good</strong> occurs more often than <strong class="source-inline">bad</strong> in negative reviews. This kind of exploration and visualization can help us rule out lines of investigation at the outset of our project that are unlikely to be fruitful. In general, bar charts such as the one in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.30</em> are a good way to represent categorical independent variables or data that occurs in <span class="No-Break">distinct classes.</span></p>
			<p>After looking at displaying the values of dependent and independent variables in graphs like <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.30</em>, we can turn to another general consideration: linear versus <span class="No-Break">log scales.</span></p>
			<h3>Log scales and linear scales</h3>
			<p>Sometimes you will see a pattern, as in <em class="italic">Figures 6.4</em> and <em class="italic">6.7</em>, where the first few items on the <em class="italic">x</em> axis have extremely high values and where the values of the other items drop off quickly. This makes it hard to see what’s going on in the part of the graph on the right where the items have lower values. If we see this pattern, it suggests that a <strong class="bold">log scale</strong> may<a id="_idIndexMarker532"/> be a better choice for the <em class="italic">y</em> axis than<a id="_idIndexMarker533"/> the <strong class="bold">linear scale</strong> in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em>. We can see a log scale<a id="_idIndexMarker534"/> for the data from <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.31</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B19005_06_31.jpg" alt="Figure 6.31 – Log scale visualization for the data in ﻿Figure 6.4"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.31 – Log scale visualization for the data in Figure 6.4</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.31</em> shows a<a id="_idIndexMarker535"/> log scale plot of the data in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>. The <em class="italic">y-a</em>xis values are the counts of the 50 most common words in equally spaced powers of 10, so each value on the <em class="italic">y</em> axis is 10 times greater than the previous one. Compared with <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, we can see that the line representing the counts of frequencies is much flatter, especially toward the right. In the log display, it is easier to compare the frequencies of these less common, but still very frequent, words. Because even the least frequent word in this chart, <strong class="source-inline">she</strong>, still occurs over 3,000 times in the dataset, the line does not go below <span class="No-Break"><span class="_-----MathTools-_Math_Number">10</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">3</span></span><span class="No-Break">.</span></p>
			<p>You should keep the option of displaying your data in a log scale in mind if you see a pattern like the one in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
			<p>So far, we’ve been looking at data in two dimensions that can easily be displayed on paper or a flat<a id="_idIndexMarker536"/> screen. What happens if we have more than two dimensions? Let’s now consider higher <span class="No-Break">dimensional data.</span></p>
			<h3>Dimensions and dimensionality reduction</h3>
			<p>The examples we’ve <a id="_idIndexMarker537"/>seen up to this point have been largely two-dimensional<a id="_idIndexMarker538"/> diagrams with <em class="italic">x</em> and <em class="italic">y</em> axes. However, many NLP techniques result in data that is of much higher dimensionality. While two dimensions are easy to plot and visualize, higher dimensional data can be harder to understand. Three-dimensional data can be displayed by adding a <em class="italic">z</em> axis and rotating the graph so that the <em class="italic">x</em> and <em class="italic">z</em> axes are at 45-degree angles on the display. A fourth or time dimension can be added by animating an on-screen graph and showing the changes over time. However, dimensions beyond four are really impossible for humans to visualize, so there is no way to display them in a <span class="No-Break">meaningful way.</span></p>
			<p>However, in many cases, it turns out that some of the higher dimensions in NLP results can be removed without seriously impacting either the visualization or the information produced by NLP. This is called <strong class="bold">dimensionality reduction</strong>. Dimensionality reduction techniques are used to remove the less important dimensions from the data so that it can be displayed more meaningfully. We will look at dimensionality reduction in more detail in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">.</span></p>
			<p>Note that dimensionality reduction was done in <em class="italic">Figures 6.28</em> and <em class="italic">6.29</em> in order to display the results on a <span class="No-Break">two-dimensional page.</span></p>
			<p>The final<a id="_idIndexMarker539"/> topic in this chapter addresses some things we can learn from visualizations and includes some suggestions for using visualization to decide how to proceed with the next phases of an <span class="No-Break">NLP project.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor142"/>Using information from visualization to make decisions about processing</h1>
			<p>This section <a id="_idIndexMarker540"/>includes guidance about how visualization can help us make decisions about processing. For example, in making a decision about whether to remove punctuation and stopwords, exploring word frequency visualizations such as frequency distribution and word clouds can tell us whether very common words are obscuring patterns in <span class="No-Break">the data.</span></p>
			<p>Looking at frequency distributions of words for different categories of data can help rule out simple keyword-based <span class="No-Break">classification techniques.</span></p>
			<p>Frequencies of different kinds of items, such as words and bigrams, can yield different insights. It can also be worth exploring the frequencies of other kinds of items, such as parts of speech or syntactic categories such as <span class="No-Break">noun phrases.</span></p>
			<p>Displaying document similarities with clustering can provide insight into the most meaningful number of classes that you would want to use in dividing <span class="No-Break">a dataset.</span></p>
			<p>The final section<a id="_idIndexMarker541"/> summarizes the information that we learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor143"/>Summary</h1>
			<p>In this chapter, we learned about some techniques for the initial exploration of text datasets. We started out by exploring data by looking at the frequency distributions of words and bigrams. We then discussed different visualization approaches, including word clouds, bar graphs, line graphs, and clusters. In addition to visualizations based on words, we also learned about clustering techniques for visualizing similarities among documents. Finally, we concluded with some general considerations for developing visualizations and summarized what can be learned from visualizing text data in various ways. The next chapter will cover how to select approaches for analyzing NLU data and two kinds of representations for text data – symbolic representations and <span class="No-Break">numerical representations.</span></p>
		</div>
	</body></html>