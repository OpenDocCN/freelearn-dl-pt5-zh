- en: Introduction to Magenta and Generative Art
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll learn the basics of generative music and what already
    exists. You'll learn about the new techniques of artwork generation, such as machine
    learning, and how those techniques can be applied to produce music and art. Google's
    Magenta open source research platform will be introduced, along with Google's
    open source machine learning platform TensorFlow, along with an overview of its
    different parts and the installation of the required software for this book. We'll
    finish the installation by generating a simple MIDI file on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of generative artwork
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New techniques with machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magenta and TensorFlow in music generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Magenta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the music software and synthesizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the code editing software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a basic MIDI file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll use the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**, **Conda**, and **pip**, to install and execute the Magenta environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magenta**, to test our setup by performing music generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magenta GPU** (**optional**), CUDA drivers, and cuDNN drivers, to make Magenta
    run on the GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FluidSynth**, to listen to the generated music sample using a software synthesizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other optional software we might use throughout this book, such as **Audacity**
    for audio editing, **MuseScore** for sheet music editing, and **Jupyter Notebook**
    for code editing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is recommended that you follow this book''s source code when you read the
    chapters in this book. The source code also provides useful scripts and tips.
    Follow these steps to check out the code in your user directory (you can use another
    location if you want):'
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to install Git, which can be installed on any platform by downloading
    and executing the installer at [git-scm.com/downloads](https://git-scm.com/downloads).
    Then, follow the prompts and make sure you add the program to your PATH so that
    it is available on the command line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, clone the source code repository by opening a new Terminal and executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each chapter has its own folder; `Chapter01`, `Chapter02`, and so on. For example,
    the code for this chapter is located at [https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter01](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter01).
    The examples and code snippets will be located in this chapter's folder. For this
    chapter, you should open `cd Chapter01` before you start.
  prefs: []
  type: TYPE_NORMAL
- en: We won't be using a lot of Git commands except `git clone`, which duplicates
    a code repository to your machine, but if you are unfamiliar with Git and want
    to learn more, a good place to start is the excellent Git Book ([git-scm.com/book/en/v2](https://git-scm.com/book/en/v2)),
    which is available in multiple languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2O847tW](http://bit.ly/2O847tW)'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of generative art
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **generative art** has been coined with the advent of the computer,
    and since the very beginning of computer science, artists and scientists used
    technology as a tool to produce art. Interestingly, generative art predates computers,
    because generative systems can be derived by hand.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll provide an overview of generative music by showing you
    interesting examples from art history going back to the 18th century. This will
    help you understand the different types of generative music by looking at specific
    examples and prepare the groundwork for later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Pen and paper generative music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a lot of examples of generative art in the history of mankind. A popular
    example dates back to the 18th century, where a game called Musikalisches Würfelspiel
    (German for *musical dice game*) grew popular in Europe. The concept of the game
    was attributed to Mozart by Nikolaus Simrock in 1792, though it was never confirmed
    to be his creation.
  prefs: []
  type: TYPE_NORMAL
- en: The players of the game throw a dice and from the result, select one of the
    predefined 272 musical measures from it. Throwing the dice over and over again
    allows the players to compose a full minute (the musical genre that is generated
    by the game) that respects the rules of the genre because it was composed in such
    a way that the possible arrangements sound pretty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table and the image that follows, a small part of a musical
    dice game can be seen. In the table, the y-axis represents the dice throw outcome
    while the x-axis represents the measure of the score you are currently generating.
    The players will throw two dices 16 times:'
  prefs: []
  type: TYPE_NORMAL
- en: On the first throw of two dices, we read the first column. A total of two will
    output the measure 96 (first row), a total of two will output the measure 32 (second
    row), and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the second throw of two dices, we read the second column. A total of two
    will output the measure 22 (first row), a total of three will output the measure
    6 (second row), and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After 16 throws, the game will have output 16 measures for the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 96 | 22 | 141 | 41 | 105 | 122 | 11 | 30 | 70 | 121 | 26 | 9 | 112 |
    49 | 109 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 32 | 6 | 128 | 63 | 146 | 46 | 134 | 81 | 117 | 39 | 126 | 56 | 174 |
    18 | 116 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 69 | 95 | 158 | 13 | 153 | 55 | 110 | 24 | 66 | 139 | 15 | 132 | 73 |
    58 | 145 | 79 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 40 | 17 | 113 | 85 | 161 | 2 | 159 | 100 | 90 | 176 | 7 | 34 | 67 | 160
    | 52 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 148 | 74 | 163 | 45 | 80 | 97 | 36 | 107 | 25 | 143 | 64 | 125 | 76 |
    136 | 1 | 93 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 104 | 157 | 27 | 167 | 154 | 68 | 118 | 91 | 138 | 71 | 150 | 29 | 101
    | 162 | 23 | 151 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 152 | 60 | 171 | 53 | 99 | 133 | 21 | 127 | 16 | 155 | 57 | 175 | 43
    | 168 | 89 | 172 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 119 | 84 | 114 | 50 | 140 | 86 | 169 | 94 | 120 | 88 | 48 | 166 | 51
    | 115 | 72 | 111 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 98 | 142 | 42 | 156 | 75 | 129 | 62 | 123 | 65 | 77 | 19 | 82 | 137
    | 38 | 149 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 3 | 87 | 165 | 61 | 135 | 47 | 147 | 33 | 102 | 4 | 31 | 164 | 144 |
    59 | 173 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 54 | 130 | 10 | 103 | 28 | 37 | 106 | 5 | 35 | 20 | 108 | 92 | 12 |
    124 | 44 | 131 |'
  prefs: []
  type: TYPE_TB
- en: The preceding table shows a small part of the whole score, with each measure
    annotated with an index. For each of the generated 16 indexes, we take the corresponding
    measure in order, which constitutes our minuet (the minuet is the style that's
    generated by this game – basically, it's a music score with specific rules).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of generative properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chance or randomness**, which the dice game is a good example of, where the
    outcome of the generated art is partially or totally defined by chance. Interestingly,
    adding randomness to a process in art is often seen as *humanizing* the process,
    since an underlying rigid algorithm might generate something that sounds *artificial*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic generation** (or rule-based generation), where the rules of the
    generation will define its outcome. Good examples of such generation include a
    cellular automaton, such as the popular Conway''s Game of Life, a game where a
    grid of cells changes each iteration according to predefined rules: each cell
    might be on or off, and the neighboring cells are updated as a function of the
    grid''s state and rules. The result of such generation is purely deterministic;
    it has no randomness or probability involved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic-based generation**, where sequences are derived from the probability
    of elements. Examples of this include Markov chains, a stochastic model in which
    for each element of a sequence, the resulting probability of the said event is
    defined only on the present state of the system. Another good example of stochastic-based
    generation is machine learning generation, which we''ll be looking at throughout
    this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use a simple definition of generative art for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>"Generative art is an artwork partially or completely created by an autonomous
    system"</q>.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should understand that we don't actually need a computer to generate
    art since the rules of a system can be derived by hand. But using a computer makes
    it possible to define complex rules and handle tons of data, as we'll see in the
    following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Computerized generative music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first instance of generative art by computer dates back to 1957, where Markov
    chains were used to generate a score on an electronic computer, the ILLIAC I,
    by composers Lejaren Hiller and Leonard Issacson. Their paper, *Musical Composition
    with a High-Speed Digital Computer*, describes the techniques that were used in
    composing the music. The composition, titled *Illac Suite*, consists of four movements,
    each exploring a particular technique of music generation, from a rule-based generation
    of *cantus firmi* to stochastic generation with Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: Many famous examples of generative composition have followed since, such as
    Xenakis's *Atrées* in 1962, which explored the idea of stochastic composition;
    Ebcioglo's composition software named CHORAL, which contained handcrafted rules;
    and David Cope's software called EMI, which extended the concept to be able to
    learn from a corpus of scores.
  prefs: []
  type: TYPE_NORMAL
- en: As of today, generative music is everywhere. A lot of tools allow musicians
    to compose original music based on the generative techniques we described previously.
    A whole genre and musical community, called **algorave**, originated from those
    techniques. Stemming from the underground electronic music scene, musicians use
    generative algorithms and software to produce live dance music on stage, hence
    the name of the genre. Software such as *TidalCycles* and *Orca* allow the musician
    to define rules on the fly and let the system generate the music autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back on those techniques, stochastic models such as Markov chains have
    been widely used in generative music. It stems from the fact that they are conceptually
    simple and easy to represent since the model is a transition probability table
    and can learn from a few examples. The problem with Markov models is that representing
    a long-term temporal structure is hard since most models will only consider *n*
    previous states, where *n* is small, to define the resulting probability. Let's
    take a look at what other types of models can be used to generate music.
  prefs: []
  type: TYPE_NORMAL
- en: In a 2012 paper titled *Ten Questions Concerning Generative Computer Art*, the
    author talks about the possibility of machine creation, the formalization of human
    aesthetics, and randomness. More importantly, it defines the limitations of such
    systems. What can a generative system produce? Can machines only do what they
    are instructed to?
  prefs: []
  type: TYPE_NORMAL
- en: New techniques with machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is important for computer science because it allows complex
    functions to be modeled without them being explicitly written. Those models are
    automatically learned from examples, instead of being manually defined. This has
    a huge implication for arts in general since explicitly writing the rules of a
    painting or a musical score is inherently difficult.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the advent of deep learning has propelled machine learning
    to new heights in terms of efficiency. Deep learning is especially important for
    our use case of music generation since using deep learning techniques doesn't
    require a preprocessing step of *feature extraction*, which is necessary for classical
    machine learning and hard to do on raw data such as image, text, and – you guessed
    it – audio. In other words, traditional machine learning algorithms do not work
    well for music generation. Therefore, all the networks in this book will be deep
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll learn what advances in deep learning allow for music
    generation and introduce the concepts we'll be using throughout this book. We'll
    also look at the different types of musical representations for those algorithms,
    which is important as it will serve as the groundwork for this book for data in
    general.
  prefs: []
  type: TYPE_NORMAL
- en: Advances in deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all know that deep learning has recently become a fast-growing domain in
    computer science. Not so long ago, no deep learning algorithms could outperform
    standard techniques. That was before 2012 when, for the first time, a deep learning
    algorithm, AlexNet, did better in an image classification competition by using
    a deep neural network trained on GPUs (see the *Further reading* section for the
    AlexNet paper, one of the most influential papers that was published in computer
    vision). Neural network techniques are more than 30 years old, but the recent
    reemergence can be explained by the availability of massive data, efficient computing
    power, and technical advances.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, a deep learning technique is *general*, in the sense that,
    as opposed to the music generation techniques we've specified previously, a machine
    learning system is agnostic and can learn from an arbitrary corpus of music. The
    same system can be used in multiple musical genres, as we'll see during this book
    when we train an existing model on jazz music in [Chapter 6](1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml),
    *Data Preparation for Training*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many techniques in deep learning were discovered a long time ago but only find
    meaningful usage today. Of the technical advances in the field that concern music
    generation, those are present in Magenta and will be explained later in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are interesting for music generation
    because they allow us to operate over sequences of vectors for the input and output.
    When using classic neural networks or convolutional networks (which are used in
    image classification), you are limited to a fixed size input vector to produce
    a fixed size output vector, which would be very limiting for music processing,
    but works well for certain types of image processing. The other advantage of RNN
    is the possibility of producing a new state vector at each pass by combining a
    function with the previous state vector, which a powerful mean of describing complex
    behavior and long-term state. We''ll be talking about RNNs in [Chapter 2](b60deee5-c58f-45eb-88a2-23718802e580.xhtml),
    *Generating Drum Sequences with Drums RNN*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long Sho****rt-Term Memory** (**LSTM**) is an RNN with slightly different
    properties. It solves the problem of vanishing gradients that is present in RNNs
    and makes it impossible for the network to learn long-term dependencies, even
    if it theoretically could. The approach of using LSTM in music generation has
    been presented by Douglas Eck and Jurgen Schmidhuber in 2002 in a paper called
    *Finding temporal structure in music: Blues improvisation with LSTM recurrent
    networks*. We''ll be talking about LSTM in [Chapter 3](48023567-4100-492a-a28e-53b18a63e01e.xhtml),
    *Generating Polyphonic Melodies*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational autoencoders** (**VAEs**) are analogous to classical autoencoders,
    in the sense that their architecture is similar, consisting of an encoder (for
    the input to a hidden layer), a decoder (for a hidden layer to the output), and
    a loss function, with the model learning to reconstruct the original input with
    specific constraints. The usage of VAE in generative models is recent but has
    shown interesting results. We''ll be talking about VAE in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with Music VAE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**) are a class of machine learning
    systems where two neural networks compete with each other in a game: a generative
    network generates candidates while a discriminating network evaluates them. We''ll
    be talking about GANs in [Chapter 5](feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml),
    *Audio Generation with NSynth and GANSynth*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recent deep learning advances have profoundly changed not only music generation
    but also genre classification, audio transcription, note detection, composition,
    and more. We won''t be talking about these subjects here, but they all share common
    ground: musical representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Representation in music processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These systems can work with different representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolic representation**, such as the **MIDI** (**Musical Instrument Digital
    Interface** (**MIDI**), describes the music using a notation containing the musical
    notes and timing, but not the sound or timbre of the actual sound. In general,
    sheet music is a good example of this. A symbolic representation of music has
    no sound by itself; it has to be played by instruments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sub-symbolic representation**, such as a raw audio waveform or a spectrogram,
    describes the actual sound of the music.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different processes will require a different representation. For example, most
    speech recognition and synthesis models work with spectrograms, while most of
    the examples we will see in this book uses MIDI to generate music scores. Processes
    that integrate both representations are rare, but an example of this could be
    a score transcription that takes an audio file and translate it into MIDI or other
    symbolic representations.
  prefs: []
  type: TYPE_NORMAL
- en: Representing music with MIDI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other symbolic representations than MIDI, such as MusicXML and AbcNotation,
    but MIDI is by far the most common representation. The MIDI specification also
    doubles down as a protocol since it is used to carry note messages that can be
    used in real-time performance as well as control messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider some parts of a MIDI message that will be useful for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Channel [0-15]**: This indicates the track that the message is sent on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note number [0-127]**: This indicates the pitch of the note'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity [0-127]**: This indicates the volume of the note'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To represent a musical note in MIDI, you have to send two different message
    types with proper timing: a `Note On` event, followed by a `Note Off` event. This
    implicitly defines the **length** of the note, which is not present in the MIDI
    message. This is important because MIDI was defined with live performance in mind,
    so using two messages – one for a keypress and another for a key release – makes
    sense.'
  prefs: []
  type: TYPE_NORMAL
- en: From a data perspective, we'll need either need to convert MIDI notes into a
    format that has the note length encoded in it or keep a note on and note off approach,
    depending on what we're trying to do. For each model in Magenta, we'll see how
    the MIDI notes are encoded.
  prefs: []
  type: TYPE_NORMAL
- en: The following image shows a MIDI representation of a generated drum file, shown
    as a plot of time and pitch. Each MIDI note is represented by a rectangle. Because
    of the nature of percussion data, all the notes have the same length ("note on"
    followed by "note off" messages), but in general, that could vary. A drum file,
    by essence, is polyphonic, meaning that multiple notes can be played at the same
    time. We'll be talking about monophony and polyphony in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the abscissa is expressed in seconds, but it is also common to note
    it with bars or measures. The MIDI channel is absent from this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cd47be5-1cd9-4af3-9f28-a1c24ddbef99.png)'
  prefs: []
  type: TYPE_IMG
- en: The script for plotting a generated MIDI file can be found in the GitHub code
    for this chapter in the `Chapter01/provided` folder. The script is called `midi2plot.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of music generation, the majority of current deep learning systems
    use symbolic notation. This is also the case with Magenta. There are a couple
    of reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to represent the essence of music in terms of composition and harmony
    with symbolic data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing those two types of representations by using a deep learning network
    is similar, so choosing between both boils down to whichever is faster and more
    convenient. A good example of this is that the WaveNet audio generation network
    also has a MIDI implementation, known as the MidiNet symbolic generation network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll see that the MIDI format is not directly used by Magenta, but converted
    into and from `NoteSequence`, a **Protocol Buffers** (**Protobuf**) implementation
    of the musical structure that is then used by TensorFlow. This is hidden from
    the end user since the input and output data is always MIDI. The `NoteSequence`
    implementation is useful because it implements a data format that can be used
    by the models for training. For example, instead of using two messages to define
    a note's length, a `Note` in a `NoteSequence` has a length attribute. We'll be
    explaining the `NoteSequence` implementation as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: Representing music as a waveform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An audio waveform is a graph displaying amplitude changes over time. Zoomed
    out, a waveform looks rather simple and smooth, but zoomed in, we can see tiny
    variations – it is those variations that represent the sound.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how a waveform works, imagine a speaker cone that's is at rest
    when the amplitude is at 0\. If the amplitude moves to a negative value of 1,
    for example, then the speaker moves backward a little bit, or forward in the case
    of a positive value. For each amplitude variation, the speaker will move, making
    the air move, thus making your eardrums move.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the amplitude is in the waveform, the more the speaker cone moves
    in terms of distance, and the louder the sound. This is expressed in **decibel**
    (**dB**), a measure of sound pressure.
  prefs: []
  type: TYPE_NORMAL
- en: The faster the movement, the higher the pitch. This is expressed in **hertz**
    (**Hz**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, we can see the MIDI file from the previous section
    played by instruments to make a WAV recording. The instrument that''s being used
    is a 1982 Roland TR-808 drum sample pack. You can visually match some instruments,
    such as double the Conga Mid (MIDI note 48) at around 4.5 seconds. In the upper
    right corner, you can see a zoom of the waveform at 100th of a second to show
    the actual amplitude change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb612e63-ef48-4c37-aaae-935c764fc653.png)'
  prefs: []
  type: TYPE_IMG
- en: The script for plotting a WAV file can be found in the GitHub code for this
    chapter in the `Chapter01/provided` folder. The script is called `wav2plot.py`.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, using a raw audio waveform used to be uncommon as a data
    source since the computational load is bigger than other transformed representations,
    both in terms of memory and processing. But recent advances in the field, such
    as WaveNet models, makes it on par with other methods of representing audio, such
    as spectrograms, which were historically more popular for machine learning algorithms,
    especially for speech recognition and synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bear in mind that training on audio is really cost-intensive because raw audio
    is a dense medium. Basically, a waveform is a digital recreation of a dynamic
    voltage over time. Simply put, a process called **Pulse Code Modulation** (**PCM**)
    assigns a bit value to each sample at the sampling rate you are running. The sampling
    rate for recording purposes is pretty standard: 44,100 Hz, which is called the
    Nyquist Frequency. But you don''t always need a 44,100 Hz sample rate; for example,
    16,000 Hz is more than enough to cover human speech frequencies. At that frequency,
    the first second of audio is represented by 16,000 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about PCM, the sampling theory for audio, and the Nyquist
    Frequency, check out the *Further reading* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This frequency was chosen for a very specific purpose. Thanks to the Nyquist
    theorem, it allows us to recreate the original audio without a loss of sounds
    that humans can hear.
  prefs: []
  type: TYPE_NORMAL
- en: The human ear can hear sounds up to 20,000 Hz, so you need 40,000 Hz to represent
    it in a waveform since you need a negative value and a positive value to make
    a sound (see the explanation at the beginning of this subsection). Then, you can
    add 4,100 Hz for rounding errors on very low and very high frequencies to make
    44,100 Hz.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good example of a sampled (discrete) representation that can be reversed
    to its original continuous representation because the pitch spectrum the ear can
    hear is limited.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at audio representation in more detail in [Chapter 5](feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml),
    *Audio Generation with NSynth and GANSynth*, since we are going to be using NSynth,
    a Wavenet model, to generate audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: Representing music with a spectrogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, spectrograms have been a popular form of handling audio for machine
    learning, for two reasons – it is compact and extracting features from it is easier.
    To explain this, imagine the raw audio stream of the example from the previous
    section and cut it into chunks of 1/50th of a second (20 milliseconds) for processing.
    Now, you have chunks of 882 samples that are hard to represent; it is a mixed
    bag of amplitudes that don't really represent anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'A spectrogram is the result of doing a Fourier transform on the audio stream.
    A Fourier transform will decompose a signal (a function of time) into its constituent
    frequencies. For an audio signal, this gives us the intensity of a frequency band,
    with a band being a small split of the whole spectrum, for example, 50 Hz. After
    applying a Fourier transform on our previous example and taking sample 1 of the
    882 samples, we''ll end up with the intensity for each frequency band:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[0 Hz - 50 Hz]: a[1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[50 Hz - 100 Hz]: a[2]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[22000 Hz - 22050 Hz:]: a[n]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll end up with intensity *[a[1], a[2], ..., a[n]]* for each band of 50 Hz
    up to 22,050, which is the y-axis, with an assigned color spectrum for smaller
    to bigger intensities. Repeating that for each 20 ms on the x-axis until the whole
    audio is covered gives you a spectrogram. What is interesting in a spectrogram
    is that you can actually see the content of the music. If a C major chord is played,
    you'll see C, E, and G emerge in the spectrogram at their corresponding frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following spectrogram has been generated from the waveform of the previous
    section. From this, you can clearly see the frequencies that are being played
    by the TR 808 from the given MIDI file. You should be able to visually match the
    waveform from the previous section with the spectrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f226d763-cc79-4e90-b897-f514da3c7ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: The script for plotting the spectrogram of a WAV file can be in the GitHub code
    for this chapter in the `Chapter01/provided` folder. The script is called `wav2spectrogram.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spectrograms are mainly used in speech recognition. They are also used in speech
    synthesis: first, a model is trained on spectrograms aligned with text, and from
    there, the model will be able to produce a spectrogram that corresponds to a given
    text. The Griffin-Lim algorithm is used to recover an audio signal from a spectrogram.'
  prefs: []
  type: TYPE_NORMAL
- en: We won't be using spectrograms in this book, but knowing how they work and what
    they are used for is important since they are used in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun fact: musicians have been known to hide images in music that are visible
    when looking at the audio''s spectrogram. A famous example is the Aphex Twin''s
    *Windowlicker* album, where he embedded his grinning face on the second track.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned which deep learning technical advances are important
    in music generation and learned about music representation in those algorithms.
    These two topics are important because we'll be looking at them throughout this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll introduce Magenta, where you'll see much of this
    section's content come into play.
  prefs: []
  type: TYPE_NORMAL
- en: Google's Magenta and TensorFlow in music generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since its launch, TensorFlow has been important for the data scientist community
    for being *An Open Source Machine Learning Framework for Everyone*. Magenta, which
    is based on TensorFlow, can be seen the same way: even if it''s using state of
    the art machine learning techniques, it can still be used by anyone. Musicians
    and computer scientists alike can install it and generate new music in no time.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll look at the content of Magenta by introducing what it
    can and cannot do and refer to the chapters that explain the content in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a music generation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Magenta is a framework for art generation, but also for attention, storytelling,
    and the evaluation of generative music. As the book advances, we'll come to see
    and understand how those elements are crucial for pleasing music generation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and interpreting generative models is inherently hard, especially
    for audio. A common criterion in machine learning is the average log-likelihood,
    which calculates how much the generated samples deviate from the training data,
    which might give you the proximity of two elements, but not the musicality of
    the generated one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if the progress in GANs is promising in such evaluations, we are often
    left with only our ears to evaluate. We can also imagine a Turing test for a music
    piece: a composition is played to an audience that has to decide whether the piece
    was generated by a computer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using Magenta for two different purposes, assisting and autonomous
    music creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assisting music systems** helps with the process of composing music. Examples
    of this would be the Magenta interface, `magenta_midi.py`, where the musician
    can enter a MIDI sequence and Magenta will answer with a generated sequence that''s
    inspired by the provided one. These types of systems can be used alongside traditional
    systems to compose music and get new inspirations. We''ll be talking about this
    in [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml), *Making Magenta Interact
    with Music Applications*, where Magenta Studio can be integrated into a traditional
    music production tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous music systems** continuously produce music without the input of
    an operator. At the end of this book, you''ll have all the tools you''ll need
    to build an autonomous music generation system consisting of the various building
    blocks of Magenta.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at Magenta's content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remembering what we saw in the previous section, there are many ways of representing
    music: symbolic data, spectrogram data, and raw audio data. Magenta works mainly
    with symbolic data, meaning we''ll mainly work on the underlying score in music
    instead of working directly with audio. Let''s look into Magenta''s content, model
    by model.'
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating models, configurations, and pre-trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Magenta and in this book, the term **model** refers to a specific deep neural
    network that is specific for one task. For example, the Drums RNN model is an
    LSTM network with attention configuration, while the MusicVAE model is a variational
    autoencoder network. The Melody RNN model is also an LSTM network but is geared
    toward generating melodies instead of percussion patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Each model has different **configurations** that will change how the data is
    encoded for the network, as well as how the network is configured. For example,
    the Drums RNN model has a `one_drum` configuration, which encodes the sequence
    to a single class, as well as a `drum_kit` configuration, which maps the sequence
    to nine drum instruments and also configures the attention length to 32.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, each configuration comes with one or more **pre-trained models**. For
    example, Magenta provides a pre-trained Drums RNN `drum_kit` model, as well as
    multiple pre-trained MusicVAE `cat-drums_2bar_small` models.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be using this terminology throughout this book. For the first few chapters,
    we'll be using the Magenta pre-trained models, since they are already quite powerful.
    After, we'll create our own configurations and train our own models.
  prefs: []
  type: TYPE_NORMAL
- en: Generating and stylizing images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image generation and stylization can be achieved in Magenta with the *Sketch
    RNN* and *Style Transfer* models, respectively. Sketch-RNN is a **Sequence-to-Sequence**
    (**Seq2Seq**) variational autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: Seq2Seq models are used to convert sequences from one domain into another domain
    (for example, to translate a sentence in English to a sentence in French) that
    do not necessarily have the same length, which is not possible for a traditional
    model structure. The network will encode the input sequence into a vector, called
    a latent vector, from which a decoder will try to reproduce the input sequence
    as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing is not part of this book, but we'll see the usage of latent
    space in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml), *Latent Space
    Interpolation with MusicVAE*, when we use the MusicVAE model. If you are interested
    in the SketchRNN model, see the *Further reading* section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Generating audio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio generation in Magenta is done with the *NSynth*, a WaveNet-based autoencoder,
    and *GANSynth* models. What's interesting about WaveNet is that it is a convolutional
    architecture, prevalent in image applications, but seldom used in music applications,
    in favor of recurrent networks. **Convolutional neural networks** (**CNNs**) are
    mainly defined by a convolution stage, in which a filter is slid through the image,
    computing a feature map of the image. Different filter matrices can be used to
    detect different features, such as edges or curves, which are useful for image
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see the usage of these models in [Chapter 5](feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml),
    *Audio Generation with NSynth and GANSynth*.
  prefs: []
  type: TYPE_NORMAL
- en: Generating, interpolating, and transforming score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Score generation is the main part of Magenta and can be split into different
    categories representing the different parts of a musical score:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rhythms generation**: This can be done with the "Drums RNN" model, an RNN
    network that applies language modeling using an LSTM. Drum tracks are polyphonic
    by definition because multiple drums can be hit simultaneously. This model will
    be presented in [Chapter 2](b60deee5-c58f-45eb-88a2-23718802e580.xhtml), *Generating
    Drum Sequences with Drums RNN*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Melody generation**: Also known as monophonic generation, this can be done
    with the "Melody RNN" and "Improv RNN" models, which also implement the use of
    attention, allowing the models to learn longer dependencies. These models will
    be presented in [Chapter 3](48023567-4100-492a-a28e-53b18a63e01e.xhtml), *Generating
    Polyphonic Melodies*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polyphonic generation**: This can be done with the *Polyphony RNN* and *Performance
    RNN* models, where the latter also implements expressive timing (sometimes called
    groove, where the notes don''t start and stop exactly in the grid, giving it a
    human fell) and dynamics (or velocity). These models will be presented in [Chapter
    3](48023567-4100-492a-a28e-53b18a63e01e.xhtml), *Generating Polyphonic Melodies*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpolation**: This can be done with the MusicVAE model, a variational
    autoencoder that learns the latent space of a musical sequence and can interpolate
    between existing sequences. This model will be presented in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with Music VAE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation**: This can be done with the *GrooVAE* model, a variant of
    the MusicVAE model that will add groove to an existing drum performance. This
    model will be presented in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with Music VAE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Magenta and Magenta for GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing a machine learning framework is not an easy task and often a pretty
    big entry barrier, mainly because Python is an infamous language concerning dependency
    management. We'll try to make this easy by providing clear instructions and versions.
    We'll be covering installation instructions for Linux, Windows, and macOS since
    the commands and versions are mostly the same.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll be installing Magenta and Magenta for GPU, if you have
    the proper hardware. Installing Magenta for a GPU takes a bit more work but is
    necessary if you want to train a model, which we will do in [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml), *Training
    Magenta Models*. If you are unsure about doing this, you can skip this section
    and come back to it later. We'll also provide a solution if you don't have a GPU
    but still want to do the chapter by using cloud-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow will be installed through Magenta's dependencies. We'll also look
    at optional but useful programs that can help you visualize and play audio content.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, newer versions of Python and CUDA are available, but
    we are using the following versions because of incompatibilities with TensorFlow
    and TensorFlow GPU. We''ll be using Magenta 1.1.7 since it is the stable version
    of Magenta at the time of writing. You can try using a newer version for the examples
    and roll back if it doesn''t work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Magenta: 1.1.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow: 1.15.0 (this version is installed automatically when installing
    Magenta)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means that we need to use exactly the following versions for TensorFlow
    to work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python: 3.6.x'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CUDA libraries: 10.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CudRNN: 7.6.x (the latest version is OK)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at how to install those versions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Python environment with Conda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we'll be using a Python environment, a standalone and
    separate installation of Python you can switch to when you are working with a
    specific piece of software, such as when you're working on this book or another
    piece of software. This also ensures that the system-wide installation remains
    safe.
  prefs: []
  type: TYPE_NORMAL
- en: There are many Python environment managers available, but we'll use Conda here,
    which we'll come installed with a standalone Python installation called **Miniconda**.
    You can think of Miniconda as a program with a packaged Python, some dependencies,
    and Conda.
  prefs: []
  type: TYPE_NORMAL
- en: To install Miniconda, go to [docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html),
    download the installer for your platform, choose Python 3.7 as the Python version
    (this is NOT the Python version Magenta will run in), and either 32-bit or 64-bit
    (you probably have the latter).
  prefs: []
  type: TYPE_NORMAL
- en: 'For Windows, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Double-click the installer. Then, follow the prompts and leave the defaults
    as they are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `conda` to PATH by going to `Control Panel > System > Advanced system settings
    > Environment Variables... > Path > Edit... > New` and add the `condabin` folder
    to the Miniconda installation folder (which should be `C:\Users\Packt\Miniconda3\condabin`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are facing issues installing Miniconda on Windows, you can also use **Anaconda**,
    which is the same software but packaged with more tools.
  prefs: []
  type: TYPE_NORMAL
- en: First, download Anaconda from [www.anaconda.com/distribution](https://www.anaconda.com/distribution/),
    double-click the installer, follow the prompts, and leave the defaults as they
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Then, launch **Anaconda Prompt** from the Start menu instead of the **Command
    Prompt**, which will launch a new command-line window with Conda initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 'For macOS and Linux, open a Terminal where you downloaded the file as follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the script executable for your user by replacing `<platform>` with the
    platform you downloaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the script, which will install the software:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that Conda has been installed, let''s check if it works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Terminal and type in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Your output will look different, but the idea is the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to create a new environment for this book. Let''s call it "magenta":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the Python version is 3.6, as we mentioned at the beginning of this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your new environment with the correct version of Python and some dependencies
    has been created. You now have three different Python environments, each with
    a version of Python with its own dependencies :'
  prefs: []
  type: TYPE_NORMAL
- en: '**None**: This is the system-wide installation of Python from the system (this
    might be absent on Windows), and you can switch to it with `conda deactivate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**base**: This is the Miniconda Python installation of Python 3.7 we downloaded,
    and you can switch to it with `conda activate base`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**magenta**: This is our new Python 3.6 installation for this project, and
    you can switch to it with `conda activate magenta`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are still in the base environment, we need to activate the "magenta"
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `activate` flag to change environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From there, your Terminal should prefix the line with "(magenta)", meaning the
    commands you are executing are being executed in this specific environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check our Python version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you have something else here (you should have Python version 3.6.x and Anaconda
    packaging), stop and make sure you followed the installation instructions properly.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a reminder that you **need Python 3.6.x**. An older version of
    Python won't be able to run the code in this book because we are using language
    features from 3.6, and a newer version won't run TensorFlow because it doesn't
    support 3.7 yet.
  prefs: []
  type: TYPE_NORMAL
- en: Installing prerequisite software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a Python environment up and running, you''ll need some prerequisite
    software that will be useful throughout this book. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need to install `curl`, which is preinstalled on Windows and
    macOS, but not Linux (at least not in all distributions). On a Debian distribution,
    use the following command. On other distributions, use your package manager:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to install Visual MIDI, the MIDI visualization library we''ll
    use to make the diagrams of our generated scores. While in the Magenta environment,
    run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll install the tables modules, which will be useful later to read
    external datasets stored in H5 databases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Installing Magenta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our environment and the prerequisite software installed, we can now install
    Magenta version 1.1.7\. You can use a more recent version of Magenta, but do this
    at your own risk: this book''s code was written with version 1.1.7, and the Magenta
    source code has a tendency to change. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the Magenta environment, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you want to try a more recent version of Magenta, just remove the version
    information contained in the `pip` command. Then, it will install the latest version.
    If you have problems using a newer version, you can reinstall version 1.1.7 using
    the `pip install 'magenta==1.1.7' --force-reinstall` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, test the installation by importing Magenta into a Python shell and printing
    the version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Installing Magenta for GPU (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have installed Magenta, we'll install Magenta for GPU, which is
    required for Magenta to execute on the GPU. This is optional if you don't have
    a GPU, are not planning to train any models, or want to use a cloud-based solution
    for training. Before continuing, we need to make sure our GPU is CUDA enabled
    with a compute capability greater than 3.0 by checking out NVIDIA's website: [developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus).
  prefs: []
  type: TYPE_NORMAL
- en: On Windows, you need to download and install the Visual Studio Community IDE
    from [visualstudio.microsoft.com](https://visualstudio.microsoft.com/), which
    should install all the required dependencies for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, for all platforms, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the **CUDA Toolkit** from [developer.nvidia.com/cuda-10.0-download-archive](https://developer.nvidia.com/cuda-10.0-download-archive)
    and launch the installation wizard using any of the provided installation methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the CUDA driver's installation, you might get a message saying that "Your
    display drivers are more recent than the ones provided with this installation".
    This is normal since this CUDA version is not the latest. You can keep your current
    display drivers by selecting **CUDA drivers only**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that CUDA has been installed, you might have to restart your computer to
    load the NVIDIA driver. You can test your installation by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to install the **cuDNN library**, which is a toolkit for executing
    deep learning commands on the GPU with the CUDA driver. You should be able to
    use the most recent cuDNN version from [developer.nvidia.com/rdp/cudnn-download](https://developer.nvidia.com/rdp/cudnn-download)
    for CUDA 10.0\. Choose `Download cuDNN v7.6.x (...), for CUDA 10.0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure you use the `cuDNN Library for Platform` link so that we have the
    full library archive to work with (do not download the `.deb` file, for example).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once downloaded, we''ll have to copy the files from the proper location; see
    the following commands for each platform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linux: [docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'macOS: [docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-mac](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-mac)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windows: [docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we are ready to install Magenta for GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Check out the tip in the *Generating a basic MIDI file* section to verify TensorFlow
    is working properly with your GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the music software and synthesizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the course of this book, we'll be handling MIDI and audio files. Handling
    the MIDI files requires specific software that you should install now since you'll
    need it for the entirety of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the FluidSynth software synthesizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A software synthesizer is a piece of software that will play incoming MIDI notes
    or MIDI files with virtual instruments from sound banks (called SoundFont) or
    by synthesizing audio using waveforms. We will need a software synthesizer to
    play the notes that are generated by our models.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, we'll be using FluidSynth, a powerful and cross-platform software
    synth available on the command line. We'll go through the installation procedure
    for each platform in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing SoundFont
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SoundFont installation is the same for all platforms. We''ll download and
    keep the SoundFont file in an easy-access location since we''ll need it throughout
    this book. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download SoundFont at [ftp.debian.org/debian/pool/main/f/fluid-soundfont/fluid-soundfont_3.1.orig.tar.gz](http://ftp.debian.org/debian/pool/main/f/fluid-soundfont/fluid-soundfont_3.1.orig.tar.gz).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the `.tar.gz` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the `FluidR3_GM.sf2` file to an easy-access location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing FluidSynth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, for **Windows**, binaries are not maintained by the FluidSynth
    core team. Instead of building from the source, we''ll need to fetch the binaries
    from a GitHub project (the versions might be a bit behind the release schedule,
    though). Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the zip at [github.com/JoshuaPrzyborowski/FluidSynth-Windows-Builds/archive/master.zip](https://github.com/JoshuaPrzyborowski/FluidSynth-Windows-Builds/archive/master.zip).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the file and navigate to the `bin64` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the `fluidsynth-2.0.x` folder (containing the latest version of FluidSynth)
    to an easy-access location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the content of the `fluidsynth-required-dlls` file to `C:\Windows\System32`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add FluidSynth to `PATH` by going to `Control Panel > System > Advanced system
    settings > Environment Variables... > Path > Edit... > New` and add the `bin` folder
    from the copied folder from *step 3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Linux**, most distributions maintain a FluidSynth package. Here, we're
    providing the installation instruction for Debian-based distributions. Refer to
    your package manager for other distributions. In a Terminal, use the `sudo apt
    install fluidsynth` command to download FluidSynth.
  prefs: []
  type: TYPE_NORMAL
- en: For **MacOS X**, we'll be using Homebrew to install FluidSynth. Before starting,
    make sure you have the latest Homebrew version. In a Terminal, use the `brew install
    fluidsynth` command to download FluidSynth.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you can test your FluidSynth installation (do this by replacing `PATH_SF2`
    with the path to the SoundFont we installed previously):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux: `fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'macOS: `fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windows: `fluidsynth -g 1 -n -i PATH_TO_SF2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should see an output similar to the following, without any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using a hardware synthesizer (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of using a software synthesizer, you could use a hardware synthesizer
    to listen to your generated MIDI files. We'll look at this in more detail in [Chapter
    9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml), *Making Magenta Interact with
    Music Applications*, but you can already plug the synthesizer into your computer
    via USB; the device should register as a new input MIDI port. This port can be
    used by Magenta to send incoming MIDI notes.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Audacity as a digital audio editor (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We won't be handling audio until [Chapter 5](feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml),
    *Audio Generation with NSynth and GANSynth*, so you can wait until that chapter
    to install Audacity. Audacity is an amazing open source cross-platform (Windows,
    Linux, macOS) software for editor audio clips. It doesn't have the functionality
    of a Digital Audio Workstation (see the *Installing a Digital Audio Workstation* section
    for more on this), but it is easy to use and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Audacity can be used to easily record audio, cut and split audio clips, add
    simple effects, do simple equalization, and export various formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ba1f748-93eb-462c-ac14-8bc2a69726ea.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll be explaining how to use Audacity in [Chapter 5](feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml),
    *Audio Generation with NSynth and GANSynth*.
  prefs: []
  type: TYPE_NORMAL
- en: Installing MuseScore for sheet music (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we'll be working with sheet music a lot, especially MIDI.
    We'll have command-line utilities to generate still images representing the score,
    but it is useful to see and edit the sheet music in a GUI, as well as listen to
    them with digital instruments. Take note that MuseScore cannot play live MIDI,
    so it is different from a software synthesizer. It also doesn't work well with
    expressive timing (where the notes do not fall on the beginning and end steps).
    We'll make note of when not to use MuseScore in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: MuseScore is a good and free notation software available at [musescore.org](https://musescore.org)
    and works on all platforms. You can install it now if you want, or wait until
    later when you need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'MuseScore also doubles down as a collaborative sheet music database at [musescore.com](https://musescore.com),
    which we''ll use throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b61b9e6b-80b5-4f0d-afac-38d7cacdb56f.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing a Digital Audio Workstation (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing a DAW is not necessary for this book, except for [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml),
    *Making Magenta Interact with Music Applications*. Such software comes in various
    forms and complexity and is important in music production in general since it
    can handle all the necessities of music production, such as audio and MIDI handling,
    composition, effects, mastering, VSTs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Ardour ([ardour.org](https://ardour.org)) is the only open source and cross-platform
    DAW available and requires you to pay a small fee for a pre-built version of the
    software. Depending on your platform, you might want to try different DAWs. On
    Linux, you can go with Ardour. On macOS and Windows, you can use Ableton Live,
    a well-established DAW. We won't be recommending any specific software for this
    part, so you can go with whatever you are used to. In [Chapter 9](8018122a-b28e-44ff-8533-5061a0ad356b.xhtml),
    *Making Magenta Interact with Music Applications*, we'll go into more detail by
    giving specific examples for specific DAWs, so you can wait until then to install
    a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the code editing software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll recommend optional software regarding code editing. While
    not mandatory, it might help considerably to use them, especially for newcomers,
    for whom plain code editing software can be daunting.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jupyter Notebook (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Notebooks are a great way of sharing code that contains text, explanations,
    figures, and other rich content. It is used extensively in the data science community
    because it can store and display the result of long-running operations, while
    also providing a dynamic runtime to edit and execute the content in.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this book is available on GitHub as plain Python code, but also
    in the form of Jupyter Notebooks. Each chapter will have its own notebook that
    serves as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Jupyter and launch your first notebook, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the Magenta environment, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the Jupyter server by executing the following command (also
    while in the Magenta environment):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The Jupyter interface will be shown in a web browser. The previous command should
    have launched your default browser. If not, use the URL in the output of the command
    to open it.
  prefs: []
  type: TYPE_NORMAL
- en: Once in the notebook UI, you should see your disk content. Navigate to the code
    for this book and load the notebook from `Chapter01/notebook.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the selected kernel is **Python 3**. This kernel corresponds to the
    Python interpreter that's been installed in your Magenta environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code blocks using the **Run** button for each cell. This will make sure
    that Jupyter executes in a proper environment by printing the TensorFlow and Magenta
    versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is what the notebook should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0e9fce6-c40a-4eb1-aadc-acb4e4e9510d.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing and configuring an IDE (optional)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The usage of an **Integrated Development Environment** (**IDE**) is not necessary
    for this book since all the examples run from the command line. However, an IDE
    is a good tool to use since it provides autocompletion, integrated development
    tools, refactoring options, and more. It is also really useful for debugging since
    you can step into the code directly.
  prefs: []
  type: TYPE_NORMAL
- en: A good IDE for this book is JetBrains's PyCharm ([www.jetbrains.com/pycharm](https://www.jetbrains.com/pycharm)),
    a Python IDE with a community (open source) edition that provides everything you
    need.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you use PyCharm or another IDE, you'll need to change Python interpreter
    to the one we installed previously. This is the equivalent of activating our Magenta
    environment using Conda. In the project settings in the IDE, find the Python interpreter
    settings and change it to the installation path of our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t remember its location, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: On Windows, the Python interpreter is in the root folder, while on Linux or
    macOS, it is in the `bin` directory under it.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a basic MIDI file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Magenta comes with multiple command-line scripts (installed in the `bin` folder
    of your Magenta environment). Basically, each model has its own console script
    for dataset preparation, model training, and generation. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the Magenta environment, download the Drums RNN pre-trained model, `drum_kit_rnn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the following command to generate your first few MIDI files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: By default, the preceding command generates the files in `/tmp/drums_rnn/generated` (on
    Windows `C:\tmp\drums_rnn\generated`). You should see 10 new MIDI files, along
    with timestamps and a generation index.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a GPU, you can verify if TensorFlow is using it properly by
    searching for "Created TensorFlow device ... -> **physical GPU** (name: ..., compute
    capability: ...)" in the output of the script. If it''s not there, this means
    it is executing on your CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also check your GPU usage while Magenta is executing, which should go
    up if Magenta is using the GPU properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to listen to the generated MIDI, use your software synthesizer or
    MuseScore. For the software synth, refer to the following command, depending on
    your platform, and replace `PATH_TO_SF2` and `PATH_TO_MIDI` with the proper values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linux: `fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'macOS: `fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windows: `fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations! You have generated your first musical score using a machine
    learning model! You'll learn how to generate much more throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is important because it introduces the basic concepts of music
    generation with machine learning, all of which we'll build upon throughout this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we learned what generative music is and that its origins predate
    even the advent of computers. By looking at specific examples, we saw different
    types of generative music: random, algorithmic, and stochastic.'
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how machine learning is rapidly transforming how we generate
    music. By introducing music representation and various processes, we learned about
    MIDI, waveforms, and spectrograms, as well as various neural network architectures
    we'll get to look at throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw an overview of what we can do with Magenta in terms of generating
    and processing image, audio, and score. By doing that, we introduced the primary
    models we'll be using throughout this book; that is, Drums RNN, Melody RNN, MusicVAE,
    NSynth, and others.
  prefs: []
  type: TYPE_NORMAL
- en: You also installed your development environment for this book and generated
    your first musical score. Now, we're ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will delve deeper into some of the concepts we introduced in
    this chapter. We'll explain what an RNN is and why it is important for music generation.
    Then, we'll use the Drums RNN model on the command line and in Python while explaining
    its inputs and outputs. We'll finish by creating the first building block of our
    autonomous music generating system.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On what generative principle does the *musical dice game* rely upon?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What stochastic-based generation technique was used in the first computerized
    generative piece of music, *Illiac Suite*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the name of the music genre where a live coder implements generative
    music on the scene?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What model structure is important for tracking temporally distant events in
    a musical score?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between autonomous and assisting music systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are examples of symbolic and sub-symbolic representations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is a note represented in MIDI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What frequency range can be represented without loss at a sample rate of 96
    kHz? Is it better for listening to audio?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a spectrogram, a block of 1 second of intense color at 440 Hz is shown. What
    is being played?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What different parts of a musical score can be generated with Magenta?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ten Questions Concerning Generative Computer Art:** An interesting paper
    (2012) on generative computer art ([users.monash.edu/~jonmc/research/Papers/TenQuestionsLJ-Preprint.pdf](http://users.monash.edu/~jonmc/research/Papers/TenQuestionsLJ-Preprint.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pulse Code Modulation (PCM):** A short introduction to PCM ([www.technologyuk.net/telecommunications/telecom-principles/pulse-code-modulation.shtml](https://www.technologyuk.net/telecommunications/telecom-principles/pulse-code-modulation.shtml)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making Music with Computers:** A good introduction to the sampling theory
    and the Nyquist frequency ([legacy.earlham.edu/~tobeyfo/musictechnology/4_SamplingTheory.html](http://legacy.earlham.edu/~tobeyfo/musictechnology/4_SamplingTheory.html)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SketchRNN model released in Magenta:** A blog post from the Magenta team
    on SketchRNN, with a link to the corresponding paper ([magenta.tensorflow.org/sketch_rnn](https://magenta.tensorflow.org/sketch_rnn)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creation by refinement: a creativity paradigm for gradient descent learning
    networks:** An early paper (1988) on generating content using a gradient-descent
    search ([ieeexplore.ieee.org/document/23933](https://ieeexplore.ieee.org/document/23933)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A First Look at Music Composition using LSTM Recurrent Neural Networks:** An
    important paper (2002) on generating music using LSTM ([www.semanticscholar.org/paper/A-First-Look-at-Music-Composition-using-LSTM-Neural-Eck-Schmidhuber/3b70fbcd6c0fdc7697c93d0c3fb845066cf34487](https://www.semanticscholar.org/paper/A-First-Look-at-Music-Composition-using-LSTM-Neural-Eck-Schmidhuber/3b70fbcd6c0fdc7697c93d0c3fb845066cf34487)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ImageNet Classification with Deep Convolutional Neural Networks:** The AlexNet
    paper, one of the most influential papers that was published in computer vision
    ([papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WaveNet: A Generative Model for Raw Audio:** A paper (2016) on WaveNet ([arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepBach: a Steerable Model for Bach Chorales Generation:** A paper (2016)
    on Bach-like polyphonic music generation ([arxiv.org/abs/1612.01010](https://arxiv.org/abs/1612.01010)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SampleRNN: An Unconditional End-to-End Neural Audio Generation Model:** A
    paper (2017) on generating audio ([arxiv.org/abs/1612.07837](https://arxiv.org/abs/1612.07837)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
