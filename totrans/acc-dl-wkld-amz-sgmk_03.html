<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer019">
<h1 class="chapter-number" id="_idParaDest-62"><a id="_idTextAnchor059"/>3</h1>
<h1 id="_idParaDest-63"><a id="_idTextAnchor060"/>Managing SageMaker Development Environment</h1>
<p>In previous chapters, we learned about the fundamental components and capabilities of Amazon SageMaker. By now, you know how to build and deploy your first simple models on SageMaker. In many more complex cases, however, you will need to write, profile, and test your DL code before deploying it to SageMaker-managed training or hosting clusters. Being able to perform this action locally while mocking SageMaker runtime will shorten development cycles and will avoid any unnecessary costs associated with provisioning SageMaker resources for development.</p>
<p>In this chapter, we will explore how to organize your development environment to effectively develop and test your DL models for SageMaker. This chapter includes considerations for choosing your IDE software for development and testing, as well as simulated SageMaker runtimes on your local machine. We will also provide an overview of available SDKs and APIs to manage your SageMaker resources.</p>
<p>These topics will be covered in the following sections:</p>
<ul>
<li>Selecting a development environment for SageMaker</li>
<li>Debugging SageMaker code locally</li>
</ul>
<p>After reading this chapter, you will be able to set up an efficient development environment compatible with SageMaker, based on your specific use case requirements.</p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor061"/>Technical requirements</h1>
<p>In this chapter, you can use code walk-through samples, so you can develop practical skills. Full code examples are available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter3/">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter3/</a>. To follow along with this code, you will need to have the following:</p>
<ul>
<li>An AWS account and be an IAM user with permission to manage Amazon SageMaker resources.</li>
<li>Have Docker and Docker Compose installed on your local machine. If your development environment has a GPU device, you will need to install <strong class="source-inline">nvidia-docker</strong> (<a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a>).</li>
<li>Have Conda installed (<a href="https://docs.conda.io/en/latest/">https://docs.conda.io/en/latest/</a>).</li>
</ul>
<h1 id="_idParaDest-65"><a id="_idTextAnchor062"/>Selecting a development environment for SageMaker</h1>
<p>The choice of development environment and IDE is typically driven by personal preferences <a id="_idIndexMarker187"/>or corporate policies. SageMaker being <a id="_idIndexMarker188"/>a cloud platform doesn’t restrict you from using an IDE of your choice. You can run an IDE on your local machine or cloud machine (such as Amazon EC2). SageMaker also provides a set of SDKs and packages to simulate SageMaker runtime environments, so you can first test your code using local mocks before deploying anything to the cloud. </p>
<p>With advances in <a id="_idIndexMarker189"/>data science, and machine learning specifically, a <a id="_idIndexMarker190"/>new type of development runtime environment has evolved – <strong class="bold">interactive notebooks</strong>, namely <strong class="bold">Jupyter Notebooks</strong> and <strong class="bold">JupyterLab</strong> (the next generation of Jupyter Notebooks with additional development capabilities such as code debugging). While not <a id="_idIndexMarker191"/>fully replacing a classical IDE, notebooks have become popular because they allow you to explore and visualize data, and develop and share your code with others.</p>
<p>SageMaker provides several managed notebook environments:</p>
<ul>
<li>The <strong class="bold">SageMaker Studio</strong> service – a proprietary serverless notebook IDE for ML development</li>
<li><strong class="bold">SageMaker notebook instances</strong> – a managed Jupyter Notebook/JupyterLab environment</li>
</ul>
<p>All three options – a classical IDE, SageMaker notebook instances, and SageMaker Studio – have certain benefits and may be optimal for a specific set of scenarios. In the following sections, we will review IDE options in detail with their pros and cons as they relate to DL development. </p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor063"/>Setting up a local environment for SageMaker</h2>
<p>There are a <a id="_idIndexMarker192"/>number of benefits of doing your initial <a id="_idIndexMarker193"/>development locally, specifically the following:</p>
<ul>
<li>You don’t incur any running costs for doing your development locally</li>
<li>You can choose your preferred IDE, which results in more efficient development cycles</li>
</ul>
<p>However, local development runtime also has certain limitations. For instance, you cannot test and profile your code on different hardware devices. Getting the latest GPU devices designed for DL workloads can be impractical and not cost-efficient. That’s why, in many cases, you will do initial development and testing of your DL code using a CPU device to troubleshoot initial issues, and then do the final code profiling and tweaking on cloud instances with access to target GPU devices.</p>
<p>SageMaker provides a number of SDKs to allow integration between the local environment and the AWS cloud. Let’s do a practical example of how to configure your local environment to work with remote SageMaker resources.</p>
<h3>Configuring a Python environment</h3>
<p>We start <a id="_idIndexMarker194"/>our configuration by setting up and configuring a Python environment with AWS integration. It’s recommended to use Conda environment management software to isolate your SageMaker local environment:</p>
<ol>
<li>You can start by installing Conda on your local machine using the appropriate installation method (it depends on your local OS). Once Conda is installed, you can create a new Python environment by running the following command in your terminal window:<p class="source-code"><strong class="bold">conda create -n sagemaker python=3.9</strong></p></li>
</ol>
<p>Note that we are explicitly specifying which version of Python interpreter to use in this environment. </p>
<ol>
<li value="2">Next, we switch to create an environment and install the AWS and SageMaker SDKs:<p class="source-code"><strong class="bold">conda activate sagemaker</strong></p><p class="source-code"><strong class="bold">pip install boto3 awscli sagemaker</strong></p></li>
</ol>
<p>Let’s review the SDKs we just installed:</p>
<ul>
<li><strong class="source-inline">awscli</strong> is an AWS CLI toolkit that allows you to programmatically work with any AWS service. It also provides a mechanism to store and use AWS credentials locally.</li>
<li><strong class="source-inline">boto3</strong> is a Python SDK to manage your AWS resources. It uses credentials established by the AWS CLI toolkit to cryptographically sign any management requests and, thus, authenticate in AWS. </li>
<li><strong class="source-inline">sagemaker</strong> – This Python SDK should be already familiar to you at this point of book, as we used it in previous chapters to interact with SageMaker resources such as training jobs or inference endpoints. Unlike <strong class="source-inline">boto3</strong>, the SageMaker SDK abstracts many aspects of the management of underlying resources and is generally recommended whenever you need to programmatically manage your SageMaker workloads.</li>
</ul>
<ol>
<li value="3">Before we proceed, we first need to configure AWS credentials. To do so, you will <a id="_idIndexMarker195"/>need to run the following command in your terminal and provide your AWS access and secret keys:<p class="source-code"><strong class="bold">aws configure</strong></p></li>
</ol>
<p>You can <a id="_idIndexMarker196"/>read the details of how to set up AWS credentials here: <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml</a>.</p>
<h3>Configuring a Jupyter environment </h3>
<p>Once we have the basic Python environments configured and our AWS credentials established, we <a id="_idIndexMarker197"/>are ready to start the Jupyter server. In this example, we will use the JupyterLab environment. However, you are free to configure your own IDE for this purpose, as many IDEs, such as PyCharm and Visual Studio Code, support Jupyter notebooks via plugins or natively. The additional benefit of such an approach is that you can easily switch between your notebooks and training and inference scripts within the same IDE:</p>
<ol>
<li value="1">To install JupyterLab and create a kernel, run the following commands in your terminal:<p class="source-code">conda install -c conda-forge jupyterlabpython -m ipykernel install --user --name sagemaker</p></li>
<li>Next, we start the JupyterLab server on our machine:<p class="source-code">jupyter lab</p></li>
</ol>
<p>Your JupyterLab server should be now available on <strong class="source-inline">http://localhost:8888</strong>.</p>
<h3>Running model training on SageMaker</h3>
<p>In the JupyterLab instance, let’s run some tests to make sure that we can connect and manage SageMaker <a id="_idIndexMarker198"/>resources from our local machine: </p>
<ol>
<li value="1">The full notebook code and training script are in the <strong class="source-inline">chapter3</strong> directory of this book’s GitHub repository:<p class="source-code">import sagemaker, boto3</p><p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">session = sagemaker.Session()</p><p class="source-code">account = boto3.client('sts').get_caller_identity().get('Account')</p><p class="source-code">role = f"arn:aws:iam::{account}:role/service-role/AmazonSageMaker-ExecutionRole-&lt;YOUR_ROLE_ID&gt;" </p></li>
</ol>
<p class="callout-heading">SageMaker execution role</p>
<p class="callout">Please note that you will need to manually define your execution role. For SageMaker managed environments, such as SageMaker Studio or SageMaker notebook instances, you can use the <strong class="source-inline">get_execution_role()</strong> method to retrieve the execution role.</p>
<ol>
<li value="2">Now, we can configure and kick off our SageMaker training the same way as before:<p class="source-code">from sagemaker.pytorch import PyTorch</p><p class="source-code">import os</p><p class="source-code">pytorch_estimator = PyTorch(</p><p class="source-code">                        session=session,</p><p class="source-code">                        entry_point=f'{os.getcwd()}/sources/cifar10.py',</p><p class="source-code">                        role=role,</p><p class="source-code">                        instance_type="ml.m4.xlarge",</p><p class="source-code">                        instance_count=1,</p><p class="source-code">                        job_name="test",</p><p class="source-code">                        framework_version="1.9.0",</p><p class="source-code">                        py_version="py38",</p><p class="source-code">                        hyperparameters={</p><p class="source-code">                            "epochs": 1,</p><p class="source-code">                            "batch-size": 16</p><p class="source-code">                            }</p><p class="source-code">                        )</p><p class="source-code">pytorch_estimator.fit()</p></li>
<li>Once the <a id="_idIndexMarker199"/>training job is done, you can explore locally training results and where output artifacts have been stored:<p class="source-code">pytorch_estimator.latest_training_job.describe()</p></li>
</ol>
<p>As you can see, having a local development environment provides you with the flexibility to choose <a id="_idIndexMarker200"/>your preferred IDE and avoid paying for SageMaker-managed development environments. At the same time, it requires you to carefully manage your development environment, which requires specific expertise and dedicated efforts. Another potential challenge is the synchronization of the development environment between team members. </p>
<h3>Using SageMaker Notebook instances</h3>
<p>SageMaker notebook instances are managed by an AWS Jupyter environment running on top of EC2 instances. You can choose an instance type from a list of CPU and GPU-based instances. SageMaker provides a number of preconfigured Jupyter kernels with Python <a id="_idIndexMarker201"/>runtimes. It includes preconfigured runtimes with versions of PyTorch, TensorFlow, MXNet, and other popular DL and ML frameworks. You can also customize existing kernels (for instance, install new packages) or create fully custom kernels using Conda environment management.</p>
<p>Since a Jupyter environment runs directly on top of an EC2 instance, you can directly observe resource consumption during local training or inference (for example, by monitoring the <strong class="source-inline">nvidia-smi</strong> utility output). You can also run Docker operations such as building custom containers and testing them using SageMaker local mode, which we will discuss in detail in the <em class="italic">Debugging SageMaker code locally</em> section of this chapter.</p>
<p>There are scenarios when using notebook instances can be beneficial, such as the following:</p>
<ul>
<li>You need to <a id="_idIndexMarker202"/>have access to a specific type of hardware to test and debug your model (for example, finding max training throughput without running into OOM issues)</li>
<li>You want to baseline your model performance locally for a specific combination of hyperparameters and hardware before deploying to a remote environment</li>
</ul>
<p>One downside of <a id="_idIndexMarker203"/>notebook instances is the lack of flexibility. You cannot change your instance type quickly if your hardware requirements have changed. That may lead to unnecessary costs when you have a <a id="_idIndexMarker204"/>combination of tasks with different resource requirements.</p>
<p>Let’s consider a scenario where you want to locally preprocess training data and then debug your training script on this data. Typically, data processing is a CPU-bound process and doesn’t require any GPU devices. However, training DL morning will require a GPU device. So, you will have to provision an instance that satisfies the highest hardware <a id="_idIndexMarker205"/>requirements among your tasks. Alternatively, you will have to store your work between tasks and reprovision your notebook instance altogether.</p>
<p>SageMaker addressed this lack of elasticity in a newer product called SageMaker Studio notebooks. Let’s review it in detail.</p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor064"/>Using SageMaker Studio</h2>
<p><strong class="bold">SageMaker Studio</strong> is a web-based interface that allows you to interact with various SageMaker capabilities, from <a id="_idIndexMarker206"/>visual data exploration, to Model Zoo and model training, to code development and endpoint monitoring. SageMaker Studio is intended <a id="_idIndexMarker207"/>to simplify and optimize all steps of ML development by providing a single environment to work and collaborate.</p>
<p>There are multiple capabilities within SageMaker Studio. Let’s review two specific capabilities <a id="_idIndexMarker208"/>relevant for DL development:</p>
<ul>
<li><strong class="bold">Studio notebooks</strong> allow fast <a id="_idIndexMarker209"/>access to different compute instances and runtimes without the need to leave your JupyterLab application</li>
<li><strong class="bold">SageMaker JumpStart</strong> is a collection of prebuilt solutions and Model Zoo that <a id="_idIndexMarker210"/>allows you to deploy your DL solutions in a couple of clicks</li>
</ul>
<p>Next, let’s discuss these capabilities and use cases.</p>
<h3>Studio notebooks</h3>
<p>Studio notebooks <a id="_idIndexMarker211"/>provide a fully managed JupyterLab <a id="_idIndexMarker212"/>environment with the ability to quickly switch between different kernels and compute instances. During the switch, your work is persisted in a shared filesystem automatically. A shared filesystem is highly available and scales seamlessly as needed. Studio notebooks come with a set of prebuilt kernels <a id="_idIndexMarker213"/>similar to notebook instances, which <a id="_idIndexMarker214"/>can be further customized. You can also create a fully custom kernel image for Studio notebooks.</p>
<p>You can choose a compute instance from a wide spectrum of EC2 instances, the latest CPU instances, and specialized GPU instances for training and inference tasks. Studio notebooks have access <a id="_idIndexMarker215"/>to two types of EC2 instances:</p>
<ul>
<li><strong class="bold">Fast instances</strong>, which <a id="_idIndexMarker216"/>allows switching within 2 minutes.</li>
<li><strong class="bold">Regular instances</strong>, for which you need to allow around 5 minutes to start. Note that <a id="_idIndexMarker217"/>this is approximate timing that may be impacted by resource availability in a given AWS region.</li>
</ul>
<p class="callout-heading">Collaborative Capabilities</p>
<p class="callout">Studio notebooks support a sharing capability that allows you to share your code, kernel, and instance configuration with teammates in just a few clicks.</p>
<p>SageMaker notebook <a id="_idIndexMarker218"/>kernels run within Docker images. As a result, there are several limitations:</p>
<ul>
<li>You cannot build or run containers in your Studio notebooks</li>
<li>Studio notebooks don’t support local mode to debug containers before deployment on SageMaker</li>
<li>AWS provides an <strong class="bold">Image Build CLI</strong> to circumvent this limitation and allow users to <a id="_idIndexMarker219"/>build custom containers while working in Studio notebooks</li>
</ul>
<p>For most scenarios, Studio notebooks will be a convenient and cost-efficient alternative to running your own JupyterLab on an EC2 instance or using SageMaker notebook instances. However, you should be mindful of the constraints of Studio notebooks mentioned previously, and assess whether these are a dealbreaker for your particular use case or usage pattern. Additionally, Studio notebooks come as part of the SageMaker Studio platform, which provides additional benefits such as visual data exploration <a id="_idIndexMarker220"/>and processing, visual model <a id="_idIndexMarker221"/>monitoring, prebuilt solutions, UI conveniences for managing your feature stores, model building pipelines, endpoints, experiments, and more.</p>
<h3>SageMaker JumpStart</h3>
<p>SageMaker JumpStart is a library of prebuilt end-to-end ML and DL solutions, sample notebooks, and <a id="_idIndexMarker222"/>models that can be <a id="_idIndexMarker223"/>deployed on SageMaker in one click. JumpStart’s library of solutions and models is large and continuously growing. </p>
<p><strong class="bold">JumpStart solutions</strong> are designed for specific industry use cases, such as transaction fraud detection, document <a id="_idIndexMarker224"/>understanding, and predictive maintenance. Each solution includes multiple integrated components and once deployed can be immediately used by end users. Note that you will need to provide your own dataset to train JumpStart models.</p>
<p><strong class="bold">JumpStart models</strong> provide access to the SOTA model zoo. Depending on your model architecture, you may <a id="_idIndexMarker225"/>choose to immediately deploy this model to inference, fine-tune, train from scratch, or resume incremental training on your own dataset. JumpStart allows users to fully customize user actions, such as defining a size and instance type of training cluster, a hyperparameter of a training job, and the location of data. </p>
<p>Model Zoo includes <a id="_idIndexMarker226"/>models for CV and NLP tasks from TensorFlow Hub, PyTorch Hub, and Hugging Face models.</p>
<p>SageMaker JumpStart can come in handy in scenarios when your business problem can be addressed using generic solutions with your proprietary data. JumpStart can also be a friendly introduction to DL on SageMaker or for non-technical users who are looking to experiment with DL on their own.</p>
<p>In this section, we reviewed available development environment options for SageMaker. All three options come with their pros and cons, and a specific choice is largely driven by personal preferences and use case requirements. It’s generally a good idea to have both a local environment and SageMaker Studio notebooks or notebook instances available. This setup allows you to develop, test, and do initial debugging locally without paying for any cloud resources. Once your code is working locally, you can then easily run the same code on cloud hardware. Studio notebooks can be especially useful, as they allow you to easily <a id="_idIndexMarker227"/>switch between different <a id="_idIndexMarker228"/>CPU and GPU runtimes without leaving your Jupyter notebook, so you can experiment with your training config (for instance, tweak the batch size or gradient accumulation).</p>
<p>In the next section, we will focus on how to efficiently debug your SageMaker code locally before moving your workload to SageMaker cloud resources.</p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor065"/>Debugging SageMaker code locally</h1>
<p>To simplify code development and testing locally, SageMaker supports <strong class="bold">local mode</strong>. This mode <a id="_idIndexMarker229"/>allows you to run your training, inference, or data processing locally in SageMaker containers. This is particularly helpful when you want to troubleshoot your scripts before provisioning any SageMaker resources. </p>
<p>Local mode is supported for all SageMaker images as well as custom SageMaker-compatible images. It is <a id="_idIndexMarker230"/>implemented as part of the <strong class="source-inline">sagemaker</strong> Python SDK. When running your jobs in local mode, the SageMaker SDK under the hood creates a Docker Compose YAML file with your job parameters and starts a relevant container locally. The complexities of configuring a Docker runtime environment are abstracted from the user.</p>
<p>Local mode is supported <a id="_idIndexMarker231"/>for both CPU and GPU devices. You can run the following types of SageMaker jobs in local mode:</p>
<ul>
<li>Training job</li>
<li>Real-time endpoint</li>
<li>Processing job</li>
<li>Batch transform job</li>
</ul>
<h3>Limitations of local mode</h3>
<p>There are <a id="_idIndexMarker232"/>several limitations when running your SageMaker jobs locally: </p>
<ul>
<li>Only one local endpoint is supported.</li>
<li>Distributed local training for a GPU is not supported. However, you can run distributed jobs on a CPU.</li>
<li>EFS and FSx for Lustre are <a id="_idIndexMarker233"/>not supported as data sources.</li>
<li><strong class="source-inline">Gzip</strong> compression, Pipe mode, or manifest files for input are not supported.</li>
</ul>
<h3>Running training and inference in local mode</h3>
<p>Let’s train a simple <a id="_idIndexMarker234"/>model in local mode and then deploy an inference endpoint locally. The full notebook code and training script are in the <strong class="source-inline">chapter3</strong> directory of the book repository:</p>
<ol>
<li value="1">We start by installing all required dependencies for local mode:<p class="source-code">pip install 'sagemaker[local]' –upgrade</p></li>
<li>We then configure the SageMaker local runtime. Note that we are using the <strong class="source-inline">LocalSession</strong> class to let the SageMaker SDK know that we want to provision resources locally:<p class="source-code">import boto3</p><p class="source-code">from sagemaker.local import LocalSession</p><p class="source-code">sagemaker_local_session = LocalSession()</p><p class="source-code">sagemaker_local_session.config = {'local': {'local_code': True}}</p><p class="source-code">account = boto3.client('sts').get_caller_identity().get('Account')</p><p class="source-code">role = f"arn:aws:iam::{account}:role/service-role/AmazonSageMaker-ExecutionRole-&lt;YOUR_ROLE_ID&gt;" </p></li>
<li>In this notebook, we intend to use a public PyTorch image from the SageMaker ECR repository. For this, we need to store credentials so that the Docker daemon can pull the images. Run the following command in your notebook (you can also run it in your terminal window; just remove <strong class="source-inline">!</strong>):<p class="source-code">! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com</p></li>
<li>Now, we need to decide whether we will use a GPU (if available) or CPU device (the default choice). The following code snippet determines whether a CUDA-compatible device <a id="_idIndexMarker235"/>is available (the <strong class="source-inline">"local_gpu"</strong> value) and, if not, defaults to a CPU device (the <strong class="source-inline">"local"</strong> value):<p class="source-code">import subprocess</p><p class="source-code">instance_type = "local"</p><p class="source-code">try:</p><p class="source-code">    if subprocess.call("nvidia-smi") == 0:</p><p class="source-code">        instance_type = "local_gpu"</p><p class="source-code">except:</p><p class="source-code">    print("GPU device with CUDA is not available")</p><p class="source-code">print("Instance type = " + instance_type)</p></li>
<li>Once we define which local device to use, we configure and run a SageMaker training job:<p class="source-code">from sagemaker.pytorch import PyTorch</p><p class="source-code">import os</p><p class="source-code"># Configure an MXNet Estimator (no training happens yet)</p><p class="source-code">pytorch_estimator = PyTorch(</p><p class="source-code">                        session=sagemaker_local_session,</p><p class="source-code">                        entry_point=f'{os.getcwd()}/sources/cifar10.py',</p><p class="source-code">                        role=role,</p><p class="source-code">                        instance_type=instance_type,</p><p class="source-code">                        instance_count=1,</p><p class="source-code">                        job_name="test",</p><p class="source-code">                        framework_version="1.9.0",</p><p class="source-code">                        py_version="py38",</p><p class="source-code">                        hyperparameters={</p><p class="source-code">                            "epochs": 1,</p><p class="source-code">                            "batch-size": 16</p><p class="source-code">                            }</p><p class="source-code">                        )</p><p class="source-code">pytorch_estimator.fit()</p></li>
<li>The SageMaker Python SDK performs the following operations automatically:<ul><li>Pulls the <a id="_idIndexMarker236"/>appropriate PyTorch image from a public ECR repository</li><li>Generates a <strong class="source-inline">docker-compose.yml</strong> file with appropriate volume mount points to access code and training data</li><li>Starts a Docker container with the <strong class="source-inline">train</strong> command</li></ul></li>
</ol>
<p>SageMaker will output the Docker Compose command and the STDOUT/STDERR of the training container to a Jupyter cell.</p>
<p class="callout-heading">Debugging code inside a container</p>
<p class="callout">Many modern IDEs support debugging an application running inside a container. For instance, you can set a breakpoint in your training code. The code execution inside the container will stop so that you can inspect whether it’s executing correctly. Consult your IDE documentation on how to set it up.</p>
<p>After the training job has finished, let’s see how we can deploy a trained model to a local real-time <a id="_idIndexMarker237"/>endpoint. Note that, by default, we are training only for a single epoch, so don’t expect great results!</p>
<ol>
<li value="1">You can deploy an inference container locally just by running the <strong class="source-inline">deploy()</strong> method on your estimator:<p class="source-code">pytorch_estimator.deploy(initial_instance_count=1, instance_type=instance_type)</p></li>
<li>Once the endpoint is deployed, the SageMaker SDK will start sending the output of the model server to a Jupyter cell. You can also observe container logs in the Docker client UI or via the <strong class="source-inline">docker logs CONTAINER_ID</strong> terminal command.</li>
<li>We can now send a test image and observe how our inference scripts handle an inference request in the Docker logs:<p class="source-code">import requests</p><p class="source-code">import json </p><p class="source-code">payload = trainset[0][0].numpy().tobytes()</p><p class="source-code">url = 'http://127.0.0.1:8080/invocations'</p><p class="source-code">content_type = 'application/x-npy'</p><p class="source-code">accept_type = "application/json"</p><p class="source-code">headers = {'content-type': content_type, 'accept': accept_type}</p><p class="source-code">response = requests.post(url, data=payload, headers=headers)</p><p class="source-code">print(json.loads(response.content)[0])</p></li>
</ol>
<p>In the preceding code block, we did the following:</p>
<ul>
<li>Formed an inference payload and serialized it to <strong class="source-inline">bytes</strong> objects</li>
<li>Formed <strong class="source-inline">content-type</strong> and <strong class="source-inline">accept-type</strong> HTTP headers to indicate to the inference server what type of content the client is sending and what it expects</li>
<li>Sent a request to the local SageMaker endpoint</li>
<li>Read the <a id="_idIndexMarker238"/>response output</li>
</ul>
<p>If there are any issues, you can log in to a running inference container to examine the runtime environment or set up a debugging session using your IDE capabilities.</p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor066"/>Summary</h1>
<p>In this chapter, we reviewed some available solutions and best practices on how to organize the development of DL code for Amazon SageMaker. Depending on your use case requirements and personal preferences, you can choose a DIY environment locally or use one of SageMaker’s notebook environments – notebook instances and Studio notebooks. You also learned how to test SageMaker DL containers locally to speed up your development efforts and avoid any additional testing costs.</p>
<p>In the next chapter, we will focus on data management and data processing for SageMaker. As many training datasets for DL problems are large and require pre- or post-processing, it’s crucial to understand an optimal storage solution. We also discuss aspects of data labeling and data processing using SageMaker capabilities, as well as the best practices for accessing your training data.</p>
</div>
<div>
<div id="_idContainer020">
</div>
</div>
</div></body></html>