- en: '*Chapter 4*: Image Classification and Regression Using AutoKeras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：使用 AutoKeras 进行图像分类与回归'
- en: In this chapter, we will focus on the use of AutoKeras applied to images. In
    [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, we got our first contact with **deep learning** (**DL**) applied
    to images, by creating two models (a classifier and a regressor) that recognized
    handwritten digits. We will now create more complex and powerful image recognizers,
    examine how they work, and see how to fine-tune them to improve their performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将重点介绍 AutoKeras 在图像上的应用。在[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中，*入门
    AutoKeras*，我们首次接触了应用于图像的**深度学习**（**DL**），通过创建两个模型（一个分类器和一个回归器），实现了对手写数字的识别。我们现在将创建更复杂、更强大的图像识别器，分析它们的工作原理，并学习如何微调它们以提高性能。
- en: After reading this chapter, you will be able to create your own image models
    and apply them, to solve a wide range of problems in the real world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你将能够创建自己的图像模型，并将其应用于解决现实世界中的各种问题。
- en: As we discussed in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the most suitable models for recognizing images
    use a type of neural network called a **convolutional neural network** (**CNN**).
    For the two examples that we will see in this chapter, AutoKeras will also choose
    CNNs for the creation of its models. So, let's see in a little more detail what
    these types of neural networks are and how they work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中讨论的那样，*入门 AutoKeras*，最适合图像识别的模型使用一种叫做**卷积神经网络**（**CNN**）的神经网络类型。对于我们在本章中看到的两个例子，AutoKeras
    也会选择 CNN 来创建其模型。所以，让我们更详细地了解这些类型的神经网络是什么，以及它们是如何工作的。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主要主题：
- en: Understanding CNNs—what are these neural networks and how do they work?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 CNN——这些神经网络是什么，它们是如何工作的？
- en: Creating a CIFAR-10 image classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 CIFAR-10 图像分类器
- en: Creating and fine-tuning a powerful image classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和微调一个强大的图像分类器
- en: Creating an image regressor to find out the age of people
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个图像回归器以找出人的年龄
- en: Creating and fine-tuning a powerful image regressor
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和微调一个强大的图像回归器
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All coding examples in this book are available as Jupyter Notebooks that can
    be downloaded from the following link: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有编码示例都可以作为 Jupyter Notebooks 下载，链接如下：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras)。
- en: As code cells can be executed, each Notebook can be self-installable by adding
    a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook there is a code cell for environmental setup, which installs
    AutoKeras and its dependencies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码单元可以执行，每个笔记本都可以通过添加所需的代码片段进行自我安装。因此，在每个笔记本的开头都有一个环境设置的代码单元，用于安装 AutoKeras
    及其依赖项。
- en: 'So, to run the coding examples, you only need a computer with Ubuntu/Linux
    as the operating system and you can install the Jupyter Notebook with this command
    line:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要运行编码示例，你只需要一台操作系统为 Ubuntu/Linux 的计算机，并且可以通过以下命令行安装 Jupyter Notebook：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can also run these notebooks using Google Colaboratory, in
    which case you will only need a web browser—see the *AutoKeras with Google Colaboratory*
    section in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting
    Started with AutoKeras*, for more details. Furthermore, in the *Installing AutoKeras*
    section, you will also find other installation options. Let's get started by understanding
    CNNs in detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以通过 Google Colaboratory 运行这些笔记本，在这种情况下，你只需要一个网页浏览器——有关更多详情，请参见[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中关于*AutoKeras
    与 Google Colaboratory*的部分，*入门 AutoKeras*。此外，在*安装 AutoKeras*部分，你还可以找到其他安装选项。让我们通过详细了解
    CNN 来开始。
- en: Understanding CNNs
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 CNN
- en: A CNN is a type of neural network, inspired by the functioning of neurons in
    the visual cortex of a biological brain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种神经网络，灵感来源于生物大脑视觉皮层中神经元的工作原理。
- en: These types of networks perform very well in solving computer vision problems
    such as image classification, object detection, segmentation, and so on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这类网络在解决计算机视觉问题（如图像分类、物体检测、分割等）方面表现非常出色。
- en: 'The following screenshot shows how a CNN recognizes a cat:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – How a CNN recognizes a cat'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – How a CNN recognizes a cat
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: But why do these CNNs work so well, compared to a classical fully connected
    model? To answer this, let's dive into what the convolutional and pooling layers
    do.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key building block in a CNN is the convolutional layer, which uses a window
    (kernel) to scan an image and perform transformations on it to detect patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: A kernel is nothing more than a simple neural network fed by the pixel matrix
    of the scanned window that outputs a vector of numbers, which we will use as filters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine a convolutional layer with many small square templates (called
    kernels) that go through an image and look for patterns. When the square of the
    input image matches the kernel pattern, the kernel returns a positive value; otherwise,
    it returns `0` or less.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how a convolutional layer processes an image:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – How a convolutional layer processes an image'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – How a convolutional layer processes an image
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the filters, we have to reduce their dimensions using a pooling
    operation, which is explained next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function of the pooling layer is to progressively reduce the size of the
    input features matrix to reduce the number of parameters and calculations in the
    network. The most common form of pooling is max pooling, which performs downscaling
    by applying a maximum filter to non-overlapping subregions of the input features
    matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot provides an example of max pooling:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Max pooling example](img/B16953_04_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Max pooling example
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see an example of a max pooling operation
    on a features matrix. In the case of an image, this matrix would be made up of
    the pixel values of the image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Applying this operation reduces the computational cost by reducing the number
    of features to process, and it also helps prevent overfitting. Next, we will see
    how the convolutional and pooling layers are combined in a CNN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: CNN structure
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usually, a CNN is made up of a series of convolutional layers, followed by
    a pooling layer (downscaling). This combination is repeated several times, as
    we can see in the following screenshot example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Example of a CNN pipeline](img/B16953_04_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Example of a CNN pipeline
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In this process, the first layer detects simple features such as the outlines
    of an image, and the second layer begins to detect higher-level features. In the
    intermediate layers, it is already capable of detecting more complex shapes, such
    as the nose or eyes. In the final layers, it is usually able to differentiate
    human faces.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: This seemingly simple repetition process is extremely powerful, detecting features
    of a slightly higher order than its predecessor at each step and generating astonishing
    predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Surpassing classical neural networks
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classical neural network uses fully connected (dense) layers as the main feature
    transformation operations, whereas a CNN uses convolution and pooling layers (Conv2D).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'The main differences between a fully connected layer and a convolutional layer
    are outlined here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers learn global patterns in their input feature space (for
    example, in the case of a digit from the **Modified National Institute of Standards
    and Technology** (**MNIST**) dataset, seen in the example from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the input feature space would be all the pixels
    from the image).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the convolution layers learn local patterns—in the case of
    images, patterns found in small two-dimensional windows that run through the image.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see how these little windows detect local
    patterns such as lines, edges, and so on:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network](img/B16953_04_05.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation performs a transformation of the input image through
    a window (2D matrix) that scans it, generating a new image with different features.
    Each of these generated images is called a **filter**, and each filter contains
    different patterns extracted from the original image (edges, axes, straight lines,
    and so on).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The set of filters created in each intermediate layer of a CNN is called a feature
    map, which is a matrix of numbers of dimensions *r x c x n*, where *r* and *c*
    are rows and columns and *n* is the number of filters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Basically, these feature maps are the parameters that CNNs learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As we were already able to see when viewing the architecture of the MNIST classifier
    from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, CNNs stack several convolutional layers (Conv2D) combined with
    pooling layers (MaxPooling2D). The task of the latter consists of reducing the
    dimensions of the filters, keeping the most relevant values. This helps clean
    up noise and reduces training time for the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to implement some practical examples. Let's start with an image
    classifier for a well-known dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CIFAR-10 image classifier
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we are going to create will classify images from a dataset called
    `32x32` **red, green, blue** (**RGB**) colored images, classified into 10 different
    classes. It is a collection of images that is commonly used to train ML and computer
    vision algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the classes in the dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '`airplane`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`automobile`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bird`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deer`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dog`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frog`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horse`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ship`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truck`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next screenshot, you can see some random image samples found in the
    CIFAR-10 dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – CIFAR-10 image samples'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_06.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – CIFAR-10 image samples
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This a problem considered already solved. It is relatively easy to achieve a
    classification accuracy close to 80%. For better performance, we must use deep
    learning CNNs with which a classification precision greater than 90% can be achieved
    in the test dataset. Let's see how to implement it with AutoKeras.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: This is a classification task, so we can use the `ImageClassifier` class. This
    class generates and tests different models and hyperparameters, returning an optimal
    classifier to categorize each image with its corresponding class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The notebook with the complete source code can be found at [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now have a look at the relevant cells of the notebook in detail, as
    follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package manager:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`matplotlib`, a Python plotting library that we will use to plot some digit
    representations, and CIFAR-10, which contains the categorized images dataset.
    The code to import the packages is shown here:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Getting the CIFAR-10 dataset**: We have to first load the CIFAR-10 dataset
    in memory and have a quick look at the dataset shape, as follows:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output of the preceding code:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Although it is a well-known machine learning dataset, it is always important
    to ensure that the data is distributed evenly, to avoid surprises. This can be
    easily done using `numpy` functions, as shown in the following code block:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The samples are perfectly balanced, as you can see in the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Train and test dataset histograms'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_07.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Train and test dataset histograms
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are sure that our dataset is correct, it's time to create our image
    classifier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image classifier
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now use the AutoKeras `ImageClassifier` class to find the best classification
    model. Just for this little example, we set `max_trials` (the maximum number of
    different Keras models to try) to `2`, and we do not set the `epochs` parameter
    so that it will use an adaptive number of epochs automatically. For real use,
    it is recommended to set a large number of trials. The code is shown here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s run the training to search for the optimal classifier for the CIFAR-10
    training dataset, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Notebook output of image classifier training'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_08.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Notebook output of image classifier training
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the accuracy of the training dataset is increasing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: As it has to process thousands of color images, the models that AutoKeras will
    generate will be more expensive to train, so this process will take hours, even
    using `max_trials = 5`). Increasing this number would give us a more accurate
    model, although it would also take longer to finish.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要处理成千上万的彩色图像，AutoKeras生成的模型在训练时会更为昂贵，因此这个过程将需要几个小时，即使使用`max_trials = 5`。增加这个数字将给我们一个更准确的模型，尽管它也会花费更长的时间才能完成。
- en: Improving the model performance
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升模型性能
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在更短时间内获得更高精度，我们可以使用AutoKeras的高级功能来微调我们的模型，允许你自定义搜索空间。
- en: By using `AutoModel` with `ImageBlock` instead of `ImageClassifier`, we can
    create high-level configurations, such as `block_type` for the type of neural
    network to look for. We can also perform data normalization or data augmentation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`AutoModel`与`ImageBlock`代替`ImageClassifier`，我们可以创建高级配置，例如用于指定神经网络类型的`block_type`。我们还可以执行数据标准化或数据增强。
- en: If we have knowledge of deep learning and have faced this problem before, we
    can design a suitable architecture such as an `EfficientNet`-based image classifier,
    for instance, which is a deep residual learning architecture for image recognition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有深度学习的知识，并且之前遇到过这个问题，我们可以设计一个合适的架构，例如基于`EfficientNet`的图像分类器，它是用于图像识别的深度残差学习架构。
- en: 'See the following example for more details:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下示例，了解更多细节：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code block, we have done the following with the settings:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们已经使用以下设置完成了操作：
- en: With `block_type = "efficient"`, AutoKeras will only explore `EfficientNet`
    architectures.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`block_type = "efficient"`，AutoKeras将只探索`EfficientNet`架构。
- en: Initializing `augment = True` means we want to do data augmentation, a technique
    to create new artificial images from the originals. Upon activating it, AutoKeras
    will perform a series of transformations in the original image, as translations,
    zooms, rotations, or flips.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化`augment = True`表示我们希望进行数据增强，这是一种通过原始图像生成新的人工图像的技术。在激活此功能后，AutoKeras将对原始图像执行一系列变换，如平移、缩放、旋转或翻转。
- en: You can also not specify these arguments, in which case these different options
    would be tuned automatically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以不指定这些参数，在这种情况下，这些不同的选项会被自动调整。
- en: 'You can see more details about the `EfficientNet` function here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到关于`EfficientNet`函数的更多细节：
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
- en: '[https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)'
- en: Evaluating the model with the test set
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试集评估模型
- en: 'After training, it is time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can contrast the good results obtained
    from the training set with a dataset never seen before. To do this, we run the
    following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，接下来就是使用保留的测试数据集来衡量模型的实际预测效果。通过这种方式，我们可以将从训练集获得的良好结果与从未见过的数据集进行对比。为此，我们运行以下代码：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see here that prediction accuracy has a margin to improve using our test
    dataset (84.4%), although it's a pretty decent score for just a few hours of training;
    but just increasing the trials, we have achieved 98% of precision training for
    the first model (`ImageClassifier`) and running during one day in Google Colaboratory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用我们的测试数据集（84.4%）预测准确率仍有提升空间，尽管这是仅仅几个小时训练得到的相当不错的得分；但通过增加尝试次数，我们已经实现了首个模型（`ImageClassifier`）的98%精度，并且在Google
    Colaboratory中运行了一整天。
- en: 'Once we have created and trained our classifier model, let''s see how it predicts
    on a subset of test samples. To do this, we run the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建并训练了分类器模型，让我们看看它在一部分测试样本上的预测结果。为此，我们运行以下代码：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.9 – Samples with their predicted and true labels](img/B16953_04_09.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 带有预测标签和真实标签的样本](img/B16953_04_09.jpg)'
- en: Figure 4.9 – Samples with their predicted and true labels
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 带有预测标签和真实标签的样本
- en: We can see that all the predicted samples match their true value, so our classifier
    has predicted correctly. Now, let's take a look inside the classifier to understand
    how it is working.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有预测的样本都与其真实值匹配，因此我们的分类器预测正确。现在，让我们深入了解分类器，理解它是如何工作的。
- en: Visualizing the model
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'Now, we can see a little summary with the architecture of the best generated
    model found (the one with 98% accuracy), and we will explain the reason why its
    performance is so good. Run the following code to see the summary:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Best model architecture summary](img/B16953_04_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Best model architecture summary
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The key layer here is the `efficientnetb7` layer, which implements a cutting-edge
    architecture created by Google. Today, `EfficientNet` models are the best choice
    for image classification because this is a recent architecture that not only focuses
    on improving accuracy but also on the efficiency of the models so that they achieve
    higher precision and better efficiency over existing convolutional network-based
    architectures, reducing parameter sizes and **floating-point operations per second**
    (**FLOPS**) by an order of magnitude. However, we didn't need to know anything
    about it because AutoKeras automatically chose it for us.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the blocks are connected to each other in a more visual way,
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Best model architecture visualization](img/B16953_04_11.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Best model architecture visualization
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: As we explained in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, each block represents a layer, and the output
    of each is connected to the input of the next, except the first block (whose input
    is the image) and the last block (whose output is the prediction). The blocks
    before the `efficientnetb7` layer are all data-preprocessing blocks and they are
    in charge of adapting the image to a suitable format for this `EfficientNet` block,
    as well as generating extra images through augmentation techniques.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Now is the time to tackle a non-classification problem. In the following practical
    example, we are going to create a human-age predictor based on a set of celebrity
    data—a fun tool that could make anyone blush.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Creating an image regressor to find out the age of people
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a model that will find out the age of a person
    from an image of their face. For this, we will train the model with a dataset
    of faces extracted from images of celebrities in **Internet Movie Database** (**IMDb**).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: As we want to approximate the age, we will use an image regressor for this task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next screenshot, you can see some samples taken from this dataset of
    celebrity faces:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – A few image samples from IMDb faces dataset](img/B16953_04_12.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – A few image samples from IMDb faces dataset
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook with the complete source code can be found here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now explain the relevant code cells of the notebook in detail, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package manager. The code is shown here:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Importing needed packages**: We now load AutoKeras and some more used packages,
    such as matplotlib, a Python plotting library that we will use to plot some picture
    samples and the categories distribution. The code to do this is shown here:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Getting the IMDb faces dataset**: Before training, we have to download the
    IMDb cropped faces dataset that contains the images of each face, as well as metadata
    with the age tags.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following command lines are idempotent—they download and extract data only
    if it does not already exist:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output of the preceding code:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`MatLab` file.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b. The age is not in the params—it has to be calculated.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. The images are not homogeneous—they have different dimensions and colors.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To resolve these issues, we have created the following utility functions:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a. `imdb_meta_to_df(matlab_filename)`: This converts the IMDb MatLab file to
    a pandas DataFrame and calculates the age.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. `normalize_dataset(df_train_set)`: This returns a tuple of normalized images
    (resized to `128x128` and converted to grayscale) and ages converted to integers.'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the notebook, you will find more details about how these functions are working.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how to use them, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the previous code snippet, we used the `imdb_meta_to_df` function to convert
    the `imdb` metadata information stored in a MatLab file to a Pandas DataFrame.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'The DataFrame contains a lot of images; to make the training faster, we will
    use only a part of them to create the datasets, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we create the final datasets with normalized images and ages, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once all the images are the same size (128×128) and the same color (grayscale)
    and we have the labels and the estimated age, we are ready to feed our model,
    but first we have to create it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image regressor
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because we want to predict age, and this is a scalar value, we are going to
    use AutoKeras `ImageRegressor` as an age predictor. We set `max_trials` (the maximum
    number of different Keras models to try) to `10`, and we do not set the `epochs`
    parameter so that it will use an adaptive number of epochs automatically. For
    real use, it is recommended to set a large number of trials. The code is shown
    here:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s run the training model to search for the optimal regressor for the training
    dataset, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the output of the preceding code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Notebook output of our age predictor training'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_13.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – Notebook output of our age predictor training
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the loss for the training dataset is decreasing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This training process has taken 1 hour in Colaboratory. We have limited the
    search to 10 architectures (`max_trials = 10`) and restricted the number of images
    to 10,000\. Increasing these numbers would give us a more accurate model, although
    it would also take longer to finish.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程在 Colaboratory 中花费了 1 小时。我们将搜索限制为 10 种架构（`max_trials = 10`），并将图像数量限制为
    10,000。增加这些数字将给我们一个更精确的模型，尽管也会花费更多时间。
- en: Improving the model performance
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升模型性能
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在更短的时间内获得更高的精度，我们可以使用 AutoKeras 的高级功能微调我们的模型，从而自定义搜索空间。
- en: As we did earlier in the regressor example, we can use `AutoModel` with `ImageBlock`
    instead of `ImageRegressor` so that we can implement high-level configurations,
    such as define a specific architecture neural network to search using `block_type`.
    We can also perform data preprocessing operations, such as normalization or augmentation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在回归模型示例中所做的那样，我们可以使用 `AutoModel` 配合 `ImageBlock`，而不是使用 `ImageRegressor`，这样我们就可以实现更高级的配置，比如使用
    `block_type` 搜索特定架构的神经网络。我们还可以执行数据预处理操作，比如归一化或增强。
- en: As we did in the previous image classifier example, we can design a suitable
    architecture as an `EfficientNet`-based image regressor, for instance, which is
    a deep residual learning architecture for image recognition.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的图像分类器示例中所做的那样，我们可以设计一个合适的架构，例如基于 `EfficientNet` 的图像回归器，这是一种用于图像识别的深度残差学习架构。
- en: 'See the following example for more details:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下示例以获取更多细节：
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the previous code, we have done the following with the settings:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们已经做了以下设置：
- en: The `Normalization` block will transform all image values in the range between
    0 and 255 to floats between 0 and 1.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalization` 块会将所有图像值从 0 到 255 的范围转换为 0 到 1 之间的浮动值。'
- en: The shape has been set (60000, 28 * 28) with values between 0 and 1.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经设置了形状（60000, 28 * 28），值在 0 和 1 之间。
- en: With `ImageBlock(block_type="efficient"`, we are telling AutoKeras to only scan
    `EfficientNet` architectures.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ImageBlock(block_type="efficient"`，我们告诉 AutoKeras 只扫描 `EfficientNet` 架构。
- en: The `ImageAugmentation` block performs data augmentation, a technique to create
    new artificial images from the originals.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageAugmentation` 块执行数据增强，这是通过原始图像生成新的人工图像的一种技术。'
- en: You can also not specify any of these arguments, in which case these different
    options would be tuned automatically.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以选择不指定这些参数，在这种情况下，系统将自动调整这些不同的选项。
- en: 'You can see more details about the `EfficientNet` function here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看更多关于 `EfficientNet` 函数的细节：
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
- en: Evaluating the model with the test set
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试集评估模型
- en: 'After training, it''s time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can rule out that the good results
    obtained with the training set are due to overfitting. The code to do this is
    shown here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，是时候使用保留的测试数据集来衡量我们模型的实际预测效果了。这样，我们可以排除训练集获得的好结果是由于过拟合造成的。执行此操作的代码如下：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the output of the preceding code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This error still has a lot of margin to improve, but let''s have a look at
    how it''s predicting over a subset of test samples, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误还有很大的提升空间，但让我们看看它是如何在一部分测试样本上进行预测的，如下所示：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the output of the preceding code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 4.14 – Samples with their predicted and true labels'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 – 样本及其预测标签与真实标签'
- en: '](img/B16953_04_14.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_14.jpg)'
- en: Figure 4.14 – Samples with their predicted and true labels
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 样本及其预测标签与真实标签
- en: We can see that some predicted samples are near to the real age of the person
    but others aren't, so investing in more training hours and fine-tuning will make
    it predict better. Let's take a look inside the classifier to understand how it
    is working.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些预测样本接近真实年龄，但另一些则不然，因此投入更多的训练时间和微调将使其预测得更好。让我们深入了解分类器，理解它是如何工作的。
- en: Visualizing the model
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'We can now see a little summary with the architecture of the best generated
    model found by running the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到通过运行以下代码找到的最佳生成模型的架构简要总结：
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output of the preceding code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 4.15 – Best model architecture summary'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15 – 最佳模型架构总结'
- en: '](img/B16953_04_15.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_15.jpg)'
- en: Figure 4.15 – Best model architecture summary
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The key layers here are the convolution and pooling blocks, as we explained
    at the beginning of this chapter. These layers learn local patterns from the image
    that help to perform the predictions. Here is a visual representation of this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Best model architecture visualization](img/B16953_04_16.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Best model architecture visualization
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: First, there are some data preprocessing blocks that normalize the images and
    do data augmentation; then, there are several stacked convolution and pooling
    blocks; then, a dropout block to do the regularization (a technique to reduce
    overfitting based on dropping random neurons while training, to reduce the correlation
    between the closest neurons); and finally, we see the regression block, to convert
    the output to a scalar (the age).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how convolutional networks work, how to implement
    an image classifier, and how to fine-tune it to improve its accuracy. We have
    also learned how to implement an image regressor and fine-tune it to improve its
    performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to work with images, we are ready to move on to
    the next chapter, where you will learn how to work with text by implementing classification
    and regression models using AutoKeras.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
