- en: '*Chapter 4*: Image Classification and Regression Using AutoKeras'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the use of AutoKeras applied to images. In
    [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, we got our first contact with **deep learning** (**DL**) applied
    to images, by creating two models (a classifier and a regressor) that recognized
    handwritten digits. We will now create more complex and powerful image recognizers,
    examine how they work, and see how to fine-tune them to improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you will be able to create your own image models
    and apply them, to solve a wide range of problems in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the most suitable models for recognizing images
    use a type of neural network called a **convolutional neural network** (**CNN**).
    For the two examples that we will see in this chapter, AutoKeras will also choose
    CNNs for the creation of its models. So, let's see in a little more detail what
    these types of neural networks are and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs—what are these neural networks and how do they work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a CIFAR-10 image classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an image regressor to find out the age of people
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All coding examples in this book are available as Jupyter Notebooks that can
    be downloaded from the following link: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).'
  prefs: []
  type: TYPE_NORMAL
- en: As code cells can be executed, each Notebook can be self-installable by adding
    a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook there is a code cell for environmental setup, which installs
    AutoKeras and its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to run the coding examples, you only need a computer with Ubuntu/Linux
    as the operating system and you can install the Jupyter Notebook with this command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can also run these notebooks using Google Colaboratory, in
    which case you will only need a web browser—see the *AutoKeras with Google Colaboratory*
    section in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting
    Started with AutoKeras*, for more details. Furthermore, in the *Installing AutoKeras*
    section, you will also find other installation options. Let's get started by understanding
    CNNs in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A CNN is a type of neural network, inspired by the functioning of neurons in
    the visual cortex of a biological brain.
  prefs: []
  type: TYPE_NORMAL
- en: These types of networks perform very well in solving computer vision problems
    such as image classification, object detection, segmentation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how a CNN recognizes a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – How a CNN recognizes a cat'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – How a CNN recognizes a cat
  prefs: []
  type: TYPE_NORMAL
- en: But why do these CNNs work so well, compared to a classical fully connected
    model? To answer this, let's dive into what the convolutional and pooling layers
    do.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key building block in a CNN is the convolutional layer, which uses a window
    (kernel) to scan an image and perform transformations on it to detect patterns.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel is nothing more than a simple neural network fed by the pixel matrix
    of the scanned window that outputs a vector of numbers, which we will use as filters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine a convolutional layer with many small square templates (called
    kernels) that go through an image and look for patterns. When the square of the
    input image matches the kernel pattern, the kernel returns a positive value; otherwise,
    it returns `0` or less.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how a convolutional layer processes an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – How a convolutional layer processes an image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – How a convolutional layer processes an image
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the filters, we have to reduce their dimensions using a pooling
    operation, which is explained next.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function of the pooling layer is to progressively reduce the size of the
    input features matrix to reduce the number of parameters and calculations in the
    network. The most common form of pooling is max pooling, which performs downscaling
    by applying a maximum filter to non-overlapping subregions of the input features
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot provides an example of max pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Max pooling example](img/B16953_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Max pooling example
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see an example of a max pooling operation
    on a features matrix. In the case of an image, this matrix would be made up of
    the pixel values of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Applying this operation reduces the computational cost by reducing the number
    of features to process, and it also helps prevent overfitting. Next, we will see
    how the convolutional and pooling layers are combined in a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: CNN structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usually, a CNN is made up of a series of convolutional layers, followed by
    a pooling layer (downscaling). This combination is repeated several times, as
    we can see in the following screenshot example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Example of a CNN pipeline](img/B16953_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Example of a CNN pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In this process, the first layer detects simple features such as the outlines
    of an image, and the second layer begins to detect higher-level features. In the
    intermediate layers, it is already capable of detecting more complex shapes, such
    as the nose or eyes. In the final layers, it is usually able to differentiate
    human faces.
  prefs: []
  type: TYPE_NORMAL
- en: This seemingly simple repetition process is extremely powerful, detecting features
    of a slightly higher order than its predecessor at each step and generating astonishing
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Surpassing classical neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classical neural network uses fully connected (dense) layers as the main feature
    transformation operations, whereas a CNN uses convolution and pooling layers (Conv2D).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main differences between a fully connected layer and a convolutional layer
    are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers learn global patterns in their input feature space (for
    example, in the case of a digit from the **Modified National Institute of Standards
    and Technology** (**MNIST**) dataset, seen in the example from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the input feature space would be all the pixels
    from the image).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the convolution layers learn local patterns—in the case of
    images, patterns found in small two-dimensional windows that run through the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see how these little windows detect local
    patterns such as lines, edges, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network](img/B16953_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation performs a transformation of the input image through
    a window (2D matrix) that scans it, generating a new image with different features.
    Each of these generated images is called a **filter**, and each filter contains
    different patterns extracted from the original image (edges, axes, straight lines,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: The set of filters created in each intermediate layer of a CNN is called a feature
    map, which is a matrix of numbers of dimensions *r x c x n*, where *r* and *c*
    are rows and columns and *n* is the number of filters.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, these feature maps are the parameters that CNNs learn.
  prefs: []
  type: TYPE_NORMAL
- en: As we were already able to see when viewing the architecture of the MNIST classifier
    from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, CNNs stack several convolutional layers (Conv2D) combined with
    pooling layers (MaxPooling2D). The task of the latter consists of reducing the
    dimensions of the filters, keeping the most relevant values. This helps clean
    up noise and reduces training time for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to implement some practical examples. Let's start with an image
    classifier for a well-known dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CIFAR-10 image classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we are going to create will classify images from a dataset called
    `32x32` **red, green, blue** (**RGB**) colored images, classified into 10 different
    classes. It is a collection of images that is commonly used to train ML and computer
    vision algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the classes in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`airplane`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`automobile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bird`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frog`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horse`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ship`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truck`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next screenshot, you can see some random image samples found in the
    CIFAR-10 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – CIFAR-10 image samples'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – CIFAR-10 image samples
  prefs: []
  type: TYPE_NORMAL
- en: This a problem considered already solved. It is relatively easy to achieve a
    classification accuracy close to 80%. For better performance, we must use deep
    learning CNNs with which a classification precision greater than 90% can be achieved
    in the test dataset. Let's see how to implement it with AutoKeras.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classification task, so we can use the `ImageClassifier` class. This
    class generates and tests different models and hyperparameters, returning an optimal
    classifier to categorize each image with its corresponding class.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The notebook with the complete source code can be found at [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now have a look at the relevant cells of the notebook in detail, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package manager:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`matplotlib`, a Python plotting library that we will use to plot some digit
    representations, and CIFAR-10, which contains the categorized images dataset.
    The code to import the packages is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Getting the CIFAR-10 dataset**: We have to first load the CIFAR-10 dataset
    in memory and have a quick look at the dataset shape, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Although it is a well-known machine learning dataset, it is always important
    to ensure that the data is distributed evenly, to avoid surprises. This can be
    easily done using `numpy` functions, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The samples are perfectly balanced, as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Train and test dataset histograms'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Train and test dataset histograms
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are sure that our dataset is correct, it's time to create our image
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now use the AutoKeras `ImageClassifier` class to find the best classification
    model. Just for this little example, we set `max_trials` (the maximum number of
    different Keras models to try) to `2`, and we do not set the `epochs` parameter
    so that it will use an adaptive number of epochs automatically. For real use,
    it is recommended to set a large number of trials. The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the training to search for the optimal classifier for the CIFAR-10
    training dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Notebook output of image classifier training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Notebook output of image classifier training
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the accuracy of the training dataset is increasing.
  prefs: []
  type: TYPE_NORMAL
- en: As it has to process thousands of color images, the models that AutoKeras will
    generate will be more expensive to train, so this process will take hours, even
    using `max_trials = 5`). Increasing this number would give us a more accurate
    model, although it would also take longer to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  prefs: []
  type: TYPE_NORMAL
- en: By using `AutoModel` with `ImageBlock` instead of `ImageClassifier`, we can
    create high-level configurations, such as `block_type` for the type of neural
    network to look for. We can also perform data normalization or data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: If we have knowledge of deep learning and have faced this problem before, we
    can design a suitable architecture such as an `EfficientNet`-based image classifier,
    for instance, which is a deep residual learning architecture for image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have done the following with the settings:'
  prefs: []
  type: TYPE_NORMAL
- en: With `block_type = "efficient"`, AutoKeras will only explore `EfficientNet`
    architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing `augment = True` means we want to do data augmentation, a technique
    to create new artificial images from the originals. Upon activating it, AutoKeras
    will perform a series of transformations in the original image, as translations,
    zooms, rotations, or flips.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also not specify these arguments, in which case these different options
    would be tuned automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see more details about the `EfficientNet` function here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the model with the test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, it is time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can contrast the good results obtained
    from the training set with a dataset never seen before. To do this, we run the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that prediction accuracy has a margin to improve using our test
    dataset (84.4%), although it's a pretty decent score for just a few hours of training;
    but just increasing the trials, we have achieved 98% of precision training for
    the first model (`ImageClassifier`) and running during one day in Google Colaboratory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created and trained our classifier model, let''s see how it predicts
    on a subset of test samples. To do this, we run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Samples with their predicted and true labels](img/B16953_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Samples with their predicted and true labels
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all the predicted samples match their true value, so our classifier
    has predicted correctly. Now, let's take a look inside the classifier to understand
    how it is working.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can see a little summary with the architecture of the best generated
    model found (the one with 98% accuracy), and we will explain the reason why its
    performance is so good. Run the following code to see the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Best model architecture summary](img/B16953_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Best model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: The key layer here is the `efficientnetb7` layer, which implements a cutting-edge
    architecture created by Google. Today, `EfficientNet` models are the best choice
    for image classification because this is a recent architecture that not only focuses
    on improving accuracy but also on the efficiency of the models so that they achieve
    higher precision and better efficiency over existing convolutional network-based
    architectures, reducing parameter sizes and **floating-point operations per second**
    (**FLOPS**) by an order of magnitude. However, we didn't need to know anything
    about it because AutoKeras automatically chose it for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the blocks are connected to each other in a more visual way,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Best model architecture visualization](img/B16953_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Best model architecture visualization
  prefs: []
  type: TYPE_NORMAL
- en: As we explained in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, each block represents a layer, and the output
    of each is connected to the input of the next, except the first block (whose input
    is the image) and the last block (whose output is the prediction). The blocks
    before the `efficientnetb7` layer are all data-preprocessing blocks and they are
    in charge of adapting the image to a suitable format for this `EfficientNet` block,
    as well as generating extra images through augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Now is the time to tackle a non-classification problem. In the following practical
    example, we are going to create a human-age predictor based on a set of celebrity
    data—a fun tool that could make anyone blush.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an image regressor to find out the age of people
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a model that will find out the age of a person
    from an image of their face. For this, we will train the model with a dataset
    of faces extracted from images of celebrities in **Internet Movie Database** (**IMDb**).
  prefs: []
  type: TYPE_NORMAL
- en: As we want to approximate the age, we will use an image regressor for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next screenshot, you can see some samples taken from this dataset of
    celebrity faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – A few image samples from IMDb faces dataset](img/B16953_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – A few image samples from IMDb faces dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook with the complete source code can be found here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now explain the relevant code cells of the notebook in detail, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package manager. The code is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Importing needed packages**: We now load AutoKeras and some more used packages,
    such as matplotlib, a Python plotting library that we will use to plot some picture
    samples and the categories distribution. The code to do this is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Getting the IMDb faces dataset**: Before training, we have to download the
    IMDb cropped faces dataset that contains the images of each face, as well as metadata
    with the age tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following command lines are idempotent—they download and extract data only
    if it does not already exist:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`MatLab` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b. The age is not in the params—it has to be calculated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. The images are not homogeneous—they have different dimensions and colors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To resolve these issues, we have created the following utility functions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a. `imdb_meta_to_df(matlab_filename)`: This converts the IMDb MatLab file to
    a pandas DataFrame and calculates the age.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. `normalize_dataset(df_train_set)`: This returns a tuple of normalized images
    (resized to `128x128` and converted to grayscale) and ages converted to integers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the notebook, you will find more details about how these functions are working.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how to use them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, we used the `imdb_meta_to_df` function to convert
    the `imdb` metadata information stored in a MatLab file to a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DataFrame contains a lot of images; to make the training faster, we will
    use only a part of them to create the datasets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create the final datasets with normalized images and ages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Once all the images are the same size (128×128) and the same color (grayscale)
    and we have the labels and the estimated age, we are ready to feed our model,
    but first we have to create it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fine-tuning a powerful image regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because we want to predict age, and this is a scalar value, we are going to
    use AutoKeras `ImageRegressor` as an age predictor. We set `max_trials` (the maximum
    number of different Keras models to try) to `10`, and we do not set the `epochs`
    parameter so that it will use an adaptive number of epochs automatically. For
    real use, it is recommended to set a large number of trials. The code is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the training model to search for the optimal regressor for the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Notebook output of our age predictor training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – Notebook output of our age predictor training
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the loss for the training dataset is decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: This training process has taken 1 hour in Colaboratory. We have limited the
    search to 10 architectures (`max_trials = 10`) and restricted the number of images
    to 10,000\. Increasing these numbers would give us a more accurate model, although
    it would also take longer to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  prefs: []
  type: TYPE_NORMAL
- en: As we did earlier in the regressor example, we can use `AutoModel` with `ImageBlock`
    instead of `ImageRegressor` so that we can implement high-level configurations,
    such as define a specific architecture neural network to search using `block_type`.
    We can also perform data preprocessing operations, such as normalization or augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: As we did in the previous image classifier example, we can design a suitable
    architecture as an `EfficientNet`-based image regressor, for instance, which is
    a deep residual learning architecture for image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we have done the following with the settings:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Normalization` block will transform all image values in the range between
    0 and 255 to floats between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape has been set (60000, 28 * 28) with values between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `ImageBlock(block_type="efficient"`, we are telling AutoKeras to only scan
    `EfficientNet` architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ImageAugmentation` block performs data augmentation, a technique to create
    new artificial images from the originals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also not specify any of these arguments, in which case these different
    options would be tuned automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see more details about the `EfficientNet` function here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model with the test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, it''s time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can rule out that the good results
    obtained with the training set are due to overfitting. The code to do this is
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This error still has a lot of margin to improve, but let''s have a look at
    how it''s predicting over a subset of test samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Samples with their predicted and true labels'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – Samples with their predicted and true labels
  prefs: []
  type: TYPE_NORMAL
- en: We can see that some predicted samples are near to the real age of the person
    but others aren't, so investing in more training hours and fine-tuning will make
    it predict better. Let's take a look inside the classifier to understand how it
    is working.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now see a little summary with the architecture of the best generated
    model found by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Best model architecture summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – Best model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: 'The key layers here are the convolution and pooling blocks, as we explained
    at the beginning of this chapter. These layers learn local patterns from the image
    that help to perform the predictions. Here is a visual representation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Best model architecture visualization](img/B16953_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Best model architecture visualization
  prefs: []
  type: TYPE_NORMAL
- en: First, there are some data preprocessing blocks that normalize the images and
    do data augmentation; then, there are several stacked convolution and pooling
    blocks; then, a dropout block to do the regularization (a technique to reduce
    overfitting based on dropping random neurons while training, to reduce the correlation
    between the closest neurons); and finally, we see the regression block, to convert
    the output to a scalar (the age).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how convolutional networks work, how to implement
    an image classifier, and how to fine-tune it to improve its accuracy. We have
    also learned how to implement an image regressor and fine-tune it to improve its
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to work with images, we are ready to move on to
    the next chapter, where you will learn how to work with text by implementing classification
    and regression models using AutoKeras.
  prefs: []
  type: TYPE_NORMAL
