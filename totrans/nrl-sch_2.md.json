["```py\n    vocab = 'Jina is a neural search framework built with cutting age technology called deep learning'\n    ```", "```py\n    vocab = ['a', 'age', 'built', 'call', 'cut', 'deep', 'framework', 'is', 'jina', 'learn', 'neural', 'search', 'technolog', 'with']\n    ```", "```py\n    import nltk\n    doc1 = 'Jina is a neural search framework'\n    doc2 = 'Jina is built with cutting age technology called deep learning'\n    def tokenize_and_stem(doc1, doc2):\n        tokens = nltk.word_tokenize(doc1 + doc2)\n        stemmer = nltk.stem.porter.PorterStemmer()\n        stemmed_tokens = [stemmer.stem(token) for token in \n                         tokens]\n        return sorted(stemmed_tokens)\n    def encode(vocab, doc):\n        encoded = [0] * len(vocab)\n        for idx, token in enumerate(vocab):\n            if token in doc:\n                encoded[idx] = 1  # token present in doc\n        return encoded\n    if __name__ == '__main__':\n        tokens = tokenize_and_stem(doc1, doc2)\n        encoded_doc1 = encode(vocab=tokens, doc=doc1)\n        print(encoded_doc1)\n    ```", "```py\n>>> [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n```", "```py\ndoc1 = 'Jina is a neural search framework'\ndoc2 = 'Jina is built with cutting age technology called deep learning'\n```", "```py\nencoded_doc1 = [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\nencoded_doc2 = [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n```", "```py\nimport math\ndef compute_cosine_sim(encoded_doc1, encoded_doc2):\n    numerator = sum([i * j for i, j in zip(encoded_doc1, \n                encoded_doc2)])\n    denominator_1 = math.sqrt(sum([i * i for i in \n                    encoded_doc1]))\n    denominator_2 = math.sqrt(sum([i * i for i in \n                    encoded_doc2]))\n    return numerator/(denominator_1 * denominator_2)\n```", "```py\n>>> 0.40451991747794525\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'Jina is a neural search framework for neural search',\n    'Jina is built with cutting edge technology called deep \n     learning',\n]\nvectorizer = CountVectorizer(binary=True)\nX = vectorizer.fit_transform(corpus)\nprint(X.toarray())\n```", "```py\n>>> array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],\n           [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]])\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    'Jina is a neural search framework for neural search',\n    'Jina is built with cutting edge technology called deep \n     learning',\n]\nvectorizer = CountVectorizer(binary=False)\nX = vectorizer.fit_transform(corpus)\nprint(X.toarray())\n```", "```py\n>>> array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 0],\n          [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]])\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'Jina is a neural search framework for neural search',\n    'Jina is built with cutting edge technology called deep \n     learning',\n]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(X.toarray())\n```", "```py\n>>> array([[0., 0., 0., 0., 0., 0.30134034, 0.30134034, 0.21440614, 0.21440614, 0.,0.60268068, 0.60268068, 0., 0.       ],\n          [0.33310232, 0.33310232, 0.33310232, 0.33310232, 0.33310232, 0., 0\\. , 0.23700504, 0.23700504, 0.33310232, 0., 0., 0.33310232, 0.33310232]])\n```", "```py\ndoc1 = 'Jina is a neural [MASK] framework'\n```", "```py\ndoc = '[CLS] Jina is a neural [MASK] framework [SEP] Jina is built with cutting edge technology called deep learning'.\n```"]