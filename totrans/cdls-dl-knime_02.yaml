- en: '*Chapter 1:* Introduction to Deep Learning with KNIME Analytics Platform'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start our journey of exploring **Deep Learning** (**DL**) paradigms by
    looking at KNIME Analytics Platform. If you have always been drawn to neural networks
    and deep learning architectures and have always thought that the coding part would
    be an obstacle to you developing a quick learning curve, then this is the book
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning can be quite complex, and we must make sure that the journey is
    worth the result. Thus, we'll start this chapter by stating, once again, the relevance
    of deep learning techniques when it comes to successfully implementing applications
    for data science.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue by providing a quick overview of the tool of choice for this
    book – KNIME Software – and focus on how it complements both KNIME Analytics Platform
    and KNIME Server.
  prefs: []
  type: TYPE_NORMAL
- en: The work we'll be doing throughout this book will be implemented in KNIME Analytics
    Platform, which is open source and available for free. We will dedicate a full
    section to how to download, install, and use KNIME Analytics Platform, even though
    more details will be provided in the chapters to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Among the benefits of KNIME Analytics Platform is, of course, its codeless Deep
    Learning - Keras Integration extension, which we will be making extensive use
    of throughout this book. In this chapter, we will just focus on the basic concepts
    and requirements for this KNIME extension.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will conclude this chapter by stating the goal and structure of
    this book. We wanted to give it a practical flavor, so most of the chapters will
    revolve around a practical case study that includes real-world data. In each chapter,
    we will take the chance to dig deeper into the required neural architecture, data
    preparation, deployment, and other aspects necessary to make the case study at
    hand a success.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Deep Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring KNIME Software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring KNIME Analytics Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing KNIME Deep Learning – Keras Integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goals and Structure of this Book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start by stating the importance of deep learning when it comes to successful
    data science applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have been working in the field of **data science** – or **Artificial
    Intelligence** (**AI**), as it is called nowadays – for a few years, you might
    have noticed the recent sudden explosion of scholarly and practitioner articles
    about successful solutions based on deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The big breakthrough happened in 2012 when the deep learning-based AlexNet network
    won the ImageNet challenge by an unprecedented margin. This victory kicked off
    a surge in the usage of deep learning networks. Since then, these have expanded
    to many different domains and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are we referring to exactly when we talk about deep learning? Deep
    learning covers a subset of **Machine Learning** (**ML**) algorithms, most of
    which stem from neural networks. Deep learning is indeed the modern evolution
    of traditional neural networks. Apart from the classic feedforward, fully connected,
    backpropagation-trained, and multilayer perceptron architectures, *deeper* architectures
    have been added. Deeper indicates more hidden layers and a few new additional
    neural paradigms, including **Recurrent Neural Networks** (**RNNs**), **Long-Short
    Term Memory** (**LSTM**), **Convolutional Neural Networks** (**CNNs**), **Generative
    Adversarial Networks** (**GANs**), and more.
  prefs: []
  type: TYPE_NORMAL
- en: The recent success of these new types of neural networks is due to several reasons.
    First, the increased computational power in modern machines has favored the introduction
    and development of new paradigms and more complex neural architectures. Training
    a complex neural network in minutes leaves space for more experimentation compared
    to training the same network for hours or days. Another reason is due to their
    flexibility. Neural networks are universal function approximators, which means
    that they can approximate almost anything, provided that their architecture is
    sufficiently complex.
  prefs: []
  type: TYPE_NORMAL
- en: Having mathematical knowledge of these algorithms, experience with the most
    effective paradigms and architectures, and domain wisdom are all basic, important,
    and necessary ingredients for the success of any data science project. However,
    there are other, more contingent factors – such as ease of learning, speed of
    prototyping, options for debugging and testing to ensure the correctness of the
    solution, flexibility to experiment, availability of help from external experts,
    and automation and security capabilities – that also influence the final result
    of the project.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we'll present deep learning solutions that can be implemented
    with the open source, visual programming-based, free-to-use tool known as KNIME
    Analytics Platform. The deployment phases for some of these solutions also use
    a few features provided by KNIME Server.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about how KNIME Analytics Platform and KNIME Server complement
    each other, as well as which tasks both should be used for.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring KNIME Software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will mainly be working with two KNIME products: KNIME Analytics Platform
    and KNIME Server. KNIME Analytics Platform includes ML and deep learning algorithms
    and data operations needed for data science projects. KNIME Server, on the other
    hand, provides the IT infrastructure for easy and secure deployment, as well as
    model monitoring over time.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll concentrate on KNIME Analytics Platform first and provide an overview
    of what it can accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Analytics Platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**KNIME Analytics Platform** is an open source piece of software for all your
    data needs. It is free to download from the KNIME website ([https://www.knime.com/downloads](https://www.knime.com/downloads))
    and free to use. It covers all the main data wrangling and machine learning techniques
    available at the time of writing, and it is based on visual programming.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual programming** is a key feature of KNIME Analytics Platform for quick
    prototyping. It makes the tool very easy to use. In visual programming, a **Graphical
    User Interface** (**GUI**) guides you through all the necessary steps for building
    a pipeline (workflow) of dedicated blocks (nodes). Each node implements a given
    task; each workflow of nodes takes your data from the beginning to the end of
    the designed journey. A workflow substitutes a script; a node substitutes one
    or more script lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Without extensive coverage when it comes to commonly used data wrangling techniques,
    machine learning algorithms, and data types and formats, and without integration
    with most common database software, data sources, reporting tools, external scripts,
    and programming languages, the software's ease of use would be limited. For this
    reason, KNIME Analytics Platform has been designed to be open to different data
    formats, data types, data sources, and data platforms, as well as external tools
    such as Python and R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by looking at a few ML algorithms. KNIME Analytics Platform covers
    most machine learning algorithms: from decision trees to random forest and gradient
    boosted trees, from recommendation engines to a number of clustering techniques,
    from Naïve Bayes to linear and logistic regression, from neural networks to deep
    learning. Most of these algorithms are native to KNIME Analytics Platform, though
    some can be integrated from other open source tools such as Python and R.'
  prefs: []
  type: TYPE_NORMAL
- en: To train different deep learning architectures, such as RNNs, autoencoders,
    and CNNs, KNIME Analytics Platform has integrated the **Keras** deep learning
    library through the **KNIME Deep Learning - Keras Integration** extension ([https://www.knime.com/deeplearning/keras](https://www.knime.com/deeplearning/keras)).
    Through this extension, it is possible to drag and drop nodes to define complex
    neural architectures and train the final network without necessarily writing any
    code.
  prefs: []
  type: TYPE_NORMAL
- en: However, defining the network is just one of the many steps that must be taken.
    Ensuring the data is in the right form to train the network is another crucial
    step. For this, a very large number of nodes are available so that we can implement
    a myriad of **Data Wrangling** techniques. By combining nodes dedicated to small
    tasks, you can implement very complex data transformation operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'KNIME Analytics Platform also connects to most of the required data sources:
    from databases to cloud repositories, from big data platforms to files.'
  prefs: []
  type: TYPE_NORMAL
- en: But what if all of this is not enough? What if you need a specific procedure
    for a specific domain? What if you need a specific network manipulation function
    from Python? Where KNIME Analytics Platform and its extensions cannot reach, you
    can integrate with other scripting and programming languages, such as **Python**,
    **R**, **Java**, and **Javascript**, just to mention a few. In addition, KNIME
    Analytics Platform has seamless integration with BIRT, a business intelligence
    and reporting tool. Integrations with other reporting platforms such as Tableau,
    QlickView, PowerBI, and Spotfire are also available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several JavaScript-based nodes are dedicated to implementing data visualization
    plots and charts: from a simple scatter plot to a more complex sunburst chart,
    from a simple histogram to a parallel coordinate plot, and more. These nodes seem
    simple but are potentially quite powerful. If you combine them within a **component**,
    you can interactively select data points across multiple charts. By doing this,
    the component inherits and combines all the views from the contained nodes and
    connects them in a way that, if the points are selected and visualized in one
    chart, they can also be selected and visualized in the other charts of the component''s
    composite view.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1.1* shows an example of a composite view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Composite view of a component containing a scatter plot, a bar
    chart, and a parallel coordinate plot](img/B16391_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Composite view of a component containing a scatter plot, a bar
    chart, and a parallel coordinate plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1.1* shows the composite view of a component containing a scatter plot,
    a bar chart, and a parallel coordinate plot. The three plots visualize the same
    data and are connected in a way that, by selecting data in the bar chart, it selects
    and optionally visualizes the data that''s been selected in the other two charts.'
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to creating a data science solution, KNIME Analytics Platform
    provides everything you need. However, KNIME Server offers a few additional features
    to ease your job when it comes to moving the solution to production.
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Server for the Enterprise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step in any data science cycle is to deploy the solution to production
    – and in the case of an enterprise, providing an easy, comfortable, and secure
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: This process of moving the application into the real world is called *moving
    into production*. The process of including the trained model in this final application
    is called **deployment**. Both phases are deeply connected and can be quite problematic
    since all the errors that occurred in the application design show up at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible, though limited, to move an application into production using
    KNIME Analytics Platform. If you, as a lone data scientist or a data science student,
    do not regularly deploy applications and models, KNIME Analytics Platform is probably
    enough for your needs. However, if you are just a bit more involved in an enterprise
    environment, where scheduling, versioning, access rights, disaster recovery, web
    applications and REST services, and all the other typical functions of a production
    server are needed, then just using KNIME Analytics Platform for production can
    be cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, **KNIME Server**, which comes with an annual license fee, can
    make your life easier. First of all, it is going to fit the governance of the
    enterprise's IT environment better. It also offers a protected collaboration environment
    for your group and the entire data science lab. And of course, its main advantage
    consists of making model deployment and moving it into production easier and safer
    since it uses the *integrated deployment* feature and allows you to use *one-click
    deployment* into production. End users can then run the application from a KNIME
    Analytics Platform client or – even better – from a web browser.
  prefs: []
  type: TYPE_NORMAL
- en: Remember those composite views that offer interactive interconnected views of
    selected points? These become fully formed web pages when the application is executed
    on a web browser via **KNIME Server's WebPortal**.
  prefs: []
  type: TYPE_NORMAL
- en: Using the components as touchpoints within the workflow, we get [a **Guided
    Analytics** (](https://www.knime.com/blog/principles-of-guided-analytics)) application
    within the web browser. Guided analytics inserts touchpoints to be consumed by
    the end user from a web browser within the flow of the application. The end user
    can take advantage of these touchpoints to insert knowledge or preferences and
    to steer the analysis in the desired direction.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's download KNIME Analytics Platform and give it a try!
  prefs: []
  type: TYPE_NORMAL
- en: Exploring KNIME Analytics Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To install KNIME Analytics Platform, foll[ow these steps:](http://www.knime.com/downloads)
  prefs: []
  type: TYPE_NORMAL
- en: '[Go to](http://www.knime.com/downloads) .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide some details about yourself (step **1** in *Figure 1.2*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the version that's suitable for your operating system (step **2** in
    *Figure 1.2*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While you''re waiting for the appropriate version to download, browse through
    the different steps to get started (step **3** in *Figure 1.2*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Steps for downloading the KNIME Analytics Platform package](img/B16391_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Steps for downloading the KNIME Analytics Platform package
  prefs: []
  type: TYPE_NORMAL
- en: Once you've downloaded the package, locate it, start it, and follow the instructions
    that appear onscreen to install it in any directory that you have write permissions
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Once it's been installed, locate your instance of KNIME Analytics Platform –
    from the appropriate folder, desktop link, application, or link in the start menu
    – and start it.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the splash screen appears, a window will ask for the location of your
    workspace (*Figure 1.3*). This workspace is a folder on your machine that will
    host all your work. The default workspace folder is called `knime-workspace`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – The KNIME Analytics Platform Launcher window asking for the
    workspace folder](img/B16391_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – The KNIME Analytics Platform Launcher window asking for the workspace
    folder
  prefs: []
  type: TYPE_NORMAL
- en: After clicking **Launch**, the workbench for **KNIME Analytics Platform** will
    open.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workbench of KNIME Analytics Platform is organized as depicted in *Figure
    1.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – The KNIME Analytics Platform workbench](img/B16391_01_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – The KNIME Analytics Platform workbench
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNIME workbench consists of different panels that can be resized, removed
    by clicking the `View` menu. Let''s take a look at these panels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNIME Explorer**: The **KNIME Explorer** panel in the upper-left corner displays
    all the workflows in the selected (**LOCAL**) workspace, possible connections
    to mounted KNIME servers, a connection to the **EXAMPLES** server, and a connection
    to the **My-KNIME-Hub** space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **LOCAL** workspace displays all workflows, saved in the workspace folder
    that were selected when KNIME Analytics Platform was started. The very first time
    the platform is opened, the LOCAL workspace only contains workflows and data in
    the *Example Workflows* folder. These are example applications to be used as starting
    points for your projects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **EXAMPLES** server is a read-only KNIME hosted server that contains many
    more example workflows, organized into categories. Just double-click it to be
    automatically logged in with read-only mode. Once you've done this, you can browse,
    open, explore, and download all available example workflows. Once you have located
    a workflow, double-click it to explore it or drag and drop it into **LOCAL** to
    create a local editable copy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**My-KNIME-Hub** provides access to the KNIME community shared repository (**KNIME
    Hub**), either in public or private mode. You can use **My-KNIME-Hub/Public**
    to share your work with the KNIME community or **My-KNIME-Hub/Private** as a space
    for your current work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Workflow Coach**: **Workflow Coach** is a node recommendation engine that
    aids you when you''re building workflows. Based on worldwide user statistics or
    your own private statistics, it will give you suggestions on which nodes you should
    use to complete your workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node Repository**: The **Node Repository** contains all the KNIME nodes you
    have currently installed, organized into categories. To help you with orientation,
    a search box is located at the top of the **Node Repository** panel. The magnifier
    lens on its left switches between the exact match and the fuzzy search option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow Editor**: The **Workflow Editor** is the canvas at the center of
    the page and is where you assemble workflows, configure and execute nodes, inspect
    results, and explore data. Nodes are added from the **Node Repository** panel
    to the workflow editor by drag and drop or double-click. Upon starting KNIME Analytics
    Platform, the Workflow Editor will open on the **Welcome Page** panel, which includes
    a number of useful tips on where to find help, courses, events, and the latest
    news about the software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outline**: The **Outline** view displays the entire workflow, even if only
    a small part is visible in the workflow editor. This part is marked in gray in
    the **Outline** view. Moving the gray rectangle in the **Outline** view changes
    the portion of the workflow that''s visible in the Workflow Editor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Console** and **Node Monitor**: The **Console** and the **Node Monitor**
    share one panel with two tabs. The **Console** tab prints out possible error and
    warning messages. The same information is written to a log file, located in the
    workspace directory. The **Node Monitor** tab shows you the data that''s available
    at the output ports of the selected executed node in the Workflow Editor. If a
    node has multiple output ports, you can select the data of interest from a dropdown
    menu. By default, the data at the top output port is sh[own.](http://www.hub.knime.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**KNIME Hub**: The **KNI**](http://www.hub.knime.com)**ME Hub** ([https://hub.knime.com/](https://hub.knime.com/))
    is an external space where KNIME users can share their work. This panel allows
    you to search for workflows, nodes, and components shared by members of the KNIME
    community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: The **Description** panel displays information about the selected
    node or category. In particular, for nodes, it explains the node''s task, the
    algorithm behind it (if any), the dialog options, the available views, the expected
    input data, and the resulting output data. For categories, it displays all contained
    nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, at the very top, you can find the **Top Menu**, which includes menus
    for file management and preference settings, workflow editing options, additional
    views, node commands, and help documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the core software, KNIME Analytics Platform benefits from external **extensions**
    provided by the KNIME community. The **install KNIME extensions** and **update
    KNIME** commands, available in the **File** menu, allow you to expand your current
    instance with external extensions or update it to a newer version.
  prefs: []
  type: TYPE_NORMAL
- en: Under the top menu, a **toolbar** is available. When a workflow is open, the
    toolbar offers commands for workflow editing, node execution, and customization.
  prefs: []
  type: TYPE_NORMAL
- en: A **workflow** can be built by dragging and dropping nodes from the **Node Repository**
    panel onto the **Workflow Editor** window or by just double-clicking them. Nodes
    are the basic processing units of any workflow. Each **node** has several input
    and/or output ports. **Data** flows over a connection from an **output port**
    to the **input port**(s) of other nodes. Two nodes are connected – and the data
    flow is established – by clicking the mouse at the output port of the first node
    and releasing the mouse at the input port of the next node. A pipeline of such
    nodes makes a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 1.5*, under each node, you will see a **status light**: red, yellow,
    or green:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Node structure and status lights](img/B16391_01_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Node structure and status lights
  prefs: []
  type: TYPE_NORMAL
- en: When a new node is created, the status light is usually red, which means that
    the node's settings still need to be configured for the node to be able to execute
    its task.
  prefs: []
  type: TYPE_NORMAL
- en: To configure a node, right-click it and select **Configure** or just double-click
    it. Then, adjust the necessary settings in the node's dialog. When the dialog
    is closed by pressing the **OK** button, the node is configured, and the status
    light changes to yellow; this means that the node is ready to be executed. Right-clicking
    on the node again shows an enabled **Execute** option; pressing it will execute
    the node.
  prefs: []
  type: TYPE_NORMAL
- en: The ports on the left are input ports, where the data from the outport of the
    predecessor node is fed into the node. Ports on the right are outgoing ports.
    The result of the node's operation on the data is provided by the output port
    of the successor nodes. When you hover over the port, a tooltip will provide information
    about the output dimension of the node.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Only ports of the same type can be connected!
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ports** (black triangles) are the most common type of node ports and
    transfer flat data tables from node to node. **Database ports** (brown squares)
    transfer SQL queries from node to node. Many more node ports exist and transfer
    different objects from one node to the next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After successful execution, the status light of the node turns green, indicating
    that the processed data is now available on the outports. The result(s) can be
    inspected by exploring the outport view(s): the last entries in the context menu
    open them.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have completed our quick tour of the workbench in KNIME Analytics
    Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at where we can find starting examples and help.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Links and Materials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we have already looked at the `read file` and you will get a
    list of example workflows illustrating how to read `CSV` files, `.table` files,
    `Excel` files, and so on. (*Figure 1.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Resulting list of workflows from searching for "read file" on
    the KNIME Hub](img/B16391_01_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Resulting list of workflows from searching for "read file" on the
    KNIME Hub
  prefs: []
  type: TYPE_NORMAL
- en: 'All workflows described in this book are also available on the KNIME Hub for
    you: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you've isolated the workflow you are interested in, click on it to open
    its page, and then download it or open it in KNIME Analytics Platform to customize
    it to your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, to share your work on the KNIME Hub, just copy your workflows
    from your local workspace into the *My-KNIME-Hub/Public* folder in the **KNIME
    Explorer** panel within the KNIME workbench. It will be automatically available
    to all members of the KNIME community.
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME community is also very active, with tips and tricks available on the
    **KNIME Forum** ([https://forum.knime.com/](https://forum.knime.com/)). Here,
    you can ask questions or search for answers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, contributions from the community are available as posts on the **KNIME
    Blog** ([https://www.knime.com/blog](https://www.knime.com/blog)), as books via
    **KNIME Press** ([https://www.knime.com/knimepress](https://www.knime.com/knimepress)**E
    TV** () channel on YouTube.
  prefs: []
  type: TYPE_NORMAL
- en: The two books *KNIME Beginner's Luck* and *KNIME Advanced Luck* provide tutorials
    for those users who are starting out in data science with KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's build our first workflow, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Build and Execute Your First Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll build our first, simple, small workflow. We''ll start
    with something basic: reading data from an ASCII file, performing some filtering,
    and displaying the results in a bar chart.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In KNIME Explorer, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new empty folder by doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Right-click `Chapter 1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click **Finish**. You should then see a new folder with that name in the **KNIME
    Explorer** panel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Folders in KNIME Explorer are called **Workflow Groups**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Similarly, you can create a new workflow, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new workflow by doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Right-click the `Chapter 1` folder (or anywhere you want your workflow to
    be).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Select `My_first_workflow`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click **Finish**. You should then see a new workflow with that name in the **KNIME
    Explorer** panel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After clicking **Finish**, the Workflow Editor will open the canvas for the
    empty workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: By default, the canvas for a new workflow opens with the grid on; to turn it
    off, click the **Open the settings dialog for the workflow editor** button (the
    button before the last one) in the toolbar. This button opens a window where you
    can customize the workflow's appearance (for example, allowing curved connections)
    and perform editing (turn the grid on/off).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1.7* shows the **New Workflow Group**... option in the KNIME Explorer''s
    context menu. It allows you to create a new, empty folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Context menu for creating a new folder and a new workflow in
    KNIME Explorer](img/B16391_01_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Context menu for creating a new folder and a new workflow in KNIME
    Explorer
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do in our workflow is read an ASCII file with the
    data. Let's read the *adult.csv* file that comes with the installation of KNIME
    Analytics Platform. This can be found under **Example Workflows/The Data/Basics**.
    adult.csv is a US Census public file that describes 30K people by age, gender,
    origin, and professional and private life.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s **create** the node so that we can read the *adult.csv* ASCII file:'
  prefs: []
  type: TYPE_NORMAL
- en: a) In the `Node Repository`, search for the **File Reader** node (it is actually
    located in the **IO/Read** category).
  prefs: []
  type: TYPE_NORMAL
- en: b) Drag and drop the `File Reader` node onto the **Workflow Editor** panel.
  prefs: []
  type: TYPE_NORMAL
- en: c) Alternatively, just double-click the `File Reader` node in the `Node Repository`;
    this will automatically create it in the **Workflow Editor** panel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 1.8*, see the `File Reader` node located in the `Node Repository`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – The File Reader node under IO/Read in the Node Repository](img/B16391_01_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – The File Reader node under IO/Read in the Node Repository
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s `File Reader` node in the Workflow Editor and manually configure
    it with the file path to the *adult.csv* file. Alternatively, just drag and drop
    the *adult.csv* file from the **KNIME Explorer** panel (or from anywhere on your
    machine) onto the **Workflow Editor** window. You can see this action in *Figure
    1.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Dragging and dropping the adult.csv file onto the Workflow Editor
    panel.](img/B16391_01_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Dragging and dropping the adult.csv file onto the Workflow Editor
    panel.
  prefs: []
  type: TYPE_NORMAL
- en: This automatically generates a File Reader node that contains most of the correct
    configuration settings for reading the file.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Advanced** button in the File Reader configuration window leads you to
    additional advanced settings: reading files with special characters, such as quotes;
    allowing lines with different lengths; using different encodings; and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: To execute this node, just right-click it and from the context menu, select
    **Execute**; alternatively, click on the **Execute** buttons (single and double
    white arrows on a green background) that are available in the toolbar.
  prefs: []
  type: TYPE_NORMAL
- en: To inspect the output data table that's produced by this node's execution, right-click
    on the node and select the last option available in the context menu. This opens
    the data table that appears as a result of reading the *adult.csv* file. You will
    notice columns such as **Age**, **Workclass**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Data in KNIME Analytics Platform is organized into tables. Each cell is uniquely
    identified via the **column header** and the **row ID**. Therefore, column headers
    and row IDs need to have unique values.
  prefs: []
  type: TYPE_NORMAL
- en: '`fnlwgt` is one column for which we were never sure of what it meant. So, let''s
    remove it from further analysis by using the **Column Filter** node.'
  prefs: []
  type: TYPE_NORMAL
- en: To do this, search for `File Reader` node to the input of the `Column Filter`
    node. Alternatively, we can select the `File Reader` node in the `Column Filter`
    node in the Node Repository. This automatically creates a node and its connections
    in the Workflow Editor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Column Filter` node and its configuration window are shown in *Figure
    1.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Configuring the Column Filter node to remove the column named
    fnlwgt from the input data table](img/B16391_01_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Configuring the Column Filter node to remove the column named
    fnlwgt from the input data table
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, double-click or right-click the node and then select **Configure** to
    configure it. This configuration window contains three options that can be selected
    via three radio buttons: **Manual Selection**, **Wildcard/Regex Selection**, and
    **Type Selection**. Let''s take a look at these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual Selection** offers an Include/Exclude framework so that you can manually
    transfer columns from the **Include** set into the **Exclude** set and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wildcard/Regex Selection** extracts the columns you wish to keep, based on
    a wildcard (using ***** as the wildcard) or regex expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type Selection** keeps the columns based on the data types they carry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this is our first workflow, we'll go for the easiest approach; that is,
    Manual Selection. Go to the `fnlwgt` column to the **Exclude** set via the buttons
    in-between the two frames (these can be seen in *Figure 1.10*).
  prefs: []
  type: TYPE_NORMAL
- en: After executing the Column Filter node, if we inspect the output data table
    (right-click and select the last option in the context menu), we'll see a table
    that doesn't contain the `fnlwgt` column.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's extract all the records of people who work more than 20 hours/week.
    `hours-per-week` is the column that contains the data of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we need to create a Row Filter node and implement the required condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Again, let's locate the `Column Filter` node to its input port, and open its
    configuration window.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the configuration window of the `Row Filter` node (*Figure 1.11*), we''ll
    find three default filtering criteria: **use pattern matching**, **use range checking**,
    and **only missing values match**. Let''s take a look at what they do:'
  prefs: []
  type: TYPE_NORMAL
- en: '**use pattern matching** matches the given pattern to the content of the selected
    column in the **Column to test** field and keeps the matching rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use range checking** keeps only those data rows whose value in the **Column
    to test** columns falls between the **lower bound** and **upper bound** values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**only missing values match** only keeps the data rows where a missing value
    is present in the selected column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default behavior is to include the matching data rows in the output data
    table. However, this can be changed by enabling **Exclude rows by attribute value**
    via the radio buttons on the left-hand side of the configuration window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternative filtering criteria can be done by row number or by row ID. This
    can also be enabled via the radio buttons on the left-hand side of the configuration
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Configuring the Row Filter node to keep only rows with hours-per-week
    > 20 in the input data table](img/B16391_01_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Configuring the Row Filter node to keep only rows with hours-per-week
    > 20 in the input data table
  prefs: []
  type: TYPE_NORMAL
- en: 'After execution, upon opening the output data table (*Figure 1.12*), no data
    rows with *hours-per-week < 20* should be present:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Right-clicking a successfully executed node and selecting the
    last option shows the data table that was produced by the node](img/B16391_01_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Right-clicking a successfully executed node and selecting the
    last option shows the data table that was produced by the node
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at some very basic visualization. Let''s visualize the number
    of men versus women in this dataset, which contains people who work more than
    20 hours/week:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – The Bar Chart node and its configuration window](img/B16391_01_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – The Bar Chart node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, locate the `Row Filter` node, and open its configuration window
    (*Figure 1.13*). Here, there are four tabs we can use for configuration purposes.
    **Options** covers all data settings, **General Plot Options** covers all plot
    settings, **Control Options** covers all control options, and **Interactivity**
    covers all subscription events when it comes to interacting with other plots,
    views, and charts when they''ve been assembled to create a component. Again, since
    this is just a beginner''s workflow, we''ll adopt all the default settings and
    just set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: From the `sex`, ensuring it appears on the *x* axis. Then, select **Occurrence
    Count** in order to count the number of rows by sex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the **General Plot Options** tab, set a title, a subtitle, and the axis
    labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This node does not produce data, but rather a view of the bar chart. So, to
    inspect the results produced by this node after its execution, right-click it
    and select the central option; that is, **Interactive View: Group Bar Chart**
    (*Figure 1.14*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Right-clicking a successfully executed visualization node and
    selecting the Interactive View: Grouped Bar Chart option to see the chart/plot
    that has been produced](img/B16391_01_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14 – Right-clicking a successfully executed visualization node and
    selecting the Interactive View: Grouped Bar Chart option to see the chart/plot
    that has been produced'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the three buttons in the top-right corner of the view on the right of
    *Figure 1.14*. These three buttons enable **zooming**, **toggling to full screen**,
    and **node settings**, respectively. From the view itself, you can explore how
    the chart would look if different settings were to be selected, such as a different
    category column or a different title.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Most data visualization nodes produce a view and not a data table. To see the
    respective view, right-click the successfully executed node and select the **Interactive
    View: …** option.'
  prefs: []
  type: TYPE_NORMAL
- en: The second lower input port of the **Bar Chart** node is optional (a white triangle)
    and is used to read a color map so that you can color the bars in the bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a number of different data visualization nodes are available in the
    Node Repository: JavaScript, Local(Swing), Plotly, and so on. `Bar Chart` node
    from the JavaScript category in the **Node Repository** panel here.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll add a few comments to document the workflow. You can add comments
    at the node level or at the general workflow level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node in the workflow is created with a default label of *Node xx* under
    it. Upon double-clicking it, the node label editor appears. This allows you to
    customize the text, the font, the color, the background, and other similar properties
    of the node (*Figure 1.15*). We need to write a little comment under each node
    to make it clear what tasks they are implementing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Editor for customizing the labels under each node](img/B16391_01_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Editor for customizing the labels under each node
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also write annotations at the workflow level. Just right-click anywhere
    in the Workflow Editor and select **New Workflow Annotation**. A yellow frame
    will appear in editing mode. Here, you can add text and customize it, as well
    as its frame. To close the annotation editor, just click anywhere else in the
    Workflow Editor. To reopen the annotation editor, double-click in the top-left
    corner of the annotation (*Figure 1.16*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Creating and editing workflow annotations](img/B16391_01_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Creating and editing workflow annotations
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations! You have just built your first workflow with KNIME Analytics
    Platform. It should look something like the one in *Figure 1.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.17 – My_first_Workflow](img/B16391_01_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – My_first_Workflow
  prefs: []
  type: TYPE_NORMAL
- en: That was a quick introduction to how to use KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's make sure we have KNIME Deep Learning – Keras Integration installed
    and functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Installing KNIME Deep Learning – Keras Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to install and set up **KNIME Deep Learning
    - Keras Integration** in order to train neural networks in KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Analytics Platform consists of a software core and several provided extensions
    and integrations. Such extensions and integrations are provided by the KNIME community
    and extend the original software core through a variety of data science functionalities,
    including advanced algorithms for AI.
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME extension of interest here is called **KNIME Deep Learning – Keras
    Integration**. It offers a codeless GUI-based integration of the Keras library,
    while using TensorFlow as its backend. This means that a number of functions from
    Keras libraries have been wrapped into KNIME nodes, within KNIME's classic, easy-to-use
    visual dialog window. Due to this integration, you can read, write, create, train,
    and execute deep learning networks without writing code.
  prefs: []
  type: TYPE_NORMAL
- en: Another deep learning integration that's available is called **KNIME Deep Learning
    - TensorFlow Integration**. This extension allows you to convert **Keras** models
    into **TensorFlow** models, as well as read, execute, and write TensorFlow models.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is an open source library provided by Google that includes a number
    of deep learning paradigms. TensorFlow functions can run on single devices, as
    well as on multiple CPUs and multiple GPUs. This parallel calculation feature
    is the key to speeding up the computationally intensive training that's required
    for deep learning networks.
  prefs: []
  type: TYPE_NORMAL
- en: However, using the TensorFlow library within Python can prove quite complicated,
    even for an expert Python programmer or a deep learning pro. Thus, a number of
    simplified interfaces have been developed on top of TensorFlow that expose a subset
    of its functions and parameters. The most successful of such TensorFlow-based
    libraries is Keras. However, even Keras still requires some programming skills.
    The KNIME Deep Learning – Keras Integration puts the KNIME GUI on top of the Keras
    libraries that are available, mostly eliminating the need to code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the KNIME Deep Learning – Keras Integration work, a few pieces of the
    puzzle need to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras and TensorFlow nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with the first piece: installing the Keras and TensorFlow nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Keras and TensorFlow Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To add nodes to the Node Repository, you must install a few extensions and integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install them from within KNIME Analytics Platform by clicking on **File**
    from the top menu and selecting **Install KNIME Extension…**. This opens the dialog
    shown in *Figure 1.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Dialog for installing extensions](img/B16391_01_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Dialog for installing extensions
  prefs: []
  type: TYPE_NORMAL
- en: From this new dialog, you can select the extensions and integrations you want
    to install. Using the search bar at the top is helpful for filtering the available
    extensions and integrations.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Another way you can install extensions is by dragging and dropping them from
    the KNIME Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Keras and TensorFlow nodes that will be used in the case studies
    described in this book, you need to select the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNIME Deep Learning – Keras Integration**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KNIME Deep Learning – TensorFlow Integration**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, press the **Next** button, accept the terms and conditions, and click
    **Finish**. Once the installation is done, you need to restart KNIME Analytics
    Platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should have the Keras and TensorFlow nodes in your Node
    Repository (*Figure 1.19*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Installed deep learning nodes in the Node Repository](img/B16391_01_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Installed deep learning nodes in the Node Repository
  prefs: []
  type: TYPE_NORMAL
- en: 'A large number of nodes implement neural layers: the nodes for input and dropout
    layers can be found in the **Core** sub-category, the nodes for LSTM layers can
    be found in **Recurrent**, and the nodes for embedding layers can be found in
    **Embedding**. Then, there are the Learner, Reader, and Writer nodes, which can
    be used to train, load, and store a network, respectively. All these nodes have
    a configuration window and don''t require any coding. The Python deep learning
    nodes allow you to define, train, execute, and edit networks using Python code.
    The last subcategory contains TensorFlow-based nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to set up the Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Python Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The KNIME Keras Integration and the KNIME TensorFlow Integration depend on an
    existing **Python** installation, which requires certain Python dependencies to
    be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the KNIME Python Integration, the KNIME Deep Learning Integration
    uses **Anaconda** to manage Python environments. If you have already installed
    Anaconda for, for example, the KNIME Python Integration, you can skip the first
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: First, get and install the latest Anaconda version (Anaconda ≥ 2019.03, conda
    ≥ 4.6.2) from [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
    On the Anaconda download page, you can choose between Anaconda with Python 3.x
    or Python 2.x. Either one should work (if you're not sure, we suggest selecting
    Python 3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to create an environment with the correct libraries installed.
    To do so, from within KNIME Analytics Platform, open the Python Deep Learning
    preferences. From here, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, select **File -> Preferences** from the top menu. This will open a new
    dialog with a list on the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the dialog, select **KNIME** **-> Python Deep Learning**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now see a dialog like that in *Figure 1.20*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Python Deep Learning preference page](img/B16391_01_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.20 – Python Deep Learning preference page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From this page, create some Conda environments with the correct packages installed
    for Keras or TensorFlow 2\. For the case studies in this book, it will be sufficient
    to set up an environment for Keras.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To create and set up a new environment, enable **Use special Deep Learning configuration**
    and set **Keras** to **Library used for DL Python**. Next, enable **Conda** and
    provide the path to your Conda installation directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, to create a new environment for Keras, click on the **New environment…**
    button in the Keras framework.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This opens a new dialog, as in *Figure 1.21*, where you can set the new environment''s
    name:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Dialog for setting the new environment''s name](img/B16391_01_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.21 – Dialog for setting the new environment's name
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Create new CPU environment** or **Create new GPU environment**
    button to create a new environment for using either a CPU or GPU, if available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, you can get started. In this section, you were introduced to the most
    convenient way of setting up a Python environment. Other options can be found
    in the KNIME documentation: [https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#keras-integration](https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#keras-integration).'
  prefs: []
  type: TYPE_NORMAL
- en: Goal and Structure of this Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, our aim is to provide you with a strong theoretical basis about
    deep learning architectures and training paradigms, as well as some detailed codeless
    experience of their implementations for solving practical case studies based on
    real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: For this journey, we have adopted the codeless tool, KNIME Analytics Platform.
    KNIME Analytics Platform is based on visual programming and exploits a user-friendly
    GUI to make data analytics a more affordable task without the barrier of coding.
    As with many other external extensions, KNIME Analytics Platform has integrated
    the Keras libraries under this same GUI, thus including deep learning as part
    of its list of codeless extensions. From within KNIME Analytics Platform, you
    can build, train, and test a deep learning architecture with just a few drag and
    drops and a few clicks of the mouse. We provided a little introduction to the
    tool in this chapter, but we will provide more detailed information about it in
    [*Chapter 2*](B16391_02_Final_SK_ePUB.xhtml#_idTextAnchor051), *Data Access and
    Preprocessing with KNIME Analytics Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: After that, in [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073),
    *Getting Started with Neural Networks*, we will provide a quick overview of the
    basic concepts behind neural networks and deep learning. This chapter will by
    no means provide complete coverage of all the architectures and paradigms involved
    in neural networks and deep learning. Instead, it will provide a quick overview
    of them to help you familiarize yourself with the concept, either for the first
    time or again, before you continue implementing them. Please refer to more specialized
    literature if you want to know more about the mathematical background of deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: As we stated previously, we decided to talk about deep learning techniques in
    a very practical way; that is, always with reference to real case studies where
    a particular deep learning technique had been successfully implemented. We'll
    start this trend in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Network*, where we'll describe a few basic
    example applications we can use to train and apply the basic concepts surrounding
    deep learning networks that we explored in [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073),
    *Getting Started with Neural Networks*. Although these are simple toy examples,
    they are still useful for illustrating how to apply the theoretical concepts we
    described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152), *Autoencoder
    for Fraud Detection*, we'll start looking at real case studies. The first case
    study we'll describe in this chapter aims to prevent fraud detection in credit
    card transactions by firing an alarm every time a suspicious transaction is detected.
    To implement this subspecies of anomaly detection, we'll use an approach based
    on the autoencoder architecture, as well as the calculated distance between the
    output and the input values of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'With [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152), *Autoencoder
    for Fraud Detection*, we are still in the realm of classic neural networks, including
    feedforward networks and those trained with backpropagation, albeit with an original
    architecture. In [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181),
    *Recurrent Neural Networks for Demand Prediction*, we''ll enter the realm of deep
    learning network with RNNs – specifically, with LSTMs. Here, the dynamic character
    of such networks and their capability to capture the time evolution of a signal
    will be exploited to solve a classic time series analysis problem: demand prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon introducing RNNs, we will learn how to use them for **Natural Language
    Processing** (**NLP**) case studies. [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*, covers a few such NLP use cases: sentiment analysis,
    free text generation, and product name generation, to name a few. All such use
    cases are similar in the sense that they analyze streams of text. All of them
    are also slightly different in that they find a solution to a different problem:
    classification for sentiment analysis for the former case, and unconstrained generation
    of sequences of words or characters for the other two use cases. Nevertheless,
    data preparation techniques and RNN architectures are similar for all case studies,
    which is why they have been placed into one single chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16391_08_Final_SK_ePUB.xhtml#_idTextAnchor290), *Neural Machine
    Translation*, describes a spin-off case of free text generation with RNNs. Here,
    a sequence of words will be generated at the output of the network as a response
    to a corresponding sequence of words in the input layer. The output sequence will
    be generated in the target language, while the input sequence will be provided
    in the source language.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning does not just come in the form of RNNs and text mining. Actually,
    the first examples of deep learning networks came from the field of image processing.
    [*Chapter 9*](B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316), *Convolutional
    Neural Networks for Image Classification*, is dedicated to describing a case study
    where histopathology slide images must be classified as one of three different
    types of cancer. To do that, we will introduce CNNs. Training networks for image
    analysis is not a simple task in terms of time, the amount of data, and computational
    resources. Often, to train a neural network so that it recognizes images, we must
    rely on the benefits of transfer learning, as described in [*Chapter 9*](B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316),
    *Convolutional Neural Networks for Image Classification*, as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316), *Convolutional
    Neural Networks for Image Classification*, concludes our in-depth look into how
    deep learning techniques can be implemented for real case studies. We are aware
    of the fact that other deep learning paradigms have been used to produce solutions
    for other data science problems. However, here, we decided to only report the
    common paradigms in which we had real-life experiences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After training a network, the deployment phase must take place. Deployment
    is often conveniently forgotten since this is the phase where all problems are
    put to the test. This includes errors in the application''s design, in training
    the network, in accessing and preparing the data: all of them will show up here,
    during deployment. Due to this, the last two chapters of this book are dedicated
    to the deployment phase of trained deep learning networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367), *Deploying
    a Deep Learning Network*, will show you how to build a deployment application,
    while [*Chapter 11*](B16391_11_Final_NM_ePUB.xhtml#_idTextAnchor386), *Best Practices
    and Other Deployment Options*, will show you all the deployment options that are
    available (a web application or a REST service). It will also provide you with
    a few tips and tricks from our own experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Each chapter comes with its own set of questions so that you can test your understanding
    of the material that's been provided.
  prefs: []
  type: TYPE_NORMAL
- en: With that, please read on to discover the various deep learning architectures
    that can be applied to real use cases using KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This first chapter aimed to prepare you for the content provided in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we started this chapter by reminding you of the importance of deep learning,
    as well as the surge in popularity it garnered following the first deep learning
    success stories. Such a surge in popularity is probably what brought you here,
    with the desire to learn more about practical implementations of deep learning
    networks for real use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, the main barrier that we come across when learning about deep learning
    is the coding skills that are required. Here, we adopted KNIME software, and in
    particular the open source KNIME Analytics Platform, so that we can look at the
    case studies that will be proposed throughout this book. To do this, we described
    KNIME software and KNIME Analytics Platform in detail.
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Analytics Platform also benefits from an extension known as KNIME Deep
    Learning – Keras Integration, which helps with integrating Keras deep learning
    libraries. It does this by wrapping Python-based libraries into the codeless KNIME
    GUI. We dedicated a full section to installing it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we concluded this chapter by providing an overview of what the remaining
    chapters in this book will cover.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the math and applications of deep learning networks, we
    will use the next chapter to familiarize ourselves with the basic features of
    KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
