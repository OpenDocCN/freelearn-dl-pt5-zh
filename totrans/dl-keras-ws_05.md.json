["```py\nfrom keras.regularizers import l1\nkeras.regularizers.l1(0.01)\n```", "```py\nfrom keras.regularizers import l2\nkeras.regularizers.l2(0.01)\n```", "```py\nfrom keras.regularizers import l1_l2\nkeras.regularizers.l1_l2(l1=0.01, l2=0.01)\n```", "```py\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\nmodel.add(Dense(8, kernel_regularizer=l2(0.01), \\\n          bias_regularizer=l2(0.01)))\n```", "```py\n    # Load the data\n    import pandas as pd\n    X = pd.read_csv('../data/tree_class_feats.csv')\n    y = pd.read_csv('../data/tree_class_target.csv')\n    \"\"\"\n    Split the dataset into training set and test set with a 80-20 ratio\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    seed = 1\n    X_train, X_test, \\\n    y_train, y_test = train_test_split(X, y, \\\n                                       test_size=0.2, \\\n                                       random_state=seed)\n    ```", "```py\n    #Define your model\n    from keras.models import Sequential\n    from keras.layers import Dense, Activation\n    import numpy as np\n    from tensorflow import random\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_1 = Sequential()\n    model_1.add(Dense(16, activation='relu', input_dim=10))\n    model_1.add(Dense(12, activation='relu'))\n    model_1.add(Dense(8, activation='relu'))\n    model_1.add(Dense(4, activation='relu'))\n    model_1.add(Dense(1, activation='sigmoid'))\n    ```", "```py\n    model_1.compile(optimizer='sgd', loss='binary_crossentropy')\n    # train the model\n    model_1.fit(X_train, y_train, epochs=300, batch_size=50, \\\n                verbose=0, shuffle=False)\n    # evaluate on test set\n    print(\"Test Loss =\", model_1.evaluate(X_test, y_test))\n    ```", "```py\n    2000/2000 [==============================] - 0s 23us/step\n    Test Loss = 0.1697693831920624\n    ```", "```py\n    \"\"\"\n    define the keras model with dropout in the first hidden layer\n    \"\"\"\n    from keras.layers import Dropout\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_2 = Sequential()\n    model_2.add(Dense(16, activation='relu', input_dim=10))\n    model_2.add(Dropout(0.1))\n    model_2.add(Dense(12, activation='relu'))\n    model_2.add(Dense(8, activation='relu'))\n    model_2.add(Dense(4, activation='relu'))\n    model_2.add(Dense(1, activation='sigmoid'))\n    model_2.compile(optimizer='sgd', loss='binary_crossentropy')\n    # train the model\n    model_2.fit(X_train, y_train, \\\n                epochs=300, batch_size=50, \\\n                verbose=0, shuffle=False)\n    # evaluate on test set\n    print(\"Test Loss =\", model_2.evaluate(X_test, y_test))\n    ```", "```py\n    2000/2000 [==============================] - 0s 29us/step\n    Test Loss = 0.16891103076934816\n    ```", "```py\n    # define the keras model with dropout in all hidden layers\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_3 = Sequential()\n    model_3.add(Dense(16, activation='relu', input_dim=10))\n    model_3.add(Dropout(0.2))\n    model_3.add(Dense(12, activation='relu'))\n    model_3.add(Dropout(0.1))\n    model_3.add(Dense(8, activation='relu'))\n    model_3.add(Dropout(0.1))\n    model_3.add(Dense(4, activation='relu'))\n    model_3.add(Dropout(0.1))\n    model_3.add(Dense(1, activation='sigmoid'))\n    model_3.compile(optimizer='sgd', loss='binary_crossentropy')\n    # train the model\n    model_3.fit(X_train, y_train, epochs=300, \\\n                batch_size=50, verbose=0, shuffle=False)\n    # evaluate on test set\n    print(\"Test Loss =\", model_3.evaluate(X_test, y_test))\n    ```", "```py\n    2000/2000 [==============================] - 0s 40us/step\n    Test Loss = 0.19390961921215058\n    ```", "```py\nhistory=model.fit(X_train, y_train, validation_data=(X_test, y_test), \\\n                  epochs=epochs)\n```", "```py\nfrom keras.callbacks import EarlyStopping\nes_callback = EarlyStopping(monitor='val_loss', mode='min')\n```", "```py\nes_callback = EarlyStopping(monitor='val_loss', \\\n                            mode='min', patience=20)\n```", "```py\nes_callback = EarlyStopping(monitor='val_loss', \\\n                            mode='min', min_delta=1)\nes_callback = EarlyStopping(monitor='val_loss', \\\n                            mode='min', baseline=0.2)\n```", "```py\nhistory=model.fit(X_train, y_train, validation_data=(X_test, y_test), \\\n                  epochs=epochs, callbacks=[es_callback])\n```", "```py\n    # Load the data\n    import pandas as pd\n    X = pd.read_csv('../data/tree_class_feats.csv')\n    y = pd.read_csv('../data/tree_class_target.csv')\n    \"\"\"\n    Split the dataset into training set and test set with an 80-20 ratio\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    seed=1\n    X_train, X_test, \\\n    y_train, y_test = train_test_split(X, y, test_size=0.2, \\\n                                       random_state=seed)\n    ```", "```py\n    # Define your model\n    from keras.models import Sequential\n    from keras.layers import Dense, Activation\n    import numpy as np\n    from tensorflow import random\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_1 = Sequential()\n    model_1.add(Dense(16, activation='relu', \\\n                      input_dim=X_train.shape[1]))\n    model_1.add(Dense(8, activation='relu'))\n    model_1.add(Dense(4, activation='relu'))\n    model_1.add(Dense(1, activation='sigmoid'))\n    ```", "```py\n    model_1.compile(optimizer='sgd', loss='binary_crossentropy')\n    # train the model\n    history = model_1.fit(X_train, y_train, \\\n                          validation_data=(X_test, y_test), \\\n                          epochs=300, batch_size=50, \\\n                          verbose=0, shuffle=False)\n    ```", "```py\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline\n    ```", "```py\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], \\\n               loc='upper right')\n    ```", "```py\n    #Define your model with early stopping on test error\n    from keras.callbacks import EarlyStopping\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_2 = Sequential()\n    model_2.add(Dense(16, activation='relu', \\\n                      input_dim=X_train.shape[1]))\n    model_2.add(Dense(8, activation='relu'))\n    model_2.add(Dense(4, activation='relu'))\n    model_2.add(Dense(1, activation='sigmoid'))\n    \"\"\"\n    Choose the loss function to be binary cross entropy and the optimizer to be SGD for training the model\n    \"\"\"\n    model_2.compile(optimizer='sgd', loss='binary_crossentropy')\n    # define the early stopping callback\n    es_callback = EarlyStopping(monitor='val_loss', \\\n                                mode='min')\n    # train the model\n    history=model_2.fit(X_train, y_train, \\\n                        validation_data=(X_test, y_test), \\\n                        epochs=300, batch_size=50, \\\n                        callbacks=[es_callback], verbose=0, \\\n                        shuffle=False)\n    ```", "```py\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], \\\n               loc='upper right')\n    ```", "```py\n    \"\"\"\n    Define your model with early stopping on test error with patience=10\n    \"\"\"\n    from keras.callbacks import EarlyStopping\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model_3 = Sequential()\n    model_3.add(Dense(16, activation='relu', \\\n                      input_dim=X_train.shape[1]))\n    model_3.add(Dense(8, activation='relu'))\n    model_3.add(Dense(4, activation='relu'))\n    model_3.add(Dense(1, activation='sigmoid'))\n    \"\"\"\n    Choose the loss function to be binary cross entropy and the optimizer to be SGD for training the model\n    \"\"\"\n    model_3.compile(optimizer='sgd', loss='binary_crossentropy')\n    # define the early stopping callback\n    es_callback = EarlyStopping(monitor='val_loss', \\\n                                mode='min', patience=10)\n    # train the model\n    history=model_3.fit(X_train, y_train, \\\n                        validation_data=(X_test, y_test), \\\n                        epochs=300, batch_size=50, \\\n                        callbacks=[es_callback], verbose=0, \\\n                        shuffle=False)\n    ```", "```py\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], \\\n               loc='upper right')\n    ```", "```py\nfrom keras.layers import GaussianNoise\nmodel.add(GaussianNoise(0.1))\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(4, input_dim=30, activation='relu'))\nmodel.add(GaussianNoise(0.01))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid')) \n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\ndef build_model():\n    model = Sequential(optimizer)\n    model.add(Dense(10, input_dim=X_train.shape[1], \\\n                    activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', \\\n                  optimizer= optimizer)\n    return model\n```", "```py\noptimizer = ['rmsprop', 'adam', 'sgd', 'adagrad']\nepochs = [100, 150]\nbatch_size = [1, 5, 10]\nparam_grid = dict(optimizer=optimizer, epochs=epochs, \\\n                  batch_size= batch_size)\n```", "```py\nfrom keras.wrappers.scikit_learn import KerasRegressor\nmodel = KerasRegressor(build_fn=build_model, \\\n                       verbose=0, shuffle=False)\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator=model, \\\n                           param_grid=param_grid, cv=10)\nresults = grid_search.fit(X, y)\n```", "```py\nfrom keras.wrappers.scikit_learn import KerasRegressor\nmodel = KerasRegressor(build_fn=build_model, verbose=0)\nfrom sklearn.model_selection import RandomizedSearchCV\ngrid_search = RandomizedSearchCV(estimator=model, \\\n                                 param_distributions=param_grid, \\\n                                 cv=10, n_iter=12)\nresults = grid_search.fit(X, y)\n```", "```py\nBest cross-validation score= 0.7862895488739013\nParameters for Best cross-validation score= {'batch_size': 20, 'epochs': 100, 'rate': 0.0}\nAccuracy 0.786290 (std 0.013557) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.0}\nAccuracy 0.786098 (std 0.005184) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.05}\nAccuracy 0.772004 (std 0.013733) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.1}\n```"]