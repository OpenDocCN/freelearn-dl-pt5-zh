<html><head></head><body>
		<div id="_idContainer249" class="Content">
			<h1 id="_idParaDest-234"><a id="_idTextAnchor261"/>Appendix</h1>
		</div>
		<div id="_idContainer277" class="Content">
			<h1 id="_idParaDest-235"><a id="_idTextAnchor262"/>1. Building Blocks of Deep Learning</h1>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor263"/>Activity 1.01: Solving a Quadratic Equation Using an Optimizer</h2>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor264"/>Solution</h2>
			<p>Let's solve the following quadratic equation:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="image/B15385_01_29.jpg" alt="Figure 1.29: Quadratic equation to be solved&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.29: Quadratic equation to be solved</p>
			<p>We already know that the solution to this quadratic equation is <strong class="source-inline">x=5</strong>.</p>
			<p>We can use an optimizer to solve this. For the optimizer, <strong class="source-inline">x</strong> is the variable and the cost function is the left-hand side expression, which is as follows:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B15385_01_30.jpg" alt="Figure 1.30: Left-hand side expression&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.30: Left-hand side expression</p>
			<p>The optimizer will find the value of <strong class="source-inline">x</strong> for which the expression is the minimum – in this case, it is <strong class="source-inline">0</strong>. Please note that this will work only for quadratic equations that are perfect squares, such as in this case. The left-hand side expression is a perfect square that can be explained with the following equation:</p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B15385_01_31.jpg" alt="Figure 1.31: Perfect square&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.31: Perfect square</p>
			<p>Now, let's look at the code for solving this:</p>
			<ol>
				<li>Open a new Jupyter Notebook and rename it <em class="italic">Activity 1.01</em>.</li>
				<li>Import <strong class="source-inline">tensorflow</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create the variable <strong class="source-inline">x</strong> and initialize it to 0.0:<p class="source-code">x=tf.Variable(0.0)</p></li>
				<li>Construct the <strong class="source-inline">loss</strong> function as a <strong class="source-inline">lambda</strong> function:<p class="source-code">loss=lambda:abs(x**2-10*x+25)</p></li>
				<li>Create an instance of an optimizer with a learning rate of <strong class="source-inline">.01</strong>:<p class="source-code">optimizer=tf.optimizers.Adam(.01)</p></li>
				<li>Run the optimizer through 10,000 iterations. You can start with a smaller number such as 1,000 and keep increasing the number of iterations until you get the solution:<p class="source-code">for i in range(10000):</p><p class="source-code">    optimizer.minimize(loss,x)</p></li>
				<li>Print the value of <strong class="source-inline">x</strong>:<p class="source-code">tf.print(x)</p><p>The output is as follows:</p><p class="source-code">4.99919891</p></li>
			</ol>
			<p>This is the solution to our quadratic equation. It may be noted that, irrespective of the number of iterations, you will never get a perfect 5.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gBTFGA">https://packt.live/3gBTFGA</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Dqa2Id">https://packt.live/2Dqa2Id</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor265"/>2. Neural Networks</h1>
			<h2 id="_idParaDest-239">Activity 2.01: Build a Multilayer Ne<a id="_idTextAnchor266"/>ural Network to Classify Sonar Signals</h2>
			<h2 id="_idParaDest-240">Solution <a id="_idTextAnchor267"/></h2>
			<p>Let's see how the solution looks. Remember—this is one solution, but there could be many variations:</p>
			<ol>
				<li value="1">Import all the required libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.preprocessing import LabelEncoder</p><p class="source-code"># Import Keras libraries</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Load and examine the data:<p class="source-code">df = pd.read_csv('sonar.csv')</p><p class="source-code">df.head()</p><p>The output is:</p><div id="_idContainer253" class="IMG---Figure"><img src="image/B15385_02_37.jpg" alt="Figure 2.37: Contents of sonar.csv&#13;&#10;"/></div><p class="figure-caption">Figure 2.37: Contents of sonar.csv</p><p>Observe that there are 60 features, and the target has two values—Rock and Mine.</p><p>This means that this is a binary classification problem. Let's prepare the data before we build the neural network.</p></li>
				<li>Separate the features and the labels:<p class="source-code">X_input = df.iloc[:, :-1]</p><p class="source-code">Y_label = df['Class'].values</p><p>In this code, <strong class="source-inline">X_input</strong> is selecting all the rows of all the columns except the <strong class="source-inline">Class</strong> column, and <strong class="source-inline">Y_label</strong> is just selecting the <strong class="source-inline">Class</strong> column.</p></li>
				<li>Labels are in text format. We need to encode them as numbers before we can use them with our model:<p class="source-code">labelencoder_Y = LabelEncoder() </p><p class="source-code">Y_label = labelencoder_Y.fit_transform(Y_label)</p><p class="source-code">Y_label = Y_label.reshape([208, 1])</p><p>The <strong class="source-inline">reshape</strong> function at the end will convert the labels into matrix format, which is expected by the model.</p></li>
				<li>Build the multilayer model with Keras:<p class="source-code">model = Sequential()</p><p class="source-code">model.add(Dense(300,input_dim=60, activation = 'relu'))</p><p class="source-code">model.add(Dense(200, activation = 'relu'))</p><p class="source-code">model.add(Dense(100, activation = 'relu'))</p><p class="source-code">model.add(Dense(1, activation = 'sigmoid'))</p><p>You can experiment with the number of layers and neurons, but the last layer can only have one neuron with a sigmoid activation function, since this is a binary classifier.</p></li>
				<li>Set the training parameters:<p class="source-code">model.compile(optimizer='adam',loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Train the model:<p class="source-code">model.fit(X_input, Y_label, epochs=30)</p><p>The truncated output will be somewhat similar to the following:</p><p class="source-code">Train on 208 samples</p><p class="source-code">Epoch 1/30</p><p class="source-code">208/208 [==============================] - 0s 205us/sample - </p><p class="source-code">loss: </p><p class="source-code">  0.1849 - accuracy: 0.9038</p><p class="source-code">Epoch 2/30</p><p class="source-code">208/208 [==============================] - 0s 220us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.1299 - accuracy: 0.9615</p><p class="source-code">Epoch 3/30</p><p class="source-code">208/208 [==============================] - 0s 131us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.0947 - accuracy: 0.9856</p><p class="source-code">Epoch 4/30</p><p class="source-code">208/208 [==============================] - 0s 151us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.1046 - accuracy: 0.9712</p><p class="source-code">Epoch 5/30</p><p class="source-code">208/208 [==============================] - 0s 171us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.0952 - accuracy: 0.9663</p><p class="source-code">Epoch 6/30</p><p class="source-code">208/208 [==============================] - 0s 134us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.0777 - accuracy: 0.9856</p><p class="source-code">Epoch 7/30</p><p class="source-code">208/208 [==============================] - 0s 129us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.1043 - accuracy: 0.9663</p><p class="source-code">Epoch 8/30</p><p class="source-code">208/208 [==============================] - 0s 142us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.0842 - accuracy: 0.9712</p><p class="source-code">Epoch 9/30</p><p class="source-code">208/208 [==============================] - 0s 155us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.1209 - accuracy: 0.9423</p><p class="source-code">Epoch 10/30</p><p class="source-code">208/208 [==============================] - ETA: 0s - loss: </p><p class="source-code">  0.0540 - accuracy: 0.98 - 0s 334us/sample - los</p></li>
				<li>Let's evaluate the trained model and examine its accuracy:<p class="source-code">model.evaluate(X_input, Y_label)</p><p>The output is as follows:</p><p class="source-code">208/208 [==============================] - 0s 128us/sample – </p><p class="source-code">loss: </p><p class="source-code">  0.0038 - accuracy: 1.0000</p><p class="source-code"> [0.003758653004367191, 1.0]</p><p>As you can see, we have been able to successfully train a multilayer binary neural network and get 100% accuracy within 30 epochs.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38EMoDi">https://packt.live/38EMoDi</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2W2sygb">https://packt.live/2W2sygb</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor268"/>3. Image Classification with Convolutional Neural Networks (CNNs)</h1>
			<h2 id="_idParaDest-242">Activity 3.01: Building a M<a id="_idTextAnchor269"/>ulticlass Classifier Based on the Fashion MNIST Dataset</h2>
			<h2 id="_idParaDest-243">Solution <a id="_idTextAnchor270"/></h2>
			<ol>
				<li value="1">Open a new Jupyter Notebook.</li>
				<li>Import <strong class="source-inline">tensorflow.keras.datasets.fashion_mnist</strong>:<p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p></li>
				<li>Load the Fashion MNIST dataset using <strong class="source-inline">fashion_mnist.load_data()</strong> and save the results to <strong class="source-inline">(features_train, label_train), (features_test, label_test)</strong>:<p class="source-code">(features_train, label_train), (features_test, label_test) = \</p><p class="source-code">fashion_mnist.load_data()</p></li>
				<li>Print the shape of the training set:<p class="source-code">features_train.shape</p><p>The output will be as follows:</p><p class="source-code">(60000, 28, 28)</p><p>The training set is composed of <strong class="source-inline">60000</strong> images of size <strong class="source-inline">28</strong> by <strong class="source-inline">28</strong>. We will need to reshape it and add the channel dimension.</p></li>
				<li>Print the shape of the testing set:<p class="source-code">features_test.shape</p><p>The output will be as follows:</p><p class="source-code">(10000, 28, 28)</p><p>The testing set is composed of <strong class="source-inline">10000</strong> images of size <strong class="source-inline">28</strong> by <strong class="source-inline">28</strong>. We will need to reshape it and add the channel dimension</p></li>
				<li>Reshape the training and testing sets with the dimensions <strong class="source-inline">(number_rows, 28, 28, 1)</strong>:<p class="source-code">features_train = features_train.reshape(60000, 28, 28, 1)</p><p class="source-code">features_test = features_test.reshape(10000, 28, 28, 1)</p></li>
				<li>Create three variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong> that take the values <strong class="source-inline">16</strong>, <strong class="source-inline">28</strong>, and <strong class="source-inline">28</strong>, respectively:<p class="source-code">batch_size = 16</p><p class="source-code">img_height = 28</p><p class="source-code">img_width = 28</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image \</p><p class="source-code">import ImageDataGenerator</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> called <strong class="source-inline">train_img_gen</strong> with data augmentation: <strong class="source-inline">rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'</strong>:<p class="source-code">train_img_gen = ImageDataGenerator(rescale=1./255, \</p><p class="source-code">                                   rotation_range=40, \</p><p class="source-code">                                   width_shift_range=0.1, \</p><p class="source-code">                                   height_shift_range=0.1, \</p><p class="source-code">                                   shear_range=0.2, \</p><p class="source-code">                                   zoom_range=0.2, \</p><p class="source-code">                                   horizontal_flip=True, \</p><p class="source-code">                                   fill_mode='nearest')</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> called <strong class="source-inline">val_img_gen</strong> with rescaling (by dividing by 255):<p class="source-code">val_img_gen = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">.flow()</strong> and specify the batch size, features, and labels from the training set:<p class="source-code">train_data_gen = train_img_gen.flow(features_train, \</p><p class="source-code">                                    label_train, \</p><p class="source-code">                                    batch_size=batch_size)</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">.flow()</strong> and specify the batch size, features, and labels from the testing set: <p class="source-code">val_data_gen = train_img_gen.flow(features_test, \</p><p class="source-code">                                  label_test, \</p><p class="source-code">                                  batch_size=batch_size)</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>: <p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class into a variable called <strong class="source-inline">model</strong> with the following layers: A convolution layer with <strong class="source-inline">64</strong> kernels of shape <strong class="source-inline">3</strong>, <strong class="source-inline">ReLU</strong> as the activation function, and the necessary input dimensions; a max pooling layer; a convolution layer with <strong class="source-inline">128</strong> kernels of shape <strong class="source-inline">3</strong> and <strong class="source-inline">ReLU</strong> as the activation function; a max pooling layer; a flatten layer; a fully connected layer with <strong class="source-inline">128</strong> units and <strong class="source-inline">ReLU</strong> as the activation function; a fully connected layer with <strong class="source-inline">10</strong> units and <strong class="source-inline">softmax</strong> as the activation function.<p>The code should be as follows:</p><p class="source-code">model = tf.keras.Sequential\</p><p class="source-code">        ([layers.Conv2D(64, 3, activation='relu', \</p><p class="source-code">                        input_shape=(img_height, \</p><p class="source-code">                                     img_width ,1)), \</p><p class="source-code">                        layers.MaxPooling2D(), \</p><p class="source-code">                        layers.Conv2D(128, 3, \</p><p class="source-code">                                      activation='relu'), \</p><p class="source-code">                        layers.MaxPooling2D(),\</p><p class="source-code">                        layers.Flatten(), \</p><p class="source-code">                        layers.Dense(128, \</p><p class="source-code">                                     activation='relu'), \</p><p class="source-code">                        layers.Dense(10, \</p><p class="source-code">                                     activation='softmax')])</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called optimizer:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']</strong>:<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Fit the neural networks with <strong class="source-inline">fit_generator()</strong> and provide the train and validation data generators, <strong class="source-inline">epochs=5</strong>, the steps per epoch, and the validation steps:<p class="source-code">model.fit_generator(train_data_gen, \</p><p class="source-code">                    steps_per_epoch=len(features_train) \</p><p class="source-code">                                    // batch_size, \</p><p class="source-code">                    epochs=5, \</p><p class="source-code">                    validation_data=val_data_gen, \</p><p class="source-code">                    validation_steps=len(features_test) \</p><p class="source-code">                                     // batch_size)</p><p>The expected output will be as follows:</p><div id="_idContainer254" class="IMG---Figure"><img src="image/B15385_03_30.jpg" alt="Figure 3.30: Model training log&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.30: Model training log</p>
			<p>We trained our CNN on five epochs, and we achieved accuracy scores of <strong class="source-inline">0.8271</strong> on the training set and <strong class="source-inline">0.8334</strong> on the validation set, respectively. Our model is not overfitting much and achieved quite a high score. The accuracy is still increasing after five epochs, so we may get even better results if we keep training it. This is something you may try by yourself.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ObmA8t">https://packt.live/2ObmA8t</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fiyyJi">https://packt.live/3fiyyJi</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-244">Activity 3.02: Fruit Classif<a id="_idTextAnchor271"/>ication with Transfer Learning</h2>
			<h2 id="_idParaDest-245">Solution <a id="_idTextAnchor272"/></h2>
			<ol>
				<li value="1">Open a new Jupyter Notebook.</li>
				<li>Import <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing the link to the dataset:<p class="source-code">file_url = 'https://github.com/PacktWorkshops'\</p><p class="source-code">           '/The-Deep-Learning-Workshop'\</p><p class="source-code">           '/raw/master/Chapter03/Datasets/Activity3.02'\</p><p class="source-code">           '/fruits360.zip'</p><p class="callout-heading">Note</p><p class="callout">In the aforementioned step, we are using the dataset stored at <a href="https://packt.live/3eePQ8G">https://packt.live/3eePQ8G</a>. If you have stored the dataset at any other URL, please change the highlighted path accordingly. </p></li>
				<li>Download the dataset using <strong class="source-inline">tf.keras.get_file</strong> with <strong class="source-inline">'fruits360.zip', origin=file_url, extract=True</strong> as parameters and save the result to a variable called <strong class="source-inline">zip_dir</strong>:<p class="source-code">zip_dir = tf.keras.utils.get_file('fruits360.zip', \</p><p class="source-code">                                  origin=file_url, \</p><p class="source-code">                                  extract=True)</p></li>
				<li>Import the <strong class="source-inline">pathlib</strong> library:<p class="source-code">import pathlib</p></li>
				<li>Create a variable called <strong class="source-inline">path</strong> containing the full path to the <strong class="source-inline">fruits360_filtered</strong> directory using <strong class="source-inline">pathlib.Path(zip_dir).parent</strong>:<p class="source-code">path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'</p></li>
				<li>Create two variables called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong> that take the full paths to the train (<strong class="source-inline">Training</strong>) and validation (<strong class="source-inline">Test</strong>) folders, respectively:<p class="source-code">train_dir = path / 'Training'</p><p class="source-code">validation_dir = path / 'Test'</p></li>
				<li>Create two variables called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong> that will get the number of images for the training and validation sets, that is, <strong class="source-inline">11398</strong> and <strong class="source-inline">4752</strong>:<p class="source-code">total_train = 11398</p><p class="source-code">total_val = 4752</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image \</p><p class="source-code">import ImageDataGenerator</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> called <strong class="source-inline">train_img_gen</strong> with data augmentation: <strong class="source-inline">rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'</strong>:<p class="source-code">train_img_gen = ImageDataGenerator(rescale=1./255, \</p><p class="source-code">                                   rotation_range=40, \</p><p class="source-code">                                   width_shift_range=0.1, \</p><p class="source-code">                                   height_shift_range=0.1, \</p><p class="source-code">                                   shear_range=0.2, \</p><p class="source-code">                                   zoom_range=0.2, \</p><p class="source-code">                                   horizontal_flip=True, \</p><p class="source-code">                                   fill_mode='nearest')</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> called <strong class="source-inline">val_img_gen</strong> with rescaling (by dividing by 255):<p class="source-code">val_img_gen = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create four variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, <strong class="source-inline">img_width</strong>, and <strong class="source-inline">channel</strong> that take the values <strong class="source-inline">16</strong>, <strong class="source-inline">100</strong>, <strong class="source-inline">100</strong>, and <strong class="source-inline">3</strong>, respectively:<p class="source-code">batch_size=16</p><p class="source-code">img_height = 100</p><p class="source-code">img_width = 100</p><p class="source-code">channel = 3</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">.flow_from_directory()</strong> and specify the batch size, training folder, and target size:<p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size=batch_size, \</p><p class="source-code">                 directory=train_dir, \</p><p class="source-code">                 target_size=(img_height, img_width))</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">.flow_from_directory()</strong> and specify the batch size, validation folder, and target size:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">               (batch_size=batch_size, \</p><p class="source-code">               directory=validation_dir, \</p><p class="source-code">               target_size=(img_height, img_width))</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Import <strong class="source-inline">VGG16</strong> from <strong class="source-inline">tensorflow.keras.applications</strong>:<p class="source-code">from tensorflow.keras.applications import VGG16</p></li>
				<li>Instantiate a <strong class="source-inline">VGG16</strong> model into a variable called <strong class="source-inline">base_model</strong> with the following parameters: <p class="source-code">base_model = VGG16(input_shape=(img_height, \</p><p class="source-code">                                img_width, channel), \</p><p class="source-code">                                weights='imagenet', \</p><p class="source-code">                                include_top=False)</p></li>
				<li>Set this model to non-trainable using the <strong class="source-inline">.trainable</strong> attribute:<p class="source-code">base_model.trainable = False</p></li>
				<li>Print the summary of this <strong class="source-inline">VGG16</strong> model:<p class="source-code">base_model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer255" class="IMG---Figure"><img src="image/B15385_03_31.jpg" alt="Figure 3.31: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 3.31: Model summary</p><p>This output shows us the architecture of <strong class="source-inline">VGG16</strong>. We can see that there are <strong class="source-inline">14,714,688</strong> parameters in total, but there is no trainable parameter. This is expected as we have frozen all the layers of this model.</p></li>
				<li>Create a new model using <strong class="source-inline">tf.keras.Sequential()</strong> by adding the base model to the following layers: <strong class="source-inline">Flatten()</strong>, <strong class="source-inline">Dense(1000, activation='relu')</strong>, and <strong class="source-inline">Dense(120, activation='softmax')</strong>. Save this model to a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential([base_model, \</p><p class="source-code">                             layers.Flatten(), \</p><p class="source-code">                             layers.Dense(1000, \</p><p class="source-code">                                          activation='relu'), \</p><p class="source-code">                             layers.Dense(120, \</p><p class="source-code">                                          activation='softmax')])</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']</strong>:<p class="source-code">model.compile(loss='categorical_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Fit the neural networks with <strong class="source-inline">fit_generator()</strong> and provide the train and validation data generators, <strong class="source-inline">epochs=5</strong>, the steps per epoch, and the validation steps. This model may take a few minutes to train:<p class="source-code">model.fit_generator(train_data_gen, \</p><p class="source-code">                    steps_per_epoch=len(features_train) \</p><p class="source-code">                                    // batch_size, \</p><p class="source-code">                    epochs=5, \</p><p class="source-code">                    validation_data=val_data_gen, \</p><p class="source-code">                    validation_steps=len(features_test) \</p><p class="source-code">                                     // batch_size)</p><p>The expected output will be as follows:</p><div id="_idContainer256" class="IMG---Figure"><img src="image/B15385_03_32.jpg" alt="Figure 3.32: Expected output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.32: Expected output</p>
			<p>Here, we used transfer learning to customize a pretrained <strong class="source-inline">VGG16</strong> model on ImageNet so that it fits our fruit classification dataset. We replaced the head of the model with our own fully connected layers and trained these layers on five epochs. We achieved an accuracy score of <strong class="source-inline">0.9106</strong> for the training set and <strong class="source-inline">0.8920</strong> for the testing set. These are quite remarkable results given the time and hardware used to train this model. You can try to fine-tune this model and see whether you can achieve an even better score.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2DsVRCl">https://packt.live/2DsVRCl</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor273"/>4. Deep Learning for Text – Embeddings</h1>
			<h2 id="_idParaDest-247">Activity 4.01: Text <a id="_idTextAnchor274"/>Preprocessing of the 'Alice in Wonderland' Text</h2>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor275"/>Solution</h2>
			<p>You need to perform the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before commencing this activity, make sure you have defined the <strong class="source-inline">alice_raw</strong> variable as demonstrated in the section titled <em class="italic">Downloading Text Corpora Using NLTK</em>.</p>
			<ol>
				<li value="1">Change the data to lowercase and separate into sentences:<p class="source-code">txt_sents = tokenize.sent_tokenize(alice_raw.lower())</p></li>
				<li>Tokenize the sentences:<p class="source-code">txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]</p></li>
				<li>Import <strong class="source-inline">punctuation</strong> from the <strong class="source-inline">string</strong> module and <strong class="source-inline">stopwords</strong> from NLTK:<p class="source-code">from string import punctuation</p><p class="source-code">stop_punct = list(punctuation)</p><p class="source-code">from nltk.corpus import stopwords</p><p class="source-code">stop_nltk = stopwords.words("english")</p></li>
				<li>Create a variable holding the contextual stop words <strong class="source-inline">--</strong> and <strong class="source-inline">said</strong>:<p class="source-code">stop_context = ["--", "said"]</p></li>
				<li>Create a master list for the stop words to remove words that contain terms from punctuation, NLTK stop words, and contextual stop words:<p class="source-code">stop_final = stop_punct + stop_nltk + stop_context</p></li>
				<li>Define a function to drop these tokens from any input sentence (tokenized):<p class="source-code">def drop_stop(input_tokens):</p><p class="source-code">    return [token for token in input_tokens \</p><p class="source-code">            if token not in stop_final]</p></li>
				<li>Remove the terms in <strong class="source-inline">stop_final</strong> from the tokenized text:<p class="source-code">alice_words_nostop = [drop_stop(sent) for sent in txt_words]</p><p class="source-code">print(alice_words_nostop[:2])</p><p>Here's what the first two sentences look like:</p><p class="source-code">[['alice', "'s", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', "'and", 'use', 'book', 'thought', 'alice', "'without", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close']]</p></li>
				<li>Using the <strong class="source-inline">PorterStemmer</strong> algorithm from NLTK, perform stemming on the result. Print out the first five sentences of the result:<p class="source-code">from nltk.stem import PorterStemmer</p><p class="source-code">stemmer_p = PorterStemmer()</p><p class="source-code">alice_words_stem = [[stemmer_p.stem(token) for token in sent] \</p><p class="source-code">                     for sent in alice_words_nostop]</p><p class="source-code">print(alice_words_stem[:5])</p><p>The output will be as follows:</p><p class="source-code">[['alic', "'s", 'adventur', 'wonderland', 'lewi', 'carrol', '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', 'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', 'peep', 'book', 'sister', 'read', 'pictur', 'convers', "'and", 'use', 'book', 'thought', 'alic', "'without", 'pictur', 'convers'], ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', 'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', 'daisi', 'suddenli', 'white', 'rabbit', 'pink', 'eye', 'ran', 'close'], ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', 'rabbit', 'say', "'oh", 'dear'], ['oh', 'dear'], ['shall', 'late']]</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor276"/>Activity 4.02: Text Representation for Alice in Wonderland</h2>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor277"/>Solution</h2>
			<p>You need to perform the following steps:</p>
			<ol>
				<li value="1">From <em class="italic">Activity 4.01</em>, <em class="italic">Text Preprocessing Alice in Wonderland</em>, print the first three sentences from the result after stop word removal. This is the data you will work with:<p class="source-code">print(alice_words_nostop[:3])</p><p>The output is as follows:</p><p class="source-code">[['alice', "'s", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', "'and", 'use', 'book', 'thought', 'alice', "'without", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close'], ['nothing', 'remarkable', 'alice', 'think', 'much', 'way', 'hear', 'rabbit', 'say', "'oh", 'dear']]</p></li>
				<li>Import <strong class="source-inline">word2vec</strong> from Gensim and train your word embeddings with default parameters:<p class="source-code">from gensim.models import word2vec</p><p class="source-code">model = word2vec.Word2Vec(alice_words_nostop)</p></li>
				<li>Find the <strong class="source-inline">5</strong> terms most similar to <strong class="source-inline">rabbit</strong>:<p class="source-code">model.wv.most_similar("rabbit", topn=5)</p><p>The output is as follows:</p><p class="source-code">[('alice', 0.9963310360908508),</p><p class="source-code"> ('little', 0.9956872463226318),</p><p class="source-code"> ('went', 0.9955698251724243),</p><p class="source-code"> ("'s", 0.9955658912658691),</p><p class="source-code"> ('would', 0.9954401254653931)]</p></li>
				<li>Using a <strong class="source-inline">window</strong> size of <strong class="source-inline">2</strong>, retrain the word vectors:<p class="source-code">model = word2vec.Word2Vec(alice_words_nostop, window=2)</p></li>
				<li>Find the terms most similar to <strong class="source-inline">rabbit</strong>:<p class="source-code">model.wv.most_similar("rabbit", topn=5)</p><p>The output will be as follows:</p><p class="source-code">[('alice', 0.9491485357284546),</p><p class="source-code"> ("'s", 0.9364748001098633),</p><p class="source-code"> ('little', 0.9345826506614685),</p><p class="source-code"> ('large', 0.9341927170753479),</p><p class="source-code"> ('duchess', 0.9341296553611755)]</p></li>
				<li>Retrain word vectors using the Skip-gram method with a window size of <strong class="source-inline">5</strong>:<p class="source-code">model = word2vec.Word2Vec(alice_words_nostop, window=5, sg=1)</p></li>
				<li>Find the terms most similar to <strong class="source-inline">rabbit</strong>:<p class="source-code">model.wv.most_similar("rabbit", topn=5)</p><p>The output will be as follows:</p><p class="source-code">[('gardeners', 0.9995723366737366),</p><p class="source-code"> ('end', 0.9995588064193726),</p><p class="source-code"> ('came', 0.9995309114456177),</p><p class="source-code"> ('sort', 0.9995298385620117),</p><p class="source-code"> ('upon', 0.9995272159576416)]</p></li>
				<li>Find the representation for the phrase <strong class="source-inline">white rabbit</strong> by averaging the vectors for <strong class="source-inline">white</strong> and <strong class="source-inline">rabbit</strong>:<p class="source-code">v1 = model.wv['white']</p><p class="source-code">v2 = model.wv['rabbit']</p><p class="source-code">res1 = (v1+v2)/2</p></li>
				<li>Find the representation for <strong class="source-inline">mad hatter</strong> by averaging the vectors for <strong class="source-inline">mad</strong> and <strong class="source-inline">hatter</strong>:<p class="source-code">v1 = model.wv['mad']</p><p class="source-code">v2 = model.wv['hatter']</p><p class="source-code">res2 = (v1+v2)/2</p></li>
				<li>Find the cosine similarity between these two phrases:<p class="source-code">model.wv.cosine_similarities(res1, [res2])</p><p>This gives us the following value:</p><p class="source-code">array([0.9996213], dtype=float32)</p></li>
				<li>Load the pre-trained GloVe embeddings of size 100D using the formatted keyed vectors:<p class="source-code">from gensim.models.keyedvectors import KeyedVectors</p><p class="source-code">glove_model = KeyedVectors.load_word2vec_format\</p><p class="source-code">("glove.6B.100d.w2vformat.txt", binary=False)</p></li>
				<li>Find representations for <strong class="source-inline">white rabbit</strong> and <strong class="source-inline">mad hatter</strong>:<p class="source-code">v1 = glove_model['white']</p><p class="source-code">v2 = glove_model['rabbit']</p><p class="source-code">res1 = (v1+v2)/2</p><p class="source-code">v1 = glove_model['mad']</p><p class="source-code">v2 = glove_model['hatter']</p><p class="source-code">res2 = (v1+v2)/2</p></li>
				<li>Find the <strong class="source-inline">cosine</strong> similarity between the two phrases. Has the cosine similarity changed?<p class="source-code">glove_model.cosine_similarities(res1, [res2])</p><p>The following is the output of the preceding code:</p><p class="source-code">array([0.4514577], dtype=float32)</p></li>
			</ol>
			<p>Here, we can see that the cosine similarity between the two phrases "<strong class="source-inline">mad hatter</strong>" and "<strong class="source-inline">white rabbit</strong>" is far lower from the GloVe model. This is because the GloVe model hasn't seen the terms together in its training data as much as they appear in the book. In the book, the terms <strong class="source-inline">mad</strong> and <strong class="source-inline">hatter</strong> appear together a lot because they form the name of an important character. In other contexts, of course, we don't see <strong class="source-inline">mad</strong> and <strong class="source-inline">hatter</strong> together as often.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor278"/>5. Deep Learning for Sequences</h1>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor279"/>Activity 5.01: Using a Plain RNN Model to Predict IBM Stock Prices</h2>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor280"/>Solution </h2>
			<ol>
				<li value="1">Import the necessary libraries, load the <strong class="source-inline">.csv </strong>file, reverse the index, and plot the time series (the <strong class="source-inline">Close</strong> column) for visual inspection:<p class="source-code">import pandas as pd, numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">inp0 = pd.read_csv("IBM.csv")</p><p class="source-code">inp0 = inp0.sort_index(ascending=False)</p><p class="source-code">inp0.plot("Date", "Close")</p><p class="source-code">plt.show()</p><p>The output will be as follows, with the closing price plotted on the <em class="italic">Y-axis</em>:</p><p> </p><div id="_idContainer257" class="IMG---Figure"><img src="image/B15385_05_40.jpg" alt="Figure 5.40: The trend for IBM stock prices&#13;&#10;"/></div><p class="figure-caption">Figure 5.40: The trend for IBM stock prices</p></li>
				<li>Extract the values for <strong class="source-inline">Close</strong> from the DataFrame as a <strong class="source-inline">numpy</strong> array and plot them using <strong class="source-inline">matplotlib</strong>:<p class="source-code">ts_data = inp0.Close.values.reshape(-1,1)</p><p class="source-code">plt.figure(figsize=[14,5])</p><p class="source-code">plt.plot(ts_data)</p><p class="source-code">plt.show()</p><p>The resulting trend is as follows, with the index plotted on the <em class="italic">X-axis</em>:</p><div id="_idContainer258" class="IMG---Figure"><img src="image/B15385_05_41.jpg" alt="Figure 5.41: The stock price data visualized&#13;&#10;"/></div><p class="figure-caption">Figure 5.41: The stock price data visualized</p></li>
				<li>Assign the final 25% data as test data and the first 75% as train data:<p class="source-code">train_recs = int(len(ts_data) * 0.75)</p><p class="source-code">train_data = ts_data[:train_recs]</p><p class="source-code">test_data = ts_data[train_recs:]</p><p class="source-code">len(train_data), len(test_data)</p><p>The output will be as follows:</p><p class="source-code">(1888, 630)</p></li>
				<li>Using <strong class="source-inline">MinMaxScaler</strong> from <strong class="source-inline">sklearn</strong>, scale the train and test data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">train_scaled = scaler.fit_transform(train_data)</p><p class="source-code">test_scaled = scaler.transform(test_data)</p></li>
				<li>Using the <strong class="source-inline">get_lookback</strong> function we defined earlier in this chapter (refer to the <em class="italic">Preparing the Data for Stock Price Prediction</em> section), get the lookback data for the train and test sets using a lookback period of 10:<p class="source-code">look_back = 10</p><p class="source-code">trainX, trainY = get_lookback(train_scaled, look_back=look_back)</p><p class="source-code">testX, testY = get_lookback(test_scaled, look_back= look_back)</p><p class="source-code">trainX.shape, testX.shape</p><p>The output will be as follows:</p><p class="source-code">((1888, 10), (630, 10))</p></li>
				<li>From Keras, import all the necessary layers for employing plain RNNs (<strong class="source-inline">SimpleRNN</strong>, <strong class="source-inline">Activation</strong>, <strong class="source-inline">Dropout</strong>, <strong class="source-inline">Dense</strong>, and <strong class="source-inline">Reshape</strong>) and 1D convolutions (Conv1D). Also, import the <strong class="source-inline">mean_squared_error</strong> metric from <strong class="source-inline">sklearn</strong>:<p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import SimpleRNN, Activation, Dropout, Dense, Reshape, Conv1D</p><p class="source-code">from sklearn.metrics import mean_squared_error</p></li>
				<li>Build a model with a 1D convolution layer (5 filters of size 3) and an RNN layer with 32 neurons. Add 25% dropout after the RNN layer. Print the model's summary:<p class="source-code">model_comb = Sequential()</p><p class="source-code">model_comb.add(Reshape((look_back,1), \</p><p class="source-code">                        input_shape = (look_back,)))</p><p class="source-code">model_comb.add(Conv1D(5, 3, activation='relu'))</p><p class="source-code">model_comb.add(SimpleRNN(32))</p><p class="source-code">model_comb.add(Dropout(0.25))</p><p class="source-code">model_comb.add(Dense(1))</p><p class="source-code">model_comb.add(Activation('linear'))</p><p class="source-code">model.summary()</p><p>The output will be as follows:</p><div id="_idContainer259" class="IMG---Figure"><img src="image/B15385_05_42.jpg" alt="Figure 5.42: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 5.42: Summary of the model</p></li>
				<li>Compile the model with the <strong class="source-inline">mean_squared_error</strong> loss and the <strong class="source-inline">adam</strong> optimizer. Fit this on the train data in five epochs, with a validation split of 10% and a batch size of 1:<p class="source-code">model_comb.compile(loss='mean_squared_error', \</p><p class="source-code">                   optimizer='adam')</p><p class="source-code">model_comb.fit(trainX, trainY, epochs=5, \</p><p class="source-code">               batch_size=1, verbose=2, \</p><p class="source-code">               validation_split=0.1)</p><p>The output will be as follows:</p><div id="_idContainer260" class="IMG---Figure"><img src="image/B15385_05_43.jpg" alt="Figure 5.43: Training and validation loss&#13;&#10;"/></div><p class="figure-caption">Figure 5.43: Training and validation loss</p></li>
				<li>Using the <strong class="source-inline">get_model_perf</strong> method, print the RMSE of the model:<p class="source-code">get_model_perf(model_comb)</p><p>The output will be as follows:</p><p class="source-code">Train RMSE: 0.03 RMSE</p><p class="source-code">Test RMSE: 0.03 RMSE</p></li>
				<li>Plot the predictions – the entire view, as well as the zoomed-in view:<p class="source-code">%matplotlib notebook</p><p class="source-code">plt.figure(figsize=[10,5])</p><p class="source-code">plot_pred(model_comb)</p><p>We should see the following plot of predictions (dotted lines) versus the actuals (solid lines):</p><p> </p><div id="_idContainer261" class="IMG---Figure"><img src="image/B15385_05_44.jpg" alt="Figure 5.44: Predictions versus actuals&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.44: Predictions versus actuals</p>
			<p>The zoomed-in view is as follows:</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/B15385_05_45.jpg" alt="Figure 5.45: Predictions (dotted lines) versus actuals (solid lines) – detailed view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.45: Predictions (dotted lines) versus actuals (solid lines) – detailed view</p>
			<p>We can see that the model does a great job of catching the finer patterns and does extremely well at predicting the daily stock price.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZctArW">https://packt.live/2ZctArW</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38EDOEA">https://packt.live/38EDOEA</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor281"/>6. LSTMs, GRUs, and Advanced RNNs</h1>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor282"/>Activity 6.01: Sentiment Analysis of Amazon Product Reviews</h2>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor283"/>Solution</h2>
			<ol>
				<li value="1">Read in the data files for the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> sets. Examine the shapes of the datasets and print out the top <strong class="source-inline">5</strong> records from the <strong class="source-inline">train</strong> data:<p class="source-code">import pandas as pd, numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">train_df = pd.read_csv("Amazon_reviews_train.csv")</p><p class="source-code">test_df = pd.read_csv("Amazon_reviews_test.csv")</p><p class="source-code">print(train_df.shape, train_df.shape)</p><p class="source-code">train_df.head(5)</p><p>The dataset's shape and header are as follows:</p><div id="_idContainer263" class="IMG---Figure"><img src="image/B15385_06_26.jpg" alt="Figure 6.26: First five records from the train dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.26: First five records from the train dataset</p></li>
				<li>For convenience, when it comes to processing, separate the raw text and the labels for the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> sets. You should have <strong class="source-inline">4</strong> variables, as follows: <strong class="source-inline">train_raw</strong> comprising raw text for the train data, <strong class="source-inline">train_labels</strong> with labels for the train data, <strong class="source-inline">test_raw</strong> containing raw text for the test data, and <strong class="source-inline">test_labels</strong> comprising Labels for the test data. Print the first two reviews from the <strong class="source-inline">train</strong> text.<p class="source-code">train_raw = train_df.review_text.values</p><p class="source-code">train_labels = train_df.label.values</p><p class="source-code">test_raw = test_df.review_text.values</p><p class="source-code">test_labels = test_df.label.values</p><p class="source-code">train_raw[:2]</p><p>The preceding code results in the following output:</p><div id="_idContainer264" class="IMG---Figure"><img src="image/B15385_06_27.jpg" alt="Figure 6.27: Raw text from the train dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.27: Raw text from the train dataset</p></li>
				<li>Normalize the case and tokenize the test and train texts using NLTK's <strong class="source-inline">word_tokenize</strong> (after importing it, of course – hint: use a list comprehension for cleaner code). Download <strong class="source-inline">punkt</strong> from <strong class="source-inline">nltk</strong> if you haven't used the tokenizer before. Print the first review from the train data to check if the tokenization worked.<p class="source-code">import nltk</p><p class="source-code">nltk.download('punkt')</p><p class="source-code">from nltk.tokenize import word_tokenize</p><p class="source-code">train_tokens = [word_tokenize(review.lower()) \</p><p class="source-code">                for review in train_raw]</p><p class="source-code">test_tokens = [word_tokenize(review.lower()) \</p><p class="source-code">               for review in test_raw]</p><p class="source-code">print(train_tokens[0])</p><p>The tokenized data gets printed as follows:</p><div id="_idContainer265" class="IMG---Figure"><img src="image/B15385_06_28.jpg" alt="Figure 6.28: Tokenized review from the train dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.28: Tokenized review from the <strong class="source-inline">train</strong> dataset</p></li>
				<li>Import any stop words (built in to NLTK) and punctuation from the string module. Define a function (<strong class="source-inline">drop_stop</strong>) to remove these tokens from any input tokenized sentence. Download <strong class="source-inline">stopwords</strong> from NLTK if you haven't used it before:<p class="source-code">from string import punctuation</p><p class="source-code">stop_punct = list(punctuation)</p><p class="source-code">nltk.download("stopwords")</p><p class="source-code">from nltk.corpus import stopwords</p><p class="source-code">stop_nltk = stopwords.words("english")</p><p class="source-code">stop_final = stop_punct + stop_nltk</p><p class="source-code">def drop_stop(input_tokens):</p><p class="source-code">    return [token for token in input_tokens \</p><p class="source-code">            if token not in stop_final]</p></li>
				<li>Using the defined function (<strong class="source-inline">drop_stop</strong>), remove the redundant stop words from the <strong class="source-inline">train</strong> and the <strong class="source-inline">test</strong> texts. Print the first review of the processed <strong class="source-inline">train</strong> texts to check whether the function worked:<p class="source-code">train_tokens_no_stop = [drop_stop(sent) \</p><p class="source-code">                        for sent in train_tokens]</p><p class="source-code">test_tokens_no_stop = [drop_stop(sent) \</p><p class="source-code">                       for sent in test_tokens]</p><p class="source-code">print(train_tokens_no_stop[0])</p><p>We'll get the following output:</p><p class="source-code">['stuning', 'even', 'non-gamer', 'sound', 'track', 'beautiful', </p><p class="source-code"> 'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', </p><p class="source-code"> 'people', 'hate', 'vid', 'game', 'music', 'played', 'game', </p><p class="source-code"> 'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', </p><p class="source-code"> 'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', </p><p class="source-code"> 'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', </p><p class="source-code"> 'impress', 'anyone', 'cares', 'listen', '^_^']</p></li>
				<li>Using <strong class="source-inline">PorterStemmer</strong> from NLTK, stem the tokens for both the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> data:<p class="source-code">from nltk.stem import PorterStemmer</p><p class="source-code">stemmer_p = PorterStemmer()</p><p class="source-code">train_tokens_stem = [[stemmer_p.stem(token) for token in sent] \</p><p class="source-code">                     for sent in train_tokens_no_stop]</p><p class="source-code">test_tokens_stem = [[stemmer_p.stem(token) for token in sent] \</p><p class="source-code">                     for sent in test_tokens_no_stop]</p><p class="source-code">print(train_tokens_stem[0])</p><p>The result should be printed as follows:</p><p class="source-code">['stune', 'even', 'non-gam', 'sound', 'track', 'beauti', 'paint', </p><p class="source-code"> 'seneri', 'mind', 'well', 'would', 'recomend', 'even', 'peopl', </p><p class="source-code"> 'hate', 'vid', 'game', 'music', 'play', 'game', 'chrono', 'cross', </p><p class="source-code"> 'game', 'ever', 'play', 'best', 'music', 'back', 'away', 'crude', </p><p class="source-code"> 'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', </p><p class="source-code"> 'orchestra', 'would', 'impress', 'anyon', 'care', 'listen', '^_^']</p></li>
				<li>Create the strings for each of the <strong class="source-inline">train</strong> and <strong class="source-inline">text</strong> reviews. This will help us work with the utilities in Keras to create and pad the sequences. Create the <strong class="source-inline">train_texts</strong> and <strong class="source-inline">test_texts</strong> variables. Print the first review from the processed <strong class="source-inline">train</strong> data to confirm this:<p class="source-code">train_texts = [" ".join(txt) for txt in train_tokens_stem]</p><p class="source-code">test_texts = [" ".join(txt) for txt in test_tokens_stem]</p><p class="source-code">print(train_texts[0])</p><p>The result of the preceding code is as follows:</p><p class="source-code">stune even non-gam sound track beauti paint seneri mind well would recommend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^</p></li>
				<li>From Keras' preprocessing utilities for text (<strong class="source-inline">keras.preprocessing.text</strong>), import the <strong class="source-inline">Tokenizer</strong> module. Define a vocabulary size of <strong class="source-inline">10000</strong> and instantiate the tokenizer with this vocabulary:<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">vocab_size = 10000</p><p class="source-code">tok = Tokenizer(num_words=vocab_size)</p></li>
				<li>Fit the tokenizer on the <strong class="source-inline">train</strong> texts. This works just like <strong class="source-inline">CountVectorizer</strong> did in <em class="italic">Chapter 4, Deep Learning for Text – Embeddings</em>, and trains the vocabulary. After fitting, use the <strong class="source-inline">texts_to_sequences</strong> method of the tokenizer on the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> sets to create the sequences for them. Print the sequence for the first review in the train data:<p class="source-code">tok.fit_on_texts(train_texts)</p><p class="source-code">train_sequences = tok.texts_to_sequences(train_texts)</p><p class="source-code">test_sequences = tok.texts_to_sequences(test_texts)</p><p class="source-code">print(train_sequences[0])</p><p>The encoded sequence is as follows:</p><p class="source-code"> [22, 514, 7161, 85, 190, 184, 1098, 283, 20, 11, 1267, 22, </p><p class="source-code">  56, 370, 9682, 114, 41, 71, 114, 8166, 1455, 114, 51, 71, </p><p class="source-code">  29, 41, 58, 182, 2931, 2153, 75, 8167, 816, 2666, 829, 719, </p><p class="source-code">  3871, 11, 483, 120, 268, 110]</p></li>
				<li>We need to find the optimal length of the sequences to process the model. Get the length of the reviews from the <strong class="source-inline">train</strong> set into a list and plot a histogram of the lengths:<p class="source-code">seq_lens = [len(seq) for seq in train_sequences]</p><p class="source-code">plt.hist(seq_lens)</p><p class="source-code">plt.show()</p><p>The distribution of the lengths is as follows:</p><div id="_idContainer266" class="IMG---Figure"><img src="image/B15385_06_29.jpg" alt="Figure 6.29: Histogram of text lengths&#13;&#10;"/></div><p class="figure-caption">Figure 6.29: Histogram of text lengths</p></li>
				<li> The data is now in the same format as the IMDb data we used in this chapter. Using a sequence length of <strong class="source-inline">100</strong> (define the <strong class="source-inline">maxlen = 100</strong> variable), use the <strong class="source-inline">pad_sequences</strong> method from the <strong class="source-inline">sequence</strong> module in Keras' preprocessing utilities (<strong class="source-inline">keras.preprocessing.sequence</strong>) to limit the sequences to <strong class="source-inline">100</strong> for both the <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> data. Check the shape of the result for the train data:<p class="source-code">maxlen = 100</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import pad_sequences</p><p class="source-code">X_train = pad_sequences(train_sequences, maxlen=maxlen)</p><p class="source-code">X_test = pad_sequences(test_sequences, maxlen=maxlen)</p><p class="source-code">X_train.shape</p><p>The shape is as follows:</p><p class="source-code">(25000, 100)</p></li>
				<li>To build the model, import all the necessary layers from Keras (<strong class="source-inline">embedding</strong>, <strong class="source-inline">spatial dropout</strong>, <strong class="source-inline">LSTM</strong>, <strong class="source-inline">dropout</strong>, and <strong class="source-inline">dense</strong>) and import the <strong class="source-inline">Sequential</strong> model. Initialize the <strong class="source-inline">Sequential</strong> model:<p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, GRU, LSTM</p><p class="source-code">model_lstm = Sequential()</p></li>
				<li>Add an embedding layer with <strong class="source-inline">32</strong> as the vector size (<strong class="source-inline">output_dim</strong>). Add a spatial dropout of <strong class="source-inline">40%</strong>:<p class="source-code">model_lstm.add(Embedding(vocab_size, output_dim=32))</p><p class="source-code">model_lstm.add(SpatialDropout1D(0.4))</p></li>
				<li>Build a stacked LSTM model with <strong class="source-inline">2</strong> layers that have <strong class="source-inline">64</strong> cells each. Add a dropout layer with <strong class="source-inline">40%</strong> dropout:<p class="source-code">model_lstm.add(LSTM(64, return_sequences=True))</p><p class="source-code">model_lstm.add(LSTM(64, return_sequences=False))</p><p class="source-code">model_lstm.add(Dropout(0.4))</p></li>
				<li>Add a dense layer with <strong class="source-inline">32</strong> neurons with <strong class="source-inline">relu</strong> activation, then a <strong class="source-inline">50%</strong> dropout layer, followed by another dense layer of <strong class="source-inline">32</strong> neurons with <strong class="source-inline">relu</strong> activation, and follow this up with another dropout layer with <strong class="source-inline">50%</strong> dropout:<p class="source-code">model_lstm.add(Dense(32, activation='relu'))</p><p class="source-code">model_lstm.add(Dropout(0.5))</p><p class="source-code">model_lstm.add(Dense(32, activation='relu'))</p><p class="source-code">model_lstm.add(Dropout(0.5))</p></li>
				<li>Add a final dense layer with a single neuron with <strong class="source-inline">sigmoid</strong> <strong class="source-inline">activation</strong> and compile the model. Print the model summary:<p class="source-code">model_lstm.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_lstm.compile(loss='binary_crossentropy', \</p><p class="source-code">                   optimizer='rmsprop', \</p><p class="source-code">                   metrics=['accuracy'])</p><p class="source-code">model_lstm.summary()</p><p>The summary of the model will be as follows:</p><div id="_idContainer267" class="IMG---Figure"><img src="image/B15385_06_30.jpg" alt="Figure 6.30: Stacked LSTM model summary&#13;&#10;"/></div><p class="figure-caption">Figure 6.30: Stacked LSTM model summary</p></li>
				<li>Fit the model on the training data with a <strong class="source-inline">20%</strong> validation split and a batch size of <strong class="source-inline">128</strong>. Train for <strong class="source-inline">5</strong> <strong class="source-inline">epochs</strong>:<p class="source-code">history_lstm = model_lstm.fit(X_train, train_labels, \</p><p class="source-code">                              batch_size=128, \</p><p class="source-code">                              validation_split=0.2, \</p><p class="source-code">                              epochs = 5)</p><p>We will get the following training output:</p><div id="_idContainer268" class="IMG---Figure"><img src="image/B15385_06_31.jpg" alt="Figure 6.31: Stacked LSTM model training output&#13;&#10;"/></div><p class="figure-caption">Figure 6.31: Stacked LSTM model training output</p></li>
				<li>Make a prediction on the test set using the <strong class="source-inline">predict_classes</strong> method of the model. Then, print out the confusion matrix:<p class="source-code">from sklearn.metrics import accuracy_score, confusion_matrix</p><p class="source-code">test_pred = model_lstm.predict_classes(X_test)</p><p class="source-code">print(confusion_matrix(test_labels, test_pred))</p><p>We will get the following result:</p><p class="source-code">[[10226,  1931],</p><p class="source-code"> [ 1603, 11240]]</p></li>
				<li>Using the <strong class="source-inline">accuracy_score</strong> method from <strong class="source-inline">scikit-learn</strong>, calculate the accuracy of the test set.<p class="source-code">print(accuracy_score(test_labels, test_pred))</p><p>The accuracy we get is:</p><p class="source-code">0.85864</p></li>
			</ol>
			<p>As we can see, the accuracy score is around <strong class="source-inline">86%</strong>, and looking at the confusion matrix (output of <em class="italic">step 18</em>), the model does a decent job of predicting both classes well. We got this accuracy without doing any hyperparameter tuning. You can tweak the hyperparameters to get significantly higher accuracy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fpo0YI">https://packt.live/3fpo0YI</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Wi75QH">https://packt.live/2Wi75QH</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor284"/>7. Generative Adversarial Networks</h1>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor285"/>Activity 7.01: Implementing a DCGAN for the MNIST Fashion Dataset</h2>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor286"/>Solution</h2>
			<ol>
				<li value="1">Open a new Jupyter Notebook and name it <strong class="source-inline">Activity 7.01</strong>. Import the following library packages:<p class="source-code"># Import the required library functions</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from matplotlib import pyplot</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Input</p><p class="source-code">from tensorflow.keras.initializers import RandomNormal</p><p class="source-code">from tensorflow.keras.models import Model, Sequential</p><p class="source-code">from tensorflow.keras.layers \</p><p class="source-code">import Reshape, Dense, Dropout, Flatten,Activation</p><p class="source-code">from tensorflow.keras.layers import LeakyReLU,BatchNormalization</p><p class="source-code">from tensorflow.keras.layers import Conv2D, UpSampling2D,Conv2DTranspose</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p></li>
				<li>Create a function that will generate real data samples from the fashion MNIST data:<p class="source-code"># Function to generate real data samples</p><p class="source-code">def realData(batch):</p><p class="source-code">    # Get the MNIST data </p><p class="source-code">    (X_train, _), (_, _) = fashion_mnist.load_data()</p><p class="source-code">    # Reshaping the input data to include channel</p><p class="source-code">    X = X_train[:,:,:,np.newaxis]</p><p class="source-code">    # normalising the data to be between 0 and 1</p><p class="source-code">    X = (X.astype('float32') - 127.5)/127.5</p><p class="source-code">    # Generating a batch of data</p><p class="source-code">    imageBatch = X[np.random.randint(0, X.shape[0], \</p><p class="source-code">                                     size=batch)]</p><p class="source-code">    return imageBatch</p><p>The output from this function is the batch of MNIST data. Please note that we normalize the input data by subtracting <strong class="source-inline">127.5</strong>, which is half the max pixel value, and dividing by the same value. This will help in converging the solution faster.</p></li>
				<li>Now, let's generate a set of images from the MNIST dataset:<p class="source-code"># Generating a set of  sample images </p><p class="source-code">fashionData = realData(25)</p><p>You should get the following output:</p><div id="_idContainer269" class="IMG---Figure"><img src="image/B15385_07_36.jpg" alt="Figure 7.36: Generating images from MNIST&#13;&#10;"/></div><p class="figure-caption">Figure 7.36: Generating images from MNIST</p></li>
				<li>Now, let's visualize the images with <strong class="source-inline">matplotlib</strong>:<p class="source-code"> # for j in range(5*5):</p><p class="source-code">    pyplot.subplot(5,5,j+1)</p><p class="source-code">    # turn off axis </p><p class="source-code">    pyplot.axis('off') </p><p class="source-code">    pyplot.imshow(fashionData[j,:,:,0],cmap='gray_r')</p><p>You should get an output similar to the one shown here:</p><div id="_idContainer270" class="IMG---Figure"><img src="image/B15385_07_37.jpg" alt="Figure 7.37: Plotted images&#13;&#10;"/></div><p class="figure-caption">Figure 7.37: Plotted images</p><p>From the output, we can see the visualization of several fashion articles. We can see that the images are centrally located within a white background. This are the images that we'll try to recreate. </p></li>
				<li>Now, let's define the function to generate inputs for the generator network. The inputs are random data points that are generated from a random uniform distribution:<p class="source-code"># Function to generate inputs for generator function</p><p class="source-code">def fakeInputs(batch,infeats):</p><p class="source-code">    # Generate random noise data with shape (batch,input features)</p><p class="source-code">    x_fake = np.random.uniform(-1,1,size=[batch,infeats])</p><p class="source-code">    return x_fake</p><p>This function generates the fake data that was sampled from the random distribution as the output.</p></li>
				<li>Let's define the function for building the generator network:<p class="source-code-heading">Activity7.01.ipynb</p><p class="source-code"># <a id="_idTextAnchor287"/>Function for the generator model</p><p class="source-code">def genModel(infeats):</p><p class="source-code">    # Defining the Generator model</p><p class="source-code">    Genmodel = Sequential()</p><p class="source-code">    Genmodel.add(Dense(512,input_dim=infeats))</p><p class="source-code">    Genmodel.add(Activation('relu'))</p><p class="source-code">    Genmodel.add(BatchNormalization())</p><p class="source-code">    # second layer of FC =&gt; RElu =&gt; BN layers</p><p class="source-code">    Genmodel.add(Dense(7*7*64))</p><p class="source-code">    Genmodel.add(Activation('relu'))</p><p class="source-code">    Genmodel.add(BatchNormalization())</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3fpobDm">https://packt.live/3fpobDm</a></p><p>Building the generator network is similar to building any CNN network. In this generator network, we will use the transpose convolution method for upsampling images. In this model, we can see the progressive use of the transpose convolution. The initial input starts with a dimension of 100, which is our input feature. The dimension of the MNIST dataset is batch size x 28 x 28. Therefore, we have upsampled the data twice to get the output as batch size x 28 x 28.</p></li>
				<li>Next, we define the function that will be used to create fake samples: <p class="source-code"># Function to create fake samples using the generator model</p><p class="source-code">def fakedataGenerator(Genmodel,batch,infeats):</p><p class="source-code">    # first generate the inputs to the model</p><p class="source-code">    genInputs = fakeInputs(batch,infeats)</p><p class="source-code">    """</p><p class="source-code">    use these inputs inside the generator model \</p><p class="source-code">    to generate fake distribution</p><p class="source-code">    """</p><p class="source-code">    X_fake = Genmodel.predict(genInputs)</p><p class="source-code">    return X_fake</p><p>In this function, we only return the <strong class="source-inline">X</strong> variable. The output from this function is the fake dataset.</p></li>
				<li>Define the parameters that we will use in many of the functions, along with the summary of the generator network:<p class="source-code"># Define the arguments like batch size and input feature</p><p class="source-code">batch = 128</p><p class="source-code">infeats = 100</p><p class="source-code">Genmodel = genModel(infeats,)</p><p class="source-code">Genmodel.summary()</p><p>You should get the following output:</p><p> </p><div id="_idContainer271" class="IMG---Figure"><img src="image/B15385_07_38.jpg" alt="Figure 7.38: Summary of the generative model&#13;&#10;"/></div><p class="figure-caption">Figure 7.38: Summary of the generative model</p><p>From the summary, please note how the dimension of the input noise changes with each transpose convolution operation. Finally, we get an output that is equal in dimension to the real dataset, <strong class="source-inline">( None,28 ,28,1)</strong>.</p></li>
				<li>Let's use the generator function to generate a fake sample before training:<p class="source-code"># Generating a fake sample and printing the shape</p><p class="source-code">fake = fakedataGenerator(Genmodel,batch,infeats)</p><p class="source-code">fake.shape</p><p>You should get the following output:</p><p class="source-code">(128, 28, 28, 1)</p></li>
				<li>Now, let's plot the generated fake sample:<p class="source-code"># Plotting the fake sample</p><p class="source-code">plt.imshow(fake[1, :, :, 0], cmap='gray_r')</p><p>You should get an output similar to the following:</p><div id="_idContainer272" class="IMG---Figure"><img src="image/B15385_07_39.jpg" alt="Figure 7.39: Output of the fake sample&#13;&#10;"/></div><p class="figure-caption">Figure 7.39: Output of the fake sample</p><p>This is the plot of the fake sample before training. After training, we want samples like these to look like the MNIST fashion samples we visualized earlier in this activity.</p></li>
				<li>Build the discriminator model as a function. The network architecture will be similar to a CNN architecture:<p class="source-code-heading">Activity7.01.ipynb</p><p class="source-code"># Descriminator model as a function</p><p class="source-code">def discModel():</p><p class="source-code">    Discmodel = Sequential()</p><p class="source-code">    Discmodel.add(Conv2D(32,kernel_size=(5,5),strides=(2,2),\</p><p class="source-code">                  padding='same',input_shape=(28,28,1)))</p><p class="source-code">    Discmodel.add(LeakyReLU(0.2))</p><p class="source-code">    # second layer of convolutions</p><p class="source-code">    Discmodel.add(Conv2D(64, kernel_size=(5,5), strides=(2, 2), \</p><p class="source-code">                  padding='same'))</p><p class="source-code">    Discmodel.add(LeakyReLU(0.2))</p><p class="source-code-link">The full code for this step can be found at <a href="https://packt.live/3fpobDm">https://packt.live/3fpobDm</a></p><p>In the discriminator network, we have included all the necessary layers, such as the convolutional operations and <strong class="source-inline">LeakyReLU</strong>. Please note that the last layer is a sigmoid layer as we want the output as a probability of whether the sample is real (1) or fake (0).</p></li>
				<li>Print the summary of the discriminator network:<p class="source-code"># Print the summary of the discriminator model</p><p class="source-code">Discmodel = discModel()</p><p class="source-code">Discmodel.summary()</p><p>You should get the following output:</p><div id="_idContainer273" class="IMG---Figure"><img src="image/B15385_07_40.jpg" alt="Figure 7.40: Discriminator model summary&#13;&#10;"/></div><p class="figure-caption">Figure 7.40: Discriminator model summary</p></li>
				<li>Define the GAN model as a function:<p class="source-code"># Define the combined generator and discriminator model, for updating the generator</p><p class="source-code">def ganModel(Genmodel,Discmodel):</p><p class="source-code">    # First define that discriminator model cannot be trained</p><p class="source-code">    Discmodel.trainable = False</p><p class="source-code">    Ganmodel = Sequential()</p><p class="source-code">    # First adding the generator model</p><p class="source-code">    Ganmodel.add(Genmodel)</p><p class="source-code">    """</p><p class="source-code">    Next adding the discriminator model </p><p class="source-code">    without training the parameters</p><p class="source-code">    """</p><p class="source-code">    Ganmodel.add(Discmodel)</p><p class="source-code">    """</p><p class="source-code">    Compile the model for loss to optimise the Generator model</p><p class="source-code">    """</p><p class="source-code">    Ganmodel.compile(loss='binary_crossentropy',\</p><p class="source-code">                     optimizer = 'adam')</p><p class="source-code">    return Ganmodel</p><p>The structure of the GAN model is similar to the one we developed in <em class="italic">Exercise 7.05</em>, <em class="italic">Implementing the DCGAN</em>.</p></li>
				<li>Now, it's time to invoke the GAN function:<p class="source-code"># Initialise the GAN model</p><p class="source-code">gan_model = ganModel(Genmodel,Discmodel)</p><p class="source-code"># Print summary of the GAN model</p><p class="source-code">gan_model.summary()</p><p>Please note that the inputs to the GAN model are the previously defined generator model and the discriminator model. You should get the following output:</p><div id="_idContainer274" class="IMG---Figure"><img src="image/B15385_07_41.jpg" alt="Figure 7.41: GAN model summary&#13;&#10;"/></div><p class="figure-caption">Figure 7.41: GAN model summary</p><p>Please note that the parameters of each layer of the GAN model are equivalent to the parameters of the generator and discriminator models. The GAN model is just a wrapper around the two models we defined earlier.</p></li>
				<li>Define the number of epochs to train the network on using the following code:<p class="source-code"># Defining the number of epochs</p><p class="source-code">nEpochs = 5000</p></li>
				<li>Now, we can start the process of training the network:<p class="callout-heading">Note:</p><p class="callout">Before you run the following code, make sure you have created a folder called <strong class="source-inline">output</strong> in the same path as your Jupyter Notebook.</p><p class="source-code-heading">Activity7.01.ipynb</p><p class="source-code"># Train the GAN network</p><p class="source-code">for i in range(nEpochs):</p><p class="source-code">    """</p><p class="source-code">    Generate samples equal to the batch size </p><p class="source-code">    from the real distribution</p><p class="source-code">    """</p><p class="source-code">    x_real = realData(batch)</p><p class="source-code">    #Generate fake samples using the fake data generator function</p><p class="source-code">    x_fake = fakedataGenerator(Genmodel,batch,infeats)</p><p class="source-code">    # Concatenating the real and fake data </p><p class="source-code">    X = np.concatenate([x_real,x_fake])</p><p class="source-code">    #Creating the dependent variable and initializing them as '0'</p><p class="source-code">    Y = np.zeros(batch * 2)</p><p class="source-code-link">The complete code for this step can be found on <a href="https://packt.live/3fpobDm">https://packt.live/3fpobDm</a></p><p>It needs to be noted here that the training of the discriminator model with the fake and real samples and the training of the GAN model happens concurrently. The only difference is the training of the GAN model proceeds without updating the parameters of the discriminator model. The other thing to note is that, inside the GAN, the labels for the fake samples would be 1 to generate large loss terms that will be backpropagated through the discriminator network to update the generator parameters. We also display the predicted probability of the GAN for every 50 epochs. When calculating the probability, we combine a sample of real data and a sample of fake data and then take the mean of the predicted probability. We also save a copy of the generated image.</p><p>You should get an output similar to the following:</p><p class="source-code">Discriminator probability:0.5276428461074829</p><p class="source-code">Discriminator probability:0.5038391351699829</p><p class="source-code">Discriminator probability:0.47621315717697144</p><p class="source-code">Discriminator probability:0.48467564582824707</p><p class="source-code">Discriminator probability:0.5270703434944153</p><p class="source-code">Discriminator probability:0.5247280597686768</p><p class="source-code">Discriminator probability:0.5282968282699585</p><p>Let's also look at some of the plots that were generated from the training process at various epochs:</p><div id="_idContainer275" class="IMG---Figure"><img src="image/B15385_07_42.jpg" alt="Figure 7.42: Images generated during the training process&#13;&#10;"/></div><p class="figure-caption">Figure 7.42: Images generated during the training process</p><p>From the preceding plots, we can see the progression of the training process. We can see that by epoch 100, the plots were mostly noise. By epoch 600, the forms of the fashion articles started to become more pronounced. At epoch 1,500, we can see that the fake images are looking very similar to the fashion dataset.</p><p class="callout-heading">Note:</p><p class="callout">You can take a closer look at these images by going to <a href="https://packt.live/2W1FjaI">https://packt.live/2W1FjaI</a>.</p></li>
				<li>Now, let's look at the images that were generated after training:<p class="source-code"> # Images generated after training</p><p class="source-code">x_fake = fakedataGenerator(Genmodel,25,infeats)</p><p class="source-code"># Displaying the plots</p><p class="source-code">for j in range(5*5):</p><p class="source-code">pyplot.subplot(5,5,j+1)</p><p class="source-code">    # turn off axis </p><p class="source-code">    pyplot.axis('off')</p><p class="source-code">    pyplot.imshow(x_fake[j,:,:,0],cmap='gray_r')</p><p>You should get an output similar to the following:</p><p> </p><div id="_idContainer276" class="IMG---Figure"><img src="image/B15385_07_43.jpg" alt="Figure 7.43: Images generated after the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.43: Images generated after the training process</p>
			<p>From the training accuracy levels, you can see that the accuracy of the discriminator model hovers around the .50 range, which is the desired range. The purpose of the generator is to create fake images that look like real ones. When the generator generates images that look very similar to real images, the discriminator gets confused as to whether the image has been generated from the real distribution or fake distribution. This phenomenon manifests in an accuracy level of around 50% for the discriminator, which is the desired level.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fpobDm">https://packt.live/3fpobDm</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
		</div>
	</body></html>