<html><head></head><body>
		<div id="_idContainer1786">
			<h1 id="_idParaDest-331"><em class="italic"><a id="_idTextAnchor365"/>Chapter 17</em>: Smart City and Cybersecurity</h1>
			<p>Smart cities are expected to be one of the defining experiences of the next decades. A smart city collects a lot of data using sensors located in various parts of the city, such as on the roads, utility infrastructures, and water resources. The data is then used to make data-driven and automated decisions, such as how to allocate the city's resources, manage traffic in real time, and identify and mitigate infrastructure problems. This prospect comes with two challenges: how to program the automation and how to protect the highly connected city assets from cyberattacks. Fortunately, <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) can help with both.</p>
			<p>In this chapter, we will cover three problems related to smart cities and cybersecurity and describe how to model them as RL problems. Along the way, we will introduce you to the Flow library, a framework that connects traffic simulation software with RL libraries, and use it to solve an example traffic light control problem.</p>
			<p>In particular, here are the problems we will address in this chapter:</p>
			<ul>
				<li>Traffic light control to optimize vehicle flow</li>
				<li>Providing an ancillary service to a power grid</li>
				<li>Detecting cyberattacks in a smart grid</li>
			</ul>
			<p>This will be a fun ride, so let's get started! </p>
			<h1 id="_idParaDest-332"><a id="_idTextAnchor366"/>Traffic light control to optimize vehicle flow</h1>
			<p>One of the key <a id="_idIndexMarker1383"/>challenges of a smart city is optimizing traffic flows on road networks. There are numerous benefits in reducing traffic congestions, such as the following:</p>
			<ul>
				<li>Reducing the time and energy wasted in traffic</li>
				<li>Saving on gas and resulting exhaust emissions</li>
				<li>Increasing vehicle and road lifetime</li>
				<li>Decreasing the number of accidents</li>
			</ul>
			<p>There has already been a lot of research gone into this area; but recently, RL has emerged as a competitive alternative to traditional control approaches. So, in this section, we will optimize the traffic flow on a road network by controlling the traffic light behavior<a id="_idIndexMarker1384"/> using multi-agent RL. To this end, we will use the Flow framework, which is an open source library for RL, and run experiments on realistic traffic microsimulations. </p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor367"/>Introducing Flow</h2>
			<p>Transportation <a id="_idIndexMarker1385"/>research significantly relies on simulation software, such as <strong class="bold">SUMO</strong> (<strong class="bold">Simulation</strong> <strong class="bold">of</strong> <strong class="bold">Urban</strong> <strong class="bold">Mobility</strong>) and Aimsun, for areas such as traffic light control, vehicle route <a id="_idIndexMarker1386"/>choice, traffic surveillance, and traffic forecast, which involves the optimal control of these agents. On the other side, the rise of deep RL as an alternative to traditional control approaches has led to the creation of numerous libraries, such as RLlib and OpenAI Baselines. Flow is an open source framework that connects these two worlds of traffic simulators and RL libraries.</p>
			<p>In this section, as in previous chapters, we will use RLlib as the RL backend. For traffic simulation, we will use SUMO, a powerful open source library that has been developed since the early 2000s.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Here, we will give only a glimpse into applying RL to traffic problems. Detailed documentation and tutorials (which we closely follow here) are available on the Flow website: <a href="https://flow-project.github.io/">https://flow-project.github.io/</a>. SUMO documentation and libraries are available at <a href="https://www.eclipse.org/sumo/">https://www.eclipse.org/sumo/</a>.</p>
			<p>Let's start by installing Flow and SUMO.</p>
			<h3>Installing Flow and SUMO</h3>
			<p>In order to install <a id="_idIndexMarker1387"/>Flow, we<a id="_idIndexMarker1388"/> need to create a new virtual environment since it depends on library versions different than what we used in earlier chapters:</p>
			<p class="source-code">$ sudo apt-get install python3-pip </p>
			<p class="source-code">$ virtualenv flowenv</p>
			<p class="source-code">$ source flowenv/bin/activate</p>
			<p>To install Flow, we need to download the repo and run the following commands:</p>
			<p class="source-code">$ git clone https://github.com/flow-project/flow.git</p>
			<p class="source-code">$ cd flow</p>
			<p class="source-code">$ pip3 install -e .</p>
			<p class="source-code">$ pip3 install ipykernel</p>
			<p class="source-code">$ python -m ipykernel install --user --name=flowenv</p>
			<p>These commands install the necessary dependencies, including TensorFlow and RLlib. The last two commands are needed to run Flow on Jupyter Notebook, which is what the Flow tutorials, as well as our example code, are on. To install SUMO on Ubuntu 18.04, use the following:</p>
			<p class="source-code">$ scripts/setup_sumo_ubuntu1804.sh</p>
			<p class="source-code">$ source ~/.bashrc</p>
			<p>Setup scripts for earlier Ubuntu versions and macOS are also available in the same folder.</p>
			<p>You can see Flow and SUMO in action by running the following (in the Flow folder and with your virtual environment activated):</p>
			<p class="source-code">$ python examples/simulate.py ring</p>
			<p>A window similar to the one shown in <em class="italic">Figure 17.1</em> should appea<a id="_idTextAnchor368"/>r:</p>
			<div>
				<div id="_idContainer1718" class="IMG---Figure">
					<img src="image/B14160_17_01.jpg" alt="Figure 17.1 – A sample SUMO window simulating the traffic on a ring road&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.1 – A sample SUMO window simulating the traffic on a ring road</p>
			<p>If you run into<a id="_idIndexMarker1389"/> issues with the setup, the Flow documentation can help you with<a id="_idIndexMarker1390"/> troubleshooting.</p>
			<p>Now that we have set things up, let's dive into how to put together a traffic environment using Flow. </p>
			<h2 id="_idParaDest-334"><a id="_idTextAnchor369"/>Creating an experiment in Flow</h2>
			<p>Now that we have the<a id="_idIndexMarker1391"/> setup, we can create environments and experiments in Flow. We will then connect them to RLlib to train RL agents.</p>
			<p>There are certain ingredients that go into a Flow experiment:</p>
			<ul>
				<li><strong class="bold">A road network</strong>, such <a id="_idIndexMarker1392"/>as a ring road (as in <em class="italic">Figure 17.1</em>), or a Manhattan-like grid network. Flow comes with a set of predefined networks. For advanced users, it also allows creating custom networks. Traffic lights are defined together with the road network.</li>
				<li><strong class="bold">A simulation backend</strong>, which is not our focus here. We will use the defaults for this.</li>
				<li><strong class="bold">An RL environment</strong> that configures what is controlled, observed, and rewarded in the experiment, similar to a Gym environment.</li>
				<li><strong class="bold">Vehicles</strong> are essential<a id="_idIndexMarker1393"/> to the entire experiment, and their behavior and characteristics are defined separately.</li>
			</ul>
			<p>All of these components are parametrized and are passed to Flow separately. We then pack them all to create a Flow parameters object.</p>
			<p>It could be difficult to use a bottom-up approach here and start with individual parameters for each component to compose the big picture. In addition, these details are out of the scope of this chapter. Instead, it is much easier to unpack a prebuilt Flow parameters object. Let's do that next.</p>
			<h3>Analyzing a Flow parameters object</h3>
			<p>Flow has some <a id="_idIndexMarker1394"/>benchmark experiments defined for traffic light optimization on a grid network. Take a look at the Flow parameters object for the Grid-0 experiment:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter17/Traffic Lights on a Grid Network.ipynb</p>
			<p class="source-code">from flow.benchmarks.grid0 import flow_params</p>
			<p class="source-code">flow_params</p>
			<p class="source-code"><strong class="bold">{'exp_tag': 'grid_0',</strong></p>
			<p class="source-code"><strong class="bold"> 'env_name': flow.envs.traffic_light_grid.TrafficLightGridBenchmarkEnv,</strong></p>
			<p class="source-code"><strong class="bold"> 'network': flow.networks.traffic_light_grid.TrafficLightGridNetwork,</strong></p>
			<p class="source-code"><strong class="bold"> 'simulator': 'traci',</strong></p>
			<p class="source-code"><strong class="bold"> 'sim': &lt;flow.core.params.SumoParams at 0x7f25102d1350&gt;,</strong></p>
			<p class="source-code"><strong class="bold"> 'env': &lt;flow.core.params.EnvParams at 0x7f25102d1390&gt;,</strong></p>
			<p class="source-code"><strong class="bold"> 'net': &lt;flow.core.params.NetParams at 0x7f25102d13d0&gt;,</strong></p>
			<p class="source-code"><strong class="bold"> 'veh': &lt;flow.core.params.VehicleParams at 0x7f267c1c9650&gt;,</strong></p>
			<p class="source-code"><strong class="bold"> 'initial': &lt;flow.core.params.InitialConfig at 0x7f25102d6810&gt;}</strong></p>
			<p>We can, for example, inspect what is inside the network parameters:</p>
			<p class="source-code">print(dir(flow_params['net']))</p>
			<p class="source-code">flow_params['net'].additional_params</p>
			<p class="source-code"><strong class="bold">...</strong></p>
			<p class="source-code"><strong class="bold">{'speed_limit': 35,</strong></p>
			<p class="source-code"><strong class="bold"> 'grid_array': {'short_length': 300,</strong></p>
			<p class="source-code"><strong class="bold">  'inner_length': 300,</strong></p>
			<p class="source-code"><strong class="bold">  'long_length': 100,</strong></p>
			<p class="source-code"><strong class="bold">  'row_num': 3,</strong></p>
			<p class="source-code"><strong class="bold">...</strong></p>
			<p>And, of course, a <a id="_idIndexMarker1395"/>good way to understand what they do is to visually run the experiment:</p>
			<p class="source-code">from flow.core.experiment import Experiment</p>
			<p class="source-code">sim_params = flow_params['sim']</p>
			<p class="source-code">sim_params.render = True</p>
			<p class="source-code">exp = Experiment(flow_params)</p>
			<p class="source-code">results = exp.run(1)</p>
			<p>This will pop up a SUMO screen similar to the one shown in <em class="italic">Figu<a id="_idTextAnchor370"/>re 17.2</em>:</p>
			<div>
				<div id="_idContainer1719" class="IMG---Figure">
					<img src="image/B14160_17_02.jpg" alt="Figure 17.2 – SUMO rendering of the grid network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.2 – SUMO rendering of the grid network</p>
			<p>With that, now we have<a id="_idIndexMarker1396"/> an example up and running. Before we go into RL modeling and training, let's discuss how to get a baseline reward for this experiment.</p>
			<h3>Getting a baseline reward</h3>
			<p>The Jupyter notebook<a id="_idIndexMarker1397"/> in our GitHub repo includes a code snippet taken from the Flow code base to get a baseline reward on this environment. It has some carefully optimized traffic light phase definitions that lead to a -204 reward on average. We will use this reward to benchmark the RL result. Also, feel free to modify the phases to see their impact on the traffic pattern on the network.</p>
			<p>With that, we are now ready to define the RL environment.</p>
			<h2 id="_idParaDest-335"><a id="_idTextAnchor371"/>Modeling the traffic light control problem</h2>
			<p>As always, we need<a id="_idIndexMarker1398"/> to define the action, observation, and reward for the RL problem. We will do so in the following sections.</p>
			<h3>Defining the action</h3>
			<p>We would like to train a <a id="_idIndexMarker1399"/>single controller for all of the lights at a given intersection that is illustrated in <em class="italic">Figure 17.2</em>, in example <em class="italic">b</em>. In the figure, the lights are in a green-red-green-red state. We define a binary action that tells us 0: continue and 1: switch. When instructed to switch, the state of the lights on the figure would become yellow-red-yellow-red, and then red-green-red-green after a few seconds.</p>
			<p>The default environment accepts a continuous action for each intersection, <img src="image/Formula_17_001.png" alt=""/>, and rounds it up to discretize as we described previously. </p>
			<h3>Single versus multi-agent modeling</h3>
			<p>The next design<a id="_idIndexMarker1400"/> decision we have to make is whether to control all of the traffic lights using a centralized agent or adapt a multi-agent approach. If we pick the latter, whether we train a single policy for all intersections or train multiple policies, we need to note the following: </p>
			<ul>
				<li>The advantage of the centralized approach is that, in theory, we can perfectly coordinate all of the intersections and achieve a better reward. On the other hand, a trained agent may not be easily applied to a different road network. In addition, for larger networks, the centralized approach won't scale easily.</li>
				<li>If we decide to use a multi-agent approach, we don't have a lot of reasons to differentiate between the intersections and the policies they use. So, training a single policy makes more sense.</li>
				<li>Training a single generic policy for all intersections where the agents (the lights at the intersections) collaboratively try to maximize the reward is a scalable and efficient approach. Of course, this lacks the full coordination capabilities of a centralized, single-agent approach. In practice, this would be a trade-off you would have to evaluate.</li>
			</ul>
			<p>So, we will go with the multi-agent setting, in which the policy will be trained with the data coming from all of the agents. The agents will retrieve actions from the policy according <a id="_idIndexMarker1401"/>to their local observations.</p>
			<p>With that, let's define the observation.</p>
			<h3>Defining the observation</h3>
			<p>The default <a id="_idIndexMarker1402"/>multi-agent grid environment uses the following as the observations:</p>
			<ul>
				<li>Speeds of the <img src="image/Formula_17_002.png" alt=""/> closest vehicles heading to the intersection</li>
				<li>Distances of the <img src="image/Formula_17_003.png" alt=""/> closest vehicles heading to the intersection</li>
				<li>The IDs of the road edges that these ­<img src="image/Formula_17_004.png" alt=""/> vehicles are on</li>
				<li>The traffic density, average velocity, and traffic direction on each of the local edges</li>
				<li>Whether the lights are currently in a yellow state</li>
			</ul>
			<p>For more detailed information, you can check the <strong class="source-inline">flow.envs.multiagent.traffic_light_grid</strong> module in the Flow repo. </p>
			<p>Finally, let's define the reward.</p>
			<h3>Defining the reward</h3>
			<p>The environment<a id="_idIndexMarker1403"/> has a simple and intuitive cost definition for a given time step, which measures the average vehicle delay compared to the top speed allowed:</p>
			<div>
				<div id="_idContainer1724" class="IMG---Figure">
					<img src="image/Formula_17_005.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_17_006.png" alt=""/> is the velocity of the <img src="image/Formula_17_007.png" alt=""/> of <img src="image/Formula_17_008.png" alt=""/> total vehicles, and <img src="image/Formula_17_009.png" alt=""/> is the maximum allowed speed. The reward can then be defined as the negative of this cost term. </p>
			<p>Now that we have<a id="_idIndexMarker1404"/> all the formulations in place, it is time to solve the problem.</p>
			<h2 id="_idParaDest-336"><a id="_idTextAnchor372"/>Solving the traffic control problem using RLlib</h2>
			<p>Since we will use the multi-agent interface of RLlib, we need to <a id="_idIndexMarker1405"/>do the following:</p>
			<ol>
				<li>Register the environment in RLlib with a name and environment creation function.</li>
				<li>Define the names of the policies we will train, which we have only one of, <strong class="source-inline">tlight</strong>.</li>
				<li>Define a function that generates the arguments needed for the RLlib trainer for the policy.</li>
				<li>Define a function that maps agents to the policies, which again is simple to do in our case since all the agents map to the same policy.</li>
			</ol>
			<p>So, these can be achieved with the following code:</p>
			<p class="source-code">create_env, env_name = make_create_env(params=flow_params, </p>
			<p class="source-code">                                       version=0)</p>
			<p class="source-code">register_env(env_name, create_env)</p>
			<p class="source-code">test_env = create_env()</p>
			<p class="source-code">obs_space = test_env.observation_space</p>
			<p class="source-code">act_space = test_env.action_space</p>
			<p class="source-code">def gen_policy():</p>
			<p class="source-code">    return PPOTFPolicy, obs_space, act_space, {}</p>
			<p class="source-code">def policy_mapping_fn(_):</p>
			<p class="source-code">    return 'tlight'</p>
			<p class="source-code">policy_graphs = {'tlight': gen_policy()}</p>
			<p class="source-code">policies_to_train = ['tlight']</p>
			<p>Once defined, we need to pass these functions <a id="_idIndexMarker1406"/>and lists to the RLlib config:</p>
			<p class="source-code">config['multiagent'].update({'policies': policy_graphs})</p>
			<p class="source-code">config['multiagent'].update({'policy_mapping_fn': </p>
			<p class="source-code">                             policy_mapping_fn})</p>
			<p class="source-code">config['multiagent'].update({'policies_to_train': </p>
			<p class="source-code">                             policies_to_train})</p>
			<p>The rest is the regular RLlib training loop. We use the hyperparameters identified in the Flow benchmarks with PPO. The full code for all of this is available in <strong class="source-inline">Chapter17/Traffic Lights on a Grid Network.ipynb</strong>.</p>
			<h3>Obtaining and observing the results</h3>
			<p>After a couple <a id="_idIndexMarker1407"/>million<a id="_idIndexMarker1408"/> training steps, the reward converges around -243, which is a bit lower than the handcrafted benchmark. The training progress can be observed on TensorBoard:</p>
			<div>
				<div id="_idContainer1729" class="IMG---Figure">
					<img src="image/B14160_17_03.jpg" alt="Figure 17.3 – Training progress of the multi-agent traffic light environment in Flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.3 – Training progress of the multi-agent traffic light environment in Flow</p>
			<p>You can also<a id="_idIndexMarker1409"/> visualize how<a id="_idIndexMarker1410"/> the trained agent is doing with a command on Jupyter Notebook in the following format:</p>
			<p class="source-code">!python /path/to/your/flow/visualize/visualizer_rllib.py /path/to/your/ray_results/your_exp_tag/your_run_id 100</p>
			<p>Here, the argument at the end refers to the checkpoint number, which is generated regularly during training.</p>
			<p>Next, let's also discuss why the RL performance is falling a bit short of the handcrafted policy.</p>
			<h3>Further improvements</h3>
			<p>There are several <a id="_idIndexMarker1411"/>reasons why RL may not have reached the baseline performance:</p>
			<ul>
				<li>Lack of more hyperparameter tuning and training. This factor is always there. There is no way to know whether the performance can be improved with more fiddling with the model architecture and training until you try, which we encourage you to do.</li>
				<li>The baseline policy is exercising a finer control over the yellow light durations, whereas the RL model did not have control over that.</li>
				<li>The baseline policy coordinates all the intersections on the network, whereas each RL agent makes local decisions. So, we might be running into the shortcomings of decentralized control here. </li>
				<li>This can be mitigated by adding observations that will help coordinate the agents with their neighbors.</li>
				<li>It is not uncommon for RL algorithms to struggle with crossing the last mile in the optimization and reach the very peak of the reward curve. This may require fine control over the training procedure by reducing the learning rates and adjusting the batch sizes.</li>
			</ul>
			<p>So, although there is room for improvement, our<a id="_idIndexMarker1412"/> agents have successfully learned how to control traffic lights, which is much more scalable than manually crafting policies.</p>
			<p>Before we wrap up this topic, let's discuss a few more resources to learn more about the problem and the libraries we used.</p>
			<h2 id="_idParaDest-337"><a id="_idTextAnchor373"/>Further reading</h2>
			<p>We have already provided links to the Flow and SUMO documentation. The Flow library and the benchmarks obtained with it are explained in <em class="italic">Wu et al., 2019</em>, and <em class="italic">Vinitsky et al</em>., <em class="italic">2018</em>. In these resources, you will discover additional problems that you can model and solve using various RL libraries.</p>
			<p>Congratulations! We have done a lot in such a short time and space to leverage RL for traffic control problems. Next, we will cover another interesting problem, which is to modulate the electricity demand to stabilize a po<a id="_idTextAnchor374"/>wer grid.</p>
			<h1 id="_idParaDest-338"><a id="_idTextAnchor375"/>Providing an ancillary service to a power grid</h1>
			<p>In this section, we <a id="_idIndexMarker1413"/>will describe how RL can help with <a id="_idIndexMarker1414"/>integrating clean energy resources into a power grid by managing smart appliances in home and office buildings. </p>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor376"/>Power grid operations and ancillary services</h2>
			<p>The transmission<a id="_idIndexMarker1415"/> and distribution of electrical power from generators to<a id="_idIndexMarker1416"/> consumers is a massive operation that requires continuous monitoring and control of the system. In particular, the generation and consumption should be nearly equal in a region to keep the electric current at the standard frequency (60 Hz in the United States) to prevent blackouts and damages. This is a challenging undertaking for various reasons:</p>
			<ul>
				<li>The power supply is planned ahead in energy markets with the generators in the region to match the demand.</li>
				<li>Despite this planning, the future power supply is uncertain, especially when obtained from renewable resources. The amount of wind and solar energy may be less or more than expected, causing under or oversupply. </li>
				<li>Future demand is uncertain too, as consumers are mostly free to decide when and how much to consume.</li>
				<li>Failures in the grid, such as at generators or transmission lines, can cause sudden changes in the supply or demand, putting the reliability of the system at risk.</li>
			</ul>
			<p>The balance between the supply and demand is maintained by authorities called <strong class="bold">Independent System Operators</strong> (<strong class="bold">ISOs</strong>). Traditionally, ISOs ask generators to ramp up or down their<a id="_idIndexMarker1417"/> supply based on the changes in the grid, which is an ancillary service provided by generators to ISOs for a price. On the other hand, there are several issues regarding generators providing this service:</p>
			<ul>
				<li>Generators are usually slow to respond to sudden changes in the grid balance. For example, it may take hours to bring in a new generation unit to address a supply deficit in the grid.</li>
				<li>In recent years, there has been a significant increase in renewable energy supply, adding to the volatility in the grid.</li>
			</ul>
			<p>For these reasons, a line of research has been initiated to enable consumers to provide these ancillary services to the grid. In other words, the goal is to modulate the demand in addition to the supply to better maintain the balance. This requires more sophisticated control<a id="_idIndexMarker1418"/> mechanisms, which<a id="_idIndexMarker1419"/> is what we bring in RL to help with.</p>
			<p>After this introduction, let's now more concretely define the control problem here.  </p>
			<h2 id="_idParaDest-340"><a id="_idTextAnchor377"/>Describing the environment and the decision-making problem</h2>
			<p>To reiterate, our goal is to dynamically increase or decrease the total electricity consumption in an area. Let's first describe the parties involved in this setting and their roles.</p>
			<h3>Independent system operator</h3>
			<p>The ISO of the <a id="_idIndexMarker1420"/>region continuously monitors the supply and demand balance and broadcasts an automated signal to all ancillary service providers in the region to adjust their demand. Let's call this signal <img src="image/Formula_17_010.png" alt=""/>, which is simply a number in the range <img src="image/Formula_17_011.png" alt=""/>. We will come back to what this number precisely means in a moment. For now, let's state that the ISO updates this signal every 4 seconds (which is a particular type of ancillary service called a regulation service). </p>
			<h3>Smart building operator</h3>
			<p>We assume<a id="_idIndexMarker1421"/> that there is a <strong class="bold">smart building operator</strong> (<strong class="bold">SBO</strong>) that is in charge of modulating the total demand in a (collection of) building(s) to follow the ISO signal. The SBO, which will be our RL agent, operates as follows:</p>
			<ul>
				<li>The SBO sells the regulation service to the ISO of the region. According to this obligation, the SBO promises to maintain the consumption at a rate of <img src="image/Formula_17_012.png" alt=""/> kW and adjust it up or down up to <img src="image/Formula_17_013.png" alt=""/> kW at the ISO's request. We assume that <img src="image/Formula_17_014.png" alt=""/> and <img src="image/Formula_17_015.png" alt=""/> are predetermined for our problem.</li>
				<li>When <img src="image/Formula_17_016.png" alt=""/>, the SBO needs to quickly decrease the consumption in the neighborhood to <img src="image/Formula_17_017.png" alt=""/> kW. When <img src="image/Formula_17_018.png" alt=""/>, the consumption rate needs to go up to <img src="image/Formula_17_019.png" alt=""/> kW.</li>
				<li>In general, the SBO needs to control the consumption to follow an <img src="image/Formula_17_020.png" alt=""/> kW rate.</li>
			</ul>
			<p>The SBO controls<a id="_idIndexMarker1422"/> a population of smart appliances/units, such<a id="_idIndexMarker1423"/> as <strong class="bold">heating, ventilation, and air conditioning</strong> (<strong class="bold">HVAC</strong>) units and <strong class="bold">electric</strong> <strong class="bold">vehicles</strong> (<strong class="bold">EVs</strong>), to abide by the<a id="_idIndexMarker1424"/> signal. This is where we will leverage RL.</p>
			<p>We illustrate this setup in <em class="italic">Figure 17.4</em>:</p>
			<div>
				<div id="_idContainer1741" class="IMG---Figure">
					<img src="image/B14160_17_04.jpg" alt="Figure 17.4 – Regulation service provision by a smart building operator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.4 – Regu<a id="_idTextAnchor378"/>lation service provision by a smart building operator</p>
			<p>Next, let's go a bit more into the details of how smart appliances operate.</p>
			<h3>Smart appliances</h3>
			<p>You may feel uncomfortable <a id="_idIndexMarker1425"/>with the idea that some algorithm is interfering with your appliances and causing them to turn on or off. After all, who would want the TV to shut down to save power while watching the Super Bowl, or turn it on in the middle of the night just because there is excess electricity generation due to higher-than-anticipated winds outside? This certainly does not make sense. On the other hand, you would be more okay if the AC turned on a minute late or earlier than normal. Or you would not mind whether your EV reached full battery at 4 a.m. or 5 a.m. in the morning as long as it is ready for you before you leave home. So, the point is that some appliances have more room for flexibility in terms of when to operate, which is of interest to us in this case. </p>
			<p>We also assume that <a id="_idIndexMarker1426"/>these appliances are smart and have the following capabilities:</p>
			<ul>
				<li>They can communicate with the SBO to receive the actions.</li>
				<li>They can assess the "utility," which is a measure of how much need there is for the appliance to consume power at a given moment.</li>
			</ul>
			<h4>Defining the utility</h4>
			<p>Let's give two examples <a id="_idIndexMarker1427"/>of how the utility changes in different situations. Consider an EV that needs to be fully charged by 7 a.m.:</p>
			<ul>
				<li>The utility would be high if it is 6 a.m. and the battery is still low. </li>
				<li>Conversely, the utility would be low if there is still plenty of time until departure and/or the battery is close to full. </li>
			</ul>
			<p>Similarly, an air conditioner would have high utility when the room temperature is about to exceed the user's comfort zone and low utility when it is close to the bottom. </p>
			<p>See <em class="italic">Figure 17.5</em> for an illustration of these situations:</p>
			<div>
				<div id="_idContainer1742" class="IMG---Figure">
					<img src="image/B14160_17_05.jpg" alt="Figure 17.5 – Utility levels under different conditions for an (a) EV and (b) AC&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.5 – Ut<a id="_idTextAnchor379"/>ility levels under different conditions for an (a) EV and (b) AC</p>
			<p>Next, let's discuss<a id="_idIndexMarker1428"/> why this is a sequential decision-making problem.</p>
			<h3>Defining the sequential decision-making problem</h3>
			<p>By now, you may have <a id="_idIndexMarker1429"/>noticed how SBO actions taken now will have implications later. Fully charging the EVs in the system too early may limit how much the consumption may be ramped up later when needed. Conversely, keeping room temperatures too high for too many rooms for too long may cause all ACs to turn on later together to bring the room temperatures to normal levels.</p>
			<p>In the next section, let's cast this as an RL problem.</p>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor380"/>RL model</h2>
			<p>As always, we need to<a id="_idIndexMarker1430"/> define the action, observation, and reward to create an RL model. </p>
			<h3>Defining the action</h3>
			<p>There are different<a id="_idIndexMarker1431"/> approaches to how we can define the SBO control: </p>
			<ul>
				<li>First, and a more obvious approach, would be to directly control each appliance in the system by observing their utilities. On the other hand, this would make the model inflexible and potentially intractable: we would have to modify and retrain the agent when a new appliance is added. In addition, when there are many appliances, the action and observation space would be too big.</li>
				<li>Another approach would be to train a policy for each appliance class (ACs, heating units, and EVs) in a multi-agent setting. This would bring in the inherent complexities of multi-agent RL. For example, we would have to design a mechanism for the coordination of individual appliances.</li>
				<li>A middle ground<a id="_idIndexMarker1432"/> is to apply indirect control. In this approach, the SBO would broadcast its action and let each appliance decide on what to do for itself.</li>
			</ul>
			<p>Let's describe what such an indirect control might look like in more detail.</p>
			<h4>Indirect control of the appliances</h4>
			<p>Here is how <a id="_idIndexMarker1433"/>we define the indirect control/action:</p>
			<ul>
				<li>Assume that there are <img src="image/Formula_17_021.png" alt=""/> appliance types, such as ACs, EVs, and refrigerators.</li>
				<li>At any given time, an appliance, <img src="image/Formula_17_022.png" alt=""/>, has a utility, <img src="image/Formula_17_023.png" alt=""/>, that takes a maximum value of <img src="image/Formula_17_024.png" alt=""/>.</li>
				<li>At every time step, the SBO broadcasts an action, <img src="image/Formula_17_025.png" alt=""/>, for each appliance type, <img src="image/Formula_17_026.png" alt=""/>. Therefore, the action is <img src="image/Formula_17_027.png" alt=""/> and <img src="image/Formula_17_028.png" alt=""/></li>
				<li>Each appliance, when off, checks the action for its class once in a while. This won't be at every time step and will depend on its type. For example, AC units might check the broadcasted action more frequently than EVs.</li>
				<li>When an appliance, <img src="image/Formula_17_029.png" alt=""/>, of type <img src="image/Formula_17_030.png" alt=""/> checks action <img src="image/Formula_17_031.png" alt=""/>, it turns on if and only if <img src="image/Formula_17_032.png" alt=""/>. Therefore, the action acts like the <strong class="bold">price</strong> for electricity. The appliance is willing to turn <a id="_idIndexMarker1434"/>on only when its utility is greater than or equal to the <strong class="bold">price</strong>.</li>
				<li>Once turned on, an appliance stays on for a certain time. Then, it turns off and starts periodically checking the action again.</li>
			</ul>
			<p>With this mechanism, the SBO is able to influence the demand indirectly. It gives less precise control over the environment, but at a much-reduced complexity compared to a direct or multi-agent control.</p>
			<p>Next, let's define the observation space.</p>
			<h3>Defining the observation</h3>
			<p>The SBO could<a id="_idIndexMarker1435"/> use the following observations to make informed decisions:</p>
			<ul>
				<li>The ISO signal at time <img src="image/Formula_17_033.png" alt=""/>, <img src="image/Formula_17_034.png" alt=""/>, as the SBO is obliged to track it by adjusting its demand.</li>
				<li>The number of appliances that are on at time <img src="image/Formula_17_035.png" alt=""/> for each type, <img src="image/Formula_17_036.png" alt=""/>. For simplicity, a fixed electricity consumption rate could be assumed for an appliance of type <img src="image/Formula_17_037.png" alt=""/>, <img src="image/Formula_17_038.png" alt=""/>.</li>
				<li>Time and date features, such as time of day, day of the week, holiday calendar, and more.</li>
				<li>Auxiliary information, such as weather temperature.</li>
			</ul>
			<p>In addition to making all these observations at each time step, what will also be needed is to keep a memory <a id="_idIndexMarker1436"/>of the observations. This is a partially observable environment where the energy needs of the appliances and the state of the grid are hidden from the agent. So, keeping a memory will help the agent uncover these hidden states. </p>
			<p>Finally, let's describe the reward function.</p>
			<h3>Defining the reward function</h3>
			<p>In this model, the<a id="_idIndexMarker1437"/> reward function consists of two parts: the tracking cost and the utility. </p>
			<p>We mentioned that the SBO is obliged to track the ISO signal as it is paid for this service. </p>
			<p>Therefore, we assign a penalty for deviating from the target implied by the signal at time <img src="image/Formula_17_039.png" alt=""/>:</p>
			<div>
				<div id="_idContainer1762" class="IMG---Figure">
					<img src="image/Formula_17_040.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_17_041.png" alt=""/> is the target and <img src="image/Formula_17_042.png" alt=""/> is the actual consumption rate at time <img src="image/Formula_17_043.png" alt=""/>.</p>
			<p>The second part of the reward function is the total utility realized by the appliances. We want the appliances to turn on and consume energy but to do so when they really need it. An example of why this is beneficial is the following: an AC would consume less energy when the average room temperature is kept closer to the top of the comfort zone (76°F in <em class="italic">Figure 17.3</em>) where the utility is the highest than when it is kept closer to the bottom while the outside temperature is above the comfort zone. So, the total utility realized at <a id="_idIndexMarker1438"/>time <img src="image/Formula_17_044.png" alt=""/> is as follows:</p>
			<div>
				<div id="_idContainer1767" class="IMG---Figure">
					<img src="image/Formula_17_045.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_17_046.png" alt=""/> is the set of appliances that turn on within discrete time step <img src="image/Formula_17_047.png" alt=""/>. Then, the RL objective becomes the following:</p>
			<div>
				<div id="_idContainer1770" class="IMG---Figure">
					<img src="image/Formula_17_048.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_17_049.png" alt=""/> is some coefficient to control the trade-off between utility and tracking cost, and <img src="image/Formula_17_050.png" alt=""/> is the discount factor.</p>
			<h3>Terminal conditions</h3>
			<p>Finally, let's talk about<a id="_idIndexMarker1439"/> the terminal conditions for this problem. Normally, this is a continuous task without a natural terminal state. However, we can introduce terminal conditions, for example, to stop the episode if the tracking error is too large. Other than that, we can convert this into an episodic task by taking the episode length as a day.</p>
			<p>That's it! We've left the exact implementation of this model out, but you now have a solid idea about how to approach this problem. If you need more details, you can check out the references at the end of this chapter by Bilgin and Caramanis.</p>
			<p>Last but not least, let's switch gears to model the early detection of cyberattacks in a power grid.</p>
			<h1 id="_idParaDest-342"><a id="_idTextAnchor381"/>Detecting cyberattacks in a smart grid</h1>
			<p>Smart cities, by <a id="_idIndexMarker1440"/>defin<a id="_idTextAnchor382"/>ition, run on intense digital communications<a id="_idIndexMarker1441"/> between their assets. Despite its benefits, this makes smart cities prone to cyberattacks. As RL is finding its way into cybersecurity, in this section, we will describe how it can be applied to detecting attacks on a smart power grid infrastructure. Throughout the section, we will follow the model proposed in <em class="italic">Kurt et al., 2019</em>, leaving the details to the paper.</p>
			<p>Let's start by describing the power grid environment.</p>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor383"/>The problem of early detection of cyberattacks in a power grid</h2>
			<p>An electric <a id="_idIndexMarker1442"/>power grid consists of nodes, called <strong class="bold">buses</strong>, which<a id="_idIndexMarker1443"/> correspond to <a id="_idIndexMarker1444"/>generation, demand, or power line intersection points. Grid authorities collect measurements from these buses to make certain decisions such as bringing in additional power generation units. To this end, a critical quantity measured is the <strong class="bold">phase angle</strong> at each<a id="_idIndexMarker1445"/> bus (except the reference bus), which makes it a potential target for cyber-attackers, hence our interest in it:</p>
			<ul>
				<li>Not surprisingly, the measurements from the meters are noisy and subject to errors. </li>
				<li>A cyberattack on these meters and their measurements has the potential to mislead the decisions made by grid authorities and cause the system to collapse.</li>
				<li>Therefore, it is important to detect when there is an attack on the system. </li>
				<li>However, it is not easy to differentiate the noise and real system changes from anomalies caused by an attack. Normally, waiting and collecting more measurements are helpful to this end.</li>
				<li>On the other hand, being late in declaring an attack can lead to incorrect decisions in the meantime. Therefore, our goal is to identify these attacks as soon as possible, but without too many false alarms.</li>
			</ul>
			<p>So, the set of <a id="_idIndexMarker1446"/>possible actions that our cybersecurity agent<a id="_idIndexMarker1447"/> can take is simple: declare an attack or not. A sample timeline of false and true (but delayed) alarms is illustrated in <em class="italic">Figure 17.6</em>:</p>
			<div>
				<div id="_idContainer1773" class="IMG---Figure">
					<img src="image/B14160_17_06.jpg" alt="Figure 17.6 – A sample timeline of (a) false and (b) true but delayed alarms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 17.6 – A sample timeline of (a) false and (b) tr<a id="_idTextAnchor384"/>ue but delayed alarms</p>
			<p>Here are the details of the episode life cycle and rewards:</p>
			<ul>
				<li>Once an attack is declared, the episode terminates.</li>
				<li>If it is a false alarm, a reward of <img src="image/Formula_17_051.png" alt=""/> is incurred. If it is a true alarm, the reward is 0.</li>
				<li>If there is an attack but the action continues (and doesn't declare an attack), for each time step, a reward of <img src="image/Formula_17_052.png" alt=""/>, <img src="image/Formula_17_053.png" alt=""/> is incurred.</li>
				<li>The reward is 0 in all other time steps.</li>
			</ul>
			<p>With that, the goal of the agent is to minimize the following cost function (or maximize its negative):</p>
			<div>
				<div id="_idContainer1777" class="IMG---Figure">
					<img src="image/Formula_17_054.jpg" alt=""/>
				</div>
			</div>
			<p>Here, the first term is the probability of a false alarm, the second term is the expected (positive) delay <a id="_idIndexMarker1448"/>in declaring an attack, and <img src="image/Formula_17_055.png" alt=""/> is the cost<a id="_idIndexMarker1449"/> coefficient to manage the trade-off in between. </p>
			<p>One missing piece is the observations, which we will discuss next.</p>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor385"/>Partial observability of the grid state</h2>
			<p>The true state<a id="_idIndexMarker1450"/> of the system, which is whether there is an <a id="_idIndexMarker1451"/>attack or not, is not observable to the agent. Instead, it collects measurements of phase angles, <img src="image/Formula_17_056.png" alt=""/>. A key contribution of <em class="italic">Kurt et al., 2019</em>, is to use the phase angle measurements in the following way:</p>
			<ol>
				<li value="1">Use a Kalman filter to predict the true phase angles from the previous observations.</li>
				<li>Based on this prediction, estimate the expected measurements, <img src="image/Formula_17_057.png" alt=""/>.</li>
				<li>Define <img src="image/Formula_17_058.png" alt=""/> as a measure of the discrepancy between <img src="image/Formula_17_059.png" alt=""/> and <img src="image/Formula_17_060.png" alt=""/>, which then becomes the observation used by the agent.</li>
				<li>Observe <img src="image/Formula_17_061.png" alt=""/> and carry a memory of past observations for the agent's use.</li>
			</ol>
			<p>The paper uses a tabular SARSA method to solve this problem by discretizing <img src="image/Formula_17_062.png" alt=""/>, and shows the<a id="_idIndexMarker1452"/> effectiveness of the approach. An interesting extension <a id="_idIndexMarker1453"/>would be to use deep RL methods without discretization and under varying grid topographies and attack characteristics.</p>
			<p>With that, we conclude our discussion on the topic and the chapter. Great job, we have done a lot! Let's summarize what we have covered in the chapter.</p>
			<h1 id="_idParaDest-345"><a id="_idTextAnchor386"/>Summary</h1>
			<p>RL is poised to play a significant role in automation. Smart cities are a great field to leverage the power of RL. In this chapter, we discussed three sample applications: traffic light control, ancillary service provision by electricity-consuming appliances, and detecting cyberattacks in a power grid. The first problem allowed us to showcase a multi-agent setting, we used a price-like indirect control mechanism for the second one, and the last one was a good example of advanced input preprocessing in partially observed environments.</p>
			<p>In the next and final chapter, we will wrap up the book with a discussion on the challenges of real-life RL and future directions.</p>
			<h1 id="_idParaDest-346"><a id="_idTextAnchor387"/>References</h1>
			<ul>
				<li>Wu, C., et al. (2019). <em class="italic">Flow: A Modular Learning Framework for Autonomy in Traffic</em>. ArXiv:1710.05465 [Cs]. arXiv.org, <a href="http://arxiv.org/abs/1710.05465">http://arxiv.org/abs/1710.05465</a></li>
				<li>Vinitsky, E., Kreidieh, A., Flem, L.L., Kheterpal, N., Jang, K., Wu, C., Wu, F., Liaw, R., Liang, E., &amp; Bayen, A.M. (2018). <em class="italic">Benchmarks for reinforcement learning in mixed-autonomy traffic</em>. Proceedings of The 2nd Conference on Robot Learning, in PMLR 87:399-409, <a href="http://proceedings.mlr.press/v87/vinitsky18a.html">http://proceedings.mlr.press/v87/vinitsky18a.html</a></li>
				<li>Bilgin, E., Caramanis, M. C., Paschalidis, I. C., &amp; Cassandras, C. G. (2016). <em class="italic">Provision of Regulation Service by Smart Buildings</em>. IEEE Transactions on Smart Grid, vol. 7, no. 3, pp. 1683-1693, DOI: 10.1109/TSG.2015.2501428</li>
				<li>Bilgin, E., Caramanis, M. C., &amp; Paschalidis, I. C. (2013). <em class="italic">Smart building real time pricing for offering load-side Regulation Service reserves</em>. 52nd IEEE Conference on Decision and Control, Florence, pp. 4341-4348, DOI: 10.1109/CDC.2013.6760557</li>
				<li>Caramanis, M., Paschalidis, I. C., Cassandras, C., Bilgin, E., &amp; Ntakou, E. (2012). <em class="italic">Provision of regulation service reserves by flexible distributed loads</em>. IEEE 51st IEEE Conference on Decision and Control (CDC), Maui, HI, pp. 3694-3700, DOI: 10.1109/CDC.2012.6426025</li>
				<li>Bilgin, E. (2014). <em class="italic">Participation of distributed loads in power markets that co-optimize energy and reserves</em>. Dissertation, Boston University</li>
				<li>Kurt, M. N., Ogundijo, O., Li C., &amp; Wang, X. (2019). <em class="italic">Online Cyber-Attack Detection in Smart Grid: A Reinforcement Learning Approach</em>. IEEE Transactions on Smart Grid, vol. 10, no. 5, pp. 5174-5185, DOI: 10.1109/TSG.2018.2878570</li>
			</ul>
		</div>
	</body></html>