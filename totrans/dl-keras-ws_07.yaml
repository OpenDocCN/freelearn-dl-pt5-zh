- en: 7\. Computer Vision with Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers computer vision and how this is accomplished with neural
    networks. You will learn to build image processing applications and classify models
    with convolutional neural networks. You will also study the architecture of convolutional
    neural networks and how to utilize techniques such as max pooling and flattening,
    feature mapping, and feature detection. By the end of this chapter, you will be
    able to not only build your own image classifiers but also evaluate them effectively
    for your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored model evaluation in detail. We covered
    `accuracy` and why it may be misleading for some datasets, especially for classification
    tasks with highly imbalanced classes. Datasets with imbalanced classes such as
    the prediction of hurricanes in the Pacific Ocean or the prediction of whether
    someone will default on their credit card loan have positive instances that are
    relatively rare compared to negative instances, so accuracy scores are misleading
    since the null accuracy is so high.
  prefs: []
  type: TYPE_NORMAL
- en: To combat class imbalance, we learned about techniques that we can use to appropriately
    evaluate our model, including calculating model evaluation metrics such as the
    sensitivity, specificity, false positive rate, and `AUC score`, and plotting the
    `ROC curve`. In this chapter, we will learn how to classify another type of dataset—namely,
    images. Image classification is extremely useful and there are many real-world
    applications of it, as we will discover.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer vision** is one of the most important concepts in machine learning
    and artificial intelligence. With the wide use of smartphones for capturing, sharing,
    and uploading images every day, the amount of data that''s generated through images
    is increasing exponentially. So, the need for experts who are specialized in the
    field of computer vision is at an all-time high. Industries such as the health
    care industry are on the verge of a revolution due to the progress that''s been
    made in the field of medical imaging.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to computer vision and the various industries
    in which computer vision is used. You will also learn about `ANNs`, which use
    vectors as inputs, CNN uses images as its input. In this chapter, we will be studying
    `CNNs` in greater detail, along with the associated concepts of **max pooling**,
    **flattening**, **feature maps**, and **feature selection**. We will use Keras
    as a tool to run image processing algorithms on real-life images.
  prefs: []
  type: TYPE_NORMAL
- en: Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand computer vision, let's discuss human vision. Human vision is the
    ability of the human eye and brain to see and recognize objects. Computer vision
    is the process of giving a machine a similar, if not better, understanding of
    seeing and identifying objects in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: It is fairly simple for the human eye to precisely identify whether an animal
    is a tiger or a lion, but it takes a lot of training for a computer system to
    understand such objects distinctly. Computer vision can also be defined as building
    mathematical models that can mimic the function of a human eye and brain. Basically,
    it is about training computers to understand and process images and videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision is an integral part of many cutting-edge areas of robotics:
    health care and medical (X-rays, MRI scans, CT scans, and so on), drones, self-driving
    cars, sports and recreation, and so on. Almost all businesses need computer vision
    to run successfully.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a large amount of data that's generated by CCTV footage across the world,
    the number of pictures our smartphones capture each day, the number of videos
    that are shared on internet sites such as YouTube on a daily basis, and the pictures
    we share on popular social networking sites such as Facebook and Instagram. All
    of this generates huge volumes of image data. To process and analyze this data
    and make computers more intelligent in terms of processing, this data requires
    high-level experts who specialize in computer vision. Computer vision is a highly
    lucrative field in machine learning. The following sections will describe how
    computer vision is achieved with neural networks—and particularly convolutional
    neural networks—that perform well for computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talk about computer vision, we talk about CNNs in the same breath.
    CNN is a class of deep neural network that is mostly used in the field of computer
    vision and imaging. CNNs are used to identify images, cluster them by their similarity,
    and implement object recognition within scenes. CNN has different layers— namely,
    the input layer, the output layer, and multiple hidden layers. These hidden layers
    of a CNN consist of fully connected layers, convolutional layers, a `ReLU layer`
    as an `activation function`, `normalization layers`, and `pooling layers`. On
    a very simple level, CNNs help us identify images and label them appropriately;
    for example, a tiger image will be identified as a tiger:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: A generalized CNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1: A generalized CNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a CNN classifying a tiger:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: A CNN classifying an image of a tiger into the class “Tiger”'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: A CNN classifying an image of a tiger into the class "Tiger"'
  prefs: []
  type: TYPE_NORMAL
- en: The Architecture of a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main components of CNN architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input image`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Convolutional layer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pooling layer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flattening`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An `input image` forms the first component of a CNN architecture. An image
    can be of any type: a human, an animal, scenery, a medical X-ray image, and so
    on. Each image is converted into a mathematical matrix of zeros and ones. The
    following figure explains how a computer views an image of the letter **T**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the blocks that have a value of one represent the data, while the zeros
    represent blank space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Matrix for the letter ‘T’'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.3: Matrix for the letter ''T'''
  prefs: []
  type: TYPE_NORMAL
- en: Convolution Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `convolution layer` is the place where image processing starts. A convolution
    layer consists of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Feature detector` or `filter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Feature map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Feature detector` or a `filter`: This is a matrix or pattern that you put
    on an image to transform it into a feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: Feature detector'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.4: Feature detector'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, this feature detector is put (superimposed) on the original
    image and the computation is done on the corresponding elements. The computation
    is done by multiplying the corresponding elements, as shown in the following figure.
    This process is repeated for all the cells. This results in a new processed image—
    `(0x0+0x0+0x1) + (0x1+1x0+0x0) + (0x0+0x1+0x1) = 0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: Feature detector masked in an image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.5: Feature detector masked in an image'
  prefs: []
  type: TYPE_NORMAL
- en: '`Feature Map`: This is the reduced image that is produced by the convolution
    of an `image` and `feature detector`. We have to put the feature detector on all
    the possible locations of the original image and derive a smaller image from it;
    that derived image is the feature map of the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Feature map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.6: Feature map'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `feature detector` is the filter and the `feature map` is the reduced
    image. Some information is lost while reducing the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an actual CNN, a number of feature detectors are used to produce a number
    of feature maps, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Multiple feature detectors and maps'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Multiple feature detectors and maps'
  prefs: []
  type: TYPE_NORMAL
- en: The Pooling Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `pooling layer` helps us ignore the less important data in the image and
    reduces the image further, all while preserving its important features. Consider
    the following three images, which contain four cats in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: Example of cat images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.8: Example of cat images'
  prefs: []
  type: TYPE_NORMAL
- en: To identify whether an image has a cat in it or not, the neural network analyzes
    the picture. It may look at ear shape, eye shape, and so on. At the same time,
    the image consists of lots of features that are not related to cats. The tree
    and leaves in the first two images are useless in the identification of the cat.
    The pooling mechanism helps the algorithm understand which parts of the image
    are relevant and which parts are irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature map derived from the convolution layer is passed through a pooling
    layer to further reduce the image, all while preserving the most relevant part
    of the image. The pooling layer consists of functions such as max pooling, min
    pooling, and average pooling. What this means is that we select a matrix size,
    say `2x2`, and we scan the feature map and select the maximum number from the
    `2x2` matrix that fits in that block. The following image gives us a clear idea
    of how max pooling works. Refer to the colors; the max number in each of the colored
    boxes from the feature map is selected in the pooled feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9: Pooling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.9: Pooling'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case of the box that has number `4` in it. Let''s assume that
    number `4` represents the ears of a cat, while the blank space around the ears
    is `0` and `1`. So, we ignore the `0` and `1` of that block and only select `4`.
    The following is some example code that we would use to add a pooling layer; here,
    `Maxpool2D` is used for max pooling, which helps identify the most important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Flattening
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Flattening` is part of a CNN where the image is made ready to use as an input
    to an ANN. As the name suggests, the pooled image is flattened and converted into
    a single column. Each row is made into a column and stacked one over another.
    Here, we have converted a `3x3` matrix into a `1xn` matrix, where `n`, in our
    case, is `9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10: Flattening'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.10: Flattening'
  prefs: []
  type: TYPE_NORMAL
- en: 'In real-time, we have a number of pooled feature maps, and we flatten them
    into a single column. This single column is used as input for an ANN. The following
    figure shows a number of pooled layers flattened into a single column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11: Pooling and flattening'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.11: Pooling and flattening'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is some example code that we would use to add a flattening layer;
    here `Flatten` is used for flattening the CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the overall structure of a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12: CNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.12: CNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is some example code that we would use to add the first layer
    to a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`32,3,3` refers to the fact that there are `32` feature detectors of size `3x3`.
    As a good practice, always start with `32`; you can add `64` or `128` later.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input_shape`: Since all the images are of different shapes and sizes, this
    `input_image` converts all the images into a uniform shape and size. `(64,64)`
    is the dimension of the converted image. It can be set to `128` or `256`, but
    if you are working on a CPU on a laptop, it is advisable to use `64x64`. The last
    argument, `3`, is used because the image is a colored image (coded in red, blue,
    and green, or RGB). If the image is black and white, the argument can be set to
    one. The activation function that''s being used is ReLU.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are using Keras with TensorFlow as the backend in this book. If the backend
    is Theano, then `input_image` will be coded as (`3,64,64`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to fit the data that''s been created. Here is the code that
    we use to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`steps_per_epoch` is the number of training images. `validation_steps` is the
    number of test images.'
  prefs: []
  type: TYPE_NORMAL
- en: Image Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word **augmentation** means the action or process of making or becoming
    greater in size or amount. **Image** or **data augmentation** works in a similar
    manner. Image/data augmentation creates many batches of our images. Then, it applies
    random transformations to random images inside the batches. Data transformation
    can be rotating images, shifting them, flipping them, and so on. By applying this
    transformation, we get more diverse images inside the batches, and we also have
    much more data than we had originally.
  prefs: []
  type: TYPE_NORMAL
- en: 'A cylinder can be rotated from different angles and seen differently. In the
    following figure, a single cylinder can be seen from five different angles. So,
    we have effectively created five different images from a single image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13: Image augmentation of a cylinder'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.13: Image augmentation of a cylinder'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is some example code that we would use for image augmentation;
    here, the `ImageDataGenerator` class is used for processing. `shear_range`, `zoom_range`,
    and `horizontal_flip` are all used for transforming the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Advantages of Image Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image augmentation is an important part of processing images:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduces overfitting**: It helps reduce overfitting by creating multiple versions
    of the same image, rotated by a given amount.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increases the number of images**: A single image acts as multiple images.
    So, essentially, the dataset has fewer images, but each image can be converted
    into multiple images with image augmentation. Image augmentation will increase
    the number of images and each image will be treated differently by the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy to predict new images**: Imagine that a single image of a football is
    looked at from different angles and each angle is considered a distinct image.
    This will mean that the algorithm will be more accurate at predicting new images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.14: Image augmentation of an image of a football'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14: Image augmentation of an image of a football'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the concepts and theory behind computer vision
    with CNNs, let's work on some practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will start with an exercise in which we''ll build a simple CNN. In
    the following exercises and activities, we will tweak our CNN using permutation
    and combining the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding more CNN layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more ANN layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the optimizer function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin by creating our first CNN so that we can classify images of cars
    and flowers into their respective classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.01: Building a CNN and Identifying Images of Cars and Flowers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we have images of cars and flowers, which have been divided
    into training and testing sets, and we have to build a CNN that identifies whether
    an image is a car or a flower.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the exercises and activities in this chapter will be developed in Jupyter
    notebooks. Please download this book's GitHub repository, along with all the prepared
    templates, from [https://packt.live/39tID2C](https://packt.live/39tID2C).
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin, ensure that you have downloaded the image datasets from this
    book's GitHub repository to your own working directory. You will need a `training_set`
    folder to train your model and a `test_set` folder to test your model. Each of
    these folders will contain a `cars` folder, containing car images, and a `flowers`
    folder, containing flower images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for completing this exercise are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary Keras libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, set a seed and initiate the model with the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first layer of the `CNN`, set the input shape to `(64, 64, 3)`, the
    dimension of each image, and set the activation function as a `ReLU`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`32,3,3` shows that there are `32` feature detectors of `3x3` size.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, add the pooling layer with the image size as `2x2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Flatten the output of the pooling layer by adding a flattening layer to the
    `CNN` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first `Dense` layer of the `ANN`. Here, `128` is the output of the
    number of nodes. As a good practice, `128` is good to get started. `activation`
    is `relu`. As a good practice, the power of two is preferred:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the ANN. This is a binary classification problem, so
    the size is `1` and the activation is `sigmoid`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `adam` optimizer and compute the accuracy during
    the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create training and test data generators. Rescale the training and test images
    by `1/255` so that all the values are between `0` and `1`. Set these parameters
    for the training data generators only – `shear_range=0.2`, `zoom_range=0.2`, and
    `horizontal_flip=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training set from the `training set` folder. `''../dataset/training_set''`
    is the folder where our data has been placed. Our CNN model has an image size
    of `64x64`, so the same size should be passed here too. `batch_size` is the number
    of images in a single batch, which is `32`. `Class_mode` is set to `binary` since
    we are working on binary classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 10* for the test set while setting the folder to the location
    of the test images, that is, `''../dataset/test_set''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, fit the data. Set the `steps_per_epoch` to `10000` and the `validation_steps`
    to `2500`. The following step might take some time to execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy on the validation set is `84.22%`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To get more accurate results, try increasing the number of epochs to about `25`.
    This will increase the time that it takes to process the data, and the total time
    is dependent on the configuration of your machine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38njqHU](https://packt.live/38njqHU).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iqFpSN](https://packt.live/3iqFpSN).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That completes this exercise on processing images and identifying the contents
    of the images. An important thing to remember here is that this is a robust code
    for any binary classification problem in computer vision. This means that the
    code remains the same, even if the image data changes. We will test our knowledge
    of this by modifying some of the parameters of our model in the next activity
    and evaluating the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7.01: Amending Our Model with Multiple Layers and the Use of softmax'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we have run a `CNN model` successfully, the next logical step is to try
    and improve the performance of our algorithm. There are many ways to improve its
    performance, and one of the most straightforward ways is by adding multiple ANN
    layers to the model, which we will learn about in this activity. We will also
    change the activation from sigmoid to softmax. By doing this, we can compare the
    result with that of the previous exercise. Follow these steps to complete this
    activity:'
  prefs: []
  type: TYPE_NORMAL
- en: To build a CNN import library, set a seed and create a `Sequential` class and
    import `Conv2D`, `MaxPool2D`, `Flatten`, and `Dense`. `Conv2D` is used to build
    the convolution layer. Since our pictures are in 2D, we have used 2D here. Similarly,
    `Maxpool2D` is used for max pooling, `Flatten` is used for flattening the CNN,
    and `Dense` is used to add a fully connected CNN to an ANN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start building a CNN architecture using the preceding libraries. After adding
    the first layer, add two additional layers to your CNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a pooling and flattening layer to it, which will serve as the input for
    the ANN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a fully connected ANN whose inputs will be the output of the CNN. After
    adding the first layer of your ANN, add three additional layers. For the output
    layer of your ANN, use the softmax activation function. Compile the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform image augmentation to process and transform the data. The `ImageDataGenerator`
    class is used for processing. `shear_range`, `zoom_range`, and `horizontal_flip`
    are all used for the transformation of images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the training and test set data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, fit the data that's been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After implementing these steps, you should get the following expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 439.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we have modified our CNN model to try and improve the accuracy
    of our image classifier. We have added additional convolutional layers and additional
    ANN fully connected layers and changed the activation function in the output layer.
    By doing so our accuracy has decreased. In the next exercise, we will change the
    activation function back to a sigmoid. We will evaluate the performance by observing
    the accuracy evaluated on the validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.02: Amending Our Model by Reverting to the Sigmoid Activation Function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will rebuild our model but revert the activation function
    from softmax back to sigmoid. By doing this, we can compare the accuracy with
    our previous model''s. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary Keras libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, set the seed and initiate the model with the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first layer of the CNN, set the input shape to `(64, 64, 3)`, the dimension
    of each image, and set the activation function as a ReLU. Then, add `32` feature
    detectors of size `(3, 3)`. Add two additional convolutional layers with `32`
    feature detectors of size `(3, 3)`, also with ReLU activation functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, add the pooling layer with the image size as `2x2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add one more `Conv2D` with the same parameters as in *step 3* and a pooling
    layer to supplement it with the same parameters that we used in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Flatten the output of the pooling layer by adding a flattening layer to the
    `CNN model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first `Dense` layer of the ANN. Here, `128` is the output of the number
    of nodes. As a good practice, `128` is good to get started. `activation` is `relu`.
    As a good practice, the power of two is preferred. Add three additional layers
    with the same parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the `ANN`. This is a binary classification problem,
    so the output is `1` and the activation is `sigmoid`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an Adam optimizer and compute the accuracy during
    the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create training and test data generators. Rescale the training and test images
    by `1/255` so that all the values are between `0` and `1`. Set these parameters
    for the training data generators only – `shear_range=0.2`, `zoom_range=0.2`, and
    `horizontal_flip=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training set from the `training set` folder. `../dataset/training_set`
    is the folder where our data is placed. Our CNN model has an image size of 64x64,
    so the same size should be passed here too. `batch_size` is the number of images
    in a single batch, which is `32`. `class_mode` is binary since we are working
    on binary classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 11* for the test set by setting the folder to the location of
    the test images, that is, `''../dataset/test_set''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, fit the data. Set the `steps_per_epoch` to `10000` and the `validation_steps`
    to `2500`. The following step might take some time to execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy of the model is `86.75%`, which is clearly greater than the accuracy
    of the model we built in the previous exercise. This shows the importance of activation
    functions. Just changing the output activation function from softmax to sigmoid
    increased the accuracy from `46.91%` to `86.75%`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZD9nKM](https://packt.live/2ZD9nKM).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dPZiiQ](https://packt.live/3dPZiiQ).
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will experiment with a different optimizer and observe
    how that affects the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In a binary classification problem (in our case, cars versus flowers), it is
    always better to use sigmoid as the activation function for the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.03: Changing the Optimizer from Adam to SGD'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will amend the model again by changing the optimizer to
    `SGD`. By doing this, we can compare the accuracy with our previous models. Follow
    these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` library and the necessary Keras libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, initiate the model with the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first layer of the `CNN`, set the input shape to `(64, 64, 3)`, the
    dimension of each image, and set the activation function as `ReLU`. Then, add
    `32` feature detectors of size (`3, 3`). Add two additional convolutional layers
    with the same number of feature detectors with the same size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, add the pooling layer with the image size as `2x2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add one more `Conv2D` with the same parameters as in *step 3* and a pooling
    layer to supplement it with the same parameters that we used in *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a `Flatten` layer to complete the CNN architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first `Dense` layer of the ANN of size `128`. Add three more dense
    layers to the network with the same parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the ANN. This is a binary classification problem, so
    the output is `1` and the activation is `sigmoid`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the network with an `SGD optimizer` and compute the accuracy during
    the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create training and test data generators. Rescale the training and test images
    by `1/255` so that all the values are between `0` and `1`. Set these parameters
    for the training data generators only – `shear_range=0.2`, `zoom_range=0.2`, and
    `horizontal_flip=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training set from the `training set` folder. `../dataset/training_set`
    is the folder where our data is placed. Our CNN model has an image size of `64x64`,
    so the same size should be passed here too. `batch_size` is the number of images
    in a single batch, which is `32`. `class_mode` is binary since we are creating
    a binary classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *step 11* for the test set by setting the folder to the location of
    the test images, that is, `''../dataset/test_set''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, fit the data. Set the `steps_per_epoch` to `10000` and the `validation_steps`
    to `2500`. The following step might take some time to execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy is `84.54%` since we have used multiple `ANNs` and `SGD` as the optimizer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31Hu9vm](https://packt.live/31Hu9vm).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3gqE9x8](https://packt.live/3gqE9x8).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So far, we have worked with a number of different permutations and combinations
    of our model. It seems like the best accuracy for this dataset can be obtained
    by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding multiple CNN layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding multiple ANN layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having the activation as sigmoid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having the optimizer as adam.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the epoch size to about `25` (this takes a lot of computational time
    – make sure you have a GPU to do this). This will increase the accuracy of your predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will go ahead and predict a new unknown image, pass it to the algorithm,
    and validate whether the image is classified correctly. In the next exercise,
    we will demonstrate how to use the model to classify new images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.04: Classifying a New Image'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will try to classify a new image. The image hasn't been
    exposed to the algorithm, so we will use this exercise to test our algorithm.
    You can run any of the algorithms in this chapter (although the one that gets
    the highest accuracy is preferred) and then use the model to classify the image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The image that's being used in this exercise can be found in this book's GitHub
    repository at [https://packt.live/39tID2C](https://packt.live/39tID2C).
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin, ensure that you have downloaded `test_image_1` from this book's
    GitHub repository to your own working directory. This exercise follows on from
    the previous exercises, so ensure that you have one of the algorithms from this
    chapter ready to run in your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for completing this exercise are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the image. `''test_image_1.jpg''` is the path of the test image. Please
    change the path to where you have saved the dataset in your system. Look at the
    image to verify what it is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the class labels located in the `class_indices` attribute of the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Process the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the new image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `prediction` method will output the image as `1` or `0`. To map `1` and
    `0` to `flower` or `car`, use the `class_indices` method with an `if…else` statement,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`test_image_1` is the image of a car (you can see this by viewing the image
    for yourself) and was correctly predicted to be a car by the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we trained our model and then gave the model an image of a
    car. By doing this, we found out that the algorithm is classifying the image correctly.
    You can train the model on any type of an image by using the same process. For
    example, if you train the model with scans of lung infections and healthy lungs,
    then the model will be able to classify whether a new scan represents an infected
    lung or a healthy lung.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31I6B9F](https://packt.live/31I6B9F).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2BzmEMx](https://packt.live/2BzmEMx).
  prefs: []
  type: TYPE_NORMAL
- en: In the next activity, we will put our knowledge into practice by using a model
    that we trained in *Exercise 7.04*, *Classifying a New Image*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7.02: Classifying a New Image'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will try to classify another new image, just like we
    did in the preceding exercise. The image is not exposed to the algorithm, so we
    will use this activity to test our algorithm. You can run any of the algorithms
    in this chapter (although the one that gets the highest accuracy is preferred)
    and then use the model to classify your images. The steps to implement this activity
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Run any one of the algorithms from this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the image (`test_image_2`) from your directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the image using the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the subject of the new image. You can view the image yourself to check
    whether the prediction is correct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The image that's being used in this activity can be found in this book's GitHub
    repository at [https://packt.live/39tID2C](https://packt.live/39tID2C).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before starting, ensure you have downloaded `test_image_2` from this book's
    GitHub repository to your own working directory. This activity follows on directly
    from the previous exercises, so please ensure that you have one of the algorithms
    from this chapter ready to run in your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'After implementing these steps, you should get the following expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 442.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we trained the most performant model in this chapter when
    given the various parameters that were modified, including the optimizer and the
    activation function in the output layer according to the accuracy on the validation
    dataset. We tested the classifier on a test image and found it to be correct.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied why we need computer vision and how it works. We
    learned why computer vision is one of the hottest fields in machine learning.
    Then, we worked with convolutional neural networks, learned about their architecture,
    and looked at how we can build CNNs in real-life applications. We also tried to
    improve our algorithms by adding more ANN and CNN layers and by changing the activation
    and optimizer functions. Finally, we tried out different activation functions
    and loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we were able to successfully classify new images of cars and flowers
    through the algorithm. Remember, the images of cars and flowers can be substituted
    with any other images, such as tigers and deer, or MRI scans of brains with and
    without a tumor. Any binary classification computer imaging problem can be solved
    with the same approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study an even more efficient technique for working
    on computer vision, which is less time-consuming and easier to implement. The
    following chapter will teach us how to fine-tune pre-trained models for our own
    applications that will help create more accurate models that can be trained in
    faster times. The models that will be used are called VGG-16 and ResNet50 and
    are popular pre-trained models that are used to classify images.
  prefs: []
  type: TYPE_NORMAL
