<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Long Short-Term Memory Networks for Stock Forecasting</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter will show you how to use <strong>long short-term memory</strong> (<strong>LSTM</strong>) models to forecast stock prices. This type of model is particularly useful for time series-based forecasting tasks. An LSTM model is a special type of <strong>recurrent neural network</strong> (<strong>RNN</strong>). These models contain special characteristics that allow you to reuse recent output as input. In this way, these types of models are often described as having memory. We will begin by creating a simple baseline model for predicting stock prices. From there, we will create a minimal LSTM model and we will dive deeper into the advantages of this model type over our baseline model, as well as explore how this model type is an improvement over a more traditional RNN. Lastly, we will look at some ways to tune our model to further improve its performance.</p>
<p class="mce-root">In this chapter,<span> we will cover the following topics:</span></p>
<ul>
<li>Understanding common methods for stock market prediction</li>
<li>Preparing and preprocessing data</li>
<li>Configuring a data generator</li>
<li>Training and evaluating the model</li>
<li>Tuning hyperparameters to improve performance</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You can find the code files used in this chapter at </span><a href="https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R">https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding common methods for stock market prediction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about a different type of neural network called an RNN. In particular, we will apply a type of RNN known as an LSTM model to predict stock prices. Before we begin, let's first look at some common methods of predicting stock prices to better understand the problem.</p>
<p>Predicting stock prices is a time-series problem. With most other machine learning problems, variables can be split at random and used in training and test datasets, but this is not possible <span>when solving a time-series problem</span>. The variables must remain in order. The features to solve the problem can be found in the sequence of events and, consequently, the chronology of how events occurred must be maintained to generate meaningful predictions of what will happen next. While this places a constraint on which methods can be used, it also provides an opportunity to use some specific models that are well suited <span>for these types of tasks.</span></p>
<p>Let's start with some approaches that are relatively straightforward to create a baseline model before creating our deep learning solution, which we can use to compare results later. The first model that we will construct is an <strong><span>Auto-Regressive</span><span> </span><span>Integrated Moving Average</span></strong> <span>(<strong>ARIMA</strong>) model.</span><span> Actually, the concepts from the name of this model explain a lot about the particular challenges of modeling on time-series data.</span></p>
<p>The <strong>AR</strong> in ARIMA stands for <strong>auto-regressive</strong> and refers to the fact that the model inputs will be for a given observation and set number of lagged observations. <span>The <strong>MA</strong> in ARIMA stands for <strong>moving average</strong> and refers to the autoregressive nature of the model, which states that the variables will include an observation at a given point in time and a set of lagged variables. The moving average component takes into account the mean value between a set of variables over time to capture a more generalized value explaining a trend over time.</span></p>
<p>The <strong>I</strong> in ARIMA stands for <strong>integrated</strong>, which in this context means that the entire time-series is considered as a whole. More specifically, it refers to the idea that the solutions must be generalizable across the entire series just as we strive for with other machine learning solutions. To aid with this, with time-series problems we transform the data so that it is said to be stationary. With ARIMA, we look at the differences between an observation and the observation preceding it and then use these relative differences. By using the relative differences, we can maintain a more generalized shape over the series, which helps to control the mean and variance, which are important elements in forecasting predicted future states.</p>
<p>With this context, let's now create an ARIMA model. To get started, we will use the <kbd>quantmod</kbd> package to load stock information. The <kbd>getSymbols</kbd> function from this package is a very convenient way to pull in stock price information for any company within a set time frame from a number of sources. We set <kbd>auto.assign</kbd> to <kbd>FALSE</kbd> since we will assign this ourselves. For our example, we will load 5 years of Facebook stock:</p>
<ol>
<li>In the following code, we will read in the data as well as load all the libraries we will be using:</li>
</ol>
<pre style="padding-left: 60px">library(quantmod)<br/>library(tseries) <br/>library(ggplot2)<br/>library(timeSeries)<br/>library(forecast)<br/>library(xts)<br/>library(keras)<br/>library(tensorflow)<br/><br/>FB &lt;- getSymbols('FB', from='2014-01-01', to='2018-12-31', source = 'google', auto.assign=FALSE)</pre>
<p style="padding-left: 60px">After running this code, we will see that we have an <kbd>FB</kbd> object in our environment with an <kbd>xts</kbd> class type. This object class type is similar to a standard data frame, but the row names are dates. Let's inspect a few rows of <kbd>xts</kbd> data.</p>
<ol start="2">
<li>We can view the first five rows from the <kbd>FB</kbd> object by using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">FB[1:5,]</pre>
<p style="padding-left: 60px">After running this code, you will see the following output printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/70d9271c-8576-4b59-b820-d60a5e88b5c6.png"/></p>
<p style="padding-left: 60px">We can see from these selected rows that we have a number of stock price points taken from the day, including the opening and closing price, as well as the highest and lowest price for the stock during the day. The volume of stock traded is also included. For our purposes, we will use the price at the close of trading.</p>
<ol start="3">
<li>We select just the closing prices from the data and store them in a new object by running the following code:</li>
</ol>
<pre style="padding-left: 60px">closing_prices &lt;- FB$FB.Close</pre>
<ol start="4">
<li>We can now use the <kbd>plot.xts</kbd> function to plot the stock data. Usually, plotting this data would require the dates to be held in a column, but this plotting function is a convenient way of plotting a time series without needing the dates present within a column in the data frame. We can plot the full 5 years of Facebook closing stock prices by running the following line of code:</li>
</ol>
<pre style="padding-left: 60px">plot.xts(closing_prices,main="Facebook Closing Stock Prices")</pre>
<p style="padding-left: 60px">After we run this line of code, we will see the following plot generated in our <strong>Plots</strong><span class="packt_screen"><strong> </strong></span>tab:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bb32a7ae-6c0c-49c6-97c3-2b235c5e8ab9.png" style="width:45.67em;height:21.08em;"/></p>
<ol start="5">
<li class="CDPAlignLeft CDPAlign"><span>Now that we can see how stock prices for Facebook have changed over this time frame, let's build an ARIMA model. Afterward, we will create a forecast using the ARIMA model and then plot these forecast v</span><span>alues. We create our model, forecast, and plot by running the following code:</span></li>
</ol>
<pre style="padding-left: 60px">arima_mod &lt;- auto.arima(closing_prices)<br/><br/>forecasted_prices &lt;- forecast(arima_mod,h=365)<br/><br/>autoplot(forecasted_prices)</pre>
<p style="padding-left: 60px">After running the preceding code, we will see the following plot in the <strong>Plots</strong> tab:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99086d54-705a-474e-824e-b11bfb521a46.png" style="width:46.67em;height:21.92em;"/></p>
<p style="padding-left: 60px">The ARIMA model did not find a pattern and instead offers an upper and lower bound, within which prices are predicted, which is not very helpful. ARIMA is a popular baseline model for forecasting time-series data that often performs well. However, in this case, we can see that our ARIMA model doesn't appear to provide much useful information.</p>
<ol start="6">
<li>Before continuing, we can add in the actual stock price values from this frame to see whether the prices did fall within the bounds predicted by the ARIMA model. To pull in the data and add it to our plot, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">fb_future &lt;- getSymbols('FB', from='2019-01-01', to='2019-12-31', source = 'google', auto.assign=FALSE)<br/><br/>future_values &lt;- ts(data = fb_future$FB.Close, start = 1258, end = 1509)<br/><br/>autoplot(forecasted_prices) + autolayer(future_values, series="Actual Closing Prices")</pre>
<p style="padding-left: 60px">After we run the preceding code, we will see the following plot in the <strong>Plots</strong> tab:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa240e84-9b4f-46b4-b891-351f51a51a53.png" style="width:43.58em;height:20.58em;"/></p>
<p>As noted, the values for these dates are outside the bounds of the ARIMA model. We can see that we chose a difficult time-series dataset to model since the stock price is on a downward trend and then begins to rise again. With this being said, over the 5-year period, there is a general upward trend. Can we build a model that learns this upward trend and reflects it properly in the predicted results? Let's take what we have learned about the particular challenges of modeling time-series data and see whether we can improve upon our baseline results using a deep learning approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing and preprocessing data</h1>
                </header>
            
            <article>
                
<p class="mce-root">When working with time-series data, there are a number of data type formats to choose from and use for conversion. We have already used two of these formats, of which there are three that are most widely used. Let's briefly review these data types before moving on to our deep learning model.</p>
<p>When we wanted to add actual data as an overlay to our ARIMA model plot, we used the <kbd>ts</kbd> function to create a time-series data object. For this object, the index values must be integers. In the case of using the <kbd>autolayer</kbd> function with the <kbd>arima</kbd> plot, a time-series data object is required. This is one of the more simple time-series data types and it will look like a vector in your <strong>Environment</strong> tab. However, this only works with regular time series.</p>
<p>Another data type is <kbd>zoo</kbd>. The <kbd>zoo</kbd> data type will work with regular and irregular time series, and zoo objects can also contain a number of different data types as index values. The drawback of the <kbd>zoo</kbd> objects is that there is not much information available in the <strong>Environment</strong> pane. The only detail provided is the date range. At times, the <kbd>zoo</kbd> data works better for plotting, especially when overlaying multiple time-series objects, which is what we will use it for later in this chapter.</p>
<p>The last time-series data type is <kbd>xts</kbd>. This data type is an extension of <kbd>zoo</kbd>. Like <kbd>zoo</kbd>, the index values are dates. However, in addition, the data is stored in a matrix and numerous attributes are present in the <strong>Environment</strong> pane, making it easier to inspect the size and contents of the data object. This is generally a good choice for working with time-series data, unless there are particular reasons to use one of the other types that we have, and will, cover. Another benefit of <kbd>xts</kbd> is that the default plot function uses different formatting to the base plot.</p>
<p>When we are working with time-series data, there is another part of preprocessing in addition to data type conversion that is often useful for modeling: converting our data to delta values rather than absolute values in order to make the data stationary. In this case, we will also log transform the price values to further control the outliers. When we are done with this process, we will need to remove the missing value that we introduced as well. Let's convert our combined stock price data from the closing price to the daily change in the log value of the closing price by running the following code:</p>
<pre>future_prices &lt;- fb_future$FB.Close<br/><br/>closing_deltas &lt;- diff(log(rbind(closing_prices,future_prices)),lag=1)<br/>closing_deltas &lt;- closing_deltas[!is.na(closing_deltas)]</pre>
<p>After running this code, we have data that should be more stationary than the data we were working with before. Let's view what our data looks like now by running the following code:</p>
<pre>plot(closing_deltas,type='l', main='Facebook Daily Log Returns')</pre>
<p>When we run the preceding code, we will see the following plot generated in the <strong>Plots</strong> pane:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60ee2ff4-12f0-437c-8270-03c2b92ff69c.png" style="width:47.75em;height:22.17em;"/></p>
<p>The values we are working with are far more constrained and the patterns from one section appear as if they will generalize better to explain movement in a different section. Even though we can see that the values are better scaled here, we can also run a quick test to prove that the data is now stationary. To check for stationarity, we can use the <strong>Augmented Dickey-Fuller test</strong>, which we can run on our data using this line of code:</p>
<pre>adf.test(closing_deltas)</pre>
<p>After running this code, we will see the following output printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7b7bd7e7-f212-475c-80d1-11d854923f48.png"/></p>
<p>Although the actual p-value isn't reported as output when running this function, we can see that the p-value is small enough that we can accept the alternate hypothesis that the data is stationary. With this preprocessing complete, we can now move on to setting up our <kbd>train</kbd> and <kbd>test</kbd> sets for our deep learning model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a data generator</h1>
                </header>
            
            <article>
                
<p>Similar to ARIMA, for our LSTM model, we would like the model to use lagging historical data to predict actual data at a given point in time. However, in order to feed this data forward to an LSTM model, we must format the data so that a given number of columns contain all the lagging values and one column contains the target value. In the past, this was a slightly tedious process, but now we can use a data generator to make this task much simpler. In our case, we will use a time-series generator that produces a tensor that we can use for our LSTM model.</p>
<p>The arguments we will include when generating our data are the data objects we will use along with the target. In this case, we can use the same data object as values for both arguments. The reason this is possible has to do with the next argument, called <kbd>length</kbd>, which configures the time steps to look back in order to populate the lagging price values. Afterward, we define the sampling rate and the stride, which determine the number of time steps between consecutive values for the target values per row and the lagging values per sequence, respectively. We also define the starting index value and the ending index value. We also need to determine whether data should be shuffled or kept in chronological order and whether the data should be in reverse chronological order or maintain its current sort order. Finally, we select a batch size, which specifies how many time series samples should be in each batch of the model.</p>
<p>For this model, we will create our generated time-series data with a <kbd>length</kbd> value of <kbd>3</kbd>, meaning that we will look back 3 days to predict a given day. We will keep the sampling rate and stride at <kbd>1</kbd> to include all data. Next, we will split the <kbd>train</kbd> and <kbd>test</kbd> sets with an index point of <kbd>1258</kbd>. We will not shuffle or reverse the data, but rather maintain its chronological order and set the batch size to <kbd>1</kbd> so that we model one price at a time. We create our <kbd>train</kbd> and <kbd>test</kbd> sets using these values for our parameters via the following code:</p>
<pre>train_gen &lt;- timeseries_generator(<br/>  closing_deltas,<br/>  closing_deltas,<br/>  length = 3,<br/>  sampling_rate = 1,<br/>  stride = 1,<br/>  start_index = 1,<br/>  end_index = 1258,<br/>  shuffle = FALSE,<br/>  reverse = FALSE,<br/>  batch_size = 1<br/>)<br/><br/>test_gen &lt;- timeseries_generator(<br/>  closing_deltas,<br/>  closing_deltas,<br/>  length = 3,<br/>  sampling_rate = 1,<br/>  stride = 1,<br/>  start_index = 1259,<br/>  end_index = 1507,<br/>  shuffle = FALSE,<br/>  reverse = FALSE,<br/>  batch_size = 1<br/>)</pre>
<p>After running this code, you will see two tensor objects in your <strong>Environment</strong> pane. Now that we have configured our data generator and used it to create two sequence tensors, we are ready to model our data using LSTM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and evaluating the model</h1>
                </header>
            
            <article>
                
<p>Our data is properly formatted and we can now train our model. For this task, we are using LSTM. This is a particular type of RNN. These types of neural networks are a good choice for time-series data because they are able to take time into account during the modeling process.</p>
<p>Most neural networks are classified as <strong>feedforward</strong> <strong>networks</strong>. In these model architectures, the signals start at the input node and are passed forward to any number of hidden layers until they reach an output node. There is some variation in feedforward networks. A multilayer perceptron model is composed of all dense, fully connected layers while a convolutional neural network includes layers that operate on particular parts of the input data before arriving at a dense layer and subsequent output layer. In these types of models, the backpropagation step passes back derivatives from the cost function, but this happens after the entire feedforward pass is complete. RNNs differ in a very important way. Rather than waiting until the entire forward pass is complete, a certain data point that represents a point in time is fed forward and evaluated by the activation function in a hidden layer unit. The output from this activation function is then fed back into the node and calculated with the next time-based data element. In this way, RNNs can use what they just learned to inform how to process the next data point. We can see now why these work so well when we are considering data that includes a time component.</p>
<p>While RNNs are designed to work well for time-series data, they do have one important limitation. The recurrent element of the model only takes into account the time period that just preceded it and, during backpropagation, the signal being passed back can decay when there are a large number of hidden layers. These two model characteristics mean that a given node cannot use the information it has learned on a distant time horizon, even though it may be useful. The LSTM model solves this problem.</p>
<p>In the LSTM model, there are two paths for data entering a node. One is the same as in an RNN and contains the given time-based data point as well as the output from the time point preceding it. However, if the output from the activation function of this vector is greater than <kbd>0</kbd>, then it is passed forward. In the next node, it goes on a separate path towards an activation function known as the <strong>forget gate</strong>. If the data passes through this function with a positive value, then it is combined with the output from the activation function that takes the current state and immediate past output as input. In this way, we can see how data from many time periods in the past can continue to be included as input to nodes much further forward. Using this model design, we can overcome the limitations of a traditional RNN. Let's get started with training our model:</p>
<ol>
<li>First, we run the following line of code to initiate a Keras sequential model:</li>
</ol>
<pre style="padding-left: 60px">model &lt;- keras_model_sequential()</pre>
<ol start="2">
<li>After running this line of code, we will now add our LSTM layer. In our LSTM layer, we will choose the number of units for our hidden layer and also define our input shape, which is the length of the look-back defined earlier as one dimension and <kbd>1</kbd> as the other. We will then add a dense layer with one unit, which will be assigned our predicted price. To define our LSTM model, we use the following code:</li>
</ol>
<pre style="padding-left: 60px">model %&gt;%<br/>  layer_lstm(units = 4,<br/>             input_shape = c(3, 1)) %&gt;%<br/>  layer_dense(units = 1)</pre>
<ol start="3">
<li>After defining our model, we next include the <kbd>compile</kbd> step. In this case, we will use mean squared error (<kbd>mse</kbd>) as our loss function, since this is a regression task, and we will use <kbd>adam</kbd> as our optimizer. We define the <kbd>compile</kbd> step and view our model using the following code:</li>
</ol>
<pre style="padding-left: 60px">model %&gt;%<br/>  compile(loss = 'mse', optimizer = 'adam')<br/><br/>model</pre>
<p style="padding-left: 60px">After running this block of code, you will see the following printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/28154366-27a3-495a-951d-5ac1a97d1e3f.png" style="width:41.75em;height:19.17em;"/></p>
<ol start="4">
<li>We can now train our model. To train our model, we use the generated <kbd>train</kbd> dataset. We will run our model for <kbd>100</kbd> rounds initially and just take one time step every round. We will set the verbose argument to print the results of every round. We train our LSTM model using the following code:</li>
</ol>
<pre style="padding-left: 60px">history &lt;- model %&gt;% fit_generator(<br/>  train_gen,<br/>  epochs = 100,<br/>  steps_per_epoch=1,<br/>  verbose=2<br/>)</pre>
<ol start="5">
<li>Now that the model has been trained, we can make our predictions. In the <kbd>predict</kbd> step, we choose a given number of time steps for both the <kbd>train</kbd> and <kbd>test</kbd> data. Afterward, we can compare these predictions with the actual values. We use our LSTM model to predict stock prices by running the following code:</li>
</ol>
<pre style="padding-left: 60px">testpredict &lt;- predict_generator(model, test_gen, steps = 200)<br/>trainpredict &lt;- predict_generator(model, train_gen, steps = 1200)</pre>
<p style="padding-left: 60px">After running the preceding code, we now have our predictions.</p>
<ol start="6">
<li>Our next step will be to plot these together with the actual values to see how well our model worked. This step will require converting our data between a number of formats. Our first step will be to convert our vector of prediction to an <kbd>xts</kbd> object. In order to do this, we need to define the index values. We will use the <kbd>4</kbd> through <kbd>1203</kbd> index values for our <kbd>trainpredict</kbd> data. The reason that we start at four is because we have three lagging values being used to predict the value at the fourth index point. We will do the same for the test, but we start at the <kbd>1263</kbd> index point. We create our <kbd>xts</kbd> data objects from our predictions by running the following code:</li>
</ol>
<pre style="padding-left: 60px">trainpredict &lt;- data.frame(pred = trainpredict)<br/>rownames(trainpredict) &lt;- index(closing_deltas)[4:1203]<br/>trainpredict &lt;- as.xts(trainpredict)<br/><br/>testpredict &lt;- data.frame(pred = testpredict)<br/>rownames(testpredict) &lt;- index(closing_deltas)[1262:1461]<br/>testpredict &lt;- as.xts(testpredict)</pre>
<ol start="7">
<li>Now, we add the values from these <kbd>xts</kbd> objects to our <kbd>closing_deltas</kbd> object. Next, we plot our actual values, along with overlaying our predicted values. In order to do this, we first add columns for all the NAs and then populate just the rows that match the index points reflected in the prediction objects. We add additional columns to our <kbd>closing_delta</kbd> and <kbd>xts</kbd> objects, reflecting the predictions on the <kbd>train</kbd> and <kbd>test</kbd> sets by running the following code:</li>
</ol>
<pre style="padding-left: 60px">closing_deltas$trainpred &lt;- rep(NA,1507)<br/>closing_deltas$trainpred[4:1203] &lt;- trainpredict$pred<br/><br/>closing_deltas$testpred &lt;- rep(NA,1507)<br/>closing_deltas$testpred[1262:1461] &lt;- testpredict$pred</pre>
<ol start="8">
<li>Now that we have the predictions merged together with the actual data, we can plot the results. We plot predictions as solid dark lines over the actual values represented by light gray, dotted lines using the following code:</li>
</ol>
<pre style="padding-left: 60px">plot(as.zoo(closing_deltas), las=1, plot.type = "single", col = c("light gray","black","black"), lty = c(3,1,1))</pre>
<p style="padding-left: 60px">After running this code, you will see the following plot in your <strong>Plots</strong> tab:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/555809c0-ce1e-4060-8386-589e250971cf.png" style="width:43.92em;height:17.33em;"/></p>
<p style="padding-left: 60px">While the predicted results are a little conservative, note how the model is detecting the nuance at various time points. This model is finding more patterns and resulting in output with more movement than our ARIMA model.</p>
<ol start="9">
<li>In addition to plotting our data, we can also print the results of calling the <kbd>evaluate_generator</kbd> <span>function </span>to calculate the error rate. To print the error rate for our model, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">evaluate_generator(model, test_gen, steps = 200)<br/>evaluate_generator(model, train_gen, steps = 1200)</pre>
<p style="padding-left: 60px">After running the preceding code, we see the following error rate values:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1fd80fda-9471-4c1b-bf3a-aeeaf1349cae.png" style="width:38.83em;height:22.25em;"/></p>
<p>The warnings printed in the console can be ignored. At the time of writing, this is a known issue with TensorFlow through Keras. Our LSTM model so far is fairly minimal. Let's take a look at tuning some hyperparameters next. We will see what we can tune to try to achieve better performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters to improve performance</h1>
                </header>
            
            <article>
                
<p>To improve our model, we will now tune our hyperparameters. There are a number of options for tuning our LSTM model. We will focus on adjusting the length value when creating the time-series data with our data generator. In addition, we will add additional layers, adjust the number of units in the layer, and modify our optimizer.</p>
<p>We will do so using the following steps:</p>
<ol>
<li>To get started, let's switch the value that we pass to the <kbd>length</kbd> argument in the <kbd>timeseries_generator</kbd> function from <kbd>3</kbd> to <kbd>10</kbd> so that our model has a longer window of prices to use for forecasting calculations. To make this change, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">train_gen &lt;- timeseries_generator(<br/>  closing_deltas,<br/>  closing_deltas,<br/>  <strong>length = 10,</strong><br/>  sampling_rate = 1,<br/>  stride = 1,<br/>  start_index = 1,<br/>  end_index = 1258,<br/>  shuffle = FALSE,<br/>  reverse = FALSE,<br/>  batch_size = 1<br/>)<br/><br/>test_gen &lt;- timeseries_generator(<br/>  closing_deltas,<br/>  closing_deltas,<br/>  <strong>length = 10,</strong><br/>  sampling_rate = 1,<br/>  stride = 1,<br/>  start_index = 1259,<br/>  end_index = 1507,<br/>  shuffle = FALSE,<br/>  reverse = FALSE,<br/>  batch_size = 1<br/>)</pre>
<p style="padding-left: 60px">We have kept this code the same as we did earlier with just the one change to <kbd>length</kbd>.</p>
<ol start="2">
<li>Next, we will make our LSTM model deeper by adding an additional LSTM layer, dropout layer, and one additional dense layer. We will also change the input shape to reflect the next <kbd>length</kbd> parameter in the generator. Lastly, we set <kbd>return_sequences</kbd> to <kbd>True</kbd> in the first layer so that the signal can flow through to the additional layers. Without setting this to <kbd>True</kbd>, you will get an error related to the expected and actual dimensions of the data entering the second LSTM layer. We add additional layers to our LSTM model by running the following code:</li>
</ol>
<pre style="padding-left: 60px">model &lt;- keras_model_sequential()<br/><br/>model %&gt;%<br/> layer_lstm(units = 256,input_shape = c(10, 1),return_sequences="True") %&gt;%<br/> layer_dropout(rate = 0.3) %&gt;%<br/> layer_lstm(units = 256,input_shape = c(10, 1),return_sequences="False") %&gt;%<br/> layer_dropout(rate = 0.3) %&gt;%<br/> layer_dense(units = 32, activation = "relu") %&gt;%<br/> layer_dense(units = 1, activation = "linear")</pre>
<ol start="3">
<li>Our last modification will be to make a change to the optimizer. In this case, we will lower the learning rate for our optimizer. We do this to avoid any major spikes in our predicted values. We can adjust our optimizer by running the following code:</li>
</ol>
<pre style="padding-left: 60px">model %&gt;%<br/>  compile(<br/>    optimizer = optimizer_adam(<strong>lr = 0.001</strong>), <br/>    loss = 'mse',<br/>    metrics = 'accuracy')<br/><br/>model</pre>
<p style="padding-left: 60px">After running the preceding code, the following will be printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f8a77053-d0f5-46d4-82b4-42f6894d5866.png" style="width:35.08em;height:22.00em;"/></p>
<ol start="4">
<li>Next, we can train our model as before using the following code:</li>
</ol>
<pre style="padding-left: 60px">history &lt;- model %&gt;% fit_generator(<br/> train_gen,<br/> epochs = 100,<br/> steps_per_epoch=1,<br/> verbose=2<br/>)</pre>
<ol start="5">
<li>After training our model, we can evaluate how well it performed and compare this with our first model. We evaluate our model and print the results to our console using the following code:</li>
</ol>
<pre style="padding-left: 60px">evaluate_generator(model, train_gen, steps = 1200)<br/>evaluate_generator(model, test_gen, steps = 200)</pre>
<p style="padding-left: 60px">When we run this code, we see the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d8a8d662-0e64-4421-b651-58c7fec386ff.png" style="width:33.17em;height:26.25em;"/></p>
<p class="CDPAlignLeft CDPAlign">Our modifications have produced mixed results. While the loss value for the <kbd>train</kbd> data is slightly worse, the error rate for the <kbd>test</kbd> data is improved.</p>
<p>At this point, we have walked through all the steps necessary to create an LSTM model and have taken one pass at adjusting parameters to improve performance. Creating deep learning models is often as much an art as it is a science, so we encourage you to continue making adjustments and seeing whether you can further improve on the model. You may want to try different optimizers other than <kbd>adam</kbd> or experiment with including additional hidden layers. With these foundations in place, you are ready to make additional changes or apply this approach to a different dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started by creating a baseline model to predict stock prices. To do this, we used an ARIMA model. Based on this model, we explored some important components of machine learning with time-series data, including using lagging variable values to predict a current variable value and the importance of stationarity. From there, we built a deep learning solution using Keras to assemble LSTM and then tuned this model further. In the process, we observed that this deep learning approach has some marked advantages compared to other traditional models, such as ARIMA. In the next chapter, we will use a <span>generative adversarial network to create a synthetic face image.</span></p>


            </article>

            
        </section>
    </body></html>