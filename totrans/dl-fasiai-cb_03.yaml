- en: '*Chapter 3*: Training Models with Tabular Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to ingest various kinds of datasets
    using fastai and how to clean up datasets. In this chapter, we are going to get
    into the details of training a model with fastai using tabular data. **Tabular
    data**, which is data organized in rows and columns that you would find in a spreadsheet
    file or a database table, is critical to most businesses. The fastai framework
    acknowledges the importance of tabular data by providing a full suite of features
    to support deep learning applications based on tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: To explore deep learning with tabular data in fastai, we will return to the
    `ADULT_SAMPLE` dataset, one of the datasets we examined in [*Chapter 2*](B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057)*,
    Exploring and Cleaning Up Data with fastai*. By using this dataset, we will train
    a deep learning model, while also learning about the `TabularDataLoaders` (used
    to define the training and test datasets) and `tabular_learner` (used to define
    and train the model) objects.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at datasets outside the set of curated datasets to learn how
    we can ingest non-curated datasets to train deep learning models in fastai. We
    will wrap up this chapter by exploring what makes a tabular dataset a decent candidate
    for training a fastai deep learning model and how to save a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a model in fastai with a curated tabular dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model in fastai with a non-curated tabular dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model with a standalone dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing whether a tabular dataset is a good candidate for fastai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving a trained tabular model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensure that you have completed the setup sections in [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, and have a working Gradient instance or Colab setup.
    Ensure that you have cloned the repository for this book ([https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook](https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook))
    and have access to the `ch3` folder. This folder contains the code samples described
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model in fastai with a curated tabular dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057), *Exploring
    and Cleaning Up Data with fastai*, you learned how to ingest and examine the `ADULT_SAMPLE`
    curated tabular dataset. In this recipe, we will go through the process of training
    a deep learning model on this dataset using fastai. This will give you an overview
    of the *happy path* to creating a tabular deep learning model with fastai. The
    goal of this recipe is to use this dataset to train a deep learning model with
    fastai, which predicts whether the person described in a particular record will
    have a salary above or below 50k.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_tabular_datasets.ipynb` notebook
    in the `ch3` directory of your repository.
  prefs: []
  type: TYPE_NORMAL
- en: I am grateful for the opportunity to include the ADULT_SAMPLE dataset featured
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Ron Kohavi. (1996) *Scaling Up the Accuracy of Naive-Bayes Classifers: a Decision-Tree
    Hybrid* ([http://robotics.stanford.edu/~ronnyk/nbtree.pdf](http://robotics.stanford.edu/~ronnyk/nbtree.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, you will be running through the `training_with_tabular_datasets.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Define transforms`, `dependent variable`,
    `continuous and categorical columns` cell. By running these cells, you will be
    setting up the notebook and ingesting the `ADULT_SAMPLE` curated tabular dataset
    into a pandas DataFrame, which you will use through the rest of this notebook.
    These cells are identical to the ones in the notebook shown in [*Chapter 2*](B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057),
    *Exploring and Cleaning Up Data with fastai*, for examining tabular curated datasets
    `examining_tabular_datasets.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the first new cell in the notebook with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This cell sets the following values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `procs`: This is a list of the transformations that will be applied in the
    `TabularDataLoaders` object. `FillMissing` specifies that missing the values in
    a column will be replaced with the median value for the column. `Categorify` specifies
    that the values in categorical columns will be replaced with numeric identifiers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `dep_var`: This will be used to identify which column in the dataset contains
    the dependent variable. This column is also known as the target value or the y
    value for the model – it is the value that the trained model will predict. For
    this model, we are predicting the value for the `salary` column.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `cont` and `cat`: These are lists of continuous and categorical columns
    from the `df` DataFrame that are returned by `cont_cat_split`, respectively. This
    function is a major benefit that fastai provides for tabular deep learning models.
    It saves a lot of repetitive coding by automatically detecting which columns are
    continuous (that is, can take on an unlimited set of values, such as currency
    amounts, physical dimensions, or counts of objects) or categorical (that is, can
    only take a finite set of distinct values, such as states of the US or days of
    the week).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell with the following code to define the `TabularDataLoaders`
    object called `dls`. Note that some of the parameters specified in the definition
    of this object (such as batch size) are usually associated with the training process
    rather than defining the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The definition of the `TabularDataLoaders` object uses the following arguments:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `df, path, procs`: The DataFrame containing the ingested dataset, the path
    object for the dataset, and the list of transformations defined in the previous
    step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `cat_names, cont_names`: The lists of categorical and continuous columns
    defined in the previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `y_names`: The column containing the dependent variable/target values.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `valid_idx`: The index values of the subset of rows of the `df` DataFrame
    that will be reserved as the validation dataset for the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `bs`: Batch size for the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell with the following code to define and train the model. The first
    line specifies that the model is being defined using the `TabularDataLoaders`
    object, which was defined in the previous step with the default number of layers,
    and using accuracy as the metric that was optimized in the training process. The
    second line triggers the training process for three epochs; that is, three iterations
    through the entire dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell will be the training results by epoch. The results
    include the epoch''s number, the training loss, the validation loss, and the elapsed
    time for each epoch:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Training results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.1 – Training results
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the following cell to get a sample result of the trained model's predictions.
    You can compare the `salary` column with the `salary_pred` column (the prediction
    made by the model – highlighted in the preceding screenshot) to get a snapshot
    of how the model performed for this sample of rows from the dataset. In this sample
    set, the model's predictions match the actual values of the dependent variable
    in the `salary` column:![Figure 3.2 – Output of show_results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.2 – Output of show_results
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get a summary of the structure of the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell includes the following details:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) A list of all the layers that make up the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – List of layers that make up the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – List of layers that make up the model
  prefs: []
  type: TYPE_NORMAL
- en: 'b) The parameters in the trained model, the optimizer and loss functions, and
    the callbacks used. Callbacks ([https://docs.fast.ai/callback.core.html](https://docs.fast.ai/callback.core.html))
    specify actions to be taken during the training process, such as stopping the
    training process prior to executing all the epochs. In the case of this trained
    model, callbacks are automatically specified by fastai to track the number of
    epochs done (`TrainEvalCallback`), track losses and metrics by batch/epoch (`Recorder`),
    and display the progress bars shown while the model is being trained (`ProgressCallBack`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Additional details in the output of summary()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Additional details in the output of summary()
  prefs: []
  type: TYPE_NORMAL
- en: You have now completely trained a deep learning model with a curated tabular
    dataset using fastai.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you saw in this recipe, once the data has been ingested, it only takes a
    few lines of code with fastai to get a trained deep learning model. The simplicity
    and compactness of fastai code is partially down to fastai making reasonable default
    choices when possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, fastai determines that when the target column is categorical,
    the model should be predicting choices in a category (in the case of this model,
    a binary choice of whether a person''s salary is above or below 50 k) rather than
    continuous values. The following are additional benefits provided by fastai for
    training deep learning models with tabular data:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting which columns in a DataFrame are categorical or continuous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting appropriate callbacks for the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the layers for the deep learning model (including embedding layers
    for the categorical columns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's compare what we would have to do for a Keras deep learning model. In a
    Keras deep learning model for a tabular dataset, each of these characteristics
    of the model would have to be explicitly coded, resulting in longer, more complex
    code to create the model. In addition, somebody learning about deep learning models
    for tabular datasets would face a bigger challenge by needing to deal with many
    more details to get their first working trained model. The bottom line is that
    fastai makes it possible to get to a basic deep learning model faster.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next recipe, it's worth digging into the model from
    this recipe because the other recipes in this chapter will follow the same pattern.
    A deeply detailed description of the model is beyond the scope of this book, so
    we will just focus on some highlights here.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the recipe, the model is defined as a `tabular_learner` object
    (documentation here: [https://docs.fast.ai/tabular.learner.html](https://docs.fast.ai/tabular.learner.html)).
    This object is a specialization of the fastai `learner` object which you first
    saw in the *Understanding the world in four applications: tables, text, recommender
    systems, and images* section of [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019)*,
    Getting Started with fastai*. You can see the structure of this model from the
    output of `learn.summary()`. The beginning of the output specifies `Input shape:
    [''64 x 9'', ''64 x 6'']` - the second dimension of the first element corresponds
    with the number of categorical columns, and the second dimension of the second
    element corresponds with the number of continuous columns. There are embedding
    layers (documentation here: [https://docs.fast.ai/layers.html#Embeddings](https://docs.fast.ai/layers.html#Embeddings))
    defined for the categorical columns (the columns in the `cat` list) and a `BatchNorm`
    layer (documentation here: [https://docs.fast.ai/layers.html#BatchNorm-layers](https://docs.fast.ai/layers.html#BatchNorm-layers))
    for the continuous columns.'
  prefs: []
  type: TYPE_NORMAL
- en: To see additional details about the structure of the model, see the output of
    `learn.model`. In particular, you can see that for the final `Linear` layer, `out_features
    = 2`, corresponds with the binary output of the model (the individual's income
    is above/below 50 k).
  prefs: []
  type: TYPE_NORMAL
- en: Training a model in fastai with a non-curated tabular dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057), *Exploring
    and Cleaning Up Data with fastai*, you reviewed the curated datasets provided
    by fastai. In the previous recipe, you created a deep learning model that had
    been trained on one of these curated datasets. What if you want to train a fastai
    model for a tabular dataset that is not one of these curated datasets?
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will go through the process of ingesting a non-curated dataset
    – the Kaggle house prices dataset ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data))
    – and training a deep learning model on it. This dataset presents some additional
    challenges. Compared to a curated fastai dataset, there are additional steps required
    to ingest the dataset, and its structure requires special handling to deal with
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this recipe is to use this dataset to train a deep learning model,
    that then predicts whether a house has a sale price that is above or below the
    average price for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe you will need a Kaggle ID. If you don''t already have
    one, you can get one here: [https://www.kaggle.com/account/login](https://www.kaggle.com/account/login).
    Once you have your Kaggle ID, complete the following steps to get the token for
    accessing the Kaggle house prices dataset from within your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in with your Kaggle ID, click on your account (top right), and then click
    on **Account**:![Figure 3.5 – Kaggle account menu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Kaggle account menu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the `kaggle.json` file will be downloaded on your local system:![Figure 3.6
    – Selecting Create New API Token to get a new Kaggle API token
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – Selecting Create New API Token to get a new Kaggle API token
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In your Gradient environment, open a Terminal session, go to the `/root` directory,
    and create a new directory called `.kaggle`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the `kaggle.json` file that you downloaded in *Step 2* to the new directory
    that you just created; that is, `/root/.kaggle`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps will prepare you to use the Kaggle house price dataset ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data))
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: I am grateful for the opportunity to include the house price dataset featured
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Dean De Cock (2011). Ames, Iowa: Alternative to the Boston Housing Data as
    an End of Semester Regression Project ([http://jse.amstat.org/v19n3/decock.pdf)](http://jse.amstat.org/v19n3/decock.pdf)
    Journal of Statistics Education Volume 19, Number 3(2011), ([www.amstat.org/publications/jse/v19n3/decock.pdf](http://www.amstat.org/publications/jse/v19n3/decock.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, you will be running through the `accessing_non_curated_datasets.ipynb`
    notebook, as well as the fastai dataset documentation, to understand the datasets
    that fastai curates. Once you have the notebook open in your fastai environment,
    complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not already done so, install the `kaggle` library by running the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the first three cells of the notebook to load the libraries that you will
    need for this recipe and prepare the notebook for fastai.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your Gradient environment, go to the `/root/.kaggle` directory and open
    the `kaggle.json` file. The contents of this file should look like this, with
    your ID and your 32-character key as the first and second values, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the contents of your `kaggle.json` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your copy of the `accessing_non_curated_datasets.ipynb` notebook, paste
    the content of the `kaggle.json` file into single quotes to assign the value to
    the variable creds. Then, run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to set the credential path for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to set the path for your dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to create the target directory for the dataset, download the
    dataset, unzip the dataset in the target directory, and list the contents of the
    target directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the `path.ls()` function shows the structure of the dataset.
    In this recipe, we will use `train.csv` to train the deep learning model and then
    use `test.csv` to exercise the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to ingest the `train.csv` file into a pandas DataFrame called
    `df_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to ingest the `test.csv` file into a pandas DataFrame called
    `df_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the `shape` command to get the dimensions of `df_train` and `df_test`. Notice
    that `df_test` has one fewer columns than `df_train` – can you think of why this
    would be the case and which column is missing from `df_test`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this cell to define the `under_over()` function, which returns `''0''`
    if the input value is less than the mean and `''1''` if not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to use the `under_over()` function, which will replace the values
    in the `SalePrice` column with indicators of whether the value was above or below
    the average for the column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you display the contents of the `df_train` DataFrame, you will see that
    the values in the `SalePrice` column have been replaced with zeros and ones:![Figure
    3.7 – The values in the SalePrice column have been replaced
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.7 – The values in the SalePrice column have been replaced
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to see the count of each of the new values in the `SalePrice`
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to define the transformation to apply to the dataset, the column
    that contains the dependent variable (target), and the continuous and categorical
    columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This cell sets the following values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `procs`: This is a list of the transformations that will be applied to the
    `TabularDataLoaders` object. `FillMissing` specifies that the missing values in
    a column are replaced with the median value for the column. `Categorify` specifies
    that the values in the categorical columns are replaced with numeric identifiers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `dep_var`: This will be used to identify which column in the dataset contains
    the dependent variable; that is, the column that contains the value that we want
    the model to predict. For this model, we are predicting the value for the `SalePrice`
    column.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `cont` and `cat`: These are lists of continuous and categorical columns
    from the `df_train` DataFrame returned by using `cont_cat_split`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to check for missing values in the `df_train` DataFrame. The
    first line gets a count of missing values in the DataFrame, while the second line
    defines a new DataFrame, `df_train_missing`, that has a row for each column in
    the original DataFrame that has at least one missing value. The columns of this
    DataFrame are the names of the columns with missing values, the missing value
    count for each column, and the proportion of values that are missing in the column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the values in `df_train_missing`, we can see that some columns have
    a large number of missing values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Rows from df_train_missing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.8 – Rows from df_train_missing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to deal with the missing values in the `df_train` and `df_test`
    DataFrames. The first two statements replace the missing values in the categorical
    columns with the most common non-missing value, while the second two statements
    replace the missing values in the continuous columns with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to check for missing values in the `df_train` DataFrame once
    more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, when you check the contents of the `df_train_missing` DataFrame, it will
    be empty, confirming that all the missing values have been dealt with:![Figure
    3.9 – Confirmation that the missing values have been dealt with
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.9 – Confirmation that the missing values have been dealt with
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to create a `TabularDataLoaders` object. The first line defines
    the transformation procedures to be applied in the `TabularDataLoaders` object,
    while the second line defines the `TabularDataLoaders` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `TabularDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `procs`: This is a list of the transformations that will be applied to the
    `TabularDataLoaders` object. `Normalize` specifies that the values are all scaled
    to a consistent range. `Categorify` specifies that the values in the categorical
    columns are replaced with numeric identifiers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `df_train, path, procs`: The DataFrame containing the ingested dataset,
    the path object for the dataset, and the list of transformations defined in the
    previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `cat_names, cont_names`: The lists of categorical and continuous columns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `y_names`: The column containing the dependent variable/target values.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `valid_idx`: The index values of the subset of rows of the `df` DataFrame
    that will be reserved as the validation dataset for the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f) `bs`: The batch size for the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to define and train the deep learning model. The first line specifies
    that the model is being created using the `TabularDataLoaders` object we defined
    in the previous step, with the default number of layers and accuracy as the metrics
    being optimized in the training process. The second line triggers the training
    process for `5` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training process produces an output that shows the training loss, validation
    loss, and accuracy for each epoch. This means we have trained a model that can
    predict whether the cost of a given property from the validation set is above
    or below the average with 92% accuracy. Can you think of some reasons why the
    accuracy of this model is somewhat better than the accuracy of the model you trained
    in the previous recipe?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Results of training on the non-curated dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.10 – Results of training on the non-curated dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can run this cell to apply the trained model to the test dataset. Note
    that because of the structure of this dataset, the test dataset does not include
    y-dependent values, which means that while you can apply the model to the test
    records, you don''t have any way of assessing the accuracy of the predictions
    that the model makes on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to get a sample of predictions that the trained model makes on
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `learn.show_results()` lets you see the result of applying the
    trained model to the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – The subset of results from applying the model trained to a
    non-curated dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – The subset of results from applying the model trained to a non-curated
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: You have now gone through the process of using a non-curated dataset to train
    a fastai deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you learned how to adapt to a different kind of dataset where
    the train and test data is separate. In this case, you cannot rely on the transformations
    in the `TabularDataLoaders` object to deal with missing data. That's why the code
    associated with this recipe deals with the missing values in each of the train
    and test datasets individually.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model with a standalone dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes in this chapter, we looked at training fastai models
    on a curated tabular dataset and a dataset directly loaded from Kaggle. In this
    recipe, we are going to examine how to train a model with a dataset that is from
    a self-standing file. The dataset we will use in this recipe is made up of property
    listings in Kuala Lumpur, Malaysia and is available from the Kaggle site at [https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur](https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is not like the tabular datasets we have seen so far. The datasets
    we have already encountered have been well-behaved and have only required a small
    amount of cleanup. The Kualu Lumpur property dataset, by contrast, is a real-world
    dataset. In addition to missing values, it contains many errors and irregularities.
    It is also large enough (over 50k records) to give deep learning a decent chance
    to be useful on it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure you have followed the steps in [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, so that you have a fastai environment set up. Confirm
    that you can open the `training_model_standalone_tabular_dataset.ipynb` notebook
    in the `ch3` directory of your repository. Also, ensure that you have uploaded
    the data file by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download `data_kaggle.csv.zip` from [https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur](https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the downloaded file to extract `data_kaggle.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the Terminal in your Gradient environment, make your current directory
    `/storage/archive`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a folder called `/storage/archive/kl_property:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload `data_kaggle.csv` to `/storage/archive/kl_property`. You can use the
    upload button in JupyterLab in Gradient to do the upload, but you need to do so
    by performing several steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) From the Terminal in your Gradient environment, make `/notebooks` your current
    directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'temp your current folder, select the upload button, as shown in the following
    screenshot, and select the data_kaggle.csv file from your local system folder
    where you extracted it in *step 2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.12 – Upload button in JupyterLab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Upload button in JupyterLab
  prefs: []
  type: TYPE_NORMAL
- en: 'd) From the Terminal in your Gradient environment, copy `data_kaggle.csv` into
    `/storage/archive/kl_property`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: I want to acknowledge the dataset featured in this section and express my gratitude
    for the opportunity to include it in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: Jas S (2019). *Property Listings in Kuala Lumpur* ([https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur](https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur))
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to go about it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will be running through the `training_model_standalone_tabular_dataset.ipynb`
    notebook to train a model using the Kuala Lumpur property prices dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the notebook open in your fastai environment, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the first three cells to import the necessary libraries and set up the notebook
    for fastai.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this cell to associate `path` with the directory that you copied the `data_kaggle.csv`
    file into:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to ingest the dataset into the `df_train` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to see the first few rows of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df_train.head()` shows a sample of rows from the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – A sample of rows from the Kuala Lumpur property dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.13 – A sample of rows from the Kuala Lumpur property dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note the values in the `Price` column. The goal of this recipe is to train
    a deep learning model to predict whether values in this column are above or below
    the average. To do this, we need to start with numeric values in this column.
    You can see three problems just from this small sample of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The values include `RM`, the symbol for the ringgit, the Malaysian currency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The values include a comma thousand separator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Some rows have a missing value in this column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Over the next few cells of the notebook, we will deal with these problems so
    that we end up with this column containing entirely valid numeric values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to get the number of rows in the dataset. The first element of
    the output is the number of rows in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the cells to define the `remove_currency()` and `remove_after_space()` functions.
    You will need these functions to clean up the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this cell to address the problems with the `Price` column demonstrated
    by the output of `df_train.head()`. This cell drops rows in the dataset where
    the `Price` value is missing (first statement), removes the currency symbol from
    the `Price` column (second statement), and converts the values in the `Price`
    column into numeric values after removing any commas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run this cell to get the number of rows in the dataset again. It's important
    to know this number because we have eliminated some rows from the dataset in the
    last cell, and we will remove more as we clean up the `Size` column. By checking
    the shape of the DataFrame as we are removing rows, we can be sure we're not losing
    too much information:![Figure 3.14 – Getting the shape of the DataFrame prior
    to dropping rows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.14 – Getting the shape of the DataFrame prior to dropping rows
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `Size` column has a lot of useful information in it, but we have to do
    some work to get it ready to help train a deep learning model. To start with,
    run the following cell to see some examples of the values in the `Size` column.
    Here is the information we want to extract from this column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Extract the prefix (for example, `Built-up`, `Land area`, and so on) into
    a new column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Get a single numeric value for the remainder of the contents of the column;
    that is, replace 1,335 sq. ft. with 1,335 and replace 22 x 80 sq. ft. with 1,760.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Drop rows where the suffix in the `Size` column cannot yield a numeric value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `df_train[''Size''].head()` shows examples of the values in the
    `Size` column:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Examples of values in the Size column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.15 – Examples of values in the Size column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the operations we want to perform on the `Size` column to prepare
    it to train a deep learning model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Extract the prefix (for example, `Built-up`, `Land area`, and so on) into
    a new column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Where possible, get a single numeric value for the remainder of the contents
    of the `Size` column. For example, we want to replace 1,335 sq. ft. with 1,335
    and replace 22 x 80 sq. ft. with 1,760.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) For rows where it is not possible to get a numeric value from the remainder
    of the contents of the `Size` column, drop the row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell to define the `clean_up_size()` function. You will use this function
    to perform the following set of cleanup steps on the `Size` column. The result
    will be a DataFrame where all values in the `Size` column are numeric values representing
    the area of the property. Here are some of the transformations that are performed
    by the `clean_up_size()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Lowercase all the values in the `Size` column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Split the `Size` column into a new column (`Size_type`) that contains the
    non-numeric information and a remainder `Size` column that contains the numeric
    information about the area of the property.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Replace the missing values in the new `Size` column with `0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Remove the rows that do not contain any digits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Remove the rows that contain problematic substrings, as listed in `clean_up_list`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Replace extraneous characters so that all the `Size` entries are either numeric
    or of the `numerica * numericb` form.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: g) Replace values of the `numerica * numericb` form with the product of the
    two values; that is, `numerica` and `numericb`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to execute the `clean_up_size()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the `shape` command again to get the shape of the DataFrame after dropping
    the rows that don't contain enough data to be useful. This confirms that we lost
    about 2% of the rows from the original dataset after removing the offending rows:![Figure
    3.16 – Getting the shape of the DataFrame after cleanup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.16 – Getting the shape of the DataFrame after cleanup
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run `df_train.head()` to see a sample of the DataFrame after the cleanup steps.
    Note that now, we have the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) There is a new `Size_type` column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The `Size` column contains numeric values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `df_train.head()` shows what the DataFrame looks like after the
    `Price` and `Size` columns have been cleaned up:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17 – The DataFrame after performing the cleanup steps on the Price
    and Size columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.17 – The DataFrame after performing the cleanup steps on the Price
    and Size columns
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the cell to define the `under_over()` function. You will run this function
    to replace the values in the `Price` column with an indicator of whether the price
    is under or over the average price.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to replace the values in the `Price` column with indicators
    of whether the price is above or below the average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this cell, the values in the `Price` column will be replaced:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18 – The DataFrame after replacing the Price values with under/over
    average indicators'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.18 – The DataFrame after replacing the Price values with under/over
    average indicators
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to define the transformations to be applied to `TabularDataLoaders`
    object, the target column (`Price`), and the continuous and categorical column
    lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to define the `TabularDataLoaders` object with the arguments
    that you have defined so far in the notebook, including the dataset (`df_train`),
    the list of transformations (`procs`) to be applied to the dataset, the continuous
    and categorical column lists (`cont` and `cat`), and the dependent variable (`dep_var`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run this cell to fit the model and see the model's performance. Your accuracy
    and loss may be slightly different, but you should see over 90% accuracy, which
    is good for a model that's been trained on fewer than 100k records:![Figure 3.19
    – Results of fitting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.19 – Results of fitting the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run this cell to see the results in the validation set. You will see that for
    this set of results, the model correctly predicts whether a property will have
    a price above or below the average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Model predictions of whether properties will have a price above
    or below the average'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – Model predictions of whether properties will have a price above
    or below the average
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained a deep learning model with fastai on a dataset
    that required non-trivial cleanup before it could be used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you saw that while fastai provides utilities that make it easier
    to train a deep learning model on a tabular dataset, you still have to ensure
    that the dataset is capable of training the model. This means that non-numeric
    values need to be removed from numeric columns. This may require iterative cleanup
    steps, as you saw while working through the notebook featured in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset featured in this recipe contains the kind of anomalies and inconsistencies
    that are typical of real-world datasets, so the techniques you exercised in this
    recipe (including pandas manipulations to remove rows that had problematic values
    in certain columns, as well as string replacement techniques) will be applicable
    to other real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Even with a messy dataset, fastai makes it easy to get a high performing deep
    learning model, thanks to it picking intelligent defaults and automating key operations
    (such as identifying categorical and continuous columns and dealing with missing
    values).
  prefs: []
  type: TYPE_NORMAL
- en: Assessing whether a tabular dataset is a good candidate for fastai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have created three deep learning models for tabular
    datasets using fastai. But what if you want to determine whether a new dataset
    is a good candidate for training a deep learning model with fastai? In this recipe,
    we'll go through the process of assessing whether a dataset is a good candidate
    for deep learning with fastai.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure you have followed the steps in [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, to get a fastai environment set up.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you have seen so far in this chapter, you have many choices surrounding
    datasets that could possibly be applied to deep learning. To assess whether a
    dataset is a good candidate, we will go through the process of creating a new
    notebook from scratch and ingesting data from an online API. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new notebook in Gradient. You can do this in Gradient JupyterLab by
    following these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Click on the new launcher button (**+**) in the main window:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Opening New Launcher in JupyterLab'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.21 – Getting a new launcher in JupyterLab
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) When the launcher pane opens, click on **Python 3** to open a new notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22 – The Launcher pane in JupyterLab'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.22 – The launcher pane in JupyterLab
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the new notebook, create and run two new cells with the statements required
    to set up the notebook:![Figure 3.23 – The cells to set up a fastai notebook for
    tabular datasets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.23 – The cells to set up a fastai notebook for tabular datasets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to import the additional libraries required to investigate
    this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to load a dataset of stock prices for the company AstraZeneca
    (stock ticker = `AZN`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check the output of `df.head()` to see the contents of the dataframe:![Figure
    3.24 – Sample of the stock prices dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.24 – Sample of the stock prices dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Get the shape of the dataframe `df` to determine how many rows are in the dataframe.
    Do you think this dataset is big enough to successfully train a deep learning
    model?![Figure 3.25 – Getting the shape of df
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.25 – Getting the shape of df
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the following cell to prepare to check the dataframe for missing values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now confirm the dataset has no missing values![Figure 3.26 – Confirming the
    dataset has no missing values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.26 – Confirming the dataset has no missing values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to set the parameters for the training run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define the TabularDataLoaders object::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define and train the model::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see from the following output that the performance of the model is poor:![Figure
    3.27 – Poor performance when training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_03_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.27 – Poor performance when training the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we want to make some changes to try to get a model that has better performance.
    To start with, run the following cell to define a function to create a new target
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define the new target column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to specify that the new `target` column is the dependent
    variable and to limit `cont`, the set of continuous columns used to train the
    model. Note that in the first model in this section, the dependent variable was
    `Close`, a continuous column. That means the first model was trying to predict
    a continuous value. Because `target` is a categorical column, the new model will
    predict a categorical value rather than a continuous value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to train a new model using the new dependent variable
    and the new set of continuous columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see that this new model has much better performance than the previous
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Improved performance with the second model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.28 – Improved performance with the second model
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you tried two variations on a deep learning model for a dataset
    of stock price information. The first model had a continuous dependent variable
    and used fastai defaults throughout. Unlike the other recipes in this chapter,
    the first model had poor performance. If you attempt to train the first model
    with more epochs you will see that the performance does not improve. The second
    model, where the dependent variable is changed from continuous to categorical,
    has much better performance.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first model in this section was unsuccessful. Attempting to predict a continuous
    value with a deep learning model trained on 1.3 k records is not likely to work.
    Generally speaking, you need a training set that is an order of magnitude bigger,
    in the hundreds of thousands or millions of records, to predict a continuous outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a trained tabular model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have trained a series of fastai deep learning models on tabular datasets.
    These models are available to us in the Python session where we train the model,
    but what can we do to save the models so that we can use them later in a different
    session? In this recipe, we will learn how to save a fastai deep learning model
    to a file and access that model in another Python session.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure you have followed the steps in [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, to get a fastai environment set up. Confirm that
    you can open the `saving_models_trained_with_tabular_datasets.ipynb` and `loading_saved_models_trained_with_tabular_datasets.ipynb`
    notebooks in the `ch3` directory of your repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will be running through the `saving_models_trained_with_tabular_datasets.ipynb`
    notebook to train a model – the same model that you trained in the first recipe
    of this chapter – and save it. Then, you will use the `loading_saved_models_trained_with_tabular_datasets.ipynb`
    notebook to load and exercise a saved fastai model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the `saving_models_trained_with_tabular_datasets.ipynb` notebook
    open in your fastai environment, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Save the trained model` cell. By running
    these cells, you will be ingesting the `ADULT_SAMPLE` curated tabular dataset
    into a pandas DataFrame and training a fastai model on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the next two cells to set the value of the path for the model to a writable
    directory. Ensure that the directory that you set learn.path to exists and is
    writable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this cell to save the trained model to the `adult_sample_model.pkl` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you have saved the trained model into a file, you must load it into
    another notebook to test the process you would go through to retrieve the model
    in a new Python session, and then use the saved model to make a prediction on
    the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `loading_saved_models_trained_with_tabular_datasets.ipynb` notebook
    in a Gradient session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the cells up to the `Load the saved, trained model` cell to load the required
    libraries and set up the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this cell to load the model you saved earlier in this recipe into this
    new notebook. Ensure that you specify the path where you saved the model earlier
    in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to load the test dataset into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run this cell to select the first row of the test dataset and apply the trained
    model to get a prediction for this data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result includes the prediction of the model on this data point. You can
    see that for this data point, the model is predicting a `salary` value of `1.0`,
    which means that it is predicting that this individual will have a salary of over
    50k:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Results of applying the saved model to a test data point'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.29 – Results of applying the saved model to a test data point
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully saved a fastai deep learning model, loaded
    the saved model in a new notebook, and applied the saved model to get a prediction
    on a row of test data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fastai framework includes support to make it easy to save deep learning
    models to your filesystem using the `export()` method of `learner` objects. In
    this recipe, you saw an example of how you can save a trained model to a pickle
    file. You also learned how to load the pickle file back into Python and then apply
    the trained model to a new data example. This is a peek ahead at the process of
    performing inference on a deployed model. In [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178),
    *Deployment and Model Maintenance*, you will see complete examples of performing
    inference on a deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have completed the recipes in this chapter, follow the steps shown
    here to exercise what you have learned. You will do this by adapting one of the
    notebooks you worked through in this chapter so that it works with a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to upload a new tabular dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the site for the Kaggle competition on future sales prediction ([https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data))
    and accept the conditions for the competition to get access to the datasets associated
    with the competition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the `sales_train.csv.zip` and `test.csv.zip` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the downloaded files to extract `sales_train.csv` and `test.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the Terminal in your Gradient environment, make your current directory
    `/storage/archive`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a folder called `/storage/archive/price_prediction`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload `sales_train.csv` and `test.csv` to `/storage/archive/price_prediction`.
    You can use the upload button in JupyterLab in Gradient to do the upload via the
    `/notebooks/temp` directory that you created earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) In the JupyterLab file browser, make `temp` your current folder, select
    the upload button, as shown in the following screenshot, and select `sales_train.csv`
    and `test.csv` from the local system folder where you extracted them in *step
    2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Upload button in JupyterLab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30 – Upload button in JupyterLab
  prefs: []
  type: TYPE_NORMAL
- en: 'b) From the Terminal in your Gradient environment, make `/storage/archive/price_prediction`
    your current directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'c) Copy `sales_train.csv`, `test.csv`, and `data_kaggle.csv` into `/storage/archive/price_prediction`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have uploaded the dataset, it''s time to create a notebook to
    ingest the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a copy of the `training_model_standalone_tabular_dataset.ipynb` notebook
    that you worked through in the *Training a model with a standalone dataset* recipe.
    Call the copy `training_model_new_tabular_dataset.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your new notebook, update the cells that ingest the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df_train.head()` should show you the structure of the dataset.
    You can find a description of the columns of this dataset at [https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data?select=sales_train.csv](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data?select=sales_train.csv):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – A sample of the contents of sales_train.csv'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_03_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31 – A sample of the contents of sales_train.csv
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have ingested another standalone tabular dataset into a
    fastai notebook. You can apply similar techniques to make other tabular datasets
    available so that they can used in fastai solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have ingested the dataset, consider the steps you would take to
    prepare this dataset for training a deep learning model. How would you deal with
    any missing values? Are there tests you can apply to some of the columns to detect
    and correct incorrect values? The Kaggle competition predicts the total sales
    for every product and store for the next month. Completing a fastai model to tackle
    this problem is beyond the scope of this book, but consider how you might refactor
    the dataset to prepare it for this problem.
  prefs: []
  type: TYPE_NORMAL
