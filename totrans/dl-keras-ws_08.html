<html><head></head><body>
		<div>
			<div id="_idContainer159" class="Content">
			</div>
		</div>
		<div id="_idContainer160" class="Content">
			<h1 id="_idParaDest-159"><a id="_idTextAnchor160"/>8. Transfer Learning and Pre-Trained Models</h1>
		</div>
		<div id="_idContainer176" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces the concept of pre-trained models and utilizing them for different applications from those for which they were trained, known as transfer learning. By the end of this chapter, you will be able to apply feature extraction to pre-trained models, exploit pre-trained models for image classification, and apply fine-tuning to pre-trained models to classify images of flowers and cars into their respective classes. We will see that this achieves the same task that we completed in the previous chapter but with greater accuracy and shorter training times.</p>
			<h1 id="_idParaDest-160">Introduction<a id="_idTextAnchor161"/></h1>
			<p>In the previous chapter, we learned how to create a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) from scratch with Keras. We experimented with different architectures by adding more convolutional and Dense layers and changing the activation function. We compared the performance of each model by classifying images of cars and flowers into their respective classes and comparing their accuracies.</p>
			<p>In real-world projects, however, you almost never code a convolutional neural network from scratch. You always tweak and train them as per the requirements. This chapter will introduce you to the important concepts of <strong class="bold">transfer learning</strong> and <strong class="bold">pre-trained networks</strong> (also known as <strong class="bold">pre-trained models</strong>), both of which are used in the industry. </p>
			<p>We will use images and, rather than building a CNN from scratch, we will match these images on pre-trained models to try and classify them. We will also tweak our models to make them more flexible. The models we will use in this chapter are called <strong class="bold">VGG16</strong> and <strong class="bold">ResNet50</strong>, and we will discuss them later in this chapter. Before we start working on pre-trained models, we need to understand transfer learning.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/>Pre-Trained Sets and Transfer Learning</h1>
			<p>Humans learn by experience. We apply the knowledge we gain in one situation to similar situations we face in the future. Suppose you want to learn how to drive an SUV. You have never driven an SUV; all you know is how to drive a small hatchback car.</p>
			<p>The dimensions of the SUV are considerably larger than the hatchback, so navigating the SUV in traffic will surely be a challenge. Still, some basic systems (such as the clutch, accelerator, and brakes) remain similar to that of the hatchback. So, knowing how to drive a hatchback will surely be of great help to you when you are learning to drive the SUV. All the knowledge that you acquired while driving a hatchback can be used when you learn to drive a big SUV.</p>
			<p>This is precisely what transfer learning is. By definition, transfer learning is a concept in machine learning in which we store and use the knowledge gained in one activity while learning another similar activity. The hatchback-SUV model fits this definition perfectly.</p>
			<p>Suppose we want to know whether a picture is of a dog or a cat; here, we can have two approaches. One is building a deep learning model from scratch and then passing on the new pictures to the networks. Another option is to use a pre-trained deep learning neural network model that has already been built by using cats' and dogs' images, instead of creating a neural network from scratch. </p>
			<p>Using the pre-trained model saves us computational time and resources. There can be some unforeseen advantages of using a pre-trained network. For example, almost all the pictures of dogs and cats will have some more objects in the picture, such as trees, the sky, and furniture. We can even use this pre-trained network to identify objects such as trees, the sky, and furniture.</p>
			<p>So, a pre-trained network is a saved network (a neural network, in the case of deep learning) that was trained on a very large dataset, mostly on image classification problems. To work on a pre-trained network, we need to understand the concepts of feature extraction and fine-tuning.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor163"/>Feature Extraction</h2>
			<p>To understand feature extraction, we need to revisit the architecture of a convolutional neural network.</p>
			<p>You may recall that the full architecture of a <strong class="source-inline">CNN</strong>, at a high level, consists of the following components:</p>
			<ul>
				<li>A <strong class="bold">convolution layer</strong></li>
				<li>A <strong class="bold">pooling and flattening layer</strong></li>
				<li>An <strong class="bold">Artificial Neural Network</strong> (<strong class="bold">ANN</strong>)</li>
			</ul>
			<p>The following figure shows a complete CNN architecture:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B15777_08_01.jpg" alt="Figure 8.1: CNN architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1: CNN architecture</p>
			<p>Now, let's divide this architecture into two parts. The first part contains everything but the <strong class="source-inline">ANN</strong>, while the second part only contains the <strong class="source-inline">ANN</strong>. The following figure shows a split <strong class="source-inline">CNN</strong> architecture:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B15777_08_02.jpg" alt="Figure 8.2: CNN split architecture – convolutional base and classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2: CNN split architecture – convolutional base and classifier</p>
			<p>The first part is called a <strong class="bold">convolutional base</strong> while the second part is called the <strong class="bold">classifier</strong>.</p>
			<p>In feature extraction, we keep reusing the convolutional base, and the classifier is changed. So, we preserve the learnings of the convolutional layer, and we can pass different classifiers on top of the convolutional layer. A classifier can be dog versus cat, bikes versus cars, or even medical X-ray images to classify tumors, infections, and so on. The following diagram shows some convolutional base layers that are used for different classifiers:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B15777_08_03.jpg" alt="Figure 8.3: Reusable convolutional base layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3: Reusable convolutional base layer</p>
			<p>The obvious next question is, can't we reuse the classifier too, like the base layer? The general answer is no. The reason is that learning from the convolutional base is likely to be more generic and, therefore, more reusable. However, the learning of the classifier is mostly specific to the classes that the model was trained on. Therefore, it is advisable to only reuse the convolutional base layer and not the classifier.</p>
			<p>The amount of generalized learning from a convolutional base layer depends on the depth of the layer. For example, in the case of a cat, the initial layers of the model learn about general traits such as edges and the background, while the higher layers may learn more about specific details such as eyes, ears, or the shape of the nose. So, if your new dataset is something very different from the original dataset—for example, if you wish to identify fruit instead of cats—then it is better to only use some initial layers of the convolutional base layer rather than using the whole layer.</p>
			<p><strong class="bold">Freezing convolutional layers</strong>: One of the most important features of pre-trained learning is to understand the concept of freezing some layers of a pre-trained network. Freezing essentially means that we stop the process of the weight updating some of the convolutional layers. Since we are using a pre-trained network, it is important to understand that we will need the information stored in the initial layers of the network. If that information is updated in training a network, we might lose some general concepts that have been learned and stored in the pre-trained network. If we add a classifier (<strong class="source-inline">CNN</strong>), many Dense layers on top of the network are randomly initialized, and there may be cases where, due to backpropagation, the learning of the initial layers of the network will be totally destroyed.</p>
			<p>To avoid this information decay, we freeze some layers. This is done by making the layers non-trainable. The process of freezing some layers and training others is called fine-tuning a network.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor164"/>Fine-Tuning a Pre-Trained Network </h1>
			<p>Fine-tuning means tweaking our neural network in such a way that it becomes more relevant to the task at hand. We can freeze some of the initial layers of the network so that we don't lose information stored in those layers. The information stored there is generic and useful. However, if we can freeze those layers while our classifier is learning and then unfreeze them, we can tweak them a little so that they fit even better to the problem at hand. Suppose we have a pre-trained network that identifies animals. If we want to identify specific animals, such as dogs and cats, we can tweak the layers a little bit so that they can learn what dogs and cats look like. This is like using the whole pre-trained network and then adding a new layer that consists of images of dogs and cats. We will be doing a similar activity by using a pre-built network and adding a classifier on top of it, which will be trained on pictures of dogs and cats.</p>
			<p>There is a three-point system to working with fine-tuning:</p>
			<ol>
				<li>Add a classifier (<strong class="source-inline">ANN</strong>) on top of a pre-trained system.</li>
				<li>Freeze the <strong class="source-inline">convolutional base</strong> and train the network.</li>
				<li>Train the added <strong class="source-inline">classifier</strong> and the unfrozen part of the <strong class="source-inline">convolutional base</strong> jointly.</li>
			</ol>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/>The ImageNet Dataset</h2>
			<p>In real practical work experience, you almost never need to build a base convolutional model on your own. You will always use pre-trained models. But where do you get the data from? For visual computing, the answer is ImageNet. The ImageNet dataset is a large visual database that is used in visual object recognition. It consists of more than 14 million labeled images with object names. ImageNet contains more than 20,000 categories.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/>Some Pre-Trained Networks in Keras</h2>
			<p>The following pre-trained networks can be thought of as the base convolutional layers. You use these networks and fit a classifier (ANN):</p>
			<ul>
				<li><strong class="source-inline">VGG16</strong></li>
				<li><strong class="source-inline">Inception V3</strong></li>
				<li><strong class="source-inline">Xception</strong></li>
				<li><strong class="source-inline">ResNet50</strong></li>
				<li><strong class="source-inline">MobileNet</strong></li>
			</ul>
			<p>Different vendors have created the preceding pre-trained networks. For example, <strong class="source-inline">ResNet50</strong> was created by <strong class="source-inline">Microsoft</strong>, while <strong class="source-inline">Inception V3</strong> and <strong class="source-inline">MobileNet</strong> were created by <strong class="source-inline">Google</strong>. In this chapter, we will be working with the <strong class="source-inline">VGG16</strong> and <strong class="source-inline">ResNet50</strong> models.</p>
			<p><strong class="source-inline">VGG16</strong> is a convolutional neural network model with 16 layers and was proposed by K. Simonyan and A. Zisserman from the University of Oxford. The model was submitted to the <strong class="source-inline">ImageNet Large Scale Visual Recognition Challenge</strong> (<strong class="source-inline">ILSVRC</strong>) in 2014—a challenge used to test state-of-the-art models that use the <strong class="source-inline">ImageNet</strong> dataset. <strong class="source-inline">ResNet50</strong> is another convolutional neural network that was trained on the <strong class="source-inline">ImageNet</strong> dataset that has 50 layers and won first place in the <strong class="source-inline">ILSVRC</strong> in 2015.</p>
			<p>Now that we understand what these networks are, we will practice utilizing these pre-trained neural networks to classify an image of a slice of pizza with the <strong class="source-inline">VGG16</strong> model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the exercises and activities in this chapter will be developed in Jupyter notebooks. Please download this book's GitHub repository, along with all the prepared templates, from <a href="https://packt.live/2uI63CC">https://packt.live/2uI63CC</a>.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor167"/>Exercise 8.01: Identifying an Image Using the VGG16 Network</h2>
			<p>We have a picture of a slice of pizza. We will use the <strong class="source-inline">VGG16</strong> network to process and identify the image. Before completing the following steps, ensure you have downloaded the <strong class="source-inline">pizza</strong> image from GitHub and saved it to your working directory:</p>
			<ol>
				<li value="1">Import the libraries:<p class="source-code">import numpy as np</p><p class="source-code">from keras.applications.vgg16 import VGG16</p><p class="source-code">from keras.preprocessing import image</p><p class="source-code">from keras.applications.vgg16 import preprocess_input</p></li>
				<li>Initiate the model (this may take a while):<p class="source-code">classifier = VGG16()</p><p class="callout-heading">Note</p><p class="callout">The last layer of predictions (<strong class="source-inline">Dense</strong>) has 1,000 values. This means that <strong class="source-inline">VGG16</strong> has a total of 1,000 labels and our image will be one out of those 1,000 labels.</p></li>
				<li>Load the image. <strong class="source-inline">'../Data/Prediction/pizza.jpg.jpg'</strong> is the path of the image on our system; it may be different on your system:<p class="source-code">new_image= image.load_img('../Data/Prediction/pizza.jpg', \</p><p class="source-code">                          target_size=(224, 224))</p><p class="source-code">new_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer164" class="IMG---Figure"><img src="image/B15777_08_04.jpg" alt="Figure 8.4: An image of a slice of pizza&#13;&#10;"/></div><p class="figure-caption">Figure 8.4: An image of a slice of pizza</p><p>The target size should be <strong class="source-inline">224x224</strong> since <strong class="source-inline">VGG16</strong> only accepts (<strong class="source-inline">224,224</strong>).</p></li>
				<li>Change the image to an array by using the <strong class="source-inline">img_to_array</strong> function:<p class="source-code">transformed_image = image.img_to_array(new_image)</p><p class="source-code">transformed_image.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(224, 224, 3)</p></li>
				<li>The image has to be in a four-dimensional form for <strong class="source-inline">VGG16</strong> to allow further processing. Expand the dimension of the image, as follows:<p class="source-code">transformed_image = np.expand_dims(transformed_image, axis=0)</p><p class="source-code">transformed_image.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(1, 224, 224, 3)</p></li>
				<li>Preprocess the image using the <strong class="source-inline">preprocess_input</strong> function:<p class="source-code">transformed_image = preprocess_input(transformed_image)</p><p class="source-code">transformed_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer165" class="IMG---Figure"><img src="image/B15777_08_05.jpg" alt="Figure 8.5: A screenshot of image preprocessing&#13;&#10;"/></div><p class="figure-caption">Figure 8.5: A screenshot of image preprocessing</p></li>
				<li>Create the <strong class="source-inline">predictor</strong> variable:<p class="source-code">y_pred = classifier.predict(transformed_image)</p><p class="source-code">y_pred</p></li>
				<li>Check the shape of the image. It should be (<strong class="source-inline">1,1000</strong>). It's <strong class="source-inline">1000</strong> because the <strong class="source-inline">ImageNet</strong> database has <strong class="source-inline">1000</strong> categories of images. The predictor variable shows the probability of our image being one of those images:<p class="source-code">y_pred.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(1, 1000)</p></li>
				<li>Print the top five probabilities of what our image is using the <strong class="source-inline">decode_predictions</strong> function and pass the function of the predictor variable, <strong class="source-inline">y_pred</strong>, and the number of predictions and corresponding labels to output:<p class="source-code">from keras.applications.vgg16 import decode_predictions</p><p class="source-code">decode_predictions(y_pred,top=5)</p><p>The preceding code produces the following output:</p><p class="source-code">[[('n07873807', 'pizza', 0.97680503),</p><p class="source-code">  ('n07871810', 'meat_loaf', 0.012848727),</p><p class="source-code">  ('n07880968', 'burrito', 0.0019428912),</p><p class="source-code">  ('n04270147', 'spatula', 0.0019108421),</p><p class="source-code">  ('n03887697', 'paper_towel', 0.0009799759)]]</p><p>The first column of the array is the internal code number. The second is the possible label, while the third is the probability of the image being the label.</p></li>
				<li>Put the predictions in a human-readable form. Print the most probable label from the output from the result of the <strong class="source-inline">decode_predictions</strong> function:<p class="source-code">label = decode_predictions(y_pred)</p><p class="source-code">"""</p><p class="source-code">Most likely result is retrieved, for example, the highest probability</p><p class="source-code">"""</p><p class="source-code">decoded_label = label[0][0]</p><p class="source-code"># The classification is printed </p><p class="source-code">print('%s (%.2f%%)' % (decoded_label[1], \</p><p class="source-code">      decoded_label[2]*100 ))</p><p>The preceding code produces the following output:</p><p class="source-code">pizza (97.68%)</p></li>
			</ol>
			<p>In this exercise, we predicted an image that says (with <strong class="source-inline">97.68%</strong> probability) that the picture is pizza. Clearly, higher accuracy here means a relatively similar object to our picture is present in the ImageNet database, and our algorithm has successfully identified the image.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dXqdsQ">https://packt.live/3dXqdsQ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3dZMZAq">https://packt.live/3dZMZAq</a>.</p>
			<p>In the following activity, we will put our knowledge to practice by using the <strong class="source-inline">VGG16</strong> network to classify an image of a motorbike.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>Activity 8.01: Using the VGG16 Network to Train a Deep Learning Network to Identify Images</h2>
			<p>You are given an image of a motorbike. Use the <strong class="source-inline">VGG16</strong> network to predict the image. Before you start, ensure that you have downloaded the image (<strong class="source-inline">test_image_1</strong>) to your working directory. To complete this activity, follow these steps:</p>
			<ol>
				<li value="1">Import the required libraries, along with the <strong class="source-inline">VGG16</strong> network.</li>
				<li>Initiate the pre-trained <strong class="source-inline">VGG16</strong> model.</li>
				<li>Load the image that is going to be classified.</li>
				<li>Preprocess the image by applying the transformations.</li>
				<li>Create a predictor variable to predict the image.</li>
				<li>Label the image and classify it.<p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 444.</p></li>
			</ol>
			<p>With that, we have completed this activity. Unlike in <em class="italic">Chapter 7</em>, <em class="italic">Computer Vision with Convolutional Neural Networks</em>, we did not build a <strong class="source-inline">CNN</strong> from scratch. Instead, we used a pre-trained model. We just uploaded a picture that needs to be classified. From this, we can see that, with <strong class="source-inline">84.33%</strong> accuracy, it is predicted to be a moped. In the next exercise, we'll work with an image for which there is no matching image in the ImageNet database.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor169"/>Exercise 8.02: Classifying Images That Are Not Present in the ImageNet Database</h2>
			<p>Now, let's work with an image that is not part of the <strong class="source-inline">1000</strong> labels in our <strong class="source-inline">VGG16</strong> network. In this exercise, we will work with an image of a stick insect, and there are no labels for stick insects in our pre-trained network. Let's see what results we get:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary <strong class="source-inline">Keras</strong> libraries:<p class="source-code">import numpy as np</p><p class="source-code">from keras.applications.vgg16 import VGG16</p><p class="source-code">from keras.preprocessing import image</p><p class="source-code">from keras.applications.vgg16 import preprocess_input</p></li>
				<li>Initiate the model and print a summary of the model:<p class="source-code">classifier = VGG16()</p><p class="source-code">classifier.summary()</p><p><strong class="source-inline">classifier.summary()</strong> shows us the architecture of the network. The following are the points to be noted – it has a four-dimensional input shape (<strong class="source-inline">None, 224, 224, 3</strong>) and it has three convolutional layers. The following figure shows the last four layers of the output:</p><div id="_idContainer166" class="IMG---Figure"><img src="image/B15777_08_06.jpg" alt="Figure 8.6: Summary of the image using the VGG16 classifier&#13;&#10;"/></div><p class="figure-caption">Figure 8.6: Summary of the image using the VGG16 classifier</p><p class="callout-heading">Note</p><p class="callout">The last layer of predictions (<strong class="source-inline">Dense</strong>) has <strong class="source-inline">1000</strong> values. This means that <strong class="source-inline">VGG16</strong> has a total of <strong class="source-inline">1000</strong> labels and that our image will be one out of those <strong class="source-inline">1000</strong> labels.</p></li>
				<li>Load the image. <strong class="source-inline">'../Data/Prediction/stick_insect.jpg'</strong> is the path of the image on our system. It will be different on your system:<p class="source-code">new_image = \</p><p class="source-code">image.load_img('../Data/Prediction/stick_insect.jpg', \</p><p class="source-code">               target_size=(224, 224))</p><p class="source-code">new_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer167" class="IMG---Figure"><img src="image/B15777_08_07.jpg" alt="Figure 8.7: Sample stick insect image for prediction&#13;&#10;"/></div><p class="figure-caption">Figure 8.7: Sample stick insect image for prediction</p><p>The target size should be <strong class="source-inline">224x224</strong> since <strong class="source-inline">VGG16</strong> only accepts (<strong class="source-inline">224,224</strong>).</p></li>
				<li>Change the image to an array by using the <strong class="source-inline">img_to_array</strong> function:<p class="source-code">transformed_image = image.img_to_array(new_image)</p><p class="source-code">transformed_image.shape</p></li>
				<li>The image must be in a four-dimensional form for <strong class="source-inline">VGG16</strong> to allow further processing. Expand the dimension of the image along the 0<span class="superscript">th</span> axis using the <strong class="source-inline">expand_dims</strong> function:<p class="source-code">transformed_image = np.expand_dims(transformed_image, axis=0)</p><p class="source-code">transformed_image.shape</p></li>
				<li>Preprocess the image using the <strong class="source-inline">preprocess_input</strong> function:<p class="source-code">transformed_image = preprocess_input(transformed_image)</p><p class="source-code">transformed_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer168" class="IMG---Figure"><img src="image/B15777_08_08.jpg" alt="Figure 8.8: Screenshot showing a few instances of image preprocessing&#13;&#10;"/></div><p class="figure-caption">Figure 8.8: Screenshot showing a few instances of image preprocessing</p></li>
				<li>Create the <strong class="source-inline">predictor</strong> variable:<p class="source-code">y_pred = classifier.predict(transformed_image)</p><p class="source-code">y_pred</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer169" class="IMG---Figure"><img src="image/B15777_08_09.jpg" alt="Figure 8.9: Creating the predictor variable&#13;&#10;"/></div><p class="figure-caption">Figure 8.9: Creating the predictor variable</p></li>
				<li>Check the shape of the image. It should be (<strong class="source-inline">1,1000</strong>). It's <strong class="source-inline">1000</strong> because, as we mentioned previously, the ImageNet database has <strong class="source-inline">1000</strong> categories of images. The predictor variable shows the probabilities of our image being one of those images:<p class="source-code">y_pred.shape</p><p>The preceding code produces the following code:</p><p class="source-code">(1, 1000)</p></li>
				<li>Select the top five probabilities of what our image label is out of the <strong class="source-inline">1000</strong> labels that the <strong class="source-inline">VGG16</strong> network has:<p class="source-code">from keras.applications.vgg16 import decode_predictions</p><p class="source-code">decode_predictions(y_pred, top=5)</p><p>The preceding code produces the following code:</p><p class="source-code">[[('n02231487', 'walking_stick', 0.30524516),</p><p class="source-code">  ('n01775062', 'wolf_spider', 0.26035702),</p><p class="source-code">  ('n03804744', 'nail', 0.14323168),</p><p class="source-code">  ('n01770081', 'harvestman', 0.066652186),</p><p class="source-code">  ('n01773549', 'barn_spider', 0.03670299)]]</p><p>The first column of the array is an internal code number. The second is the label, while the third is the probability of the image being the label.</p></li>
				<li>Put the predictions in a human-readable format. Print the most probable label from the output from the result of the <strong class="source-inline">decode_predictions</strong> function:<p class="source-code">label = decode_predictions(y_pred)</p><p class="source-code">"""</p><p class="source-code">Most likely result is retrieved, for example, the highest probability</p><p class="source-code">"""</p><p class="source-code">decoded_label = label[0][0]</p><p class="source-code"># The classification is printed</p><p class="source-code">print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))</p><p>The preceding code produces the following code:</p><p class="source-code">walking_stick (30.52%)</p><p>Here, you can see that the network predicted that our image was a walking stick with <strong class="source-inline">30.52%</strong> accuracy. Clearly, the image is not a walking stick but a stick insect; out of all the labels that the <strong class="source-inline">VGG16</strong> network contains, a walking stick is the closest thing to a stick insect. The following image is that of a walking stick:</p><div id="_idContainer170" class="IMG---Figure"><img src="image/B15777_08_10.jpg" alt="Figure 8.10: Walking stick&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.10: Walking stick</p>
			<p>To avoid such outputs, we could freeze the existing layer of <strong class="source-inline">VGG16</strong> and add our own layer. We could also add a layer that contains images of walking sticks and stick insects so that we can obtain better output. </p>
			<p>If you have a large number of a walking stick and stick insect images, you could perform a similar task to improve the model's ability to classify images into their respective classes. You could then test it by rerunning the previous exercise.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31I7bnR">https://packt.live/31I7bnR</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31Hv1QE">https://packt.live/31Hv1QE</a>.</p>
			<p>To understand this in detail, let's work on a different example, where we freeze the last layer of the network and add our own layer with images of cars and flowers. This will help the network improve its accuracy in classifying images of cars and flowers.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/>Exercise 8.03: Fine-Tuning the VGG16 Model</h2>
			<p>Let's work on fine-tuning the <strong class="source-inline">VGG16</strong> model. In this exercise, we will freeze the network and remove the last layer of <strong class="source-inline">VGG16</strong>, which has <strong class="source-inline">1000</strong> labels in it. After removing the last layer, we will build a new flower-car classifier <strong class="source-inline">ANN</strong>, just like we did in <em class="italic">Chapter 7</em>, <em class="italic">Computer Vision with Convolutional Neural Networks</em>, and will connect this <strong class="source-inline">ANN</strong> to <strong class="source-inline">VGG16</strong> instead of the original one with <strong class="source-inline">1000</strong> labels. Essentially, what we will do is replace the last layer of <strong class="source-inline">VGG16</strong> with a user-defined layer.</p>
			<p>Before we begin, ensure you have downloaded the image datasets from this book's GitHub repository to your own working directory. You will need a <strong class="source-inline">training_set</strong> folder and a <strong class="source-inline">test_set</strong> folder to test your model. Each of these folders will contain a <strong class="source-inline">cars</strong> folder, containing car images, and a <strong class="source-inline">flowers</strong> folder, containing flower images.</p>
			<p>The steps for completing this exercise are as follows:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Unlike the original new model, which had <strong class="source-inline">1000</strong> labels (<strong class="source-inline">100</strong> different object categories), this new fine-tuned model will only have images of flowers or cars. So, whatever image you provide as an input to the model, it will categorize it as a flower or car based on its prediction probability.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library, TensorFlow's <strong class="source-inline">random</strong> library, and the necessary <strong class="source-inline">Keras</strong> libraries:<p class="source-code">import numpy as np</p><p class="source-code">import keras</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from tensorflow import random</p></li>
				<li>Initiate the <strong class="source-inline">VGG16</strong> model:<p class="source-code">vgg_model = keras.applications.vgg16.VGG16()</p></li>
				<li>Check the model <strong class="source-inline">summary</strong>:<p class="source-code">vgg_model.summary()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer171" class="IMG---Figure"><img src="image/B15777_08_11.jpg" alt="Figure 8.11: Model summary after initiating the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.11: Model summary after initiating the model</p></li>
				<li>Remove the last layer, <strong class="source-inline">labeled predictions</strong> in the preceding image, from the model summary. Create a new Keras model of the sequential class and iterate through all the layers of the VGG model. Add all of them to the new model, except for the last layer:<p class="source-code">last_layer = str(vgg_model.layers[-1])</p><p class="source-code">np.random.seed(42)</p><p class="source-code">random.set_seed(42)</p><p class="source-code">classifier= keras.Sequential()</p><p class="source-code">for layer in vgg_model.layers:</p><p class="source-code">    if str(layer) != last_layer:</p><p class="source-code">        classifier.add(layer)</p><p>Here, we have created a new model name's classifier instead of <strong class="source-inline">vgg_model</strong>. All the layers, except the last layer, that is, <strong class="source-inline">vgg_model</strong>, have been included in the classifier.</p></li>
				<li>Print the <strong class="source-inline">summary</strong> of the newly created model:<p class="source-code">classifier.summary()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer172" class="IMG---Figure"><img src="image/B15777_08_12.jpg" alt="Figure 8.12: Rechecking the summary after removing the last layer&#13;&#10;"/></div><p class="figure-caption">Figure 8.12: Rechecking the summary after removing the last layer</p><p>The last layer of prediction (<strong class="source-inline">Dense</strong>) has been deleted.</p></li>
				<li>Freeze the layers by iterating through the layers and setting the <strong class="source-inline">trainable</strong> parameter to <strong class="source-inline">False</strong>: <p class="source-code">for layer in classifier.layers:</p><p class="source-code">    layer.trainable=False</p></li>
				<li>Add a new output layer of size <strong class="source-inline">1</strong> with a <strong class="source-inline">sigmoid</strong> activation function and print the model summary:<p class="source-code">classifier.add(Dense(1, activation='sigmoid'))</p><p class="source-code">classifier.summary()</p><p>The following function shows the output of the preceding code:</p><div id="_idContainer173" class="IMG---Figure"><img src="image/B15777_08_13.jpg" alt="Figure 8.13: Rechecking the summary after adding the new layer&#13;&#10;"/></div><p class="figure-caption">Figure 8.13: Rechecking the summary after adding the new layer</p><p>Now, the last layer is the newly created user-defined layer.</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and binary cross-entropy loss and compute the <strong class="source-inline">accuracy</strong> during training:<p class="source-code">classifier.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">                   metrics=['accuracy'])</p><p>Create some training and test data generators, just like we did in <em class="italic">Chapter 7</em>, <em class="italic">Computer Vision with Convolutional Neural Networks</em>. Rescale the training and test images by <strong class="source-inline">1/255</strong> so that all the values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Set the following parameters for the training data generators only: <strong class="source-inline">shear_range=0.2</strong>, <strong class="source-inline">zoom_range=0.2</strong>, and <strong class="source-inline">horizontal_flip=True</strong>.</p></li>
				<li>Next, create a training set from the <strong class="source-inline">training set</strong> folder. <strong class="source-inline">../Data/dataset/training_set</strong> is the folder where our data is placed. Our CNN model has an image size of <strong class="source-inline">224x224</strong>, so the same size should be passed here too. <strong class="source-inline">batch_size</strong> is the number of images in a single batch, which is <strong class="source-inline">32</strong>. <strong class="source-inline">class_mode</strong> is binary since we are creating a binary classifier.<p class="callout-heading">Note</p><p class="callout">Unlike in <em class="italic">Chapter 7</em>, <em class="italic">Computer Vision with Convolutional Neural Networks</em>, where the image size was <strong class="source-inline">64x64</strong>, VGG16 needs an image size of <strong class="source-inline">224x224</strong>.</p><p>Finally, fit the model to the training data:</p><p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">generate_train_data = \</p><p class="source-code">ImageDataGenerator(rescale = 1./255,\</p><p class="source-code">                   shear_range = 0.2,\</p><p class="source-code">                   zoom_range = 0.2,\</p><p class="source-code">                   horizontal_flip = True)</p><p class="source-code">generate_test_data = ImageDataGenerator(rescale =1./255)</p><p class="source-code">training_dataset = \</p><p class="source-code">generate_train_data.flow_from_directory(\</p><p class="source-code">    '../Data/Dataset/training_set',\</p><p class="source-code">    target_size = (224, 224),\</p><p class="source-code">    batch_size = 32,\</p><p class="source-code">    class_mode = 'binary')</p><p class="source-code">test_datasetset = \</p><p class="source-code">generate_test_data.flow_from_directory(\</p><p class="source-code">    '../Data/Dataset/test_set',\</p><p class="source-code">    target_size = (224, 224),\</p><p class="source-code">    batch_size = 32,\</p><p class="source-code">    class_mode = 'binary')</p><p class="source-code">classifier.fit_generator(training_dataset,\</p><p class="source-code">                         steps_per_epoch = 100,\</p><p class="source-code">                         epochs = 10,\</p><p class="source-code">                         validation_data = test_datasetset,\</p><p class="source-code">                         validation_steps = 30,\</p><p class="source-code">                         shuffle=False)</p><p>There are 100 training images here, so set <strong class="source-inline">steps_per_epoch =100</strong>, set <strong class="source-inline">validation_steps=30</strong>, and set <strong class="source-inline">shuffle=False</strong>:</p><p class="source-code">100/100 [==============================] - 2083s 21s/step - loss: 0.5513 - acc: 0.7112 - val_loss: 0.3352 - val_acc: 0.8539</p></li>
				<li>Predict the new image (the code is the same as it was in <em class="italic">Chapter 7</em>, <em class="italic">Computer Vision with Convolutional Neural Networks</em>). First, load the image from <strong class="source-inline">'../Data/Prediction/test_image_2.jpg'</strong> and set the target size to (<strong class="source-inline">224, 224</strong>) since the <strong class="source-inline">VGG16</strong> model accepts images of that size.<p class="source-code">from keras.preprocessing import image</p><p class="source-code">new_image = \</p><p class="source-code">image.load_img('../Data/Prediction/test_image_2.jpg', \</p><p class="source-code">               target_size = (224, 224))</p><p class="source-code">new_image</p><p>At this point, you can view the image by executing the code <strong class="source-inline">new_image</strong> and the class labels by running <strong class="source-inline">training_dataset.class_indices</strong>.</p><p>Next, preprocess the image, first by converting the image into an array using the <strong class="source-inline">img_to_array</strong> function, then by adding another dimension along the 0<span class="superscript">th</span> axis using the <strong class="source-inline">expand_dims</strong> function. Finally, make the prediction using the <strong class="source-inline">predict</strong> method of the classifier and printing the output in human-readable format:</p><p class="source-code">new_image = image.img_to_array(new_image)</p><p class="source-code">new_image = np.expand_dims(new_image, axis = 0)</p><p class="source-code">result = classifier.predict(new_image)</p><p class="source-code">if result[0][0] == 1:</p><p class="source-code">    prediction = 'It is a flower'</p><p class="source-code">else:</p><p class="source-code">    prediction = 'It is a car'</p><p class="source-code">print(prediction)</p><p>The preceding code produces the following output:</p><p class="source-code">It is a car</p></li>
				<li>As a final step, you can save the classifier by running <strong class="source-inline">classifier.save('car-flower-classifier.h5')</strong>.</li>
			</ol>
			<p>Here, we can see that the algorithm has done the correct image classification by identifying the image of the car. We just used a pre-built <strong class="source-inline">VGG16</strong> model for image classification by tweaking its layers and molding it as per our requirements. This is a very powerful technique for image classification.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZxCqzA">https://packt.live/2ZxCqzA</a></p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<p>In the next exercise, we will utilize a different pre-trained model, known as <strong class="source-inline">ResNet50</strong>, and demonstrate how to classify images with this model.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>Exercise 8.04: Image Classification with ResNet</h2>
			<p>Finally, before closing this chapter, let's work on an exercise with the <strong class="source-inline">ResNet50</strong> network. We'll use an image of a Nascar racer and try to predict it through the network. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import numpy as np</p><p class="source-code">from keras.applications.resnet50 import ResNet50, preprocess_input</p><p class="source-code">from keras.preprocessing import image </p></li>
				<li>Initiate the <strong class="source-inline">ResNet50</strong> model and print the <strong class="source-inline">summary</strong> of the model:<p class="source-code">classifier = ResNet50()</p><p class="source-code">classifier.summary()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer174" class="IMG---Figure"><img src="image/B15777_08_14.jpg" alt="Figure 8.14: A summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.14: A summary of the model</p></li>
				<li>Load the image. <strong class="source-inline">'../Data/Prediction/test_image_3.jpg'</strong> is the path of the image on our system. It will be different on your system:<p class="source-code">new_image = \</p><p class="source-code">image.load_img('../Data/Prediction/test_image_3.jpg', \</p><p class="source-code">               target_size=(224, 224))</p><p class="source-code">new_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer175" class="IMG---Figure"><img src="image/B15777_08_15.jpg" alt="Figure 8.15: Sample Nascar racer image for prediction&#13;&#10;"/></div><p class="figure-caption">Figure 8.15: Sample Nascar racer image for prediction</p><p>Note that the target size should be <strong class="source-inline">224x224</strong> since <strong class="source-inline">ResNet50</strong> only accepts (<strong class="source-inline">224,224</strong>).</p></li>
				<li>Change the image to an array by using the <strong class="source-inline">img_to_array</strong> function:<p class="source-code">transformed_image = image.img_to_array(new_image)</p><p class="source-code">transformed_image.shape</p></li>
				<li>The image has to be in a four-dimensional form for <strong class="source-inline">ResNet50</strong> to allow further processing. Expand the dimension along the 0<span class="superscript">th</span> axis using the <strong class="source-inline">expand_dims</strong> function:<p class="source-code">transformed_image = np.expand_dims(transformed_image, axis=0)</p><p class="source-code">transformed_image.shape</p></li>
				<li>Preprocess the image using the <strong class="source-inline">preprocess_input</strong> function:<p class="source-code">transformed_image = preprocess_input(transformed_image)</p><p class="source-code">transformed_image</p></li>
				<li>Create the predictor variable by using the classifier to predict the image using its <strong class="source-inline">predict</strong> method:<p class="source-code">y_pred = classifier.predict(transformed_image)</p><p class="source-code">y_pred</p></li>
				<li>Check the shape of the image. It should be (<strong class="source-inline">1,1000</strong>):<p class="source-code">y_pred.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(1, 1000)</p></li>
				<li>Select the top five probabilities of what our image is using the <strong class="source-inline">decode_predictions</strong> function and by passing the predictor variable, <strong class="source-inline">y_pred</strong>, as the argument and the top number of predictions and corresponding labels:<p class="source-code">from keras.applications.resnet50 import decode_predictions</p><p class="source-code">decode_predictions(y_pred, top=5)</p><p>The preceding code produces the following output:</p><p class="source-code">[[('n04037443', 'racer', 0.8013074),</p><p class="source-code">  ('n04285008', 'sports_car', 0.06431753),</p><p class="source-code">  ('n02974003', 'car_wheel', 0.024077434),</p><p class="source-code">  ('n02504013', 'Indian_elephant', 0.019822922),</p><p class="source-code">  ('n04461696', 'tow_truck', 0.007778575)]]</p><p>The first column of the array is an internal code number. The second is the label, while the third is the probability of the image being the label.</p></li>
				<li>Put the predictions in a human-readable format. Print the most probable label from the output from the result of the <strong class="source-inline">decode_predictions</strong> function:<p class="source-code">label = decode_predictions(y_pred)</p><p class="source-code">"""</p><p class="source-code">Most likely result is retrieved, for example, the highest probability</p><p class="source-code">"""</p><p class="source-code">decoded_label = label[0][0]</p><p class="source-code"># The classification is printed</p><p class="source-code">print('%s (%.2f%%)' % (decoded_label[1], \</p><p class="source-code">      decoded_label[2]*100 ))</p><p>The preceding code produces the following output:</p><p class="source-code">racer (80.13%)</p></li>
			</ol>
			<p>Here, the model clearly shows (with a probability of <strong class="source-inline">80.13%</strong>) that the picture is that of a racer. This is the power of pre-trained models, and Keras gives us the flexibility to use and tweak these models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BzvTMK">https://packt.live/2BzvTMK</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eWelJh">https://packt.live/3eWelJh</a>.</p>
			<p>In the next activity, we will classify another image using the pre-trained <strong class="source-inline">ResNet50</strong> model.</p>
			<h2 id="_idParaDest-171">Activity 8.02: <a id="_idTextAnchor172"/>Image Classification with ResNet </h2>
			<p>Now, let's work on an activity that uses another pre-trained network, known as <strong class="source-inline">ResNet</strong>. We have an image of television located at <strong class="source-inline">../Data/Prediction/test_image_4</strong>. We will use the <strong class="source-inline">ResNet50</strong> network to predict the image. To implement the activity, follow these steps:</p>
			<ol>
				<li value="1">Import the required libraries.</li>
				<li>Initiate the <strong class="source-inline">ResNet</strong> model.</li>
				<li>Load the image that needs to be classified.</li>
				<li>Preprocess the image by applying the appropriate transformations.</li>
				<li>Create a predictor variable to predict the image.</li>
				<li>Label the image and classify it.<p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 448.</p></li>
			</ol>
			<p>So, the network says, with close to <strong class="source-inline">100%</strong> accuracy, that the image is that of a television. This time, we used a <strong class="source-inline">ResNet50</strong> pre-trained model to classify the image of television and obtained similar results to those we obtained using the <strong class="source-inline">VGG16</strong> model to predict the image of a slice of pizza.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Summary</h1>
			<p>In this chapter, we covered the concept of transfer learning and how is it related to pre-trained networks. We utilized this knowledge by using the pre-trained deep learning networks <strong class="source-inline">VGG16</strong> and <strong class="source-inline">ResNet50</strong> to predict various images. We practiced how to take advantage of such pre-trained networks using techniques such as feature extraction and fine-tuning to train models faster and more accurately. Finally, we learned the powerful technique of tweaking existing models and making them work according to our dataset. This technique of building our own <strong class="source-inline">ANN</strong> over an existing <strong class="source-inline">CNN</strong> is one of the most powerful techniques used in the industry.</p>
			<p>In the next chapter, we will learn about sequential modeling and sequential memory by looking at some real-life cases with Google Assistant. Furthermore, we will learn how sequential modeling is related to <strong class="source-inline">Recurrent Neural Networks</strong> (<strong class="source-inline">RNN</strong>). We will learn about the vanishing gradient problem in detail and how using an <strong class="source-inline">LSTM</strong> is better than a simple <strong class="source-inline">RNN</strong> to overcome the vanishing gradient problem. We will apply what we have learned to time series problems by predicting stock trends that come out as fairly accurate.</p>
		</div>
	</body></html>