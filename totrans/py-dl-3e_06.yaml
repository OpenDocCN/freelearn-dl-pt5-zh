- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Natural Language Processing and Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理和循环神经网络
- en: This chapter will introduce two different topics that nevertheless complement
    each other – **natural language processing** (**NLP**) and **recurrent neural
    networks** (**RNNs**). NLP teaches computers to process and analyze natural language
    text to perform tasks such as machine translation, sentiment analysis, and text
    generation. Unlike images in computer vision, natural text represents a different
    type of data, where the order (or sequence) of the elements matters. Thankfully,
    RNNs are suitable for processing sequential data, such as text or time series.
    They help us deal with sequences of variable length by defining a recurrence relation
    over these sequences (hence the name). This makes NLP and RNNs natural allies.
    In fact, RNNs can be applied to any problem since it has been proven that they
    are Turing-complete – theoretically, they can simulate any program that a regular
    computer would not be able to compute.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍两个不同但互补的主题——**自然语言处理**（**NLP**）和**循环神经网络**（**RNNs**）。NLP教会计算机处理和分析自然语言文本，以执行诸如机器翻译、情感分析和文本生成等任务。与计算机视觉中的图像不同，自然文本代表了一种不同类型的数据，其中元素的顺序（或序列）非常重要。幸运的是，RNNs非常适合处理顺序数据，如文本或时间序列。通过在这些序列上定义递归关系（因此得名），它们帮助我们处理可变长度的序列。这使得NLP和RNNs成为天然的盟友。事实上，RNNs可以应用于任何问题，因为已经证明它们是图灵完备的——从理论上讲，它们可以模拟任何常规计算机无法计算的程序。
- en: However, it is not only good news, and we’ll have to start with a disclaimer.
    Although RNNs have great theoretical properties, we now know that there are practical
    limitations to their use. These limitations have been mostly surpassed by a more
    recent **neural network** (**NN**) architecture called **transformer**, which
    we’ll discuss in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202). In theory, the
    transformer has more limitations compared to RNNs. But as sometimes happens, it
    works better in practice. Nevertheless, I believe that this chapter will be beneficial
    to you. On one hand, RNNs have elegant architecture and still represent one of
    the major NN classes; on the other hand, the progression of knowledge presented
    in this and the next three chapters will closely match the real-world progression
    of research on these topics. So, you’ll be able to apply the concepts you’ll learn
    here in the next few chapters as well. This chapter will also allow you to fully
    appreciate the advantages of the newer models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不全是好消息，我们需要从一个免责声明开始。尽管RNNs具有很好的理论特性，但我们现在知道它们在实际应用中有一定的局限性。这些局限性大多已经被一种更新的**神经网络**（**NN**）架构——**transformer**克服，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论它。从理论上讲，transformer相比RNNs有更多的限制。但有时候，实践证明它表现得更好。尽管如此，我相信本章对你仍然是有益的。一方面，RNNs具有优雅的架构，仍然代表着神经网络中的重要一类；另一方面，本章和接下来的三章所呈现的知识进展，将与这些主题在实际研究中的进展紧密相符。因此，你将在接下来的几章中也能应用这里学到的概念。本章还将帮助你充分理解新模型的优势。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Natural language processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Introducing RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍RNNs
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and the
    TorchText package ([https://github.com/pytorch/text](https://github.com/pytorch/text)).
    If you don’t have an environment set up with these tools, fret not – the example
    is available as a Jupyter Notebook on Google Colab. You can find the code examples
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和TorchText包（[https://github.com/pytorch/text](https://github.com/pytorch/text)）来实现示例。如果你没有配置这些工具的环境，不必担心——该示例可以在Google
    Colab上的Jupyter Notebook中运行。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06)。
- en: Natural language processing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: 'NLP is a subfield of machine learning that allows computers to interpret, manipulate,
    and comprehend human language. This definition sounds a little dry, so, to provide
    a little clarity, let’s start with a non-exhaustive list of the types of tasks
    that fall under the NLP umbrella:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是机器学习的一个子领域，使计算机能够解释、操作和理解人类语言。这个定义听起来有点枯燥，因此，为了提供一些清晰度，让我们从一个非详尽的任务列表开始，看看都有哪些任务属于NLP的范畴：
- en: '**Text classification**: This assigns a single label to the entire input text.
    For example, **sentiment analysis** can determine whether a product review is
    positive or negative.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：这会为整个输入文本分配一个标签。例如，**情感分析**可以判断一篇产品评论是积极的还是消极的。'
- en: '**Token classification**: This assigns a label for each token of the input
    text. A token is a building block (or a unit) of text. Words can be tokens. A
    popular token classification task is **named entity recognition**, which assigns
    each token to a list of predefined classes such as place, company, or person.
    **Part-of-speech** (**POS**) tagging assigns each word to a particular part of
    speech, such as a noun, verb, or adjective.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记分类**：这为每个输入文本的标记分配一个标签。标记是文本的构建块（或单位）。单词可以是标记。一个流行的标记分类任务是**命名实体识别**，它为每个标记分配一个预定义类别列表，如地点、公司或人物。**词性**（**POS**）标注为每个单词分配一个特定的词性，如名词、动词或形容词。'
- en: '**Text generation**: This uses the input text to generate new text with arbitrary
    length. Text generation tasks include machine translation, question answering,
    and text summarization (creating a shorter version of the original text while
    preserving its essence).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：这是利用输入文本生成具有任意长度的新文本。文本生成任务包括机器翻译、问答和文本摘要（在保留原文精髓的同时创建简短版本）。'
- en: 'Solving NLP problems is not trivial. To understand why, let’s go back to computer
    vision ([*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)), where the input images
    are represented as 2D tensors of pixel intensities with the following properties:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 解决自然语言处理（NLP）问题并非易事。为了理解其原因，我们先回顾一下计算机视觉（[*第4章*](B19627_04.xhtml#_idTextAnchor107)），其中输入的图像以像素强度的二维张量表示，具有以下特点：
- en: The image is composed of pixels and doesn’t have any other explicitly defined
    structure
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像由像素构成，并且没有其他显式定义的结构
- en: The pixels form implicit hierarchical structures of larger objects, based on
    their proximity to each other
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素基于彼此的接近度，形成了隐式的更大物体的层次结构
- en: There is only one type of pixel, which is defined only by its scalar intensity
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一种类型的像素，其仅由标量强度来定义
- en: Thanks to its homogenous structure, we can feed the (almost) raw image to a
    **convolutional neural network** (**CNN**) and let it do its magic with relatively
    little data pre-processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其同质化的结构，我们可以将（几乎）原始的图像输入到**卷积神经网络**（**CNN**）中，让它以相对较少的数据预处理做出处理。
- en: 'Now, let’s return to text data, which has the following properties:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到文本数据，它具有以下特点：
- en: There are different types of characters with different semantical meanings,
    such as letters, digits, and punctuation marks. In addition, we might encounter
    previously unknown symbols.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有不同类型的字符，具有不同的语义意义，如字母、数字和标点符号。此外，我们还可能遇到以前未见过的符号。
- en: The natural text has an explicit hierarchy in the form of characters, words,
    sentences, and paragraphs. We also have quotes, titles, and a hierarchy of headings.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然文本有着显式的层次结构，包括字符、单词、句子和段落。我们还有引号、标题和层次结构的标题。
- en: Some parts of the text may be related to distant parts of the sequence, rather
    than their immediate context. For example, a fictional story can introduce a person
    by their name but later refer to them only as *he* or *she*. These references
    can be separated by long text sequences, yet, we still have to be able to find
    this relation.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本的某些部分可能与序列中较远的部分有关，而不是它们的直接上下文。例如，一篇虚构故事可能会先介绍一个人名，但随后只用*他*或*她*来提及。这些指代可能被长篇文本序列分隔开，但我们仍然需要能够找到这种关系。
- en: The complexity of natural text requires several pre-processing steps before
    the actual NN model comes into play. The first step is **normalization**, which
    involves operations such as removing extra whitespace and converting all letters
    into lowercase. The next steps are not as straightforward, so we’ll dedicate the
    next two sections to them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自然文本的复杂性要求在实际神经网络模型发挥作用之前，进行几步预处理。第一步是**归一化**，包括去除多余的空白字符和将所有字母转换为小写。接下来的步骤并不像前面那样简单，因此我们将专门用接下来的两节来讨论这些步骤。
- en: Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: One intuitive way to approach an NLP task is to split the corpus into words,
    which will represent the basic input units of our model. However, using words
    as input is not set in stone and we can use other elements, such as individual
    characters, phrases, or even whole sentences. The generic term for these units
    is **tokens**. A token refers to a text corpus in the same way as a pixel refers
    to an image. The process of splitting the corpus into tokens is called **tokenization**
    (what a surprise!). The entity
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的处理自然语言处理任务的方法是将语料库拆分为单词，这些单词将代表我们模型的基本输入单元。然而，使用单词作为输入并不是固定不变的，我们还可以使用其他元素，比如单个字符、短语，甚至整个句子。这些单元的通用术语是**标记**。标记指代文本语料库的方式，就像像素指代图像一样。将语料库拆分成标记的过程被称为**标记化**（真是意外！）。实体
- en: (for example, an algorithm) that performs this tokenization is called a **tokenizer**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （例如，执行这种标记化的算法）称为**标记器**。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The tokenizers we’ll discuss in this section are universal in the sense that
    they can work with different NLP ML algorithms. Therefore, the pre-processing
    algorithms in this section are commonly used with transformer models, which we’ll
    introduce in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中讨论的标记器是通用的，意味着它们可以与不同的自然语言处理机器学习算法配合使用。因此，本节中的预处理算法通常用于变换器模型，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中介绍这些模型。
- en: 'With that, let’s discuss the types of tokenizers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论几种标记器的类型：
- en: '**Word-based**: Each word represents a unique token. This is the most intuitive
    type of tokenization, but it has serious drawbacks. For example, the words *don’t*
    and *do not* will be represented by different tokens, but they mean the same thing.
    Another example is the words *car* and *cars* or *ready* and *readily*, which
    will be represented by different tokens, whereas a single token would be more
    appropriate. Because natural language is so diverse, there are many corner cases
    like these. The issue isn’t just that semantically similar words will have unrelated
    tokens, but also the large number of unique tokens that come out of this. This
    will make the model computationally inefficient. It will also produce many tokens
    with a small number of occurrences, which will prove challenging for the model
    to learn. Finally, we might encounter unknown words in a new text corpus.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于词**：每个单词代表一个独特的标记。这是最直观的标记化方式，但也有严重的缺点。例如，单词*don’t*和*do not*将被表示为不同的标记，但它们的含义是相同的。另一个例子是单词*car*和*cars*，或*ready*和*readily*，它们会被表示为不同的标记，而一个单一的标记会更合适。由于自然语言如此多样，像这样的特殊情况非常多。问题不仅仅在于语义相似的单词会有不同的标记，还在于由此产生的大量唯一标记。这会导致模型计算效率低下。它还会产生许多出现次数较少的标记，这对模型的学习来说是一个挑战。最后，我们可能会遇到在新文本语料库中无法识别的单词。'
- en: '**Character-based**: Each character (letter, digit, punctuation, and so on)
    in the text is a unique token. In this way, we have fewer tokens, as the total
    number of characters is limited and finite. Since we know all the characters in
    advance, we cannot encounter unknown symbols.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于字符**：文本中的每个字符（字母、数字、标点符号等）都是一个独特的标记。通过这种方式，我们可以减少标记数量，因为字符的总数是有限的并且是有限的。由于我们事先知道所有的字符，因此不会遇到未知的符号。'
- en: However, this tokenization is less intuitive than the word-based model because
    a context composed of characters is less meaningful than a context based on words.
    While the number of unique tokens is relatively small, the total number of tokens
    in the corpus will be very large (equal to the total number of characters).
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，与基于词的模型相比，这种标记化方法不太直观，因为由字符组成的上下文比基于词的上下文意义较小。虽然唯一标记的数量相对较少，但语料库中的标记总数将非常庞大（等于字符总数）。
- en: '**Subword tokenization**: This is a two-step process that starts by splitting
    the corpus into words. The most obvious way to split the text is on whitespace.
    In addition, we can also split it on whitespace *and punctuation marks*. In NLP
    parlance, this step is known as **pre-tokenization**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词标记化**：这是一个两步过程，首先将语料库分割成单词。分割文本最明显的方式是通过空格。此外，我们还可以通过空格*和标点符号*来分割文本。在自然语言处理术语中，这一步骤被称为**预标记化**。'
- en: (the prefix implies that tokenization will follow). Then, it preserves the frequently
    used words and decomposes the rare words into meaningful subwords, which are more
    frequent. For example, we can decompose the word *tokenization* into the core
    word *token* and the suffix *ization*, each with its own token. Then, when we
    encounter the word *carbonization*, we can decompose it into *carbon* and *ization*.
    In this way, we’ll have two occurrences of *ization* instead of a single occurrence
    of *tokenization* and *carbonization*. Subword tokenization also makes it possible
    to decompose unknown words into known tokens.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （前缀意味着接下来会进行标记化）。然后，它保留常用词，并将稀有词拆解为更频繁的有意义子词。例如，我们可以将单词*tokenization*分解为核心词*token*和后缀*ization*，每个部分都有自己的标记。然后，当我们遇到*carbonization*这个词时，我们可以将其分解为*carbon*和*ization*。这样，我们会得到两个*ization*的实例，而不是一个*tokenization*和一个*carbonization*。子词标记化还使得可以将未知词分解为已知标记。
- en: Special service tokens
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊服务标记。
- en: 'For the concept of tokenization to work, it introduces some service tokens.
    These include the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使标记化的概念起作用，它引入了一些服务性标记。以下是一些服务性标记：
- en: '**UNK**: Replaces unknown tokens in the corpus (think of rare words such as
    alphanumeric designations)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UNK**：替换语料库中的未知标记（可以理解为稀有词汇，如字母数字标识符）。'
- en: '**EOS**: An end-of-sentence (or sequence) token'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EOS**：句子（或序列）结束标记。'
- en: '**BOS**: A beginning-of-sentence (or sequence) token'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BOS**：句子（或序列）开始标记。'
- en: '**SEP**: This separates two semantically different text sequences, such as
    question and answer'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SEP**：用来分隔两个语义上不同的文本序列，例如问题和答案。'
- en: '**PAD**: This is a padding token that is appended to an existing sequence so
    that it can reach some predefined length and fit in a fixed-length mini-batch.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PAD**：这是一个填充标记，它会附加到现有序列中，以便它可以达到某个预定义长度并适应固定长度的小批次。'
- en: For example, we can tokenize the sentence *I bought a product called FD543C*
    into *BOS I bought a product called UNK EOS PAD PAD* to fit a fixed input with
    a length of 10.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将句子*I bought a product called FD543C*标记化为*BOS I bought a product called
    UNK EOS PAD PAD*，以适应长度为10的固定输入。
- en: Subword tokenization is the most popular type of tokenization because it combines
    the best features of character-based (smaller vocabulary size) and word-based
    (meaningful context) tokenization. In the next few sections, we’ll discuss some
    of the most popular subword tokenizers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 子词标记化是最流行的标记化方式，因为它结合了基于字符（较小的词汇量）和基于词语（有意义的上下文）标记化的最佳特性。在接下来的几个部分中，我们将讨论一些最流行的子词标记器。
- en: Byte-Pair Encoding and WordPiece
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字节对编码和WordPiece。
- en: '**Byte-Pair Encoding** (**BPE**, Neural Machine Translation of Rare Words with
    Subword Units, [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909))
    is a popular subword tokenization algorithm. As with other such tokenizers, it
    begins with pre-tokenization, which splits the corpus into words.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**字节对编码**（**BPE**，使用子词单元进行稀有词的神经机器翻译，[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)）是一种流行的子词标记化算法。与其他此类标记器一样，它从预标记化开始，将语料库拆分为单词。'
- en: 'Using this dataset as a starting point, BPE works in the following way:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个数据集为起点，BPE的工作方式如下：
- en: Start with an initial **base** (or **seed**) **vocabulary**, which consists
    of the individual characters of all words in the text corpus. Therefore, each
    word is a sequence of single-character tokens.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始的**基础**（或**种子**）**词汇**开始，该词汇由文本语料库中所有单词的单个字符组成。因此，每个单词都是一系列单字符标记。
- en: 'Repeat the following until the size of the token vocabulary reaches a certain
    maximum threshold:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤，直到标记词汇的大小达到某个最大阈值：
- en: Find the pair of tokens (initially, these are single characters) that occur
    together most frequently and merge them into a new composite token.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出最常一起出现的一对标记（最初这些是单个字符），并将它们合并成一个新的复合标记。
- en: Extend the existing token vocabulary with the new composite token.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的复合标记扩展现有的标记词汇。
- en: Update the tokenized text corpus with the new token structure.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的标记结构更新标记化的文本语料库。
- en: 'To understand BPE, let’s assume that our corpus consists of the following (imaginary)
    words: `{dab: 5, adab: 4, aab: 7, bub: 9, bun: 2}`. The digit following each word
    indicates the number of occurrences of that word in the text. And here is the
    same corpus, but split into tokens (that is, characters): `{(d, a, b): 5, (a,
    d, a, b): 4, (a, a, b): 7, (b, u, b): 9, (b, u, c): 2}`. Based on this, we can
    build our initial token vocabulary with occurrence counts for each token: `{b:
    36, a: 27, u: 11, d: 9, c: 2}`. The following list illustrates the first four
    merge operations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解BPE，让我们假设我们的语料库包含以下（虚构的）单词：`{dab: 5, adab: 4, aab: 7, bub: 9, bun: 2}`。每个单词后面的数字表示该单词在文本中出现的次数。以下是相同的语料库，但已经按符号（即字符）拆分：`{(d,
    a, b): 5, (a, d, a, b): 4, (a, a, b): 7, (b, u, b): 9, (b, u, c): 2}`。基于此，我们可以构建我们的初始符号词汇表，每个符号的出现次数为：`{b:
    36, a: 27, u: 11, d: 9, c: 2}`。以下列表展示了前四次合并操作：'
- en: 'The most common pair of tokens is `(a, b)`, which occurs `freq((a, b)) = 5
    + 4 + 7 = 16` times. Therefore, we merge them, and the corpus becomes `{(d,` `):
    5, (a, d,` `): 4, (a,` `): 7, (b, u, b): 9, (b, u, c): 2}`. The new token vocabulary
    is `{b: 20,` `: 16, a: 11, u: 11, d: 9,` `c: 2}`.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最常见的符号对是`(a, b)`，其出现次数为`freq((a, b)) = 5 + 4 + 7 = 16`次。因此，我们将它们合并，语料库变为`{(d,`
    `): 5, (a, d,` `): 4, (a,` `): 7, (b, u, b): 9, (b, u, c): 2}`。新的符号词汇表是`{b: 20,`
    `: 16, a: 11, u: 11, d: 9,` `c: 2}`。'
- en: 'The new most common token pair is `(b, u)` with `freq((b, u)) = 9 + 2 = 11`
    occurrences. Again, we proceed to combine them in a new token: `{(d, ab): 5, (a,
    d, ab): 4, (a, ab): 7, (``, b): 9, (``, c): 2}`. The updated token vocabulary
    is `{ab: 16, a: 11,` `: 11, b: 9, d: 9,` `c: 2}`.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '新的最常见的符号对是`(b, u)`，其`freq((b, u)) = 9 + 2 = 11`次出现。接着，我们将它们合并为一个新的符号：`{(d,
    ab): 5, (a, d, ab): 4, (a, ab): 7, (``, b): 9, (``, c): 2}`。更新后的符号词汇表是`{ab: 16,
    a: 11,` `: 11, b: 9, d: 9,` `c: 2}`。'
- en: 'The next token pair is `(d, ab)` and it occurs `freq((d, ab)) = 5 + 4 = 9`
    times. After combining them, the tokenized corpus becomes `{(``): 5, (a,` `):
    4, (a, ab): 7, (bu, b): 9, (bu, c): 2}`. The new token vocabulary is `{a: 11,
    bu: 11, b: 9,` `: 9, ab: 7,` `c: 2}`.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '下一个符号对是`(d, ab)`，其出现次数为`freq((d, ab)) = 5 + 4 = 9`次。合并后，符号化的语料库变为`{(``): 5,
    (a,` `): 4, (a, ab): 7, (bu, b): 9, (bu, c): 2}`。新的符号词汇表是`{a: 11, bu: 11, b: 9,`
    `: 9, ab: 7,` `c: 2}`。'
- en: 'The new pair of tokens is `(bu, b)` with nine occurrences. After merging them,
    the corpus becomes `{(dab): 5, (a, dab): 4, (a, ab): 7, (``): 9, (bu, c): 2}`,'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '新的符号对是`(bu, b)`，其出现次数为9次。将它们合并后，语料库变为`{(dab): 5, (a, dab): 4, (a, ab): 7, (``):
    9, (bu, c): 2}`，'
- en: 'and the token vocabulary becomes `{a: 11,` `: 9,` `: 9, ab: 7, bu: 2,` `c:
    2}`.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '而符号词汇表变为`{a: 11,` `: 9,` `: 9, ab: 7, bu: 2,` `c: 2}`。'
- en: BPE stores all token-merge rules and their order and not just the final token
    vocabulary. During model inference, it applies the rules in the same order on
    the new unknown text to tokenize it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BPE会存储所有符号合并规则及其顺序，而不仅仅是最终的符号词汇表。在模型推理过程中，它会按照相同的顺序将规则应用于新的未知文本，以对其进行符号化。
- en: End-of-word tokens
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 词尾符号
- en: The original BPE implementation appends a special end-of-word token, `<w/>`,
    at the end of each word – for example, the word `aab` becomes `aab<w/>`. Other
    implementations can place the special token at the beginning of the word, instead
    of the end. This makes it possible for the algorithm to distinguish between, say,
    the token `ab`, as presented in the word `ca<w/>`, and the same token in `a``<w/>`.
    In this way, the algorithm can restore the original corpus from the tokenized
    one (**de-tokenization**), which wouldn’t be possible otherwise. In this section,
    we have omitted the end-of-word token for clarity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始BPE实现会在每个单词的末尾添加一个特殊的词尾符号`<w/>`，例如，单词`aab`变为`aab<w/>`。其他实现可以将该特殊符号放在单词的开头，而不是末尾。这使得算法能够区分，例如，单词`ca<w/>`中的符号`ab`，与`a``<w/>`中的相同符号。因此，算法可以从符号化后的语料库恢复出原始语料库（**去符号化**），否则是无法做到的。本节中，为了简洁起见，我们省略了词尾符号。
- en: Let’s recall that our base vocabulary includes all characters of the text corpus.
    If these are Unicode characters (which is the usual case), we could end up with
    a vocabulary of up to 150,000 tokens. And this is before we even start the token-merge
    process. One trick to solve this issue is with the help of **byte-level BPE**.
    Each Unicode character can be encoded with multiple (up to 4) bytes. Byte-level
    BPE initially splits the corpus into a sequence of bytes, instead of full-fledged
    Unicode characters. If a character is encoded with *n* bytes, the tokenizer will
    treat it as a sequence of *n* one-byte tokens. In this way, the size of the base
    vocabulary will always be 256 (the maximum unique values that we can store in
    a byte). In addition, byte-level BPE guarantees that we won’t encounter unknown
    tokens.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下，我们的基础词汇表包括文本语料库中的所有字符。如果这些是 Unicode 字符（这是通常的情况），我们最终可能会得到一个最多包含 150,000
    个词汇的词汇表。而且这还只是我们开始词汇合并过程之前的情况。解决这个问题的一个技巧是借助 **字节级 BPE**。每个 Unicode 字符可以使用多个（最多
    4 个）字节进行编码。字节级 BPE 最初将语料库拆分为字节序列，而不是完整的 Unicode 字符。如果一个字符使用 *n* 个字节编码，分词器将把它当作
    *n* 个单字节词汇进行处理。通过这种方式，基础词汇表的大小将始终为 256（字节中可以存储的最大唯一值）。此外，字节级 BPE 保证我们不会遇到未知的词汇。
- en: '**WordPiece** ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    is another subword tokenization algorithm. It is similar to BPE but with one main
    difference. Like BPE, it starts with a base vocabulary of individual characters
    and then proceeds to merge them into new composite tokens. However, it defines
    the merge order based on a score, computed with the following formula (unlike
    BPE, which uses frequent co-occurrence):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**WordPiece** ([https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144))
    是另一种子词分词算法。它与 BPE 相似，但有一个主要区别。像 BPE 一样，它从单个字符的基础词汇表开始，然后将它们合并成新的复合词汇。然而，它根据一个得分来定义合并顺序，得分通过以下公式计算（与使用频繁共现的
    BPE 不同）：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced><mo>=</mo><mfrac><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced></mrow><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub></mrow></mfenced><mo>×</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/397.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced><mo>=</mo><mfrac><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mfenced open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mfenced></mrow><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>1</mn></msub></mrow></mfenced><mo>×</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><msub><mi>n</mi><mn>2</mn></msub></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/397.png)'
- en: In this way, the algorithm prioritizes the merging of pairs where the individual
    tokens are less frequent in the corpus. Let’s compare this approach with BPE,
    which merges tokens based only on the potential gains of the new token. In contrast,
    WordPiece balances the gain (the nominator in the formula) with the potential
    loss of the existing tokens (the denominator). This makes sense because the new
    token will exist instead of the old pair of tokens, rather than alongside them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，算法优先合并那些在语料库中出现频率较低的词对。让我们将这种方法与 BPE 进行比较，BPE 仅根据新词汇的潜在增益来合并词汇。相比之下，WordPiece
    在增益（公式中的分子）和现有词汇的潜在损失（分母）之间进行平衡。这是有道理的，因为新词汇将取代旧的词对，而不是与它们并存。
- en: In-word tokens
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内部词汇
- en: 'WordPiece adds a special *##* prefix to all tokens inside a word, except for
    the first. For example, it will tokenize the word *aab* as `[a, ##a, ##b]`. The
    token merge removes the *##* between the tokens. So, when we merge *##a* and *##b*,
    *aab* becomes `[``a, ##ab]`.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'WordPiece 为单词中的所有标记添加一个特殊的 *##* 前缀，除了第一个。例如，它会将单词 *aab* 标记为 `[a, ##a, ##b]`。标记合并会去掉标记之间的
    *##*。因此，当我们合并 *##a* 和 *##b* 时，*aab* 会变成 `[``a, ##ab]`。'
- en: 'Unlike BPE, WordPiece only stores the final token vocabulary. When it tokenizes
    a new word, it finds the longest matching subword in the vocabulary and splits
    the word on it. For example, let’s assume that we want to split the word *abcd*
    with a token vocabulary of `[a, ##b, ##c, ##d, ab, ##cd, ##bcd]`. Following the
    new rule, WordPiece will first select the longest subword, *bcd*, and it will
    tokenize *abcd* as `[``a, ##bcd]`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '与 BPE 不同，WordPiece 只存储最终的标记词汇。当它对新词进行标记时，它会在词汇中找到最长的匹配子词，并在此处分割单词。例如，假设我们想用标记词汇
    `[a, ##b, ##c, ##d, ab, ##cd, ##bcd]` 来分割单词 *abcd*。根据新规则，WordPiece 会首先选择最长的子词
    *bcd*，然后将 *abcd* 标记为 `[``a, ##bcd]`。'
- en: BPE and WordPiece are greedy algorithms – they will always merge tokens deterministically,
    based on frequency criteria. However, encoding the same text sequence with different
    tokens might be possible. This could act as regularization for a potential NLP
    algorithm. Next, we’ll introduce a tokenization technique that takes advantage
    of this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 和 WordPiece 都是贪心算法——它们总是根据频率标准，确定性地合并标记。然而，使用不同的标记对相同的文本序列进行编码是可能的。这可能作为潜在
    NLP 算法的正则化方法。接下来，我们将介绍一种利用这一点的标记化技术。
- en: Unigram
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Unigram
- en: 'Unlike BPE and WordPiece, the **Unigram** (*Subword Regularization: Improving
    Neural Network Translation Models with Multiple Subword Candidates*, [https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959))
    algorithm starts with a large base vocabulary and progressively tries to reduce
    it. The initial base vocabulary is a union of all unique characters and the most
    common substrings of the corpus. One way to find the most common substrings is
    with BPE. The algorithm assumes that each token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png),
    occurs independently (hence the Unigram name). Because of this assumption, the
    probability of a token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/399.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/400.png),
    is just the number of its occurrences divided by the total size of the rest of
    the corpus. Then, the probability of a sequence of tokens with length *M*, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>X</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/401.png),
    is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BPE 和 WordPiece 不同，**Unigram**（*子词正则化：通过多个子词候选改进神经网络翻译模型*，[https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959)）算法从一个大词汇表开始，并逐步尝试将其缩减。初始词汇表是所有独特字符和语料库中最常见子串的并集。找到最常见子串的一种方法是使用
    BPE。该算法假设每个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png)，是独立发生的（因此得名
    Unigram）。基于这一假设，一个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/399.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/400.png)，的概率就是它出现的次数除以语料库其他部分的总大小。然后，长度为
    *M* 的标记序列，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>X</mml:mtext><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/401.png)，的概率如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>,</mo><mo>∀</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi><mo>,</mo><mrow><munderover><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><mrow
    /></munderover><mrow><mi>P</mi><mfenced open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/402.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>,</mo><mo>∀</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi><mo>,</mo><mrow><munderover><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><mrow
    /></munderover><mrow><mi>P</mi><mfenced open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/402.png)'
- en: Here, *V* is the full token vocabulary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*V* 是完整的标记词汇表。
- en: Say that we have the same token sequence, *X*, and multiple token segmentation
    candidates, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/403.png),
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有相同的令牌序列，*X*，并且有多个令牌分割候选项，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/403.png)，
- en: 'for that sequence. The most probable segmentation candidate, *x**, for *X*
    is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该序列。最可能的分割候选项，*x**，对于*X*如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>x</mi><mi
    mathvariant="normal">*</mi></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>,</mo><mi>x</mi><mo>∈</mo><mi>S</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mrow></math>](img/404.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>x</mi><mi
    mathvariant="normal">*</mi></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi>P</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>,</mo><mi>x</mi><mo>∈</mo><mi>S</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mrow></math>](img/404.png)'
- en: 'Let’s clarify this with an example. We’ll assume that our corpus consists of
    some (imaginary) words, `{dab: 5, aab: 7, bun: 4}`, where the digits indicate
    the number of occurrences of that word in the text. Our initial token vocabulary
    is a union of all unique characters and all possible substrings (the numbers indicate
    frequency): `{a: 19, b: 16, ab: 12, aa: 7, da: 5, d: 5, bu: 4, un: 4}`. The sum
    of all token frequencies is 19 + 16 + 12 + 7 + 5 + 5 + 4 + 4 = 72\. Then, the
    independent probability for each token is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mn>72</mml:mn></mml:math>](img/405.png)
    – for example, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mo>)</mo><mo>=</mo><mn>19</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.264</mn></mrow></mrow></mrow></math>](img/406.png),'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们通过一个例子来澄清这一点。我们假设我们的语料库包含一些（假想的）单词，`{dab: 5, aab: 7, bun: 4}`，其中数字表示该单词在文本中的出现次数。我们的初始令牌词汇是所有唯一字符和所有可能子字符串的并集（数字表示频率）：`{a:
    19, b: 16, ab: 12, aa: 7, da: 5, d: 5, bu: 4, un: 4}`。所有令牌频率的总和为 19 + 16 + 12
    + 7 + 5 + 5 + 4 + 4 = 72。然后，每个令牌的独立概率为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mn>72</mml:mn></mml:math>](img/405.png)
    – 例如，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mo>)</mo><mo>=</mo><mn>19</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.264</mn></mrow></mrow></mrow></math>](img/406.png)，'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo><mo>=</mo><mn>12</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.167</mn></mrow></mrow></mrow></math>](img/407.png),
    and so on.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo><mo>=</mo><mn>12</mn><mo>/</mo><mn>72</mn><mo>=</mo><mn>0.167</mn></mrow></mrow></mrow></math>](img/407.png)，等等。'
- en: Our extended vocabulary presents us with the possibility to tokenize each sequence
    (we’ll focus on words for simplicity) in multiple ways. For example, we can represent
    *dab* as either `{d, a, b}`, `{da, b}`, or `{d, ab}`. Here, the probabilities
    for each candidate are P({d, a, b}) = P(d) * P(a) * P(b) = 0.07 * 0.264 * 0.222
    = 0.0041; ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mi>a</mi><mo>,</mo><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.222</mn><mo>=</mo><mn>0.015</mn></mrow></mrow></mrow></math>](img/408.png);
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mo>,</mo><mi>a</mi><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.167</mn><mo>=</mo><mn>0.012</mn></mrow></mrow></mrow></math>](img/409.png).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展的词汇表使我们能够以多种方式对每个序列（为了简化起见，我们将重点放在单词上）进行分词。例如，我们可以将*dab*表示为`{d, a, b}`、`{da,
    b}`或`{d, ab}`。在这里，每个候选项的概率为P({d, a, b}) = P(d) * P(a) * P(b) = 0.07 * 0.264 *
    0.222 = 0.0041；![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mi>a</mi><mo>,</mo><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.222</mn><mo>=</mo><mn>0.015</mn></mrow></mrow></mrow></math>](img/408.png)；![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mo>{</mo><mi>d</mi><mo>,</mo><mi>a</mi><mi>b</mi><mo>}</mo><mo>)</mo><mo>=</mo><mn>0.07</mn><mi
    mathvariant="normal">*</mi><mn>0.167</mn><mo>=</mo><mn>0.012</mn></mrow></mrow></mrow></math>](img/409.png)。
- en: The candidate with the highest probability is *x** = `{da, b}`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 概率最高的候选项是*x** = `{da, b}`。
- en: 'With that, here’s how Unigram tokenization works step by step:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，以下是单元字(token)分词法的逐步实现过程：
- en: Start with the initial large base vocabulary, *V*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始的大型基础词汇表*V*开始。
- en: 'Repeat the following steps until the size of |*V*| reaches some minimum threshold
    value:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复以下步骤，直到|*V*|的大小达到某个最小阈值：
- en: Find the *l*-best tokenization candidates, *x**, for all words in the corpus
    with the help of the **Viterbi** algorithm ([https://en.wikipedia.org/wiki/Viterbi_algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
    – using this algorithm is necessary because this is a computationally intensive
    task). Taking *l* candidates, instead of one, makes it possible to sample different
    token sequences over the same text. You can think of this as a data augmentation
    technique over the input data, which provides additional regularization to the
    NLP algorithm. Once we have a tokenized corpus in this way, we can estimate the
    probabilities, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:math>](img/410.png),
    for all tokens of the current token vocabulary, *V*, with the help of an **expectation-minimization**
    algorithm ([https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)).
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**维特比**算法([https://en.wikipedia.org/wiki/Viterbi_algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm))，找到语料库中所有单词的*l*最佳分词候选项*x**。使用此算法是必要的，因为这是一项计算密集型任务。选择*l*个候选项，而不是一个，使得可以在相同文本上采样不同的词元序列。你可以将这看作是对输入数据的一种数据增强技术，它为NLP算法提供了额外的正则化。一旦我们以这种方式得到了一个分词后的语料库，就可以利用**期望最大化**算法([https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm))估计当前词汇表*V*中所有词元的概率![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:math>](img/410.png)。
- en: For each token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png),
    compute a special loss function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/412.png),
    which determines how the likelihood of the corpus is reduced if we remove ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)
    from the token vocabulary.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/113.png)，计算一个特殊的损失函数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/412.png)，它确定如果我们从标记词汇中移除![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/115.png)，语料库的概率如何减少。
- en: Sort the tokens by their ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/414.png)
    and preserve only the top *n* % of the tokens (for example, *n = 80*). Always
    preserve the individual characters to avoid unknown tokens.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照它们的![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/414.png)排序，并只保留前*n*%的标记（例如，*n
    = 80*）。始终保留个别字符，以避免未知标记。
- en: This concludes our introduction to tokenization. Some of these techniques were
    developed alongside the transformer architecture and we’ll make the most use of
    them in the following chapters. But for now, let’s focus on another fundamental
    technique in the NLP pipeline.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对分词的介绍。这些技术中的一些是在Transformer架构出现时发展起来的，我们将在接下来的章节中充分利用它们。但现在，让我们集中讨论NLP管道中的另一项基础技术。
- en: Introducing word embeddings
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入词嵌入
- en: Now that we’ve learned how to tokenize the text corpus, we can proceed to the
    next step in the NLP data processing pipeline. For the sake of clarity, we’ll
    assume that we’ve tokenized the corpus into words, rather than subwords or characters
    (in this section, *word* and *token* are interchangeable).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何对文本语料库进行分词，我们可以继续NLP数据处理管道中的下一步。为了简便起见，我们假设我们已将语料库分割成单词，而不是子词或字符（在本节中，*单词*和*标记*是可以互换的）。
- en: 'One way to feed the words of the sequence as input to the NLP algorithm is
    with one-hot encoding. Our input vector will have the same size as the number
    of tokens in the vocabulary and each token will have a unique one-hot encoded
    representation. However, this approach has a few drawbacks, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将序列中的词作为输入传递给NLP算法的一种方法是使用独热编码。我们的输入向量的大小将与词汇中标记的数量相同，每个标记将具有唯一的独热编码表示。然而，这种方法有一些缺点，如下所示：
- en: '**Sparse inputs**: The one-hot representation consists of mostly zeros and
    a single value. If our NLP algorithm is an NN (and it is), this type of input
    will activate only a small portion of its weights per word. Because of this, we’ll
    need a large training set to include a sufficient number of training samples of
    each word of the vocabulary.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏输入**：独热编码表示大多数值为零，只有一个非零值。如果我们的NLP算法是神经网络（而且确实如此），这种类型的输入每个词只会激活其权重的一小部分。因此，我们需要一个大规模的训练集，以包含每个词汇中足够数量的训练样本。'
- en: '**Computational intensity**: The large size of the vocabulary will result in
    large input tensors, which require large NNs and more computational resources.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算强度**：词汇的庞大规模将导致输入张量很大，这需要更大的神经网络和更多的计算资源。'
- en: '**Impracticality**: Every time we add a new word to the vocabulary, we’ll increase
    its size. However, the size of the one-hot encoded input will also increase. Therefore,
    we’ll have to change the structure of our NN to accommodate the new size and we’ll
    perform additional training.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不切实际**：每次我们向词汇表中添加一个新单词时，词汇表的大小会增加。然而，独热编码的输入大小也会增加。因此，我们必须改变神经网络的结构以适应新的大小，并且需要进行额外的训练。'
- en: '**Lack of context**: Words such as *dog* and *wolf* are semantically similar,
    but the one-hot representation lacks a way to convey this similarity.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏上下文**：像*dog*和*wolf*这样的单词在语义上是相似的，但独热编码表示无法传达这种相似性。'
- en: 'In this section, we’ll try to solve these issues with the help of a lower-dimensional
    distributed representation of the words, known as **word embeddings** (*A Neural
    Probabilistic Language Model*, [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)).
    The distributed representation is created by learning an embedding function that
    transforms the one-hot encoded words into a lower-dimensional space of word embeddings,
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过低维分布式表示法来解决这些问题，这种表示被称为**词嵌入**（*神经概率语言模型*，[http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)）。分布式表示是通过学习一个嵌入函数来创建的，该函数将独热编码的单词转化为低维的词嵌入空间，具体如下：
- en: '![Figure 6.1 – Words -> one-hot encoding -> word embedding vectors](img/B19627_06_1.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 词汇 -> 独热编码 -> 词嵌入向量](img/B19627_06_1.jpg)'
- en: Figure 6.1 – Words -> one-hot encoding -> word embedding vectors
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 词汇 -> 独热编码 -> 词嵌入向量
- en: Words from the vocabulary with size *V* are transformed into one-hot encoding
    vectors of size *V*. Then, an **embedding function** transforms this *V*-dimensional
    space into a distributed representation (vector) of a **fixed** size, *D* (here,
    *D=4*). This vector serves as input to the NLP algorithm. We can see that the
    fixed and smaller vector size solves the issues of sparsity, computational intensity,
    and impracticality we just described. Next, we’ll see how it solves the context
    issue.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从词汇表中，大小为*V*的单词被转化为大小为*V*的独热编码向量。然后，**嵌入函数**将这个*V*维空间转化为一个**固定**大小的分布式表示（向量），*D*（这里，*D=4*）。这个向量作为输入传递给NLP算法。我们可以看到，固定且较小的向量大小解决了我们刚才提到的稀疏性、计算强度和不切实际的问题。接下来，我们将看到它是如何解决上下文问题的。
- en: The embedding function learns semantic information about the words. It maps
    each word in the vocabulary to a continuous-valued vector representation – that
    is, the word embedding. Each word corresponds to a point in this embedding space,
    and different dimensions correspond to the grammatical or semantic properties
    of these words. The concept of embedding space is similar to the latent space
    representation, which we first discussed in the context of diffusion models in
    [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入函数学习关于单词的语义信息。它将词汇表中的每个单词映射到一个连续值向量表示——即词嵌入。每个单词在这个嵌入空间中对应一个点，不同的维度对应这些单词的语法或语义属性。嵌入空间的概念类似于潜在空间表示，我们在[*第
    5 章*](B19627_05.xhtml#_idTextAnchor146)中首次讨论了这一点，涉及到扩散模型。
- en: The goal is to ensure that the words close to each other in the embedding space
    have similar meanings. By *close to each other*, we mean a high value of the dot
    product (similarity) of their embedding vectors. In this way, the information
    that some words are semantically similar can be exploited by the ML algorithm.
    For example, it might learn that *fox* and *cat* are semantically related and
    that both *the quick brown fox* and *the quick brown cat* are valid phrases. A
    sequence of words can then be replaced with a sequence of embedding vectors that
    capture the characteristics of these words. We can use this sequence as a base
    for various NLP tasks. For example, a classifier trying to classify the sentiment
    of an article might be trained on previously learned word embeddings, instead
    of one-hot encoding vectors. In this way, the semantic information of the words
    becomes readily available for the sentiment classifier.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是确保在嵌入空间中彼此接近的词语具有相似的含义。这里所说的*接近*是指它们的嵌入向量的点积（相似度）值较高。通过这种方式，某些词语在语义上相似的信息可以被机器学习算法利用。例如，它可能会学到*fox*和*cat*在语义上是相关的，并且*the
    quick brown fox*和*the quick brown cat*都是有效的短语。然后，一个词语序列可以被一组嵌入向量所替代，这些向量捕捉了这些词语的特征。我们可以将这个序列作为各种自然语言处理（NLP）任务的基础。例如，试图对文章情感进行分类的分类器，可能会基于之前学到的词嵌入进行训练，而不是使用独热编码向量。通过这种方式，词语的语义信息可以轻松地为情感分类器所用。
- en: The mapping between one-hot representation and embedding vectors
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 独热表示与嵌入向量之间的映射
- en: Let’s assume that we have already computed the embedding vectors of each token.
    One way to implement the mapping between the one-hot representation and the actual
    embedding vector is with the help of a *V×D*-shaped matrix, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png).
    We can think of the matrix rows as a lookup table, where each row represents one
    word embedding vector. It works thanks to the one-hot encoded input word, which
    is a vector of all zeros, except for the index of the word itself. Because of
    this, the input word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/416.png),
    will only activate its unique row (vector) of weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/417.png),
    in ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/418.png).
    So, for each input sample (word), only the word’s embedding vector will participate.
    We can also think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)
    as a weight matrix of a **fully connected** (**FC**) NN layer. In this way, we
    can embed the embeddings (get it?) as the first NN layer – that is, the NN takes
    the one-hot encoded token as input and the embedding layer transforms it into
    a vector. Then, the rest of the NN uses the embedding vector instead of the one-hot
    representation. This is a standard implementation across all deep learning libraries.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经计算出了每个词元的嵌入向量。一种实现一热编码表示与实际嵌入向量之间映射的方法是借助一个*V×D*形状的矩阵，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)。我们可以把矩阵的行看作查找表，其中每一行代表一个词的嵌入向量。这个过程之所以可行，是因为输入的词是经过一热编码的，这个向量中除了对应词的索引位置是1外，其它位置全为0。因此，输入的词，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/416.png)，将仅激活其对应的唯一行（向量）权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/417.png)，位于![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/418.png)中。因此，对于每一个输入样本（词），只有该词的嵌入向量会参与计算。我们还可以把![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/415.png)看作一个**全连接**（**FC**）神经网络层的权重矩阵。通过这种方式，我们可以将嵌入（明白了吗？）作为神经网络的第一层
    —— 即，神经网络将一热编码的词元作为输入，嵌入层将其转换为一个向量。然后，神经网络的其余部分使用嵌入向量而不是一热编码表示。这是所有深度学习库中常见的标准实现。
- en: The concept of word embeddings was first introduced more than 20 years ago but
    remains one of the central paradigms in NLP today. **Large language models** (**LLMs**),
    such as ChatGPT, use improved versions of word embeddings, which we’ll discuss
    in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的概念最早是在20多年前提出的，但至今仍是自然语言处理领域的核心范式之一。**大型语言模型**（**LLMs**），例如ChatGPT，使用的是改进版的词嵌入，我们将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论。
- en: Now that we are familiar with embedding vectors, we’ll continue with the algorithm
    to obtain and compute them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了嵌入向量，我们将继续进行获取和计算嵌入向量的算法。
- en: Word2Vec
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'A lot of research has gone into creating better word embedding models, in particular
    by omitting to learn the probability function over sequences of words. One of
    the most popular ways to do this is with **Word2Vec** ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
    https://arxiv.org/abs/1301.3781, and https://arxiv.org/abs/1310.4546). It creates
    embedding vectors based on the context (surrounding words) of the word in focus.
    More specifically, the context is the *n* preceding and the *n* following words
    of the focus word. The following figure shows the context window as it slides
    across the text, surrounding different focus words:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 很多研究都致力于创建更好的词嵌入模型，特别是通过省略对单词序列的概率函数学习来实现。其中一种最流行的方法是**Word2Vec** ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
    https://arxiv.org/abs/1301.3781, 和 https://arxiv.org/abs/1310.4546)。它基于目标词的上下文（周围单词）创建嵌入向量。更具体地说，上下文是目标词前后的*n*个单词。下图展示了上下文窗口在文本中滑动，围绕不同的目标词：
- en: "![Figure 6.2 – A Word2Vec sliding context window with n=2\\. The same type\
    \ of context window applies to both CBOW and \uFEFFskip-gram](img/B19627_06_2.jpg)"
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 一个 Word2Vec 滑动上下文窗口，n=2。相同类型的上下文窗口适用于 CBOW 和 skip-gram](img/B19627_06_2.jpg)'
- en: Figure 6.2 – A Word2Vec sliding context window with n=2\. The same type of context
    window applies to both CBOW and skip-gram
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 一个 Word2Vec 滑动上下文窗口，n=2。相同类型的上下文窗口适用于 CBOW 和 skip-gram
- en: 'Word2vec comes in two flavors: **Continuous Bag of Words** (**CBOW**) and **skip-gram**.
    We’ll start with CBOW and then we’ll continue with skip-gram.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 有两种版本：**连续词袋模型** (**CBOW**) 和 **skip-gram**。我们将从 CBOW 开始，然后继续讨论 skip-gram。
- en: CBOW
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CBOW
- en: 'CBOW predicts the most likely word given its context (surrounding words). For
    example, given the sequence *the quick _____ fox jumps*, the model will predict
    *brown*. It takes all words within the context window with equal weights and doesn’t
    consider their order (hence the *bag* in the name). We can train the model with
    the help of the following simple NN with input, hidden, and output layers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 根据上下文（周围单词）预测最可能的词。例如，给定序列 *the quick _____ fox jumps*，模型将预测 *brown*。它对上下文窗口内的所有单词赋予相等的权重，并且不考虑它们的顺序（因此名字中有“*bag*”）。我们可以借助以下简单的神经网络进行训练，该网络包含输入层、隐藏层和输出层：
- en: '![Figure 6.3 – A CBOW model NN](img/B19627_06_3.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 一个 CBOW 模型神经网络](img/B19627_06_3.jpg)'
- en: Figure 6.3 – A CBOW model NN
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 一个 CBOW 模型神经网络
- en: 'Here’s how the model works:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是模型的工作方式：
- en: The input is the one-hot-encoded word representation (its length is equal to
    the vocabulary size, *V*).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是一个独热编码的单词表示（其长度等于词汇表大小，*V*）。
- en: The embedding vectors are represented by the *input-to-hidden* matrix, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入向量由*输入到隐藏*矩阵表示，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)。
- en: The embedding vectors of all context words are averaged to produce the output
    of the hidden network layer (there is no activation function).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有上下文单词的嵌入向量被平均以产生隐藏网络层的输出（没有激活函数）。
- en: The hidden activations serve as input to the output **Softmax** layer of size
    *V* (with the hidden-to-output weight matrix, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/421.png)),
    which predicts the most likely word to be found in the context (proximity) of
    the input words. The index with the highest activation represents the one-hot-encoded
    related word.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层激活值作为输入传递给**Softmax**输出层，大小为*V*（与隐藏到输出的权重矩阵，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/421.png))，用于预测最可能出现在输入词汇上下文（邻近）中的词汇。具有最高激活值的索引表示最相关的单词，采用独热编码表示。
- en: We’ll train the NN with gradient descent and backpropagation. The training set
    consists of (context and label) one-hot encoded pairs of words that appear close
    to each other in the text. For example, if part of the text is `[the, quick, brown,
    fox, jumps]` and *n=2*, the training tuples will include `([quick, brown], the),
    ([the, brown, fox], quick)`, `([the, quick, fox jumps], brown)`, and so on. Since
    we are only interested in the embeddings, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/422.png),
    we’ll discard the output NN weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/423.png),
    when the training is finished.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用梯度下降和反向传播训练神经网络。训练集包含的是（上下文和标签）一对一的独热编码单词对，这些单词在文本中彼此接近。例如，如果文本的一部分是 `[the,
    quick, brown, fox, jumps]` 且 *n=2*，训练元组将包括 `([quick, brown], the)`，`([the, brown,
    fox], quick)`，`([the, quick, fox jumps], brown)` 等等。由于我们只关心词嵌入，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/422.png)，我们将在训练完成后丢弃输出神经网络的权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/423.png)。
- en: CBOW will tell us which word is most likely to appear in a given context. This
    could be a problem for rare words. For example, given the context *The weather
    today is really _____*, the model will predict the word *beautiful* rather than
    *fabulous* (hey, it’s just an example). CBOW is several times faster to train
    than the skip-gram and achieves slightly better accuracy for frequent words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 会告诉我们在给定上下文中最可能出现的单词。这对于稀有词可能是一个问题。例如，给定上下文 *今天的天气真是_____*, 模型会预测单词 *beautiful*
    而不是 *fabulous*（嘿，这只是个例子）。CBOW 的训练速度是 skip-gram 的几倍，而且对于常见单词的准确度稍好。
- en: Skip-gram
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Skip-gram
- en: 'The skip-gram model can predict the context of a given input word (the opposite
    of CBOW). For example, the word *brown* will predict the words *The quick fox
    jumps*. Unlike CBOW, the input is a single one-hot encoded word vector. But how
    do we represent the context words in the output? Instead of trying to predict
    the whole context (all surrounding words) simultaneously, skip-gram transforms
    the context into multiple training pairs, such as `(fox, the)`, `(fox, quick)`,
    `(fox, brown)`, and `(fox, jumps)`. Once again, we can train the model with a
    simple single-layer NN:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型可以预测给定输入单词的上下文（与 CBOW 相反）。例如，单词 *brown* 会预测单词 *The quick fox jumps*。与
    CBOW 不同，输入是单一的独热编码单词向量。但如何在输出中表示上下文单词呢？Skip-gram 不试图同时预测整个上下文（所有周围单词），而是将上下文转化为多个训练对，例如
    `(fox, the)`，`(fox, quick)`，`(fox, brown)` 和 `(fox, jumps)`。再次强调，我们可以用一个简单的单层神经网络训练该模型：
- en: '![Figure 6.4 – A skip-gram model NN](img/B19627_06_4.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6.4 – A skip-gram model NN](img/B19627_06_4.jpg)'
- en: Figure 6.4 – A skip-gram model NN
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 一个 Skip-gram 模型神经网络
- en: As with CBOW, the output is a softmax, which represents the one-hot-encoded
    most probable context word. The input-to-hidden weights, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/424.png),
    represent the word embeddings lookup table, and the hidden-to-output weights,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/425.png),
    are only relevant during training. The hidden layer doesn’t have an activation
    function (that is, it uses linear activation).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与CBOW一样，输出是一个softmax，表示独热编码的最可能上下文单词。输入到隐藏层的权重，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/424.png)，表示词嵌入查找表，隐藏到输出的权重，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:math>](img/425.png)，仅在训练过程中相关。隐藏层没有激活函数（即，它使用线性激活）。
- en: 'We’ll train the model with backpropagation (no surprises here). Given a sequence
    of words, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/426.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/427.png),
    the objective of the skip-gram model is to maximize the average log probability,
    where *n* is the window size:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用反向传播训练模型（这里没有惊讶的地方）。给定一系列单词，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/426.png)![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/427.png)，skip-gram模型的目标是最大化平均对数概率，其中*n*是窗口大小：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/428.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/428.png)'
- en: 'The model defines the probability, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/429.png),
    as the following softmax formula:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型定义了概率，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/429.png)，如以下softmax公式所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>O</mi></msub><mo>|</mo><msub><mi>w</mi><mi>I</mi></msub></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>O</mi></msub><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></msubsup><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><mi>w</mi><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow></mrow></mfrac></mrow></mrow></math>](img/430.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>O</mi></msub><mo>|</mo><msub><mi>w</mi><mi>I</mi></msub></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msubsup><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>O</mi></msub><mrow><mo>′</mo><mi
    mathvariant="normal">⊤</mi></mrow></msubsup><msub><mi mathvariant="bold">v</mi><msub><mi>w</mi><mi>I</mi></msub></msub></mrow></mfenced></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mml:math>](img/430.png)'
- en: In this example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math>](img/431.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math>](img/432.png)
    are the input and output words, and **v** and **v**’ are the corresponding word
    vectors in the input and output weight matrices, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png),
    respectively (we keep the original notation of the paper). Since the NN doesn’t
    have a hidden activation function, its output value for one input/output word
    pair is simply the multiplication of the input word vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/435.png),
    and the output word vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/436.png)
    (hence the transpose operation).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math>](img/431.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math>](img/432.png)
    是输入和输出单词，而**v**和**v**' 是输入和输出权重矩阵中的相应词向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math>](img/420.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)，分别表示（我们保留了论文中的原始符号）。由于神经网络没有隐藏激活函数，其对于一对输入/输出单词的输出值仅仅是输入词向量![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/435.png)
    和输出词向量 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/436.png)（因此需要进行转置操作）。
- en: The authors of the Word2Vec paper note that word representations cannot represent
    idiomatic phrases that are not compositions of individual words. For example,
    *New York Times* is a newspaper, and not just a natural combination of the meanings
    of *New*, *York*, and *Times*. To overcome this, the model can be extended to
    include whole phrases. However, this significantly increases the vocabulary size.
    And, as we can see from the preceding formula, the softmax denominator needs to
    compute the output vectors for all words of the vocabulary. Additionally, every
    weight of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)
    matrix is updated on every training step, which slows the training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 论文的作者指出，词表示无法表示那些不是由单个词组成的习语。例如，*New York Times* 是一家报纸，而不仅仅是 *New*、*York*
    和 *Times* 各自含义的自然组合。为了解决这个问题，模型可以扩展到包括完整的短语。然而，这会显著增加词汇表的大小。而且，正如我们从前面的公式中看到的，softmax
    的分母需要计算所有词汇的输出向量。此外，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/434.png)
    矩阵的每个权重都在每一步训练时被更新，这也减慢了训练速度。
- en: To solve this, we can replace the softmax operation with the so-called `(fox,
    brown)`), as well as *k* additional negative pairs (for example, `(fox, puzzle)`),
    where *k* is usually in the range of [5,20]. Instead of predicting the word that
    best matches the input word (softmax), we’ll simply predict whether the current
    pair of words is true or not. In effect, we convert the multinomial classification
    problem (classified as one of many classes) into a binary logistic regression
    (or binary classification) problem. By learning the distinction between positive
    and negative pairs, the classifier will eventually learn the word vectors in the
    same way, as with multinomial classification. In Word2Vec, the words for the negative
    pairs are drawn from a special distribution, which draws less frequent words more
    often, compared to more frequent ones.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以用所谓的 `(fox, brown)` 替代 softmax 操作，并添加 *k* 个额外的负样本对（例如，`(fox, puzzle)`），其中
    *k* 通常在 [5,20] 的范围内。我们不再预测最符合输入词的单词（softmax），而是直接预测当前的词对是否为真实的。实际上，我们将多项分类问题（从多个类别中选择一个）转化为二元逻辑回归（或二分类）问题。通过学习正负词对的区别，分类器最终会以与多项分类相同的方式学习词向量。在
    Word2Vec 中，负样本词是从一个特殊分布中抽取的，该分布比频繁的词更常抽取不常见的词。
- en: Some of the most frequent words to occur carry less information value compared
    to the rare words. Examples of such words are the definite and indefinite articles
    *a*, *an*, and *the*. The model will benefit more from observing the pairs *London*
    and *city* compared to *the* and *city* because almost all words co-occur frequently
    with *the*. The opposite is also true – the vector representations of frequent
    words do not change significantly after training on many examples. To counter
    the imbalance between the rare and frequent words, the authors of the paper propose
    a subsampling approach, where each word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/114.png),
    of the training set is discarded with some probability, computed by the heuristic
    formula where
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与稀有词相比，一些最常出现的词携带的信息量较少。此类词的例子包括定冠词和不定冠词 *a*、*an* 和 *the*。与 *the* 和 *city* 相比，模型从观察
    *London* 和 *city* 的搭配中获益更多，因为几乎所有的词都与 *the* 经常同时出现。反之亦然——在大量例子上训练后，频繁词的向量表示不会发生显著变化。为了应对稀有词和频繁词之间的不平衡，论文的作者提出了一种子采样方法，其中训练集中的每个词，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/114.png)，会以某个概率被丢弃，这个概率通过启发式公式计算得出。
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/439.png)
    is the frequency of word ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/15.png)
    and *t* is a threshold (usually around ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/441.png)):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/439.png)
    是单词 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>](img/15.png)
    的频率，*t*是一个阈值（通常约为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/441.png)）：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><msqrt><mfrac><mi>t</mi><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced></mrow></mfrac></msqrt></mrow></mrow></math>](img/442.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><msqrt><mfrac><mi>t</mi><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mi>i</mi></msub></mfenced></mrow></mfrac></msqrt></mrow></mrow></math>](img/442.png)'
- en: It aggressively subsamples words with a frequency greater than *t* but also
    preserves the ranking of the frequencies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 它会积极地对频率大于*t*的单词进行子采样，同时保持频率的排名。
- en: We can say that, in general, skip-gram performs better on rare words than CBOW,
    but it takes longer to train.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，一般而言，跳字模型（skip-gram）在稀有词上的表现比CBOW更好，但训练时间更长。
- en: Now that we’ve learned about embedding vectors, let’s learn how to visualize
    them.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了嵌入向量，让我们学习如何可视化它们。
- en: Visualizing embedding vectors
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化嵌入向量
- en: 'A successful word embedding function will map semantically similar words to
    vectors with high dot product similarity in the embedding space. To illustrate
    this, we’ll implement the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成功的词嵌入函数将语义相似的单词映射到嵌入空间中具有高点积相似度的向量。为了说明这一点，我们将实现以下步骤：
- en: Train a Word2Vec skip-gram model on the `text8` dataset, which consists of the
    first 100,000,000 bytes of plain text from Wikipedia ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)).
    Each embedding vector is 100-dimensional, which is the default value for this
    type of model.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`text8`数据集上训练Word2Vec跳字模型，该数据集包含来自维基百科的前1亿字节的纯文本（[http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)）。每个嵌入向量是100维的，这是该类型模型的默认值。
- en: Select a list of *seed* words. In this case, the words are *mother*, *car*,
    *tree*, *science*, *building, elephant*, and *green*.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个*种子*词列表。在此案例中，词语包括*mother*、*car*、*tree*、*science*、*building*、*elephant*和*green*。
- en: Compute the dot-product similarity between the Word2Vec embedding vector of
    each seed word and the embedding vectors of all other words in the vocabulary.
    Then, select a cluster of the top-*k* (in our case, *k=5*) similar words (based
    on their dot-product similarity) for each seed word.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个种子词的Word2Vec嵌入向量与词汇表中所有其他单词嵌入向量之间的点积相似度。然后，为每个种子词选择一组前*k*（在我们的例子中，*k=5*）个最相似的单词（基于它们的点积相似度）。
- en: 'Visualize the similarity between the seed embeddings and the embeddings of
    their respective clusters of similar words in a 2D plot. Since the embeddings
    are 100-dimensional, we’ll use the t-SNE ([https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding))
    dimensionality-reduction algorithm. It maps each high-dimensional embedding vector
    on a two- or three-dimensional point in a way where similar objects are modeled
    on nearby points and dissimilar objects are modeled on distant points with a high
    probability. We can see the result in the following scatterplot:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在二维图中可视化种子嵌入与其相似词汇聚类嵌入之间的相似性。由于嵌入是100维的，我们将使用t-SNE（[https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)）降维算法。它将每个高维嵌入向量映射到二维或三维点，方法是将相似的对象建模为邻近点，而将不相似的对象建模为距离较远的点，且这种建模方式具有较高的概率。我们可以在下面的散点图中看到结果：
- en: '![Figure 6.5 – t-SNE visualization of the seed words and their clusters of
    the most similar words](img/B19627_06_5.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – t-SNE 可视化种子词及其最相似词汇的聚类](img/B19627_06_5.jpg)'
- en: Figure 6.5 – t-SNE visualization of the seed words and their clusters of the
    most similar words
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – t-SNE 可视化种子词及其最相似词汇的聚类
- en: This graph proves that the obtained word vectors contain relevant information
    for the words.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该图证明了所获得的词向量包含了与单词相关的信息。
- en: Word2Vec (and similar models) create **static** (or **context independent**)
    **embeddings**. Each word has a single embedding vector, based on all occurrences
    (that is, all contexts) of that word in the text corpus. This imposes some limitations.
    For example, *bank* has a different meaning in different contexts, such as *river
    bank*, *savings bank*, and *bank holiday*. Despite this, it is represented with
    a single embedding. In addition, the static embedding doesn’t take into account
    the word order in the context. For example, the expressions *I like apples, but
    I don’t like oranges* and *I like oranges, but I don’t like apples* have opposite
    meanings, but Word2Vec interprets them as the same. We can solve these problems
    with the so-called **dynamic** (**context dependent**) **embeddings**, which we’ll
    discuss in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec（和类似的模型）创建**静态**（或**上下文无关**）**嵌入**。每个单词都有一个单一的嵌入向量，基于该单词在文本语料库中的所有出现（即所有上下文）。这带来了一些局限性。例如，*bank*在不同的上下文中有不同的含义，如*river
    bank*（河岸）、*savings bank*（储蓄银行）和*bank holiday*（银行假日）。尽管如此，它还是通过单一的嵌入进行表示。此外，静态嵌入没有考虑上下文中的单词顺序。例如，表达式*I
    like apples, but I don’t like oranges*（我喜欢苹果，但我不喜欢橙子）和*I like oranges, but I don’t
    like apples*（我喜欢橙子，但我不喜欢苹果）具有相反的含义，但Word2Vec将它们视为相同的句子。我们可以通过所谓的**动态**（**上下文相关**）**嵌入**来解决这些问题，后者将在[*第7章*](B19627_07.xhtml#_idTextAnchor202)中讨论。
- en: So far, we’ve focused on single words (or tokens). Next, we’ll expand our scope
    to text sequences.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于单个单词（或标记）。接下来，我们将扩展我们的研究范围，探索文本序列。
- en: Language modeling
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: A word-based **language model** (**LM**) defines a probability distribution
    over sequences of **tokens**. For this section, we’ll assume that the tokens are
    words. Given a sequence of words of length *m* (for example, a sentence), an LM
    assigns a probability, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/443.png),
    that the full sequence of words could exist. One application of these probabilities
    is a generative model to create new text – a word-based LM can compute the likelihood
    of the next word, given an existing sequence of words that precede it. Once we
    have this new word, we can append it to the existing sequence and predict yet
    another new word, and so on. In this way, we can generate new text sequences with
    arbitrary length. For example, given the sequence *the quick brown*, the LM might
    predict *fox* as the next most likely word. Then, the sequence becomes *the quick
    brown fox*, and we task the LM to predict the new most likely word based on the
    updated sequence. A model whose output depends on its previous values, as well
    as its stochastic (that is, with some randomness) output (new value), is called
    an **autoregressive model**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll focus on the properties of the word sequence, rather than the model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Even the most advanced LLMs, such as ChatGPT, are autoregressive models – they
    just predict the next word, one word at a time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Understanding N-grams
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The inference of the probability of a long sequence, say ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/444.png),
    is typically infeasible. To understand why, let’s note that we can calculate the
    joint probability of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/445.png)
    with the chain rule of joint probability ([*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/446.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'The probability of the later words given the earlier words would be especially
    difficult to estimate from the data. That’s why this joint probability is typically
    approximated by an independence assumption that the *i*-th word is only dependent
    on the *n-1* previous words. We’ll only model the joint probabilities of combinations
    of *n* sequential words, called *n*-grams. For example, in the phrase *the quick
    brown fox*, we have the following *n*-grams:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**1-gram** (**unigram**): *the*, *quick*, *brown*, and *fox* (this is where
    Unigram tokenization takes its name)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-gram** (**bigram**): *the quick*, *quick brown*, and *brown fox*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3-gram** (**trigram**): *the quick brown* and *quick* *brown fox*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4-gram**: *the quick* *brown fox*'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The term *n*-grams can refer to other types of sequences of length *n*, such
    as *n* characters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The inference of the joint distribution is approximated with the help of *n*-gram
    models that split the joint distribution into multiple independent parts. If we
    have a large corpus of text, we can find all the *n*-grams up until a certain
    *n* (typically 2 to 4) and count the occurrence of each *n*-gram in that corpus.
    From these counts, we can estimate the probabilities of the last word of each
    *n*-gram, given the previous *n-1* words:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Unigram: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>total number of words in the corpus</mml:mtext></mml:mrow></mml:mfrac></mml:math>](img/447.png)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bigram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/448.png)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n*-gram: ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub><mo>|</mo><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi></mrow></msub></mrow></mfenced></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>+</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced></mrow></mfrac></mstyle></mrow></mrow></math>](img/449.png)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independent assumption that the *i*-th word is only dependent on the previous
    *n-1* words can now be used to approximate the joint distribution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can approximate the joint distribution for a unigram with the
    following formula:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/450.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'For a trigram, we can approximate the joint distribution with the following
    formula:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/451.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: We can see that, based on the vocabulary size, the number of *n*-grams grows
    exponentially with *n*. For example, if a small vocabulary contains 100 words,
    then the number of possible 5-grams would be ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>10,000,000,000</mml:mn></mml:math>](img/452.png)
    different 5-grams. In comparison, the entire works of Shakespeare contain around
    30,000 different words, illustrating the infeasibility of using *n*-grams with
    a large *n*. Not only is there the issue of storing all the probabilities, but
    we would also need a very large text corpus to create decent *n*-gram probability
    estimations for larger values of *n*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: When the number of possible input variables (words) increases, the number of
    different combinations of these input values increases exponentially. This problem
    is known as the curse of dimensionality. It arises when the learning algorithm
    needs at least one example per relevant combination of values, which is the case
    in *n*-gram modeling. The larger our *n*, the better we can approximate the original
    distribution and the more data we would need to make good estimations of the *n*-gram
    probabilities.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: But fret not, as the *n*-gram LM gives us some important clues on how to proceed.
    Its theoretical formulation is sound, but the curse of dimensionality makes it
    unfeasible. In addition, the *n*-gram model reinforces the importance of the word
    context, just as with Word2Vec. In the next few sections, we’ll learn how to simulate
    an *n*-gram model probability distribution with the help of NNs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Introducing RNNs
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RNN is a type of NN that can process sequential data with variable length.
    Examples of such data include text sequences or the price of a stock at various
    moments in time. By using the word *sequential*, we imply that the sequence elements
    are related to each other and their order matters. For example, if we take a book
    and randomly shuffle all the words in it, the text will lose its meaning, even
    though we’ll still know the individual words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs get their name because they apply the same function over a sequence recurrently.
    We can define an RNN as a recurrence relation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/453.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f* is a differentiable function, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    is a vector of values called internal RNN state (at step *t*), and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    is the network input at step *t*. Unlike regular NNs, where the state only depends
    on the current input (and RNN weights), here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)
    is a function of both the current input, as well as the previous state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png).
    You can think of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/458.png)
    as the RNN’s summary of all previous inputs. The recurrence relation defines how
    the state evolves step by step over the sequence via a feedback loop over previous
    states, as illustrated in the following diagram:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – An unfolded RNN](img/B19627_06_6.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – An unfolded RNN
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: On the left, we have a visual illustration of the RNN recurrence relation. On
    the right, we have the RNN states recurrently unfolded over the sequence *t-1*,
    *t*, *t+1*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN has three sets of parameters (or weights), shared between all steps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**U**: Transforms the input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    into the state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**W**: Transforms the previous state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/457.png),
    into the current state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: Maps the newly computed internal state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png),
    to the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/464.png)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**U**, **V**, and **W** apply linear transformation over their respective inputs.
    The most basic case of such a transformation is the familiar FC operation we know
    and love (therefore, **U**, **V**, and **W** are weight matrices). We can now
    define the internal state and the RNN output as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/465.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">V</mml:mi></mml:math>](img/466.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Here, *f* is the non-linear activation function (such as tanh, sigmoid, or ReLU).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a word-level LM, the input, *x*, will be a sequence of word
    embedding vectors (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/467.png)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The state, *s*, will be a sequence of state vectors (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/468.png)).
    Finally, the output, *y*, will be a sequence of probability vectors (![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/469.png))
    of the next words in the sequence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Note that in an RNN, each state is dependent on all previous computations via
    this recurrence relation. An important implication of this is that RNNs have memory
    over time because the states, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/454.png),
    contain information based on the previous steps. In theory, RNNs can remember
    information for an arbitrarily long period, but in practice, they are limited
    to looking back only a few steps. We will address this issue in more detail in
    the *Vanishing and exploding* *gradients* section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN we described is somewhat equivalent to a single-layer regular NN (with
    an additional recurrence relation). But as with regular NNs, we can stack multiple
    RNNs to form a **stacked RNN**. The cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/471.png),
    of an RNN cell at level *l* at time *t* will take the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/472.png),
    of the RNN cell from level *l-1* and the previous cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/473.png),
    of the cell at the same level *l* as the input:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math>](img/474.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we can see an unfolded, stacked RNN:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Stacked RNN](img/B19627_06_7.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Stacked RNN
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Because RNNs are not limited to processing fixed-size inputs, they expand the
    possibilities of what we can compute with NNs. We can identify several types of
    tasks, based on the relationship between the input and output sizes:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '**One-to-one**: Non-sequential processing, such as feedforward NNs and CNNs.
    There isn’t much difference between a feedforward NN and applying an RNN to a
    single time step. An example of one-to-one processing is image classification.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-to-many**: This generates a sequence based on a single input – for example,
    caption generation from an image (*Show and Tell: A Neural Image Caption* *Generator*,
    [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-one**: This outputs a single result based on a sequence – for example,
    sentiment classification of text.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-many indirect**: A sequence is encoded into a state vector, after
    which this state vector is decoded into a new sequence – for example, language
    translation (*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    and *Sequence to Sequence Learning with Neural* *Networks*, [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-many direct**: Outputs a result for each input step – for example,
    frame phoneme labeling in speech recognition.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The many-to-many models are often referred to as **sequence-to-sequence** (**seq2seq**)
    models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graphical representation of the preceding input-output combinations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – RNN input-output combinations, inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/](img/B19627_06_8.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – RNN input-output combinations, inspired by [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced RNNs, let’s improve our knowledge by implementing
    a simple RNN example.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: RNN implementation and training
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the preceding section, we briefly discussed what RNNs are and what problems
    they can solve. Let’s dive into the details of an RNN and how to train it with
    a very simple toy example: counting ones in a sequence.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: We’ll teach a basic RNN how to count the number of ones in the input and then
    output the result at the end of the sequence. This is an example of a many-to-one
    relationship, which we defined in the previous section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll implement this example with Python (no DL libraries) and numpy. An example
    of the input and output is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The RNN we’ll use is illustrated in the following diagram:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Basic RNN for counting ones in the input](img/B19627_06_9.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Basic RNN for counting ones in the input
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Since ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/475.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/476.png),
    *U*, *W*, and *y* are scalar values (**x** remains a vector), we won’t use the
    matrix notation (bold capital letters) in the RNN implementation and training
    section and its subsections. We’ll use italic notation instead. In the code sections,
    we’ll denote them as variables. However, note that the generic versions of these
    formulas use matrix and vector parameters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN will have only two parameters: an input weight, *U*, and a recurrence
    weight, *W*. The output weight, *V*, is set to 1 so that we just read out the
    last state as the output, *y*.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s add some code so that our example can be executed. We’ll import
    numpy and define our training set – inputs, **x**, and labels, *y*. **x** is two-dimensional
    since the first dimension represents the sample in the mini-batch. *y* is a single
    numerical value (it still has a batch dimension). For the sake of simplicity,
    we’ll use a mini-batch with a single sample:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The recurrence relation defined by this RNN is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>U</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/477.png).
    Note that this is a linear model since we don’t apply a non-linear function in
    this formula. We can implement a recurrence relationship in the following way:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The states, `s_t`, and the weights, `W` and `U`, are single scalar values. `x_t`
    represents a single element of the input sequence (in our case, one or zero).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution to this task is to just get the sum of the elements of the input
    sequence. If we set `U=1`, then whenever input is received, we will get its full
    value. If we set `W=1`, then the value we would accumulate would never decay.
    So, for this example, we would get the desired output: 3\. Nevertheless, let’s
    use this simple example to explain the training and implementation of the RNN.
    This will be interesting, as we will see in the rest of this section.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of an RNN as a special type of regular NN by unfolding it through
    time for a certain number of time steps (as illustrated in the preceding diagram).
    This regular NN has as many hidden layers as the size of the elements of the input
    sequence. In other words, one hidden layer represents one step through time. The
    only difference is that each layer has multiple inputs: the previous state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/478.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/479.png).
    The parameters, *U* and *W*, are shared between all of the hidden layers.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass unfolds the RNN along the sequence and builds a stack of states
    for each step. In the following code block, we can see an implementation of the
    forward pass, which returns the activation, *s*, for each recurrent step and each
    sample in the batch:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the RNN forward pass, let’s look at how to train our unfolded
    RNN.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Backpropagation through time** (**BPTT**) is the typical algorithm we use
    to train RNNs (*Backpropagation Through Time: What It Does and How to Do It*,
    [http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)).
    As its name suggests, it’s an adaptation of the backpropagation algorithm we discussed
    in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we’ll use the **mean squared error** (**MSE**) cost function.
    Now that we also have our forward step implementation, we can define how the gradient
    is propagated backward. Since the unfolded RNN is equivalent to a regular feedforward
    NN, we can use the backpropagation chain rule we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Because the weights, *W* and *U*, are shared across the layers, we’ll accumulate
    the error derivatives for each recurrent step, and in the end, we’ll update the
    weights with the accumulated value.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to get the gradient of the output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/480.png),
    concerning the loss function, *J*, *∂J/∂s*. Once we have it, we’ll propagate it
    backward through the stack of activities we built during the forward step. This
    backward pass pops activities off of the stack to accumulate their error derivatives
    at each time step. The recurrence relation that propagates this gradient through
    the RNN can be written as follows (chain rule):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of the weights, *U* and *W*, are accumulated as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/482.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/483.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Armed with this knowledge, let’s implement the backward pass:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Accumulate the gradients for `U` and `W` in `gU` and `gW`, respectively:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use gradient descent to optimize our RNN. Compute the gradients (using MSE)
    with the help of the backward function and use them to update the weights value:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the training for 150 epochs:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, display the loss function and the gradients for each weight over the
    epochs. We’ll do this with the help of the `plot_training` function, which is
    not implemented here but is available in the full example on GitHub. `plot_training`
    produces the following graph:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The RNN loss – uninterrupted line  – loss value; dashed lines
    – the weight gradients during training](img/B19627_06_10.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – The RNN loss – uninterrupted line – loss value; dashed lines –
    the weight gradients during training
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about backpropagation through time, let’s discuss how
    the familiar vanishing and exploding gradient problems affect it.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding example has an issue. To illustrate it, let’s run the training
    process with a longer sequence:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The reason for these warnings is that the final parameters, `U` and `W`, end
    up as `plot_training` function to produce the following result:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Parameters and loss function during an exploding gradients
    scenario](img/B19627_06_11.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Parameters and loss function during an exploding gradients scenario
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initial epochs, the gradients slowly increase, similar to the way they
    increased for the shorter sequence. However, when they get to epoch 23 (the exact
    epoch is unimportant, though), the gradient becomes so large that it goes out
    of the range of the float variable and becomes NaN (as illustrated by the jump
    in the plot). This problem is known as **exploding gradients**. We can stumble
    upon exploding gradients in a regular feedforward NN, but it is especially pronounced
    in RNNs. To understand why, let’s recall the recurrent gradient propagation chain
    rule for the two consecutive sequence steps we defined in the *Backpropagation
    through* *time* section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:math>](img/481.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Depending on the sequence’s length, an unfolded RNN can be much deeper compared
    to a regular NN. At the same time, the weights, *W*, of an RNN are shared across
    all of the steps. Therefore, we can generalize this formula to compute the gradient
    between two non-consecutive steps of the sequence. Because *W* is shared, the
    equation forms a geometric progression:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math>](img/485.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: "In our simple linear RNN, the gradient grows exponentially if *|W|>1* (exploding\
    \ gradient), where *W* is a single scalar weight – for example, 50 time steps\
    \ over *W=1.5* is ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msup><mi>W</mi><mrow><mo>(</mo><mn>50</mn><mo>)</mo></mrow></msup><mo>≈</mo><mn>637</mn><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mo>,</mo><mn>621</mn><mi mathvariant=\"normal\"\
    >\uFEFF</mi><mo>,</mo><mn>500</mn></mrow></mrow></math>](img/486.png). The gradient\
    \ shrinks exponentially if *|W|<1* (vanishing gradient), for example, 10 time\
    \ steps over *W=0.6* is ![<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"\
    \ xmlns:m=\"http://schemas.openxmlformats.org/officeDocument/2006/math\"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>10</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.00097</mml:mn></mml:math>](img/487.png).\
    \ If the weight parameter, *W*, is a matrix instead of a scalar, this exploding\
    \ or vanishing gradient is related to the largest eigenvalue, *ρ*, of *W* (also\
    \ known as a spectral radius). It is sufficient for *ρ<1* for the gradients to\
    \ vanish, and it is necessary for *ρ>1* for them to explode."
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The vanishing gradients problem, which we first mentioned in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    has another more subtle effect in RNNs: the gradient decays exponentially over
    the number of steps to a point where it becomes extremely small in the earlier
    states. In effect, they are overshadowed by the larger gradients from more recent
    time steps, and the RNN’s ability to retain the history of these earlier states
    vanishes. This problem is harder to detect because the training will still work,
    and the NN will produce valid outputs (unlike with exploding gradients). It just
    won’t be able to learn long-term dependencies.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are familiar with some of the problems surrounding RNNs. This
    knowledge will serve us well because, in the next section, we’ll discuss how to
    solve these problems with the help of a special type of RNN cell.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Long-short term memory
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hochreiter and Schmidhuber studied the problems of vanishing and exploding
    gradients extensively and came up with a solution called **long short-term memory**
    (**LSTM** – [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)
    and *Learning to Forget: Continual Prediction with LSTM*, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf).
    LSTMs can handle long-term dependencies due to a specially crafted memory cell.
    They work so well that most of the current accomplishments in training RNNs on
    a variety of problems are due to the use of LSTMs. In this section, we’ll explore
    how this memory cell works and how it solves the vanishing gradients issue.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a diagram of an LSTM cell:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). Inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/](img/B19627_06_12.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). Inspired by [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea of LSTM is the cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/488.png)
    (in addition to the hidden RNN state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/489.png)),
    where the information can only be explicitly written in or removed so that the
    state stays constant if there is no outside interference. The cell state can only
    be modified by specific gates, which are a way to let information pass through.
    A typical LSTM is composed of three gates: a **forget gate**, an **input gate**,
    and an **output gate**. The cell state, input, and output are all vectors so that
    the LSTM can hold a combination of different information blocks at each time step.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: LSTM notations
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/490.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/491.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/492.png)
    are the LSTM’s input, cell memory state, and output (or hidden state) vectors
    in moment *t*. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/493.png)
    is the candidate cell state vector (more on that later). The input, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/494.png),
    and the previous cell output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/495.png),
    are connected to each gate and the candidate cell vector with sets of FC weights,
    **W** and **U**, respectively. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/496.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/497.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/498.png)
    are the forget, input, and output gates of the LSTM cell (the gates use vector
    notation as well).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The gates are composed of FC layers, sigmoid activations, and element-wise
    multiplication (denoted with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>⨀</mml:mo></mml:math>](img/499.png)).
    Because the sigmoid only outputs values between 0 and 1, the multiplication can
    only reduce the value running through the gate. Let’s discuss them in order:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/500.png):
    It decides whether we want to erase parts of the existing cell state or not. It
    bases its decision on the weighted vector sum of the output of the previous cell,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/501.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png):'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/503.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding formula, we can see that the forget gate applies element-wise
    sigmoid activations to each element of the previous state vector, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/504.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/505.png)
    (note the circle-dot notation). Since the operation is elementwise, the values
    of this vector are squashed in the [0, 1] range. An output of 0 erases a specific
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/506.png)
    cell block completely and an output of 1 allows the information in that cell block
    to pass through. In this way, the LSTM can get rid of irrelevant information in
    its cell state vector.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/507.png):
    It decides what new information is going to be added to the memory cell in a multi-step
    process. The first step determines whether any information is going to be added.
    As in the forget gate, its decision is based on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/508.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png):
    it outputs 0 or 1 through the sigmoid function for each cell of the candidate
    state vector. An output of 0 means that no information is added to that cell block’s
    memory. As a result, the LSTM can store specific pieces of information in its
    cell state vector:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/510.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'In the next step of the input gate sequence, we compute the new candidate cell
    state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/511.png).
    It is based on the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/512.png),
    and the current input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    and is transformed via a tanh function:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/514.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: 'Then, we combine ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/515.png)
    with the sigmoid outputs of the input gate via element-wise multiplication: ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/516.png).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, the forget and input gates decide what information to forget and
    include from the previous and candidate cell states, respectively. The final version
    of the new cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/517.png),
    is just an element-wise sum between these two components:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/518.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: '**Output gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/519.png):
    It decides what the total cell output is going to be. It takes ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/520.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png)
    as inputs. It outputs a value in the (0, 1) range (via the sigmoid function) for
    each block of the cell’s memory. Like before, 0 means that the block doesn’t output
    any information and 1 means that the block can pass through as a cell’s output.
    Therefore, the LSTM can output specific blocks of information from its cell state
    vector:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/522.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the LSTM cell’s output is transferred by a tanh function:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/523.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: Because all these formulas are derivable, we can chain LSTM cells together,
    just like when we chain simple RNN states together and train the network via backpropagation
    through time.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: But how does the LSTM protect us from vanishing gradients? Let’s start with
    the forward phase. Notice that the cell state is copied identically from step
    to step if the forget gate is 1 and the input gate is
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '0: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/524.png).
    Only the forget gate can completely erase the cell’s memory. As a result, the
    memory can remain unchanged over a long period. Also, note that the input is a
    tanh activation that’s been added to the current cell’s memory. This means that
    the cell’s memory doesn’t blow up and is quite stable.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use an example to demonstrate how an LSTM cell is unfolded. For the sake
    of simplicity, we’ll assume that it has one-dimensional (single scalar value)
    input, state, and output vectors. Because the values are scalar, we won’t use
    vector notation for the rest of this example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Unrolling an LSTM through time](img/B19627_06_13.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Unrolling an LSTM through time
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: First, we have a value of 3 as a candidate state. The input gate is set to ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/525.png)
    and the forget gate is set to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/526.png).
    This means that the previous state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:math>](img/527.png),
    is erased and replaced with the new state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:mi>N</mml:mi><mml:mo>⊕</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/528.png).
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the next two time steps, the forget gate is set to 1, while the input gate
    is set to 0\. By doing this, all the information is kept throughout these steps
    and no new information is added because the input gate is set to 0: ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⨀</mml:mo><mml:mn>3</mml:mn><mml:mo>⊕</mml:mo><mml:mn>0</mml:mn><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math>](img/529.png).'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the output gate is set to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/530.png)
    and 3 is output and remains unchanged. We have successfully demonstrated how the
    internal state is stored across multiple steps.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s focus on the backward phase. The cell state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/531.png),
    can mitigate the vanishing/exploding gradients as well with the help of the forget
    gate, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png).
    Like the regular RNN, we can use the chain rule to compute the partial derivative,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/533.png),
    for two consecutive steps. Following the formula ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/534.png)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'and without going into details, its partial derivative is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/535.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'We can also generalize this to non-consecutive steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>…</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/536.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: If the forget gate values are close to 1, gradient information can pass back
    through the network states almost unchanged. This is because ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    uses sigmoid activation and information flow is still subject to the vanishing
    gradient that’s specific to sigmoid activations. But unlike the gradients in the
    regular RNN, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/532.png)
    has a different value at each time step. Therefore, this is not a geometric progression,
    and the vanishing gradient effect is less pronounced.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll introduce a new type of lightweight RNN cell that still preserves
    the properties of LSTM.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent units
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **gated recurrent unit** (**GRU**) is a type of recurrent block that was
    introduced in 2014 (*Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)
    and *Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*,
    https://arxiv.org/abs/1412.3555) as an improvement over LSTM. A GRU unit usually
    has similar or better performance than an LSTM, but it does so with fewer parameters
    and operations:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – A GRU cell diagram](img/B19627_06_14.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – A GRU cell diagram
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the classic RNN, a GRU cell has a single hidden state, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/539.png).
    You can think of it as a combination of the hidden and cell states of an LSTM.
    The GRU cell has two gates:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**Update gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/540.png):
    Combines the input and forget LSTM gates. It decides what information to discard
    and what new information to include in its place based on the network input, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/352.png),
    and the previous hidden state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/542.png).
    By combining the two gates, we can ensure that the cell will forget information,
    but only when we are going to include new information in its place:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/543.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: '**Reset gate**, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/544.png):
    Uses the previous hidden state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/545.png),
    and the network input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/357.png),
    to decide how much of the previous state to pass through:'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/547.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'Next, we have the candidate state, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold">U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/549.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the GRU output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/550.png),
    at time *t* is an element-wise sum between the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png),
    and the candidate output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⨀</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>⨀</mml:mo><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/553.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Since the update gate allows us to both forget and store data, it is directly
    applied to the previous output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/551.png),
    and applied over the candidate output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/548.png).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll conclude our introduction to RNNs by returning to the disclaimer at the
    start of this chapter – the practical limitations of RNNs. We can solve one of
    them – the vanishing and exploding gradients – with the help of LSTM or GRU cells.
    However, there are two others:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The RNN’s internal state is updated after each element of the sequence – a new
    element requires all preceding elements to be processed in advance. Therefore,
    the RNN sequence processing cannot be parallelized and RNNs cannot take advantage
    of the GPU parallelization capabilities.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The information of all preceding sequence elements is summarized in a single
    hidden cell state. The RNN doesn’t have direct access to the historical sequence
    elements and has to rely on the cell state instead. In practice, this means that
    an RNN (even LSTM or GRU) can meaningfully process sequences with a maximum length
    of around 100 elements.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we’ll see in the next chapter, the transformer architecture successfully
    solves both of these limitations. But for now, let’s see how to use LSTMs in practice.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Implementing text classification
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll use LSTM to implement a sentiment analysis example over
    the Large Movie Review Dataset (**IMDb**, [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)),
    which consists of 25,000 training and 25,000 testing reviews of popular movies.
    Each review has a binary label that indicates whether it is positive or negative.
    This type of problem is an example of a **many-to-one** relationship, which we
    defined in the *Recurrent neural networks (**RNNs)* section.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentiment analysis model is displayed in the following diagram:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Sentiment analysis with word embeddings and LSTM](img/B19627_06_15.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Sentiment analysis with word embeddings and LSTM
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe the model components (these are valid for any text classification
    algorithm):'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Each word of the sequence is replaced with its embedding vector. These embeddings
    can be produced with word2vec.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The word embedding is fed as input to the LSTM cell.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell output, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/556.png),
    serves as input to an FC layer with two output units and softmax. The softmax
    output represents the probability of the review being positive (1) or negative
    (0).
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The network can be produced with Word2Vec.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output for the final element of the sequence is taken as a result of the
    whole sequence.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To implement this example, we’ll use PyTorch and the TorchText package. It
    consists of data processing utilities and popular datasets for natural language.
    We’ll only include the interesting portions of the code, but the full example
    is available in this book’s GitHub repo. With that, let’s start:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the device (by default, this is GPU with a fallback on CPU):'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Start the training and testing dataset pipeline. First, define the `basic_english`
    tokenizer, which splits the text on spaces (that is, word tokenization):'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, use `tokenizer` to build the token `vocabulary`:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `IMDB(split='train')` provides an iterator of all movie reviews in the
    training set (each review is represented as a string). The `yield_tokens(IMDB(split='train'))`
    generator iterates over all samples and splits them into words. The result serves
    as input to `build_vocab_from_iterator`, which iterates over the tokenized samples
    and builds the token `vocabulary`. Note that the vocabulary only includes training
    samples. Therefore, any token that exists in the test set (but not the training
    one) will be replaced with the default unknown `<``unk>` token.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, define the `collate_batch` function, which takes a `batch` of tokenized
    samples with varying lengths, and concatenates them in a single long sequence
    of tokens:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the `samples` list aggregates all tokenized `_sample` instances of `batch`.
    In the end, they are concatenated into a single list. The `offsets` list contains
    the offset from the start of each concatenated sample. This information makes
    it possible to reverse-split the long `samples` sequence into separate items again.
    The purpose of the function is to create a compressed `batch` representation.
    This is necessary because of the varying length of each sample. The alternative
    would be to pad all samples to match the length of the longest one so that they
    can fit in the batch tensor. Fortunately, PyTorch provides us with the `offsets`
    optimization to avoid this. Once we feed the compressed batch to the RNN, it will
    automatically reverse it back into separate samples.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we define the LSTM model:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The model implements the scheme we introduced at the start of this section.
    As its name suggests, the `embedding` property (an instance of `EmbeddingBag`)
    maps the token (in our case, word) index to its embedding vector. We can see that
    the constructor takes the vocabulary size (`num_embeddings`) and the embedding
    vector size (`embedding_dim`). In theory, we could initialize `EmbeddingBag` with
    pre-computed Word2Vec embedding vectors. But in our case, we’ll simply use random
    initialization and let the model learn them as part of the training. `embedding`
    also takes care of the compressed batch representation (hence the `offsets` parameter
    in the `forward` method). The embedding’s output serves as input to the `rnn`
    LSTM cell, which, in turn, feeds the output `fc` layer.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Define the `train_model(model, cost_function, optimizer, data_loader)` and `test_model(model,
    cost_function, data_loader)` functions. These are almost the same functions that
    we first defined in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079), so we won’t
    include them here. However, they have been adapted to the compressed batch representation
    and the additional `offsets` parameter.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Proceed with the experiment. Instantiate the LSTM model, the cross-entropy
    cost function, and the Adam optimizer:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define `train_dataloader`, `test_dataloader`, and their respective datasets
    (use a mini-batch size of 64):'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the training for 5 epochs:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The model achieves a test accuracy in the realm of 87%.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This concludes our small practical example of LSTM text classification. Coincidentally,
    it also concludes this chapter.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced two complementary topics – NLP and RNNs. We discussed
    the tokenization technique and the most popular tokenization algorithms – BPE,
    WordPiece, and Unigram. Then, we introduced the concept of word embedding vectors
    and the Word2Vec algorithm to produce them. We also discussed the *n*-gram LM,
    which provided us with a smooth transition to the topic of RNNs. There, we implemented
    a basic RNN example and introduced two of the most advanced RNN architectures
    – LSTM and GRU. Finally, we implemented a sentiment analysis model.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll supercharge our NLP potential by introducing the
    attention mechanism and transformers.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
