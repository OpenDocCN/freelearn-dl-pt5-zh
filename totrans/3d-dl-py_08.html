<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer133">
<h1 class="chapter-number" id="_idParaDest-91" lang="en-GB"><a id="_idTextAnchor094"/>7</h1>
<h1 id="_idParaDest-92" lang="en-GB"><a id="_idTextAnchor095"/>Exploring Controllable Neural Feature Fields</h1>
<p lang="en-GB">In the previous chapter, you learned how to represent a 3D scene using <strong class="bold" lang="">Neural Radiance Fields</strong> (<strong class="bold" lang="">NeRF</strong>). We trained <a id="_idIndexMarker316"/>a single neural network on posed multi-view images of a 3D scene to learn an implicit representation of it. Then, we used the NeRF model to render the 3D scene from various other viewpoints and viewing angles. With this model, we assumed that the objects and the background <span class="No-Break" lang="">are unchanging.</span></p>
<p lang="en-GB">But it is fair to wonder whether it is possible to generate variations of the 3D scene. Can we control the number of objects, their poses, and the scene background? Can we learn about the 3D nature of things without posed images and without understanding the <span class="No-Break" lang="">camera parameters?</span></p>
<p lang="en-GB">By the end of this chapter, you will learn that it is indeed possible to do all these things. Concretely, you should have a better understanding of GIRAFFE, a very novel method for controllable 3D image synthesis. This combines ideas from the fields of image synthesis and implicit 3D representation learning using NeRF-like models. This will become clear as we cover the <span class="No-Break" lang="">following topics:</span></p>
<ul>
<li lang="en-GB">Understanding GAN-based <span class="No-Break" lang="">image synthesis</span></li>
<li lang="en-GB">Introducing compositional 3D-aware <span class="No-Break" lang="">image synthesis</span></li>
<li lang="en-GB">Generating <span class="No-Break" lang="">feature fields</span></li>
<li lang="en-GB">Mapping feature fields <span class="No-Break" lang="">to images</span></li>
<li lang="en-GB">Exploring controllable <span class="No-Break" lang="">scene generation</span></li>
<li lang="en-GB">Training the <span class="No-Break" lang="">GIRAFFE model</span></li>
</ul>
<h1 id="_idParaDest-93" lang="en-GB"><a id="_idTextAnchor096"/>Technical requirements</h1>
<p lang="en-GB">In order to run the example code snippets in this book, ideally, you need to have a computer with a GPU that has around 8 GB of GPU memory. Running code snippets with only CPUs is not impossible but will be extremely slow. The recommended computer configuration is <span class="No-Break" lang="">as follows:</span></p>
<ul>
<li lang="en-GB">A GPU device – for example, the Nvidia GTX series or the RTX series with at least 8 GB <span class="No-Break" lang="">of memory</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Python 3.7+</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Anaconda3</span></li>
</ul>
<p lang="en-GB">The code snippets for this chapter can be found <span class="No-Break" lang="">at </span><a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python"><span class="No-Break" lang="">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python</span></a><span class="No-Break" lang="">.</span></p>
<h1 id="_idParaDest-94" lang="en-GB"><a id="_idTextAnchor097"/>Understanding GAN-based image synthesis</h1>
<p lang="en-GB">Deep <a id="_idIndexMarker317"/>generative models have been shown to produce photorealistic 2D images when trained on a distribution from a particular domain. <strong class="bold" lang="">Generative Adversarial Networks</strong> (<strong class="bold" lang="">GANs</strong>) are one of the most widely used <a id="_idIndexMarker318"/>frameworks for this purpose. They can synthesize high-quality photorealistic images at resolutions of 1,024 x 1,024 and beyond. For example, they have been used to generate <span class="No-Break" lang="">realistic faces:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 7.1: Randomly generated faces as high-quality 2D images using StyleGAN2 " height="385" src="image/B18217_07_1a.jpg" width="383"/>
</div>
</div>
<p class="IMG---Figure" lang="en-GB"> </p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="Figure 7.1: Randomly generated faces as high-quality 2D images using StyleGAN2 " height="387" src="image/B18217_07_1b.jpg" width="385"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="Figure 7.1: Randomly generated faces as high-quality 2D images using StyleGAN2 " height="388" src="image/B18217_07_1c.jpg" width="386"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: Randomly generated faces as high-quality 2D images using StyleGAN2</p>
<p lang="en-GB">GANs can be trained to generate similar-looking images from any data distribution. The same StyleGAN2 model, when trained on a car dataset, can generate high-resolution images <span class="No-Break" lang="">of cars:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 7.2: Randomly generated cars as 2D images using StyleGAN2 " height="129" src="image/B18217_07_2.jpg" width="726"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: Randomly generated cars as 2D images using StyleGAN2</p>
<p lang="en-GB">GANs are <a id="_idIndexMarker319"/>based on a game-theoretic scenario where a generator neural network generates an image. However, in order to be successful, it must fool the discriminator into classifying it as a realistic image. This tug of war between the two neural networks (that is, the generator and the discriminator) can lead to a generator that produces photorealistic images. The generator network does this by creating a probability distribution on a multi-dimensional latent space such that the points on that distribution are realistic images from the domain of the training images. In order to generate a novel image, we just need to sample a point on the latent space and let the generator create an image <span class="No-Break" lang="">from it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 7.3: A canonical GAN " height="200" src="image/B18217_07_3.jpg" width="840"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: A canonical GAN</p>
<p lang="en-GB">Synthesizing high-resolution photorealistic images is great, but it is not the only desirable property of a generative model. More real-life applications open if the generation process is disentangled and controllable in a simple and predictable manner. More importantly, we need attributes such as object shape, size, and pose to be as disentangled as possible so that we can vary them without changing other attributes in <span class="No-Break" lang="">the image.</span></p>
<p lang="en-GB">Existing GAN-based image generation approaches generate 2D images without truly understanding the underlying 3D nature of the image. Therefore, there are no built-in explicit <a id="_idIndexMarker320"/>controls for varying attributes such as object position, shape, size, and pose. This results in GANs that have entangled attributes. For simplicity, think about an example of a GAN model that generates realistic faces, where changing the head pose also changes the perceived gender of the generated face. This can happen if the gender and head pose attributes become entangled. This is undesirable for most practical use cases. We need to be able to vary one attribute without affecting any of <span class="No-Break" lang="">the others.</span></p>
<p lang="en-GB">In the next section, we are going to look at a high-level overview of a model that can generate 2D images with an implicit understanding of the 3D nature of the <span class="No-Break" lang="">underlying scene.</span></p>
<h1 id="_idParaDest-95" lang="en-GB"><a id="_idTextAnchor098"/>Introducing compositional 3D-aware image synthesis</h1>
<p lang="en-GB">Our goal <a id="_idIndexMarker321"/>is controllable image synthesis. We need control over the number of objects in the image, their position, shape, size, and pose. The GIRAFFE model is <a id="_idIndexMarker322"/>one of the first to achieve all these desirable properties while also generating high-resolution photorealistic images. In order to have control over these attributes, the model must have some awareness of the 3D nature of <span class="No-Break" lang="">the scene.</span></p>
<p lang="en-GB">Now, let us look at how the GIRAFFE model builds on top of other established ideas to achieve this. It makes use of the following <span class="No-Break" lang="">high-level concepts:</span></p>
<ul>
<li lang="en-GB"><strong class="bold" lang="">Learning 3D representation</strong>: A NeRF-like model for learning implicit 3D representation <a id="_idIndexMarker323"/>and feature fields. Unlike the <a id="_idIndexMarker324"/>standard NeRF model, this model outputs a feature field instead of the color intensity. This NeRF-like model is used to enforce a 3D consistency in the <span class="No-Break" lang="">images generated.</span></li>
<li lang="en-GB"><strong class="bold" lang="">Compositional operator</strong>: A <a id="_idIndexMarker325"/>parameter-free <a id="_idIndexMarker326"/>compositional operator to compose feature fields of multiple objects into a single feature field. This will help in creating images with the desired number of objects <span class="No-Break" lang="">in them.</span></li>
<li lang="en-GB"><strong class="bold" lang="">Neural rendering model</strong>: This <a id="_idIndexMarker327"/>uses the <a id="_idIndexMarker328"/>composed feature field to create an image. This is a 2D <strong class="bold" lang="">Convolutional Neural Network</strong> (<strong class="bold" lang="">CNN</strong>) that <a id="_idIndexMarker329"/>upsamples the feature field to create a higher dimensional <span class="No-Break" lang="">output image.</span></li>
<li lang="en-GB"><strong class="bold" lang="">GAN</strong>: The GIRAFFE <a id="_idIndexMarker330"/>model uses the GAN <a id="_idIndexMarker331"/>model architecture to generate new scenes. The preceding three components form the generator. The model also consists of a discriminator neural network that distinguishes between fake images and real images. Due to the presence of a NeRF model along with a composition operator, this model will make the image generation process both compositional and <span class="No-Break" lang="">3D aware.</span></li>
</ul>
<p lang="en-GB">Generating an image is a <span class="No-Break" lang="">two-step process:</span></p>
<ol>
<li lang="en-GB">Volume-render a feature field given the camera viewing angle along with some information about the objects you want to render. This object information is some abstract vectors that you will learn about in <span class="No-Break" lang="">future sections.</span></li>
<li lang="en-GB">Use a neural rendering model to map the feature field to a <span class="No-Break" lang="">high-resolution image.</span></li>
</ol>
<p lang="en-GB">This two-step <a id="_idIndexMarker332"/>approach was found to be better at generating high-resolution images as compared to directly generating the RGB values from the NeRF model output. From the previous chapter, we know that a NeRF model is trained on images from the same scene. A trained model can only generate an image from the same scene. This was one of the big limitations of the <span class="No-Break" lang="">NeRF model.</span></p>
<p lang="en-GB">In contrast, the GIRAFFE model is trained on images of unposed images from different scenes. A trained model can generate images from the same distribution as what it was trained on. Typically, this model is trained on the same kind of data. That is, the training data distribution comes from a single domain. For example, if we train a model on the <em class="italic" lang="">Cars </em>dataset, we can expect the images generated by this model to be some version of a car. It cannot generate images from a completely unseen distribution such as faces. While this is still a limitation of what the model can do, it is much less limited as compared to the standard <span class="No-Break" lang="">NeRF model.</span></p>
<p lang="en-GB">The fundamental <a id="_idIndexMarker333"/>concepts implemented in the GIRAFFE model that we have discussed so far are summarized in the <span class="No-Break" lang="">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 7.4: The GIRAFFE model " height="1314" src="image/B18217_07_4.jpg" width="1030"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4: The GIRAFFE model</p>
<p lang="en-GB">The generator <a id="_idIndexMarker334"/>model uses the chosen camera pose and <em class="italic" lang="">N</em>, the number of objects (including the background), and the corresponding number of shape and appearance codes along with affine transformations to first synthesize feature fields. The individual feature fields corresponding to individual objects are then composed together to form an aggregate feature field. It then volume renders the feature field along the ray using the standard principles <a id="_idIndexMarker335"/>of volume rendering. Following this, a neural rendering network transforms this feature field to a pixel value in the <span class="No-Break" lang="">image space.</span></p>
<p lang="en-GB">In this section, we gained a very broad understanding of the GIRAFFE model. Now let us zoom into the individual components of it to get a more <span class="No-Break" lang="">in-depth understanding.</span></p>
<h1 id="_idParaDest-96" lang="en-GB"><a id="_idTextAnchor099"/>Generating feature fields</h1>
<p lang="en-GB">The first step of the scene generation process is generating a feature field. This is analogous to <a id="_idIndexMarker336"/>generating an RGB image in the NeRF model. In the NeRF model, the output of the model is a feature field that happens to be an image made up of RGB values. However, a feature field can be any abstract notion of the image. It is a generalization of an image matrix. The difference here is that instead of generating a three-channel RGB image, the GIRAFFE model generates a more abstract image that we refer to as the feature field with dimensions H<span class="subscript" lang="">V</span>, W<span class="subscript" lang="">V</span>, and M<span class="subscript" lang="">f</span>, where H<span class="subscript" lang="">V</span> is the height of the feature field, W<span class="subscript" lang="">V </span>is its width, and M<span class="subscript" lang="">f</span> is the number of channels in the <span class="No-Break" lang="">feature field.</span></p>
<p lang="en-GB">For this section, let us assume that we have a trained GIRAFFE model. It has been trained on some predefined dataset that we are not going to think about now. To generate a new image, we need to do the following <span class="No-Break" lang="">three things:</span></p>
<ol>
<li lang="en-GB" value="1">Specify the camera pose: This defines the viewing angle of the camera. As a preprocessing step, we use this camera pose to cast a ray into the scene and generate a direction vector (dj) along with sampled points (x<span class="subscript" lang="">ij</span>). We will project many such rays into <span class="No-Break" lang="">the scene.</span></li>
<li lang="en-GB">Sample 2N latent codes: We sample two latent codes corresponding to each object we wish to see in the rendered output image. One latent code corresponds to the shape of the object and the other latent code corresponds to its appearance. These codes are sampled from a standard <span class="No-Break" lang="">normal distribution.</span></li>
<li lang="en-GB">Specify <em class="italic" lang="">N</em> affine transformations: This corresponds to the pose of the object in <span class="No-Break" lang="">the scene.</span></li>
</ol>
<p lang="en-GB">The generator part of the model does <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB">For each expected object in the scene, use the shape code, the appearance code, the object’s pose information (that is, the affine transformation), the viewing <a id="_idIndexMarker337"/>direction vector, and a point in the scene (x<span class="subscript" lang="">ij</span>) to generate a feature field (a vector) and a volume density for that point. This is the NeRF model <span class="No-Break" lang="">in action.</span></li>
<li lang="en-GB">Use the compositional operator to compose these feature fields and densities into a single feature field and density value for that point. Here, the compositional operator does <span class="No-Break" lang="">the following:</span></li>
</ul>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="194" src="image/Formula_07_001.png" width="228"/></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="194" src="image/Formula_07_002.png" width="341"/></p>
<p lang="en-GB">The volume density at a point can be simply summed up. The feature field is averaged by assigning importance proportional to the volume density of the object at that point. One important benefit of such a simple operator is that it is differentiable. Therefore, it can be introduced inside a neural network since the gradients can flow through this operator during the model <span class="No-Break" lang="">training phase.</span></p>
<ul>
<li lang="en-GB">We use volume rendering to render a feature field for each ray generated for the input camera pose by aggregating feature field values along the ray. We do this for multiple rays to create a full feature field of dimension H<span class="subscript" lang="">V</span> x W<span class="subscript" lang="">V</span>. Here, V is generally a small value. So, we are creating a low-resolution <span class="No-Break" lang="">feature field.</span></li>
</ul>
<p class="callout-heading" lang="en-GB">Feature fields</p>
<p class="callout" lang="en-GB">A feature field is <a id="_idIndexMarker338"/>an abstract notion of an image. They are not RGB values and are typically in low spatial dimensions (such as 16 x 16 or 64 x 64) but high channel dimensions. We need an image that is spatially high dimensional (for example, 512 x 512), but in three channels (RGB). Let us look at a way to do that with a <span class="No-Break" lang="">neural network.</span></p>
<h1 id="_idParaDest-97" lang="en-GB"><a id="_idTextAnchor100"/>Mapping feature fields to images</h1>
<p lang="en-GB">After we generate a feature field of dimensions H<span class="subscript" lang="">V</span> x W<span class="subscript" lang="">V</span> x M<span class="subscript" lang="">f</span>, we need to map this to an image of <a id="_idIndexMarker339"/>dimension H x W x 3. Typically, H<span class="subscript" lang="">V</span> &lt; H, W<span class="subscript" lang="">V</span> &lt; W, and M<span class="subscript" lang="">f</span> &gt; 3. The GIRAFFE model uses the two-stage approach since an ablation analysis <a id="_idIndexMarker340"/>showed it to be better than using a single-stage approach to generate the <span class="No-Break" lang="">image directly.</span></p>
<p lang="en-GB">The mapping operation is a parametric function that can be learned with data, and using a 2D CNN is best suited for this task since it is a function in the image domain. You can think of this function as an upsampling neural network like a decoder in an auto-encoder. The output of this neural network is the rendered image that we can see, understand, and evaluate. Mathematically, this can be defined <span class="No-Break" lang="">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="" height="141" src="image/Formula_07_03.jpg" width="1650"/>
</div>
</div>
<p lang="en-GB">This neural network consists of a series of upsampling layers done using <em class="italic" lang="">n</em> blocks of nearest neighbor upsampling, followed by a 3 x 3 convolution and leaky ReLU. This creates a series of <em class="italic" lang="">n</em> different spatial resolutions of the feature field. However, in each spatial resolution, the feature field is mapped to a three-channel image of the same spatial resolution via a 3 x 3 convolution. At the same time, images from the previous spatial resolution are upsampled using a non-parametric bilinear upsampling operator and added to the image of the new spatial resolution. This is repeated until we reach the desired spatial resolution of H <span class="No-Break" lang="">x W.</span></p>
<p lang="en-GB">The skip connections from the feature field to a similar dimensional image help with a strong gradient flow to the feature fields in each spatial resolution. Intuitively, this ensures that the neural rendering model has a strong understanding of the image in each spatial resolution. Additionally, the skip connections ensure that the final image that is generated is a combination of the image understanding at <span class="No-Break" lang="">various resolutions.</span></p>
<p lang="en-GB">This concept becomes very clear with the following diagram of the neural <span class="No-Break" lang="">rendering model:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 7.5: Neural rendering model; this is a 2D CNN with a series of nearest neighbor upsampling operators with a parallel mapping to the RGB image domain " height="702" src="image/B18217_07_5.jpg" width="922"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5: Neural rendering model; this is a 2D CNN with a series of nearest neighbor upsampling operators with a parallel mapping to the RGB image domain</p>
<p lang="en-GB">The neural <a id="_idIndexMarker341"/>rendering model takes the feature field output <a id="_idIndexMarker342"/>from the previous stage and generates a high-resolution RGB image. Since the feature field is generated using a NeRF-based generator, it should understand the 3D nature of the scene, the objects in them, and their position, pose, shape, and appearance. And since we use a compositional operator, the feature field also encodes the number of objects in <span class="No-Break" lang="">the scene.</span></p>
<p lang="en-GB">In the next section, you will discover how we can control the scene generation process and the control mechanisms we have to <span class="No-Break" lang="">achieve it.</span></p>
<h1 id="_idParaDest-98" lang="en-GB"><a id="_idTextAnchor101"/>Exploring controllable scene generation</h1>
<p lang="en-GB">To truly appreciate and learn what a computer vision model generates, we need to visualize the <a id="_idIndexMarker343"/>outputs of the trained model. Since we are dealing with a generative approach, it is easy to do this by simply visualizing the images generated by the model. In this section, we will explore pre-trained GIRAFFE models and look at how well they can generate controllable scenes. We will use pre-trained checkpoints provided by the creators of the GIRAFFE model. The instructions provided in this section are based on the open source GitHub repository <span class="No-Break" lang="">at </span><a href="https://github.com/autonomousvision/giraffe"><span class="No-Break" lang="">https://github.com/autonomousvision/giraffe</span></a><span class="No-Break" lang="">.</span></p>
<p lang="en-GB">Create the Anaconda environment called <strong class="source-inline" lang="">giraffe</strong> with the <span class="No-Break" lang="">following commands:</span></p>
<p class="source-code" lang="en-GB">$ cd chap7/giraffe</p>
<p class="source-code" lang="en-GB">$ conda env create -f environment.yml</p>
<p class="source-code" lang="en-GB">$ conda activate giraffe</p>
<p lang="en-GB">Once the <strong class="source-inline" lang="">conda</strong> environment has been activated, you can start rendering images for various datasets using their corresponding pre-trained checkpoints. The creators of the GIRAFFE model <a id="_idIndexMarker344"/>have shared pre-trained models from five <span class="No-Break" lang="">different datasets:</span></p>
<ul>
<li lang="en-GB"><strong class="bold" lang="">Cars dataset</strong>: This consists of 136,726 images of 196 classes <span class="No-Break" lang="">of cars.</span></li>
<li lang="en-GB"><strong class="bold" lang="">CelebA-HQ dataset</strong>: This consists of 30,000 high-resolution face images selected from the original <span class="No-Break" lang=""><em class="italic" lang="">CelebA</em></span><span class="No-Break" lang=""> dataset.</span></li>
<li lang="en-GB"><strong class="bold" lang="">LSUN Church dataset</strong>: This consists of about 126,227 images <span class="No-Break" lang="">of churches.</span></li>
<li lang="en-GB"><strong class="bold" lang="">CLEVR dataset</strong>: This is a dataset primarily used for visual question-answering research. It consists of 54,336 images of objects of different sizes, shapes, <span class="No-Break" lang="">and positions.</span></li>
<li lang="en-GB"><strong class="bold" lang="">Flickr-Faces-HQ dataset</strong>: This consists of 70,000 high-quality images of faces obtained <span class="No-Break" lang="">from Flickr.</span></li>
</ul>
<p lang="en-GB">We will explore the model outputs on two different datasets just to get an understanding <span class="No-Break" lang="">of them.</span></p>
<h2 id="_idParaDest-99" lang="en-GB"><a id="_idTextAnchor102"/>Exploring controllable car generation</h2>
<p lang="en-GB">In this subsection, we are going to explore a model trained on the <em class="italic" lang="">Cars </em>dataset. The appearance <a id="_idIndexMarker345"/>and shape code provided to the model will generate cars since that is what the model is trained on. You can run the following command to generate <span class="No-Break" lang="">image samples:</span></p>
<p class="source-code" lang="en-GB">$ python render.py configs/256res/cars_256_pretrained.yaml</p>
<p lang="en-GB">Here, the <strong class="source-inline" lang="">config</strong> file specifies the path to the output folder where the generated images are stored. The <strong class="source-inline" lang="">render.py</strong> script will automatically download the GIRAFFE model checkpoints and use them to render images. The output images are stored in <strong class="source-inline" lang="">out/cars256_pretrained/rendering</strong>. This folder will have the <span class="No-Break" lang="">following subfolders:</span></p>
<p class="source-code" lang="en-GB">- out</p>
<p class="source-code" lang="en-GB">  - cars256_pretrained</p>
<p class="source-code" lang="en-GB">    - rendering</p>
<p class="source-code" lang="en-GB">      - interpolate_app</p>
<p class="source-code" lang="en-GB">      - interpolate_shape</p>
<p class="source-code" lang="en-GB">      - translation_object_depth</p>
<p class="source-code" lang="en-GB">      - interpolate_bg_app</p>
<p class="source-code" lang="en-GB">      - rotation_object</p>
<p class="source-code" lang="en-GB">      - translation_object_horizontal</p>
<p lang="en-GB">Each of these folders contains images obtained when we change specific inputs of the GIRAFFE model. For example, take a look at <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB"><strong class="source-inline" lang="">interpolate_app</strong>: This is a set of images to demonstrate what happens when we slowly vary the object <span class="No-Break" lang="">appearance code.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">interpolate_bg_app</strong>: This demonstrates what happens when we vary the background <span class="No-Break" lang="">appearance code.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">interpolate_shape</strong>: This demonstrates what happens when we vary the shape code of <span class="No-Break" lang="">the object.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">translation_object_depth</strong>: This demonstrates what happens when we change the object depth. This is part of the affine transformation matrix code that is part of <span class="No-Break" lang="">the input.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">translation_object_horizontal</strong>: This demonstrates what happens when we want to move the object sideways in the image. This is part of the affine transformation matrix code that is part of <span class="No-Break" lang="">the input.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">rotation_object</strong>: This demonstrates what happens when we want to change the object pose. This is part of the affine transformation matrix code that is part of <span class="No-Break" lang="">the input.</span></li>
</ul>
<p lang="en-GB">Let us look <a id="_idIndexMarker346"/>at the images inside the <strong class="source-inline" lang="">rotation_object</strong> folder and <span class="No-Break" lang="">analyze them:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 7.6: The Rotation object model images " height="260" src="image/B18217_07_6a.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 7.6: The Rotation object model images " height="260" src="image/B18217_07_6b.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 7.6: The Rotation object model images " height="260" src="image/B18217_07_6c.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 7.6: The Rotation object model images " height="260" src="image/B18217_07_6d.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6: The Rotation object model images</p>
<p lang="en-GB">Images in each row were obtained by, first, choosing an appearance and shape code and varying the affine transformation matrix to just rotate the object. The horizontal and depth translation parts of the affine transformation code were kept fixed. The background <a id="_idIndexMarker347"/>object code, appearance, and shape code of the object were also kept fixed. Different rows were obtained by using different appearance and shape code. Here are <span class="No-Break" lang="">some observations:</span></p>
<ul>
<li lang="en-GB">The background for all the images does not change across images for the same object. This suggests that we have successfully disentangled the background for the remaining parts of <span class="No-Break" lang="">the image.</span></li>
<li lang="en-GB">Color, reflection, and shadows: As the object is rotated, the image color and reflection are fairly consistent as expected of a physical object rotation. This is typical because of the usage of NeRF-like <span class="No-Break" lang="">model architecture.</span></li>
<li lang="en-GB">Left-right consistency: The left and right views of a car <span class="No-Break" lang="">are consistent.</span></li>
<li lang="en-GB">There are some unnatural artifacts such as blurry object edges and smudged backgrounds. High-frequency variations in the image are not very well captured by the <span class="No-Break" lang="">GIRAFFE model.</span></li>
</ul>
<p lang="en-GB">You can now explore other folders to understand the model’s consistency and quality of the generated image when the object is translated or when the background <span class="No-Break" lang="">is varied.</span></p>
<h2 id="_idParaDest-100" lang="en-GB"><a id="_idTextAnchor103"/>Exploring controllable face generation</h2>
<p lang="en-GB">In this subsection, we are going to explore a model trained on the <em class="italic" lang="">CelebA-HQ</em> dataset. The appearance <a id="_idIndexMarker348"/>and shape codes provided to the model will generate faces since that is what the model is trained on. You can run the following command to generate <span class="No-Break" lang="">image samples:</span></p>
<p class="source-code" lang="en-GB">$ python render.py configs/256res/celebahq_256_pretrained.yaml</p>
<p lang="en-GB">The <strong class="source-inline" lang="">config</strong> file specifies the path to the output folder where the generated images are stored. The output images are stored in <strong class="source-inline" lang="">out/celebahq_256_pretrained /rendering</strong>. This folder will have the <span class="No-Break" lang="">following subfolders:</span></p>
<p class="source-code" lang="en-GB">- out</p>
<p class="source-code" lang="en-GB">  - celebahq_256_pretrained</p>
<p class="source-code" lang="en-GB">    - rendering</p>
<p class="source-code" lang="en-GB">      - interpolate_app</p>
<p class="source-code" lang="en-GB">      - interpolate_shape</p>
<p class="source-code" lang="en-GB">      - rotation_object</p>
<p lang="en-GB">Let us <a id="_idIndexMarker349"/>look at images inside the <strong class="source-inline" lang="">interpolate_app</strong> folder and <span class="No-Break" lang="">analyze them:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 7.7: The interpolate app images " height="260" src="image/B18217_07_7a.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 7.7: The interpolate app images " height="260" src="image/B18217_07_7b.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 7.7: The interpolate app images " height="260" src="image/B18217_07_7c.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 7.7: The interpolate app images " height="264" src="image/B18217_07_7d.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7: The interpolate app images</p>
<p lang="en-GB">Images in each row were obtained by, first, choosing a shape code and varying the appearance code to just change the appearance of the face. The affine transformation matrix code <a id="_idIndexMarker350"/>was kept fixed too. Different rows were obtained by using different shape code. Here are <span class="No-Break" lang="">some observations:</span></p>
<ul>
<li lang="en-GB">The shape of the generated face is largely fixed across a single row of faces. This suggests that the shape code is robust to changes in <span class="No-Break" lang="">appearance code.</span></li>
<li lang="en-GB">The appearance of the face (features such as skin tone, skin shine, hair color, eyebrow color, eye color, lip expression, and nose shape) changes as the appearance code is changed. This suggests that the appearance code encodes facial <span class="No-Break" lang="">appearance features.</span></li>
<li lang="en-GB">The shape code encodes the perceived gender of the face. This largely makes sense since there is a large perceived variation between the facial shape of male and female images in the <span class="No-Break" lang="">training dataset.</span></li>
</ul>
<p lang="en-GB">Let us look at images inside the <strong class="source-inline" lang="">interpolate_shape</strong> folder and <span class="No-Break" lang="">analyze them:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 7.8: The interpolate shape images " height="260" src="image/B18217_07_8a.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 7.8: The interpolate shape images " height="260" src="image/B18217_07_8b.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 7.8: The interpolate shape images " height="260" src="image/B18217_07_8c.jpg" width="1550"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 7.8: The interpolate shape images " height="260" src="image/B18217_08_8d.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8: The interpolate shape images</p>
<p lang="en-GB">Images in each row were obtained by, first, choosing an appearance code and varying the shape <a id="_idIndexMarker351"/>code to just change the shape of the face. The affine transformation matrix code was kept fixed too. Different rows were obtained by using different appearance code. Here are <span class="No-Break" lang="">some observations:</span></p>
<ul>
<li lang="en-GB">The appearance of the face (features such as skin tone, skin shine, hair color, eyebrow color, eye color, lip expression, and nose shape) is largely the same as the shape code is changed. This suggests that the appearance code is robust to changes in facial <span class="No-Break" lang="">shape features.</span></li>
<li lang="en-GB">The shape of the generated face changes as the shape code is varied. This suggests that the shape code is correctly encoding the shape features of <span class="No-Break" lang="">the face.</span></li>
<li lang="en-GB">The shape code encodes the perceived gender of the face. This largely makes sense since there is a large perceived variation between the facial shape of male and female images in the <span class="No-Break" lang="">training dataset.</span></li>
</ul>
<p lang="en-GB">In this section, we explored controllable 3D scene generation using the GIRAFFE model. We generated cars using a model trained on the <em class="italic" lang="">Cars </em>dataset. Additionally, we generated faces using a model trained on the <em class="italic" lang="">CelebA-HQ</em> dataset. In each of these cases, we saw that the input parameters of the model are very well disentangled. We used a pre-trained model provided by the creators of the GIRAFFE model. In the next section, we will learn more about how to train such a model on a <span class="No-Break" lang="">new dataset.</span></p>
<h1 id="_idParaDest-101" lang="en-GB"><a id="_idTextAnchor104"/>Training the GIRAFFE model</h1>
<p lang="en-GB">So far in <a id="_idIndexMarker352"/>this chapter, we have understood how a trained GIRAFFE model works. We have understood the different components that make up the generator part of <span class="No-Break" lang="">the model.</span></p>
<p lang="en-GB">But to train the model, there is another part that we have not looked at so far, namely, the discriminator. Like in any other GAN model, this discriminator part of the model is not used during image synthesis, but it is a vital component for training the model. In this chapter, we will investigate it in more detail and gain an understanding of the loss function used. We will train a new model from scratch using the training module provided by the authors <span class="No-Break" lang="">of GIRAFFE.</span></p>
<p lang="en-GB">The generator takes as input the various latent code corresponding to object rotation, background rotation, camera elevation, horizontal and depth translation, and object size. This is used to first generate a feature field and then map it to RGB pixels using a neural rendering module. This is the generator. The discriminator is fed with two images: one is <a id="_idIndexMarker353"/>the real image from the training dataset and the other is the image generated by the generator. The goal of the discriminator is to classify the real image as real and the generated image as fake. This is the <span class="No-Break" lang="">GAN objective.</span></p>
<p class="callout-heading" lang="en-GB">Important note</p>
<p class="callout" lang="en-GB">The training dataset is unlabeled. There is no annotation for the object pose parameters, depth, or position in the image. However, for each dataset, we roughly know the parameters such as object rotation rate, background rotation range, camera elevation range, horizontal translation, depth translation range, and the object scale range. During training, the inputs are randomly sampled from the range of values assuming a uniform distribution within <span class="No-Break" lang="">the range.</span></p>
<p lang="en-GB">The discriminator is a 2D CNN that takes as input an image and outputs confidence scores for real and <span class="No-Break" lang="">fake images.</span></p>
<h2 id="_idParaDest-102" lang="en-GB"><a id="_idTextAnchor105"/>Frechet Inception Distance</h2>
<p lang="en-GB">In order <a id="_idIndexMarker354"/>to evaluate the quality of generated <a id="_idIndexMarker355"/>images, we use the <strong class="bold" lang="">Frechet Inception Distance</strong> (<strong class="bold" lang="">FID</strong>). This is a measure of the distance between features extracted from real and generated images. This is not a metric on a single image. Rather, it is a statistic on the entire population of the images. Here is how we calculate the <span class="No-Break" lang="">FID score:</span></p>
<ol>
<li lang="en-GB" value="1">First, we make use of the InceptionV3 model (a popular deep learning backbone used in many real-world applications) to extract a feature vector from the image. Typically, this is the last layer of the model before the classification layer. This feature vector summarizes the image in a <span class="No-Break" lang="">low-dimensional space.</span></li>
<li lang="en-GB">We extract feature vectors for the entire collection of real and <span class="No-Break" lang="">generated images.</span></li>
<li lang="en-GB">We calculate <a id="_idIndexMarker356"/>the mean and the covariance <a id="_idIndexMarker357"/>of these feature vectors separately for the collection of real and <span class="No-Break" lang="">generated images.</span></li>
<li lang="en-GB">The mean and covariance statistics are used in a distance formula to derive a <span class="No-Break" lang="">distance metric.</span></li>
</ol>
<h2 id="_idParaDest-103" lang="en-GB"><a id="_idTextAnchor106"/>Training the model</h2>
<p lang="en-GB">Let us <a id="_idIndexMarker358"/>look at how can initiate model training on the <span class="No-Break" lang=""><em class="italic" lang="">Cars </em></span><span class="No-Break" lang="">dataset:</span></p>
<p class="source-code" lang="en-GB">python train.py .yaml configs/256res/celebahq_256.yaml</p>
<p lang="en-GB">The training parameters can be understood by looking at the configuration <span class="No-Break" lang="">file, </span><span class="No-Break" lang=""><strong class="source-inline" lang="">configs/256res/celebahq_256.yaml</strong></span><span class="No-Break" lang="">:</span></p>
<ul>
<li lang="en-GB"><strong class="bold" lang="">Data</strong>: This section <a id="_idIndexMarker359"/>of the config file specifies the path to the training dataset <span class="No-Break" lang="">to use:</span><p class="source-code" lang="en-GB">data:</p><p class="source-code" lang="en-GB">  path: data/comprehensive_cars/images/*.jpg</p><p class="source-code" lang="en-GB">  fid_file: data/comprehensive_cars/fid_files/comprehensiveCars_256.npz</p><p class="source-code" lang="en-GB">  random_crop: True</p><p class="source-code" lang="en-GB">  img_size: 256</p></li>
<li lang="en-GB"><strong class="bold" lang="">Model</strong>: This <a id="_idIndexMarker360"/>specifies the <span class="No-Break" lang="">modeling parameters:</span><p class="source-code" lang="en-GB">model:</p><p class="source-code" lang="en-GB">  background_generator_kwargs:</p><p class="source-code" lang="en-GB">    rgb_out_dim: 256</p><p class="source-code" lang="en-GB">  bounding_box_generator_kwargs:</p><p class="source-code" lang="en-GB">    scale_range_min: [0.2, 0.16, 0.16]</p><p class="source-code" lang="en-GB">    scale_range_max: [0.25, 0.2, 0.2]</p><p class="source-code" lang="en-GB">    translation_range_min: [-0.22, -0.12, 0.]</p><p class="source-code" lang="en-GB">    translation_range_max: [0.22, 0.12, 0.]</p><p class="source-code" lang="en-GB">  generator_kwargs:</p><p class="source-code" lang="en-GB">    range_v: [0.41667, 0.5]</p><p class="source-code" lang="en-GB">    fov: 10</p><p class="source-code" lang="en-GB">  neural_renderer_kwargs:</p><p class="source-code" lang="en-GB">    input_dim: 256</p><p class="source-code" lang="en-GB">    n_feat: 256</p><p class="source-code" lang="en-GB">  decoder_kwargs:</p><p class="source-code" lang="en-GB">    rgb_out_dim: 256</p></li>
<li lang="en-GB"><strong class="bold" lang="">Training</strong>: This <a id="_idIndexMarker361"/>specifies the training parameters such as <span class="No-Break" lang="">output </span><span class="No-Break" lang=""><strong class="source-inline" lang="">d</strong></span></li>
<li lang="en-GB">Directory path and learning rate, among <span class="No-Break" lang="">other things:</span><p class="source-code" lang="en-GB">training:</p><p class="source-code" lang="en-GB">  out_dir:  out/cars256</p><p class="source-code" lang="en-GB">  learning_rate: 0.00025</p></li>
</ul>
<p class="callout-heading" lang="en-GB">Important note</p>
<p class="callout" lang="en-GB">Training the model is a computationally intensive task. It would most likely take anywhere between 1 and 4 days to fully train the model on a single GPU, depending on the GPU <span class="No-Break" lang="">device used.</span></p>
<h1 id="_idParaDest-104" lang="en-GB"><a id="_idTextAnchor107"/>Summary</h1>
<p lang="en-GB">In this chapter, you explored controllable 3D-aware image synthesis using the GIRAFFE model. This model borrows concepts from NeRF, GANs, and 2D CNNs to create 3D scenes that are controllable. First, we had a refresher on GANs. Then, we dove deeper into the GIRAFFE model, how feature fields are generated, and how those feature fields are then transformed into RGB images. We then explored the outputs of this model and understood its properties and limitations. Finally, we briefly touched on how to train <span class="No-Break" lang="">this model.</span></p>
<p lang="en-GB">In the next chapter, we are going to explore a relatively new technique used to generate realistic human bodies in three dimensions called the SMPL model. Notably, the SMPL model is one of the small numbers of models that do not use deep neural networks. Instead, it uses more classical statistical techniques such as principal component analysis to achieve its objectives. You will learn the importance of good mathematical problem formulation in building models that use <span class="No-Break" lang="">classical techniques.</span></p>
</div>
</div></body></html>