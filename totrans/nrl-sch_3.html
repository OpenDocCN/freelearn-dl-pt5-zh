<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer042">
<h1 class="chapter-number" id="_idParaDest-40"><a id="_idTextAnchor044"/>3</h1>
<h1 id="_idParaDest-41"><a id="_idTextAnchor045"/>System Design and Engineering Challenges</h1>
<p>Understanding <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) and deep learning concepts is essential, but if you’re looking to build an effective search solution powered by <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) and deep learning, you need production engineering capabilities as well. Effectively deploying ML models requires competencies more commonly found in technical fields such as software engineering and DevOps. These competencies are called <strong class="bold">MLOps</strong>. This is particularly the case for a search system that requires high useability and low latency.</p>
<p>In this chapter, you will learn the basics of designing a search system. You will understand core concepts such as <strong class="bold">indexing</strong> and <strong class="bold">querying</strong> and how to use them to save and retrieve information. </p>
<p>In this chapter, we’re going to cover the following main topics in particular:</p>
<ul>
<li>Indexing and querying</li>
<li>Evaluating a neural search system</li>
<li>Engineering challenges in building a neural search system</li>
</ul>
<p>By the end of the chapter, you will have a full understanding of the capabilities and possible difficulties to overcome when putting a neural search into production. You will be able to assess when it is useful to use neural search and which approach would be the best for your own search system.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor046"/>Technical requirements</h1>
<p>This chapter has the following technical requirements:</p>
<ul>
<li>A laptop with a minimum of 4 GB of RAM; 8 GB is suggested.</li>
<li>Python installed, with version 3.7, 3.8, or 3.9 on a Unix-like operating system, such as macOS or Ubuntu.</li>
</ul>
<p>The code files for the chapter are available at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina</a>.<a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina%0D"/></p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor047"/>Introducing indexing and querying</h1>
<p>In this section, you will go through two important high-level tasks to build a search system:</p>
<ul>
<li><strong class="bold">Indexing</strong>: This is <a id="_idIndexMarker186"/>the process of collecting, parsing, and storing data to facilitate fast and accurate information retrieval. This includes adding, updating, deleting, and reading documents to be indexed.</li>
<li><strong class="bold">Querying</strong>: Querying<a id="_idIndexMarker187"/> is the process of parsing, matching, and ranking the user query and sending relevant information back to the user.</li>
</ul>
<p>In a neural search system, both indexing and querying are composed of a sequence of tasks. Let’s take a deep look at indexing and querying components.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor048"/>Indexing</h2>
<p>Indexing is an <a id="_idIndexMarker188"/>important process in search systems. It forms the core functionality since it assists in retrieving information efficiently. Indexing reduces the documents to the useful information contained in them. It maps the terms to the respective documents containing the information. The process of finding a relevant document in a search system is essentially identical to the process of looking at a dictionary, where the index helps you find words effectively.</p>
<p>Before introducing the details, we start by asking the following questions to understand where we stand:</p>
<ul>
<li>What are the major components of an indexing pipeline? </li>
<li>What content can be indexed? </li>
<li>How do we index incrementally and how do we index at speed? </li>
</ul>
<p>If you don’t know the answers to these questions, don’t worry. Just keep reading!</p>
<p>In an indexing pipeline, we normally have three major components:</p>
<ul>
<li><strong class="bold">Preprocessor</strong>: A preprocessor<a id="_idIndexMarker189"/> takes the raw documents into the <a id="_idIndexMarker190"/>system and performs some preprocessing tasks. For instance, if we need to index text of modality <strong class="source-inline">text/plain</strong>, we might need a tokenizer and a stemmer, as was introduced in <a href="B17488_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Neural Networks for Neural Search</em>. If we want to index an image of modality <strong class="source-inline">image/jpeg</strong>, we might want a component to resize or transform the input image into the expected format of the neural networks. It highly depends on your task and input data.</li>
<li><strong class="bold">Encoder</strong>: An encoder, in a <a id="_idIndexMarker191"/>neural search system, is identical to neural<a id="_idIndexMarker192"/> networks. This neural network takes your preprocessed input as a vector representation (embeddings). After this step, each raw document composed of text, images, videos, or even DNA information should be represented as a vector of numerical values.</li>
<li><strong class="bold">Indexer (for storage)</strong>: An<a id="_idIndexMarker193"/> indexer, better known as <strong class="bold">StorageIndexer</strong>, at the <a id="_idIndexMarker194"/>indexing stage stores the vectors produced from the<a id="_idIndexMarker195"/> encoder into storage, such as memory or a database. This includes relational databases (PostgresSQL), NoSQL (MongoDB), or even better, a vector database, such as Elasticsearch.</li>
</ul>
<p>It should be noted that<a id="_idIndexMarker196"/> every indexing task is independent. It could vary from different perspectives. For instance, if you are working on a multi-model search engine in an e-commerce context, your objective is to create a search system that can take both text and an image as a query and find the most relevant products. In this case, your indexing might have two pathways:</p>
<ul>
<li>Textual information should be preprocessed and encoded using text-based preprocessors and encoders.</li>
<li>Likewise, image data should be preprocessed and encoded using image-based preprocessors and encoders.</li>
</ul>
<p>You might wonder what can be indexed. Anything, as long as you have an encoder and your data can be encoded. Some common data types you can index include text, image, video, and audio. As we discussed in previous chapters, you can encode source code to build a source code search system or gene information to build a search system around that. The following figure illustrates an <a id="_idIndexMarker197"/>indexing pipeline:</p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 3.1 – A simple indexing pipeline takes documents as input and applies preprocessing and encoding. In the end, save the encoded features into storage " height="196" src="image/Figure_3.1_B17488.jpg" width="1545"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A simple indexing pipeline takes documents as input and applies preprocessing and encoding. In the end, save the encoded features into storage</p>
<p>Now, we move on to an important topic: incremental indexing. Firstly, let’s discuss what incremental indexing is.</p>
<h3>Understanding incremental indexing</h3>
<p><strong class="bold">Incremental indexing</strong> is a <a id="_idIndexMarker198"/>crucial feature for any search system. Given the fact that the data collection we want to index is likely to change dramatically every day, we cannot afford to index the entire data collection each time there is a small change. </p>
<p>In general, there are two common practices to perform an indexing task, as follows:</p>
<ul>
<li><strong class="bold">Real-time indexing</strong>: Given <a id="_idIndexMarker199"/>any data being sent to the collection, the<a id="_idIndexMarker200"/> indexer immediately adds the document to the index.</li>
<li><strong class="bold">Scheduled indexing</strong>: Given<a id="_idIndexMarker201"/> any data being sent to the <a id="_idIndexMarker202"/>collection, the scheduler triggers the indexing task and performs the indexing job.</li>
</ul>
<p>The preceding practices have their advantages and disadvantages. In real-time indexing, the user gets newly added documents immediately (if it is a match), while also consuming more system resources and potentially introducing data inconsistency. However, in the case of scheduled indexing, users don’t get access to newly added results in real time but it is less error prone and easier to manage.</p>
<p>The indexing strategy you choose depends on your task. If the task is time sensitive, it is better to use <a id="_idIndexMarker203"/>real-time indexing. Otherwise, it is good to set up a cron job and index your data incrementally at a certain time. </p>
<h3>Speeding up indexing</h3>
<p>Another crucial <a id="_idIndexMarker204"/>issue when performing an indexing task in neural search is the speed of indexing. While a symbolic search system works only on textual data, the input of a neural search system could be three-dimensional (<em class="italic">Height * Width * ColorChannel</em>), such as an RGB image, or four-dimensional (<em class="italic">Frame * Height * Width * ColorChannel</em>), such as a video. This kind of data can be indexed with different modalities, which can dramatically slow down the data preprocessing and encoding process. </p>
<p>In general, there are several strategies that we can use to boost the indexing speed. Some of these are as follows:</p>
<ul>
<li><strong class="bold">Preprocessor</strong>: Applying certain preprocessing on certain datasets could greatly boost your indexing speed. For instance, if you want to index high-resolution images, it’s better to resize them and make them smaller.</li>
<li><strong class="bold">GPU inference</strong>: In a neural search system, encoding takes most of the indexing time. To be more specific, given a preprocessed document, it takes time to employ the deep neural networks to encode the document into a vector. It could be greatly improved by making use of a GPU instance for encoding. Since the GPU has much higher bandwidth memory and L1 cache, the GPU is suitable for ML tasks.</li>
<li><strong class="bold">Horizontal scaling</strong>: Indexing a huge amount of data on a single machine makes the process slow, but it could be much faster if we distribute data across multiple machines and perform indexing in parallel. For example, the following figure demonstrates assigning more encoders to the pipeline:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference in parallel " height="496" src="image/Figure_3.2_B17488.jpg" width="1536"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference in parallel</p>
<p>It is worth mentioning that if you come from a text retrieval background, <strong class="bold">index compression</strong> also <a id="_idIndexMarker205"/>matters when constructing an inverted index for symbolic search. This is not exactly the same in a neural search system anymore:</p>
<ul>
<li>First of all, the encoder takes a document as input and encodes the content of the document into an N-dimensional vector (embeddings). Thus, we can think of the encoder itself as a compression function.</li>
<li>Second, compression of the dense vectors will eventually sacrifice the quality of the vectors. Normally, larger-dimensionality vectors bring better search results since they can better represent the documents being encoded.</li>
</ul>
<p>In practice, we need to find a balance point between dimensionality and memory usage in order to load all vectors into memory to perform a large-scale similarity search. In the next section, we will dive into the querying part, which will allow you to understand how to conduct a large-scale similarity search.</p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor049"/>Querying</h2>
<p>When it comes to the querying<a id="_idIndexMarker206"/> pipeline, it has a lot of components overlapping with the indexing pipeline, but with a few modifications and additional components, such as a ranker. At this stage, the input of the pipeline is a single user query. There are four major components of a typical querying task:</p>
<ul>
<li><strong class="bold">Preprocessor</strong>: This component is similar to the preprocessor<a id="_idIndexMarker207"/> in the indexing pipeline. It <a id="_idIndexMarker208"/>takes the query document as input and applies the same preprocessors as the indexing pipeline to the input.</li>
<li><strong class="bold">Encoder</strong>: The encoder<a id="_idIndexMarker209"/> takes the preprocessed query document as input and <a id="_idIndexMarker210"/>produces vectors as output. It should be noted that in a cross-modal search system, your encoder from indexing might be different from the encoder at the querying step. This will be explained in <a href="B17488_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Advanced Use Cases of Jina</em>.</li>
<li><strong class="bold">Indexer</strong>: This indexer, better <a id="_idIndexMarker211"/>named <strong class="bold">SearchIndexer</strong>, takes<a id="_idIndexMarker212"/> vectors produced <a id="_idIndexMarker213"/>from the encoder as input and conducts a large-scale similarity search over all indexed documents. This is called <strong class="bold">Approximate Nearest Neighbor</strong> (<strong class="bold">ANN</strong>) search. We’ll <a id="_idIndexMarker214"/>elaborate more on this concept in the following section.</li>
<li><strong class="bold">Ranker</strong>: The ranker<a id="_idIndexMarker215"/> takes the query vector and similarity scores against each of the item within the <a id="_idIndexMarker216"/>collection, produces a ranked list in descending order, and returns results to the user.</li>
</ul>
<p>One major difference between indexing<a id="_idIndexMarker217"/> and querying is that indexing (in most cases) is an offline<a id="_idIndexMarker218"/> task while querying is an online task. To be more concrete, when we bootstrap a neural search system and create a query, the system will return an empty list since nothing has been indexed at the moment. Before we <em class="italic">expose</em> the search system to the user, we should have pre-indexed all the documents within the data collection. This indexing is performed offline.</p>
<p>On the other hand, in a querying task, the user sends one query to the system and expects to get matches immediately. All the preprocessing, encoding, index searching, and ranking should be finished during the waiting time. Thus, it is an online task.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Real-time indexing can be considered an online task.</p>
<p>Unlike indexing, while querying, each <a id="_idIndexMarker219"/>user sends a single document as a query to the <a id="_idIndexMarker220"/>system. The preprocessing and encoding take a very short time. On the other hand, finding similar items in the index storage becomes a critical engineering challenge that impacts the neural search system performance. Why is that?</p>
<p>For instance, you have pre-indexed a billion documents, and at querying time, the user sends one query to the system, the document is then preprocessed and encoded into a vector (embedding). Given the query vector, now you need to find the top N similar vectors out of 1 million vectors. How do you achieve that? Conducting similarity searches by computing distances between vectors one by one could take ages. Rather than that, we perform an ANN search.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">When we’re talking about ANN search, we’re considering a million-/billion-scale search. If you want to build a toy example and search across hundreds or thousands of documents, a normal linear scan is fast enough. In a production environment, please follow the selection strategy as will be introduced in the next section.</p>
<h3>ANN search</h3>
<p>ANN search, as <a id="_idIndexMarker221"/>defined by its name, is a trade-off between different factors: accuracy, runtime, and memory consumption. Compared with brute-force search, it ensures the running time will be accepted by the user while sacrificing precision/recall to a certain degree. How fast can it achieve? Given a billion 100-dimensional vectors, it can be fitted into a server with 32 GB memory with a 10 ms response rate. Before diving into details about ANN search, let’s first take a look at the following figure:</p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor Search, Yusuke Matsui) " height="576" src="image/Figure_3.3_B17488.jpg" width="1024"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor Search, Yusuke Matsui)</p>
<p>The preceding <a id="_idIndexMarker222"/>figure illustrates <em class="italic">how to select the ANN library given your search system</em>. In the figure, <em class="italic">N</em> represents the number of documents inside your <em class="italic">StorageIndexer</em>. Different numbers of N can be optimized using different ANN search libraries, such as <strong class="source-inline">FAISS</strong> (<a href="https://github.com/facebookresearch/faiss">https://github.com/facebookresearch/faiss</a>) or <strong class="source-inline">NMSLIB</strong> (<a href="https://github.com/nmslib/nmslib">https://github.com/nmslib/nmslib</a>). Meanwhile, as you’re most likely to be a Python user, the <strong class="source-inline">Annoy</strong> library (<a href="https://github.com/spotify/annoy">https://github.com/spotify/annoy</a>) has provided a user-friendly interface with reasonable performance, and it works well enough for a million-scale vector search.</p>
<p>The <a id="_idIndexMarker223"/>aforementioned libraries were implemented based on different algorithms, the most<a id="_idIndexMarker224"/> popular ones <a id="_idIndexMarker225"/>being <strong class="bold">KD-Trees</strong>, <strong class="bold">Locally Sensitive Hashing</strong> (<strong class="bold">LSH</strong>), and <strong class="bold">Product Quantization</strong> (<strong class="bold">PQ</strong>).</p>
<p>KD-Trees<a id="_idIndexMarker226"/> follows an iterative process to construct a tree. To make the visualization easier, we suppose the data only consists of two features, <em class="italic">f1</em> (<em class="italic">x</em> axis) and <em class="italic">f2</em> (<em class="italic">y</em> axis), which looks like this:</p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 3.4 – KD-Trees, sample dataset to index " height="640" src="image/Figure_3.4_B17488.jpg" width="700"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – KD-Trees, sample dataset to index</p>
<p>Construction of a<a id="_idIndexMarker227"/> KD-Tree starts with selecting a practical feature and setting a threshold for this feature. To illustrate the idea, we begin with a manual selection of f1 and a feature threshold of 0.5. To this end, we get a boundary like this: </p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 3.5 – KD-Trees construction iteration 1 " height="437" src="image/Figure_3.5_B17488.jpg" width="1064"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – KD-Trees construction iteration 1</p>
<p>As you can see from <em class="italic">Figure 3.5</em>, the feature space has been split into two parts by our first selection of f1 with a threshold of 0.5. How is it reflected for the tree? When building the index, we’re essentially creating a binary search tree. The first selection of f1 with a threshold of 0.5 became our root node. Given each data point, if the f1 is greater than 0.5, it will be placed to the right of the node. Otherwise, as shown in <em class="italic">Figure 3.6</em>, we put it to the left of the node:</p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 3.6 – KD-Trees construction iteration 2 " height="429" src="image/Figure_3.6_B17488.jpg" width="1066"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – KD-Trees construction iteration 2</p>
<p>We continue <a id="_idIndexMarker228"/>from the preceding tree. In the second iteration, let’s define our rule as: given f1 &gt; 0.5, select f2 with threshold 0.5. As was shown in the preceding graph, now we split the feature space again based on the new rule, and it is also reflected on our tree: we created a new node, <strong class="bold">f2-0.5</strong>, in the figure (the <strong class="bold">none</strong> node is only for visualization purpose; we haven’t created this node). This is shown in the following figure: </p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 3.7 – KD-Tree construction iteration N (final iteration) " height="426" src="image/Figure_3.7_B17488.jpg" width="1064"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – KD-Tree construction iteration N (final iteration)</p>
<p>As shown in <em class="italic">Figure 3.7</em>, the <a id="_idIndexMarker229"/>entire feature space has been split into six bins. Compared with before, we added three new nodes, including two leaf nodes:</p>
<ul>
<li>The previous <strong class="bold">none</strong> was replaced by an actual node, <strong class="bold">f2-0.65</strong>; this node split the space of f2 based on the threshold 0.65, and it only happens when f1&lt;0.5.</li>
<li>When f2&lt;0.65, we further split f1 by a threshold of 0.2.</li>
<li>When f2&gt;0.65, we further split f1 by a threshold of 0.3.</li>
</ul>
<p>To this end, our tree has three leaf nodes, each leaf node can construct two bins (less/larger than the threshold), and we have six bins in total. Also, each data point can be placed into one of the bins. Then, we finish the construction of the KD-Tree. It should be noted that constructing a KD-Tree could be non-trivial since you need to consider some hyperparameters, such as how to set the threshold or how many bins we should create (or the stop criteria). In practice, there are no golden rules. Normally, the mean or median can be used to set the threshold. The number of bins could be highly dependent on the evaluation of the results and fine-tuning.</p>
<p>At search time, given a user query, it can be placed into one of the bins inside the feature space. We are able to compute the distance between the query and all the items within the bin as candidates for nearest neighbors. We also need to compute the minimum distance between the query and all other bins. If the distance between the query vector and other bins is greater than the distance between the query vector and the candidate for nearest neighbors, we can ignore all data points within that bin by pruning the leaf node of the tree. Otherwise, we consider the data points within that bin as candidates for nearest neighbors as well.</p>
<p>By constructing a<a id="_idIndexMarker230"/> KD-Tree, we do not necessarily compute the similarity between a query vector and each document anymore. Only a certain number of bins should be considered as candidates. Thus, the search time can be greatly reduced.</p>
<p>In practice, KD-Trees suffer from the curse of dimensionality. It is tricky to apply them to high-dimensional data because there are so many bins to search through simply because for each feature, we always create several thresholds. <strong class="bold">Locality Sensitive Hashing</strong> (<strong class="bold">LSH</strong>) could be a good alternative algorithm.</p>
<p>The basic idea behind LSH is that similar documents share the same hash code and it is designed to maximize collisions. To be more concrete: given a set of vectors, we want to have a hashing function that is able to encode similar documents into the same hashing bucket. Then, we only need to find similar vectors within the bucket (without the need to scan all the data).</p>
<p>Let’s start with <a id="_idIndexMarker231"/>LSH index construction. At indexing time, we first need to create <a id="_idIndexMarker232"/>random planes (hyperplanes) to split the feature space into <em class="italic">bins</em>.</p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<img alt="Figure 3.8 – LSH index construction with random hyperplanes " height="741" src="image/Figure_3.8_B17488.jpg" width="904"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – LSH index construction with random hyperplanes</p>
<p>In <em class="italic">Figure 3.8</em>, we have created<a id="_idIndexMarker233"/> six hyperplanes. Each hyperplane is able to split our feature space into two bins, either left/right or up/bottom, which can be represented as binary codes (or signs): 0 or 1. This is called the index of a bin.</p>
<p>Let’s try to get the bin index of the bottom-right bin (which has four points in the bin). The bin is <a id="_idIndexMarker234"/>located at the following points:</p>
<ul>
<li>Right of <strong class="bold">plane1</strong>, so the sign at position 0 is 1</li>
<li>Right of <strong class="bold">plane2</strong>, so the sign at position 1 is 1</li>
<li>Right of <strong class="bold">plane3</strong>, so the sign at position 2 is 1</li>
<li>Right of <strong class="bold">plane4</strong>, so the sign at position 3 is 1</li>
<li>Bottom of <strong class="bold">plane5</strong>, so the sign at position 4 is 0</li>
<li>Bottom of <strong class="bold">plane6</strong>, so the sign at position 5 is 0</li>
</ul>
<p>So, we can represent the bottom-right bin as 111100. If we iterate this process and annotate each bin with a bin index, we’ll end up with a hash map. The keys of the hash map are the bin indexes, while<a id="_idIndexMarker235"/> the values of the hash map are the IDs of the data points within the bin.</p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<img alt="Figure 3.9 – LSH index construction with bin index " height="736" src="image/Figure_3.9_B17488.jpg" width="904"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – LSH index construction with bin index</p>
<p>Searching on top of LSH is easy. Intuitively, given a query, you can just search all data points within its own bin, or you can search through its neighboring bins.</p>
<p>How do you search through its neighboring bins? Take a look at <em class="italic">Figure 3.9</em>. The bin <a id="_idIndexMarker236"/>index is represented as binary code; the neighboring bins will only have a 1-bit difference compared with its own bin index. Apparently, you can consider the difference between bin index as a hyperparameter, and search through more neighboring bins. For example, if you set the hyper parameters as 2, means you allow LSH to search through 2 neighbor bins.</p>
<p>To better understand this, we’ll look into the Annoy implementation<a id="_idIndexMarker237"/> of LSH, namely LSH with random projection. Given a list of vectors produced by a deep neural network, we first do the following:</p>
<ol>
<li>Randomly initialize a hyperplane.</li>
<li>Dot product the normal (the vector that is perpendicular to the hyperplane) against the vectors. For each vector, if the value is positive, we generate a binary code of 1, otherwise 0.</li>
<li>We generate N hyperplanes and iterate the process N times. At the end, each vector is represented by a binary vector of 0s and 1s.</li>
<li>We<a id="_idIndexMarker238"/> treat each binary code as a bucket and save all documents with the same binary code into the same bucket.</li>
</ol>
<p>The following code block demonstrates a simple implementation<a id="_idIndexMarker239"/> of LSH with random projection:</p>
<pre class="source-code">pip install numpy
pip install spacy
spacy download en_core_web_md</pre>
<p>We preprocess two pieces of sentences into buckets:</p>
<pre class="source-code">from collections import defaultdict
import numpy as np
import spacy
n_hyperplanes = 10
nlp = spacy.load('en_core_web_md')
# process 2 sentences using the model
docs = [
    nlp('What a nice day today!'),
    nlp('Hi how are you'),
]
# Get the mean vector for the entire sentence
assert docs[0].vector.shape == (300,)
# Random initialize 10 hyperplanes, dimension identical to embedding shape
hyperplanes = np.random.uniform(-10, 10, (n_hyperplanes, docs[0].vector.shape[0]))
def encode(doc, hyperplanes):
    code = np.dot(doc.vector, hyperplanes.T)  # dot product vector with norm vector
    binary_code = np.where(code &gt; 0, 1, 0)
    return binary_code
def create_buckets(docs, hyperplanes):
    buckets = defaultdict()
    for doc in docs:
        binary_code = encode(doc, hyperplanes)
        binary_code = ''.join(map(str, binary_code))
        buckets[binary_code] = doc.text
    return buckets
if __name__ == '__main__':
    buckets = create_buckets(docs, hyperplanes)
    print(buckets)</pre>
<p>In this way, we <a id="_idIndexMarker240"/>map millions of documents into multiple buckets. At search time, we use the same hyperplanes to encode the search document, get the binary code, and find similar documents within the same bucket.</p>
<p>In the Annoy implementation, search speed is dependent on two parameters:</p>
<ul>
<li><strong class="source-inline">search_k</strong>: This parameter is the top <strong class="source-inline">k</strong> elements you want to get back from the index.</li>
<li><strong class="source-inline">N_trees</strong>: This parameter represents the number of buckets you want to search from.</li>
</ul>
<p>It is obvious that the search runtime highly depends on these two parameters, and the user needs to fine-tune the parameters based on their use case.</p>
<p>Another popular ANN search<a id="_idIndexMarker241"/> algorithm is PQ. Before we dive into PQ, it is important to understand what <em class="italic">quantization</em> is. Suppose you have a million documents to index, and you created 100 <em class="italic">centroids</em> for all documents. A <strong class="bold">quantizer</strong> is a <a id="_idIndexMarker242"/>function that can map a vector to a centroid. You might find the idea very familiar. Actually, the K-means algorithm<a id="_idIndexMarker243"/> is a function that can help you generate such centroids. If you do not remember, K-means works as follows:</p>
<ol>
<li value="1">Randomly initialize <strong class="source-inline">k</strong> centroids.</li>
<li>Assign each vector to its closest centroid. Each centroid represents a cluster.</li>
<li>Compute new<a id="_idIndexMarker244"/> centroids based on the mean of all assignments, until converge.</li>
</ol>
<p>Once K-means converge, we get K clusters given all vectors to index. For each document to index, we create a map between the document ID and cluster index. At search time, we compute the distance query vector against the centroids and get the closest clusters, then find the closest vectors within these clusters.</p>
<p>This quantization algorithm has a relatively good compression ratio. You don’t have to linearly scan all vectors in order to get the closest ones; you only need to scan certain clusters produced by the quantizer. On the other hand, the recall rate at searching time could be very low if the number of centroids is small. This is because there are too many edge cases that cannot be correctly distributed to the correct cluster. Also, if we simplify the set number of centroids to a large number, our K-means operations will take a long time to converge. This becomes a bottleneck at both offline indexing time and online searching time.</p>
<p>The basic idea behind <a id="_idIndexMarker245"/>PQ is to split high-dimensional vectors into subvectors, as illustrated in the following steps:</p>
<ol>
<li value="1">We split each vector into <em class="italic">m</em> subvectors.</li>
<li>For each subvector, we apply quantization. To this end, for each subvector, we have a unique cluster ID (the closest cluster of the subvector to its centroids).</li>
<li>For the full vector, we have a list of cluster IDs, which can be used as the codebook of the full vector. The dimensionality of the codebook is identical to the number of subvectors.</li>
</ol>
<p>The following figure illustrates the PQ algorithm: given a vector, we cut it into subvectors of lower dimensionality and apply quantization. To this end, each quantized subvector gets a code:</p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<img alt="Figure 3.10 – Product quantization " height="1005" src="image/Figure_3.10_B17488.jpg" width="1344"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Product quantization</p>
<p>At search time, again, we split the high-dimensional query vector into subvectors and generate a codebook (bucket). We compute subvector-level cosine similarity against each of the <a id="_idIndexMarker246"/>vectors inside the collection and sum up the subvector-level similarity score. We sort the final results based on the vector-level cosine similarity.</p>
<p>In practice, FAISS has a high-performant implementation of PQ (and beyond PQ). For more info, please refer to the documentation (<a href="https://github.com/facebookresearch/faiss/wiki">https://github.com/facebookresearch/faiss/wiki</a>).</p>
<p>Now we have learned two fundamental tasks, indexing and querying, for neural search. In the next section, we are going to cover neural search system evaluation to make your neural search system complete and production-ready.</p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor050"/>Evaluating a neural search system</h1>
<p>Evaluating the <a id="_idIndexMarker247"/>effectiveness of a neural search system is critical once you set up some baseline. By monitoring the evaluation metrics, you can immediately know how well your system performs. By diving deep into the queries, you can also conduct failure analysis and learn how to improve your system.</p>
<p>In this section, we will give a brief overview of the most commonly used evaluation metrics. If you want to have a more detailed mathematical understanding of this topic, we strongly recommend you go through <em class="italic">Evaluation in information retrieval</em> (<a href="https://nlp.stanford.edu/IR-book/pdf/08eval.pdf">https://nlp.stanford.edu/IR-book/pdf/08eval.pdf</a>).</p>
<p>In general, given the difference between search tasks, normally we can group search evaluation into two categories:</p>
<ul>
<li><strong class="bold">Evaluation of unranked results</strong>: These metrics are widely used in some retrieval or classification tasks, including precision, recall, and F-Score.</li>
<li><strong class="bold">Evaluation of ranked results</strong>: These metrics are mainly used in typical search applications given the results are ordered (ranked).</li>
</ul>
<p>First, let’s start with precision, recall, and F-Score:</p>
<ul>
<li>In a typical search scenario, precision <a id="_idIndexMarker248"/>is defined as follows:</li>
</ul>
<p><em class="italic">Precision = (Num of Relevant Documents Retrieved) / (Num of Retrieved Documents)</em></p>
<p>The idea is straightforward. Suppose our search system returns 10 documents, where 7 out of 10 are relevant, then the precision would be 0.7.</p>
<p>It should be noted that during evaluation, we care about the top <strong class="source-inline">k</strong> retrieved results. Just like the aforementioned example, we evaluate relevant documents in the top 10 results. This is normally referred to as precision, such as <strong class="bold">Precision@10</strong>. It <a id="_idIndexMarker249"/>also applies to other metrics we will introduce later in this section, such as Recall@10, mAP@10, and nDCG@10.</p>
<ul>
<li>Similarly, recall<a id="_idIndexMarker250"/> is defined as follows:</li>
</ul>
<p><em class="italic">Recall = (Num of Relevant Documents Retrieved) / (Num of Relevant Documents)</em></p>
<p>For instance, if we search <strong class="source-inline">cat</strong> in our system, and we already know that there are 100 cat-related images being indexed, and 80 images are returned, then the recall rate is 0.8. It is an evaluation metric to measure the <em class="italic">completeness</em> of how the search system performs.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Recall is the<a id="_idIndexMarker251"/> most important evaluation metric to evaluate the performance of an ANN algorithm since it depicts the fraction of true nearest neighbors found for all the queries on average.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Accuracy can be a good metric for typical ML tasks, such as classification. But this is not the case for search tasks since most of the datasets in search tasks are skewed/imbalanced.</p>
<p>As a search <a id="_idIndexMarker252"/>system designer, you might already notice that these two numbers are trade-offs against each other: with an increased number of K, we can always expect lower precision but higher recall and vice versa. It is your decision to optimize precision or recall or optimize these two numbers as one evaluation metric, that is, F1-Score.</p>
<ul>
<li>F1-Score is <a id="_idIndexMarker253"/>defined as follows:</li>
</ul>
<p><em class="italic">F1-Score = (2 * Precision * Recall) / (Precision + Recall)</em></p>
<p>It is a weighted harmonic mean of precision and recall. In reality, a higher recall tends to be associated with a lower precision rate. Imagine you are evaluating a ranked list and you care about the top 10 items being retrieved (and there are 10 relevant documents in the entire collection):</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Document</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Precision</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Recall</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc1</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>1/1</p>
</td>
<td class="No-Table-Style">
<p>1/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc2</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>2/2</p>
</td>
<td class="No-Table-Style">
<p>2/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc3</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>2/3</p>
</td>
<td class="No-Table-Style">
<p>2/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc4</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>2/4</p>
</td>
<td class="No-Table-Style">
<p>2/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc5</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>3/5</p>
</td>
<td class="No-Table-Style">
<p>3/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc6</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>3/6</p>
</td>
<td class="No-Table-Style">
<p>3/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc7</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>3/7</p>
</td>
<td class="No-Table-Style">
<p>3/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc8</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>4/8</p>
</td>
<td class="No-Table-Style">
<p>4/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc9</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>4/9</p>
</td>
<td class="No-Table-Style">
<p>4/10</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc10</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>4/10</p>
</td>
<td class="No-Table-Style">
<p>4/10</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 – Precision recall for 10 of the top 10 documents</p>
<p><em class="italic">Table 3.1</em> shows the <a id="_idIndexMarker254"/>precision and recall at different levels given binary labels.</p>
<p>Being familiar with precision, we can now move on to calculate the <strong class="bold">Average Precision</strong> (<strong class="bold">AP</strong>). This <a id="_idIndexMarker255"/>metric will give us a better understanding of our search system’s ability to sort the results of a query. </p>
<p>Specifically, given the preceding-ranked list, <strong class="source-inline">aP@10</strong> is as follows:</p>
<p><strong class="source-inline">aP@10 = (1/1 + 2/2 + 3/5 + 4/8) / 10 = 0.31</strong></p>
<p>Note that only the precision of relevant documents is taken into consideration when computing aP.</p>
<p>Now, the aP has been calculated against one specific user query. However, to give a more robust search system evaluation, we want to evaluate the performance of a collection of user queries as a test set. This is <a id="_idIndexMarker256"/>called <strong class="bold">Mean Average Precision</strong> (<strong class="bold">mAP</strong>). For each query, we calculate <strong class="source-inline">aP@k</strong>, then we average all the aPs over a set of queries to get the mAP score.</p>
<p>mAP is one of the<a id="_idIndexMarker257"/> most important search system evaluation metrics given an ordered rank list. To conduct mAP evaluation on your search system, normally you have to follow these steps:</p>
<ol>
<li value="1">Compose a list of queries to have a good representation of users’ information needs. The number is dependent on your situation, such as 50, 100, or 200.</li>
<li>If your documents already have labels that indicate the degree of relevance, use the labels directly to compute aP per query. If your documents do not contain any relevant information against each query, we need expert annotation or pooling to access relevant degrees.</li>
<li>Compute mAP over a list of queries by taking the average of the aP. As was mentioned previously, if you do not have a relevant assessment for ranked documents, one common technique is called <strong class="bold">pooling</strong>. It <a id="_idIndexMarker258"/>requires us to set up multiple search systems (such as three) for testing. Given each query, we collect the top K documents returned by each of these three search systems. A human annotator judges the degree of relevance of all 3 * K documents. Afterward, we consider all documents outside this pool as irrelevant, while all documents inside this pool are relevant. Then, the search results can be evaluated on top of the pools.</li>
</ol>
<p>At this point, even though mAP is evaluating a ranked list, the nature of the definition of precision still neglects some of the nature of a search task: precision is evaluated based on binary labels, either relevant or irrelevant. It does not reflect the <em class="italic">relatedness</em> of the query against documents. <strong class="bold">Normalized Discounted Cumulative Gain</strong> (<strong class="bold">nDCG</strong>) is used for<a id="_idIndexMarker259"/> evaluating <a id="_idIndexMarker260"/>search system performance over the degree of relatedness. </p>
<p>nDCG can have multiple levels of rating for each document, such as <em class="italic">irrelevant</em>, <em class="italic">relevant</em>, or <em class="italic">highly relevant</em>. In this case, mAP does not work anymore.</p>
<p>For instance, given three degrees of relevance (irrelevant, relevant, and highly relevant), these differences in degrees of relevance can be represented as information gain that users can obtain by getting each document. For highly relevant documents, the gain could be assigned a value of <em class="italic">3</em>, relevant could be <em class="italic">1</em>, and not relevant could be set to <em class="italic">0</em>. Then, if a highly relevant document is ranked higher than documents that are not relevant, the user could cumulate more <em class="italic">gain</em>, which is referred to as <strong class="bold">Cumulative Gain</strong> (<strong class="bold">CG</strong>). The <a id="_idIndexMarker261"/>following table shows the information gain we get per document given the top 10 ranked documents produced by a search system:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Document</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Gain</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc1</p>
</td>
<td class="No-Table-Style">
<p>Highly Relevant</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc2</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc3</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc4</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc5</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc6</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc7</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc8</p>
</td>
<td class="No-Table-Style">
<p>Highly Relevant</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc9</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc10</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.2 – Top 10 documents with information gain</p>
<p>In the <a id="_idIndexMarker262"/>preceding table, the system returned the top 10 ranked documents to the user. Based on the relevance degree, we assign 3 as the gain to highly relevant documents, 1 as the gain to relevant documents, and 0 as the gain to irrelevant documents. The CG is the sum of all gains in the top 10 documents, such as the following:</p>
<p><strong class="source-inline">CG@10 = 3 + 1 + 1 + 3 = 8</strong></p>
<p>But think about the nature of a search engine: the users scan the ranked list from top to bottom. So, by nature, the top-ranked documents should have more gains than the documents ranked lower so that our search system will try to rank highly relevant documents in higher positions. So, in practice, we will penalize the gain by the position. See the following example:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Document</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Gain</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Discounted Gain</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc1</p>
</td>
<td class="No-Table-Style">
<p>Highly Relevant</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc2</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1/log2</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc3</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc4</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc5</p>
</td>
<td class="No-Table-Style">
<p>Relevant</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1/log5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc6</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc7</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc8</p>
</td>
<td class="No-Table-Style">
<p>Highly Relevant</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>3/log8</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc9</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Doc10</p>
</td>
<td class="No-Table-Style">
<p>Irrelevant</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.3 – Top 10 documents of gain and discounted gain</p>
<p>In the <a id="_idIndexMarker263"/>preceding table, given the gain of a document and its ranked position, we penalize the gain a little bit by dividing the gain by a factor. In this case, it’s the logarithm of the ranked <a id="_idIndexMarker264"/>position. The sum of the gain is called the <strong class="bold">Discounted Cumulative Gain</strong> (<strong class="bold">DCG</strong>):</p>
<p><strong class="source-inline">DCG@10 = 3 + 1/log2 + 1/log5 + 3/log8 = 6.51</strong></p>
<p>Before we start to compute nDCG, it’s important to understand the concept of ideal DCG. It simply means the best-ranking result we could achieve. In the preceding case, if we look at the top 10 positions, ideally the ranked list should contain all highly relevant documents with a gain of 3. So, the iDCG should be as follows:</p>
<p><strong class="source-inline">iDCG@10 = 3 + 3/log2 + 3/log3 + 3/log4 + 3/log5 + 3/log6 + 3/log7 + 3/log8 + 3/log9 + 3/log10 = 21.41</strong></p>
<p>In the end, the final nDCG is as follows:</p>
<p><strong class="source-inline">nDCG = DCG/iDCG</strong></p>
<p>In our preceding example, we have the following:</p>
<p><strong class="source-inline">nDCG@10 = 6.51/21.41 = 0.304</strong></p>
<p>It is worth mentioning that even though <a id="_idIndexMarker265"/>nDCG is well suited to evaluating a search system that reflects the degree of relevance, the <em class="italic">relatedness</em> itself is biased toward different factors, such as search context and user preference. It is non-trivial to perform such an evaluation in a real-world scenario. In the next chapter, we will dive into the details of such challenges and briefly introduce how to resolve them.</p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor051"/>Engineering challenges of building a neural search system</h1>
<p>Now, you would have <a id="_idIndexMarker266"/>noticed that the most important building blocks of the neural search system are the encoder and indexer. The quality of encoding posts has a direct impact on the final search result, while the speed of the indexer determines the scalability of your neural search system.</p>
<p>Meanwhile, this is still not enough to make your neural search system ready to use. Many other topics need to be taken into consideration as well. The first question is: does your encoder (neural model) have the same distribution as your data? For new people coming into the neural search system world who are using a pretrained deep neural network, such as<a id="_idIndexMarker267"/> ResNet trained <a id="_idIndexMarker268"/>on ImageNet, it is trivial to quickly set up a search system. However, if your target is to build a neural search system on a specific domain, let’s say a fashion product image search, it is not going to produce satisfying results.</p>
<p>One important topic before we really start creating an encoder and setting up our search system is applying transfer learning to your dataset and evaluating the match results. This means taking a pretrained deep learning model, such as ResNet, chopping off the head layer, freezing the weights of the pretrained model, and attaching a new embedding layer to the end of the model, then training it on your new dataset on your domain. This could greatly boost search performance.</p>
<p>Apart from that, in some vision-based search systems, purely relying on the encoder might not be sufficient. For instance, a lot of vision-based search systems rely heavily on an object detector. Before sending the full image into the encoder, it should be sent to the object detector first and the meaningful part of the image extracted (and remove the background noise). This is likely to improve the embedding quality. Meanwhile, some vision-based classification models could also be employed to enrich the search context as a hard filter. For instance, if you are building a neural search system that allows people to search similar automobiles given an image as a query, a pretrained brand classifier could be useful. To be more concrete, you pretrain an automobile brand classifier to <em class="italic">recognize</em> different car brands based on images and apply the recognition to the indexing and searching pipeline. Once a vision-based search is complete, you can apply the recognized brand as a hard filter to filter out cars of other brands.</p>
<p>Similarly, for text-based search, when a user provides keywords as a query, directly applying an embedding-based similarity search might be insufficient. For example, you can create a <strong class="bold">Named Entity Recognition</strong> (<strong class="bold">NER</strong>) module<a id="_idIndexMarker269"/> in your indexing and querying pipeline to enrich the metadata.</p>
<p>For web-based search <a id="_idIndexMarker270"/>engines such as Google, Bing, or Baidu, it is very common to see query automatic completion. It might be also very interesting to add a deep neural network-powered keyword extraction component to your indexing and searching pipeline to use a similar user experience.</p>
<p>To summarize, to build a production-ready neural search system, it is very challenging to design a feature-complete indexing and querying pipeline, given the fact that search is such a complex task. Designing such a system is already challenging, let alone engineering the infrastructure. Luckily, Jina can already help you with most of the most challenging tasks.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor052"/>Summary</h1>
<p>In this chapter, we have discussed the fundamental tasks to build a neural search system, which are the indexing and querying pipelines. We looked into both of them and introduced the most challenging part, such as encoding and indexing. </p>
<p>You should have basic knowledge of the basic building blocks of indexing and querying, such as preprocessing, encoding, and indexing. You should also notice that the quality of the search results highly depends on the encoder, while the scalability of the neural search system highly depends on the indexer and the most popular algorithms behind the indexer.</p>
<p>As you need to build a production-ready search system, you will realize that purely relying on the basic building blocks is not enough. As a search system is complex to implement, it is always needed to design and add your own building blocks to the indexing and querying pipeline, in order to bring better search results.</p>
<p>In the next chapter, we will start to introduce Jina, the most popular framework that helps you engineer a neural search system. You will realize that Jina has tackled the most difficult problems for you and could make your life as a neural search system engineer/scientist much easier.</p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer043">
<h1 id="_idParaDest-49"><a id="_idTextAnchor053"/>Part 2: Introduction to Jina Fundamentals</h1>
</div>
<div id="_idContainer044">
<p>In this part, you will learn about what Jina is and its basic components. You will understand its architecture and how can it be used to develop deep-learning searches on the cloud. The following chapters are included in this part:</p>
<ul>
<li><a href="B17488_04.xhtml#_idTextAnchor054"><em class="italic">Chapter 4</em></a>, <em class="italic">Learning Jina's Basics</em></li>
<li><a href="B17488_05.xhtml#_idTextAnchor068"><em class="italic">Chapter 5</em></a>, <em class="italic">Multiple Search Modalities</em></li>
</ul>
</div>
<div>
<div id="_idContainer045">
</div>
</div>
<div>
<div id="_idContainer046">
</div>
</div>
</div>
</body></html>