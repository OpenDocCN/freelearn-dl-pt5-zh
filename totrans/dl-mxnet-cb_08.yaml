- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving Training Performance with MXNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we have leveraged MXNet capabilities to solve computer
    vision and `GluonCV` and `GluonNLP`. We trained these models using different approaches:
    *from scratch*, *transfer learning*, and *fine-tuning*. In this chapter, we will
    focus on improving the performance of the training process itself and accelerating
    how we can obtain those results.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the objective of optimizing the performance of our training loops,
    MXNet contains different features. We have already briefly used some of those
    features such as the concept of **lazy evaluation**, which was introduced in [*Chapter
    1*](B16591_01.xhtml#_idTextAnchor016). We will revisit it in this chapter, in
    combination with automatic parallelization. Moreover, we will optimize how to
    access data efficiently, leveraging Gluon DataLoaders in different contexts (CPU,
    GPU) to perform data transforms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we will explore how to combine multiple GPUs to accelerate training,
    making use of techniques such as data parallelization for optimal performance.
    We will also explore how we can use different data types with `MXNet` to dynamically
    optimize the different data formats.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Finally, using problems already explored in the book, we will apply all these
    techniques with examples. For our computer vision task, we will choose image segmentation,
    and for our NLP task, we will choose translating text from English to German.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter is structured with the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Introducing training optimization features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing training for image segmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing training for translating text from English to German
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have completed the *Installing MXNet, Gluon, GluonCV and GluonNLP*
    recipe from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098)
    and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you have completed [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can access each recipe directly from Google Colab. For example,
    the code for the first recipe of this chapter can be found here: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Introducing training optimization features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how we could leverage *MXNet*, *GluonCV*, and
    *GluonNLP* to retrieve pre-trained models in certain datasets (such as **ImageNet**,
    **MS COCO**, or **IWSLT2015**) and use them for our specific tasks and datasets.
    Furthermore, we used transfer learning and fine-tuning techniques to improve the
    performance on those tasks/datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们展示了如何利用*MXNet*、*GluonCV*和*GluonNLP*来检索特定数据集（如**ImageNet**、**MS COCO**或**IWSLT2015**）中的预训练模型，并将其用于我们的特定任务和数据集。此外，我们还使用了迁移学习和微调技术来提高这些任务/数据集上的性能。
- en: In this recipe, we will introduce (and revisit) several concepts and features
    that will optimize our training loops, after which we will analyze the trade-offs
    involved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍（并重温）几个概念和特性，这些将优化我们的训练循环，之后我们将分析其中的权衡。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Similar to the previous chapters, in this recipe, we will be using some matrix
    operations and linear algebra, but it will not be hard at all, as you will find
    lots of examples and code snippets to facilitate your learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前几章，在本教程中，我们将使用一些矩阵操作和线性代数，但这不会很困难，因为你会发现许多示例和代码片段来帮助你学习。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will work through the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过以下步骤进行操作：
- en: Working with lazy evaluation and automatic parallelization
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用懒评估和自动并行化
- en: 'Optimizing DataLoaders: GPU preprocessing and CPU threads'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化DataLoader：GPU预处理和CPU线程
- en: Training with `Float32`, `Float16`, and Automatic Mixed Precision
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Float32`、`Float16`和自动混合精度进行训练
- en: Training with multiple GPUs and data parallelization
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多个GPU和数据并行化进行训练
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这些步骤。
- en: Working with lazy evaluation and automatic parallelization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用懒评估和自动并行化
- en: In the *NumPy and MXNet NDArrays* recipe of [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016),
    we introduced lazy evaluation, the strategy that MXNet follows when computing
    operations. This strategy is optimal for large compute loads, where the actual
    calculation is deferred until the values are actually needed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B16591_01.xhtml#_idTextAnchor016)的*NumPy和MXNet NDArrays*教程中，我们介绍了懒评估，MXNet在计算操作时采用的策略。这种策略对于大计算负载来说是最优的，因为实际的计算会被延迟，直到这些值真正需要时才会计算。
- en: Furthermore, by not executing the computation of the operations until they are
    needed, MXNet can also parallelize some of those computations, meaning the data
    involved is not sequentially processed. This process is done automatically and
    is very useful when sharing data across multiple hardware resources such as CPUs
    and GPUs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MXNet通过推迟操作计算，直到它们真正需要时，能够并行化一些计算，这意味着涉及的数据不会按顺序处理。这一过程是自动完成的，对于在多个硬件资源（如CPU和GPU）之间共享数据时非常有用。
- en: 'As a toy example, we can run some experiments with matrix multiplication. Our
    first experiment will run the generation of four matrices and then a combination
    of multiplications among them. After each computation, we will force the computation
    to be finalized (by adding calls to the `wait_to_read()` function). We will compute
    the results for two configurations. The initial configuration will be to force
    MXNet to work with one thread (`NaiveEngine`). With this configuration, the computation
    took this long:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例，我们可以进行一些矩阵乘法实验。我们的第一个实验将生成四个矩阵，然后进行它们之间的乘法组合。在每次计算后，我们将强制计算完成（通过添加`wait_to_read()`函数调用）。我们将计算两种配置下的结果。初始配置将强制MXNet使用一个线程（`NaiveEngine`）。在这种配置下，计算花费的时间是：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The second configuration tested will be MXNet in its usual, default configuration
    (`ThreadedEnginePerDevice`, with four CPU threads). With this configuration, the
    computation took this long:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种配置是测试MXNet的常规默认配置（`ThreadedEnginePerDevice`，四个CPU线程）。在这种配置下，计算花费的时间是：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, forcing each computation to be finalized before moving to the
    next one (`wait_to_read()` calls) is counter-productive when working in a multi-threading
    configuration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，强制每次计算在进入下一步之前完成（通过调用`wait_to_read()`）在多线程配置中是适得其反的。
- en: 'Our second experiment will be very similar; however, this time, we will remove
    all calls to the `wait_to_read()` function. We will only ensure that all calculations
    for the multiplication of the matrices are finalized before computing the time
    taken. For the initial configuration (*NaiveEngine*), the computation takes the
    following amount of time:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As expected, this is a very similar duration to only working with one thread,
    as all computations are sequential.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'With our second configuration (*ThreadedEnginePerDevice*, with four CPU threads),
    the computation for this second experiment took the following amount of time:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The results show that when using multiple threads (the default automatic configuration
    for MXNet), we achieved a ~20% improvement (improvements can be even higher with
    different workloads more suited for multi-threading).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Please note how in the code, we used the `mx.nd.waitall()` function to verify
    that all computations had been strictly completed before computing the time these
    operations took.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing DataLoaders – GPU preprocessing and CPU threads
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the *Understanding image datasets – loading, managing, and visualizing the
    fashion MNIST dataset* recipe of [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    we introduced **Gluon DataLoader**, an efficient mechanism to generate batch sizes
    to feed into our models for training and evaluation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader has two important roles to play in our data preprocessing. On the
    one hand, as we have explored in previous chapters, our models are optimized for
    parallel data processing, meaning that we can ingest several samples (for example,
    images for an image segmentation task) at the same time in the same *batch* and
    it will be processed in parallel by the *GPU*. This parameter is called *batch
    size*. On the other hand, samples typically need to be preprocessed in order to
    maximize the performance of the model (for example, images are resized and its
    values allocated to `[0,` `1]` from `[0,` `255]`). These operations are time-consuming
    and optimizing them can save large amounts of time and compute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze the effect of the preprocessing of the data in the GPU, compared
    to the general, default behavior of using the CPU. As a baseline, let’s compute
    how long it takes to just load the dataset using only the CPU. We select the validation
    split of a segmentation dataset, and the result is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, when loading the dataset, we typically apply certain `transform` operations
    that maximize our network performance. The usual transform operations including
    image resizing, cropping, transforming to tensors, and normalizing can be defined
    in MXNet with the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With these transform operations applied when processing the validation split
    of a segmentation dataset using only the CPU, the processing time is the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we can see, the processing time has increased by more than 50%, from ~24s
    to ~39s. However, when we leverage the GPU for the data preprocessing, the processing
    time is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，处理时间增加了超过50%，从大约24秒增加到大约39秒。然而，当我们利用GPU进行数据预处理时，处理时间如下：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, the GPU-based preprocessing operations have an overhead that
    is almost negligible (<5%).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，基于GPU的预处理操作几乎没有额外开销（<5%）。
- en: 'Furthermore, performing the preprocessing in the GPU has another advantage:
    the data can be kept stored in the GPU for our models to process, whereas when
    preprocessing with the CPU, we need to send a copy of the data to the GPU memory,
    which can take a significant amount of time. If we actually measure our end-to-end
    preprocessing pipeline, combining the data preprocessing with the copy operation
    to the GPU memory, these are the results. With the CPU only, the end-to-end processing
    time is the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在GPU上执行预处理还有另一个优势：数据可以保存在GPU中供我们的模型处理，而使用CPU进行预处理时，我们需要将数据复制到GPU内存中，这可能会占用大量时间。如果我们实际测量端到端的预处理流水线，将数据预处理与复制操作到GPU内存结合起来，得到的结果如下：仅使用CPU时，端到端处理时间如下：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we can see, the copy time is significant, taking the whole pipeline more
    than 1 minute. However, the result when using the GPU is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，复制时间非常长，整个流水线需要超过1分钟。然而，使用GPU时的结果如下：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This shows a significant improvement (<40%) in the time it took for the full
    preprocessing. In summary, this was due to two factors: the fact that the preprocessing
    operations are faster in the GPU, and secondly, that the data needs to be copied
    to the GPU at the end of the process so that our models (which are also stored
    in the GPU) process the data efficiently.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明完整预处理所需的时间有了显著改善（<40%）。总的来说，这是由于两个因素：首先，预处理操作在GPU上更快；其次，数据需要在过程结束时复制到GPU，这样我们的模型（也存储在GPU中）才能高效地处理数据。
- en: The most important drawback of this approach is the need to keep the full dataset
    in the GPU. Typically, GPU memory space is optimized for each batch that you use
    for training or inference, not the whole dataset. This is the reason why this
    approach typically finishes with the processed data being copied back out of the
    GPU memory space and into the CPU memory space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最主要的缺点是需要将整个数据集保存在GPU中。通常，GPU内存空间是为你在训练或推理中使用的每个批次进行优化的，而不是为整个数据集进行优化。这就是为什么这种方法通常会以将处理后的数据从GPU内存空间复制回CPU内存空间的方式结束。
- en: However, there are situations where keeping the data in the GPU memory space
    might be the right approach – for example, when you are experimenting with datasets,
    and maybe loading several datasets and testing different preprocessing pipelines.
    In these situations you want fast turn-around times for your experiments and,
    therefore, speed is the right variable to optimize for. Moreover, sometimes you
    are not working with the full training/validation/test splits of a dataset, but
    just a part of it (again, for example, for experiments). In these cases, optimizing
    for speed makes sense as well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些情况下，将数据保存在GPU内存空间可能是正确的做法——例如，当你在实验不同的数据集时，可能会加载多个数据集并测试不同的预处理流水线。在这种情况下，你希望实验能快速完成，因此速度是需要优化的变量。此外，有时你并不是在处理数据集的完整训练/验证/测试集，而只是其中的一部分（例如，为了实验）。在这种情况下，优化速度也是合理的。
- en: 'For other, more production-oriented environments, the right approach is to
    preprocess in GPU memory space but keep the data (copying back) in CPU memory
    space. In this scenario, the results vary slightly:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他更面向生产的环境，正确的方法是在GPU内存空间中进行预处理，但将数据（回复制）保留在CPU内存空间。在这种情况下，结果略有不同：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the preprocessing step being done in the GPU is still a significant
    increase (~50%) in performance, even taking into account the necessary data movements
    from the CPU to the GPU and back.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，即使考虑到必要的数据移动（从CPU到GPU，再从GPU回到CPU），在GPU中进行预处理仍然能显著提高性能（约50%）。
- en: 'Now, we will take a deeper look at how we can leverage two important parameters
    that Gluon DataLoader takes as input: the number of workers and the batch size.
    The number of workers is the number of threads that DataLoader will launch in
    parallel (multi-threading) for data preprocessing. Batch size, as mentioned, is
    the number of samples that will be processed in parallel.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨如何利用Gluon DataLoader作为输入的两个重要参数：工作线程数和批量大小。工作线程数是DataLoader将并行启动的线程数量（多线程）用于数据预处理。批量大小，如前所述，是将并行处理的样本数量。
- en: 'These parameters are directly related to the number of cores the CPU has, and
    can be optimized to use the available HW for maximum performance. To find out
    how many cores our CPU has, Python provides a very simple API:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数与CPU的核心数量直接相关，并且可以通过优化使用可用的硬件来实现最大性能。为了了解CPU的核心数，Python提供了一个非常简单的API：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the environment selected, the number of cores available is shown as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在所选环境中，显示的可用核心数如下：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Combining the usage of the CPU and the GPU, we can compute the best performance
    taking into account different values for the number of workers and the batch size.
    The results for the selected environment are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合使用CPU和GPU，我们可以计算最佳性能，考虑不同的工作线程数和批量大小值。为所选环境计算的结果如下：
- en: "![Figure 8.1 – Runtim\uFEFFe(s) versus Batch Size for different computation\
    \ regimes (CPU/GPU and number of workers)](img/B16591_08_1.jpg)"
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 不同计算模式下（CPU/GPU和工作线程数）运行时间与批量大小的关系](img/B16591_08_1.jpg)'
- en: Figure 8.1 – Runtime(s) versus Batch Size for different computation regimes
    (CPU/GPU and number of workers)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 不同计算模式下（CPU/GPU和工作线程数）运行时间与批量大小的关系
- en: 'From *Figure 8**.1*, we can conclude three important aspects:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图8.1*中，我们可以得出以下三个重要结论：
- en: A GPU preprocessing pipeline (data processing plus memory storage) is much faster
    (+50% runtime improvement), even when copying back the data.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU预处理管道（数据处理加内存存储）要快得多（+50% 的运行时提升），即使是将数据复制回CPU时也是如此。
- en: When combining both the GPU and CPU, as we are only working with one GPU in
    this environment, it bottlenecks when we copy the data back to the CPU, as it’s
    done per sample (not per batch).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当结合使用GPU和CPU时，由于在这个环境下我们只使用一个GPU，因此当将数据复制回CPU时会遇到瓶颈，因为数据是逐个样本复制的（而不是按批次）。
- en: If working only with a CPU, adding workers improves the processing time. However,
    the limit is the number of threads. Adding more workers than threads (four in
    our case) will give no improvement in performance. An increase in the batch size
    yields better performance until a given number (8 in our case) and won’t improve
    performance further than that.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果仅使用CPU，增加工作线程可以改善处理时间。然而，限制因素是线程的数量。添加的工作线程数超过线程数（在我们的例子中为四个）将不会提高性能。增加批量大小能够提升性能，直到达到某一数量（在我们的例子中为8），超过该数量后，性能不会进一步提高。
- en: Important Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When working with the GPU, the MXNet Gluon DataLoader only supports the value
    `0` (zero) for the number of workers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU时，MXNet Gluon DataLoader仅支持工作线程数为`0`（零）的值。
- en: Training with Float32, Float16, and automatic mixed precision
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Float32、Float16和自动混合精度进行训练
- en: In the previous recipes, we have seen how to optimize our training loops by
    using different approaches to optimize the CPU and the GPU for maximum performance
    for a given model. In this recipe, we will explore how our data inputs, our model
    parameters, and the different arithmetic calculations around them are computed,
    and how we can optimize them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们已经看到了如何通过不同的方式优化训练循环，以最大限度地提高给定模型的CPU和GPU性能。在本示例中，我们将探讨如何计算我们的数据输入、模型参数及其周围的各种算术运算，并了解如何优化它们。
- en: 'First of all, let’s understand how computations work. The default data type
    for the data inputs and the model parameters is `Float32`, as can be verified
    (see the recipe code), which yields the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解计算是如何进行的。数据输入和模型参数的默认数据类型是`Float32`，可以通过（参见示例代码）验证这一点，产生以下输出：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This output indicates, as expected, that the data type of our data inputs and
    our model parameters is `Float32` (single-precision). But what does this mean?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出结果如预期所示，表明我们的数据输入和模型参数的数据类型是`Float32`（单精度）。但这意味着什么呢？
- en: '`Float32` indicates two things: on the one hand, that it is a data type that
    supports decimal numbers using a floating radix point, and on the other hand,
    that 32 bits are used to store a single number in this format. The most important
    features of this format are the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The ability to represent large numbers, from 10-45 to 10+38
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable precision
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `Float32` as the data type has numerous advantages, mostly connected
    to its variable precision. However, the training process is an iterative optimization
    process, for which many of the calculations involved do not require the precision
    of the `Float32` data type. We could afford, in a controlled way, to trade off
    some precision if it allowed us to speed up the training process. One of the ways
    we can achieve that balanced trade-off is with the `Float16` data type (half-precision).
    Similarly to `Float32`, the most important features of `Float16` are the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The ability to represent large numbers, from 2-24 to 2+16
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable precision
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example of the loss of precision, we can display the approximated value
    of 1/3 in both formats with this code excerpt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This yields the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we can see, none of the representations are exact, with `Float32` yielding
    higher precision as expected, and `Float16` having more limited accuracy, but
    potentially enough for some use cases (such as model training, as we will prove
    shortly).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, this loss of accuracy is a trade-off, where we obtain large speed
    gains in our training loops. To enable `Float16` (half-precision) for our training
    loops, we need to apply certain changes to our code. First of all, we need to
    update our model parameters to `Float16`, which we can do with one simple line
    of code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After this, when our model is going to process the data and ground truth, these
    need to be updated to `Float16` too, so in our training loop, we add the following
    lines:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With these changes, we can now run an experiment to compare the performance
    of both training loops. For example, we are going to fine-tune a DeepLabv3 pre-trained
    model in an image segmentation task (see the *Improving performance for segmenting
    images* recipe of [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)). For `Float32`,
    we obtain the following results:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For `Float16`, these are the results we obtained:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Unfortunately, for `Float16`, although our training time took ~1/3rd than the
    `Float32` training loop, it did not converge. This is due to several reasons:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Limited support for large numbers, as any integer larger than `65519` is represented
    as infinity
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited support for small numbers, as any positive decimal number smaller than
    `1e-7` is represented as `0` (zero)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thankfully, MXNet offers a solution that automatically combines the best of
    both worlds:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Applying `Float32` (single-precision) where it is necessary
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying `Float16` (half-precision) where it is not, for runtime optimization
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This approach is called **Automatic Mixed Precision** (**AMP**), and in order
    to enable it, we just need to make a few changes in our code. First of all, before
    creating our model, we need to initialize the library:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, after initializing our trainer/optimizer, we need to link it with AMP:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And finally, in order to prevent underflow or overflow, we need to enable `Float16`
    data type. This is done quite conveniently in the training loop:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we apply these changes and repeat the previous experiment for `Float16`
    (now with AMP enabled), we obtain the following results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see, we obtained very similar results for the validation loss in a
    much shorter amount of time (~33%).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'As the memory footprint of our training loop is now approximately half of what
    it was before, we can typically either double the size of our model (more layers
    and larger resolutions), or double our batch size, as the GPU memory consumed
    will be the same in this case compared to a full `Float32` training loop. Running
    the same experiment with a double batch size yields the following results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, increasing the batch size has an excellent effect on the performance
    of our training loop, with a much lower validation loss, and still benefiting
    from a significantly smaller amount of training time (~33%).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'However, typically, as a **Machine Learning Engineer** (**MLE**) or **Data
    Scientist** (**DS**), we will work with large amounts of data and large models,
    running training loops expected to last for hours or days. Therefore, it is very
    common for MLEs/DSs at work to start training loops just before the end of the
    working day, leaving the training running in the background, and coming back the
    next working day to analyze and evaluate the results. In such an environment,
    it is actually a better strategy to optimize performance given an expected training
    time. With MXNet, we can optimize our training parameters for this as well. For
    example, we could adjust the training time by doubling the number of epochs. In
    this case, the experiment yields the following results:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Compared to a vanilla `Float32` training loop, these results are excellent.
    However, let’s not forget that the actual results depend on the specific task,
    datasets, model, hyperparameters, and so on. You are encouraged to try different
    options and hyperparameters with toy training loops to find the solution that
    works best for each case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Training with multiple GPUs and data parallelization
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we will leverage having multiple GPUs in our environment to
    optimize our training further. MXNet and Gluon allow us to update our training
    loops to include multiple GPUs very easily.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'From a high-level perspective, there are two paradigms to leverage multiple
    GPUs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '**Model parallelization**: The model is split into parts and each part is deployed
    to a specific GPU. This paradigm is very useful when the model does not fit in
    a single GPU.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data parallelization**: The data batches are split into parts and each part
    is deployed to a specific GPU that can perform a forward and a backward pass using
    that data fully.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will work exclusively with data parallelization as it is the most common
    use case, yielding high speed-ups, and is also the most convenient given the simplicity
    of its approach.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to apply data parallelization, we will need to make modifications
    to our training loop, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting the context**: The context is now a list, where each element is a
    specific GPU context.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Initializing our model in those contexts**: In data parallelization, each
    GPU will store a copy of all the model parameters.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adapting hyperparameters**: Batch size is typically set to the largest possible
    without filling up the GPU memory. When working with several GPUs in parallel,
    this number can typically be multiplied by the number of GPUs in the context.
    However, this also has a side effect on the learning rate, which must be multiplied
    by the same number to keep gradient updates in the same range.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distributing the data**: Each GPU must have a slice of each batch and run
    the forward and backward passes with it.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computing the losses and updating the gradients**: Each GPU will compute
    the losses associated with their slice of each batch. MXNet automatically combines
    the losses and computes the gradients that are distributed to each GPU to update
    their model copy.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Displaying results**: Statistics such as the training loss and the validation
    loss are typically computed and accumulated during each batch and visualized at
    the end of each epoch.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s see some examples of how to apply each of these steps.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to set the context in an environment with four GPUs is very easy
    with MXNet and just requires one line of code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Initializing the model and custom layers is as easy as that. For our environment,
    this is how we can initialize a Deeplabv3 network with a `ResNet-101` backbone:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To update the hyperparameters, we just need to compute the number of GPUs in
    the context and update the previously computed batch size and learning rates.
    For our example, this simply means adding/modifying some lines of code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In order to distribute the data evenly across every GPU, MXNet and Gluon have
    a very convenient function, `split_and_load()`, which automatically allocates
    the data according to the number of GPUs in the context. For our environment,
    this is done as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To compute the losses and update the gradients, the data distributed in each
    GPU is processed in parallel using a loop. As MXNet provides automatic parallelization,
    the calls are not blocking and each GPU computes its outputs and losses independently.
    Furthermore, MXNet combines those losses to generate the full gradient updates,
    and redistributes this to each GPU, and all of this is done automatically. We
    can achieve all this with just a few lines of code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Lastly, in order to display the loss computations, each GPU loss needs to be
    processed and combined. Using automatic parallelization, this can be achieved
    with just one line of code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With these simple steps, we have been able to modify our training loop to support
    multiple GPUs, and we are now ready to measure the performance increase of these
    changes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, using one GPU, we reached the following performance (with a
    batch size of four):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In our environment, with 4 GPUs, we could increase the batch size to 16, the
    results of which would be as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As expected, we have been able to reduce the time spent in training to ~25%
    (the expected reduction when going from 1 GPU to 4 GPUs, with some expected loss
    due to the data distribution) while maintaining our validation scores (even slightly
    improving them).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we take a deeper look at how MXNet and Gluon can help us optimize
    our training loops. We have leveraged our HW (CPUs and GPUs) to address each of
    the steps in the training loop:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: We revisited how lazy evaluation and automatic parallelization mechanisms work
    together to optimize all MXNet-based flows.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We leveraged all our CPU threads to load data and optimized that process further
    via preprocessing in GPU. We also compared the trade-offs between speed and memory
    optimizations.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16` where possible, using AMP.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We compared each of these scenarios by running two experiments, comparing the
    performance before a specific optimization to the performance afterward, emphasizing
    potential trade-offs that have to be taken into account when using these optimizations.
    In the recipes that follow, we will apply all these optimization techniques concurrently
    to optimize two familiar tasks: **image segmentation** and **text translation**.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the optimization features shown in this recipe have been thoroughly described
    in the research literature. The following are some introductory links to start
    understanding each of the features in depth:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Lazy evaluation and automatic** **parallelization:** [https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization](https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gluon** **DataLoaders:** [**https:**//mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AMP:** [https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0](https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training with multiple** **GPUs:** [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing training for image segmentation
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we saw how we could leverage MXNet and Gluon to optimize
    the training of our models with a variety of different techniques. We understood
    how we can jointly use lazy evaluation and automatic parallelization for parallel
    processing. We saw how to improve the performance of our DataLoaders by combining
    preprocessing in the CPU and GPU, and how using half-precision (`Float16`) in
    combination with AMP can halve our training times. Lastly, we explored how to
    take advantage of multiple GPUs to further reduce training times.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can revisit a problem we have been working with throughout the book:
    **image segmentation**. We have worked on this task in recipes from previous chapters.
    In the *Segmenting objects semantically with MXNet Model Zoo – PSPNet and DeepLabv3*
    recipe in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), we learned how to use
    pre-trained models from GluonCV Model Zoo, and introduced the task and the datasets
    that we will be using in this recipe: **MS COCO** and the **Penn-Fudan Pedestrian**
    dataset. Furthermore, in the *Improving performance for segmenting images* recipe
    in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148)*, Optimizing Models with Transfer
    Learning and Fine-Tuning* we compared the different approaches that we could take
    when dealing with a target dataset, training our models from scratch, or leveraging
    the existing knowledge of pre-trained models and adjusting it for our task using
    the different modalities of transfer learning and fine-tuning.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will apply all these optimization techniques for the specific
    task of training an image segmentation model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all, as you will find lots of examples
    and code snippets to facilitate your learning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying training optimization techniques
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing the results
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s dive into each of these steps.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the *Improving performance for segmenting images* recipe in [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    we processed the data with the following approach:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Loaded the data from storage into the *CPU* *memory space*
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocessed the data using the *CPU*
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Used the **default parameters** to process the data during training
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This was a valid approach to compare the different training alternatives available
    to us (training from scratch, pre-trained models, transfer learning, and fine-tuning)
    without adding complexity to the experiments. For example, this approach worked
    quite well to introduce and evaluate the technique of fine-tuning directly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the aforementioned approach on the dataset selected for this recipe
    (*Penn-Fudan Pedestrian*), the CPU-based preprocessing took the following amount
    of time:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Furthermore, when combined with the necessary step of reloading the data in
    batches and copying it to the GPU, we obtain the following performance:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations directly by using the
    technique of fine-tuning. In combination with this approach, we will use the following
    hyperparameters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In these conditions, the training process duration and performance achieved
    were as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we can see, we got an excellent validation performance (~0.09) in a little
    over 10 minutes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of the training loss and the validation loss across each epoch
    looks as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Revisiting training: training loss versus validation loss](img/B16591_08_2.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2 – Revisiting training: training loss versus validation loss'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.2*, we can see the evolution of the training and validation
    loss. As explored throughout the chapters, we select the model that provide the
    minimal validation loss (in this case, this was achieved in the last epoch, epoch
    10).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training is completed, we can verify the overall performance in the
    test split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As expected, we got excellent results by training for just a limited number
    of epochs (10 in this case).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, this is what we have:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training](img/B16591_08_3.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the results show how the model has learned to focus on the people
    in the foreground, avoiding the ones in the background.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Applying training optimization techniques
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the *Introducing training optimization features* recipe at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take when training a machine learning
    model, including preprocessing the data and training and evaluating the model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will show how, with MXNet and Gluon and just a few lines
    of code, we can easily apply all the techniques we’ve been introduced to.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the first recipe of this chapter, MXNet applies by default the best
    policy (`ThreadedEnginePerDevice`) to optimize lazy evaluation and automatic parallelization,
    taking into account the number of CPU threads available, so there is no need for
    us to apply any changes here (please note that this technique is also applied
    automatically when working with multiple GPUs).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'We also showed how we could optimize our data preprocessing pipeline by combining
    the usage of CPU threads and GPUs, taking into account the number of devices available
    for each, and optimizing accordingly. For this experiment, specific HW was chosen
    with the following characteristics:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In order to use this optimization technique, we had to apply some changes to
    our code. Specifically, we define the GPUs available for use:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Furthermore, in our preprocessing pipeline, we now need a specific step that
    takes the data from CPU memory space and copies it to GPU memory space:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As discussed in the first recipe of this chapter, in a typical production-oriented
    environment, we do not want to keep the data in the GPU, occupying precious GPU
    memory. It is usual to optimize the batch size for the GPU memory available, and
    to load the data from the CPU memory space into the GPU memory space in batches
    using *MXNet Gluon DataLoaders*. Therefore, for our GPU-based preprocessing pipeline
    to be complete, we need a final step to copy the data back into the CPU memory
    space:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'With these code changes, our optimal preprocessing pipeline is ready, and we
    can continue with the next optimization technique: applying `Float16` optimizations,
    including AMP.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the first recipe of this chapter, in order to enable this technique,
    we just need a few changes in our code. First of all, we initialize the library:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Secondly, we attach the trainer/optimizer to the library:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And lastly, due to the limitations of the `Float16` data type, there is a risk
    of gradients over/under-flowing; therefore, we need to adjust (scale) the loss
    accordingly, which can be done automatically with these lines of code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: With these three simple changes, we have updated our training loop to work efficiently
    with the `Float16` data type (when appropriate).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note in the preceding code snippet how we are now working with a list
    of losses, instead of a single instance. This is due to our next and last training
    optimization technique: working with *multiple GPUs*.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: As we will see, working with multiple GPUs optimally implies working with them
    in parallel, and therefore, computing losses and executing the training backward
    pass in parallel, yielding the losses list described in the previous paragraph.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to work with multiple GPUs in parallel, we need to define the new
    context as a list (seen before for preprocessing, and shown here again for convenience):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As we now have multiple GPUs, we can increase our batch size to optimally use
    the available GPU memory space:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Furthermore, when reading from Gluon DataLoaders, we need to split the batches
    of data across the GPUs. Thankfully, Gluon also provides a function that simplifies
    that action. We just need the following lines of code to be added (for each training
    and validation batch):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As mentioned, this split across GPUs allows us to compute in parallel the model
    outputs and the losses associated with those outputs (a measure of the difference
    between the actual outputs and the expected outputs). This can be achieved with
    the following lines of code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And lastly, we compute the backward pass used to update the weights of our
    model (combined with the scaled loss of AMP):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With these minimal code changes, we now have an optimal preprocessing and training
    pipeline and can run our experiments to analyze the performance changes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the results
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we revisited the previous performance of our preprocessing
    and training pipelines, and we reviewed how we had to apply the necessary changes
    for our training optimization techniques, specifically for our image segmentation
    task.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Our preprocessing pipeline steps are now the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Load the data from storage into CPU memory space.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data using the GPU.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy back the data to CPU memory space.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the optimized parameters to process the data during training.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our experiments, we are going to use the technique of fine-tuning directly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the approach described earlier on the dataset selected for this recipe
    (*Penn-Fudan Pedestrian*), the preprocessing took the following amount of time:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'An end-to-end preprocessing pipeline must take into account the process of
    batching using the *Gluon DataLoader* to load the data – in our case, into multiple
    GPUs as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Compared to the initial section of this recipe (where the preprocessing took
    `0.4` seconds), we can see how, even with the added overhead of copying back the
    data to the CPU memory space, we have improved the preprocessing performance by
    >2 times.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Please note how, by adding multiple GPUs to the training process, we can increase
    the batch size (multiplied by the number of GPUs), and we can also increase the
    learning rate (from 0.1 to 0.5). In these conditions, the training process duration
    and performance achieved were as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As can be seen, we got excellent validation performance (~0.09) in less than
    1 minute. When comparing with the results obtained in the recipe, we can see how
    there was a minimal decrease in the loss (a positive change that we will confirm
    with our performance analysis shortly), but the largest improvement by far was
    a >10x decrease in the training time. This improvement is due to all the training
    optimization techniques that we have applied. In a nutshell, each of the optimizations
    provided the following improvements:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '**Using 4 GPUs**: Provided a 4x decrease in time'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Float16 and AMP**: Provided a 2x decrease (8x combined)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing the datasets**: Provided a 1.25x decrease (>10x combined)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evolution of the training loss and the validation loss across each epoch
    was the following:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Optimized training: training loss versus validation loss](img/B16591_08_4.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4 – Optimized training: training loss versus validation loss'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.4*, we can see the evolution of the training and validation
    losses. As explored throughout the chapters so far, we select the model that provided
    the minimal validation loss (in this case, achieved in the last epoch, epoch 10).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'After training is completed, we can verify the overall performance in the test
    split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (10 in this case). We can also confirm that the minimal improvement
    in the validation loss provided a minimal improvement in our test metrics (compared
    with 0.96/0.91 in our initial experiment).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we have the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training](img/B16591_08_5.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the results show how the model has learned to focus on the different
    people in the foreground, avoiding the ones in the background.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we applied the different training optimization techniques seen
    in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address
    each of the steps in the training loop:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: We revisited how lazy evaluation and automatic parallelization mechanisms worked
    together to optimize all MXNet-based flows.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We leveraged all our CPU threads to load data and optimized that process further
    via preprocessing in the GPU. We also compared the trade-offs between speed and
    memory optimizations.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16` where possible, using AMP.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compared each of these scenarios applied specifically to the task of image
    segmentation, running two experiments. In the first experiment, we did not apply
    any of the training optimization techniques described in the previous recipe,
    following the approach seen in previous chapters of the book. In the second experiment,
    we applied all the techniques in parallel, trying to optimize as much as we could.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: This proved quite useful, delivering similar algorithmic performance, with 10x
    improvement in training time (from 10 minutes to 1 minute). This was mostly due
    to using multiple GPUs (4x decrease), leveraging `Float16` AMP (2x decrease),
    and the optimized preprocessing (1.25x decrease).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have described, implemented, executed, and evaluated several training optimization
    techniques. However, there are even more advanced techniques that can be leveraged
    to achieve the optimal training loop.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'One such technique is **learning rate schedules**. Throughout the book, we
    have been working with constant learning rates. However, there are multiple advantages
    of using a dynamically adjusted learning rate. Some of them are as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '**Warmup**: When working with pre-trained models, it’s not advisable to start
    with a large learning rate. The initial epochs must be used for the gradients
    to start adjusting. This can be thought of as a way of *adjusting the model from
    the source task to the target task*, retaining and leveraging the knowledge from
    the previous task, so smaller learning rates are recommended.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decay**: In optimal training loops, as the model learns the expected representation
    of inputs to outputs, the objective of the training is to produce finer and finer
    improvements. Smaller learning rates achieve better performance at these stages
    (smaller and more stable weight updates). Therefore, a decaying learning rate
    is preferred after a few epochs.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dive into Deep Learning* provides great insights on how to implement these
    techniques in MXNet: [https://d2l.ai/chapter_optimization/lr-scheduler.html.](https://d2l.ai/chapter_optimization/lr-scheduler.html)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing training for translating text from English to German
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first recipe of this chapter, we saw how we could leverage MXNet and
    Gluon to optimize the training of our models, applying different techniques. We
    understood how to jointly use lazy evaluation and automatic parallelization for
    parallel processing and improved the performance of our DataLoaders by combining
    preprocessing in the CPU and GPU. We saw how using half-precision (`Float16`)
    in combination with AMP can halve our training times, and explored how to take
    advantage of multiple GPUs for further reduced training times.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can revisit a problem we have been working with throughout the book,
    that of **translating text from English to German**. We have worked with translation
    tasks in recipes in previous chapters. In the *Translating text from Vietnamese
    to English* recipe from [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), we introduced
    the task of translating text, while also learning how to use pre-trained models
    from GluonCV Model Zoo. Furthermore, in the *Improving performance for translating
    English to German* recipe from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    we introduced the datasets that we will be using in this recipe: *WMT2014* and
    *WMT2016*, and compared the different approaches that we could take when dealing
    with a target dataset, training our models from scratch or leveraging past knowledge
    from pre-trained models and adjusting it for our task, using the different modalities
    of transfer learning and fine-tuning.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this recipe, we will apply all these optimization techniques for
    the specific task of training an *English-to-German text* *translation model*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be difficult to understand at all.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will work through the following steps:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying training optimization techniques
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing the results
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s dive into each of these steps.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting our current preprocessing and training pipeline
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the *Improving performance for translating English to German* recipe from
    [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), we processed the data with the
    following approach:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Loaded the data from storage into CPU memory space
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessed the data using CPU
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used the default parameters to process the data during training
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was a valid approach to compare the different training alternatives available
    for us (training from scratch, pre-trained models, transfer learning, and fine-tuning)
    without adding complexity to the experiments. For example, this approach worked
    quite well to introduce and evaluate the technique of fine-tuning, which is the
    technique that we have selected to work with in this recipe.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the approach described earlier on the dataset selected for this recipe
    (*WMT2016*), the CPU-based preprocessing took the following amount of time:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Furthermore, when combined with the necessary step of reloading the data in
    batches and copying it to the GPU, we obtain the following performance:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In these conditions, the training process duration and performance achieved
    were as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As we can see, we got an excellent validation performance (~1.4) for a training
    time of ~3 hours.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of the training loss and the validation loss across each epoch
    looked as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Revisiting training: training loss versus validation loss](img/B16591_08_6.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6 – Revisiting training: training loss versus validation loss'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.6*, we can see the evolution of the training and validation
    loss. As explored throughout the chapters, we select the model that provide the
    minimal validation loss (in this case, it was achieved in the first epoch, epoch
    1).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training is completed, we can verify the overall performance in the
    test split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (10 in this case).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we can also check how well our model is performing
    by testing it with an example sentence. In our case, we chose `I learn new things
    every day`, and the output obtained is as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The German sentence obtained in the output (`Immer wieder erfährt ich Neues`)
    means `I'm always learning new things`, and therefore, as can be seen from the
    results, the text has been almost perfectly translated from English to German.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Applying training optimization techniques
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the *Introducing training optimization features* recipe at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take when training a machine learning
    model, including preprocessing the data and training and evaluating the model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will show how, with MXNet and Gluon and just a few lines
    of code, we can easily apply all of the techniques we’ve been introduced to.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the first recipe of this chapter, MXNet applies by default the best
    policy (`ThreadedEnginePerDevice`) to optimize lazy evaluation and automatic parallelization,
    taking into account the number of CPU threads available, so there is no need for
    us to apply any changes here (please note that this technique is also applied
    automatically when working with multiple GPUs).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'We have shown how we could optimize our data preprocessing pipeline by combining
    the usage of CPU threads and GPUs, taking into account the number of devices available
    for each and optimizing accordingly. For this experiment, specific HW was chosen
    with the following characteristics:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In order to apply this optimization technique, we had to apply some changes
    to our code. Specifically, we defined the GPUs available for use:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Furthermore, in our preprocessing pipeline, we now need a specific step that
    takes the data from the CPU memory space and copies it to the GPU memory space:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: As discussed in the first recipe of this chapter, in a typical production-oriented
    environment, we do not want to keep the data in the GPU as it occupies precious
    GPU memory. It is usual to optimize the batch size for the GPU memory available,
    and to load the data from the CPU memory space into the GPU memory space in batches
    using MXNet Gluon DataLoaders. Therefore, for our GPU-based preprocessing pipeline
    to be complete, we need a final step to copy the data back into the CPU memory
    space. As introduced in the *Improving performance for translating English to
    German* recipe from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), we are using
    the `ShardedDataLoader` class from MXNet `GluonNLP` library. This class performs
    that data transfer back to the CPU memory space automatically.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: However, as will be seen in our experiments, when working with multiple GPUs,
    performance is better when working directly with MXNet Gluon DataLoaders, as these
    are designed to be parallelized optimally afterward.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'With these code changes, our optimal preprocessing pipeline is ready, and we
    can continue with the next optimization technique: applying `Float16` optimizations,
    including AMP.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the first recipe of this chapter, in order to enable this technique,
    we just need a few changes in our code. First of all, we initialize the library:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Secondly, we attach the trainer/optimizer to the library:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the previous recipe, when dealing with images, we described how, due to the
    risk of gradients over/under-flowing, there was a need to adjust (scale) the loss
    accordingly. This is not necessary for our use case; therefore, we do not apply
    **loss** **scaling** here.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: With these two simple changes, we have updated our training loop to work efficiently
    with the `Float16` data type (when appropriate).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can apply our next and last training optimization technique: working
    with multiple GPUs.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: As we will see, working with multiple GPUs optimally implies working with them
    in parallel, and therefore, computing losses and executing the training backward
    pass in parallel, yielding the losses list described in the previous paragraph.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to work with multiple GPUs in parallel, we need to define the new
    context as a list (seen before for preprocessing, and shown here again for convenience):'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'As we now have multiple GPUs, we can increase our batch size to optimally use
    the available GPU memory space:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Furthermore, when reading from Gluon DataLoaders, we need to split the batches
    of data across the GPUs. Thankfully, Gluon also provides a function that simplifies
    that action. We just need the following lines of code to be added (for each training
    and validation batch):'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As mentioned, this split across the GPUs allows us to compute in parallel the
    model outputs and the losses associated with those outputs (a measure of the difference
    between the actual outputs and the expected outputs). This can be achieved with
    the following lines of code:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Typically, in order to finalize our updates to work with multiple GPUs in the
    training loop, we would need to apply further changes to our loss scaling. However,
    as discussed, for our use case, this is not necessary.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: With these minimal code changes, we now have an optimal preprocessing and training
    pipeline, and we can run the required experiments to analyze the performance changes.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the results
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we revisited the previous performance of our preprocessing
    and training pipelines and reviewed how we had to apply the necessary changes
    for our training optimization techniques, specifically for our task of translating
    text from English to German.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Our preprocessing pipeline steps are now the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Load the data from storage into the CPU memory space.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data using the GPU (although as we will see, we will change this
    to the CPU).
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy back the data to the CPU memory space (won’t be necessary).
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the optimized parameters to process the data during training.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our experiments, we are going to use the technique of fine-tuning directly.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the aforementioned approach on the dataset selected for this recipe
    (*WMT2016*), the GPU-based preprocessing took the following amount of time:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'An end-to-end preprocessing pipeline must take into account the process of
    batching using the Gluon DataLoader to load the data (in our case, into multiple
    GPUs), giving us the following performance:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Compared to the initial section of this recipe (where the preprocessing took
    27 seconds), we can see how, in this case, preprocessing in the GPU has not been
    effective. This is due to the nature of the text data, which is not as straightforward
    to parallelize as it is with images, for example.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, a CPU-based preprocessing pipeline is best, avoiding the
    Gluon NLP`ShardedDataLoader` class and using the `Gluon DataLoader` class instead
    (which is better suited for parallelizing). Applying this pipeline, we get the
    following results:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This gives us a minimal edge (2 seconds), but, as mentioned, this is the best
    we can get with the usage of Gluon DataLoader and its parallelization capabilities.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'After the preprocessing, the next step is the training process. As described,
    we will evaluate the effect of our training optimizations using the technique
    of fine-tuning directly. In combination with this approach, we use the following
    hyperparameters:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Please note how by adding multiple GPUs to the training process, we can increase
    the batch size (multiplied by the number of GPUs), and we can also increase the
    learning rate (from 0.00003 to 0.0001). In these conditions, the training process
    duration and achieved performance is as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we can see, we got excellent validation performance (~1.4) with training
    that took ~3 hours. When compared to the results obtained in the initial section
    of this recipe, we can see how there was a minimal decrease in the loss (a positive
    change, which we will confirm with our performance analysis shortly), but the
    largest improvement by far has been a 5.5x decrease in the training time. This
    improvement is due to all the training optimization techniques that we have applied.
    In a nutshell, each of the optimizations provided the following improvements:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '**Using 4 GPUs**: Provided a 4x decrease (as expected).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Float16` without compromising algorithmic performance.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing the datasets**: In this case, there were negligible improvements.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evolution of the training loss and the validation loss across each epoch
    looked as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Optimized training: training loss versus validation loss](img/B16591_08_7.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7 – Optimized training: training loss versus validation loss'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.7*, we can see the evolution of the training and validation
    losses. As explored throughout the chapters, we select the model that provided
    the minimal validation loss (in this case, achieved in the first epoch).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'After training is completed, we can verify the overall performance in the test
    split of our dataset. From a quantitative point of view, these are the results
    we obtained:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: As expected, we got excellent results just by training for a limited number
    of epochs (5 in this case). We can also confirm how the minimal improvement in
    the validation loss provided a minimal improvement in our test metrics (compared
    to 27.05 as initially obtained).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we can also check how well our model is performing
    by testing it with an example sentence. In our case, we chose `I learn new things
    every day`, and the output obtained is as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The German sentence obtained in the output (`Ich lerne jedes Mal Neues`) means
    `I learn something new every time`, and therefore, as can be seen from the results,
    the text has been almost perfectly translated from English to German.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we applied the different training optimization techniques seen
    in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address
    each of the steps in the training loop:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: We revisited how lazy evaluation and automatic parallelization mechanisms work
    together to optimize all MXNet-based flows.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We leveraged all our CPU threads to load data and tested to optimize that process
    further via preprocessing in the GPU. In this case, it was shown how a CPU-based
    preprocessing pipeline in combination with Gluon DataLoader was the optimal approach.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyzed different data types and combined the accuracy and precision of
    `Float32` with the speed-ups of `Float16`, and where possible, AMP.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We increased the performance of our training loops by using multiple GPUs (assuming
    our HW has these devices available).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compared each of these scenarios applied specifically to the task of *translating
    text from English to German*, running two experiments. In the first experiment,
    we did not apply any of the training optimization techniques described, following
    the approaches seen in previous chapters of the book. In the second experiment,
    we applied all the techniques in parallel, trying to optimize as much as we could.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: This proved quite useful, delivering similar algorithmic performance, with a
    5.5x improvement in training time (from 3 hours to 30 minutes). This was mostly
    due to using multiple GPUs (4x decrease) and leveraging `Float16` and AMP (1.4x
    decrease), whereas the optimized preprocessing provided negligible improvements.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have described, implemented, executed, and evaluated several training optimization
    techniques. However, there are even more advanced techniques that can be leveraged
    to achieve the optimal training loop.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: One such technique is **Reinforcement Learning from Human Feedback** (**RLHF**),
    where a *human-in-the-loop* process is introduced. In this process, after a model
    has been trained, a person is presented with different options for output by the
    model (for example, different potential translations) and they rank those responses
    according to how they better represent the original sentence. These human inputs
    are then used to train a reward model that scores the output of the model and
    selects the one with the highest score. This technique has been proven to be extremely
    powerful. As an example, **OpenAI** developed **ChatGPT** on top of the **GPT-3**
    language model using RLHF.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about *ChatGPT* and *RLHF*, the following article is recommended:
    [https://huyenchip.com/2023/05/02/rlhf.html](https://huyenchip.com/2023/05/02/rlhf.html).'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
