["```py\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(randomForest)\n```", "```py\nfm <- readr::read_csv('fashionmnist/fashion-mnist_train.csv')\nfm_test <- readr::read_csv('fashionmnist/fashion-mnist_test.csv')\n```", "```py\nset.seed(0)\n\nrf_model <- randomForest::randomForest(as.factor(label)~.,\ndata = fm,\nntree=10,\nmtry=5)\n```", "```py\npred <- predict(rf_model, fm_test, type=\"response\")\n\ncaret::confusionMatrix(as.factor(fm_test$label), pred)\n\n# Accuracy : 0.8457\n```", "```py\nlibrary(neuralnet)\nlibrary(Metrics)\n```", "```py\nfm <- fm %>% dplyr::filter(label < 2)\n\nfm_test <- fm_test %>% dplyr::filter(label < 2)\n```", "```py\ntest_label <- fm_test$label\n\nfm_test <- fm_test %>% dplyr::select(-label)\n```", "```py\nn <- names(fm)\nformula <- as.formula(paste(\"label ~\", paste(n[!n == \"label\"], collapse = \" + \", sep = \"\")))\n```", "```py\nset.seed(0)\n\nnet <- neuralnet::neuralnet(formula,\n                            data = fm,\n                            hidden = 250,\n                            linear.output = FALSE,\n                            act.fct = \"logistic\"\n)\n```", "```py\nprediction_list <- neuralnet::compute(net, fm_test)\npredictions <- as.vector(prediction_list$net.result)\n\nMetrics::auc(test_label, predictions)\n```", "```py\nlibrary(keras)\n\nfashion_mnist <- dataset_fashion_mnist()\n```", "```py\ntrain <- fashion_mnist$train$x\ntrain_target <- fashion_mnist$train$y\n\ntest <- fashion_mnist$test$x\ntest_target <- fashion_mnist$test$y\n```", "```py\ntrain <- normalize(train)\ntest <- normalize(test)\n\nrange(train)\n```", "```py\nset.seed(0)\n\nmodel <- keras_model_sequential()\n```", "```py\nmodel %>%\n  layer_flatten(input_shape = c(28, 28))\n```", "```py\nmodel %>%\n  layer_dense(units = 256, activation = 'relu') \n```", "```py\nmodel %>%\n  layer_dense(units = 10, activation = 'softmax')\n```", "```py\ntest_target <- to_categorical(test_target)\ntrain_target <- to_categorical(train_target)\n```", "```py\nmodel %>% compile(\n  optimizer = 'adam',   \n  loss = 'categorical_crossentropy', \n  metrics = 'categorical_accuracy'   \n)\n```", "```py\nmodel %>% fit(train, train_target, epochs = 10\n```", "```py\nscore <- model %>% evaluate(test, test_target)\n\nscore$categorical_accuracy\n```", "```py\npreds <- model %>% predict(test)\n\npredicted_classes <- model %>% predict_classes(test)\n```", "```py\ntest_target_vector <- fashion_mnist$test$y\n\ncaret::confusionMatrix(as.factor(predicted_classes),as.factor(test_target_vector))\n```", "```py\ndim(train) <- c(nrow(train), 28, 28, 1) \ndim(test) <- c(nrow(test), 28, 28, 1) \n```", "```py\nset.seed(0)\n\nmodel <- keras_model_sequential()\n```", "```py\nmodel %>%\n  layer_conv_2d(filters = 484, kernel_size = c(7,7), activation = 'relu',\n                activation = 'relu', input_shape = c(28,28,1))\n```", "```py\nmodel %>% layer_max_pooling_2d(pool_size = c(2, 2))\n```", "```py\nmodel %>% \nlayer_flatten() %>%\n  layer_dense(units = 98, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = 'accuracy'\n)\n\nmodel %>% fit(\n  train, train_target,\n  batch_size = 100,\n  epochs = 5,\n  verbose = 1,\n  validation_data = list(test, test_target)\n)\n\nscores <- model %>% evaluate(\n  test, test_target, verbose = 1\n)\n\npreds <- model %>% predict(test)\n\npredicted_classes <- model %>% predict_classes(test)\n\ncaret::confusionMatrix(as.factor(predicted_classes),as.factor(test_target_vector))\n```", "```py\nset.seed(0)\n\nmodel <- keras_model_sequential()\n```", "```py\nmodel %>%\n  layer_conv_2d(filters = 128, kernel_size = c(7,7), activation = 'relu',\n                input_shape = c(28,28,1), padding = \"same\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) \n```", "```py\nmodel %>%\n  layer_conv_2d(filters = 64, kernel_size = c(7,7), activation = 'relu', padding = \"same\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2))\n```", "```py\nmodel %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7,7), activation = 'relu', padding = \"same\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2))\n```", "```py\nmodel %>%\nlayer_flatten() %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = 'accuracy'\n)\n\nmodel %>% fit(\n  train, train_target,\n  batch_size = 100,\n  epochs = 5,\n  verbose = 1,\n  validation_data = list(test, test_target)\n)\n\nscore <- model %>% evaluate(\n  test, test_target, verbose = 1\n)\n\npreds <- model %>% predict(test)\n\npredicted_classes <- model %>% predict_classes(test)\n\ncaret::confusionMatrix(as.factor(predicted_classes),as.factor(test_target_vector))\n```", "```py\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filters = 8, kernel_size = c(3,3), input_shape = c(28,28,1)) %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 16, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_flatten() %>%\n  layer_dense(units = 128) %>%\n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 10, activation = 'softmax')\n\n# compile model\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n# train and evaluate\nmodel %>% fit(\n  train, train_target,\n  batch_size = 100,\n  epochs = 5,\n  verbose = 1,\n  validation_data = list(test, test_target)\n)\nscores <- model %>% evaluate(\n  test, test_target, verbose = 1\n)\npreds <- model %>% predict(test)\npredicted_classes <- model %>% predict_classes(test)\ncaret::confusionMatrix(as.factor(predicted_classes),as.factor(test_target_vector))\n```", "```py\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filters = 128, kernel_size = c(7,7), input_shape = c(28,28,1), padding = \"same\") %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(7,7), padding = \"same\") %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7,7), padding = \"same\") %>%\n  layer_activation_leaky_relu() %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_flatten() %>%\n  layer_dense(units = 128) %>%\n  layer_activation_leaky_relu() %>% \n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\n# compile model\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = 'accuracy'\n)\n# train and evaluate\nmodel %>% fit(\n  train, train_target,\n  batch_size = 100,\n  epochs = 5,\n  verbose = 1,\n  validation_data = list(test, test_target)\n)\nscores <- model %>% evaluate(\n  test, test_target, verbose = 1\n)\npreds <- model %>% predict(test)\npredicted_classes <- model %>% predict_classes(test)\ncaret::confusionMatrix(as.factor(predicted_classes),as.factor(test_target_vector))\n```", "```py\nmodel %>% fit(\n  train, train_target,\n  batch_size = 100,\n  epochs = 5,\n  verbose = 1,\n  validation_data = list(test, test_target),\n  callbacks = list(\n    callback_early_stopping(patience = 2)\n  )\n)\n```"]