<html><head></head><body>
		<div id="_idContainer102">
			<h1 id="_idParaDest-163" class="chapter-number"><a id="_idTextAnchor184"/>10</h1>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor185"/>Machine Learning Part 2 – Neural Networks and Deep Learning Techniques</h1>
			<p><strong class="bold">Neural networks</strong> (<strong class="bold">NNs</strong>) have only became popular in <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) around 2010 but <a id="_idIndexMarker757"/>have since been widely applied to many <a id="_idIndexMarker758"/>problems. In addition, there are many applications of NNs to non-<strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) problems such as image classification. The fact that NNs are a general approach that can be applied across different research areas has led to some interesting synergies across <span class="No-Break">these fields.</span></p>
			<p>In this chapter, we will <a id="_idIndexMarker759"/>cover the application of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) techniques based on NNs to problems such as NLP classification. We will also cover several <a id="_idIndexMarker760"/>different kinds of commonly used NNs—specifically, fully <a id="_idIndexMarker761"/>connected <strong class="bold">multilayer perceptrons</strong> (<strong class="bold">MLPs</strong>), <strong class="bold">convolutional NNs</strong> (<strong class="bold">CNNs</strong>), and <strong class="bold">recurrent NNs</strong> (<strong class="bold">RNNs</strong>)—and show how they <a id="_idIndexMarker762"/>can be applied to problems such as classification and information extraction. We will also discuss fundamental NN concepts such as hyperparameters, learning rate, activation functions, and epochs. We will illustrate NN concepts with a classification example using the <span class="No-Break">TensorFlow/Keras libraries.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Basics <span class="No-Break">of NNs</span></li>
				<li>Example—MLP <span class="No-Break">for classification</span></li>
				<li>Hyperparameters <span class="No-Break">and tuning</span></li>
				<li>Moving <span class="No-Break">beyond MLPs—RNNs</span></li>
				<li>Looking at <span class="No-Break">another approach—CNNs</span></li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor186"/>Basics of NNs</h1>
			<p>The basic concepts behind NNs have been studied for many years but have only fairly recently been applied <a id="_idIndexMarker763"/>to NLP problems on a large scale. Currently, NNs are one of the most popular tools for solving NLP tasks. NNs are a large field and are very actively researched, so we won’t be able to give you a comprehensive understanding of NNs for NLP. However, we will attempt to provide you with some basic knowledge that will let you apply NNs to your <span class="No-Break">own problems.</span></p>
			<p>NNs are inspired by some properties of the animal nervous system. Specifically, animal nervous systems consist of a network of interconnected cells, called <em class="italic">neurons</em>, that transmit information throughout the <a id="_idIndexMarker764"/>network with the result that, given an input, the network produces an output that represents a decision about <span class="No-Break">the input.</span></p>
			<p><strong class="bold">Artificial NNs</strong> (<strong class="bold">ANNs</strong>) are designed to model this process in some respects. The decision about how to react <a id="_idIndexMarker765"/>to the inputs is determined by a sequence of processing steps starting with units (<em class="italic">neurons</em>) that receive inputs and create outputs (or <em class="italic">fire</em>) if the correct conditions are met. When a neuron fires, it sends its output to other neurons. These next neurons receive inputs from a number of other neurons, and they in turn fire if they receive the right inputs. Part of the decision process about whether to fire involves <em class="italic">weights</em> on the neurons. The way that the NN learns to do its task—that is, the <em class="italic">training</em> process—is the process of adjusting the weights to produce the best <a id="_idIndexMarker766"/>results on the <span class="No-Break">training data.</span></p>
			<p>The training process consists of a set of <em class="italic">epochs</em>, or passes through the training data, adjusting the weights <a id="_idIndexMarker767"/>on each pass to try to reduce the discrepancy between <a id="_idIndexMarker768"/>the result produced by the NN and the <span class="No-Break">correct results.</span></p>
			<p>The neurons in an NN are arranged in a series of layers, with the final layer—the output layer—producing the decision. Applying these concepts to NLP, we will start with an input text that is fed to the input layer, which represents the input being processed. Processing proceeds through all the layers, continuing through the NN until it reaches the output layer, which provides the decision—for example, is this movie review positive <span class="No-Break">or negative?</span></p>
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> represents a schematic diagram of an NN with an input layer, two hidden layers, and an output layer. The NN in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> is a <strong class="bold">fully connected NN</strong> (<strong class="bold">FCNN</strong>) because every neuron <a id="_idIndexMarker769"/>receives inputs from every neuron in the preceding layer and sends outputs to every neuron in the <span class="No-Break">following layer:</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B19005_10_01.jpg" alt="Figure 10.1 – A﻿n FCNN with two hidden layers"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – An FCNN with two hidden layers</p>
			<p>The field of NNs uses a lot of specialized vocabulary, which can sometimes make it difficult to read <a id="_idIndexMarker770"/>documentation on the topic. In the following list, we’ll provide a brief introduction to some of the most important concepts, referring to <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> <span class="No-Break">as needed:</span></p>
			<ul>
				<li><strong class="bold">Activation function</strong>: The activation function is the function that determines when a neuron has <a id="_idIndexMarker771"/>enough inputs to fire <a id="_idIndexMarker772"/>and transmit its output to the neurons in the next layer. Some <a id="_idIndexMarker773"/>common activation functions are sigmoid and <strong class="bold">rectified linear </strong><span class="No-Break"><strong class="bold">unit</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ReLU</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Backpropagation</strong>: The process <a id="_idIndexMarker774"/>of training an NN where the loss is fed back through the network to train <span class="No-Break">the weights.</span></li>
				<li><strong class="bold">Batch</strong>: A batch is a set <a id="_idIndexMarker775"/>of samples that will be <span class="No-Break">trained </span><span class="No-Break"><a id="_idIndexMarker776"/></span><span class="No-Break">together.</span></li>
				<li><strong class="bold">Connection</strong>: A link between <a id="_idIndexMarker777"/>neurons, associated with a weight that represents the strength of the connection. The lines between neurons in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> <span class="No-Break">are connections.</span></li>
				<li><strong class="bold">Convergence</strong>: A network <a id="_idIndexMarker778"/>has converged when additional epochs do not appear to produce any reduction in the loss or improvements <span class="No-Break">in accuracy.</span></li>
				<li><strong class="bold">Dropout</strong>: A technique for <a id="_idIndexMarker779"/>preventing overfitting by randomly <span class="No-Break">removing neurons.</span></li>
				<li><strong class="bold">Early stopping</strong>: Ending training <a id="_idIndexMarker780"/>before the planned number of epochs because training appears to <span class="No-Break">have converged.</span></li>
				<li><strong class="bold">Epoch</strong>: One pass <a id="_idIndexMarker781"/>through the training data, adjusting the weights to <span class="No-Break">minimize loss.</span></li>
				<li><strong class="bold">Error</strong>: The difference <a id="_idIndexMarker782"/>between the predictions produced by an NN and the reference labels. Measures how well the network predicts the classification of <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Exploding gradients</strong>: Exploding gradients occur when gradients become unmanageably <a id="_idIndexMarker783"/>large <span class="No-Break">during training.</span></li>
				<li><strong class="bold">Forward propagation</strong>: Propagation <a id="_idIndexMarker784"/>of inputs <a id="_idIndexMarker785"/>forward through an NN from the input layer through the hidden layers to the <span class="No-Break">output layer.</span></li>
				<li><strong class="bold">Fully connected</strong>: An <a id="_idIndexMarker786"/>FCNN is <a id="_idIndexMarker787"/>an NN “<em class="italic">with every neuron in one layer connecting to every neuron in the next layer</em>” (<a href="https://en.wikipedia.org/wiki/Artificial_neural_network">https://en.wikipedia.org/wiki/Artificial_neural_network</a>), as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Gradient descent</strong>: Optimizing <a id="_idIndexMarker788"/>weights by adjusting them in a direction that will <span class="No-Break">minimize loss.</span></li>
				<li><strong class="bold">Hidden layer</strong>: A layer of <a id="_idIndexMarker789"/>neurons that is not the input or <span class="No-Break">output layer.</span></li>
				<li><strong class="bold">Hyperparameters</strong>: Parameters that <a id="_idIndexMarker790"/>are not learned and are usually adjusted in a manual tuning process in order for the network to produce <span class="No-Break">optimal results.</span></li>
				<li><strong class="bold">Input layer</strong>: The layer in an NN <a id="_idIndexMarker791"/>that receives the initial data. This is the layer on the left in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Layer</strong>: A set of neurons in <a id="_idIndexMarker792"/>an NN that takes information from the previous layer and passes it on to the next layer. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> includes <span class="No-Break">four layers.</span></li>
				<li><strong class="bold">Learning</strong>: Assigning weights <a id="_idIndexMarker793"/>to connections in the training process in order to <span class="No-Break">minimize loss.</span></li>
				<li><strong class="bold">Learning rate/adaptive learning rate</strong>: Amount of adjustment to the weights after each epoch. In <a id="_idIndexMarker794"/>some approaches, the learning rate can adapt as the training progresses; for example, if learning starts to slow, it can be useful to decrease the <span class="No-Break">learning rate.</span></li>
				<li><strong class="bold">Loss</strong>: A function that provides a metric that quantifies the distance between the current <a id="_idIndexMarker795"/>model’s predictions and the goal values. The training process attempts to <span class="No-Break">minimize loss.</span></li>
				<li><strong class="bold">MLP</strong>: As <a id="_idIndexMarker796"/>described on <em class="italic">Wikipedia</em>, “<em class="italic">a fully connected class of feedforward artificial neural network (ANN). An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer</em>” (<a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">https://en.wikipedia.org/wiki/Multilayer_perceptron</a>). <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> shows an example of <span class="No-Break">an MLP.</span></li>
				<li><strong class="bold">Neuron (unit)</strong>: A unit in an NN <a id="_idIndexMarker797"/>that receives inputs and computes outputs by applying an <span class="No-Break">activation function.</span></li>
				<li><strong class="bold">Optimization</strong>: Adjustment to the <a id="_idIndexMarker798"/>learning rate <span class="No-Break">during training.</span></li>
				<li><strong class="bold">Output layer</strong>: The final layer <a id="_idIndexMarker799"/>in an NN that produces a decision about the input. This is the layer on the right in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Overfitting</strong>: Tuning the <a id="_idIndexMarker800"/>network too closely to the training data so that it does not generalize to previously unseen test or <span class="No-Break">validation data.</span></li>
				<li><strong class="bold">Underfitting</strong>: Underfitting <a id="_idIndexMarker801"/>occurs when an NN is unable to obtain good accuracy for training data. It can be addressed by using more training epochs or <span class="No-Break">more layers.</span></li>
				<li><strong class="bold">Vanishing gradients</strong>: Gradients <a id="_idIndexMarker802"/>that become so small that the network is unable to <span class="No-Break">make progress.</span></li>
				<li><strong class="bold">Weights</strong>: A property of the <a id="_idIndexMarker803"/>connection between neurons that represents the strength of the connection. Weights are learned <span class="No-Break">during training.</span></li>
			</ul>
			<p>In the next section, we <a id="_idIndexMarker804"/>will make these concepts concrete by going through an example of text classification with a <span class="No-Break">basic MLP.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor187"/>Example – MLP for classification</h1>
			<p>We will review <a id="_idIndexMarker805"/>basic NN concepts by looking at the MLP, which is conceptually one of the most straightforward types of NNs. The example we will use is the classification of movie reviews into reviews with positive and negative sentiments. Since there are only two possible categories, this is a <em class="italic">binary</em> classification problem. We will use the <em class="italic">Sentiment Labelled Sentences Data Set</em> (<em class="italic">From Group to Individual Labels using Deep Features</em>, <em class="italic">Kotzias et al.</em>, <em class="italic">KDD 2015 </em><a href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a>), avail<a id="_idTextAnchor188"/>able from the University of California, Irvine. Start by downloading the data and unzipping it into a directory in the same directory as your Python script. You will see a directory called <strong class="source-inline">sentiment labeled sentences</strong> that contains the actual data in a file called <strong class="source-inline">imdb_labeled.txt</strong>. You can install the data into another directory of your choosing, but if you do, be sure to modify the <strong class="source-inline">filepath_dict</strong> <span class="No-Break">variable accordingly.</span></p>
			<p>You can take <a id="_idIndexMarker806"/>a look at the data using the following <span class="No-Break">Python code:</span></p>
			<pre class="source-code">
import pandas as pd
import os
filepath_dict = {'imdb':   'sentiment labelled sentences/imdb_labelled.txt'}
document_list = []
for source, filepath in filepath_dict.items():
    document = pd.read_csv(filepath, names=['sentence', 'label'], sep='\t')
    document['source'] = source
    document_list.append(document)
document = pd.concat(document_list)
print(document.iloc[0])</pre>
			<p>The output from the last <strong class="source-inline">print</strong> statement will include the first sentence in the corpus, its label (<strong class="source-inline">1</strong> or <strong class="source-inline">0</strong>—that is, positive or negative), and its source (<strong class="source-inline">Internet Movie </strong><span class="No-Break"><strong class="source-inline">Database IMDB</strong></span><span class="No-Break">).</span></p>
			<p>In this example, we will <a id="_idIndexMarker807"/>vectorize the corpus using the scikit-learn count <strong class="bold">bag of words</strong> (<strong class="bold">BoW</strong>) vectorizer (<strong class="source-inline">CountVectorizer</strong>), which we saw earlier in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>The following code snippet shows the start of the vectorization process, where we set up some parameters for <span class="No-Break">the vectorizer:</span></p>
			<pre class="source-code">
from sklearn.feature_extraction.text import CountVectorizer
# min_df is the minimum proportion of documents that contain the word (excludes words that
# are rarer than this proportion)
# max_df is the maximum proportion of documents that contain the word (excludes words that
# are rarer than this proportion
# max_features is the maximum number of words that will be considered
# the documents will be lowercased
vectorizer = CountVectorizer(min_df = 0, max_df = 1.0, max_features = 1000, lowercase = True)</pre>
			<p>The <strong class="source-inline">CountVectorizer</strong> function has some useful parameters that control the maximum number <a id="_idIndexMarker808"/>of words that will be used to build the model, as well as make it possible to exclude words that are considered to be too frequent or too rare to be very useful in <span class="No-Break">distinguishing documents.</span></p>
			<p>The next step is to do the train-test split, as shown in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
# split the data into training and test
from sklearn.model_selection import train_test_split
document_imdb = document[document['source'] == 'imdb']
reviews = document_imdb['sentence'].values
y = document_imdb['label'].values
# since this is just an example, we will omit the dev test set
# 'reviews.data' is the movie reviews
# 'y_train' is the categories assigned to each review in the training data
# 'test_size = .20' is the proportion of the data that should be reserved for testing
# 'random_state = 42' is an integer that controls the randomization of the data so that the results are reproducible
reviews_train, reviews_test, y_train, y_test = train_test_split(
   reviews, y, test_size = 0.20, random_state = 42)</pre>
			<p>The preceding code shows the splitting of the training and test data, reserving 20% of the total data <span class="No-Break">for testing.</span></p>
			<p>The <strong class="source-inline">reviews</strong> variable <a id="_idIndexMarker809"/>holds the actual documents, and the <strong class="source-inline">y</strong> variable holds their labels. Note that <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> are frequently used in the literature to represent the data and the categories in an ML problem, respectively, although we’re using <strong class="source-inline">reviews</strong> for the <strong class="source-inline">X</strong> <span class="No-Break">data here:</span></p>
			<pre class="source-code">
vectorizer.fit(reviews_train)
vectorizer.fit(reviews_test)
X_train = vectorizer.transform(reviews_train)
X_test  = vectorizer.transform(reviews_test)</pre>
			<p>The preceding code shows the process of vectorizing the data, or converting each document to a numerical representation, using the vectorizer that was defined previously. You can review vectorization by going back to <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>The result is <strong class="source-inline">X_train</strong>, the count BoW of the dataset. You will recall the count BoW from <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>The next step is to set up the NN. We will be using the Keras package, which is built on top of Google’s TensorFlow ML package. Here’s the code we need <span class="No-Break">to execute:</span></p>
			<pre class="source-code">
from keras.models import Sequential
from keras import layers
from keras import models
# Number of features (words)
# This is based on the data and the parameters that were provided to the vectorizer
# min_df, max_df and max_features
input_dimension = X_train.shape[1]
print(input_dimension)</pre>
			<p>The code first prints the input dimension, which in this case is the number of words in each document <a id="_idIndexMarker810"/>vector. The input dimension is useful to know because it’s computed from the corpus, as well as the parameters we set in the <strong class="source-inline">CountVectorizer</strong> function. If it is unexpectedly large or small, we might want to change the parameters to make the vocabulary larger <span class="No-Break">or smaller.</span></p>
			<p>The following code defines <span class="No-Break">the model:</span></p>
			<pre class="source-code">
# a Sequential model is a stack of layers where each layer has one input and one output tensor
# Since this is a binary classification problem, there will be one output (0 or 1)
# depending on whether the review is positive or negative
# so the Sequential model is appropriate
model = Sequential()
model.add(layers.Dense(16, input_dim = input_dimension, activation = 'relu'))
model.add(layers.Dense(16, activation = 'relu'))
model.add(layers.Dense(16, activation = 'relu'))
# output layer
model.add(layers.Dense(1, activation = 'sigmoid'))</pre>
			<p>The model built in the preceding code includes the input layer, two hidden layers, and one output layer. Each call to the <strong class="source-inline">model.add()</strong> method adds a new layer to the model. All the layers are dense because, in this fully connected network, every neuron receives inputs from every neuron in the previous layer, as illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em>. The 2 hidden layers each contain 16 neurons. Why do we specify 16 neurons? There is no hard and fast rule <a id="_idIndexMarker811"/>for how many neurons to include in the hidden layers, but a general approach would be to start with a smaller number since the training time will increase as the number of neurons increases. The final output layer will only have one neuron because we only want one output for this problem, whether the review is positive <span class="No-Break">or negative.</span></p>
			<p>Another very important parameter is the <strong class="bold">activation function</strong>. The activation function is the function that <a id="_idIndexMarker812"/>determines how the neuron responds to its inputs. For all of the layers in our example, except the output layer, this is the ReLU activation function. The ReLU function can be seen in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em>. ReLU is a very commonly used <span class="No-Break">activation function:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B19005_10_02.jpg" alt="Figure 10.2 – Values of the ReLU function for inputs between -15 and 15"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Values of the ReLU function for inputs between -15 and 15</p>
			<p>One of the most important benefits of the ReLU function is that it is very efficient. It has also turned out to <a id="_idIndexMarker813"/>generally give good results in practice and is normally a reasonable choice as an <span class="No-Break">activation function.</span></p>
			<p>The other activation function that’s used in this NN is the sigmoid function, which is used in the output layer. We use the sigmoid function <a id="_idIndexMarker814"/>here because in this problem we want to predict the probability of a positive or negative sentiment, and the <a id="_idIndexMarker815"/>value of the sigmoid function will always be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. The formula for the sigmoid function is shown in the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>A plot of the sigmoid function is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em>, and it is easy to see that its output value will always be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> regardless of the value of <span class="No-Break">the input:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B19005_10_03.jpg" alt="Figure 10.3 – Values of the sigmoid  function for inputs between -10 and 10"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Values of the sigmoid  function for inputs between -10 and 10</p>
			<p>The sigmoid and ReLU activation functions are popular and practical activation functions, but they are only two examples of the many possible NN activation functions. If you wish to <a id="_idIndexMarker816"/>investigate this topic further, the following <em class="italic">Wikipedia</em> article is a good place to <span class="No-Break">start: </span><a href="https://en.wikipedia.org/wiki/Activation_function"><span class="No-Break">https://en.wikipedia.org/wiki/Activation_function</span></a><span class="No-Break">.</span></p>
			<p>Once the model has been defined, we can compile it, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
model.compile(loss = 'binary_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])</pre>
			<p>The <strong class="source-inline">model.compile()</strong> method requires the <strong class="source-inline">loss</strong>, <strong class="source-inline">optimizer</strong>, and <strong class="source-inline">metrics</strong>  parameters, which <a id="_idIndexMarker817"/>supply the <span class="No-Break">following information:</span></p>
			<ul>
				<li>The <strong class="source-inline">loss</strong> parameter, in this <a id="_idIndexMarker818"/>case, tells the compiler to use <strong class="source-inline">binary_crossentropy</strong> to <a id="_idIndexMarker819"/>compute the loss. <strong class="bold">Binary cross-entropy</strong> is a commonly used loss function for binary problems such as binary classification. A similar function, <strong class="source-inline">categorical_crossentropy</strong>, is used for problems when there are two or more label classes in the output. For example, if the task were to assign a star rating to reviews, we might have five output classes corresponding to the five possible star ratings, and in that case, we would use <span class="No-Break">categorical cross-entropy.</span></li>
				<li>The <strong class="source-inline">optimizer</strong> parameter adjusts the learning rate during training. We will not go into <a id="_idIndexMarker820"/>the mathematical details of <strong class="source-inline">adam</strong> here, but generally speaking, the optimizer we use here, <strong class="source-inline">adam</strong>, normally turns out to be a <span class="No-Break">good choice.</span></li>
				<li>Finally, the <strong class="source-inline">metrics</strong> parameter tells the compiler how we will evaluate the quality of the model. We <a id="_idIndexMarker821"/>can include multiple metrics in this list, but we will just include <strong class="source-inline">accuracy</strong> for now. In practice, the metrics you use will depend on your problem and dataset, but <strong class="source-inline">accuracy</strong> is a good metric to use for the purposes of our example. In <a href="B19005_13.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, we will explore other metrics and the reasons that you might want to select them in particular situations. </li>
			</ul>
			<p>It is also helpful to display a summary of the model to make sure that the model is structured as intended. The <strong class="source-inline">model.summary()</strong> method will produce a summary of the model, as shown in the following <a id="_idIndexMarker822"/><span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 16)                13952
 dense_1 (Dense)             (None, 16)                272
 dense_2 (Dense)             (None, 16)                272
 dense_3 (Dense)             (None, 1)                 17
=================================================================
Total params: 14,513
Trainable params: 14,513
<strong class="bold">Non-trainable params: 0</strong></pre>
			<p>In this output, we can <a id="_idIndexMarker823"/>see that the network, consisting of four dense layers (which are the input layer, the two hidden layers, and the output layer), is structured <span class="No-Break">as expected.</span></p>
			<p>The final step is to fit or train the network, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
history = model.fit(X_train, y_train,
                    epochs=20,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=10)</pre>
			<p>Training is the iterative process of putting the training data through the network, measuring the loss, adjusting the weights to reduce the loss, and putting the training data through the network again. This step can be quite time-consuming, depending on the size of the dataset and the size of <span class="No-Break">the model.</span></p>
			<p>Each cycle through the training data is an epoch. The number of epochs in the training process is a <em class="italic">hyperparameter</em>, which means that it’s adjusted by the developer based on the training results. For <a id="_idIndexMarker824"/>example, if the network’s performance doesn’t seem to be improving after a certain number of epochs, the number of epochs can be reduced since additional epochs are not improving the result. Unfortunately, there is no set number of epochs after which we can stop training. We have to observe the improvements in accuracy and loss over epochs to decide whether the system is <span class="No-Break">sufficiently trained.</span></p>
			<p>Setting the <strong class="source-inline">verbose = True</strong> parameter is optional but useful because this will produce a trace of the results after each epoch. If the training process is long, the trace can help you verify that the <a id="_idIndexMarker825"/>training is making progress. The batch size is another hyperparameter that defines how many data samples are to be processed before updating the model. When the following Python code is executed, with <strong class="source-inline">verbose</strong> set to <strong class="source-inline">True</strong>, at the end of every epoch, the loss, the accuracy, and the validation loss and accuracy will be computed. After training is complete, the <strong class="source-inline">history</strong> variable will contain information about the progress of the training process, and we can see plots of the <span class="No-Break">training progress.</span></p>
			<p>It is important to display how the plots of accuracy and loss change with each epoch because it will give us an idea of how many epochs are needed to get this training to converge and will make it very clear when the data is overfitting. The following code shows how to plot the accuracy and loss changes <span class="No-Break">over epochs::</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
plt.style.use('ggplot')
def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training accuracy')
    plt.plot(x, val_acc, 'r', label = 'Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()
plot_history(history)</pre>
			<p>We can see the results <a id="_idIndexMarker826"/>of the progress of our example through training over 20 epochs in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B19005_10_04.jpg" alt="Figure 10.4 – Accuracy and loss over 20 epochs of training"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Accuracy and loss over 20 epochs of training</p>
			<p>Over the 20 epochs of training, we can see that the training accuracy approaches <strong class="bold">1.0</strong> and the training loss approaches <strong class="bold">0</strong>. However, this apparently good result is misleading because the really important results are based on the validation data. Because the validation data is not <a id="_idIndexMarker827"/>being used to train the network, it is the performance on the validation data that actually predicts how the network will perform in use. We can see from the plots of the changes in validation accuracy and loss that doing more training epochs after about 10 is not improving the model’s performance on the validation data. In fact, it is increasing the loss and therefore making the model worse. This is clear from the increase in the validation loss in the graph on the right in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
			<p>Improving performance on this task will involve modifying other factors, such as hyperparameters and other tuning processes, which we will go over in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor189"/>Hyperparameters and tuning</h1>
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em> clearly shows that increasing the number of training epochs is not going to improve performance <a id="_idIndexMarker828"/>on this task. The best <a id="_idIndexMarker829"/>validation accuracy seems to be about 80% after 10 epochs. However, 80% accuracy is not very good. How can we improve it? Here are some ideas. None of them is guaranteed to work, but it is worth experimenting <span class="No-Break">with them:</span></p>
			<ul>
				<li>If more training data is available, the amount of training data can <span class="No-Break">be increased.</span></li>
				<li>Preprocessing techniques that can remove noise from the training data can be investigated—for example, stopword removal, removing non-words such as numbers and HTML tags, stemming and lemmatization, and lowercasing. Details on these techniques were covered in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></li>
				<li>Changes to the learning rate—for example, lowering the learning rate might improve the ability of the network to avoid <span class="No-Break">local minima.</span></li>
				<li>Decreasing the <span class="No-Break">batch size.</span></li>
				<li>Changing the number of layers and the number of neurons in each layer is something that can be tried, but having too many layers is likely to lead <span class="No-Break">to overfitting.</span></li>
				<li>Adding dropout by specifying a hyperparameter that defines the probability that the outputs from a layer will be ignored. This can help make the network more robust <span class="No-Break">to overfitting.</span></li>
				<li>Improvements in <a id="_idIndexMarker830"/>vectorization—for example, by using <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>) instead of <span class="No-Break">count BoW.</span></li>
			</ul>
			<p>A final strategy for<a id="_idIndexMarker831"/> improving performance <a id="_idIndexMarker832"/>is to try some of the newer ideas in NNs—specifically, RNNs, CNNs, <span class="No-Break">and transformers.</span></p>
			<p>We will conclude this chapter by briefly reviewing RNNs and CNNs. We will cover transformers in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor190"/>Moving beyond MLPs – RNNs</h1>
			<p>RNNs are a type of NN that is able to take into account the order of items in an input. In the example of the <a id="_idIndexMarker833"/>MLP that was discussed previously, the vector representing the entire input (that is, the complete document) was fed to the NN at once, so the network had no way of taking into account the order of words in the document. However, this is clearly an oversimplification in the case of text data since the order of words can be very important to the meaning. RNNs are able to take into account the order <a id="_idIndexMarker834"/>of words by using earlier outputs as <a id="_idIndexMarker835"/>inputs to later layers. This can be especially helpful in <a id="_idIndexMarker836"/>certain NLP problems where the order of words is very important, such as <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>), <strong class="bold">part-of-speech (POS) tagging</strong>, or <span class="No-Break"><strong class="bold">slot labeling</strong></span><span class="No-Break">.</span></p>
			<p>A diagram of a unit of an RNN is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B19005_10_05.jpg" alt="Figure 10.5 – A unit of an RNN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – A unit of an RNN</p>
			<p>The unit is shown <a id="_idIndexMarker837"/>at time <em class="italic">t</em>. The input at time <em class="italic">t</em>, <em class="italic">x(t)</em>, is passed to the activation function as in the case of the MLP, but the activation function also receives the output from time <em class="italic">t-1</em>—that is, <em class="italic">x(t-1)</em>. For NLP, the earlier input would most likely have been the previous word. So, in this case, the input is the current word and one previous word. Using an RNN with Keras is very similar to the MLP example that we saw earlier, with the addition of a new RNN layer in the <span class="No-Break">layer stack.</span></p>
			<p>However, as the length of the input increases, the network will tend to <em class="italic">forget</em> information from earlier inputs, because the older information will have less and less influence <a id="_idIndexMarker838"/>over the current state. Various strategies have been <a id="_idIndexMarker839"/>designed to overcome this limitation, such as <strong class="bold">gated recurrent units</strong> (<strong class="bold">GRUs</strong>) and <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>). If the input is a complete text document (as opposed to speech), we have access not only to previous inputs but also to future inputs, and a bidirectional RNN can <span class="No-Break">be used.</span></p>
			<p>We will not <a id="_idIndexMarker840"/>cover these additional variations of RNNs here, but they do often improve performance on some tasks, and it would be worth researching them. Although there is a tremendous amount of resources available on this popular topic, the <a id="_idIndexMarker841"/>following <em class="italic">Wikipedia</em> article is a good place to <span class="No-Break">start: </span><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"><span class="No-Break">https://en.wikipedia.org/wiki/Recurrent_neural_network</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor191"/>Looking at another approach – CNNs</h1>
			<p>CNNs are very <a id="_idIndexMarker842"/>popular for image recognition tasks, but they are less often used for NLP tasks than RNNs because they don’t take into account the temporal order of items in the input. However, they can be useful for document classification tasks. As you will recall from earlier chapters, the representations that are often used in classification depend only on the words that occur in the document—BoW and TF-IDF, for example—so, effective classification can often be accomplished without taking word order <span class="No-Break">into account.</span></p>
			<p>To classify documents with CNNs, we can represent a text as an array of vectors, where each word is mapped to a vector in a space made up of the full vocabulary. We can use word2vec, which we discussed in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, to represent word vectors. Training a CNN for text classification with Keras is very similar to the training process that we worked through in MLP classification. We create a sequential model as we did earlier, but we add new convolutional layers and <span class="No-Break">pooling layers.</span></p>
			<p>We will not cover the details of using CNNs for classification, but they are another option for NLP classification. As in the case of RNNs, there are many available resources on this <a id="_idIndexMarker843"/>topic, and a good starting point is <span class="No-Break"><em class="italic">Wikipedia</em></span><span class="No-Break"> (</span><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"><span class="No-Break">https://en.wikipedia.org/wiki/Convolutional_neural_network</span></a><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor192"/>Summary</h1>
			<p>In this chapter, we have explored applications of NNs to document classification in NLP. We covered the basic concepts of NNs, reviewed a simple MLP, and applied it to a binary classification problem. We also provided some suggestions for improving performance by modifying hyperparameters and tuning. Finally, we discussed the more advanced types of NNs—RNNs <span class="No-Break">and CNNs.</span></p>
			<p>In <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, we will cover the currently best-performing techniques in NLP—transformers and <span class="No-Break">pretrained models.</span></p>
		</div>
	</body></html>