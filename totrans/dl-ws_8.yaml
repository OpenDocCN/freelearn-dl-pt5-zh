- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Building Blocks of Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 1.01: Solving a Quadratic Equation Using an Optimizer'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s solve the following quadratic equation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.29: Quadratic equation to be solved'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_01_29.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.29: Quadratic equation to be solved'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: We already know that the solution to this quadratic equation is `x=5`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use an optimizer to solve this. For the optimizer, `x` is the variable
    and the cost function is the left-hand side expression, which is as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.30: Left-hand side expression'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_01_30.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.30: Left-hand side expression'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimizer will find the value of `x` for which the expression is the minimum
    – in this case, it is `0`. Please note that this will work only for quadratic
    equations that are perfect squares, such as in this case. The left-hand side expression
    is a perfect square that can be explained with the following equation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.31: Perfect square'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_01_31.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.31: Perfect square'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the code for solving this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook and rename it *Activity 1.01*.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow`:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create the variable `x` and initialize it to 0.0:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Construct the `loss` function as a `lambda` function:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create an instance of an optimizer with a learning rate of `.01`:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the optimizer through 10,000 iterations. You can start with a smaller number
    such as 1,000 and keep increasing the number of iterations until you get the solution:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Print the value of `x`:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is the solution to our quadratic equation. It may be noted that, irrespective
    of the number of iterations, you will never get a perfect 5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3gBTFGA](https://packt.live/3gBTFGA).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Dqa2Id](https://packt.live/2Dqa2Id).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Neural Networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 2.01: Build a Multilayer Neural Network to Classify Sonar Signals'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see how the solution looks. Remember—this is one solution, but there
    could be many variations:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required libraries:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load and examine the data:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.37: Contents of sonar.csv'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_37.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.37: Contents of sonar.csv'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Observe that there are 60 features, and the target has two values—Rock and Mine.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This means that this is a binary classification problem. Let's prepare the data
    before we build the neural network.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the features and the labels:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this code, `X_input` is selecting all the rows of all the columns except
    the `Class` column, and `Y_label` is just selecting the `Class` column.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Labels are in text format. We need to encode them as numbers before we can
    use them with our model:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `reshape` function at the end will convert the labels into matrix format,
    which is expected by the model.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the multilayer model with Keras:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can experiment with the number of layers and neurons, but the last layer
    can only have one neuron with a sigmoid activation function, since this is a binary
    classifier.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the training parameters:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the model:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The truncated output will be somewhat similar to the following:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s evaluate the trained model and examine its accuracy:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, we have been able to successfully train a multilayer binary
    neural network and get 100% accuracy within 30 epochs.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38EMoDi](https://packt.live/38EMoDi).
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2W2sygb](https://packt.live/2W2sygb).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. Image Classification with Convolutional Neural Networks (CNNs)
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 3.01: Building a Multiclass Classifier Based on the Fashion MNIST Dataset'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow.keras.datasets.fashion_mnist`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Load the Fashion MNIST dataset using `fashion_mnist.load_data()` and save the
    results to `(features_train, label_train), (features_test, label_test)`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Print the shape of the training set:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will be as follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The training set is composed of `60000` images of size `28` by `28`. We will
    need to reshape it and add the channel dimension.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of the testing set:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will be as follows:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The testing set is composed of `10000` images of size `28` by `28`. We will
    need to reshape it and add the channel dimension
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reshape the training and testing sets with the dimensions `(number_rows, 28,
    28, 1)`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create three variables called `batch_size`, `img_height`, and `img_width` that
    take the values `16`, `28`, and `28`, respectively:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create an `ImageDataGenerator` called `train_img_gen` with data augmentation:
    `rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1,
    shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=''nearest''`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create an `ImageDataGenerator` called `val_img_gen` with rescaling (by dividing
    by 255):'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a data generator called `train_data_gen` using `.flow()` and specify
    the batch size, features, and labels from the training set:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a data generator called `val_data_gen` using `.flow()` and specify the
    batch size, features, and labels from the testing set:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Instantiate a `tf.keras.Sequential()` class into a variable called `model`
    with the following layers: A convolution layer with `64` kernels of shape `3`,
    `ReLU` as the activation function, and the necessary input dimensions; a max pooling
    layer; a convolution layer with `128` kernels of shape `3` and `ReLU` as the activation
    function; a max pooling layer; a flatten layer; a fully connected layer with `128`
    units and `ReLU` as the activation function; a fully connected layer with `10`
    units and `softmax` as the activation function.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code should be as follows:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called optimizer:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The expected output will be as follows:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: Model training log'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_30.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.30: Model training log'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: We trained our CNN on five epochs, and we achieved accuracy scores of `0.8271`
    on the training set and `0.8334` on the validation set, respectively. Our model
    is not overfitting much and achieved quite a high score. The accuracy is still
    increasing after five epochs, so we may get even better results if we keep training
    it. This is something you may try by yourself.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ObmA8t](https://packt.live/2ObmA8t).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fiyyJi](https://packt.live/3fiyyJi).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Fruit Classification with Transfer Learning'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow` as `tf`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a variable called `file_url` containing the link to the dataset:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the aforementioned step, we are using the dataset stored at [https://packt.live/3eePQ8G](https://packt.live/3eePQ8G).
    If you have stored the dataset at any other URL, please change the highlighted
    path accordingly.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Download the dataset using `tf.keras.get_file` with `''fruits360.zip'', origin=file_url,
    extract=True` as parameters and save the result to a variable called `zip_dir`:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Import the `pathlib` library:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create a variable called `path` containing the full path to the `fruits360_filtered`
    directory using `pathlib.Path(zip_dir).parent`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create two variables called `train_dir` and `validation_dir` that take the
    full paths to the train (`Training`) and validation (`Test`) folders, respectively:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create two variables called `total_train` and `total_val` that will get the
    number of images for the training and validation sets, that is, `11398` and `4752`:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create an `ImageDataGenerator` called `train_img_gen` with data augmentation:
    `rescale=1./255, rotation_range=40, width_shift_range=0.1, height_shift_range=0.1,
    shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=''nearest''`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create an `ImageDataGenerator` called `val_img_gen` with rescaling (by dividing
    by 255):'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create four variables called `batch_size`, `img_height`, `img_width`, and `channel`
    that take the values `16`, `100`, `100`, and `3`, respectively:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a data generator called `train_data_gen` using `.flow_from_directory()`
    and specify the batch size, training folder, and target size:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create a data generator called `val_data_gen` using `.flow_from_directory()`
    and specify the batch size, validation folder, and target size:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import `VGG16` from `tensorflow.keras.applications`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Instantiate a `VGG16` model into a variable called `base_model` with the following
    parameters:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Set this model to non-trainable using the `.trainable` attribute:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Print the summary of this `VGG16` model:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The expected output will be as follows:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: Model summary'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_31.jpg)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.31: Model summary'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This output shows us the architecture of `VGG16`. We can see that there are
    `14,714,688` parameters in total, but there is no trainable parameter. This is
    expected as we have frozen all the layers of this model.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a new model using `tf.keras.Sequential()` by adding the base model to
    the following layers: `Flatten()`, `Dense(1000, activation=''relu'')`, and `Dense(120,
    activation=''softmax'')`. Save this model to a variable called `model`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Compile the neural network using `.compile()` with `loss=''categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps. This
    model may take a few minutes to train:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The expected output will be as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32: Expected output'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_32.jpg)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.32: Expected output'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Here, we used transfer learning to customize a pretrained `VGG16` model on ImageNet
    so that it fits our fruit classification dataset. We replaced the head of the
    model with our own fully connected layers and trained these layers on five epochs.
    We achieved an accuracy score of `0.9106` for the training set and `0.8920` for
    the testing set. These are quite remarkable results given the time and hardware
    used to train this model. You can try to fine-tune this model and see whether
    you can achieve an even better score.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2DsVRCl](https://packt.live/2DsVRCl).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Deep Learning for Text – Embeddings
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 4.01: Text Preprocessing of the ''Alice in Wonderland'' Text'
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You need to perform the following steps:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Before commencing this activity, make sure you have defined the `alice_raw`
    variable as demonstrated in the section titled *Downloading Text Corpora Using
    NLTK*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the data to lowercase and separate into sentences:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Tokenize the sentences:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Import `punctuation` from the `string` module and `stopwords` from NLTK:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Create a variable holding the contextual stop words `--` and `said`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create a master list for the stop words to remove words that contain terms
    from punctuation, NLTK stop words, and contextual stop words:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Define a function to drop these tokens from any input sentence (tokenized):'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Remove the terms in `stop_final` from the tokenized text:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Here''s what the first two sentences look like:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Using the `PorterStemmer` algorithm from NLTK, perform stemming on the result.
    Print out the first five sentences of the result:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will be as follows:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 4.02: Text Representation for Alice in Wonderland'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You need to perform the following steps:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Activity 4.01*, *Text Preprocessing Alice in Wonderland*, print the first
    three sentences from the result after stop word removal. This is the data you
    will work with:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as follows:'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Import `word2vec` from Gensim and train your word embeddings with default parameters:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Find the `5` terms most similar to `rabbit`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Using a `window` size of `2`, retrain the word vectors:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Find the terms most similar to `rabbit`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output will be as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Retrain word vectors using the Skip-gram method with a window size of `5`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Find the terms most similar to `rabbit`:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The output will be as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Find the representation for the phrase `white rabbit` by averaging the vectors
    for `white` and `rabbit`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Find the representation for `mad hatter` by averaging the vectors for `mad`
    and `hatter`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Find the cosine similarity between these two phrases:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This gives us the following value:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Load the pre-trained GloVe embeddings of size 100D using the formatted keyed vectors:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Find representations for `white rabbit` and `mad hatter`:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Find the `cosine` similarity between the two phrases. Has the cosine similarity changed?
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The following is the output of the preceding code:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Here, we can see that the cosine similarity between the two phrases "`mad hatter`"
    and "`white rabbit`" is far lower from the GloVe model. This is because the GloVe
    model hasn't seen the terms together in its training data as much as they appear
    in the book. In the book, the terms `mad` and `hatter` appear together a lot because
    they form the name of an important character. In other contexts, of course, we
    don't see `mad` and `hatter` together as often.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Deep Learning for Sequences
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 5.01: Using a Plain RNN Model to Predict IBM Stock Prices'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries, load the `.csv` file, reverse the index, and
    plot the time series (the `Close` column) for visual inspection:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The output will be as follows, with the closing price plotted on the *Y-axis*:'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.40: The trend for IBM stock prices'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_05_40.jpg)'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.40: The trend for IBM stock prices'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the values for `Close` from the DataFrame as a `numpy` array and plot
    them using `matplotlib`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The resulting trend is as follows, with the index plotted on the *X-axis*:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.41: The stock price data visualized'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_05_41.jpg)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.41: The stock price data visualized'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Assign the final 25% data as test data and the first 75% as train data:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output will be as follows:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Using `MinMaxScaler` from `sklearn`, scale the train and test data:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Using the `get_lookback` function we defined earlier in this chapter (refer
    to the *Preparing the Data for Stock Price Prediction* section), get the lookback
    data for the train and test sets using a lookback period of 10:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output will be as follows:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'From Keras, import all the necessary layers for employing plain RNNs (`SimpleRNN`,
    `Activation`, `Dropout`, `Dense`, and `Reshape`) and 1D convolutions (Conv1D).
    Also, import the `mean_squared_error` metric from `sklearn`:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Build a model with a 1D convolution layer (5 filters of size 3) and an RNN
    layer with 32 neurons. Add 25% dropout after the RNN layer. Print the model''s summary:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The output will be as follows:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.42: Summary of the model'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_05_42.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.42: Summary of the model'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the model with the `mean_squared_error` loss and the `adam` optimizer.
    Fit this on the train data in five epochs, with a validation split of 10% and
    a batch size of 1:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The output will be as follows:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.43: Training and validation loss'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_05_43.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.43: Training and validation loss'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the `get_model_perf` method, print the RMSE of the model:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output will be as follows:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Plot the predictions – the entire view, as well as the zoomed-in view:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We should see the following plot of predictions (dotted lines) versus the actuals
    (solid lines):'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.44: Predictions versus actuals'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_05_44.jpg)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.44: Predictions versus actuals'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'The zoomed-in view is as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.45: Predictions (dotted lines) versus actuals (solid lines) – detailed
    view'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_05_45.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.45: Predictions (dotted lines) versus actuals (solid lines) – detailed
    view'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model does a great job of catching the finer patterns and
    does extremely well at predicting the daily stock price.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 6\. LSTMs, GRUs, and Advanced RNNs
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 6.01: Sentiment Analysis of Amazon Product Reviews'
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Read in the data files for the `train` and `test` sets. Examine the shapes
    of the datasets and print out the top `5` records from the `train` data:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The dataset''s shape and header are as follows:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.26: First five records from the train dataset'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_26.jpg)'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.26: First five records from the train dataset'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For convenience, when it comes to processing, separate the raw text and the
    labels for the `train` and `test` sets. You should have `4` variables, as follows:
    `train_raw` comprising raw text for the train data, `train_labels` with labels
    for the train data, `test_raw` containing raw text for the test data, and `test_labels`
    comprising Labels for the test data. Print the first two reviews from the `train`
    text.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'The preceding code results in the following output:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.27: Raw text from the train dataset'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_27.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.27: Raw text from the train dataset'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalize the case and tokenize the test and train texts using NLTK''s `word_tokenize`
    (after importing it, of course – hint: use a list comprehension for cleaner code).
    Download `punkt` from `nltk` if you haven''t used the tokenizer before. Print
    the first review from the train data to check if the tokenization worked.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The tokenized data gets printed as follows:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.28: Tokenized review from the train dataset'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_28.jpg)'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.28: Tokenized review from the `train` dataset'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import any stop words (built in to NLTK) and punctuation from the string module.
    Define a function (`drop_stop`) to remove these tokens from any input tokenized
    sentence. Download `stopwords` from NLTK if you haven''t used it before:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Using the defined function (`drop_stop`), remove the redundant stop words from
    the `train` and the `test` texts. Print the first review of the processed `train`
    texts to check whether the function worked:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We''ll get the following output:'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Using `PorterStemmer` from NLTK, stem the tokens for both the `train` and `test`
    data:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The result should be printed as follows:'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Create the strings for each of the `train` and `text` reviews. This will help
    us work with the utilities in Keras to create and pad the sequences. Create the
    `train_texts` and `test_texts` variables. Print the first review from the processed
    `train` data to confirm this:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 `train` 和 `text` 评论创建字符串。这将帮助我们使用 Keras 中的工具来创建和填充序列。创建 `train_texts` 和 `test_texts`
    变量。打印处理后的 `train` 数据中的第一条评论，以确认这一点：
- en: '[PRE109]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的结果如下：
- en: '[PRE110]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'From Keras'' preprocessing utilities for text (`keras.preprocessing.text`),
    import the `Tokenizer` module. Define a vocabulary size of `10000` and instantiate
    the tokenizer with this vocabulary:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras 的文本预处理工具（`keras.preprocessing.text`）中导入 `Tokenizer` 模块。定义一个 `10000`
    的词汇量大小，并使用此词汇量实例化 tokenizer：
- en: '[PRE111]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Fit the tokenizer on the `train` texts. This works just like `CountVectorizer`
    did in *Chapter 4, Deep Learning for Text – Embeddings*, and trains the vocabulary.
    After fitting, use the `texts_to_sequences` method of the tokenizer on the `train`
    and `test` sets to create the sequences for them. Print the sequence for the first
    review in the train data:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `train` 文本上拟合 tokenizer。这与 *第 4 章 深度学习用于文本 – 嵌入* 中的 `CountVectorizer` 类似，并训练词汇表。拟合后，使用
    tokenizer 的 `texts_to_sequences` 方法对 `train` 和 `test` 数据集进行处理，生成它们的序列。打印训练数据中第一条评论的序列：
- en: '[PRE112]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'The encoded sequence is as follows:'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码后的序列如下：
- en: '[PRE113]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'We need to find the optimal length of the sequences to process the model. Get
    the length of the reviews from the `train` set into a list and plot a histogram
    of the lengths:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要找到处理模型的序列的最佳长度。获取 `train` 数据集中评论的长度列表，并绘制长度的直方图：
- en: '[PRE114]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'The distribution of the lengths is as follows:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长度分布如下：
- en: '![Figure 6.29: Histogram of text lengths'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.29: 文本长度直方图'
- en: '](img/B15385_06_29.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_29.jpg)'
- en: 'Figure 6.29: Histogram of text lengths'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6.29: 文本长度直方图'
- en: 'The data is now in the same format as the IMDb data we used in this chapter.
    Using a sequence length of `100` (define the `maxlen = 100` variable), use the
    `pad_sequences` method from the `sequence` module in Keras'' preprocessing utilities
    (`keras.preprocessing.sequence`) to limit the sequences to `100` for both the
    `train` and `test` data. Check the shape of the result for the train data:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据与我们在本章中使用的 IMDb 数据格式相同。使用 `100` 的序列长度（定义 `maxlen = 100` 变量），并使用 Keras 的预处理工具（`keras.preprocessing.sequence`）中的
    `pad_sequences` 方法，将 `train` 和 `test` 数据的序列限制为 `100`。检查训练数据结果的形状：
- en: '[PRE115]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The shape is as follows:'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 形状如下：
- en: '[PRE116]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'To build the model, import all the necessary layers from Keras (`embedding`,
    `spatial dropout`, `LSTM`, `dropout`, and `dense`) and import the `Sequential`
    model. Initialize the `Sequential` model:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要构建模型，从 Keras 导入所有必要的层（`embedding`，`spatial dropout`，`LSTM`，`dropout` 和 `dense`），并导入
    `Sequential` 模型。初始化 `Sequential` 模型：
- en: '[PRE117]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Add an embedding layer with `32` as the vector size (`output_dim`). Add a spatial
    dropout of `40%`:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个 `32` 维向量大小（`output_dim`）的嵌入层。添加一个 `40%` 丢弃率的空间丢弃层：
- en: '[PRE118]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Build a stacked LSTM model with `2` layers that have `64` cells each. Add a
    dropout layer with `40%` dropout:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个具有 `2` 层，每层 `64` 个单元的堆叠 LSTM 模型。添加一个 `40%` 丢弃率的 dropout 层：
- en: '[PRE119]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Add a dense layer with `32` neurons with `relu` activation, then a `50%` dropout
    layer, followed by another dense layer of `32` neurons with `relu` activation,
    and follow this up with another dropout layer with `50%` dropout:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个具有 `32` 个神经元的 dense 层，使用 `relu` 激活函数，然后是一个 `50%` 丢弃率的 dropout 层，接着是另一个具有
    `32` 个神经元的 dense 层，使用 `relu` 激活函数，最后再添加一个丢弃率为 `50%` 的 dropout 层：
- en: '[PRE120]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Add a final dense layer with a single neuron with `sigmoid` `activation` and
    compile the model. Print the model summary:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个最终的 dense 层，包含一个具有 `sigmoid` 激活函数的神经元，并编译模型。打印模型摘要：
- en: '[PRE121]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'The summary of the model will be as follows:'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '![Figure 6.30: Stacked LSTM model summary'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.30: 堆叠 LSTM 模型摘要'
- en: '](img/B15385_06_30.jpg)'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_30.jpg)'
- en: 'Figure 6.30: Stacked LSTM model summary'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6.30: 堆叠 LSTM 模型摘要'
- en: 'Fit the model on the training data with a `20%` validation split and a batch
    size of `128`. Train for `5` `epochs`:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `20%` 的验证集拆分和 `128` 的批量大小在训练数据上拟合模型。训练 `5` 个 `epochs`：
- en: '[PRE122]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'We will get the following training output:'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将获得以下训练输出：
- en: '![Figure 6.31: Stacked LSTM model training output'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.31: 堆叠 LSTM 模型训练输出'
- en: '](img/B15385_06_31.jpg)'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_31.jpg)'
- en: 'Figure 6.31: Stacked LSTM model training output'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6.31: 堆叠 LSTM 模型训练输出'
- en: 'Make a prediction on the test set using the `predict_classes` method of the
    model. Then, print out the confusion matrix:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的 `predict_classes` 方法对测试集进行预测。然后，打印混淆矩阵：
- en: '[PRE123]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'We will get the following result:'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将获得以下结果：
- en: '[PRE124]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Using the `accuracy_score` method from `scikit-learn`, calculate the accuracy
    of the test set.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'The accuracy we get is:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: As we can see, the accuracy score is around `86%`, and looking at the confusion
    matrix (output of *step 18*), the model does a decent job of predicting both classes
    well. We got this accuracy without doing any hyperparameter tuning. You can tweak
    the hyperparameters to get significantly higher accuracy.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fpo0YI](https://packt.live/3fpo0YI).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Wi75QH](https://packt.live/2Wi75QH).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Generative Adversarial Networks
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 7.01: Implementing a DCGAN for the MNIST Fashion Dataset'
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solution
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and name it `Activity 7.01`. Import the following
    library packages:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Create a function that will generate real data samples from the fashion MNIST data:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: The output from this function is the batch of MNIST data. Please note that we
    normalize the input data by subtracting `127.5`, which is half the max pixel value,
    and dividing by the same value. This will help in converging the solution faster.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s generate a set of images from the MNIST dataset:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'You should get the following output:'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.36: Generating images from MNIST'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_36.jpg)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.36: Generating images from MNIST'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s visualize the images with `matplotlib`:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'You should get an output similar to the one shown here:'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.37: Plotted images'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_37.jpg)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.37: Plotted images'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the output, we can see the visualization of several fashion articles. We
    can see that the images are centrally located within a white background. This
    are the images that we'll try to recreate.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s define the function to generate inputs for the generator network.
    The inputs are random data points that are generated from a random uniform distribution:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: This function generates the fake data that was sampled from the random distribution
    as the output.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s define the function for building the generator network:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Building the generator network is similar to building any CNN network. In this
    generator network, we will use the transpose convolution method for upsampling
    images. In this model, we can see the progressive use of the transpose convolution.
    The initial input starts with a dimension of 100, which is our input feature.
    The dimension of the MNIST dataset is batch size x 28 x 28\. Therefore, we have
    upsampled the data twice to get the output as batch size x 28 x 28.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we define the function that will be used to create fake samples:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: In this function, we only return the `X` variable. The output from this function
    is the fake dataset.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the parameters that we will use in many of the functions, along with
    the summary of the generator network:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'You should get the following output:'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.38: Summary of the generative model'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_38.jpg)'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.38: Summary of the generative model'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the summary, please note how the dimension of the input noise changes with
    each transpose convolution operation. Finally, we get an output that is equal
    in dimension to the real dataset, `( None,28 ,28,1)`.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use the generator function to generate a fake sample before training:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'You should get the following output:'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Now, let''s plot the generated fake sample:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'You should get an output similar to the following:'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.39: Output of the fake sample'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_39.jpg)'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.39: Output of the fake sample'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the plot of the fake sample before training. After training, we want
    samples like these to look like the MNIST fashion samples we visualized earlier
    in this activity.
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the discriminator model as a function. The network architecture will
    be similar to a CNN architecture:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: In the discriminator network, we have included all the necessary layers, such
    as the convolutional operations and `LeakyReLU`. Please note that the last layer
    is a sigmoid layer as we want the output as a probability of whether the sample
    is real (1) or fake (0).
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the summary of the discriminator network:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'You should get the following output:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.40: Discriminator model summary'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_40.jpg)'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.40: Discriminator model summary'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the GAN model as a function:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: The structure of the GAN model is similar to the one we developed in *Exercise
    7.05*, *Implementing the DCGAN*.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, it''s time to invoke the GAN function:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Please note that the inputs to the GAN model are the previously defined generator
    model and the discriminator model. You should get the following output:'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.41: GAN model summary'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_41.jpg)'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.41: GAN model summary'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please note that the parameters of each layer of the GAN model are equivalent
    to the parameters of the generator and discriminator models. The GAN model is
    just a wrapper around the two models we defined earlier.
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the number of epochs to train the network on using the following code:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Now, we can start the process of training the network:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: It needs to be noted here that the training of the discriminator model with
    the fake and real samples and the training of the GAN model happens concurrently.
    The only difference is the training of the GAN model proceeds without updating
    the parameters of the discriminator model. The other thing to note is that, inside
    the GAN, the labels for the fake samples would be 1 to generate large loss terms
    that will be backpropagated through the discriminator network to update the generator
    parameters. We also display the predicted probability of the GAN for every 50
    epochs. When calculating the probability, we combine a sample of real data and
    a sample of fake data and then take the mean of the predicted probability. We
    also save a copy of the generated image.
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output similar to the following:'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Let''s also look at some of the plots that were generated from the training
    process at various epochs:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.42: Images generated during the training process'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_42.jpg)'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.42: Images generated during the training process'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the preceding plots, we can see the progression of the training process.
    We can see that by epoch 100, the plots were mostly noise. By epoch 600, the forms
    of the fashion articles started to become more pronounced. At epoch 1,500, we
    can see that the fake images are looking very similar to the fashion dataset.
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note:'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can take a closer look at these images by going to [https://packt.live/2W1FjaI](https://packt.live/2W1FjaI).
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s look at the images that were generated after training:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'You should get an output similar to the following:'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.43: Images generated after the training process'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_07_43.jpg)'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.43: Images generated after the training process'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: From the training accuracy levels, you can see that the accuracy of the discriminator
    model hovers around the .50 range, which is the desired range. The purpose of
    the generator is to create fake images that look like real ones. When the generator
    generates images that look very similar to real images, the discriminator gets
    confused as to whether the image has been generated from the real distribution
    or fake distribution. This phenomenon manifests in an accuracy level of around
    50% for the discriminator, which is the desired level.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fpobDm](https://packt.live/3fpobDm).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
