<html><head></head><body>
<div id="_idContainer082">
<h1 class="c apter-number" id="_idParaDest-74"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.2.1">Solving Classification Problems</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapters, we learned how to set up and run MXNet, how to work with Gluon and DataLoader, and how to visualize datasets for regression, classification, image, and text problems. </span><span class="koboSpan" id="kobo.3.2">We also discussed the different learning methodologies. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we are going to focus on supervised learning with classification problems. </span><span class="koboSpan" id="kobo.3.4">We will learn why these problems are suitable for deep learning models with an overview of the equations that define these problems. </span><span class="koboSpan" id="kobo.3.5">We will learn how to create suitable models for them and how to train them, emphasizing the choice of hyperparameters. </span><span class="koboSpan" id="kobo.3.6">We will end each section by evaluating the models according to our data, as expected in supervised learning, and we will look at the different evaluation criteria for </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">classification problems.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following recipes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Understanding math for </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">classification models</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Defining loss functions and evaluation metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">for classification</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Training for </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">classification models</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">classification models</span></span></li>
</ul>
<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.15.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.16.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Preface</span></em><span class="koboSpan" id="kobo.18.1">, the following technical </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">requirements apply:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Ensure that you have completed the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.21.1">Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.22.1">, from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.23.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.24.1">, </span><em class="italic"><span class="koboSpan" id="kobo.25.1">Up and Running </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.26.1">with MXNet</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Ensure that you have completed the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.29.1">Toy dataset for classification – Loading, Managing, and Visualizing Iris Dataset</span></em><span class="koboSpan" id="kobo.30.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.31.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.32.1">, </span><em class="italic"><span class="koboSpan" id="kobo.33.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.34.1">and DataLoader.</span></em></span></li>
<li><span class="koboSpan" id="kobo.35.1">Most of the concepts for the model, the loss and evaluation functions, and the training were introduced in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.36.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.37.1">, </span><em class="italic"><span class="koboSpan" id="kobo.38.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">Furthermore, as we will see in this chapter, classification can be seen as a special case of regression. </span><span class="koboSpan" id="kobo.39.3">Therefore, it is strongly recommended to complete </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.40.1">Chapter </span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.41.1">3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.42.1"> first.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.43.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04"><span class="No-Break"><span class="koboSpan" id="kobo.45.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.46.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Furthermore, you can access each recipe directly from Google Colab; for example, for the first recipe of this chapter, visit the following </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">link: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.51.1">Understanding math for classification models</span></h1>
<p><span class="koboSpan" id="kobo.52.1">As we saw in the previous chapter, </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">classification</span></strong><span class="koboSpan" id="kobo.54.1"> problems are </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">supervised learning</span></strong><span class="koboSpan" id="kobo.56.1"> problems whose output is a class from a set of classes (categorical assignments) – for example, the </span><em class="italic"><span class="koboSpan" id="kobo.57.1">iris</span></em><span class="koboSpan" id="kobo.58.1"> class of </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">a flowe</span></span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">r</span></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">As we will see throughout this recipe, classification models can be seen as individual cases of regression models. </span><span class="koboSpan" id="kobo.62.2">We</span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.63.1"> will start by exploring a binary classification model. </span><span class="koboSpan" id="kobo.63.2">This is a model that will output one of two classes. </span><span class="koboSpan" id="kobo.63.3">We will label these classes </span><strong class="source-inline"><span class="koboSpan" id="kobo.64.1">[0, 1]</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.65.1">for simplicity.</span></span></p>
<p><span class="koboSpan" id="kobo.66.1">The simplest model we can use for such</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.67.1"> a binary classification problem is a </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">linear regression</span></strong><span class="koboSpan" id="kobo.69.1"> model. </span><span class="koboSpan" id="kobo.69.2">This model will output a number; therefore, to modify the output to satisfy our new classification criteria, we will modify the activation function to a more </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">suitable one.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">As in the previous recipes, we will use a neural network as our model, and we will solve the iris dataset prediction problem we introduced in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.72.1">Toy dataset for classification: Load, Manage and Visualize Iris Dataset</span></em><span class="koboSpan" id="kobo.73.1">, in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.74.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.75.1">, </span><em class="italic"><span class="koboSpan" id="kobo.76.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.77.1">and DataLoader.</span></em></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.78.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.79.1">We will be building upon the knowledge we gained in the previous recipes; therefore, it is highly recommended to read them. </span><span class="koboSpan" id="kobo.79.2">Moreover, as mentioned in the previous chapter, before jumping to understand our model, for the math part of this recipe, we will be using a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">at all.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.81.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.82.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.84.1">Defining a binary </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">classification model</span></span></li>
<li><span class="koboSpan" id="kobo.86.1">Defining a multi-label </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">classification model</span></span></li>
<li><span class="koboSpan" id="kobo.88.1">Defining </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">the features</span></span></li>
<li><span class="koboSpan" id="kobo.90.1">Initializing </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">the model</span></span></li>
<li><span class="koboSpan" id="kobo.92.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">the model</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.94.1">Defining a binary classification model</span></h3>
<p><span class="koboSpan" id="kobo.95.1">This </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.96.1">is</span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.97.1"> the perceptron model we introduced in the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">previous recipes:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.99.1"><img alt="Figure 4.1 – Perceptron" src="image/B16591_04_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.100.1">Figure 4.1 – Perceptron</span></p>
<p><span class="koboSpan" id="kobo.101.1">This </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.102.1">model could be mathematically described as </span><em class="italic"><span class="koboSpan" id="kobo.103.1">y = f(WX + b)</span></em><span class="koboSpan" id="kobo.104.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.105.1">W</span></em><span class="koboSpan" id="kobo.106.1"> is the weight vector </span><em class="italic"><span class="koboSpan" id="kobo.107.1">[W</span></em><span class="subscript"><span class="koboSpan" id="kobo.108.1">1</span></span><em class="italic"><span class="koboSpan" id="kobo.109.1">, W</span></em><span class="subscript"><span class="koboSpan" id="kobo.110.1">2</span></span><em class="italic"><span class="koboSpan" id="kobo.111.1">, …. </span><span class="koboSpan" id="kobo.111.2">W</span></em><span class="subscript"><span class="koboSpan" id="kobo.112.1">n</span></span><em class="italic"><span class="koboSpan" id="kobo.113.1">]</span></em><span class="koboSpan" id="kobo.114.1">, (</span><em class="italic"><span class="koboSpan" id="kobo.115.1">n</span></em><span class="koboSpan" id="kobo.116.1"> is the number of features), </span><em class="italic"><span class="koboSpan" id="kobo.117.1">X</span></em><span class="koboSpan" id="kobo.118.1"> is the feature vector </span><em class="italic"><span class="koboSpan" id="kobo.119.1">[X</span></em><span class="subscript"><span class="koboSpan" id="kobo.120.1">1</span></span><em class="italic"><span class="koboSpan" id="kobo.121.1">, X</span></em><span class="subscript"><span class="koboSpan" id="kobo.122.1">2</span></span><em class="italic"><span class="koboSpan" id="kobo.123.1">, …. </span><span class="koboSpan" id="kobo.123.2">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.124.1">n</span></span><em class="italic"><span class="koboSpan" id="kobo.125.1">]</span></em><span class="koboSpan" id="kobo.126.1">, </span><em class="italic"><span class="koboSpan" id="kobo.127.1">b</span></em><span class="koboSpan" id="kobo.128.1"> is the bias term, and </span><em class="italic"><span class="koboSpan" id="kobo.129.1">f()</span></em><span class="koboSpan" id="kobo.130.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">activation function.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">In the </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.133.1">regression use case, we chose the activation function as the identity function, which provided an output equal to the input; therefore, we had </span><em class="italic"><span class="koboSpan" id="kobo.134.1">y = WX + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.135.1">b</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">For our binary classification use case, we want to have an output that will help us classify our input data points into two classes (</span><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">0</span></strong><span class="koboSpan" id="kobo.139.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">1</span></strong><span class="koboSpan" id="kobo.141.1">). </span><span class="koboSpan" id="kobo.141.2">In the original Perceptron paper in 1958, Rosenblatt, who studied a binary classification problem, chose the step function, which provided 0 and 1 as its only </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">possible output.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">If we recall from </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.144.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.145.1">, </span><em class="italic"><span class="koboSpan" id="kobo.146.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.147.1">, in the third recipe, </span><em class="italic"><span class="koboSpan" id="kobo.148.1">Loss functions and evaluation metrics for regression</span></em><span class="koboSpan" id="kobo.149.1">, we imposed certain properties on those functions. </span><span class="koboSpan" id="kobo.149.2">The fourth property, differentiability, was required due to the computations required by gradient descent. </span><span class="koboSpan" id="kobo.149.3">The same property applies to activation functions, and the step function Rosenblatt used does not comply </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">with it.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">Furthermore, if </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.152.1">we could find a function that is also continuous between 0 and 1, we could assess that number as the probability or confidence of the output being </span><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">1</span></strong><span class="koboSpan" id="kobo.154.1">, as assessed by the model. </span><span class="koboSpan" id="kobo.154.2">This approach has advantages that we will explore later in </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">the recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">Therefore, as </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.157.1">the step function does not fulfill our properties, we need a new activation function. </span><span class="koboSpan" id="kobo.157.2">The most common activation function as the output of binary classification models is the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.158.1">sigmoid</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.159.1"> function:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.160.1"><img alt="Figure 4.2 – Sigmoid activation function" src="image/B16591_04_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.161.1">Figure 4.2 – Sigmoid activation function</span></p>
<p><span class="koboSpan" id="kobo.162.1">The sigmoid function complies with all the required properties and the output quickly becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">0</span></strong><span class="koboSpan" id="kobo.164.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">1</span></strong><span class="koboSpan" id="kobo.166.1">, which allows us to identify the output class suggested by </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">the model.</span></span></p>
<h3><span class="koboSpan" id="kobo.168.1">Defining a multi-label classification model</span></h3>
<p><span class="koboSpan" id="kobo.169.1">What</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.170.1"> happens when we have multiple (let’s call this number </span><em class="italic"><span class="koboSpan" id="kobo.171.1">k</span></em><span class="koboSpan" id="kobo.172.1">) classes instead of having just two classes to classify as we just saw? </span><span class="koboSpan" id="kobo.172.2">In this case, we need a different network architecture for our model. </span><span class="koboSpan" id="kobo.172.3">On the one hand, one output will not suffice now, as we need to have </span><em class="italic"><span class="koboSpan" id="kobo.173.1">k</span></em><span class="koboSpan" id="kobo.174.1"> different outputs. </span><span class="koboSpan" id="kobo.174.2">On the other </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.175.1">hand, although we could use the sigmoid function as the activation function for each of the outputs, it is very useful if each output can be assessed as probabilities of each class, as we saw for the binary case. </span><span class="koboSpan" id="kobo.175.2">Using sigmoid will not enforce the condition for probabilities that the sum of all of them must be 1 (which means that each of the inputs must correspond to one of </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">the classes).</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">In this case, a function very similar to the sigmoid function that satisfies the conditions described is the </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">softmax function:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.179.1">σ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.180.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.181.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.182.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.183.1">j</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.184.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.185.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.186.1"> </span></span><span class="_-----MathTools-_Math_Symbol_Extended_v-normal"><span class="koboSpan" id="kobo.187.1">ⅇ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.188.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.189.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.190.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.191.1">j</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.192.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.193.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.194.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.195.1">Σ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.196.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.197.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended_v-normal"><span class="koboSpan" id="kobo.198.1">ⅇ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.199.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.200.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.201.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.203.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.204.1">The class that will be selected is the one that will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">maximum value:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.206.1"><img alt="Figure 4.3 – Multi-label classification network" src="image/B16591_04_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.207.1">Figure 4.3 – Multi-label classification network</span></p>
<p><span class="koboSpan" id="kobo.208.1">The </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.209.1">full definition of our perceptron is </span><em class="italic"><span class="koboSpan" id="kobo.210.1">Y = f(WX + B)</span></em><span class="koboSpan" id="kobo.211.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.212.1">W</span></em><span class="koboSpan" id="kobo.213.1"> is now a weight matrix (with </span><em class="italic"><span class="koboSpan" id="kobo.214.1">n</span></em><span class="koboSpan" id="kobo.215.1"> x </span><em class="italic"><span class="koboSpan" id="kobo.216.1">k</span></em><span class="koboSpan" id="kobo.217.1"> shape: </span><em class="italic"><span class="koboSpan" id="kobo.218.1">n</span></em><span class="koboSpan" id="kobo.219.1"> being the number of features and </span><em class="italic"><span class="koboSpan" id="kobo.220.1">k</span></em><span class="koboSpan" id="kobo.221.1"> the number of outputs), </span><em class="italic"><span class="koboSpan" id="kobo.222.1">X</span></em><span class="koboSpan" id="kobo.223.1"> is the feature vector (</span><em class="italic"><span class="koboSpan" id="kobo.224.1">n</span></em><span class="koboSpan" id="kobo.225.1"> components), </span><em class="italic"><span class="koboSpan" id="kobo.226.1">B</span></em><span class="koboSpan" id="kobo.227.1"> is the bias vector (</span><em class="italic"><span class="koboSpan" id="kobo.228.1">k</span></em><span class="koboSpan" id="kobo.229.1"> components), and </span><em class="italic"><span class="koboSpan" id="kobo.230.1">f()</span></em><span class="koboSpan" id="kobo.231.1"> is the softmax activation function. </span><em class="italic"><span class="koboSpan" id="kobo.232.1">Y</span></em><span class="koboSpan" id="kobo.233.1"> is now a vector of </span><em class="italic"><span class="koboSpan" id="kobo.234.1">k</span></em><span class="koboSpan" id="kobo.235.1"> outputs, where the class with the </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.236.1">maximum value will be the class assigned to </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">the input.</span></span></p>
<h3><span class="koboSpan" id="kobo.238.1">Defining the features</span></h3>
<p><span class="koboSpan" id="kobo.239.1">So far, we </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.240.1">have defined our model and its behavior theoretically; we did not use our problem framing or our dataset to define it. </span><span class="koboSpan" id="kobo.240.2">In this section, we will start working at a more </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">practical level.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">The next step to continue defining our model is to decide on which features (inputs) we are going to work with. </span><span class="koboSpan" id="kobo.242.2">We will continue using the Iris dataset we already know from the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.243.1">Toy dataset for classification: loading, managing, and visualizing the House Sales dataset</span></em><span class="koboSpan" id="kobo.244.1"> in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.245.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.246.1">,</span><em class="italic"><span class="koboSpan" id="kobo.247.1"> Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.248.1">. </span><span class="koboSpan" id="kobo.248.2">This dataset contained data for 150 flowers, including the class and 4 </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">input features:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.250.1">Sepal length (</span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.252.1">Sepal width (</span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.254.1">Petal length (</span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">in cm)</span></span></li>
<li><span class="koboSpan" id="kobo.256.1">Petal width (</span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">in cm)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.258.1">If we show the first five flowers, we will see </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.260.1"><img alt="Figure 4.4 – Features of flowers (Iris dataset)" src="image/B16591_04_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.261.1">Figure 4.4 – Features of flowers (Iris dataset)</span></p>
<p><span class="koboSpan" id="kobo.262.1">Furthermore, for</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.263.1"> the Iris dataset, we have different </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">output classes:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">Setosa (0)</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">Versicolor (1)</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">Virginica (2)</span></strong></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.268.1">Initializing the model</span></h3>
<p><span class="koboSpan" id="kobo.269.1">Now that we </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.270.1">have defined the input dimension (number of features) and output dimensions, we can initialize our model using </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">random initialization:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.272.1">
Weights:
[[-1.2347414  -1.771029   -0.45138445]
 [ 0.57938355 -1.856082   -1.9768796 ]
 [-0.20801921  0.2444218  -0.03716067]
 [-0.48774993 -0.02261727  0.57461417]]
 &lt;NDArray 4x3 @cpu(0)&gt;
 Bias:
 [1.4661262  0.6862904  0.35496104]
 &lt;NDArray 3 @cpu(0)&gt;</span></pre> <p><span class="koboSpan" id="kobo.273.1">If we compare these values to the values we obtained in the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.274.1">Understanding math for regression models</span></em><span class="koboSpan" id="kobo.275.1">, from </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.276.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.277.1">, </span><em class="italic"><span class="koboSpan" id="kobo.278.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.279.1">, we </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.280.1">can see how the weights are now represented as a matrix as we have several outputs, not just one (as it was in the regression case), and the bias is a vector instead of a number, for the </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">same reason.</span></span></p>
<h3><span class="koboSpan" id="kobo.282.1">Evaluating the model</span></h3>
<p><span class="koboSpan" id="kobo.283.1">Now that our </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.284.1">model is initialized, we can use it to estimate the class of the first flower, which can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.285.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.286.1">.24</span></em><span class="koboSpan" id="kobo.287.1"> to be </span><em class="italic"><span class="koboSpan" id="kobo.288.1">Setosa</span></em><span class="koboSpan" id="kobo.289.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">0</span></strong><span class="koboSpan" id="kobo.291.1">). </span><span class="koboSpan" id="kobo.291.2">Here it is with our </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">current model:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.293.1">
0</span></pre> <p><span class="koboSpan" id="kobo.294.1">Nicely done! </span><span class="koboSpan" id="kobo.294.2">Unfortunately, this was pure chance as the model was </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">randomly initialized.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">In the next recipe, we will see how to properly evaluate our </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">classification models.</span></span></p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.298.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.299.1">Like</span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.300.1"> regression, classification models can have as many layers (depth) as needed, stacking as many of them as the problem’s </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">solution requires.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">In this recipe, we described the modifications from the perceptron described in the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.303.1">Understanding maths for regression models</span></em><span class="koboSpan" id="kobo.304.1">, in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.305.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.306.1">, </span><em class="italic"><span class="koboSpan" id="kobo.307.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.308.1">. </span><span class="koboSpan" id="kobo.308.2">There were two main modifications. </span><span class="koboSpan" id="kobo.308.3">The first one is that, as in this case, we want to categorize each input into a set of classes, we need one output per class. </span><span class="koboSpan" id="kobo.308.4">Moreover, in order to be able to understand the outputs of our model as probabilities, we needed a new activation </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">function, softmax.</span></span></p>
<p><span class="koboSpan" id="kobo.310.1">To finalize, we learned how to initialize our model, the effect initialization has on the weights and bias, and how we can use our data to evaluate it. </span><span class="koboSpan" id="kobo.310.2">We will develop all these topics further in the </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">later recipes.</span></span></p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.312.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.313.1">At the beginning of the recipe, we walked through the activation function changes from Rosenblatt (step function) to regression (linear) to classification (sigmoid). </span><span class="koboSpan" id="kobo.313.2">One of the details we discussed was the non-differentiability of the step function. </span><span class="koboSpan" id="kobo.313.3">A deeper analysis can be found at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">link: </span></span><a href="https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative"><span class="No-Break"><span class="koboSpan" id="kobo.315.1">https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative</span></span></a></p>
<p><span class="koboSpan" id="kobo.316.1">Using multi-layer architectures and/or sigmoid (or other activation functions) provides neural networks with the capability to approximate any function, which is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.317.1">Universal Approximation Theorem</span></strong><span class="koboSpan" id="kobo.318.1">. </span><span class="koboSpan" id="kobo.318.2">More</span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.319.1"> details can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">here: </span></span><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><span class="No-Break"><span class="koboSpan" id="kobo.321.1">https://en.wikipedia.org/wiki/Universal_approximation_theorem</span></span></a></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.322.1">Defining loss functions and evaluation metrics for classification</span></h1>
<p><span class="koboSpan" id="kobo.323.1">In the previous recipe, we</span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.324.1"> defined our input features, described our model, and initialized it. </span><span class="koboSpan" id="kobo.324.2">At that point, we passed a features vector of a flower to </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.325.1">predict its iris species, calculated the output, and compared it against the </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">expected class.</span></span></p>
<p><span class="koboSpan" id="kobo.327.1">We also showed how those preliminary results did not represent a proper evaluation. </span><span class="koboSpan" id="kobo.327.2">In this recipe, we will explore the topic of evaluating our </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">classification models.</span></span></p>
<p><span class="koboSpan" id="kobo.329.1">Furthermore, we will also understand which loss functions fit best for the binary and multi-label </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">classification problem.</span></span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.331.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.332.1">Loss functions and evaluation functions need to satisfy the same properties that are described in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.333.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.334.1">, </span><em class="italic"><span class="koboSpan" id="kobo.335.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.336.1">, in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.337.1">Defining Loss functions and evaluation metrics for regression</span></em><span class="koboSpan" id="kobo.338.1">; therefore, I recommend reading that chapter first for a more </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">thorough understanding.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">We will start developing our topics by analyzing the binary classification approach (two output classes), and we will generalize afterward to the multiple-label </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">classification approach.</span></span></p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.342.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.343.1">Let’s </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.344.1">discuss some</span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.345.1"> evaluation and loss functions and analyze their advantages and disadvantages. </span><span class="koboSpan" id="kobo.345.2">The functions we will describe are </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.347.1">Cross-entropy </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">loss function</span></span></li>
<li><span class="koboSpan" id="kobo.349.1">Evaluation – </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">confusion matrix</span></span></li>
<li><span class="koboSpan" id="kobo.351.1">Evaluation – </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">metrics</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.353.1">Cross-entropy loss function</span></h3>
<p><span class="koboSpan" id="kobo.354.1">As we</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.355.1"> discussed in our previous recipe, once</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.356.1"> the model has output a </span><em class="italic"><span class="koboSpan" id="kobo.357.1">probability</span></em><span class="koboSpan" id="kobo.358.1"> for each of our classes, we want to select the class that has the maximum probability as the output of </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">our model.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">When optimizing our model parameters, what we want is to find out which model parameters provide a maximum probability (</span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">1</span></strong><span class="koboSpan" id="kobo.362.1">) for our desired class and a minimum probability for the rest (</span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">0</span></strong><span class="koboSpan" id="kobo.364.1">). </span><span class="koboSpan" id="kobo.364.2">The derivation of the equation is out of the scope of the book, but you can find more information in </span><em class="italic"><span class="koboSpan" id="kobo.365.1">There’s more...</span></em><span class="koboSpan" id="kobo.366.1"> section at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">the recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">For the case of two output classes, we have the binary cross-entropy loss (for </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.369.1">N </span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">samples):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.371.1">B</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.372.1">C</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.373.1">E</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.374.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.375.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.376.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.377.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.378.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.379.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.380.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.381.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.382.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.383.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.384.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.385.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.386.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.387.1">0</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.388.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.389.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.390.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.391.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.392.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.393.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.394.1">.</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal"><span class="koboSpan" id="kobo.395.1">log</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.396.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.397.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.398.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.399.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.400.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.401.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.402.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.403.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.404.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.405.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.406.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.407.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.408.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.409.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.410.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.411.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.412.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.413.1">.</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.414.1">l</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.415.1">o</span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.416.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.417.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.418.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.419.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.420.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.421.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.422.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.423.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.424.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.425.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.426.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.427.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.428.1">We can plot this function for one sample, and assume the expected output to be </span><em class="italic"><span class="koboSpan" id="kobo.429.1">1 (yi = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.430.1">1)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.432.1"><img alt="Figure 4.5 – Binary cross-entropy loss graph (yi = 1)" src="image/B16591_04_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.433.1">Figure 4.5 – Binary cross-entropy loss graph (y</span><span class="subscript"><span class="koboSpan" id="kobo.434.1">i</span></span><span class="koboSpan" id="kobo.435.1"> = 1)</span></p>
<p><span class="koboSpan" id="kobo.436.1">For the </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.437.1">general </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.438.1">case of multiple labels, the multi-label or categorical cross-entropy loss for </span><em class="italic"><span class="koboSpan" id="kobo.439.1">M</span></em><span class="koboSpan" id="kobo.440.1"> classes becomes </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.442.1">L</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.443.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.444.1">y</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.445.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.446.1">ŷ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.447.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.448.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.449.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.450.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.451.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.452.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.453.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.454.1">0</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.455.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.456.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.457.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.458.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.459.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.460.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.461.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.462.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.463.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.464.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.465.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.466.1">(</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.467.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.468.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.469.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.470.1">j</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.471.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.472.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.473.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.474.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.475.1">(</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.476.1">ŷ</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.477.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.478.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.479.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.480.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.481.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.482.1">This equation yields the same graph as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.483.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.484.1">.5</span></em><span class="koboSpan" id="kobo.485.1"> when comparing each pair </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">of classes.</span></span></p>
<p><span class="koboSpan" id="kobo.487.1">We will use this function in combination with an optimizer during the training loop in the </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">next recipe.</span></span></p>
<h3><span class="koboSpan" id="kobo.489.1">Evaluation – confusion matrix</span></h3>
<p><span class="koboSpan" id="kobo.490.1">The </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.491.1">confusion matrix helps us measure the</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.492.1"> performance of our model, comparing the results between the expected values (ground truth) and the actual values our model will provide. </span><span class="koboSpan" id="kobo.492.2">For a binary classification problem (where we defined the classes as </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">Positive</span></strong><span class="koboSpan" id="kobo.494.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">Negative</span></strong><span class="koboSpan" id="kobo.496.1">), we have </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.498.1"><img alt="Figure 4.6 – Binary confusion matrix" src="image/B16591_04_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.499.1">Figure 4.6 – Binary confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.500.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.501.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.502.1">.6</span></em><span class="koboSpan" id="kobo.503.1">, for each combination of predicted and actual class, we have the following terms (only valid in </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">binary classification):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.505.1">TP</span></strong><span class="koboSpan" id="kobo.506.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">True positives.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.508.1">FP</span></strong><span class="koboSpan" id="kobo.509.1">: False positives. </span><span class="koboSpan" id="kobo.509.2">It is also known as a </span><em class="italic"><span class="koboSpan" id="kobo.510.1">type </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.511.1">I</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.512.1"> error.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.513.1">FN</span></strong><span class="koboSpan" id="kobo.514.1">: False negatives. </span><span class="koboSpan" id="kobo.514.2">It is also known as a </span><em class="italic"><span class="koboSpan" id="kobo.515.1">type </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.516.1">II</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.517.1"> error.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.518.1">TN</span></strong><span class="koboSpan" id="kobo.519.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">True negatives.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.521.1">Ideally, we want TP and TN to be as close to 100% as possible, and FP and FN as close to 0% </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">as possible.</span></span></p>
<p><span class="koboSpan" id="kobo.523.1">When we have a multi-label classification problem, in the confusion matrix, we will have one row and one column per class. </span><span class="koboSpan" id="kobo.523.2">For </span><em class="italic"><span class="koboSpan" id="kobo.524.1">K</span></em><span class="koboSpan" id="kobo.525.1"> classes, we will have a </span><em class="italic"><span class="koboSpan" id="kobo.526.1">KxK</span></em><span class="koboSpan" id="kobo.527.1"> matrix where, in the matrix main diagonal, we are looking for 100% probabilities and 0% in the rest of the values. </span><span class="koboSpan" id="kobo.527.2">For</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.528.1"> example, using our Iris dataset (three output classes), we can compute the confusion matrix (</span><em class="italic"><span class="koboSpan" id="kobo.529.1">3x3</span></em><span class="koboSpan" id="kobo.530.1">) for a model just randomly initialized, similar to the one we computed in the previous recipe. </span><span class="koboSpan" id="kobo.530.2">In this case, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.532.1"><img alt="Figure 4.7 – Multi-label confusion matrix" src="image/B16591_04_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.533.1">Figure 4.7 – Multi-label confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.534.1">As expected, the results shown are quite bad. </span><span class="koboSpan" id="kobo.534.2">Notably, not a single versicolor flower was correctly classified; however, this example helps us visualize a multi-label </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">confusion matrix.</span></span></p>
<h3><span class="koboSpan" id="kobo.536.1">Evaluation – accuracy, precision, recall, specificity, and F1-score metrics</span></h3>
<p><span class="koboSpan" id="kobo.537.1">To </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.538.1">characterize the</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.539.1"> performance of a model for a given binary classification problem, there are several </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">interesting metrics:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.541.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.542.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.543.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.544.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.545.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.546.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.547.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.548.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.549.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.550.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.551.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.552.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.553.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.555.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.556.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.557.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.558.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.559.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.560.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.562.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.565.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.566.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.567.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.568.1">l</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.569.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.570.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.571.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.572.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.573.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.574.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.575.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.576.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.577.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.578.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.579.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.580.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.581.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.582.1">F</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.583.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.584.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.585.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.586.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.587.1">×</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.588.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.589.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.590.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.591.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.592.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.593.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.594.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.595.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.596.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.597.1">×</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.598.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.599.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.600.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.601.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.602.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.603.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.604.1">  </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.605.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.606.1">______________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.607.1">  </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.608.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.609.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.610.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.611.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.612.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.613.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.614.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.615.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.616.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.617.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.618.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.619.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.620.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.621.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.622.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.623.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.624.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.625.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.626.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.627.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.628.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.629.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.630.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.631.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.632.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.633.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.634.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.635.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.636.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.637.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.638.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.639.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.640.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.641.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.642.1">_____________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.643.1">  </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.644.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.645.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.646.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.647.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.648.1">N</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.649.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.650.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.651.1">N</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.652.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.653.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.654.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.655.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.656.1">S</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.657.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.658.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.659.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.660.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.661.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.662.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.663.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.664.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.665.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.666.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.667.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.668.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.669.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.670.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.671.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.672.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.673.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.674.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.675.1">N</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.676.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.677.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.678.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.679.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.680.1">Each of these </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.681.1">metrics serves the purpose of helping us understand the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">our models:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.683.1">Accuracy</span></strong><span class="koboSpan" id="kobo.684.1">: From </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.685.1">all the values, which ones were correctly classified (for </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">both classes)?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.687.1">Precision</span></strong><span class="koboSpan" id="kobo.688.1">: This is</span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.689.1"> the rate of positive predictions that were correctly classified. </span><span class="koboSpan" id="kobo.689.2">However, it does not provide any information regarding </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">negative predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.691.1">Recall</span></strong><span class="koboSpan" id="kobo.692.1">: This is </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.693.1">the rate of positive labels that were correctly classified by the model. </span><span class="koboSpan" id="kobo.693.2">This figure does not include any information regarding </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">negative labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.695.1">Specificity</span></strong><span class="koboSpan" id="kobo.696.1">: Similar </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.697.1">to recall but for negative labels. </span><span class="koboSpan" id="kobo.697.2">This is the rate of negative labels that were correctly classified by the model. </span><span class="koboSpan" id="kobo.697.3">This figure does not include any information regarding positive labels. </span><span class="koboSpan" id="kobo.697.4">This metric is </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">seldom used.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.699.1">F1 score</span></strong><span class="koboSpan" id="kobo.700.1">: This is </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.701.1">the harmonic mean of precision and recall. </span><span class="koboSpan" id="kobo.701.2">By combining both metrics, this metric provides a better assessment of the model considering positive and negative classes. </span><span class="koboSpan" id="kobo.701.3">To achieve a high F1 score, the model needs to have a high precision and a high recall. </span><span class="koboSpan" id="kobo.701.4">A low F1 score will indicate that either precision, recall, or both </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">are low.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.703.1">These metrics can be computed for the multi-label scenario. </span><span class="koboSpan" id="kobo.703.2">For example, for our randomly initialized model </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.704.1">and the Iris datasets, these were the figures computed (except specificity, which does not have a metrics function </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">in </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.706.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.708.1">
Accuracy  : 0.5933333333333334
Precision : 0.4765151515151515
Recall    : 0.5933333333333334
F1-score  : 0.49722222222222223</span></pre> <p><span class="koboSpan" id="kobo.709.1">The values are very close to average results, as expected from a randomly </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">initialized model.</span></span></p>
<h3><span class="koboSpan" id="kobo.711.1">Evaluation – Area Under the Curve (AUC)</span></h3>
<p><span class="koboSpan" id="kobo.712.1">For </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.713.1">binary classification problems, the output we want to provide is a class (</span><strong class="source-inline"><span class="koboSpan" id="kobo.714.1">Positive</span></strong><span class="koboSpan" id="kobo.715.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.716.1">Negative</span></strong><span class="koboSpan" id="kobo.717.1">); however, the output of our model is a number (the probability of a positive result). </span><span class="koboSpan" id="kobo.717.2">To transform this result into a class, we need to apply </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">a threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.719.1">For example, if we define our threshold as </span><strong class="source-inline"><span class="koboSpan" id="kobo.720.1">0.5</span></strong><span class="koboSpan" id="kobo.721.1">, every probability larger than 0.5 will get assigned the </span><strong class="source-inline"><span class="koboSpan" id="kobo.722.1">Positive</span></strong><span class="koboSpan" id="kobo.723.1"> class. </span><span class="koboSpan" id="kobo.723.2">By decreasing our threshold, more values will be considered positive, increasing the number of TPs (</span><strong class="bold"><span class="koboSpan" id="kobo.724.1">True Positive Rate</span></strong><span class="koboSpan" id="kobo.725.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.726.1">TPR</span></strong><span class="koboSpan" id="kobo.727.1">) and the number of FPs (</span><strong class="bold"><span class="koboSpan" id="kobo.728.1">False Positive Rate</span></strong><span class="koboSpan" id="kobo.729.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.730.1">FPR</span></strong><span class="koboSpan" id="kobo.731.1">). </span><span class="koboSpan" id="kobo.731.2">If the threshold is increased, the effect is the contrary: fewer values will be considered positive, hence a smaller TPR </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">and FPR.</span></span></p>
<p><span class="koboSpan" id="kobo.733.1">As we modify the value of the threshold, we will have different TPR and FPR values. </span><span class="koboSpan" id="kobo.733.2">If we plot those values in a graph, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.735.1"><img alt="Figure 4.8 – AUC" src="image/B16591_04_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.736.1">Figure 4.8 – AUC</span></p>
<p><span class="koboSpan" id="kobo.737.1">If we calculate </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.738.1">the area covered between the curve, the </span><em class="italic"><span class="koboSpan" id="kobo.739.1">x</span></em><span class="koboSpan" id="kobo.740.1"> axis, the </span><em class="italic"><span class="koboSpan" id="kobo.741.1">y = 0</span></em><span class="koboSpan" id="kobo.742.1"> axis, and the </span><em class="italic"><span class="koboSpan" id="kobo.743.1">y = 1</span></em><span class="koboSpan" id="kobo.744.1"> axis, we obtain a parameter that is not dependent on the threshold value; it defines the performance of our model for the </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">given data.</span></span></p>
<p><span class="koboSpan" id="kobo.746.1">TPR and FPR are only defined for binary classification cases. </span><span class="koboSpan" id="kobo.746.2">For the multi-label classification case, we can emulate binary classification cases. </span><span class="koboSpan" id="kobo.746.3">There are two </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">possible approaches:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.748.1">One </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">versus one</span></span></li>
<li><span class="koboSpan" id="kobo.750.1">One </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">versus rest</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.752.1">If you are </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.753.1">interested, you can find more information in the </span><em class="italic"><span class="koboSpan" id="kobo.754.1">There’s more...</span></em><span class="koboSpan" id="kobo.755.1"> section of this recipe. </span><span class="koboSpan" id="kobo.755.2">These curves are also known </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.756.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.757.1">receiver operating </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.758.1">characteristic</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.759.1"> curves.</span></span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.760.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.761.1">After understanding the differences between a regression model and a classification model, including the activation functions, in this recipe, we focused on the loss functions (useful for training) and the metrics (useful for evaluation). </span><span class="koboSpan" id="kobo.761.2">We explored both cases: binary classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">multi-label classification.</span></span></p>
<p><span class="koboSpan" id="kobo.763.1">We computed the most common loss function for classification, the binary/categorical cross-entropy loss function, and we defined several evaluation metrics such as accuracy, precision, recall, and F1 score. </span><span class="koboSpan" id="kobo.763.2">Furthermore, we learned about the confusion matrix as an easy way to look at the per-class performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">our models.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">We ended the recipe by taking a look at the AUC, which provides a visualization agnostic </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">of thresholds.</span></span></p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.767.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.768.1">The mathematical formulas for cross-entropy loss were not derived. </span><span class="koboSpan" id="kobo.768.2">In these links, you can find </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">more information:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.770.1">Binary Cross-Entropy </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">Loss: </span></span><a href="https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross"><span class="No-Break"><span class="koboSpan" id="kobo.772.1">https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Sigmoid</span></span></a></li>
<li><span class="koboSpan" id="kobo.773.1">Categorical Cross-Entropy </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">Loss: </span></span><a href="https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax"><span class="No-Break"><span class="koboSpan" id="kobo.775.1">https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.776.1">For a better understanding of how to compute multi-label classification metrics, I recommend the following </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">link: </span></span><a href="https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2"><span class="No-Break"><span class="koboSpan" id="kobo.778.1">https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.779.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.780.1">To conclude, reading this AUC explanation can provide further </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">insight: </span></span><a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc"><span class="No-Break"><span class="koboSpan" id="kobo.782.1">https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.783.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.784.1">For the multi-label case, these examples can help understand the one versus one/one versus rest </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">approaches: </span></span><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"><span class="No-Break"><span class="koboSpan" id="kobo.786.1">https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.787.1">.</span></span></p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.788.1">Training for classification models</span></h1>
<p><span class="koboSpan" id="kobo.789.1">In this </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.790.1">recipe, we will visit the basic concepts of training a model to solve a classification problem. </span><span class="koboSpan" id="kobo.790.2">We will apply them to optimize the classification model we previously defined in this chapter, combined with the usage of the loss functions and evaluation metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">we discussed.</span></span></p>
<p><span class="koboSpan" id="kobo.792.1">We will predict the iris class of flowers using the dataset seen in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.793.1">Toy dataset for classification – load, manage, and visualize Iris dataset</span></em><span class="koboSpan" id="kobo.794.1">, from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.795.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.796.1">, </span><em class="italic"><span class="koboSpan" id="kobo.797.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.798.1">and DataLoader.</span></em></span></p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.799.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.800.1">In this recipe, we will follow a similar pattern as we did in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.801.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.802.1">, </span><em class="italic"><span class="koboSpan" id="kobo.803.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.804.1">, in the third recipe, </span><em class="italic"><span class="koboSpan" id="kobo.805.1">Training for regression models</span></em><span class="koboSpan" id="kobo.806.1">, so it will be interesting to revisit the concepts of the loss function, optimizer, dataset split, epochs, and </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">batch size.</span></span></p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.808.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.809.1">In this recipe, we will create our own training loop and we will evaluate how each hyperparameter influences the training. </span><span class="koboSpan" id="kobo.809.2">To achieve this, we will follow </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.811.1">Improve </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">the model.</span></span></li>
<li><span class="koboSpan" id="kobo.813.1">Define the loss function </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">and optimizer.</span></span></li>
<li><span class="koboSpan" id="kobo.815.1">Split our dataset and analyze fairness </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">and diversity.</span></span></li>
<li><span class="koboSpan" id="kobo.817.1">Put everything together for a </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">training loop.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.819.1">Improving the model</span></h3>
<p><span class="koboSpan" id="kobo.820.1">To solve </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.821.1">this problem, given the limited amount of data the dataset contains (150 samples), we will define a </span><strong class="bold"><span class="koboSpan" id="kobo.822.1">Multi-Layer Perceptron</span></strong><span class="koboSpan" id="kobo.823.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.824.1">MLP</span></strong><span class="koboSpan" id="kobo.825.1">) network </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.826.1">architecture, as we saw in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.827.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.828.1">, </span><em class="italic"><span class="koboSpan" id="kobo.829.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.830.1">, in the third recipe, </span><em class="italic"><span class="koboSpan" id="kobo.831.1">Training for regression models</span></em><span class="koboSpan" id="kobo.832.1">. </span><span class="koboSpan" id="kobo.832.2">This will have 2 hidden layers fully connected (dense) and </span><strong class="bold"><span class="koboSpan" id="kobo.833.1">ReLU</span></strong><span class="koboSpan" id="kobo.834.1"> activation function</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.835.1"> with 10 neurons in each, and an output layer with the corresponding 3 outputs (1 per class). </span><span class="koboSpan" id="kobo.835.2">The last layer is left without an activation function, although softmax was expected. </span><span class="koboSpan" id="kobo.835.3">In the next section, we will understand why. </span><span class="koboSpan" id="kobo.835.4">For this network, the necessary code is </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.837.1">
def create_classification_network(num_outputs = 3):
    # MLP with Gluon
    net = mx.gluon.nn.Sequential()
    net.add(mx.gluon.nn.Dense(10, activation="relu"))
    net.add(mx.gluon.nn.Dense(10, activation="relu"))
    net.add(mx.gluon.nn.Dense(num_outputs))
    # Note that the latest layer does not have an activation
    # function whereas Softmax was expected.
</span><span class="koboSpan" id="kobo.837.2">    # This is due to an optimization during training:
    # the loss function includes the softmax computation. </span><span class="koboSpan" id="kobo.837.3">return net</span></pre> <p><span class="koboSpan" id="kobo.838.1">We have also applied scaling to our input features. </span><span class="koboSpan" id="kobo.838.2">The number of parameters for the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.840.1">
Parameters in forward computation graph, duplicate included
   Total params: 193
   Trainable params: 193
   Non-trainable params: 0
Shared params in forward computation graph: 0
Unique parameters in model: 193</span></pre> <p><span class="koboSpan" id="kobo.841.1">The number of trainable parameters is ~200. </span><span class="koboSpan" id="kobo.841.2">For our (small) dataset, we have 4 features for each </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.842.1">of the 150 rows; therefore, our dataset is ~3 times the number of parameters of the model. </span><span class="koboSpan" id="kobo.842.2">Typically, this is the minimum for a successful model and, ideally, we would like to work with a dataset size of ~10 times the number of the parameters of </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">the model.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.844.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.845.1">Even though the comparison between the data points available and the number of parameters of the model is a very useful one, different architectures have different requirements in terms of data. </span><span class="koboSpan" id="kobo.845.2">As usual, experimentation (trial and error) is the key to finding the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">right balance.</span></span></p>
<h3><span class="koboSpan" id="kobo.847.1">Defining the loss function and optimizer</span></h3>
<p><span class="koboSpan" id="kobo.848.1">As discussed</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.849.1"> in the previous recipe, we will </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.850.1">compute the </span><strong class="bold"><span class="koboSpan" id="kobo.851.1">Categorial Cross-Entropy</span></strong><span class="koboSpan" id="kobo.852.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.853.1">CCE</span></strong><span class="koboSpan" id="kobo.854.1">) loss function. </span><span class="koboSpan" id="kobo.854.2">However, there is an optimization detail when computing the CCE loss with the softmax activation </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.855.1">function; therefore, the computation for the softmax function during training is included in the loss function. </span><span class="koboSpan" id="kobo.855.2">For </span><strong class="bold"><span class="koboSpan" id="kobo.856.1">inference</span></strong><span class="koboSpan" id="kobo.857.1">, we need to add it externally. </span><span class="koboSpan" id="kobo.857.2">Similar to what we did for regression problems, we will focus our analysis on the </span><strong class="bold"><span class="koboSpan" id="kobo.858.1">Stochastic Gradient Descent</span></strong><span class="koboSpan" id="kobo.859.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.860.1">SGD</span></strong><span class="koboSpan" id="kobo.861.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">Adam optimizers.</span></span></p>
<h3><span class="koboSpan" id="kobo.863.1">Splitting our dataset</span></h3>
<p><span class="koboSpan" id="kobo.864.1">One of </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.865.1">the strongest disadvantages of the Iris dataset is its size; with 150 samples, it is a small dataset. </span><span class="koboSpan" id="kobo.865.2">For this reason, we will apply a 50/40/10 split for the training, validation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">test sets.</span></span></p>
<p><span class="koboSpan" id="kobo.867.1">If we analyze the splits to verify fairness and diversity, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.869.1"><img alt="Figure 4.9 – Distribution for a training set (left), a validation set (middle ), and a test set (right)" src="image/B16591_04_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.870.1">Figure 4.9 – Distribution for a training set (left), a validation set (middle ), and a test set (right)</span></p>
<p><span class="koboSpan" id="kobo.871.1">We can </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.872.1">see that each of the classes is well represented across all features, the small size being the only reason for </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">the discrepancies.</span></span></p>
<h3><span class="koboSpan" id="kobo.874.1">Putting it all together for a training loop</span></h3>
<p><span class="koboSpan" id="kobo.875.1">The training loop</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.876.1"> for the classification case is very similar to the regression case, and we will follow a similar analysis as done earlier in this chapter: we will compare each of the hyperparameters, keeping the others constant (unless </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">otherwise noted).</span></span></p>
<h4><span class="koboSpan" id="kobo.878.1">Optimizer and learning rate</span></h4>
<p><span class="koboSpan" id="kobo.879.1">As </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.880.1">discussed earlier, the chosen optimizer for the training loop and the learning rate are related because, for some optimizers (such as SGD), the learning rate is kept constant, whereas for others (such as Adam), it varies from a </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">starting point.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.882.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.883.1">The best optimizer depends on several factors, and nothing trumps trial and error; I strongly suggest trying a few to see which one fits best. </span><span class="koboSpan" id="kobo.883.2">In my experience, typically, SGD and Adam are the ones that work best, including in </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">this problem.</span></span></p>
<p><span class="koboSpan" id="kobo.885.1">Let’s analyze how the training loss and validation loss vary for the SGD optimizer when we change</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.886.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.887.1">learning rate</span></strong><span class="koboSpan" id="kobo.888.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.889.1">LR</span></strong><span class="koboSpan" id="kobo.890.1">), keeping the other parameters constant: epochs = 100, batch size = 64, and loss function = </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">softmax cross-entropy:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.892.1"><img alt="Figure 4.10 – Loss for SGD optimizer with different LRs" src="image/B16591_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.893.1">Figure 4.10 – Loss for SGD optimizer with different LRs</span></p>
<p><span class="koboSpan" id="kobo.894.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.895.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.896.1">.10</span></em><span class="koboSpan" id="kobo.897.1">, we </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.898.1">can conclude that for the SGD optimizer, an LR value between 1.0 and 3.0 is optimal. </span><span class="koboSpan" id="kobo.898.2">Furthermore, we can see that for very large values of LR (&gt; 2.0), the algorithm still converges, whereas for the regression case, SGD and very large LRs made the </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">model diverge.</span></span></p>
<p><span class="koboSpan" id="kobo.900.1">Let’s analyze how the training loss and validation loss vary for the Adam optimizer when we change the LR, keeping the other parameters constant: epochs = 100, batch size = 64, and loss function = </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">softmax cross-entropy:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.902.1"><img alt="Figure 4.11 – Loss for the Adam optimizer with different LRs" src="image/B16591_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.903.1">Figure 4.11 – Loss for the Adam optimizer with different LRs</span></p>
<p><span class="koboSpan" id="kobo.904.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.905.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.906.1">.11</span></em><span class="koboSpan" id="kobo.907.1">, we can conclude that for the Adam optimizer, an LR value between 10</span><span class="superscript"><span class="koboSpan" id="kobo.908.1">-2</span></span><span class="koboSpan" id="kobo.909.1"> and 10</span><span class="superscript"><span class="koboSpan" id="kobo.910.1">-1</span></span> <span class="No-Break"><span class="koboSpan" id="kobo.911.1">is optimal.</span></span></p>
<p><span class="koboSpan" id="kobo.912.1">Although, in</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.913.1"> this case, SGD with an LR of 3.0 is yielding the best results (smallest loss), the evolution of the optimization process is much noisier than for Adam, possibly due to the limited amount of data available (the batch size did not influence this). </span><span class="koboSpan" id="kobo.913.2">A smooth optimization process is also an indication of how well a model can generalize; hence, we will choose Adam as our optimizer for the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.914.1">our tests.</span></span></p>
<h4><span class="koboSpan" id="kobo.915.1">Batch size</span></h4>
<p><span class="koboSpan" id="kobo.916.1">Let’s </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.917.1">analyze how the training loss and validation loss vary for the Adam optimizer by changing the batch size, keeping the other parameters constant: epochs = 100, LR = 10</span><span class="superscript"><span class="koboSpan" id="kobo.918.1">-2</span></span><span class="koboSpan" id="kobo.919.1">, and loss function = </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">softmax cross-entropy:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.921.1"><img alt="Figure 4.12 – Loss for the Adam optimizer by varying the batch size" src="image/B16591_04_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.922.1">Figure 4.12 – Loss for the Adam optimizer by varying the batch size</span></p>
<p><span class="koboSpan" id="kobo.923.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.924.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.925.1">.12</span></em><span class="koboSpan" id="kobo.926.1">, we can </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.927.1">conclude that for the Adam optimizer, a batch size value between 32 and 64 provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">best results.</span></span></p>
<h4><span class="koboSpan" id="kobo.929.1">Epochs</span></h4>
<p><span class="koboSpan" id="kobo.930.1">Let’s analyze how </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.931.1">the training loss and validation loss vary for the Adam optimizer by varying the epochs, keeping the other parameters constant: LR = 10</span><span class="superscript"><span class="koboSpan" id="kobo.932.1">-2</span></span><span class="koboSpan" id="kobo.933.1">, batch size = 32, and loss function = </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">softmax cross-entropy:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.935.1"><img alt="Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs" src="image/B16591_04_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.936.1">Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs</span></p>
<p><span class="koboSpan" id="kobo.937.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.938.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.939.1">.13</span></em><span class="koboSpan" id="kobo.940.1">, we</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.941.1"> can conclude that a range of 200–300 epochs is good for our problem. </span><span class="koboSpan" id="kobo.941.2">With these values, it is very likely the best result will be achieved earlier </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">than that.</span></span></p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.943.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.944.1">On our path to solving our classification problem, in this recipe, we learned how to update our model hyperparameters optimally. </span><span class="koboSpan" id="kobo.944.2">We revisited the role that each hyperparameter plays in the training loop and we performed some ablation studies for each individual hyperparameter. </span><span class="koboSpan" id="kobo.944.3">This helped us understand how our training and validation losses behaved when we modified each </span><span class="No-Break"><span class="koboSpan" id="kobo.945.1">hyperparameter individually.</span></span></p>
<p><span class="koboSpan" id="kobo.946.1">For our current problem and the chosen model, we verified that the best set of hyperparameters was </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.948.1">Optimizer: Adam</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.949.1">LR: 10</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.950.1">-2</span></span></span></li>
<li><span class="koboSpan" id="kobo.951.1">Batch </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">size: 32</span></span></li>
<li><span class="koboSpan" id="kobo.953.1">Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">epochs: 300</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.955.1">At the end of the training loop, these hyperparameters gave us a training loss of </span><strong class="source-inline"><span class="koboSpan" id="kobo.956.1">0.01</span></strong><span class="koboSpan" id="kobo.957.1"> and a validation loss </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.959.1">0.1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">.</span></span></p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.961.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.962.1">In this recipe, we mostly put together concepts we have been learning about in the previous recipes </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">and chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.964.1">We did pass through how, in our model definition, we did not explicitly use the softmax activation function. </span><span class="koboSpan" id="kobo.964.2">This is due to how cross-entropy loss and the softmax activation function work together during training (its joint derivative). </span><span class="koboSpan" id="kobo.964.3">A good reference to understand this point is </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">the following:</span></span></p>
<p><a href="https://peterroelants.github.io/posts/cross-entropy-softmax/"><span class="No-Break"><span class="koboSpan" id="kobo.966.1">https://peterroelants.github.io/posts/cross-entropy-softmax/</span></span></a></p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.967.1">Evaluating classification models</span></h1>
<p><span class="koboSpan" id="kobo.968.1">In the </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.969.1">previous recipe, we learned how to choose our training hyperparameters to optimize our training. </span><span class="koboSpan" id="kobo.969.2">We also verified how those choices affected the training and validation losses. </span><span class="koboSpan" id="kobo.969.3">In this recipe, we are going to explore how those choices affect our actual evaluation in the real world. </span><span class="koboSpan" id="kobo.969.4">You will have noticed that we split the dataset into three different sets: training, validation, and test sets. </span><span class="koboSpan" id="kobo.969.5">However, during our training, we only used the training set and the validation set. </span><span class="koboSpan" id="kobo.969.6">In this recipe, we will emulate real-world behavior by using the unseen data from our model, the </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">test set.</span></span></p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.971.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.972.1">When evaluating a model, we can perform qualitative evaluation and </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">quantitative evaluation.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.974.1">Qualitative evaluation</span></strong><span class="koboSpan" id="kobo.975.1"> is the</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.976.1"> selection of one or more random (or not so random, depending on what we are looking for) samples and analyzing the result, verifying whether it matches </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">our expectations.</span></span></p>
<p><span class="koboSpan" id="kobo.978.1">In this recipe, we will compute the evaluation metrics we defined in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.979.1">Defining loss functions and evaluation metrics for classification models</span></em><span class="koboSpan" id="kobo.980.1">, in </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.982.1">Furthermore, we</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.983.1"> are going to take a look at how training can have a large influence on </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">the evaluation.</span></span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.985.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.986.1">Before jumping into model evaluation, we will discuss how we can measure our model training performance. </span><span class="koboSpan" id="kobo.986.2">Therefore, the steps of this recipe are </span><span class="No-Break"><span class="koboSpan" id="kobo.987.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.988.1">Measuring training performance – losses </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">and accuracy</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.990.1">Qualitative evaluation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.991.1">Quantitative evaluation</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.992.1">Measuring training performance – losses and accuracy</span></h3>
<p><span class="koboSpan" id="kobo.993.1">As we </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.994.1">saw for regression, a good way to prevent overfitting was early stopping. </span><span class="koboSpan" id="kobo.994.2">When training our classification model in the previous recipe, we stored the training loss, validation loss, and validation accuracy. </span><span class="koboSpan" id="kobo.994.3">Let’s see how the training loss, validation loss, and validation accuracy evolve as the </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">training progresses:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.996.1"><img alt="Figure 4.14 – Losses ﻿and accuracy versus epochs (Adam)" src="image/B16591_04_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.997.1">Figure 4.14 – Losses and accuracy versus epochs (Adam)</span></p>
<p><span class="koboSpan" id="kobo.998.1">As we</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.999.1"> can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1000.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1001.1">.14</span></em><span class="koboSpan" id="kobo.1002.1">, at around epoch 50, the validation loss starts increasing although the training loss continues to decrease. </span><span class="koboSpan" id="kobo.1002.2">Moreover, the validation accuracy also seems to plateau (close to 1.0/100%) around that epoch. </span><span class="koboSpan" id="kobo.1002.3">We saved the model for the best accuracy and those are the values reported during training in the third recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1003.1">Training for classification models</span></em><span class="koboSpan" id="kobo.1004.1">, in </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">this chapter.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1006.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1007.1">As a reminder, if you want to use early stopping as is, MXNet provides a callback for </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">it: </span></span><a href="https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html"><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1011.1">An important thing to mention is that, in the previous recipe, we mentioned that SGD did not provide very smooth training. </span><span class="koboSpan" id="kobo.1011.2">If we plot the values, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.1013.1"><img alt="Figure 4.15 – Losses versus epochs (SGD)" src="image/B16591_04_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1014.1">Figure 4.15 – Losses versus epochs (SGD)</span></p>
<p><span class="koboSpan" id="kobo.1015.1">As we can see, the </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.1016.1">training is not stable </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">at all.</span></span></p>
<h3><span class="koboSpan" id="kobo.1018.1">Qualitative evaluation</span></h3>
<p><span class="koboSpan" id="kobo.1019.1">To verify that </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.1020.1">our model is behaving similarly to what we expect (yielding a high accuracy when predicting the iris species of a flower), a simple qualitative approach is to run our model for a random input from the test set (unseen data). </span><span class="koboSpan" id="kobo.1020.2">In our case, this is </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1022.1">
Expected Output: 0
Model Output: [[9.999949e-01 4.748235e-06 4.116847e-07]]
 Class Output: 0
Accuracy    : 1.0</span></pre> <p><span class="koboSpan" id="kobo.1023.1">For this example, as this was just one random input, accuracy can be either 100% or 0% (accurate or not), and we got the </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">right class.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1025.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1026.1">Although I tried to keep the code as reproducible as possible (including setting the seeds for all random processes), there might be some sources of randomness. </span><span class="koboSpan" id="kobo.1026.2">This means that your results might be different, but typically, the order of magnitude of errors will </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">be similar.</span></span></p>
<h3><span class="koboSpan" id="kobo.1028.1">Quantitative evaluation – confusion matrix</span></h3>
<p><span class="koboSpan" id="kobo.1029.1">For the </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.1030.1">stored results, the confusion matrix obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.1032.1"><img alt="Figure 4.16 – Confusion matrix" src="image/B16591_04_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1033.1">Figure 4.16 – Confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.1034.1">These results are excellent, as there are only values different from zero in the main diagonal. </span><span class="koboSpan" id="kobo.1034.2">This means our model yielded perfect results for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1035.1">test set.</span></span></p>
<h3><span class="koboSpan" id="kobo.1036.1">Quantitative evaluation – accuracy, precision, recall, and F1 score</span></h3>
<p><span class="koboSpan" id="kobo.1037.1">During our previous recipes, we defined these metrics and worked with them to optimize the training. </span><span class="koboSpan" id="kobo.1037.2">These evaluations were done for the training set and the validation set. </span><span class="koboSpan" id="kobo.1037.3">For the test set, we obtained the </span><span class="No-Break"><span class="koboSpan" id="kobo.1038.1">following values:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">Accuracy: 1.0</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">Precision: 1.0</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">Recall: 1.0</span></span></li>
<li><span class="koboSpan" id="kobo.1042.1">F1 </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">score: 1.0</span></span></li>
</ul>
<h2 id="_idParaDest-95"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.1044.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1045.1">In this recipe, we</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.1046.1"> explored how to evaluate our classification model. </span><span class="koboSpan" id="kobo.1046.2">To properly do this, we revisited the right decision of splitting our full dataset into a training set, a validation set, and a </span><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.1048.1">During training, we used the training set to calculate the gradients to update our model parameters, and the validation set to confirm the real-world behavior. </span><span class="koboSpan" id="kobo.1048.2">Afterward, to evaluate our model performance, we used the test set, which was the only remaining set of </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">unseen data.</span></span></p>
<p><span class="koboSpan" id="kobo.1050.1">We discovered the value of qualitatively describing our model behavior by calculating the output of random samples, and of quantitatively describing our model performance by exploring several numbers and graphs including the confusion matrix, accuracy, precision, recall, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">F1 score.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.1052.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1053.1">In this recipe, we computed the most important evaluation metrics for balanced classification datasets. </span><span class="koboSpan" id="kobo.1053.2">However, when the dataset is imbalanced, we need to be careful. </span><span class="koboSpan" id="kobo.1053.3">This is a good tutorial I like about </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">this topic:</span></span></p>
<p><a href="https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/"><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1057.1">At this stage, we have completed our path through a complete classification problem: we explored our classification dataset, decided on our evaluation metrics, and defined and initialized our model. </span><span class="koboSpan" id="kobo.1057.2">We understood the best hyperparameter combination of optimizer, learning rate, batch size, and epochs, and trained it with early stopping. </span><span class="koboSpan" id="kobo.1057.3">We ended by evaluating it qualitatively </span><span class="No-Break"><span class="koboSpan" id="kobo.1058.1">and quantitatively.</span></span></p>
</div>
</body></html>