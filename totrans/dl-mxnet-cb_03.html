<html><head></head><body>
<div id="_idContainer065">
<h1 class="c apter-number" id="_idParaDest-51"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-52"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.2.1">Solving Regression Problems</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapters, we learned how to set up and run MXNet, work with Gluon and DataLoaders, and visualize datasets for regression, classification, image, and text problems. </span><span class="koboSpan" id="kobo.3.2">We also discussed the different learning methodologies (supervised learning, unsupervised learning, and reinforcement learning). </span><span class="koboSpan" id="kobo.3.3">In this chapter, we are going to focus on supervised learning, where the expected outputs are known for at least some examples. </span><span class="koboSpan" id="kobo.3.4">Depending on the given type of these outputs, supervised learning can be decomposed into regression and classification. </span><span class="koboSpan" id="kobo.3.5">Regression outputs are numbers from a continuous distribution (such as predicting the stock price of a public company), whereas classification outputs are defined from a known set (for example, identifying whether an image corresponds to a mouse, a cat, or </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">a dog).</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Classification problems can be seen as a subset of regression problems, and therefore, in this chapter, we will start working with the latter ones. </span><span class="koboSpan" id="kobo.5.2">We will learn why these problems are suitable for deep learning models with an overview of the equations that define these problems. </span><span class="koboSpan" id="kobo.5.3">We will learn how to create suitable models and how to train them, emphasizing the choice of hyperparameters. </span><span class="koboSpan" id="kobo.5.4">We will end each section by evaluating the models according to our data, as expected in supervised learning, and we will see the different evaluation criteria for </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">regression problems.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will cover the following recipes in </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Understanding the math of </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">regression models</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Defining loss functions and evaluation metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">for regression</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Training </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">regression models</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">regression models</span></span></li>
</ul>
<h1 id="_idParaDest-53"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Preface</span></em><span class="koboSpan" id="kobo.20.1">, the following are some of the additional requirements needed for </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.22.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.23.1">Recipe, Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.24.1"> from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.25.1">Chapter 1</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.26.1"> Up and Running </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.27.1">with MXNet</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.30.1">Recipe 1, Toy dataset for regression – load, manage, and visualize a house sales dataset</span></em><span class="koboSpan" id="kobo.31.1"> from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.32.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.33.1">, Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.34.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.36.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03"><span class="No-Break"><span class="koboSpan" id="kobo.38.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.39.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">Furthermore, you can access each recipe directly from Google Colab, for example, for the</span><span class="superscript"> </span><span class="koboSpan" id="kobo.41.1">first recipe of this </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">chapter: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.43.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.44.1">.</span></span></p>
<h1 id="_idParaDest-54"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.45.1">Understanding the math of regression models</span></h1>
<p><span class="koboSpan" id="kobo.46.1">As we saw in the previous chapter, </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">regression</span></strong><span class="koboSpan" id="kobo.48.1"> problems</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.49.1"> are a type of </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">supervised learning</span></strong><span class="koboSpan" id="kobo.51.1"> problem whose output is a number from a continuous distribution, such as the price of a house or the predicted value of a company </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">stock price.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">The simplest </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.54.1">model we can use for a regression problem is a </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">linear regression</span></strong><span class="koboSpan" id="kobo.56.1"> model. </span><span class="koboSpan" id="kobo.56.2">However, these </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.57.1">models are extremely powerful for simple problems, as their parameters can be trained and are very fast and explainable, given the small number of parameters involved. </span><span class="koboSpan" id="kobo.57.2">As we will see, this number of parameters is completely dependent on the number of features </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">we use.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">Another interesting property of linear regression models is that they can be represented by neural networks, and as neural networks will be the basis for most models that we will be using throughout the book, this is the linear regression model based on neural networks that we will </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">be using.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">The simplest neural </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.62.1">network model is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">Perceptron</span></strong><span class="koboSpan" id="kobo.64.1">, and this is the building block we will be working on not only in this recipe, but throughout the </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">whole chapter.</span></span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.66.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.67.1">Before jumping in to </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.68.1">understand our model, let me mention that for the math part of this recipe, we will encounter a little bit of matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">at all.</span></span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.70.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.71.1">In this recipe, we will work through the </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.73.1">Modeling a biological </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">neuron mathematically</span></span></li>
<li><span class="koboSpan" id="kobo.75.1">Defining a </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">regression model</span></span></li>
<li><span class="koboSpan" id="kobo.77.1">Describing basic </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">activation functions</span></span></li>
<li><span class="koboSpan" id="kobo.79.1">Defining </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">the features</span></span></li>
<li><span class="koboSpan" id="kobo.81.1">Initializing </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">the model</span></span></li>
<li><span class="koboSpan" id="kobo.83.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">the model</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.85.1">Modeling a biological neuron mathematically</span></h3>
<p><span class="koboSpan" id="kobo.86.1">The</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.87.1"> Perceptron </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.88.1">was first introduced </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.89.1">by American psychologist Frank Rosenblatt in 1958 at Cornell Aeronautical Laboratory, and it was an initial attempt at replicating how information is processed by the neurons in </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">our brain.</span></span></p>
<p><span class="koboSpan" id="kobo.91.1">Rosenblatt analyzed a biological neuron and developed a mathematical model that behaved similarly. </span><span class="koboSpan" id="kobo.91.2">To make a comparison between these architectures, we will start with a very simple model of </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">a neuron.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.93.1"><img alt="Figure 3.1 – Biological neuron" src="image/B16591_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.94.1">Figure 3.1 – Biological neuron</span></p>
<p><span class="koboSpan" id="kobo.95.1">As we can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.96.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.97.1">.1</span></em><span class="koboSpan" id="kobo.98.1">, the </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.99.1">neuron is </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.100.1">composed of three </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">main parts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.102.1">Dendrites</span></strong><span class="koboSpan" id="kobo.103.1">: Where </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.104.1">the neuron receives inputs from other neurons. </span><span class="koboSpan" id="kobo.104.2">Depending on how strong the connection is, an input will be increased or decreased in </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">the dendrites.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.106.1">Cell Body</span></strong><span class="koboSpan" id="kobo.107.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.108.1">Soma</span></strong><span class="koboSpan" id="kobo.109.1">: Contains</span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.110.1"> the nucleus, which is the structure </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.111.1">that receives all the inputs from the dendrites and processes them. </span><span class="koboSpan" id="kobo.111.2">The nucleus might trigger an electrical message to be communicated to </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">other neurons.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">Axon/Axon Terminals</span></strong><span class="koboSpan" id="kobo.114.1">: This </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.115.1">is the output structure, which communicates messages with </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">other neurons.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.117.1">Rosenblatt took </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.118.1">the preceding simplified model of the neuron and assigned to it certain </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">mathematical properties:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.120.1"><img alt="Figure 3.2 – Perceptron" src="image/B16591_03_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.121.1">Figure 3.2 – Perceptron</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">Weights</span></strong><span class="koboSpan" id="kobo.123.1">: This </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.124.1">will simulate the</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.125.1"> behavior of dendrites by multiplying the inputs or features with a set of weights (</span><em class="italic"><span class="koboSpan" id="kobo.126.1">W</span></em><span class="koboSpan" id="kobo.127.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.128.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.129.1">.2</span></em><span class="koboSpan" id="kobo.130.1">, while </span><em class="italic"><span class="koboSpan" id="kobo.131.1">j</span></em><span class="koboSpan" id="kobo.132.1"> refers to any neuron in </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">the model).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.134.1">Sum</span></strong><span class="koboSpan" id="kobo.135.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">bias</span></strong><span class="koboSpan" id="kobo.137.1">: The </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.138.1">combination of input signals done in the nucleus </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.139.1">will be modeled as a sum with a bias (</span><em class="italic"><span class="koboSpan" id="kobo.140.1">θ</span></em><span class="koboSpan" id="kobo.141.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.142.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.143.1">.2</span></em><span class="koboSpan" id="kobo.144.1">) and a processing function, called </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.145.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.146.1">activation function</span></strong><span class="koboSpan" id="kobo.147.1">. </span><span class="koboSpan" id="kobo.147.2">(We will describe these functions in the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">following step.)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.149.1">Output</span></strong><span class="koboSpan" id="kobo.150.1">: Either </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.151.1">connections to other neurons, or the direct output of the whole model (</span><em class="italic"><span class="koboSpan" id="kobo.152.1">o</span></em><span class="koboSpan" id="kobo.153.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.154.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.155.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.157.1">Comparing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.158.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.159.1">.1</span></em><span class="koboSpan" id="kobo.160.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.161.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.162.1">.2</span></em><span class="koboSpan" id="kobo.163.1">, we can see the similarities between the simplified model of the biological neuron and the Perceptron. </span><span class="koboSpan" id="kobo.163.2">Furthermore, we can also see how all these parts are connected together, from processing the input to delivering </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the output.</span></span></p>
<h3><span class="koboSpan" id="kobo.165.1">Defining a regression model</span></h3>
<p><span class="koboSpan" id="kobo.166.1">Therefore, from</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.167.1"> a mathematical perspective using matrix multiplication, we can write the following equations for the model </span><em class="italic"><span class="koboSpan" id="kobo.168.1">y = f(W</span></em><span class="koboSpan" id="kobo.169.1">⋅</span><em class="italic"><span class="koboSpan" id="kobo.170.1">X + b)</span></em><span class="koboSpan" id="kobo.171.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.172.1">W</span></em><span class="koboSpan" id="kobo.173.1"> is the weight vector </span><em class="italic"><span class="koboSpan" id="kobo.174.1">[W</span></em><span class="subscript"><span class="koboSpan" id="kobo.175.1">1</span></span><em class="italic"><span class="koboSpan" id="kobo.176.1">, W</span></em><span class="subscript"><span class="koboSpan" id="kobo.177.1">2</span></span><em class="italic"><span class="koboSpan" id="kobo.178.1">, …. </span><span class="koboSpan" id="kobo.178.2">W</span></em><span class="subscript"><span class="koboSpan" id="kobo.179.1">n</span></span><em class="italic"><span class="koboSpan" id="kobo.180.1">]</span></em><span class="koboSpan" id="kobo.181.1">, (n is the number of features), </span><em class="italic"><span class="koboSpan" id="kobo.182.1">X</span></em><span class="koboSpan" id="kobo.183.1"> is the feature vector </span><em class="italic"><span class="koboSpan" id="kobo.184.1">[X</span></em><span class="subscript"><span class="koboSpan" id="kobo.185.1">1</span></span><em class="italic"><span class="koboSpan" id="kobo.186.1">, X</span></em><span class="subscript"><span class="koboSpan" id="kobo.187.1">2</span></span><em class="italic"><span class="koboSpan" id="kobo.188.1">, …. </span><span class="koboSpan" id="kobo.188.2">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.189.1">n</span></span><em class="italic"><span class="koboSpan" id="kobo.190.1">]</span></em><span class="koboSpan" id="kobo.191.1">, </span><em class="italic"><span class="koboSpan" id="kobo.192.1">b</span></em><span class="koboSpan" id="kobo.193.1"> is the bias term, and </span><em class="italic"><span class="koboSpan" id="kobo.194.1">f()</span></em><span class="koboSpan" id="kobo.195.1"> is the activation function. </span><span class="koboSpan" id="kobo.195.2">For the regression case we will be dealing with, we will work with a linear activation function, where the output is equal to </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">the input.</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">Therefore, in our case, the activation function is the identity function (output equal to the input), we have </span><em class="italic"><span class="koboSpan" id="kobo.198.1">y = W</span></em><em class="italic"><span class="koboSpan" id="kobo.199.1">⋅</span></em><em class="italic"><span class="koboSpan" id="kobo.200.1">X + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.201.1">b</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">We can easily achieve this with MXNet and its </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">NDArray library:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.205.1">
# Perceptron Model
def perceptron(weights, bias, features):
return mx.nd.dot(features, weights) + bias</span></pre> <p><span class="koboSpan" id="kobo.206.1">And that’s it! </span><span class="koboSpan" id="kobo.206.2">That’s our </span><em class="italic"><span class="koboSpan" id="kobo.207.1">y</span></em><span class="koboSpan" id="kobo.208.1"> neural network output in terms of the </span><em class="italic"><span class="koboSpan" id="kobo.209.1">X</span></em><span class="koboSpan" id="kobo.210.1"> inputs and the </span><em class="italic"><span class="koboSpan" id="kobo.211.1">W</span></em><span class="koboSpan" id="kobo.212.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.213.1">b</span></em><span class="koboSpan" id="kobo.214.1"> parameters of </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">the model.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.216.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.217.1">In the original paper by Rosenblatt, the expected output was either 0 or 1 (classification problem) and to fulfill this requirement, the activation function was defined as the step function (0 if the input is smaller than 0, or 1 if the input is larger than or equal to 0). </span><span class="koboSpan" id="kobo.217.2">This was one of the strongest limitations to Rosenblatt’s neuron model, and different activation functions were proposed later that improve the behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">In deep learning networks, we do not use a single neurons (Perceptrons) as our model. </span><span class="koboSpan" id="kobo.219.2">Typically, several layers of Perceptrons are stacked together, and the number of layers is also known </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.220.1">as the </span><strong class="bold"><span class="koboSpan" id="kobo.221.1">depth </span></strong><span class="koboSpan" id="kobo.222.1">of </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">the network.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.224.1"><img alt="Figure 3.3 – Deep learning network" src="image/B16591_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.225.1">Figure 3.3 – Deep learning network</span></p>
<p><span class="koboSpan" id="kobo.226.1">These </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.227.1">networks are quite powerful and have been proven to match or surpass human-level performance in several fields, including image recognition in computer vision and sentiment analysis in natural </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">language processing.</span></span></p>
<h3><span class="koboSpan" id="kobo.229.1">Describing basic activation functions</span></h3>
<p><span class="koboSpan" id="kobo.230.1">The most </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.231.1">common activation functions for regression </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.232.1">problems are the linear </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.233.1">activation function and the </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">ReLU</span></strong><span class="koboSpan" id="kobo.235.1"> activation function. </span><span class="koboSpan" id="kobo.235.2">Let’s briefly </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">describe them.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.237.1"><img alt="Figure 3.4 – Activation functions for regression" src="image/B16591_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.238.1">Figure 3.4 – Activation functions for regression</span></p>
<h4><span class="koboSpan" id="kobo.239.1">Linear Activation Function</span></h4>
<p><span class="koboSpan" id="kobo.240.1">In this </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.241.1">function, the output is equal to the input. </span><span class="koboSpan" id="kobo.241.2">It is </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.242.1">not bounded, and therefore it is suitable for unbounded numerical outputs, as is the case with the output for the </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">regression problems.</span></span></p>
<h4><span class="koboSpan" id="kobo.244.1">ReLU</span></h4>
<p><span class="koboSpan" id="kobo.245.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.246.1">Rectified Linear Unit</span></strong><span class="koboSpan" id="kobo.247.1"> activation function </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.248.1">is very </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.249.1">similar to the linear activation function: its output is equal to the input, but in this case, only when the input is larger than 0; the output is 0 otherwise. </span><span class="koboSpan" id="kobo.249.2">This function is suitable to only pass positive information to the next layers (sparse activation), and also provides better gradient propagation. </span><span class="koboSpan" id="kobo.249.3">Therefore its use is very common in intermediate layers on deep </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">learning networks.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.251.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.252.1">As we will see in the next recipes, training involves computing iteratively new gradients and using these computations to update the model parameters. </span><span class="koboSpan" id="kobo.252.2">When using activation functions such as the sigmoid, the larger the number of layers, the smaller the gradient becomes. </span><span class="koboSpan" id="kobo.252.3">This problem is known as the vanishing gradient problem, and the ReLU activation function behaves better in </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">those scenarios.</span></span></p>
<h3><span class="koboSpan" id="kobo.254.1">Defining the features</span></h3>
<p><span class="koboSpan" id="kobo.255.1">So far, we </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.256.1">have defined our model and its behavior theoretically, but we have not used our problem framing or our dataset to define it. </span><span class="koboSpan" id="kobo.256.2">In this section, we will start working at a more </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">practical level.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">The next step in defining our model is to decide which features (inputs) we are going to work with. </span><span class="koboSpan" id="kobo.258.2">We will continue using the House Sales Dataset we met in </span><em class="italic"><span class="koboSpan" id="kobo.259.1">Recipe 1, Toy dataset for regression – load, manage, and visualize house sales dataset</span></em><span class="koboSpan" id="kobo.260.1"> in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.261.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.262.1">, Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.263.1">. </span><span class="koboSpan" id="kobo.263.2">This dataset contained data for 21,613 houses, including prices and 19 input features. </span><span class="koboSpan" id="kobo.263.3">Although in our model we will work with all the input features, in the aforementioned recipe, we saw that the three non-correlated features that primarily contributed to the house price were </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.265.1">Square feet of </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">living space</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.267.1">Grade</span></span></li>
<li><span class="koboSpan" id="kobo.268.1">Number </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">of bathrooms</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.270.1">For an initial study, we will work with these three features. </span><span class="koboSpan" id="kobo.270.2">After selecting these features, if we show the first five houses, we will see </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.272.1"><img alt="Figure 3.5 – Filtered features for house prices" src="image/B16591_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.273.1">Figure 3.5 – Filtered features for house prices</span></p>
<p><span class="koboSpan" id="kobo.274.1">One of the paths</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.275.1"> that we did not exploit when analyzing this dataset previously is that </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">grade</span></strong><span class="koboSpan" id="kobo.277.1"> is not a continuous feature as the others are; its values come from a discrete set of values. </span><span class="koboSpan" id="kobo.277.2">This type of feature is called a categorical feature, and can be nominal or ordinal. </span><span class="koboSpan" id="kobo.277.3">A nominal feature is a class name – for example, we could have the architectural style of the house as a feature, and the values of that feature could be Victorian, Art Deco, Craftsman, and so on. </span><span class="koboSpan" id="kobo.277.4">Ordinal features, on the other hand, are class numbers. </span><span class="koboSpan" id="kobo.277.5">In our case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">grade</span></strong><span class="koboSpan" id="kobo.279.1"> is an ordinal feature consisting of numerical values in an order (1 is worst, 13 </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">is best).</span></span></p>
<p><span class="koboSpan" id="kobo.281.1">In both cases, categorical features can be represented numerically in different ways that will help our models better learn the relationship between that particular feature and the output. </span><span class="koboSpan" id="kobo.281.2">There are several approaches to dealing with categorical features. </span><span class="koboSpan" id="kobo.281.3">In this example, we are going to work with one of the simplest </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.282.1">approaches, a </span><strong class="bold"><span class="koboSpan" id="kobo.283.1">one-hot </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.284.1">encoding</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.285.1"> scheme.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.286.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.287.1">You will find more information on working with categorical data in the </span><em class="italic"><span class="koboSpan" id="kobo.288.1">There’s more</span></em><span class="koboSpan" id="kobo.289.1"> section at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">this recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">With a one-hot encoding scheme, each of the categories is decomposed into its own feature and will be assigned a binary value accordingly. </span><span class="koboSpan" id="kobo.291.2">In our case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">grade</span></strong><span class="koboSpan" id="kobo.293.1"> contains integer values from 1 to 13, and therefore, we add 13 new features to our input vector. </span><span class="koboSpan" id="kobo.293.2">Each of these new features will have a value of 0 or 1. </span><span class="koboSpan" id="kobo.293.3">For example, for a house of grade 1, the feature vector looks </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.295.1"><img alt="Figure 3.6 – One-hot encoding for grade feature" src="image/B16591_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.296.1">Figure 3.6 – One-hot encoding for grade feature</span></p>
<p><span class="koboSpan" id="kobo.297.1">If we take a</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.298.1"> close look at </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.299.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.300.1">.6</span></em><span class="koboSpan" id="kobo.301.1">, we will see that there is no one-hot encoding for the grade column corresponding to the value 2. </span><span class="koboSpan" id="kobo.301.2">This is because no actual house in our dataset had that grade, and therefore, it has not been added as </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">a feature.</span></span></p>
<p><span class="koboSpan" id="kobo.303.1">Therefore, the final number of features is 14 and we have 1 output, the price of </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">the house.</span></span></p>
<h3><span class="koboSpan" id="kobo.305.1">Initializing the model</span></h3>
<p><span class="koboSpan" id="kobo.306.1">Now </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.307.1">that we have defined the input (the features) and output dimensions, we can initialize our model. </span><span class="koboSpan" id="kobo.307.2">We will explore this in more detail in the next recipes, but it is useful to show a glimpse of how this </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">would look:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.309.1">
Weights:
 [[ 0.96975976]
 [-0.52853745]
 [-1.88909 ]
 [ 0.65479124]
 [-0.45481315]
 [ 0.32510808]
 [-1.3002341 ]
 [ 0.3679345 ]
 [ 1.4534262 ]
 [ 0.24154152]
 [ 0.47898006]
 [ 0.96885103]
 [-1.0218245 ]
 [-0.06812762]]
 &lt;NDArray 14x1 @cpu(0)&gt;
 Bias:
[-0.31868345]
 &lt;NDArray 1 @cpu(0)&gt;</span></pre> <p><span class="koboSpan" id="kobo.310.1">As expected, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">Weights</span></strong><span class="koboSpan" id="kobo.312.1"> vector has 14 components (the number of features) and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">Bias</span></strong><span class="koboSpan" id="kobo.314.1"> vector </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.315.1">has </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">1 component.</span></span></p>
<h3><span class="koboSpan" id="kobo.317.1">Evaluating the model</span></h3>
<p><span class="koboSpan" id="kobo.318.1">Now that our </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.319.1">model is initialized, we can use it to estimate the price of the first house, which can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.320.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.321.1">.6</span></em><span class="koboSpan" id="kobo.322.1"> to be around 2.2 million dollars. </span><span class="koboSpan" id="kobo.322.2">With our current model, the estimated house price is (</span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">in $):</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.324.1">
2610.2383</span></pre> <p><span class="koboSpan" id="kobo.325.1">As we have the expected price, we can compute some error metrics. </span><span class="koboSpan" id="kobo.325.2">In this case, I have chosen the absolute error and the error relative to the actual price. </span><span class="koboSpan" id="kobo.325.3">These quantities can be easily computed </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.327.1">
error_abs = abs(expected_output - model_output)
 error_perc = error_abs / expected_output * 100
 print("Absolute Error:", error_abs)
 print("Relative Error (%):", error_perc)</span></pre> <p><span class="koboSpan" id="kobo.328.1">The values obtained for the errors are </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.330.1">
Absolute Error: 219289.76171875
Relative Error (%): 98.82368711976115</span></pre> <p><span class="koboSpan" id="kobo.331.1">As you can see, 2.6k dollars is a very small price for a house of 1,180 sqft, even though it only has 1 bathroom and is of an average grade (7). </span><span class="koboSpan" id="kobo.331.2">This means our error metrics have very large values </span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.332.1">suggesting a ~99% error rate. </span><span class="koboSpan" id="kobo.332.2">This means that either we are not evaluating our model properly (in this case, we just used 1 value, we might have been unlucky) or we only used the initialized parameters that did not give us an accurate estimation. </span><span class="koboSpan" id="kobo.332.3">We need to improve our model parameters using a process called </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">training</span></strong><span class="koboSpan" id="kobo.334.1"> to improve our evaluation metrics. </span><span class="koboSpan" id="kobo.334.2">We will explore these topics in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">next recipes.</span></span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.336.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.337.1">Regression models</span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.338.1"> can be as complex as the model designer wants. </span><span class="koboSpan" id="kobo.338.2">They can have as many layers as necessary to model adequately the relationships between the input features and the desired </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">output values.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">In this recipe, we described how a biological neuron works inside our brain and simplified it to derive a simple mathematical model that we could use for our regression problem. </span><span class="koboSpan" id="kobo.340.2">In our case, we only used one layer, typically called the input layer, and we defined the weights and bias as </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">its parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.342.1">Moreover, we learned how to initialize our model, explored the effect initialization has on the weights and bias, and saw how we can use our data to evaluate the model. </span><span class="koboSpan" id="kobo.342.2">We will develop all these topics further in the </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">next recipes.</span></span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.344.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.345.1">In this recipe, we briefly introduced several topics. </span><span class="koboSpan" id="kobo.345.2">We started by describing Rosenblatt’s Perceptron. </span><span class="koboSpan" id="kobo.345.3">If you want to read the original paper, here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">link: </span></span><a href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.347.1">https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.348.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">Although in this recipe and the following ones we are going to work with some equations, we will use libraries and code that will allow us to focus on the actual outputs and their relationships with the inputs. </span><span class="koboSpan" id="kobo.349.2">However, for the interested reader, here is a </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">refresher: </span></span><a href="https://machinelearningmastery.com/gentle-introduction-linear-algebra/"><span class="No-Break"><span class="koboSpan" id="kobo.351.1">https://machinelearningmastery.com/gentle-introduction-linear-algebra/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.352.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">Moreover, we analyzed the input features in more detail, specifically working with </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">grade</span></strong><span class="koboSpan" id="kobo.355.1">, a categorical feature, using one-hot encoding. </span><span class="koboSpan" id="kobo.355.2">There are different ways to work with categorical data, which are explored at this </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">link: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63</span></span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.359.1">For more information regarding initialization and evaluation, please continue reading the recipes that follow in </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">this chapter.</span></span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.361.1">Defining loss functions and evaluation metrics for regression</span></h1>
<p><span class="koboSpan" id="kobo.362.1">In the </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.363.1">previous recipe, we defined our input features, described our model, and initialized it. </span><span class="koboSpan" id="kobo.363.2">At that point, we passed the</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.364.1"> features vector of a house to predict the price, calculated the output, and compared it against the </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">expected output.</span></span></p>
<p><span class="koboSpan" id="kobo.366.1">At the end of the previous recipe, the comparison of the expected output and the actual output of the model intuitively provided us with an idea of how good our model was. </span><span class="koboSpan" id="kobo.366.2">This is what it means to “evaluate” our model: we assessed the model’s performance. </span><span class="koboSpan" id="kobo.366.3">However, that evaluation is not complete for several reasons, as we did not correctly take into account </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">several factors:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.368.1">We only evaluated the model on one house – what about the others? </span><span class="koboSpan" id="kobo.368.2">How can we take all houses into account in </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">our evaluation?</span></span></li>
<li><span class="koboSpan" id="kobo.370.1">Is the difference between values an accurate measurement of model error? </span><span class="koboSpan" id="kobo.370.2">What other operations </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">make sense?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.372.1">In this recipe, we will cover how we can assess (that is, evaluate) the performance of our models, and study functions that are suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">this matter.</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">Furthermore, we will introduce a concept that will be very important when we optimize (i.e., train) our models: </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">loss functions.</span></span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.376.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.377.1">Before defining </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.378.1">some useful functions for evaluating regression models and computing their losses, let’s specify two required and three </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.379.1">desirable properties of the functions that we’ll use to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">our model:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.381.1">[Required] </span><strong class="bold"><span class="koboSpan" id="kobo.382.1">Continuous</span></strong><span class="koboSpan" id="kobo.383.1">: Obviously, we would like our evaluation functions to not be undefined on some potential error value, which will allow us to use these functions on a large set of pairs (expected output, actual </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">model output).</span></span></li>
<li><span class="koboSpan" id="kobo.385.1">[Required] </span><strong class="bold"><span class="koboSpan" id="kobo.386.1">Symmetric</span></strong><span class="koboSpan" id="kobo.387.1">: This is easily explained with an example. </span><span class="koboSpan" id="kobo.387.2">Let’s say that the price of a house is 2.2 million dollars – we would like our evaluation function to evaluate the model in the same way whether the output was 2 million dollars or 2.4 million dollars, as both values are the same distance from the expected value, just in </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">different directions.</span></span></li>
<li><span class="koboSpan" id="kobo.389.1">[Desirable] </span><strong class="bold"><span class="koboSpan" id="kobo.390.1">Robust</span></strong><span class="koboSpan" id="kobo.391.1">: Again, this is easier to explain with an example. </span><span class="koboSpan" id="kobo.391.2">Taking the same example as in the previous point, imagine we have 2 outputs of 2.4 and 2.8 million dollars. </span><span class="koboSpan" id="kobo.391.3">The error is already going to be large when compared with the expected output of 2.2 million dollars, so we would not want to make the error even larger due to the loss/evaluation function. </span><span class="koboSpan" id="kobo.391.4">From a mathematical perspective, we don’t want the error to grow exponentially, or it could make the computations diverge to </span><strong class="bold"><span class="koboSpan" id="kobo.392.1">Not a Number (NaN</span></strong><span class="koboSpan" id="kobo.393.1">). </span><span class="koboSpan" id="kobo.393.2">With robust functions, large errors do not make </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">computations diverge.</span></span></li>
<li><span class="koboSpan" id="kobo.395.1">[Desirable] </span><strong class="bold"><span class="koboSpan" id="kobo.396.1">Differentiable</span></strong><span class="koboSpan" id="kobo.397.1">: This is the least intuitive of all the properties. </span><span class="koboSpan" id="kobo.397.2">Typically, we would aim to achieve error rates of as close as possible to zero. </span><span class="koboSpan" id="kobo.397.3">However, that is a theoretical scenario that only happens when we have enough data to describe a problem perfectly, and when the model is large enough that it can represent the mapping from the data to the output values. </span><span class="koboSpan" id="kobo.397.4">In reality, we can never be sure that we are complying with neither of these previous assumptions, and therefore, the unrealistic expectation of zero error evolves to become the minimum error possible for our data and our model. </span><span class="koboSpan" id="kobo.397.5">We can only detect the minimum values of a function by calculating its differential function, hence this </span><strong class="bold"><span class="koboSpan" id="kobo.398.1">differentiable</span></strong><span class="koboSpan" id="kobo.399.1"> property. </span><span class="koboSpan" id="kobo.399.2">A small stroke of luck, though, is that differentiability implies continuity, therefore if our function can satisfy property #4, it automatically satisfies </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">property #1.</span></span></li>
<li><span class="koboSpan" id="kobo.401.1">[Desirable] </span><strong class="bold"><span class="koboSpan" id="kobo.402.1">Simple</span></strong><span class="koboSpan" id="kobo.403.1">: The</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.404.1"> simpler the function that satisfies all the properties, the better, because that way we can understand the results more intuitively and it will not be costly, </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">computationally speaking.</span></span></li>
</ul>
<p class="callout- eading"><span class="koboSpan" id="kobo.406.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.407.1">Not only evaluation functions must satisfy these criteria. </span><span class="koboSpan" id="kobo.407.2">As we will see in the next recipe, they must also be satisfied by a very important function for training, the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.408.1">loss function</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">.</span></span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.410.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.411.1">Let’s discuss some </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.412.1">evaluation and loss functions and analyze their advantages and disadvantages. </span><span class="koboSpan" id="kobo.412.2">The functions we will describe are </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.414.1">Mean </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">absolute error</span></span></li>
<li><span class="koboSpan" id="kobo.416.1">Mean </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">squared error</span></span></li>
<li><span class="koboSpan" id="kobo.418.1">Smooth </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">L1 loss</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.420.1">Mean absolute error</span></h3>
<p><span class="koboSpan" id="kobo.421.1">The</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.422.1"> first function we are going to study is </span><em class="italic"><span class="koboSpan" id="kobo.423.1">almost</span></em><span class="koboSpan" id="kobo.424.1"> perfect according to the five properties described earlier. </span><span class="koboSpan" id="kobo.424.2">The intuitive idea of this function is to use the difference between values as an indicator of the distance or error between those values. </span><span class="koboSpan" id="kobo.424.3">We apply the </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">abs</span></strong><span class="koboSpan" id="kobo.426.1"> function, meaning absolute value, to make </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">it symmetrical:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.428.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.429.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.430.1">E</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.431.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.432.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.433.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.434.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.435.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.436.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.437.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.438.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.439.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.440.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.441.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.442.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.443.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.444.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.445.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.446.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.447.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.448.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.449.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.450.1">j</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.451.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.452.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.453.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.454.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.455.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.456.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.457.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.458.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.459.1">|</span></span></span></p>
<p><span class="koboSpan" id="kobo.460.1">When plotted, the function produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.462.1"><img alt="Figure 3.7 – ﻿MAE graph" src="image/B16591_03_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.463.1">Figure 3.7 – MAE graph</span></p>
<p><span class="koboSpan" id="kobo.464.1">If we analyze </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.465.1">this function according to the properties defined previously, we see that all properties except #4 are fulfilled; unfortunately, the function is not differentiable at point 0. </span><span class="koboSpan" id="kobo.465.2">As we will see in the next recipe, this is particularly challenging when this function is used as a loss function. </span><span class="koboSpan" id="kobo.465.3">However, when evaluating our models, this is not required as the evaluation process does not need differentiation, and </span><strong class="bold"><span class="koboSpan" id="kobo.466.1">Mean Absolute Error</span></strong><span class="koboSpan" id="kobo.467.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.468.1">MAE</span></strong><span class="koboSpan" id="kobo.469.1">) is considered a typical </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.470.1">regression metric </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">in evaluation.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.472.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.473.1">This function can be used for evaluation and can also be used as a loss function (by itself or as a regularization term). </span><span class="koboSpan" id="kobo.473.2">In this case, it is common to call it L1 loss or term explain how this relates to the rest of the sentence, as it computes the L1 distance of the vectors corresponding to the expected output and the </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">actual output.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">Another similar metric</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.476.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.477.1">Mean Absolute Percentage Error </span></strong><span class="koboSpan" id="kobo.478.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.479.1">MAPE</span></strong><span class="koboSpan" id="kobo.480.1">). </span><span class="koboSpan" id="kobo.480.2">In this metric, each output error is normalized by the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">expected output:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.482.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.483.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.484.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.485.1">E</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.486.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.487.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.488.1">100</span></span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.489.1">%</span></span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.490.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.491.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.492.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.493.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.494.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.495.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.496.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.497.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.498.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.499.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.500.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.501.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.502.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.503.1">|</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.504.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.505.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.506.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.507.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.508.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.509.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.510.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.511.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.512.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.513.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.514.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.515.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.516.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.517.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.518.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.519.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.520.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.521.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.522.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.523.1">|</span></span></span></p>
<h3><span class="koboSpan" id="kobo.524.1">Mean squared error</span></h3>
<p><span class="koboSpan" id="kobo.525.1">To </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.526.1">solve the differentiability issue, the </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">Mean Squared Error </span></strong><span class="koboSpan" id="kobo.528.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.529.1">MSE</span></strong><span class="koboSpan" id="kobo.530.1">)</span><strong class="bold"> </strong><span class="koboSpan" id="kobo.531.1">function</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.532.1"> is very similar to the MAE, but increases the order from 1 to 2 in the difference term. </span><span class="koboSpan" id="kobo.532.2">The intuitive idea is to use the simplest quadratic differentiable </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">function (</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.534.1">x</span></em></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.535.1">2</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.537.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.538.1">S</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.539.1">E</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.540.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.541.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.542.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.543.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.544.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.545.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.546.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.547.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.548.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.549.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.550.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.551.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.552.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.553.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.556.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.557.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.558.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.559.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.560.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.561.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.562.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.563.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1">Y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.565.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.566.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.567.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.568.1">2</span></span></p>
<p><span class="koboSpan" id="kobo.569.1">When plotted, the function produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.571.1"><img alt="Figure 3.8 – ﻿MSE graph" src="image/B16591_03_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.572.1">Figure 3.8 – MSE graph</span></p>
<p><span class="koboSpan" id="kobo.573.1">If we analyze </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.574.1">this function according to the properties defined, we see that all properties except #3 are fulfilled. </span><span class="koboSpan" id="kobo.574.2">Unfortunately, the function is not as robust as the MAE. </span><span class="koboSpan" id="kobo.574.3">Large errors grow exponentially, and therefore, this evaluation function is much more susceptible to outliers, as one single data point with a very large error can cause the squared error for that value to be quite large, and therefore it will make a large contribution</span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.575.1"> to the MSE, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">erroneous conclusions.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.577.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.578.1">This function can be used for loss functions (by itself or as a regularization term). </span><span class="koboSpan" id="kobo.578.2">In this case. </span><span class="koboSpan" id="kobo.578.3">It is common to call it L2 loss or term (ridge regression), as it computes the L2 distance of the vectors corresponding to the expected output and the </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">actual output.</span></span></p>
<p><span class="koboSpan" id="kobo.580.1">In order to have the same units as the output variable, the MSE can have the squared root applied. </span><span class="koboSpan" id="kobo.580.2">This evaluation</span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.581.1"> metric is called </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">Root Mean Squared </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.583.1">Error</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.584.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.585.1">RMSE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.587.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.588.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.589.1">S</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.590.1">E</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.591.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.592.1">√</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.593.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.594.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.595.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.596.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.597.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.598.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.599.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.600.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.601.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.602.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.603.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.604.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.605.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.606.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.607.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.608.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.609.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.610.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.611.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.612.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.613.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.614.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.615.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.616.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.617.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.618.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.619.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.620.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.621.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.622.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.623.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.624.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.625.1"> </span></span></p>
<h3><span class="koboSpan" id="kobo.626.1">Smooth mean absolute error/smooth L1 loss</span></h3>
<p><span class="koboSpan" id="kobo.627.1">Can’t we </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.628.1">have the best of both worlds? </span><span class="koboSpan" id="kobo.628.2">Of course </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">we can!</span></span></p>
<p><span class="koboSpan" id="kobo.630.1">By combining both functions – the MSE for small values of the error and the MAE for the large values of the error – we get </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.632.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.633.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.634.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.635.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.636.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.637.1">h</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.638.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.639.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.640.1">L</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.641.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.642.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.643.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.644.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.645.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.646.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.647.1">{</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.648.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.649.1">0.5</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.650.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.651.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.652.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.653.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.654.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.655.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.656.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.657.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.658.1">&lt;</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.659.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.660.1">  </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.661.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.662.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.663.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.664.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.665.1">0.5</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.666.1">o</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.667.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.668.1">h</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.669.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.670.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.671.1">w</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.672.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.673.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.674.1">e</span></span></span></p>
<p><span class="koboSpan" id="kobo.675.1">When plotted, the function produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.677.1"><img alt="Figure 3.9 – Smooth mean absolute error graph" src="image/B16591_03_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.678.1">Figure 3.9 – Smooth mean absolute error graph</span></p>
<p><span class="koboSpan" id="kobo.679.1">If we </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.680.1">analyze this function according to the properties defined, we see that all properties </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">are fulfilled.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.682.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.683.1">This function can be used for loss functions. </span><span class="koboSpan" id="kobo.683.2">In this case, it is common to call it smooth </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">L1 loss.</span></span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.685.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.686.1">In the first recipe of this chapter, we designed our first regression model based on a simple biological neuron. </span><span class="koboSpan" id="kobo.686.2">We initialized its parameters randomly and performed our first naive evaluation. </span><span class="koboSpan" id="kobo.686.3">The result was not good, and we conjectured that this was due to two reasons: our evaluation mechanism was not robust enough and our model parameters had not </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">been optimized.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">In this recipe, we explored how to improve on the first reason: evaluation. </span><span class="koboSpan" id="kobo.688.2">We covered three of the most important evaluation metrics and mentioned their relationship with loss functions, which we will explore in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">next recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.690.1">Moreover, we discussed which evaluation metrics are better, exploring how the MAE is robust but unfortunately not differentiable, and the MSE is differentiable but allows outliers to steer the metric (which is not ideal). </span><span class="koboSpan" id="kobo.690.2">We concluded the recipe by combining the functions to get the best of </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">both worlds.</span></span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.692.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.693.1">A very interesting set of evaluation functions that we did not explore in this recipe is the coefficient of determination and its extensions. </span><span class="koboSpan" id="kobo.693.2">However, this set is only used for linear regression modeling. </span><span class="koboSpan" id="kobo.693.3">More information can be found at the </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">following links:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.695.1">Coefficient of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.696.1">determination</span></strong></span><span class="No-Break"><strong class="bold"> </strong></span><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><span class="No-Break"><span class="koboSpan" id="kobo.697.1">https://en.wikipedia.org/wiki/Coefficient_of_determination</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.698.1">Statistics By </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.699.1">Jim </span></strong></span><a href="https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/"><span class="No-Break"><span class="koboSpan" id="kobo.700.1">https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.701.1">Furthermore, there are generally many different functions that can be used for evaluation and loss in regression problems; you can refer to this link for further </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">details: </span></span><a href="https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html"><span class="No-Break"><span class="koboSpan" id="kobo.703.1">https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html</span></span></a></p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.704.1">Training regression models</span></h1>
<p><span class="koboSpan" id="kobo.705.1">In supervised learning, training</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.706.1"> is the process of optimizing the parameters of a model towards a specific objective. </span><span class="koboSpan" id="kobo.706.2">It is typically the most complex and the most time-consuming step in solving a deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">problem statement.</span></span></p>
<p><span class="koboSpan" id="kobo.708.1">In this recipe, we will visit the basic concepts involved in training a model. </span><span class="koboSpan" id="kobo.708.2">We will apply them to solve the regression model we previously defined in this chapter, combined with the usage of the functions </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">we discussed.</span></span></p>
<p><span class="koboSpan" id="kobo.710.1">We will predict house prices using the dataset seen in </span><em class="italic"><span class="koboSpan" id="kobo.711.1">Recipe 1, Toy dataset for regression – load, manage, and visualize house sales dataset</span></em><span class="koboSpan" id="kobo.712.1"> from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.713.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.714.1">, Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.715.1">and DataLoader.</span></em></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.716.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.717.1">There are a number </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.718.1">of concepts that we should get familiar with to understand this recipe. </span><span class="koboSpan" id="kobo.718.2">These concepts define how the training </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">will proceed:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.720.1">Loss function</span></strong><span class="koboSpan" id="kobo.721.1">: The </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.722.1">training process is an iterative optimization process. </span><span class="koboSpan" id="kobo.722.2">As the training progresses, the model is expected to perform better against an operation that compares the expected output with the actual output of the model. </span><span class="koboSpan" id="kobo.722.3">That operation is the loss function, also known as the objective, target, or cost function, which is being optimized during the </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">training process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.724.1">Optimizer</span></strong><span class="koboSpan" id="kobo.725.1">: With</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.726.1"> each iteration of the training process, each parameter of the model is updated by a quantity (calculated using the loss function). </span><span class="koboSpan" id="kobo.726.2">The optimizer is an algorithm that defines how that quantity is calculated. </span><span class="koboSpan" id="kobo.726.3">The most important hyperparameter of the optimizer is the </span><strong class="bold"><span class="koboSpan" id="kobo.727.1">learning rate</span></strong><span class="koboSpan" id="kobo.728.1">, which</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.729.1"> is the multiplier applied to the calculated quantity to update </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">the parameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.731.1">Dataset split</span></strong><span class="koboSpan" id="kobo.732.1">: Stopping </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.733.1">the training when the model can perform at its best in the real world is critical to achieving success in deep learning projects. </span><span class="koboSpan" id="kobo.733.2">One method to accomplish this is to split our dataset into training, validation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">test sets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.735.1">Epochs</span></strong><span class="koboSpan" id="kobo.736.1">: This</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.737.1"> is the number of iterations for which the training process </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">will run.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.739.1">Batch size</span></strong><span class="koboSpan" id="kobo.740.1">: The</span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.741.1"> number of training samples analyzed at a time to produce an estimation of </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">the gradient.</span></span></li>
</ul>
<h2 id="_idParaDest-66"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.743.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.744.1">In this recipe, we </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.745.1">will create our own training loop and evaluate how each hyperparameter influences the training. </span><span class="koboSpan" id="kobo.745.2">To achieve this, we will follow </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.747.1">Improving </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">the model</span></span></li>
<li><span class="koboSpan" id="kobo.749.1">Defining the loss function </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">and optimizer</span></span></li>
<li><span class="koboSpan" id="kobo.751.1">Splitting </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">our dataset</span></span></li>
<li><span class="koboSpan" id="kobo.753.1">Analyzing fairness </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">and diversity</span></span></li>
<li><span class="koboSpan" id="kobo.755.1">Defining the number of epochs and the </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">batch size</span></span></li>
<li><span class="koboSpan" id="kobo.757.1">Putting everything together for a </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">training loop</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.759.1">Improving the model</span></h3>
<p><span class="koboSpan" id="kobo.760.1">To solve this problem, the </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.761.1">architecture we explored in the previous recipes (Perceptron) will not be enough. </span><span class="koboSpan" id="kobo.761.2">We will stack several Perceptrons together and connect them through different layers. </span><span class="koboSpan" id="kobo.761.3">This architecture is known</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.762.1"> as a </span><strong class="bold"><span class="koboSpan" id="kobo.763.1">Multi-Layer Perceptron</span></strong><span class="koboSpan" id="kobo.764.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.765.1">MLP</span></strong><span class="koboSpan" id="kobo.766.1">). </span><span class="koboSpan" id="kobo.766.2">We will define a network architecture with three hidden layers that are fully connected (dense) and use </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.767.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">ReLU</span></strong><span class="koboSpan" id="kobo.769.1"> activation function (introduced in the first recipe of the chapter) with 128, 1,024, and 128 neurons, respectively, and an output layer with the corresponding 1 single output. </span><span class="koboSpan" id="kobo.769.2">The last layer is left without an activation function, also called the linear activation function (</span><em class="italic"><span class="koboSpan" id="kobo.770.1">y = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.771.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.773.1">Furthermore, the house sales dataset is a very complex problem, and finding solutions that generalize well (i.e., work sufficiently well in the real world) isn’t easy. </span><span class="koboSpan" id="kobo.773.2">For this purpose, we have included two new advanced features in </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">the model:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.775.1">Batch normalization</span></strong><span class="koboSpan" id="kobo.776.1">: With </span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.777.1">this step in the process, for each mini-batch, the input distribution is standardized. </span><span class="koboSpan" id="kobo.777.2">This helps with training convergence </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">and generalization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.779.1">Dropout</span></strong><span class="koboSpan" id="kobo.780.1">: This method</span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.781.1"> consists of disabling neurons of the network randomly (given a probability). </span><span class="koboSpan" id="kobo.781.2">This helps reduce overfitting (this concept will be explained in the next recipe) and </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">improve generalization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.783.1">Our code is </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.785.1">
def create_regression_network():
    # MultiLayer Perceptron Model (this time using Gluon)
    net = mx.gluon.nn.Sequential()
    net.add(mx.gluon.nn.Dense(128))
    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net.add(mx.gluon.nn.Activation('relu'))
    net.add(mx.gluon.nn.Dropout(.5))
    net.add(mx.gluon.nn.Dense(1024))
    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net.add(mx.gluon.nn.Activation('relu'))
    net.add(mx.gluon.nn.Dropout(.4))
    net.add(mx.gluon.nn.Dense(128))
    net.add(mx.gluon.nn.BatchNorm(axis=1, center=True, scale=True))
    net.add(mx.gluon.nn.Activation('relu'))
    net.add(mx.gluon.nn.Dropout(.3))
    net.add(mx.gluon.nn.Dense(1))
     return net</span></pre> <p><span class="koboSpan" id="kobo.786.1">One important </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.787.1">thing to note is that the input to our model has also </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">been modified:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.789.1">The numerical inputs have been scaled to produce inputs with zero mean and unit variance. </span><span class="koboSpan" id="kobo.789.2">This improves the convergence of the </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">training algorithm.</span></span></li>
<li><span class="koboSpan" id="kobo.791.1">The categorial input (grade) has been one-hot encoded. </span><span class="koboSpan" id="kobo.791.2">We introduced this concept in </span><em class="italic"><span class="koboSpan" id="kobo.792.1">Recipe 4, Toy dataset for text tasks – load, manage, and visualize enron emails dataset</span></em><span class="koboSpan" id="kobo.793.1"> from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.794.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.795.1">, Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.796.1">and DataLoader</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.797.1">This increases the number of features to 30. </span><span class="koboSpan" id="kobo.797.2">As the dataset contains ~20k rows, this provides around 600k data points. </span><span class="koboSpan" id="kobo.797.3">Let’s compare this to the number of parameters in </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">our model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.799.1">
[...]
Parameters in forward computation graph, duplicate included
   Total params: 272513
   Trainable params: 269953
   Non-trainable params: 2560
Shared params in forward computation graph: 0
Unique parameters in model: 272513</span></pre> <p><span class="koboSpan" id="kobo.800.1">The number</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.801.1"> of trainable parameters in our model is ~270k. </span><span class="koboSpan" id="kobo.801.2">The number of data points is approximately two times the number of trainable parameters in our model. </span><span class="koboSpan" id="kobo.801.3">Typically, this is the minimum for a successful model and ideally, we would like to work with a dataset size of ~10 times the number of the parameters of </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">the model.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.803.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.804.1">Even though the comparison between the data points available and the number of parameters of the model is a very useful one, different architectures have different requirements in terms of data. </span><span class="koboSpan" id="kobo.804.2">As usual, experimentation (trial and error) is the key to finding the </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">right balance.</span></span></p>
<p><span class="koboSpan" id="kobo.806.1">The last important point about our model is the initialization method, as with multilayer networks, random initialization might not yield the best results. </span><span class="koboSpan" id="kobo.806.2">The most common methods nowadays are </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.808.1">Xavier initialization</span></strong><span class="koboSpan" id="kobo.809.1">: Takes</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.810.1"> into account the number of inputs and the number of outputs when computing </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">the variance</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.812.1">MSRA PReLU</span></strong><span class="koboSpan" id="kobo.813.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.814.1">Kaiming initialization</span></strong><span class="koboSpan" id="kobo.815.1">: The Xavier initialization method has some issues </span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.816.1">with ReLU activation functions, so </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.817.1">this method </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">is preferred</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.819.1">MXNet provides very simple access to these functionalities, in this case, for MSRA </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">PReLU initialization:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.821.1">
net.collect_params().initialize(mx.init.MSRAPrelu(), ctx=ctx, force_reinit=True)</span></pre> <p class="callout- eading"><span class="koboSpan" id="kobo.822.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.823.1">Initialization methods provide values for the weights and bias so that the model activation functions are not initialized on saturated (flat) regions. </span><span class="koboSpan" id="kobo.823.2">The intuition is to have these weights and biases with zero mean and unit variance. </span><span class="koboSpan" id="kobo.823.3">For the statistical analysis, a link is provided in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.824.1">There’s more...</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.826.1">Defining the loss function and optimizer</span></h3>
<p><span class="koboSpan" id="kobo.827.1">As we saw in the previous recipe, the smooth L1 (also known as Huber) loss function will work </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">quite well.</span></span></p>
<p><span class="koboSpan" id="kobo.829.1">There are several</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.830.1"> optimizers that have been proven to </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.831.1">work well for </span><strong class="bold"><span class="koboSpan" id="kobo.832.1">supervised </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.833.1">learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.834.1"> problems:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.835.1">Gradient descent</span></strong><span class="koboSpan" id="kobo.836.1">: The</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.837.1"> intuitive idea behind this method (and all optimizers) is that the gradient (the directional derivative toward the coordinate axes) provides an indicator of how to modify the model parameters to minimize the loss. </span><span class="koboSpan" id="kobo.837.2">To calculate it, this method computes all the data in the dataset, and therefore it is very robust, but also time-consuming. </span><span class="koboSpan" id="kobo.837.3">If you only take one random data point for the calculation, it is called </span><strong class="bold"><span class="koboSpan" id="kobo.838.1">Stochastic Gradient Descent</span></strong><span class="koboSpan" id="kobo.839.1">, which</span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.840.1"> is much faster, but also very noisy. </span><span class="koboSpan" id="kobo.840.2">To get the best of both worlds, we can use </span><strong class="bold"><span class="koboSpan" id="kobo.841.1">Mini-Batch Gradient Descent</span></strong><span class="koboSpan" id="kobo.842.1">, where</span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.843.1"> we define the size of a batch and calculate the gradients of that batch. </span><span class="koboSpan" id="kobo.843.2">This is connected to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">batch_size</span></strong><span class="koboSpan" id="kobo.845.1"> parameter when we worked </span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.846.1">with </span><strong class="bold"><span class="koboSpan" id="kobo.847.1">DataLoader </span></strong><span class="koboSpan" id="kobo.848.1">in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.849.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.850.1">, Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.851.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.853.1">Momentum/Nesterov accelerated gradient</span></strong><span class="koboSpan" id="kobo.854.1">: Gradient descent can have a problem </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.855.1">with stability and can start jumping and getting trapped in local minima. </span><span class="koboSpan" id="kobo.855.2">One method to avoid these issues is to consider the past steps that the algorithm has taken, which is achieved with these </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">two optimizers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.857.1">Adagrad</span></strong><span class="koboSpan" id="kobo.858.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.859.1">Adadelta</span></strong><span class="koboSpan" id="kobo.860.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.861.1">RMSprop</span></strong><span class="koboSpan" id="kobo.862.1">: GD uses the same learning rate for all parameters, without taking into account the frequency with which they are updated. </span><span class="koboSpan" id="kobo.862.2">Adagrad </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.863.1">and these other optimizers deal with this issue by adjusting the learning rate by parameter. </span><span class="koboSpan" id="kobo.863.2">However, Adagrad learning rates decrease over time and may yield values close to zero that do not perform any further updates. </span><span class="koboSpan" id="kobo.863.3">To solve this</span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.864.1"> problem, Adadelta and RMSprop </span><a id="_idIndexMarker251"/><span class="No-Break"><span class="koboSpan" id="kobo.865.1">were developed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.866.1">Adam</span></strong><span class="koboSpan" id="kobo.867.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.868.1">AdaMax</span></strong><span class="koboSpan" id="kobo.869.1">/</span><strong class="bold"><span class="koboSpan" id="kobo.870.1">Nadam</span></strong><span class="koboSpan" id="kobo.871.1">: These state-of-the-art optimizers combine both of the improvements </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.872.1">from gradient descent: past-step calculations and adaptive learning rates. </span><span class="koboSpan" id="kobo.872.2">Adam </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.873.1">uses the L2 norm for the</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.874.1"> exponentially </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.875.1">weighted average of the gradients, whereas AdaMax uses the</span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.876.1"> infinity norm (max operation). </span><span class="koboSpan" id="kobo.876.2">Nadam replaces the momentum component in Adam with the Nesterov momentum to </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">accelerate convergence.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.878.1">MXNet and Gluon provides very simple interfaces to define the loss function and the optimizer. </span><span class="koboSpan" id="kobo.878.2">With the following two lines of code, we are choosing the Huber loss function and the </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">Adam optimizer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.880.1">
# Define Loss Function
loss_fn = mx.gluon.loss.HuberLoss()
# Define Optimizer and Hyper Parameters
trainer = mx.gluon.Trainer(net.collect_params(), "adam", {"learning_rate": 0.01})</span></pre> <h3><span class="koboSpan" id="kobo.881.1">Splitting our dataset</span></h3>
<p><span class="koboSpan" id="kobo.882.1">One of the </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.883.1">most important things to consider in all data science projects is the performance of a trained model on data outside the</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.884.1"> dataset we are going to work with. </span><span class="koboSpan" id="kobo.884.2">In supervised learning, for training and evaluation, we work with data knowing the desired (expected) output, so how can we make sure that when using our model on new data, without a known output, it is going to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">as expected?</span></span></p>
<p><span class="koboSpan" id="kobo.886.1">We deal with this issue by splitting our dataset into </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">three parts:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.888.1">Training set</span></strong><span class="koboSpan" id="kobo.889.1">: The</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.890.1"> training set is used during training to compute the updates of the </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">model parameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.892.1">Validation set</span></strong><span class="koboSpan" id="kobo.893.1">: The </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.894.1">validation set is used during training to check every epoch how the model has improved (or not) with those updates </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">previously calculated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.896.1">Test set</span></strong><span class="koboSpan" id="kobo.897.1">: Finally, once </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.898.1">the training has finished, we can compute its performance on </span><em class="italic"><span class="koboSpan" id="kobo.899.1">unseen data</span></em><span class="koboSpan" id="kobo.900.1">, which is the test set, the only part of the dataset that was not used to improve the model </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">during training.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.902.1">Furthermore, in order to have stable training that will allow our model to work properly with data outside our dataset, the data split needs to be computed taking into account </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.904.1">Size of the splits</span></strong><span class="koboSpan" id="kobo.905.1">: This depends on the amount of data available and the task. </span><span class="koboSpan" id="kobo.905.2">Typical percentage splits for training/validation/test data are 60/20/20 </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">or 80/10/10.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.907.1">Which data points to select for each of the splits</span></strong><span class="koboSpan" id="kobo.908.1">: The key here is to have a balanced dataset. </span><span class="koboSpan" id="kobo.908.2">For example, in our house prices dataset, we do not want to have only houses with two and three bedrooms on the training set, then four-bedroom houses in the validation set, and finally houses with five or more bedrooms in </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.909.1">the test set. </span><span class="koboSpan" id="kobo.909.2">Ideally, each set should be an accurate representation of the dataset. </span><span class="koboSpan" id="kobo.909.3">This is very important for sensitive datasets where fairness and diversity must </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">be considered.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.911.1">We can </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.912.1">achieve these splits easily, in this case, using a function from the well-known </span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">library </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.914.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.916.1">
# Dataset Split 80/10/10
from sklearn.model_selection import train_test_split
full_train_df, test_df = train_test_split(house_df, test_size=0.2, random_state=42)
# To match correctly 10% size, we use previous size as reference
train_df, val_df = train_test_split(full_train_df, test_size=len(test_df), random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.917.1">In the </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.918.1">preceding code snippet, we do our three-way split for train, validation, and test data in </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">two steps:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.920.1">We assign 20% of the dataset to the </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">test set.</span></span></li>
<li><span class="koboSpan" id="kobo.922.1">The remaining 80% will be split equally between the validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">test sets.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.924.1">Analyzing fairness and diversity</span></h3>
<p><span class="koboSpan" id="kobo.925.1">Imagine </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.926.1">for a moment that we work for a real-estate website, and we manage the data science team. </span><span class="koboSpan" id="kobo.926.2">There is a compelling feature that will drive traffic to our website: when homeowners want to sell a property, they can fill in some data about their house and will be able to see a machine learning-optimized estimate of the price at which they should put their house up for selling, with data indicating that it will be sold within the next 3 months at that price. </span><span class="koboSpan" id="kobo.926.3">This feature sounds really cool as homeowners can fine-tune the asking price of the house they want to sell, and potential buyers will see reasonable prices according </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">to market.</span></span></p>
<p><span class="koboSpan" id="kobo.928.1">However, we suddenly realize we do not have enough data yet for houses with two or fewer bathrooms, and we know that feature is highly sensitive for our model. </span><span class="koboSpan" id="kobo.928.2">Deploying this model for real-life properties would mean that houses with two or fewer bathrooms could be valued closer to valuations of houses with more bathrooms by our model, simply because that is all the data our model has been able to see. </span><span class="koboSpan" id="kobo.928.3">This would mean that for the cheapest houses, the most affordable ones for low-income families, we will be increasing their prices </span><em class="italic"><span class="koboSpan" id="kobo.929.1">unfairly</span></em><span class="koboSpan" id="kobo.930.1">, which would be a </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">serious problem.</span></span></p>
<p><span class="koboSpan" id="kobo.932.1">Our model cannot know better because we have not shown it better. </span><span class="koboSpan" id="kobo.932.2">In this scenario, what options do we have? </span><span class="koboSpan" id="kobo.932.3">These are the ones that might suit a </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">real-world situation:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.934.1">Be confident about the robustness of our model and deploy it in </span><span class="No-Break"><span class="koboSpan" id="kobo.935.1">production regardless.</span></span></li>
<li><span class="koboSpan" id="kobo.936.1">Convince business leaders not to deploy the model until we have all the data </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">we need.</span></span></li>
<li><span class="koboSpan" id="kobo.938.1">Deploy the model in production, but only allow sellers to use it for houses with three bathrooms </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">or more.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.940.1">The first option is </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.941.1">the least adequate of all, however, it is actually the most frequently applied one, due to </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.943.1">It is inconvenient to work on a project for several months and delay it right before it is expected to launch. </span><span class="koboSpan" id="kobo.943.2">Management typically does not expect nor want this kind of news and it could put some jobs </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">at risk.</span></span></li>
<li><span class="koboSpan" id="kobo.945.1">However, the most common reason for this to happen is that the error in the data goes unnoticed. </span><span class="koboSpan" id="kobo.945.2">There is never enough time to verify that the data is accurate/fair/diverse enough and the focus shifts to delivering an optimized model as soon </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">as possible.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.947.1">The second option is difficult to argue for a similar reason as the first option, and the third option might look good on paper, but is actually very dangerous. </span><span class="koboSpan" id="kobo.947.2">If I found myself in that situation, I would not choose the third option, simply because we cannot be sure that the data is diverse and fair across all features, so a proper data quality assessment is required. </span><span class="koboSpan" id="kobo.947.3">If we found this kind of error this late in a project, it is because not enough focus was put on data quality. </span><span class="koboSpan" id="kobo.947.4">This typically happens with companies that have been recording or storing large amounts of data and now want to do some machine learning project with it, instead of designing data collection operations with a clear objective. </span><span class="koboSpan" id="kobo.947.5">This is one of the most common reasons why machine learning projects fail in </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">such companies.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">Let us take a look at how our dataset looks from the point of view of fairness </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">and diversity:</span></span></p>
<p><span class="koboSpan" id="kobo.951.1">First, as seen in </span><em class="italic"><span class="koboSpan" id="kobo.952.1">Recipe 1, Toy dataset for regression – load, manage and visualize house sales dataset</span></em><span class="koboSpan" id="kobo.953.1"> from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.954.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.955.1">, Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.956.1">, we will start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">price distribution:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.958.1"><img alt="Figure 3.10 – Price distribution across the training, validation, and test sets" src="image/B16591_03_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.959.1">Figure 3.10 – Price distribution across the training, validation, and test sets</span></p>
<p><span class="koboSpan" id="kobo.960.1">Although we </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.961.1">can see a small dip in houses with prices lower than $500k, the price distribution is fairly represented across the three datasets and no manual modifications </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">are necessary.</span></span></p>
<p><span class="koboSpan" id="kobo.963.1">The living space in square foot looks </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.965.1"><img alt="Figure 3.11 – Living Sqft plots for the training, validation, and test sets" src="image/B16591_03_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.966.1">Figure 3.11 – Living Sqft plots for the training, validation, and test sets</span></p>
<p><span class="koboSpan" id="kobo.967.1">The largest</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.968.1"> differences we can see here are due to a very small number of high-valued houses. </span><span class="koboSpan" id="kobo.968.2">We can even consider these outliers, and if the parameters of our training are well chosen, this should not harm our </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">prediction capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.970.1">The number of bathrooms looks </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.972.1"><img alt="Figure 3.12 – Number of bathrooms across the training, validation, and test sets" src="image/B16591_03_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.973.1">Figure 3.12 – Number of bathrooms across the training, validation, and test sets</span></p>
<p><span class="koboSpan" id="kobo.974.1">This distribution is </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.975.1">quite well represented in our validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">test sets.</span></span></p>
<p><span class="koboSpan" id="kobo.977.1">Grade looks </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.979.1"><img alt="Figure 3.13 – Grade for training, validation, and test sets" src="image/B16591_03_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.980.1">Figure 3.13 – Grade for training, validation, and test sets</span></p>
<p><span class="koboSpan" id="kobo.981.1">This distribution</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.982.1"> is also well represented in our validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">test sets.</span></span></p>
<p><span class="koboSpan" id="kobo.984.1">Grouping all the individual analyses, we can conclude that our training, validation, and test sets are fairly good representations of our full dataset. </span><span class="koboSpan" id="kobo.984.2">We must remember though that our dataset has its own limitations (although I have to say, for the selected features, it is quite </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">well represented):</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.986.1">Price: [</span><span class="No-Break"><span class="koboSpan" id="kobo.987.1">75k$, 7.7M$]</span></span></li>
<li><span class="koboSpan" id="kobo.988.1">Living Sqft: [</span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">290, 13540]</span></span></li>
<li><span class="koboSpan" id="kobo.990.1">Number of Bathrooms [</span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">0, 8]</span></span></li>
<li><span class="koboSpan" id="kobo.992.1">Grade: [1, 13], but lacking 2 as we pointed </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">out earlier</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.994.1">Defining the number of epochs and batch size</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.995.1">Number of epochs</span></strong><span class="koboSpan" id="kobo.996.1"> refers</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.997.1"> to the number of iterations the training algorithm will run. </span><span class="koboSpan" id="kobo.997.2">Depending on the complexity of the problem and the optimizer and hyperparameters chosen, this number can vary from very low (say 5-10) to very high (thousands </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">of iterations).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.999.1">Batch size</span></strong><span class="koboSpan" id="kobo.1000.1"> refers </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.1001.1">to the number of training samples analyzed at the same time to estimate the error gradient. </span><span class="koboSpan" id="kobo.1001.2">In </span><em class="italic"><span class="koboSpan" id="kobo.1002.1">Recipe 3, Toy dataset for image tasks – load, manage and visualize iris dataset</span></em><span class="koboSpan" id="kobo.1003.1"> in </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1004.1">Chapter 2</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.1005.1">, Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.1006.1">, we introduced this concept as a means to optimize memory usage; the smaller the batch size, the less memory required. </span><span class="koboSpan" id="kobo.1006.2">Furthermore, this speeds up computation of the gradient; the larger the batch size, the faster computations will run (if memory permits). </span><span class="koboSpan" id="kobo.1006.3">Typical values range from 32 to </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">2,048 samples.</span></span></p>
<h3><span class="koboSpan" id="kobo.1008.1">Putting everything together for a training loop</span></h3>
<p><span class="koboSpan" id="kobo.1009.1">The training loop</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.1010.1"> is the iterative process that runs the optimizer to calculate/estimate the gradient so that on each iteration, the error computed from the loss function (the objective of the optimizer) is reduced. </span><span class="koboSpan" id="kobo.1010.2">As mentioned, each iteration is called an epoch. </span><span class="koboSpan" id="kobo.1010.3">And for each iteration, the full training set is accessed in batches to compute </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">the gradient.</span></span></p>
<p><span class="koboSpan" id="kobo.1012.1">Furthermore, as we will see, it is interesting to compute the loss function for the validation set. </span><span class="koboSpan" id="kobo.1012.2">In our case, we will also compute the loss function for the test set, as it will provide us with specific details on how our model </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">will perform.</span></span></p>
<p><span class="koboSpan" id="kobo.1014.1">In order to understand the difference in behavior when modifying the hyperparameters, we are going to run the training loop for our house price prediction dataset several times, modifying just one hyperparameter per table and keeping the rest of the variables constant (unless </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">otherwise noted).</span></span></p>
<h4><span class="koboSpan" id="kobo.1016.1">Optimizer and learning rate</span></h4>
<p><span class="koboSpan" id="kobo.1017.1">As discussed earlier, the chosen</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.1018.1"> optimizer for the training loop and the learning rate are intimately related, as for some optimizers (such as SGD) the learning rate is kept constant, whereas for others (such as Adam) it varies from </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.1019.1">a given </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">starting point.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1021.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1022.1">The best optimizer depends on several factors, and nothing trumps trial and error. </span><span class="koboSpan" id="kobo.1022.2">I strongly suggest to try a few and see which one fits best. </span><span class="koboSpan" id="kobo.1022.3">In my experience, SGD and Adam are typically the ones that work best, including this problem, the prediction of </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">house prices.</span></span></p>
<p><span class="koboSpan" id="kobo.1024.1">Let’s analyze</span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.1025.1"> how the training loss and validation loss vary for the SGD optimizer by varying the </span><strong class="bold"><span class="koboSpan" id="kobo.1026.1">learning rate</span></strong><span class="koboSpan" id="kobo.1027.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1028.1">LR</span></strong><span class="koboSpan" id="kobo.1029.1">) while keeping the other parameters constant: </span><em class="italic"><span class="koboSpan" id="kobo.1030.1">Epochs = 100, Batch Size = 128, Loss fn = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1031.1">HuberLoss</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.1033.1"><img alt="Figure 3.14 – Loss for the SGD optimizer when varying the learning rate" src="image/B16591_03_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1034.1">Figure 3.14 – Loss for the SGD optimizer when varying the learning rate</span></p>
<p><span class="koboSpan" id="kobo.1035.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1036.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1037.1">.14</span></em><span class="koboSpan" id="kobo.1038.1">, we can conclude that for the SGD optimizer, an LR value of between 10</span><span class="superscript"><span class="koboSpan" id="kobo.1039.1">-1</span></span><span class="koboSpan" id="kobo.1040.1"> and 1.0 is optimal. </span><span class="koboSpan" id="kobo.1040.2">Furthermore, we can see that for very large values of LR (&gt; 2.0) the algorithm diverges. </span><span class="koboSpan" id="kobo.1040.3">That’s why, when searching for optimal values for the LR, it’s better to </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">start small.</span></span></p>
<p><span class="koboSpan" id="kobo.1042.1">Let’s analyze how the training loss and validation loss vary for the Adam optimizer by varying the </span><strong class="bold"><span class="koboSpan" id="kobo.1043.1">learning rate</span></strong><span class="koboSpan" id="kobo.1044.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1045.1">LR</span></strong><span class="koboSpan" id="kobo.1046.1">) and </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.1047.1">keeping the other parameters constant: </span><em class="italic"><span class="koboSpan" id="kobo.1048.1">Epochs = 100, Batch Size = 128, Loss fn = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1049.1">HuberLoss</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.1051.1"><img alt="Figure 3.15 – Loss for Adam optimizer when varying the learning rate" src="image/B16591_03_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1052.1">Figure 3.15 – Loss for Adam optimizer when varying the learning rate</span></p>
<p><span class="koboSpan" id="kobo.1053.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1054.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1055.1">.15</span></em><span class="koboSpan" id="kobo.1056.1">, we can conclude that for the Adam optimizer, an LR value of between 10</span><span class="superscript"><span class="koboSpan" id="kobo.1057.1">-4</span></span><span class="koboSpan" id="kobo.1058.1"> and 10</span><span class="superscript"><span class="koboSpan" id="kobo.1059.1">-3</span></span><span class="koboSpan" id="kobo.1060.1"> is optimal. </span><span class="koboSpan" id="kobo.1060.2">As Adam calculates gradients differently, it is more difficult to make it diverge </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">than SGD.</span></span></p>
<p><span class="koboSpan" id="kobo.1062.1">Adam requires smaller values for the learning rate because it </span><em class="italic"><span class="koboSpan" id="kobo.1063.1">adapts</span></em><span class="koboSpan" id="kobo.1064.1"> the value as the training </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">process evolves.</span></span></p>
<h4><span class="koboSpan" id="kobo.1066.1">Batch size</span></h4>
<p><span class="koboSpan" id="kobo.1067.1">Let’s</span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.1068.1"> analyze how the training loss and validation loss vary for the Adam optimizer by varying the batch size, keeping the other parameters constant: </span><em class="italic"><span class="koboSpan" id="kobo.1069.1">Epochs = 100, LR = 10-2, Loss fn = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1070.1">HuberLoss</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.1072.1"><img alt="Figure 3.16 – Loss for the Adam optimizer when varying batch size" src="image/B16591_03_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1073.1">Figure 3.16 – Loss for the Adam optimizer when varying batch size</span></p>
<p><span class="koboSpan" id="kobo.1074.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1075.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1076.1">.16,</span></em><span class="koboSpan" id="kobo.1077.1"> we can conclude that for the Adam optimizer, a batch size value of between 64 and 1,024 provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">best results.</span></span></p>
<h4><span class="koboSpan" id="kobo.1079.1">Epochs</span></h4>
<p><span class="koboSpan" id="kobo.1080.1">Another </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.1081.1">hyperparameter is the number of epochs, meaning the number of times the optimizer is going to process the full </span><span class="No-Break"><span class="koboSpan" id="kobo.1082.1">training set.</span></span></p>
<p><span class="koboSpan" id="kobo.1083.1">Let’s analyze how the training loss and validation loss vary for the Adam optimizer varying the epochs, keeping the other parameters constant: </span><em class="italic"><span class="koboSpan" id="kobo.1084.1">LR = 10-2, Batch Size = 128, Loss fn = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1085.1">HuberLoss</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.1087.1"><img alt="Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs" src="image/B16591_03_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1088.1">Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs</span></p>
<p><span class="koboSpan" id="kobo.1089.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1090.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1091.1">.17</span></em><span class="koboSpan" id="kobo.1092.1">, we </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.1093.1">can conclude that around 100-200 epochs is good for our problem. </span><span class="koboSpan" id="kobo.1093.2">With these values, it is very likely the best result will be achieved earlier </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">than that.</span></span></p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.1095.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1096.1">On our</span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.1097.1"> journey toward solving our regression problem, we learned in this recipe how to update our model hyperparameters optimally. </span><span class="koboSpan" id="kobo.1097.2">We understood the role that each hyperparameter plays in the training loop and we performed some ablation studies for each individual hyperparameter. </span><span class="koboSpan" id="kobo.1097.3">This helped us understand how our training and validation losses behaved when we modified each </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">hyperparameter individually.</span></span></p>
<p><span class="koboSpan" id="kobo.1099.1">For our current problem and the chosen model, we verified that the best set of hyperparameters was </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">Optimizer: Adam</span></span></li>
<li><span class="koboSpan" id="kobo.1102.1">Learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1103.1">Rate: 10</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.1104.1">-2</span></span></span></li>
<li><span class="koboSpan" id="kobo.1105.1">Batch </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">Size: 128</span></span></li>
<li><span class="koboSpan" id="kobo.1107.1">Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">epochs: 200</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1109.1">At the end of the training loop, these hyperparameters gave us a training loss of 0.10 and a validation loss </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">of 0.10.</span></span></p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.1111.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1112.1">With our model </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.1113.1">definition, we introduced three new concepts: </span><strong class="bold"><span class="koboSpan" id="kobo.1114.1">batch normalization</span></strong><span class="koboSpan" id="kobo.1115.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1116.1">dropout</span></strong><span class="koboSpan" id="kobo.1117.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1118.1">scaling</span></strong><span class="koboSpan" id="kobo.1119.1">. </span><span class="koboSpan" id="kobo.1119.2">I find the following links useful to understand these </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">advanced topics:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1121.1">Introduction to batch </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1122.1">normalization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">: </span></span><a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/"><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1125.1">Batch normalization research </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1126.1">paper</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">: </span></span><a href="https://arxiv.org/abs/1502.03167"><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">https://arxiv.org/abs/1502.03167</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1129.1">Introduction to </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1130.1">dropout</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1131.1">: </span></span><a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/"><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1133.1">Dropout research </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1134.1">paper</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">: </span></span><a href="https://jmlr.org/papers/v15/srivastava14a.html"><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">https://jmlr.org/papers/v15/srivastava14a.html</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1137.1">Scaling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">: </span></span><a href="https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/"><span class="No-Break"><span class="koboSpan" id="kobo.1139.1">https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1140.1">On the subject of initialization, this article explores the Xavier and Kaiming methods in detail (includes links to the research </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">papers): </span></span><a href="https://pouannes.github.io/blog/initialization/"><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">https://pouannes.github.io/blog/initialization/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1144.1">In this recipe, we explored in depth how two optimizers,</span><strong class="bold"><span class="koboSpan" id="kobo.1145.1"> SGD</span></strong><span class="koboSpan" id="kobo.1146.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1147.1">Adam</span></strong><span class="koboSpan" id="kobo.1148.1">, behave. </span><span class="koboSpan" id="kobo.1148.2">These are two of the most important and best performant optimizers; however, there are many more, and some could work better for your </span><span class="No-Break"><span class="koboSpan" id="kobo.1149.1">specific problem.</span></span></p>
<p><span class="koboSpan" id="kobo.1150.1">One excellent resource to learn about which optimizers are implemented in MXNet and their characteristics is the official </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">documentation: </span></span><a href="https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1153.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1154.1">To compare the behavior and performance of each optimizer, I personally like the visualizations shown in this link (optimizers </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">section): </span></span><a href="https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1"><span class="No-Break"><span class="koboSpan" id="kobo.1156.1">https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1158.1">In this recipe, we worked with optimizers and their hyperparameters. </span><span class="koboSpan" id="kobo.1158.2">Hyperparameter choice is a very complex problem, and it always requires a little bit of trial and error with each problem, verifying that the training loop works. </span><span class="koboSpan" id="kobo.1158.3">A rule of thumb when selecting hyperparameters is to read research papers that tackle similar problems to yours and start with the hyperparameters proposed in those papers. </span><span class="koboSpan" id="kobo.1158.4">You can then move from that starting point and see what works best for your </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">particular case.</span></span></p>
<p><span class="koboSpan" id="kobo.1160.1">Apart from the training loss and the validation loss at the end of the training loop, we also provided a third loss value, </span><em class="italic"><span class="koboSpan" id="kobo.1161.1">best validation loss</span></em><span class="koboSpan" id="kobo.1162.1">, we will explore what this value means and how it is calculated in the next recipe. </span><span class="koboSpan" id="kobo.1162.2">This all maps to a question we have not answered properly yet: </span><em class="italic"><span class="koboSpan" id="kobo.1163.1">when do I stop my training loop?</span></em><span class="koboSpan" id="kobo.1164.1"> We will address this question in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">next recipe.</span></span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.1166.1">Evaluating regression models</span></h1>
<p><span class="koboSpan" id="kobo.1167.1">In the </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.1168.1">previous recipe, we learned how to choose our training hyperparameters to optimize our training. </span><span class="koboSpan" id="kobo.1168.2">We also verified how those choices affected the training and validation losses. </span><span class="koboSpan" id="kobo.1168.3">In this recipe, we are going to explore how those choices affect our actual evaluation in the real world. </span><span class="koboSpan" id="kobo.1168.4">The observant reader will have noticed that we split the dataset into three different sets: training, validation, and test. </span><span class="koboSpan" id="kobo.1168.5">However, during our training, we only used the training set and the validation set. </span><span class="koboSpan" id="kobo.1168.6">In this recipe, we will emulate some real-world behavior of our model by running it on the unseen data, the </span><span class="No-Break"><span class="koboSpan" id="kobo.1169.1">test set.</span></span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.1170.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.1171.1">When evaluating a model, we can perform qualitative evaluation and </span><span class="No-Break"><span class="koboSpan" id="kobo.1172.1">quantitative evaluation:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1173.1">Qualitative evaluation</span></strong><span class="koboSpan" id="kobo.1174.1"> is</span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.1175.1"> the </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.1176.1">selection of one or more random (or not so random, depending on what we are looking for) samples and analyzing the result, verifying whether it matches </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">our expectations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1178.1">Quantitative evaluation</span></strong><span class="koboSpan" id="kobo.1179.1"> deals</span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.1180.1"> with</span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.1181.1"> computing the outputs for a large number of inputs and calculating statistics about them (typically the mean), hence we will compute the MAE </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">and MAPE.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1183.1">Furthermore, we</span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.1184.1"> are going to take a look at how training can have a large influence on </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">the evaluation.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.1186.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.1187.1">Before jumping in to model evaluation, let’s discuss how we can measure our model training performance. </span><span class="koboSpan" id="kobo.1187.2">Therefore, the steps in this recipe are </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1189.1">Measuring training performance – </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">overfitting</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">Qualitative evaluation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">Quantitative evaluation</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.1193.1">Measuring training performance – overfitting</span></h3>
<p><span class="koboSpan" id="kobo.1194.1">Deep learning networks </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.1195.1">are quite powerful, surpassing human-level performance on a variety of problems. </span><span class="koboSpan" id="kobo.1195.2">However, when not kept in check, these networks can also provide incorrect and unexpected results. </span><span class="koboSpan" id="kobo.1195.3">One of the most important and frequent errors happens when the network, using its full capability, memorizes the samples that are being shown (the training set), yielding excellent results for that data. </span><span class="koboSpan" id="kobo.1195.4">However, in this scenario, the network has simply memorized the training samples, and when deployed in a real-world use case it is going to perform poorly. </span><span class="koboSpan" id="kobo.1195.5">This type </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.1196.1">of error is </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1198.1">overfitting</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1199.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1200.1">Thankfully, there is a very successful strategy to deal with overfitting, and we have already touched on it. </span><span class="koboSpan" id="kobo.1200.2">It starts with splitting our full dataset into a training set and a validation set, which we did in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1201.1">previous recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.1202.1">From a </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.1203.1">theoretical point of view, training and validation losses typically have behaviors similar to that shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1204.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.1205.1"><img alt="Figure 3.18 – Losses versus epochs – ideal" src="image/B16591_03_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1206.1">Figure 3.18 – Losses versus epochs – ideal</span></p>
<p><span class="koboSpan" id="kobo.1207.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1208.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1209.1">.18</span></em><span class="koboSpan" id="kobo.1210.1">, we can see how the training and validation losses typically evolve (an idealized portrayal). </span><span class="koboSpan" id="kobo.1210.2">The training loss continues decreasing as the training progresses, always optimizing (albeit more slowly as the number of epochs increases). </span><span class="koboSpan" id="kobo.1210.3">The validation loss, however, reaches a point where it does not decrease further, but rather increases. </span><span class="koboSpan" id="kobo.1210.4">At the lowest level of validation loss is where the model has reached its peak performance and where we should stop the learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">process (early).</span></span></p>
<p><span class="koboSpan" id="kobo.1212.1">Let’s</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.1213.1"> examine how this kind of behavior looks in the real world. </span><span class="koboSpan" id="kobo.1213.2">For our problem, the training loss and the validation loss evolve as the training progresses, </span><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.1215.1"><img alt="Figure 3.19 – Losses versus epochs – real" src="image/B16591_03_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1216.1">Figure 3.19 – Losses versus epochs – real</span></p>
<p><span class="koboSpan" id="kobo.1217.1">As we can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1218.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1219.1">.19</span></em><span class="koboSpan" id="kobo.1220.1">, the validation loss is much noisier than in the ideal world, and early stopping is much more difficult to achieve successfully. </span><span class="koboSpan" id="kobo.1220.2">A very easy implementation is to save the model every time the validation loss decreases. </span><span class="koboSpan" id="kobo.1220.3">This way, we are always certain that given a number of epochs the training is going to be run, the model with the best (lowest) validation loss will be saved. </span><span class="koboSpan" id="kobo.1220.4">This was the method implemented in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">previous recipe.</span></span></p>
<h3><span class="koboSpan" id="kobo.1222.1">Qualitative evaluation</span></h3>
<p><span class="koboSpan" id="kobo.1223.1">To verify our </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.1224.1">model is behaving</span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.1225.1"> similarly to what we expect (yielding a low error when predicting a house price), one easy approach is to run our model for a random input from the test set (unseen data). </span><span class="koboSpan" id="kobo.1225.2">This can easily be done with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1226.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1227.1">
scaled_input = mx.nd.array([scaled_X_train_onehot_df.values[random_index]])
# Unscaled Expected Output
expected_output = y_test[random_index]
 print("Unscaled Expected Output:", expected_output)
# Scaled Expected Output
scaled_expected_output = scaled_y_test[random_index]
 print("Scaled Expected Output:", scaled_expected_output)
# Model Output (scaled)
 output = net(scaled_input.as_in_context(ctx)).asnumpy()[0]
 print("Model Output (scaled):", output)
# Unscaled Output
unscaled_output = sc_y.inverse_transform(output)
 print("Unscaled Output:", unscaled_output)
# Absolute Error
abs_error = abs(expected_output - unscaled_output)
 print("Absolute error: ", abs_error)
# Percentage Error
perc_error = abs_error / expected_output * 100.0
print("Percentage Error: ", perc_error)</span></pre> <p><span class="koboSpan" id="kobo.1228.1">The </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.1229.1">preceding code snippet yields the </span><span class="No-Break"><span class="koboSpan" id="kobo.1230.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1231.1">
Unscaled Expected Output: [380000.]
 Scaled Expected Output: [-0.4304741]
 Model Output (scaled): [-0.45450553]
 Unscaled Output: [370690.]
 Absolute error:  [9310.]
 Percentage Error:  [2.45]</span></pre> <p><span class="koboSpan" id="kobo.1232.1">As expected, the</span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.1233.1"> error rate is quite reasonable (</span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">just 2.45%!).</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1235.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1236.1">Although I tried to keep the code as reproducible as possible, including setting the seeds for all random processes, there might be some sources of randomness. </span><span class="koboSpan" id="kobo.1236.2">This means that your results might be different, but typically the order of magnitude of errors will </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">be similar.</span></span></p>
<h3><span class="koboSpan" id="kobo.1238.1">Quantitative evaluation – MAE</span></h3>
<p><span class="koboSpan" id="kobo.1239.1">Let’s calculate</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.1240.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.1241.1">MAE</span></strong><span class="koboSpan" id="kobo.1242.1"> function, as described in the </span><em class="italic"><span class="koboSpan" id="kobo.1243.1">Defining loss functions and evaluation metrics for regression </span></em><span class="koboSpan" id="kobo.1244.1">recipe earlier in </span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">this chapter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1246.1">
Mean Absolute Error (MAE): [81103.97]</span></pre> <p><span class="koboSpan" id="kobo.1247.1">The MAE is $81k. </span><span class="koboSpan" id="kobo.1247.2">Taking into</span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.1248.1"> account that prices varied from $75k to $7.7 million, this error seems reasonable. </span><span class="koboSpan" id="kobo.1248.2">Do not forget that estimating house prices is a </span><span class="No-Break"><span class="koboSpan" id="kobo.1249.1">hard problem!</span></span></p>
<h3><span class="koboSpan" id="kobo.1250.1">Quantitative evaluation – MAPE</span></h3>
<p><span class="koboSpan" id="kobo.1251.1">The value provided </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.1252.1">by the MAE </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.1253.1">is good to get an idea of how small or large the errors are in our model’s predictions. </span><span class="koboSpan" id="kobo.1253.2">However, it does not provide a very meaningful merit figure, as the same MAE could have been achieved in </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">different ways:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1255.1">Small errors for all houses</span></em><span class="koboSpan" id="kobo.1256.1">: As houses increase in price, the absolute error number will be higher, and hence an $80k MAE might be </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">quite good.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1258.1">Very large errors for cheap houses</span></em><span class="koboSpan" id="kobo.1259.1">: In this case, an $80k MAE will mean that for the cheapest houses, the error might be 2-3 times, or even worse, the actual price of the house. </span><span class="koboSpan" id="kobo.1259.2">This scenario would be </span><span class="No-Break"><span class="koboSpan" id="kobo.1260.1">very bad.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1261.1">In general, we </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.1262.1">can add to the MAE another figure, similarly calculated to provide a </span><strong class="bold"><span class="koboSpan" id="kobo.1263.1">relative</span></strong><span class="koboSpan" id="kobo.1264.1"> error rate, instead of relying solely </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.1265.1">on an </span><strong class="bold"><span class="koboSpan" id="kobo.1266.1">absolute</span></strong><span class="koboSpan" id="kobo.1267.1"> value. </span><span class="koboSpan" id="kobo.1267.2">For our model, we get </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1269.1">
Mean Absolute Percentage Error (MAPE): [16.008343]</span></pre> <p><span class="koboSpan" id="kobo.1270.1">Looks like our model is not behaving too badly, yielding a MAPE </span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1">of 16%!</span></span></p>
<h3><span class="koboSpan" id="kobo.1272.1">Quantitative evaluation – thresholds and percentage</span></h3>
<p><span class="koboSpan" id="kobo.1273.1">Another</span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.1274.1"> question </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.1275.1">we could consider for evaluating our model is the following: </span><em class="italic"><span class="koboSpan" id="kobo.1276.1">For how many houses (in %) did we accurately predict </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1277.1">the price?</span></em></span></p>
<p><span class="koboSpan" id="kobo.1278.1">Let’s assume we consider that we accurately predicted the price of a house if the predicted price error is less than 25%. </span><span class="koboSpan" id="kobo.1278.2">In our case, this gives us </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1280.1">
Houses with a predicted price error below 25.0 %: [81.23987971]</span></pre> <p><span class="koboSpan" id="kobo.1281.1">This calculation gives us an 81%, </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">well done!</span></span></p>
<p><span class="koboSpan" id="kobo.1283.1">Furthermore, we could plot the percentage of houses we correctly predicted as a function of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">error threshold:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.1285.1"><img alt="Figure 3.20 – Percentage of correct estimations" src="image/B16591_03_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1286.1">Figure 3.20 – Percentage of correct estimations</span></p>
<p><span class="koboSpan" id="kobo.1287.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1288.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1289.1">.20</span></em><span class="koboSpan" id="kobo.1290.1">, we </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.1291.1">can see, as expected, that considering </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.1292.1">an error of 25% to deem a prediction accurate, our model yields 80%+ </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">correct predictions.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.1294.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1295.1">In this recipe, we explored how to evaluate our regression model. </span><span class="koboSpan" id="kobo.1295.2">To properly do this, we revisited the decision made previously to split our full dataset into a training set, a validation set, and a </span><span class="No-Break"><span class="koboSpan" id="kobo.1296.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.1297.1">During training, we used the training set to calculate the gradients to update our model parameters, and the validation set to confirm the real-world behavior. </span><span class="koboSpan" id="kobo.1297.2">Afterward, to evaluate our model performance, we used the test set, which was the only remaining set of </span><span class="No-Break"><span class="koboSpan" id="kobo.1298.1">unseen data.</span></span></p>
<p><span class="koboSpan" id="kobo.1299.1">We discovered the value of describing our model behavior qualitatively by calculating the output of random samples, and of quantitatively evaluating our model performance by exploring calculations and graphs of MAE </span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">and MAPE.</span></span></p>
<p><span class="koboSpan" id="kobo.1301.1">We ended the recipe by defining what constitutes an accurate prediction by setting a threshold and plotting the behavior of the model by modifying </span><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">the threshold.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.1303.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1304.1">Deep learning has surpassed human-level performance on multiple tasks. </span><span class="koboSpan" id="kobo.1304.2">However, evaluating models properly is paramount to verify how models will perform when deployed in production environments in the real world. </span><span class="koboSpan" id="kobo.1304.3">I found interesting this small list of tasks where human-level performance has been reached by </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">AI: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1306.1">https://venturebeat.com/2017/12/08/6-areas-where-artificial-neural-networks-outperform-humans/.</span></span></p>
<p><span class="koboSpan" id="kobo.1307.1">When evaluation is not performed properly, models may not behave as expected. </span><span class="koboSpan" id="kobo.1307.2">Two of the most significant large-scale problems of this type (at Google in 2015 and Microsoft in 2016, respectively) are detailed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">following articles:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1309.1">Google Mistakenly Tags Black People as ‘Gorillas,’ Showing Limits of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1310.1">Algorithms: </span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1311.1">https://www.wsj.com/articles/BL-DGB-42522</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1312.1">Statistics By </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1313.1">Jim: </span></strong></span><a href="https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/"><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/</span></span></a><a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist "/></li>
</ul>
<p><span class="koboSpan" id="kobo.1315.1">Unfortunately, although these issues are less and less frequent nowadays, they still exist. </span><span class="koboSpan" id="kobo.1315.2">A database that contains these issues has been published and is updated whenever one of these issues is </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">reported: </span></span><a href="https://incidentdatabase.ai/"><span class="No-Break"><span class="koboSpan" id="kobo.1317.1">https://incidentdatabase.ai/.</span></span></a></p>
<p><span class="koboSpan" id="kobo.1318.1">To prevent these issues, Google wrote a set of principles to develop Responsible AI. </span><span class="koboSpan" id="kobo.1318.2">I strongly recommend all AI practitioners to abide by </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">them: </span></span><a href="https://ai.google/principles/"><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">https://ai.google/principles/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1322.1">At this stage, we have completed our journey through a complete regression problem: we explored our regression dataset, decided on our evaluation metrics, and defined and initialized our model. </span><span class="koboSpan" id="kobo.1322.2">We understood the best hyperparameter combination of optimizer, learning rate, batch size, and epochs, and trained it with early stopping. </span><span class="koboSpan" id="kobo.1322.3">Lastly, we concluded by evaluating our model qualitatively </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">and quantitatively.</span></span></p>
</div>
</body></html>