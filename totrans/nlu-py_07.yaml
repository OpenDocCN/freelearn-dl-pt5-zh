- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting Approaches and Representing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the next steps in getting ready to implement a **natural
    language processing** (**NLP**) application. We start with some basic considerations
    about understanding how much data is needed for an application, what to do about
    specialized vocabulary and syntax, and take into account the need for different
    types of computational resources. We then discuss the first steps in NLP – text
    representation formats that will get our data ready for processing with NLP algorithms.
    These formats include symbolic and numerical approaches for representing words
    and documents. To some extent, data formats and algorithms can be mixed and matched
    in an application, so it is helpful to consider data representation independently
    from the consideration of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The first section will review general considerations for selecting NLP approaches
    that have to do with the type of application we’re working on, and with the data
    that we’ll be using.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting NLP approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing language for NLP applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing language numerically with vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing words with context-independent vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing words with context-dependent vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting NLP approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP can be done with a wide variety of possible techniques. When you get started
    on an NLP application, you have many choices to make, which are affected by a
    large number of factors. One of the most important factors is the type of application
    itself and the information that the system needs to extract from the data to perform
    the intended task. The next section addresses how the application affects the
    choice of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the approach to the task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016), that there are
    many different types of NLP applications divided into **interactive** and **non-interactive
    applications**. The type of application you choose will play an important role
    in choosing the technologies that will be applied to the task. Another way of
    categorizing applications is in terms of the level of detail required to extract
    the needed information from the document. At the coarsest level of analysis (for
    example, classifying documents into two different categories), techniques can
    be less sophisticated, faster to train, and less computationally intensive. On
    the other hand, if the task is training a chatbot or voice assistant that needs
    to pull out multiple entities and values from each utterance, the analysis needs
    to be more sensitive and fine-grained. We will see some specific examples of this
    in later sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how data affects our choice of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP applications are built on datasets or sets of examples of the kinds of data
    that the target system will need to process. To build a successful application,
    having the right amount of data is imperative. However, we can’t just specify
    a single number of examples for every application, because the right amount of
    data is going to be different for different kinds of applications. Not only do
    we have to have the right amount of data, but we have to have the right kind of
    data. We’ll talk about these considerations in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: How much data is enough?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), we discussed many methods
    of obtaining data, and after going through [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    you should have a good idea of where your data will be coming from. However, in
    that chapter, we did not address the question of how to tell how much data is
    needed for your application to accomplish its goal. If there are hundreds or thousands
    of different possible classifications of documents in a task, then we will need
    a sufficient number of examples of each category for the system to be able to
    tell them apart. Obviously, the system can’t detect a category if it hasn’t ever
    seen an example of that category, but it will also be quite difficult to detect
    categories where it has seen very few examples.
  prefs: []
  type: TYPE_NORMAL
- en: If there are many more examples of some classes than others, then we have an
    **imbalanced** dataset. Techniques for balancing classes will be discussed in
    detail in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248),but basically, they
    include **undersampling** (where some items in the more common class are discarded),
    **oversampling** (where items in the rarer classes are duplicated), and **generation**
    (where artificial examples from the rarer classes are generated through rules).
  prefs: []
  type: TYPE_NORMAL
- en: Systems generally perform better if they have more data, but the data also has
    to be representative of the data that the system will encounter at the time of
    testing, or when the system is deployed as an application. If a lot of new vocabulary
    has been added to the task (for example, if a company’s chatbot has to deal with
    new product names), the training data needs to be periodically updated for the
    best performance. This is related to the general question of specialized vocabulary
    and syntax since product names are a kind of specialized vocabulary. In the next
    section, we will discuss this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized vocabulary and syntax
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another consideration is how similar the data is to the rest of the natural
    language we will be processing. This is important because most NLP processing
    makes use of models that were derived from previous examples of the language.
    The more similar the data to be analyzed is to the rest of the language, the easier
    it will be to build a successful application. If the data is full of specialized
    jargon, vocabulary, or syntax, then it will be hard for the system to generalize
    from its original training data to the new data. If the application is full of
    specialized vocabulary and syntax, the amount of training data will need to be
    increased to include this new vocabulary and syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Considering computational efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The computer resources that are needed to implement a particular NLP approach
    are an important consideration in selecting an approach. Some approaches that
    yield good results when tested on laboratory benchmarks can be impractical in
    applications that are intended for deployment. In the next sections, we will discuss
    the important consideration of the time required to execute the approaches, both
    at training time and inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Training time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some modern neural net models are very computationally intensive and require
    very long training periods. Even before the actual neural net training starts,
    there may need to be some exploratory efforts aimed at identifying the best values
    for hyperparameters. **Hyperparameters** are training parameters that can’t directly
    be estimated during the training process, and that have to be set by the developer.
    When we return to machine learning techniques in *Chapters 9*, *10*, *11*, and
    *12*, we will look at specific hyperparameters and talk about how to identify
    good values.
  prefs: []
  type: TYPE_NORMAL
- en: Inference time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another important consideration is **inference time**, or the processing time
    required for a trained system to perform its task. For interactive applications
    such as chatbots, inference time is not normally a concern because today’s systems
    are fast enough to keep up with a user in an interactive application. If a system
    takes a second or two to process a user’s input, that’s acceptable. On the other
    hand, if the system needs to process a large amount of existing online text or
    audio data, the inference time should be as fast as possible. For example, Statistica.com
    ([https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/))
    estimated in February 2020 that 500 hours of videos were uploaded to YouTube every
    minute. If an application was designed to process YouTube videos and needed to
    keep up with that volume of audio, it would need to be very fast.
  prefs: []
  type: TYPE_NORMAL
- en: Initial studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Practical NLP requires fitting the tool to the problem. When there are new advances
    in NLP technology, there can be very enthusiastic articles in the press about
    what the advances mean. But if you’re trying to solve a practical problem, trying
    to use new techniques can be counterproductive because the newest technologies
    might not scale. For example, new techniques might provide higher accuracy, but
    at the cost of very long training periods or very large amounts of data. For this
    reason, it is recommended that when you’re trying to solve a practical problem,
    do some initial exploratory studies with simpler techniques to see whether they’ll
    solve the problem. Only if the simpler techniques don’t address the problem’s
    requirements should more advanced techniques be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about one of the important choices that need
    to be made when you design an NLP application – how to represent the data. We’ll
    look at both symbolic and numerical representations.
  prefs: []
  type: TYPE_NORMAL
- en: Representing language for NLP applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For computers to work with natural language, it has to be represented in a form
    that they can process. These representations can be **symbolic**, where the words
    in a text are processed directly, or **numeric**, where the representation is
    in the form of numbers. We will describe both of these approaches here. Although
    the numeric approach is the primary approach currently used in NLP research and
    applications, it is worth becoming somewhat familiar with the ideas behind symbolic
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditionally, NLP has been based on processing the words in texts directly,
    as words. This approach was embodied in a standard approach where the text was
    analyzed in a series of steps that were aimed at converting an input consisting
    of unanalyzed words into a meaning. In a traditional NLP pipeline, shown in *Figure
    7**.1*, each step in processing, from input text to meaning, produces an output
    that adds more structure to its input and prepares it for the next step in processing.
    All of these results are symbolic – that is, non-numerical. In some cases, the
    results might include probabilities, but the actual results are symbolic:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.1\uFEFF – Traditional NLP symbolic pipeline](img/B19005_07_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Traditional NLP symbolic pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we won’t review all of the components of the symbolic approach to
    processing, we can see a couple of these symbolic results in the following code
    samples, showing part of speech tagging results and parsing results, respectively.
    We will not address semantic analysis or pragmatic analysis here since these techniques
    are generally applied only for specialized problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet shows the process of importing the movie review
    database, which we have seen previously, followed by the code for selecting the
    first sentence and part of speech tagging it. The next code snippet shows the
    results of part of speech tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The tags (`NN`, `CD`, `NNS`, etc.) shown in the preceding results are those
    used by NLTK and are commonly used in NLP. They are originally based on the Penn
    Treebank tags (*Building a Large Annotated Corpus of English: The Penn Treebank*
    (Marcus et al., CL 1993)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important type of symbolic processing is `plot: two teen couples go
    to a church party, drink and then drive`, which we saw in the preceding code snippet,
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses spaCy to parse the first sentence in the movie review
    corpus. After the parse, we iterate through the tokens in the resulting document
    and print the token and its part of speech tag. These are followed by the text
    of the token’s head, or the word that it depends on, and the kind of dependency
    between the head and the token. For example, the word `couples` is tagged as a
    plural noun, it is dependent on the word `go`, and the dependency is `nsubj` since
    `couples` is the noun subject (`nsubj`) of `go`. In [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085),
    we saw an example of a visualization of a dependency parse (*Figure 4**.6*), which
    represents the dependencies as arcs between the item and its head; however, in
    the preceding code, we see more of the underlying information.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen some examples of symbolic representations of language
    based on analyzing individual words and phrases, including parts of speech of
    individual words and labels for phrases. We can also represent words and phrases
    using a completely different and completely numeric approach based on vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Representing language numerically with vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common mathematical technique for representing language in preparation for
    machine learning is through the use of vectors. Both documents and words can be
    represented with vectors. We’ll start by discussing document vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding vectors for document representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen that texts can be represented as sequences of symbols such as words,
    which is the way that we read them. However, it is usually more convenient for
    computational NLP purposes to represent text numerically, especially if we are
    dealing with large quantities of text. Another advantage of numerical representation
    is that we can also process text represented numerically with a much wider range
    of mathematical techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to represent both documents and words is by using vectors, which
    are basically one-dimensional arrays. Along with words, we can also use vectors
    to represent other linguistic units, such as lemmas or stemmed words, which were
    described in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: Binary bag of words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059) and [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134),
    we briefly discussed the `1` or `0`, depending on whether that word occurs in
    that document. Each position in the vector is a feature of the document – that
    is, whether or not the word occurs. This is the simplest form of the BoW, and
    it is called the **binary bag of words**. It is immediately clear that this is
    a very coarse way of representing documents. All it cares about is whether a word
    occurs in a document, so it fails to capture a lot of information – what words
    are nearby, where in the document the words occur, and how often the words occur
    are all missing in the binary BoW. It is also affected by the lengths of documents
    since longer documents will have more words.
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed version of the BoW approach is to count not just whether a word
    appears in a document but also how many times it appears. For this, we'll move
    on to the next technique, **count bag** **of words**.
  prefs: []
  type: TYPE_NORMAL
- en: Count bag of words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It seems intuitive that the number of times a word occurs in a document would
    help us decide how similar two documents are to each other. However, so far, we
    haven’t used that information. In the document vectors we’ve seen so far, the
    values are just one and zero – one if the word occurs in the document and zero
    if it doesn’t. If instead, we let the values represent the number of times the
    word occurs in the document, then we have more information. The BoW that includes
    the frequencies of the words in a document is a **count BoW**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw the code to generate the binary BoW in the *Bag of words and k-means
    clustering* section (*Figure 6**.15*) in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134).
    The code can be very slightly modified to compute a count BoW. The only change
    that has to be made is to increment the total count for a word when it is found
    more than once in a document. This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Comparing this code to the code in *Figure 6**.15*, we can see that the only
    difference is that the value of `features[word]`is incremented when the word is
    found in the document, rather than set to `1`. The resulting matrix, shown in
    *Figure 7**.2*, has many different values for word frequencies than the matrix
    in *Figure 6**.16*, which had only zeros and ones:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.2\uFEFF – Count BoW for the movie review corpus](img/B19005_07_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Count BoW for the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.2*, we’re looking at 10 randomly selected documents, numbered
    from `0` to `9` in the first column. Looking more closely at the frequencies of
    `film`, (recall that `film` is the most common non-stopword in the corpus), we
    can see that all of the documents except document `5` and document `6` have at
    least one occurrence of `film`. In a binary BoW, they would all be lumped together,
    but here they have different values, which allows us to make finer-grained distinctions
    among documents. At this point, you might be interested in going back to the clustering
    exercise in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134), modifying the code
    in *Figure 6**.22* to use the count BoW, and looking at the resulting clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from *Figure 7**.2* that the count BoW gives us some more information
    about the words that occur in the documents than the binary BoW. However, we can
    do an even more precise analysis by using a technique called **term frequency-inverse
    document frequency** (**TF-IDF**), which we describe in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency-inverse document frequency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering that our goal is to find a representation that accurately reflects
    similarities between documents, we can make use of some other insights. Specifically,
    consider the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The raw frequency of words in a document will vary based on the length of the
    document. This means that a shorter document with fewer overall words might not
    appear to be similar to a longer document that has more words. So, we should be
    considering the proportion of the words in the document rather than the raw number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words that occur very often overall will not be useful in distinguishing documents,
    because every document will have a lot of them. Clearly, the most problematic
    words that commonly occur within a corpus will be the stopwords we discussed in
    [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), but other words that are not
    strictly stopwords can have this property as well. Recall when we looked at the
    movie review corpus in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134) that the
    words `film` and `movie` were very common in both positive and negative reviews,
    so they won’t be able to help us tell those categories apart. The most helpful
    words will probably be the words that occur with different frequencies in different
    categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular way of taking these concerns into account is the measure *TF-IDF*.
    TF-IDF consists of two measurements – **term frequency** (**TF**) and **inverse
    document** **frequency** (**IDF**).
  prefs: []
  type: TYPE_NORMAL
- en: 'TF is the number of times a term (or word) appears in a document, divided by
    the total number of terms in the document (which takes into account the fact that
    longer documents have more words overall). We can define that value as `tf(term,
    document)`. For example, in *Figure 7**.2,* we can see that the term `film` appeared
    once in document `0`, so `tf("film",0)` is `1` over the length of document `0`.
    Since the second document contains `film` four times, `tf("film",1)` is `4` over
    the length of document `1`. The formula for term frequency is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: tf(t, d) =  f t,d _ Σ t ′ ∈d f t ′ ,d
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as we saw with stopwords, very frequent words don’t give us much information
    for distinguishing documents. Even if `TF(term, document)` is very large, that
    could just be due to the fact that `term` occurs frequently in every document.
    To take care of this, we introduce IDF. The numerator of `idf(term,Documents)`is
    the total number of documents in the corpus, *N*, which we divide by the number
    of documents (*D*) that contain the term, *t*. In case the term doesn’t appear
    in the corpus, the denominator would be 0, so `1` is added to the denominator
    to prevent division by 0\. `idf(term, documents)`is the log of this quotient.
    The formula for `idf` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'idf(t, D) = log  N ____________  |{d ∈ D : t ∈ d|}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the *TF-IDF* value for a term in a document in a given corpus is just
    the product of its TF and IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: tfidf(t, d, D) = tf ⋅ idf(t, D)
  prefs: []
  type: TYPE_NORMAL
- en: To compute the TF-IDF vectors of the movie review corpus, we will use another
    very useful package, called `scikit-learn`, since NLTK and spaCy don’t have built-in
    functions for TF-IDF. The code to compute TF-IDF in those packages could be written
    by hand using the standard formulas we saw in the preceding three equations; however,
    it will be faster to implement if we use the functions in `scikit-learn`, in particular,
    `tfidfVectorizer` in the feature extraction package. The code to compute TF-IDF
    vectors for the movie review corpus of 2,000 documents is shown in *Figure 7**.3*.
    In this example, we will look only at the top 200 terms.
  prefs: []
  type: TYPE_NORMAL
- en: A tokenizer is defined in lines 9-11\. In this example, we are using just the
    standard NLTK tokenizer, but any other text-processing function could be used
    here as well. For example, we might want to try using stemmed or lemmatized tokens,
    and these functions could be included in the `tokenize()` function. Why might
    stemming or lemmatizing text be a good idea?
  prefs: []
  type: TYPE_NORMAL
- en: One reason that this kind of preprocessing could be useful is that it will reduce
    the number of unique tokens in the data. This is because words that have several
    different variants will be collapsed into their root word (for example, *walk*,
    *walks*, *walking*, and *walked* will all be treated as the same word). If we
    believe that this variation is mostly just a source of noise in the data, then
    it’s a good idea to collapse the variants by stemming or lemmatization. However,
    if we believe that the variation is important, then it won’t be a good idea to
    collapse the variants, because this will cause us to lose information. We can
    make this kind of decision a priori by thinking about what information is needed
    for the goals of the application, or we can treat this decision as a hyperparameter,
    exploring different options and seeing how they affect the accuracy of the final
    result.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.3* shows a screenshot of the code.'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.3\uFEFF – Code to compute TF-IDF vectors for the movie review corpus](img/B19005_07_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Code to compute TF-IDF vectors for the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: Returning to *Figure 7**.3*, the next step after defining the tokenization function
    is to define the path where the data can be found (line 13) and initialize the
    token dictionary at line 14\. The code then walks the data directory and collects
    the tokens from each file. While the code is collecting tokens, it lowercases
    the text and removes punctuation (line 22). The decisions to lowercase the text
    and remove punctuation are up to the developer, similar to the decision we discussed
    previously on whether or not to stem or lemmatize the tokens. We could explore
    empirically whether or not these two preprocessing steps improve processing accuracy;
    however, if we think about how much meaning is carried by case and punctuation,
    it seems clear that in many applications, case and punctuation don’t add much
    meaning. In those kinds of applications, lowercasing the text and removing punctuation
    will improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: Having collected and counted the tokens in the full set of files, the next step
    is to initialize `tfIdfVectorizer`, which is a built-in function in scikit-learn.
    This is accomplished in lines 25-29\. The parameters include the type of input,
    whether or not to use IDF, which tokenizer to use, how many features to use, and
    the language for stopwords (English, in this example).
  prefs: []
  type: TYPE_NORMAL
- en: Line 32 is where the real work of TF-IDF is done, with the `fit_transform` method,
    where the TF-IDF vector is built from the documents and the tokens. The remaining
    code (lines 37-42) is primarily to assist in displaying the resulting TF-IDF vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting TF-IDF matrix is shown in *Figure 7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.4\uFEFF – Partial \uFEFFTF-IDF vectors for some of the documents\
    \ in the movie review corpus](img/B19005_07_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Partial TF-IDF vectors for some of the documents in the movie review
    corpus
  prefs: []
  type: TYPE_NORMAL
- en: We have now represented our movie review corpus as a matrix where every document
    is a vector of *N* dimensions, where *N* is the maximum size of vocabulary we
    are going to use. *Figure 7**.4* shows the TF-IDF vectors for a few of the 2,000
    documents in the corpus (documents 0-4 and 1995-1999), shown in the rows, and
    some of the words in the corpus, shown in alphabetical order at the top. Both
    the words and the documents are truncated for display purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.4*, we can see that there is quite a bit of difference in the
    TF-IDF values for the same words in different documents. For example, `acting`
    has considerably different scores in documents `0` and `1`. This will become useful
    in the next step of processing (classification), which we will return to in [*Chapter
    9*](B19005_09.xhtml#_idTextAnchor173). Note that we have not done any actual machine
    learning yet; so far, the goal has been simply to convert documents to numerical
    representations, based on the words they contain.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we’ve been focusing on representing documents. But what about
    representing the words themselves? The words in a document vector are just numbers
    representing their frequency, either just in a document or their frequency in
    a document relative to their frequency in a corpus (TF-IDF). We don’t have any
    information about the meanings of the words themselves in the techniques we’ve
    looked at so far. However, it seems clear that the meanings of words in a document
    should also impact the document’s similarity to other documents. We will look
    at representing the meanings of words in the next section. This representation
    is often called **word embeddings** in the NLP literature. We will begin with
    a popular representation of words as vectors, **Word2Vec**, which captures the
    similarity in meaning of words to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Representing words with context-independent vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at several ways of representing similarities among documents.
    However, finding out that two or more documents are similar to each other is not
    very specific, although it can be useful for some applications, such as intent
    or document classification. In this section, we will talk about representing the
    meanings of words with word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word2Vec is a popular library for representing words as vectors, published by
    Google in 2013 (Mikolov, Tomas; et al. (2013). *Efficient Estimation of Word Representations
    in Vector Space*. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)).
    The basic idea behind Word2Vec is that every word in a corpus is represented by
    a single vector that is computed based on all the contexts (nearby words) in which
    the word occurs. The intuition behind this approach is that words with similar
    meanings will occur in similar contexts. This intuition is summarized in a famous
    quote from the linguist J. R. Firth, “*You shall know a word by the company it
    keeps*” (*Studies in Linguistic* *Analysis*, Wiley-Blackwell).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build up to Word2Vec by starting with the idea of assigning each word
    to a vector. The simplest vector that we can use to represent words is the idea
    of `1` in a specific position in the vector, and all the rest of the positions
    are zeros (it is called one-hot encoding because one bit is on – that is, *hot*).
    The length of the vector is the size of the vocabulary. The set of one-hot vectors
    for the words in a corpus is something like a dictionary in that, for example,
    we could say that if the word is `movie`, it will be represented by `1` in a specific
    position. If the word is `actor`, it will be represented by `1` in a different
    position.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we aren’t taking into account the surrounding words yet. The
    first step in one-hot encoding is integer encoding, where we assign a specific
    integer to each word in the corpus. The following code uses libraries from scikit-learn
    to do the integer encoding and the one-hot encoding. We also import some functions
    from the `numpy` library, `array` and `argmax`, which we’ll be returning to in
    later chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the first 50 integer encodings in the first movie review, we see
    a vector of length `50`. This can be converted to a one-hot encoding, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output in the preceding code shows first a subset of the one-hot vectors.
    Since they are one-hot vectors, they have `0` in all positions except one position,
    whose value is `1`. Obviously, this is very sparse, and it would be good to provide
    a more condensed representation.
  prefs: []
  type: TYPE_NORMAL
- en: The next to the last line shows how we can invert a one-hot vector to recover
    the original word. The sparse representation takes a large amount of memory and
    is not very practical.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec approach uses a neural net to reduce the dimensionality of the
    embedding. We will be returning to the details of neural networks in [*Chapter
    10*](B19005_10.xhtml#_idTextAnchor184), but for this example, we’ll use a library
    called `Gensim` that will compute Word2Vec for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the Gensim `Word2Vec` library to create a model of
    the movie review corpus. The Gensim `model` object that was created by Word2Vec
    includes a large number of interesting methods for working with the data. The
    following code shows one of these – `most_similar`, which, given a word, can find
    the words that are the most similar to that word in the dataset. Here, we can
    see a list of the 25 words most similar to `movie` in the corpus, along with a
    score that indicates how similar that word is to `movie`, according to the Word2Vec
    analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen in the preceding code, the words that Word2Vec finds to be the
    most similar to `movie` based on the contexts in which they occur are very much
    what we would expect. The top two words, `film` and `picture`, are very close
    synonyms of `movie`. In [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), we will
    return to Word2Vec and see how this kind of model can be used in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While Word2Vec does take into account the contexts in which words occur in a
    dataset, every word in the vocabulary is represented by a single vector that encapsulates
    all of the contexts in which it occurs. This glosses over the fact that words
    can have different meanings in different contexts. The next section reviews approaches
    representing words depending on specific contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Representing words with context-dependent vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec’s word vectors are context-independent in that a word always has the
    same vector no matter what context it occurs in. However, in fact, the meanings
    of words are strongly affected by nearby words. For example, the meanings of the
    word *film* in *We enjoyed the film* and *the table was covered with a thin film
    of dust* are quite different. To capture these contextual differences in meanings,
    we would like to have a way to have different vector representations of these
    words that reflect the differences in meanings that result from the different
    contexts. This research direction has been extensively explored in the last few
    years, starting with the **BERT** (**Bidirectional Encoder Representations from
    Transformers**) system ([https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)
    (Devlin et al., NAACL 2019)).
  prefs: []
  type: TYPE_NORMAL
- en: This approach has resulted in great improvements in NLP technology, which we
    will want to discuss in depth. For that reason, we will postpone a fuller look
    at context-dependent word representations until we get to [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    where we will address this topic in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve learned how to select different NLP approaches, based
    on the available data and other requirements. In addition, we’ve learned about
    representing data for NLP applications. We’ve placed particular emphasis on vector
    representations, including vector representations of both documents and words.
    For documents, we’ve covered binary bag of words, count bag of words, and TF-IDF.
    For representing words, we’ve reviewed the Word2Vec approach and briefly introduced
    context-dependent vectors, which will be covered in much more detail in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193).
  prefs: []
  type: TYPE_NORMAL
- en: In the next four chapters, we will take the representations that we’ve learned
    about in this chapter and show how to train models from them that can be applied
    to different problems such as document classification and intent recognition.
    We will start with rule-based techniques in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159),
    discuss traditional machine learning techniques in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173),
    talk about neural networks in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),
    and discuss the most modern approaches, transformers, and pretrained models in
    [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  prefs: []
  type: TYPE_NORMAL
