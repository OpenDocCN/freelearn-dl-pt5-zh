- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Model Evaluation Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A trained deep learning model without any form of validation cannot be deployed
    to production. Production, in the context of the machine learning software domain,
    refers to the deployment and operation of a machine learning model in a live environment
    for actual consumption of its predictions. More broadly, model evaluation serves
    as a critical component in any deep learning project. Typically, a deep learning
    project will result in many models being built, and a final model will be chosen
    to serve in a production environment. A good model evaluation process for any
    project leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A better-performing final model through model comparisons and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fewer production prediction mishaps by understanding common model pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More closely aligned practitioner and final model behaviors through model insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A higher probability of project success through success metric evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final model that is less biased and fairer and produces more trusted predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generally, the model evaluation process leads to more informed decisions across
    the entire machine learning life cycle. In this first chapter of *Part 2* of the
    book, we will discuss all the categories of model evaluation that will help to
    achieve these benefits. Additionally, we will dive deep into some of these categories
    in the next four chapters, which all belong to *Part 2* of the book. Specifically,
    we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the different model evaluation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering the base model evaluation metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring custom metrics and their applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring statistical tests for comparing model metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relating the evaluation metric to success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly optimizing the metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will have a practical implementation using the Python programming
    language. To complete it, you will only need to install the `matplotlib` library
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The code files are available on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the different model evaluation methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most practitioners are familiar with accuracy-related metrics. This is the most
    basic evaluation method. Typically, for supervised problems, a practitioner will
    treat an accuracy-related metric as the golden source of truth. In the context
    of model evaluation, the term “accuracy metrics” is often used to collectively
    refer to various performance metrics such as accuracy, F1 score, recall, precision,
    and mean squared error. When coupled with a suitable cross-validation partitioning
    strategy, using metrics as a standalone evaluation strategy can go a long way
    in most projects. In deep learning, accuracy-related metrics are typically used
    to monitor the progress of the model at each epoch. The monitoring process can
    subsequently be extended to perform early stopping to stop training the model
    when it doesn’t improve anymore and to determine when to reduce the learning rate.
    Additionally, the best model weights can be loaded at the end of the training
    process defined by the weights that achieved the best metric score on the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy-related metrics alone do not provide a complete picture of a machine
    learning model’s capabilities and behavior. This is particularly true in unsupervised
    projects where accuracy metrics are superficial and only relevant to specific
    distributions. Gaining a more complete understanding of a model allows you to
    make more informed decisions across the machine’s life cycle. Some examples of
    the ways you can gain more understanding of the model includethe following:'
  prefs: []
  type: TYPE_NORMAL
- en: Model insights can be a proxy to assess the accuracy of the data being used.
    If the data is deemed to be inaccurate or has some slight flaws, you can transition
    back into the data preparation stage of the machine learning life cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In cases where bias is detected, the model may need to be retrained to remove
    the bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also important to evaluate whether the model can recognize patterns in
    the way that domain experts do. If it cannot, a different model or data preparation
    method may be required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is necessary to consider whether the model can exhibit common sense in its
    predictions. If not, it may be necessary to apply special post-processing techniques
    to enforce common sense.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing other performance metrics such as inference speed and model size can
    be critical when choosing a model. You don’t want a model that is too slow or
    too big to fit into your targeted production environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to helping a project progress toward success, gathering insights
    from a model can also help identify potential issues early on. In the machine
    learning life cycle, which was introduced in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015),
    *Deep Learning Life Cycle*, it is important to remember that projects may fail
    during planning or when delivering model insights. Failing is a natural part of
    the machine learning process and, in fact, many projects are not meant to succeed.
    This could be due to a variety of factors, such as data engineering not being
    suited for machine learning or the task being too complex. However, it is important
    to fail fast and dump the project in order to be able to redirect resources to
    other use cases that have a higher chance of success. In cases where the project
    is critical and cannot be dumped, identifying the root cause of the failure quickly
    can improve the execution efficiency of the project by diverting resources to
    fix it and cyclically transitioning between the stages of the machine learning
    life cycle. In order for a project to fail fast, you have to have a responsible
    and confident way to be able to determine whether the model is not working. In
    summary, this ability to fail quickly can be very beneficial, as it saves time
    and resources that might have otherwise been wasted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows a sufficient range of methods that can be utilized
    to evaluate deep learning models, with some of them being general methods that
    can work for non-deep learning models too:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metric engineering**: While evaluation metrics are commonly used
    in many projects, the practice of evaluation metric engineering is often overlooked.
    In this chapter, we will take a closer look at metric engineering and explore
    the process of selecting appropriate evaluation metrics. We’ll start by discussing
    foundational baseline evaluation metrics that are suitable for various types of
    problems. Then, we’ll move on to explore how to upgrade the baseline evaluation
    metric to one that is specific to the domain and use case of the project. So,
    in short, this chapter will help you understand the importance of metric engineering
    and guide you through the process of selecting the right metrics for your project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curves**: Learning curves determine the level of fit for a deep
    learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lift charts**: A lift chart offers a visual representation of the performance
    of a predictive model. It shows how much better the model is at predicting positive
    outcomes compared to random chance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Receiver operating characteristic** (**ROC**) **curves**: A graphical representation
    of the performance of a binary classification model. They plot the **true positive
    rate** (**TPR**) against the **false positive rate** (**FPR**) at various classification
    thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion matrix**: A confusion matrix is a performance evaluation tool that
    measures the classification accuracy of a machine learning model. It compares
    the predicted and actual outcomes of a model’s predictions and presents them in
    a matrix format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature importances**: This is the process of determining which features
    in a dataset have the most influence on the output of a machine learning model.
    Additionally, it is useful for identifying the most important factors in a given
    problem and can help improve the model’s overall performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B testing**: A/B testing for machine learning involves comparing the performance
    of two different models or any algorithms on a specific task, in order to determine
    which model performs better in practice. This can help practitioners make more
    informed decisions about which models to use or how to improve existing models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cohort analysis**: Cohort analysis is a technique for evaluating the performance
    of a model on different subgroups or cohorts of users. It can help identify whether
    the model is performing differently for different groups and can be useful for
    understanding how to improve the model for specific segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual analysis**: Residual analysis is a technique used to check the goodness
    of fit of a regression model by examining the difference between the observed
    values and the predicted values (**residuals**). It helps identify patterns or
    outliers in the residuals that may indicate areas for improvement in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence intervals**: Confidence intervals are a measure of the uncertainty
    in an estimate. They can be used to determine the range of values in which the
    true performance of a model is likely to fall with a certain level of confidence.
    Confidence intervals can be useful for comparing the performance of different
    models or for determining whether the performance of a model is statistically
    significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gathering insights from predictions**: This will be covered in [*Chapter
    11*](B18187_11.xhtml#_idTextAnchor172), *Explaining Neural* *Network Predictions*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpreting neural networks**: This will be covered in [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184),
    *Interpreting* *Neural Networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness analysis**: This will be covered in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial analysis**: This will be covered in [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing* *Adversarial Performance*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we will only be covering methods that relate to neural networks
    in some way. In the next section, we will start with the engineering baseline
    evaluation method, which is the model evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering the base model evaluation metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Engineering a metric for your use case is a skill that is often overlooked.
    This is most likely because most projects work on a publicly available dataset,
    which almost always already has a metric proposed. This includes projects on Kaggle
    and many public datasets people use to benchmark against. However, this does not
    happen in real life and a metric doesn’t just get served to you. Let’s explore
    this topic further here and gain this skillset.
  prefs: []
  type: TYPE_NORMAL
- en: The model evaluation metric is the first evaluation method that is essential
    in supervised projects, excluding unsupervised-based projects. There are a few
    baseline metrics that exist to be the *de facto* metrics depending on the problem
    and target type. Additionally, there are also more customized versions of these
    baseline metrics that are catered to special objectives. For example, generative-based
    tasks can be evaluated through a special human-based opinion score called the
    mean opinion score. The recommended go-to strategy here is to always start with
    a baseline metric and work your way up to metrics that reflect how errors should
    be distributed properly in different conditions in your use case, similar to how
    it is recommended to build models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are baseline metrics for different conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary** **classification problems**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: This is the percentage of correctly classified examples (true
    positives and true negatives) out of all examples. It is the most widely known
    evaluation metric across any domain. However, in reality, this is a skewed metric
    that can cloud the actual positive prediction performance of the model due to
    the natural oversupply of negatives in most datasets. If there are 99 negative
    examples and 1 positive example, predicting negative all the time without a model
    can get you 99% accuracy! Accuracy still remains the main method for model evaluation,
    but it is not practically used. When somebody says the model is *accurate*, they
    probably aren’t using the accuracy metric.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: This is the proportion of true positives among all predicted
    positive examples. It is a robust alternative that focuses on false positives.
    A prediction threshold is needed here for binary classification projects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: This is the proportion of true positives among all actual positive
    examples. It is a robust alternative that focuses on false negatives. A prediction
    threshold is needed here for binary classification projects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score**: The F1 score is the harmonic mean of precision and recall. It
    provides a balanced measure of a model’s performance. The formula of harmonic
    mean is'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: n _  1 _ x1+  1 _ x2…  1 _ xn
  prefs: []
  type: TYPE_NORMAL
- en: where is the total number of samples and is the individual sample value. There
    is also the F2 score that weighs recall more than precision. Use the F1 score
    if you care about both false positives and false negatives equally, and use the
    F2 score if you care about false negatives more than false positives. For example,
    in an intrusion detection use case, if you want to capture any intruders, you
    can’t afford to have false negatives, but you can afford to have false positives,
    so using the F2 score would be better. Note that the harmonic mean is utilized
    instead of the arithmetic mean to ensure that extreme values are penalized. For
    example, a recall of 1.0 and a precision of 0.01 will result in 0.012 instead
    of something close to 0.5\. A prediction threshold is needed here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Area under the receiver operating characteristic curve** (**AUC ROC**): This
    is the area under the ROC curve, which provides a measure of a model’s ability
    to distinguish between positive and negative examples. No threshold is needed
    here. It is recommended for use when the positive and negative classes are balanced
    so you don’t need to tune the prediction threshold like for F1 and F2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean average precision** (**mAP**): This is an extension on top of the precision
    metric where instead of a single threshold, multiple thresholds are used to compute
    precision and averaged up to obtain the more robust precision value at different
    thresholds. For multiple classes, average precision is computed independently
    and averaged up to obtain mAP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass** **classification problems**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macro**: Calculate any binary classification metric for each class individually
    and then average them'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Micro**: Determine the overall true positive and false positive rates by
    considering the highest predicted class, and then use these rates to calculate
    the overall precision'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression problems**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**): This is the average of the squared differences
    between predicted and actual values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root mean squared error** (**RMSE**): This is the square root of the MSE.
    It provides values at the same scale as the target data and is recommended over
    MSE.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**): This is the average of the absolute differences
    between predicted and actual values. Use this over RMSE when you care about differences
    between predicted and actual labels without caring about its sign.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R-squared**: A measure of how well the model fits the data, which ranges
    from 0 (poor fit) to 1 (perfect fit).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilabel**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label ranking average precision** (**LRAP**): This is the average precision
    of each ground truth label assigned to a particular sample. It takes into consideration
    the ranking of labels predicted against the ground truth labels and assigns scores
    appropriately according to how far or close a ground truth label is in the ranks.
    LRAP is beneficial for use cases such as movie recommendation systems, where predicting
    multiple labels and their rankings is important. It evaluates the model’s ability
    to predict the correct labels and their order of relevance, making it an ideal
    metric for tasks that require accurate and meaningful rankings, such as genre
    recommendations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image and video labels**: Raw image and video frames, when used directly
    as labels, require their own set of custom metrics that can provide more meaningful
    evaluations compared to standard regression metrics. These are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Peak signal-to-noise ratio** (**PSNR**): This is a measure of the quality
    of a reconstructed image or video based on the difference between the original
    and the reconstructed image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structural similarity index** (**SSIM**): This is a measure of the structural
    similarity between two images or video frames.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text labels**: Although standard classification loss and metrics can be used
    for text prediction tasks, some metrics can measure much higher-level concepts
    that are more in tune with human intuition. These are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bleu score**: This is a measure of the similarity between machine-generated
    text and human-generated text based on n-gram overlap.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word error rate** (**WER**): This is a measure of the error rate in automatic
    speech recognition systems based on the number of errors in word sequences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human quality-based metrics**: These are non-programmatically computable
    metrics that can only be evaluated by humans manually:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean opinion score** (**MOS**): This is a subjective quality rating given
    by human observers, which can be used to validate and calibrate objective quality
    metrics'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User engagement**: Metrics such as time spent on a website or app, click-through
    rate, or bounce rate can be used to measure user engagement and satisfaction'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task completion rate**: This is the proportion of users who successfully
    complete a given task or goal, which can be used to evaluate the usability and
    effectiveness of a product or service'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The baseline metrics are a set of common metrics that should be your first
    choices depending on your problem types and conditions. With that said, the choice
    of which base metric to use still depends on the specific problem and the trade-offs
    between the different aspects of the model’s performance that are important for
    the task at hand. Here are step-by-step recommendations on how to actually choose
    and utilize an appropriate model evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the problem. Consider the nature of the problem, the data, and the
    desired outcomes. This is a key step that will help you identify the key criteria
    that are most important for your task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be mindful of the quality of the data. The pillars of data quality (representativeness,
    consistency, comprehensiveness, uniqueness, fairness, and validity) introduced
    in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*,
    will all affect what the metric actually represents. If you are evaluating a model’s
    performance on a bad dataset, then the chosen metric may not reflect the model’s
    true performance on real-world data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the perspective of the users who will be interacting with the model
    or the output of the model. What are their expectations and requirements? What
    are the relevant quality factors that need to be taken into account?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define clear objectives for what the predictions need to accomplish and the
    opposite of what they need to do.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a metric that aligns with your defined objective. The metric you choose
    should align with your overall objective. For example, in a binary classification
    medical use case for detecting cancer, making a false positive diagnosis of cancer
    can ruin many years of a patient’s life, so choose a metric such as precision
    to allow you to quantitatively reduce the number of false positives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider base metrics that are commonly used in similar problems and how they
    might need to be adapted or modified to suit the current problem. Are there any
    unique aspects of the problem that require a different type of metric or a modification
    of an existing metric?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A single metric may not always capture the full performance of a model. Consider
    using multiple metrics that evaluate different aspects of the model’s performance.
    A clear example of this is the creation of an F1 score that combines two metrics:
    precision and recall.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the trade-offs between metrics. For example, in a binary classification
    project, increasing recall performance by modifying the model’s prediction threshold
    can adversely affect precision. Evaluate the trade-offs (if any) between the metrics
    and choose the one that best aligns with your objectives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validate the metric. Making sure the metric is computed on a validation
    and holdout set instead of just the training set is essential to estimating the
    model’s real-world performance. Computing the metric on both the training data
    and validation data in every epoch will also allow you to visualize the learning
    curve using the chosen metric directly instead of using the utilized loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider optimizing your model to the metric directly. Some metrics can be approximately
    reproduced in the deep learning libraries directly and utilized as loss. Using
    direct optimization can sometimes help you get a better model specifically for
    the metric you’ve chosen. However, there might also be some pitfalls that can
    happen. We will discuss this more extensively later in the *Directly optimizing
    the* *metric* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build, evaluate, and compare many models against the evaluation metric by iteratively
    improving it or just using a diverse variety of techniques. The process of building
    a machine learning model might just be the shortest process in the entire ML lifecycle.
    But remember that the amount of effort you put into the building and experimentation
    process can determine whether a project fails or succeeds. Building a variety
    of models and iteratively improving them can be a hard and time-consuming task.
    Having boilerplate code that can adapt to most use cases can make this process
    way faster and more seamless so that you can put your effort into more pressing
    issues in a project. Alternatively, you can also consider using AutoML tools to
    consistently get a variety of models trained for each use case. Look forward to
    the last chapter in this book where we will get a feel for how AutoML tools can
    streamline the model-building process!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure you define a success criterion that relates to the chosen metric.
    To translate a chosen evaluation metric into an actual success metric, it is important
    to establish a clear understanding of what constitutes success for the given problem.
    This will typically involve defining a threshold or target level of performance
    that the model needs to achieve in order to be considered successful. Sometimes,
    the success criteria can have a more fine-grained definition based on specific
    types of error or specific groups of data. We will explore this in depth later
    in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By carefully considering these recommendations, it is possible to select a unique
    metric that accurately measures the relevant aspects of the problem. Ultimately,
    this can help to develop more accurate and effective machine learning solutions
    that will lead to better performance and more successful outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Base metrics are a group of metrics that are commonly used by many practitioners
    to evaluate the performance of a model. However, in some cases, a particular problem
    may have additional criteria and unique behaviors that need to be considered when
    assessing model performance. In such situations, it may be necessary to adopt
    alternative or customized metrics that better suit the specific needs of the problem
    at hand. Base metrics can be further adapted to the additional ideals you want
    to use to judge your models. The next topic will explore custom metrics and their
    applications, including when it is appropriate to use them as the most suitable
    metric for a specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring custom metrics and their applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Base metrics are generally sufficient to meet the requirements of most use cases.
    However, custom metrics build upon base metrics and incorporate additional goals
    that are specific to a given scenario. It’s helpful to think of base metrics as
    a bachelor’s degree and custom metrics as a master’s or PhD degree. It’s perfectly
    fine to use only base metrics if they meet your needs and you don’t have any additional
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Custom ideals often arise naturally early on in a project and are highly dependent
    on the specific use case. Most real use cases don’t expose their chosen metrics
    to the public, even when the prediction of the model is meant to be utilized publicly,
    such as **Open AI**’s **ChatGPT**. However, in machine learning competitions,
    companies with real use cases accompanied by data publish their chosen metric
    publicly to find the best model that can be built. In such a setting for a project,
    the company that hosts the competition is incentivized to perform good-quality
    metric engineering work that reflects its ideals for its use case. A metric can
    affect the resulting best model and will ultimately cost the company money when
    it doesn’t engineer a good metric that matches their ideals. Some competitions
    provide prizes of up to 100,000 USD!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will present some common and publicly shared custom ideals
    along with associated metrics and use cases from machine learning competitions
    that could be useful for you to consider, regardless of your particular use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ideals** | **Use case** | **Custom metric** |'
  prefs: []
  type: TYPE_TB
- en: '| For time-series regression point-based forecasting, the targets are seasonal
    and can fluctuate widely based on the season the data is in. We want a metric
    that can make sure errors aren’t weighted heavily to any one season. | **M5 forecasting—accuracy
    (Kaggle)**: This involves predicting the number of Walmart retail goods units
    sold. The competition provided sales time-series data from Walmart that followed
    a hierarchical structure, beginning at the item level and progressing to department,
    product category, and store levels. The data was generously provided and covered
    three regions in the United States: California, Texas, and Wisconsin. | **Weighted
    root mean squared scaled error** (**WRMSSE**): The main part here is the RMSSE,
    which is a modification of RMSE. Before applying the root of MSE, RMSSE divides
    the standard MSE by the MSE that uses most recent observation as ground truth.
    This makes sure all RMSE from any season will be scaled into the same range of
    values. |'
  prefs: []
  type: TYPE_TB
- en: '| Some labels/classes don’t matter as much in reality, either because they
    don’t occur as much or they just don’t impact post-prediction decisions as much.
    Don’t judge the model too much on unimportant labels; put further emphasis on
    the errors of more important labels/classes. | `any` was made to account for all
    other types of hemorrhage not accounted for with a specific label. The `any` label
    had 2-3 times more data than any other label alone. | `any` label was weighted
    more than any other label even though it had more data. This shows that the significance
    of a label for a metric is not exclusive to the scarcity of the data associated
    with the label in a dataset; it really depends on the specific problem context.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **M5 Forecasting—accuracy (Kaggle)**: This involves predicting the number
    of Walmart retail goods units sold. The competition provided time-series sales
    data from Walmart that followed a hierarchical structure, beginning at the item
    level and progressing to department, product category, and store levels. The data
    was generously provided and covered three regions in the United States: California,
    Texas, and Wisconsin. | **WRMSSE**: The competition author valued more unit sales
    forecast from products that provided more significant sales in dollars. The weight
    for each product is obtained by using the sales volumes for the product in the
    last 28 observations of the training sample (sum of units sold multiplied by their
    respective prices). |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Walmart Recruiting—Store Sales Forecasting (Kaggle)**: This involves
    forecasting the sales of Walmart goods. | **Weighted mean absolute error** (**WMAE**):
    Walmart weighed holiday weeks forecast error more than non-holiday weeks by five-fold,
    as they have much higher sales during holiday weeks. |'
  prefs: []
  type: TYPE_TB
- en: '| We don’t really care too much about small errors, as they can be tolerated,
    but we care about big errors because they can result in the triggering of unwanted
    actions by consuming the predictions. | **Google Analytics customer revenue prediction
    (Kaggle)**: This is a regression problem about predicting the total revenue generated
    by a customer for an online store. The revenue values were highly skewed and contained
    many zero values, which made it challenging to evaluate the performance of the
    participating models using traditional metrics such as MSE or MAE. | **Root mean
    squared log error (RMSLE)**: RMSLE applies a logarithmic transformation to the
    predicted and actual values before computing RMSE, which helps to penalize large
    errors more heavily than small errors. A natural way of doing this is to simply
    train the model to predict the log values of the target and apply RMSE to achieve
    RMSLE. |'
  prefs: []
  type: TYPE_TB
- en: '| **Diabetic retinopathy detection (Kaggle)**: This is a multiclass problem
    about predicting whether high-resolution retina images have diabetic retinopathy
    or not. The problem has five classes: one for no disease and the other four for
    different severities of the disease. | **Quadratic Weighted Kappa (QWK) score**:
    The Kappa score is a statistical measure that provides a single value to quantify
    the degree of agreement between predicted and actual labels in multi-class classification
    tasks. The quadratic weighing mechanism enables the Kappa score to become more
    robust to minor errors and more sensitive to larger ones. The quadratic weighting
    scheme can address issues where the Kappa score may be inflated by a high proportion
    of agreement in easy-to-classify categories while still providing an accurate
    representation of the level of agreement for the more difficult cases. |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Two Sigma Connect** **Rental listing inquiries (Kaggle)**: This involves
    predicting how popular an apartment rental listing is based on the listing content,
    such as text descriptions, photos, number of bedrooms, and price. | **Log loss**:
    Log loss is an evaluation metric that emphasizes wrong predictions by penalizing
    models that are confident about incorrect predictions. |'
  prefs: []
  type: TYPE_TB
- en: '| In a multiclass problem, we have a higher tolerance for our multiclass model
    in our use case where we can consume several of the top predicted classes instead
    of using the single most probable class to maximize the true positive hit rate
    performance of the model. | **Airbnb new user bookings (Kaggle)**: This is a multiclass
    problem for predicting the country in which a new user will make their first booking
    with Airbnb (including a no-booking class). The predicted class will allow Airbnb
    to share more personalized content with their community, decrease the average
    time to first booking, and better forecast demand.Airbnb had a relaxed requirement
    for the multiclass predictions a model produced, where they could perform personalized
    content based on the top five countries instead of a single country to improve
    the true positive hit rate. | **Normalized discounted cumulative gain (NDCG) of
    top-k classes**: NDCG is a measure of the ranking effectiveness of top-k classes,
    usually used for recommendation use cases. This metric pairs nicely with the tolerance
    Airbnb has for multiclass predictions. It would work well for any multiclass use
    case that can tolerate using several top predicted classes instead of the single
    predicted top class. |'
  prefs: []
  type: TYPE_TB
- en: '| For multi-object video-based tracking use cases, we want to use a metric
    that penalizes undesired tracking behaviors, such as false object identification
    and failure to track objects consistently over time. Additionally, we need a metric
    that can be computed by the multiple trajectories created by a model through time
    against a fixed set of ground truth tracking trajectories. | `Track 1`, we have
    created a large-scale synthetic dataset of animated people. All camera feeds in
    our dataset are high-resolution (1080p) feeds with frame rates of 30 frames per
    second. | **Identity F1 score (IDF1)**: IDF1 handles this by evaluating the overall
    performance of the tracking algorithm based on how well it matches predicted tracks
    to ground truth tracks, taking into account the identity of each object over time.
    The algorithm penalizes false positives, false negatives, and identity switches,
    which occur when the algorithm incorrectly identifies two detections as the same
    object or incorrectly assigns two different identities to the same object over
    time. Most importantly, the model can dynamically determine matching trajectories
    based on a version of the **intersection over union** (**IOU**) algorithm between
    predicted and ground truth trajectories. |'
  prefs: []
  type: TYPE_TB
- en: '| We care about bias in our model’s performance a lot more than the overall
    model’s performance. | **Jigsaw unintended bias in toxicity classification (Kaggle)**
    [https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evalu):
    This involves predicting the different intensities of toxicity in text data. The
    data contains identities that the competition author wishes to optimize against
    in the name of bias. The identities involved aremale, female, transgender, other
    gender, heterosexual, homosexual (gay or lesbian), bisexual, other sexual orientation,
    Christian, Jewish, Muslim, Hindu, Buddhist, atheist, other religion, black, white,
    Asian, Latino, other race or ethnicity, physical disability, intellectual or learning
    disability, psychiatric or mental illness, and other disability. | **Bias-focused
    AUC**: The competition focuses on a weighted combination of multiple AUC metric
    that are computed on a different subset of the dataset based on specific identities
    that can be mentioned in the text data row. Overall, AUC is combined equally with
    the next three bias-focused metrics.**Identity-based subgroup AUC**: This involves
    analyzing a dataset that only includes comments mentioning a specific identity
    subgroup. A low score in this metric indicates that the model struggles to distinguish
    between toxic and non-toxic comments related to that identity. |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **Background positive, subgroup negative (BPSN) AUC**: This involves
    evaluating the model’s performance on non-toxic examples mentioning the identity
    and toxic examples that do not. A low score in this metric suggests that the model
    may incorrectly label non-toxic examples related to the identity as toxic. |'
  prefs: []
  type: TYPE_TB
- en: '| **Background negative, subgroup positive (BNSP) AUC**: This involves assessing
    the model’s performance on toxic examples mentioning the identity and non-toxic
    examples that do not. A low score in this metric indicates that the model may
    incorrectly label toxic examples related to the identity as non-toxic. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – A table of custom ideals with example use cases and metrics used
  prefs: []
  type: TYPE_NORMAL
- en: In the second ideal, the general metric idea is to apply weights to any metric
    you’d like to choose based on what you deem more important. Additionally, weights
    can be applied more flexibly to any auxiliary data that does not act as an input
    to a model, nor as a target to the model. Finally, for the last ideal, look forward
    to [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    to discover methods to optimize against bias.
  prefs: []
  type: TYPE_NORMAL
- en: While the examples of custom ideals and metrics we’ve discussed are useful guidelines,
    it’s important to remember that there are many different metrics and ideals that
    may be relevant to your specific use case. Don’t be afraid to dig deeper into
    your problem domain and identify metrics that are unique to your particular situation.
  prefs: []
  type: TYPE_NORMAL
- en: The examples we’ve given can serve as a helpful cheat sheet for developing custom
    metrics that are tailored to your needs. By understanding the reasoning behind
    the use of these special metrics in specific domains, you can gain insight into
    the factors that should be considered when evaluating model performance. Ultimately,
    the key is to choose metrics that align with your goals and capture the most important
    aspects of your problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss a robust strategy to compare the metric performance of
    different models across multiple metric values computed from different cross-validation
    folds or dataset partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring statistical tests for comparing model metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, metric-based model evaluation often involves using averages
    of aggregated metrics from different folds or partitions, such as holdout and
    validation sets, to compare the performance of various models. However, relying
    solely on these average performance metrics may not provide a comprehensive assessment
    of a model’s performance and generalizability. A more robust approach to model
    evaluation is the incorporation of statistical hypothesis tests, which assess
    whether observed differences in performance are statistically significant or due
    to random chance.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical hypothesis tests are procedures used to determine whether observed
    data provides sufficient evidence to reject a null hypothesis in favor of an alternative
    hypothesis, helping to quantify the likelihood that the observed differences are
    due to random chance or a genuine effect. In statistical tests, the null hypothesis
    (H0) is a default assumption that states there is no effect or relationship between
    variables, serving as a basis for comparison against the alternative hypothesis
    with the goal of determining whether the observed data provides enough evidence
    to reject this default assumption. For the purpose of comparing model metric performance
    across multiple partitions and datasets, the null hypothesis is typically that
    there is no difference between the performances of the models, while the alternative
    hypothesis is that there are differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, statistical tests offer a formal framework to objectively determine
    whether differences in performance are significant or due to chance. Additionally,
    statistical tests offer a comprehensive understanding of model performance by
    accounting for variability and uncertainty in metrics. The following table shows
    common statistical tests that you can consider using, along with the Python code
    needed to execute it, how to interpret the result, and when to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Statistical test** | **Python code** | **Result interpretation** | **Recommended
    use** |'
  prefs: []
  type: TYPE_TB
- en: '| Paired t-test | `from scipy.stats` `import ttest_rel``t_stat, p_value =`
    `ttest_rel(model1_scores, model2_scores)` | If `p_value` < 0.05, there’s a significant
    difference between the models. | Use this when comparing two dependent samples
    with normally distributed differences. |'
  prefs: []
  type: TYPE_TB
- en: '| Mann-Whitney U test | `from scipy.stats` `import mannwhitneyu``u_stat, p_value
    =` `mannwhitneyu(model1_scores, model2_scores)` | If `p_value` < 0.05, there’s
    a significant difference between the models. | Use this when comparing two independent
    samples with non-normally distributed data or ordinal data. |'
  prefs: []
  type: TYPE_TB
- en: '| **Analysis of** **variance** (**ANOVA**) | `from scipy.stats import f_oneway
    f_stat, p_value = f_oneway(model1_scores,` `model2_scores, model3_scores)` | If
    `p_value` < 0.05, there’s a significant difference among the models. | Use this
    when comparing three or more independent samples with normally distributed data
    and equal variances. |'
  prefs: []
  type: TYPE_TB
- en: '| Kruskal-Wallis H test | `from scipy.stats` `import kruskal``h_stat, p_value
    = kruskal(model1_scores,` `model2_scores, model3_scores)` | If `p_value` < 0.05,
    there are significant differences between the models. | Use this when comparing
    three or more independent samples with non-normally distributed data or ordinal
    data. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – Common statistical tests with details on their Python implementation,
    result interpretations, and recommendations on when to use them
  prefs: []
  type: TYPE_NORMAL
- en: These recommendations can help you choose the appropriate statistical test based
    on the conditions and assumptions of your data, such as the number of samples,
    the type of data (dependent or independent), and the distribution of the data
    (normal or non-normal).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss how the outcome of metric engineering can be converted
    to a success criterion.
  prefs: []
  type: TYPE_NORMAL
- en: Relating the evaluation metric to success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining success in a machine learning project is crucial and should be done
    at the early stages of the project as introduced in the *Defining success* section
    in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*.
    Success can be defined as achieving higher-level objectives, such as improving
    the efficiency of processes or increasing the accuracy of processes in comparison
    to manual labor. In some rare cases, machine learning can enable processes that
    were previously impossible due to human limitations. The ultimate success of achieving
    these objectives is to save costs or earn more revenue for an organization.
  prefs: []
  type: TYPE_NORMAL
- en: A model with a metric performance score of 0.80 F1 score or 0.00123 RMSE doesn’t
    really mean anything at face value and has to be translated to something tangible
    in the use case. For instance, one should answer questions such as what estimated
    model score can allow the project to achieve the targeted cost savings or revenue
    improvements. Quantifying the success that can be obtained from model performance
    is essential, particularly as machine learning projects can be expensive to execute.
    Sometimes, the return on investment can be low if the model fails to perform at
    a certain level. After selecting the evaluation metric, it’s important to establish
    a metric threshold for success that is realistic, achievable, and based on the
    business objective.
  prefs: []
  type: TYPE_NORMAL
- en: As a finale to the topic, let’s go through an example workflow to relate the
    evaluation metric to success based on a hypothetical use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a use case to identify defective products in a manufacturing
    process using image data. Let’s assume that the cost of producing a product is
    $50 and it has a retail price of $200\. If that product is defective and it makes
    it through to a customer, this will result in an additional chargeback of $1,000
    and the return of the $200 paid by the customer to compensate for the defective
    product, which may have caused harm. On the other hand, if a good product is identified
    as defective and scrapped, it results in a cost of $250; $50 is used to produce
    it and $200 is lost opportunity cost as it is scrapped.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we have a dataset of 10,000 product images, which is the amount
    of products produced a month. If we don’t use a model and produce and ship all
    10,000 products, we would have 95 defective products, which would cost $500,000
    in production costs (10,000 x $50) and $95,000 in chargeback costs, resulting
    in a total cost of $595,000\. After selling all the non-defective products, the
    company will gain $1,981,000 in sales (9,905 x $200). The total money earned will
    be $1,386,000.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a case in which we use a trained deep learning model to classify these
    images as either good (negative) or defective (positive). To make sure it is worth
    using a deep learning model to optimize this process, let’s say the company needs
    to gain at least $120,000 a year in cash to make this worthwhile. Let’s also consider
    that maintaining a machine learning model costs $20,000 per month. The threshold
    in any metric needs to relate to the result of making at least $30,000 gains per
    month.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to use an F score-based metric. Since false negatives affect the
    total money more than false positives, we might want to use the F2 score instead
    of the F1 score. But will models with the same metric score exhibit different
    monetary returns for either of the two metrics? This is essential to understand
    so that a proper success threshold can be set. Let’s attempt to use Python code
    to analyze score behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the number of actual positives and actual negatives and the
    total data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s define methods to compute precision, recall, F1 score, and F2 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s define the method to compute the final cash we will have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the `compute_total_cash` method to compute the baseline cash for
    the current setup where no model is used. This is so that we can find out the
    cash success threshold required for a model to be considered valuable enough to
    be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will simulate every possible combination of true positives (`tp`),
    false positives (`fp`), true negatives (`tn`), and false negatives (`fn`) and
    compute the F1 and F2 scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s plot both of the scores independently against the total cash return
    while drawing horizontal lines using `threshold_cash_line` and `baseline_cash`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![](img/B18187_10.01_(A).jpg)![Figure 10.1 – Cash \uFEFFversus F1 score and\
    \ cash \uFEFFversus F2 score](img/B18187_10.01_(B).jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Cash versus F1 score and cash versus F2 score
  prefs: []
  type: TYPE_NORMAL
- en: The figure suggests that the F2 score has huge fluctuations in the lower score
    range even though it weights recall higher. The goal here is to make sure a metric
    can be properly linked to success, so having wider cash fluctuations with the
    same score isn’t a desirable trait. F1 score would probably be the wiser choice
    here. Using the topmost horizontal line (which references the minimum 30,000 cash
    threshold we need to hit) as a reference, we want to find a point where it is
    not even possible to get a lower score than the threshold in the F1 score graph.
    Roughly, a 0.65 F1 score should guarantee that the model can produce a score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example demonstrates the level of analysis required to properly choose
    a metric and find a threshold that can be directly linked to success while taking
    into consideration both monetary profit and loss. However, it is important to
    note that not all machine learning projects can be measured in terms of dollar
    cost. Some projects may not be directly related to cash, and that is perfectly
    acceptable. However, to be successful, it is important to quantify the value of
    the model in a way that stakeholders can understand. If nobody understands the
    value that the model provides, the project is unlikely to be successful.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive into the idea of directly optimizing the metric in a deep learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Directly optimizing the metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The loss and the metric used to train a deep learning model are two separate
    components. One of the tricks you can use to improve a model’s accuracy performance
    against the chosen metric is to directly optimize against it instead of just monitoring
    performance for the purpose of choosing the best performing model weights and
    using early stopping. In other words, using the metric as a loss directly!
  prefs: []
  type: TYPE_NORMAL
- en: By directly optimizing for the metric of interest, the model has a chance to
    improve in a way that is relevant to the end goal rather than optimizing for a
    proxy loss function that may not be directly related to the ultimate performance
    of the model. This simply means that the model can result in a much better performance
    when using the metric as a loss directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, not all metrics can be used as a loss, as not all metrics can be differentiable.
    Remember that backpropagation requires all functions used to be differentiable
    so that gradients can be computed to update the neural network weights. Note that
    discontinuous methods are not all differentiable. Here are some common discontinuous
    functions that are not differentiable, along with NumPy methods to look out for
    easy identification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.min`, `np.max`, `np.argmin`, and `np.argmax`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`np.clip`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other functions in NumPy to look out for are `np.sign`, `np.piecewise`, `np.digitize`,
    `np.searchsorted`, `np.histogram`, `np.fft`, `np.count_nonzero`, `np.round`, `np.cumsum`,
    and `np.percentile`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, using a metric as a loss function can sometimes lead to suboptimal
    performance because the metric does not always capture all aspects of the problem
    that the model needs to learn in order to perform well. Some important aspects
    of a problem may be difficult to measure directly or to include in a metric. These
    aspects may form the foundation needed for a model to learn before it can proceed
    to slowly get better at the chosen metric. For example, in image recognition,
    the model needs to learn to recognize more abstract features, such as texture,
    lighting, or viewpoint before it can attempt to get better at accuracy. If these
    features are not captured in the metric, the model may not learn to recognize
    them, resulting in suboptimal performance. A good solution here is to experiment
    with more conventional loss functions initially and then fine-tune the model using
    either only the metric as a loss or a combination of the original loss and the
    metric as a loss.
  prefs: []
  type: TYPE_NORMAL
- en: While using a metric as a loss function can be beneficial in some cases, it’s
    not a surefire method of improving performance. The efficacy of this approach
    largely depends on the specific use case and the complexity of the problem being
    addressed. The performance boost achieved through this method might be minimal
    and too nuanced for some projects to even consider. However, when used successfully,
    it can lead to meaningful improvements in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly explored an overview of different model evaluation
    methods and how they can be used to measure the performance of a deep learning
    model. We started with the topic of metric engineering among all the introduced
    methods. We introduced common base model evaluation metrics. On top of this, we
    discussed the limitations of using base model evaluation metrics and introduced
    the concept of engineering a model evaluation metric tailored to the specific
    problem at hand. We also explored the idea of optimizing directly against the
    evaluation metric by using it as a loss function. While this approach can be beneficial,
    it is important to consider the potential pitfalls and limitations, as well as
    the specific use case for which this approach may be appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation of deep learning models requires careful consideration of appropriate
    evaluation methods, metrics, and statistical tests. Hopefully, after reading through
    this chapter, I have helped ease your journey into metric engineering, encouraged
    you to take the first step toward deeper metric engineering by following the guidelines
    provided, and highlighted the importance of metric engineering as a valuable component
    to improve model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: However, some information can be hidden behind a single metric value no matter
    how good and appropriate the final chosen metric is. In the next chapter, we will
    cover a key method that can help uncover either wanted or unwanted hidden behaviors
    of your neural model based on how your neural network model makes predictions.
  prefs: []
  type: TYPE_NORMAL
