<html><head></head><body>
		<div id="_idContainer093">
			<h1 id="_idParaDest-138" class="chapter-number"><a id="_idTextAnchor159"/>8</h1>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor160"/>Rule-Based Techniques</h1>
			<p><strong class="bold">Rule-based techniques</strong> are a<a id="_idIndexMarker613"/> very important and useful tool<a id="_idIndexMarker614"/> in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). Rules <a id="_idIndexMarker615"/>are used to examine text and decide how it should be analyzed in an all-or-none fashion, as opposed to the statistical techniques we will be reviewing in later chapters. In this chapter, we will discuss how to apply rule-based techniques to NLP. We will look at examples such as regular expressions, syntactic parsing, and semantic role assignment. We will primarily use the NLTK and spaCy libraries, which we have seen in <span class="No-Break">earlier chapters.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Rule-based techniques</span></li>
				<li>Why <span class="No-Break">use rules?</span></li>
				<li>Exploring <span class="No-Break">regular expressions</span></li>
				<li>Sentence-level analysis – syntactic parsing and semantic <span class="No-Break">role assignment</span></li>
			</ul>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor161"/>Rule-based techniques</h1>
			<p>Rule-based techniques<a id="_idIndexMarker616"/> in NLP are, as the name suggests, based on rules written by human developers, as opposed to machine-learned models derived from data. Rule-based techniques were, for many years, the most common approach to NLP, but as we saw in <a href="B19005_07.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, rule-based approaches have largely been superseded by numerical, machine-learned approaches for the overall design of most NLP applications. There are many reasons for this; for example, since rules are written by humans, it is possible that they might not cover all situations if the human developer has <span class="No-Break">overlooked something.</span></p>
			<p>However, for practical applications, rules can be very useful, either by themselves or, more likely, along with <span class="No-Break">machine-learned models.</span></p>
			<p>The next section will discuss the motivations for using rules in <span class="No-Break">NLP applications.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor162"/>Why use rules?</h1>
			<p>Rules are a <a id="_idIndexMarker617"/>useful approach in one or more of the <span class="No-Break">following situations:</span></p>
			<ul>
				<li>The application you are developing has a requirement to analyze fixed expressions that include thousands or even millions of variants when it would be extremely difficult to provide enough data to learn a machine model. These kinds of expressions include numbers, monetary amounts, dates, and addresses, for example. It is hard for systems to learn models when the data is so diverse. Moreover, it is usually not necessary, as rules to analyze these expressions are not difficult to write because their formats are very structured. For both of these reasons, a rule-based approach is a simpler solution for recognizing <span class="No-Break">fixed expressions.</span></li>
				<li>Very little training data is available for the application, and creating new data would be expensive. For example, annotating new data might require very specialized expertise. Although there are now techniques (such as few-shot or zero-shot learning) that can adapt large pre-trained models to cover specialized domains, if the domain-specific data is very different from the original training data in terms of syntax or vocabulary, it will be difficult for the adaptation process to work well. Medical reports and air traffic control messages are examples of these kinds <span class="No-Break">of data.</span></li>
				<li>There are existing, well-tested rules and libraries available that can easily be used in new applications, such as the Python <strong class="source-inline">datetime</strong> package for recognizing dates <span class="No-Break">and times.</span></li>
				<li>The goal is to bootstrap a machine-learned model by preliminary annotation of a corpus. Corpus annotation is needed in preparation for using that corpus as training data for a machine learning model, or for using the corpus as a <em class="italic">gold standard</em> in NLP system evaluation. In this process, the data is first annotated by applying some hand-written rules. Then, the resulting annotated corpus is usually reviewed and corrected by human annotators, since it is likely to contain errors. Even though the corpus requires a review process, an initial annotation by a rule-based system will save time over manual annotation <span class="No-Break">from scratch.</span></li>
				<li>The application needs to find named entities from a fixed, <span class="No-Break">known set.</span></li>
				<li>The results have to be very precise – for example, grammar checking, proofreading, language learning, and <span class="No-Break">authorship studies.</span></li>
				<li>A quick prototype is needed in order to test downstream processing, and the data collection and model training stages needed for machine learning would take too <a id="_idIndexMarker618"/><span class="No-Break">much time.</span></li>
			</ul>
			<p>We will start by looking at regular expressions, a very common technique for analyzing text that contains <span class="No-Break">well-understood patterns.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor163"/>Exploring regular expressions</h1>
			<p><strong class="bold">Regular expressions</strong> are a <a id="_idIndexMarker619"/>widely used rule-based technique that is often used for recognizing<a id="_idIndexMarker620"/> fixed expressions. By <strong class="bold">fixed expressions</strong>, we mean words and phrases that are formed according to their own internal rules, which are largely different from the normal patterns of <span class="No-Break">the language.</span></p>
			<p>One type of fixed expression is <em class="italic">monetary amounts</em>. There are only a few variations in formats for monetary amounts – the number of decimal places, the symbol for the type of currency, and whether the numbers are separated by commas or periods. The application might only have a requirement to recognize specific currencies, which would simplify the rules further. Other common fixed expressions include <em class="italic">dates</em>, <em class="italic">times</em>, <em class="italic">telephone numbers</em>, <em class="italic">addresses</em>, <em class="italic">email addresses</em>, <em class="italic">measurements</em>, and <em class="italic">numbers</em>. Regular expressions in NLP are most frequently used in preprocessing text that will be further analyzed with <span class="No-Break">other techniques.</span></p>
			<p>Different programming languages have slightly different formats for regular expressions. We will be using the <a id="_idIndexMarker621"/>Python formats defined at <a href="https://docs.python.org/3/library/re.html">https://docs.python.org/3/library/re.html</a> and available in the Python <strong class="source-inline">re</strong> library. We will not define regular expression syntax here because there are many resources on the web that describe regular expression syntax, including the Python documentation, and we don’t need to duplicate that. You might find the information at <a href="https://www.h2kinfosys.com/blog/nltk-regular-expressions/">https://www.h2kinfosys.com/blog/nltk-regular-expressions/</a> and <a href="https://python.gotrained.com/nltk-regex/">https://python.gotrained.com/nltk-regex/</a> useful for getting into the details of regular expressions <span class="No-Break">in NLTK.</span></p>
			<p>We will start by going over the basics of operating on strings with regular expressions, followed by some tips for making it easier to apply and debug <span class="No-Break">regular expressions.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor164"/>Recognizing, parsing, and replacing strings with regular expressions</h2>
			<p>The <a id="_idIndexMarker622"/>simplest use of regular expressions is<a id="_idIndexMarker623"/> to simply note that a match occurred. What <a id="_idIndexMarker624"/>we want to do after a fixed expression has <a id="_idIndexMarker625"/>been matched depends on the goal of the <a id="_idIndexMarker626"/>application. In some applications, all we<a id="_idIndexMarker627"/> want to do is recognize that a fixed expression has occurred or did not occur. This can be useful, for example, in validating user input in web forms, so that users can correct invalid address formats. The following code shows how to recognize a US address with <span class="No-Break">regular expressions:</span></p>
			<pre class="source-code">
import re
# process US street address
# the address to match
text = "223 5th Street NW, Plymouth, PA 19001"
print(text)
# first define components of an address
# at the beginning of a string, match at least one digit
street_number_re = "^\d{1,}"
# match street names containing upper and lower case letters and digits, including spaces,
# followed by an optional comma
street_name_re = "[a-zA-Z0-9\s]+,?"
# match city names containing letters, but not spaces, followed by a comma
# note that two word city names (like "New York") won't get matched
# try to modify the regular expression to include two word city names
city_name_re = " [a-zA-Z]+(\,)?"
# to match US state abbreviations, match any two upper case alphabetic characters
# notice that this overgenerates and accepts state names that don't exist because it doesn't check for a valid state name
state_abbrev_re = " [A-Z]{2}"
# match US postal codes consisting of exactly 5 digits. 9 digit codes exist, but this expression doesn't match them
postal_code_re = " [0-9]{5}$"
# put the components together -- define the overall pattern
address_pattern_re = street_number_re + street_name_re + city_name_re + state_abbrev_re + postal_code_re
# is this an address?
is_match = re.match(address_pattern_re,text)
if is_match is not None:
    print("matches address_pattern")
else:
    print("doesn't match")</pre>
			<p>In other cases, we might want to parse the expression and assign meanings to the components – for example, in a date, it could be useful to recognize a month, a day, and a year. In still other cases, we might want to replace the expression with another expression, delete it, or normalize it so that all occurrences of the expression are in the same form. We could even want to do a combination of these operations. For example, if the application is classification, it is likely that we only need to know whether or not the regular expression occurred; that is, we don’t need its content.  In that case, we can replace the expression with a <strong class="source-inline">class</strong> token such as <strong class="source-inline">DATE</strong>, so that a sentence such as <em class="italic">We received the package on August 2, 2022</em> becomes <em class="italic">We received the package on DATE</em>. Replacing the whole expression with a <strong class="source-inline">class</strong> token can also be used for redacting sensitive text such as social <span class="No-Break">security numbers.</span></p>
			<p>The preceding<a id="_idIndexMarker628"/> code shows how to use regular <a id="_idIndexMarker629"/>expressions to match patterns in text <a id="_idIndexMarker630"/>and shows the code to confirm a match. However, this <a id="_idIndexMarker631"/>example just shows how to <a id="_idIndexMarker632"/>confirm that a match exists. We might want to do<a id="_idIndexMarker633"/> other things, such as replace the address with a class label, or label the matched portion of a string. The following code shows how to use the regular expression <strong class="source-inline">sub</strong> method to substitute a class label for <span class="No-Break">an address:</span></p>
			<pre class="source-code">
# the address to match
text = "223 5th Street NW, Plymouth, PA 19001"
# replace the whole expression with a class tag -- "ADDRESS"
address_class = re.sub(address_pattern_re,"ADDRESS",text)
print(address_class)
ADDRESS</pre>
			<p>Another useful operation is to label the whole expression with a semantic label such as <strong class="source-inline">address</strong>, as shown in the following code. This code shows how to add a label to the address. This enables us to identify US addresses in text and do tasks such as counting them or extracting all the addresses <span class="No-Break">from texts:</span></p>
			<pre class="source-code">
# suppose we need to label a matched portion of the string
# this function will label the matched string as an address
def add_address_label(address_obj):
    labeled_address = add_label("address",address_obj)
    return(labeled_address)
# this creates the desired format for the labeled output
def add_label(label, match_obj):
    labeled_result = "{" + label + ":" + "'" + match_obj.group() + "'" + "}"
    return(labeled_result)
# add labels to the string
address_label_result = re.sub(address_pattern_re,add_address_label,text)
print(address_label_result)</pre>
			<p>The result of running the preceding code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
{address:'223 5th Street NW, Plymouth, PA 19001'}</pre>
			<p>Finally, regular<a id="_idIndexMarker634"/> expressions are useful if <a id="_idIndexMarker635"/>we want<a id="_idIndexMarker636"/> to <a id="_idIndexMarker637"/>remove the whole match from the text – for<a id="_idIndexMarker638"/> example, to remove <span class="No-Break">HTML </span><span class="No-Break"><a id="_idIndexMarker639"/></span><span class="No-Break">markup.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor165"/>General tips for using regular expressions</h2>
			<p>Regular expressions can<a id="_idIndexMarker640"/> easily become very complex and difficult to modify and debug. They can also easily fail to recognize some examples of what they’re supposed to recognize and falsely recognize what they’re not supposed to recognize. While it is tempting to try to match the regular expression so that it recognizes exactly what it is supposed to recognize and nothing else, this can make the expression so complicated that it is difficult to understand. Sometimes, it can be better to miss a few edge cases to keep the <span class="No-Break">expression simple.</span></p>
			<p>If we find that an existing regular expression is failing to find some expressions that we want to capture, or incorrectly finding expressions that we don’t want to capture, it can sometimes be difficult to revise the existing expression without breaking some things that used to work. Here are a few tips that can make regular expressions easier to <span class="No-Break">work with:</span></p>
			<ul>
				<li>Write down what you want the regular expression to match first (such as <em class="italic">any two consecutive uppercase alphabetic characters</em>). This will be helpful in both clarifying what you’re trying to do as well as in helping catch any cases that you might <span class="No-Break">have overlooked.</span></li>
				<li>Break complex expressions into components and test each component independently before putting them together. Besides helping with debugging, the component expressions can potentially be reused in other complex expressions. We saw this in the first code block in the previous section with components such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">street_name_re</strong></span><span class="No-Break">.</span></li>
				<li>Use existing tested regular <a id="_idIndexMarker641"/>expressions for common expressions, for example, the Python <strong class="source-inline">datetime</strong> package (see <a href="https://docs.python.org/3/library/datetime.html">https://docs.python.org/3/library/datetime.html</a>), before trying to write your own regular expressions. They have<a id="_idIndexMarker642"/> been well tested over many years by <span class="No-Break">many developers.</span></li>
			</ul>
			<p>The next two sections cover specific ways to analyze two of the most important aspects of natural language: words <span class="No-Break">and sentences.</span></p>
			<p>The next section will start this topic by talking about analyzing <span class="No-Break">individual words.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor166"/>Word-level analysis</h1>
			<p>This section will discuss two<a id="_idIndexMarker643"/> approaches to analyzing words. The first one, lemmatization, involves breaking words down into their components in order reduce the variation in texts. The second one discusses some ideas for making use of hierarchically organized semantic information about the meanings of words in the form <span class="No-Break">of ontologies.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor167"/>Lemmatization</h2>
			<p>In our earlier discussion of <a id="_idIndexMarker644"/>preprocessing text in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we went over the task of <strong class="bold">lemmatization</strong> (and the related task of stemming) as a tool for regularizing text documents so that there is less<a id="_idIndexMarker645"/> variation in the documents we are analyzing. As we discussed, the process of lemmatization converts each word in the text to its root word, discarding information such as plural endings like <em class="italic">-s</em> in English. Lemmatization also requires a dictionary, because the dictionary supplies the root words for the words being lemmatized. We used Princeton<a id="_idIndexMarker646"/> University’s <strong class="bold">WordNet </strong>(<a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a>) as a dictionary when we covered lemmatization in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<p>We will use WordNet’s semantic information about the relationships among words in the next section, where we discuss ontologies and <span class="No-Break">their applications.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor168"/>Ontologies</h2>
			<p><strong class="bold">Ontologies</strong>, which we <a id="_idIndexMarker647"/>briefly mentioned in <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> in the <em class="italic">Semantic analysis</em> section, represent the meanings of related words in hierarchical structures like the one shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> for the word <em class="italic">airplane</em>. The ontology in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> represents the <a id="_idIndexMarker648"/>information that an airplane is a type of heavier-than-aircraft, which is a type of craft, which is a type of vehicle, and so on up to the very generic, top-level <span class="No-Break">category, </span><span class="No-Break"><strong class="source-inline">entity</strong></span><span class="No-Break">.</span></p>
			<p>The ontology in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> is part of the <strong class="bold">WordNet</strong> ontology for English and a number of other languages. These hierarchical relationships are sometimes <a id="_idIndexMarker649"/>called <strong class="bold">is a</strong> relationships. For <a id="_idIndexMarker650"/>example, <em class="italic">an airplane is a vehicle</em>. In this example, we say that <em class="italic">vehicle</em> is a <strong class="bold">superordinate</strong> term<a id="_idIndexMarker651"/> and <em class="italic">airplane</em> is a <strong class="bold">subordinate</strong> term. WordNet uses some of its own terminology. In WordNet terminology, <strong class="bold">hypernym</strong> is the <a id="_idIndexMarker652"/>same thing as a superordinate term, and <strong class="bold">hyponym</strong> is the<a id="_idIndexMarker653"/> same thing as a <span class="No-Break">subordinate term.</span></p>
			<p>WordNet also includes many other semantic relationships, such as synonymy and <em class="italic">part-whole</em>. For example, we can find out that a wing is part of an airplane from WordNet. In addition, WordNet also includes part-of-speech information, and you will recall that we used this part-of-speech information in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> for part-of-speech tagging texts in preparation <span class="No-Break">for lemmatization.</span></p>
			<p>There are other ontologies besides WordNet, and you can even construct your own ontologies with tools such as Stanford University’s Protégé (<a href="https://protege.stanford.edu/">https://protege.stanford.edu/</a>). However, WordNet is a good way to <span class="No-Break">get started.</span></p>
			<p>How can we make use of ontologies such as WordNet in NLP applications? Here are a <span class="No-Break">few ideas:</span></p>
			<ul>
				<li>Develop a <a id="_idIndexMarker654"/>writing tool that helps authors find <a id="_idIndexMarker655"/>synonyms, antonyms, and definitions of words they would like <span class="No-Break">to use.</span></li>
				<li>Count the number of mentions of different categories of words in texts. For example, you might be interested in finding all mentions of vehicles. Even if the text actually says <em class="italic">car</em> or <em class="italic">boat</em>, you could tell that the text mentions a vehicle by looking for words with <em class="italic">vehicle</em> as <span class="No-Break">their hypernym.</span></li>
				<li>Generate additional examples of training data for machine learning by substituting different words with the same superordinate term in different sentence patterns. For example, suppose we have a chatbot that provides advice about cooking. It would probably get questions such as <em class="italic">How can I tell whether a pepper is ripe?</em>, or <em class="italic">Can I freeze tomatoes?</em> There are hundreds of types of food that could be substituted for <em class="italic">pepper</em> and <em class="italic">tomatoes</em> in those questions. It would be very tedious to create training examples for all of them. To avoid this, you could find all of the different types of vegetables in WordNet and generate training data by putting them into sentence templates to create <span class="No-Break">new sentences.</span></li>
			</ul>
			<p>Let’s see an example of the <span class="No-Break">previous strategy.</span></p>
			<p>You probably recall from earlier mentions of WordNet that it is included in NLTK, so we can import it and ask for the list of senses<a id="_idIndexMarker656"/> of <em class="italic">vegetable</em> (<strong class="bold">synsets</strong>) <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
import nltk
from nltk.corpus import wordnet as wn
wn.synsets('vegetable')</pre>
			<p>We’ll then see that there are two <em class="italic">senses</em>, or meanings, of <em class="italic">vegetable</em>, and we can ask for their definitions in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
[Synset('vegetable.n.01'), Synset('vegetable.n.02')]
print(wn.synset('vegetable.n.01').definition())
print(wn.synset('vegetable.n.02').definition())</pre>
			<p>The format of the<a id="_idIndexMarker657"/> sense names such as <strong class="source-inline">vegetable.n.01</strong> should be interpreted as <strong class="source-inline">word</strong> and <strong class="source-inline">part-of-speech</strong> (<em class="italic">n</em> means <em class="italic">noun</em> here), followed by the word’s order in the list of senses. We print the definitions of each of the two senses<a id="_idIndexMarker658"/> so that we can see what the WordNet senses mean. The resulting definitions are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
edible seeds or roots or stems or leaves or bulbs or tubers or nonsweet fruits of any of numerous herbaceous plant
any of various herbaceous plants cultivated for an edible part such as the fruit or the root of the beet or the leaf of spinach or the seeds of bean plants or the flower buds of broccoli or cauliflower</pre>
			<p>The first sense refers to the part that we eat, and the second sense refers to the plants whose parts we eat. If we are interested in cooking, we probably want the first sense of <em class="italic">vegetable</em> as <em class="italic">food</em>. Let’s get the list of all the vegetables in the first sense, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
word_list = wn.synset('vegetable.n.01').hyponyms()
simple_names = []
for word in range (len(word_list)):
    simple_name = word_list[word].lemma_names()[0]
    simple_names.append(simple_name)
print(simple_names)
['artichoke', 'artichoke_heart', 'asparagus', 'bamboo_shoot', 'cardoon', 'celery', 'cruciferous_vegetable', 'cucumber', 'fennel', 'greens', 'gumbo', 'julienne', 'leek', 'legume', 'mushroom', 'onion', 'pieplant', 'plantain', 'potherb', 'pumpkin', 'raw_vegetable', 'root_vegetable', 'solanaceous_vegetable', 'squash', 'truffle']</pre>
			<p>The code goes through the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Collect all the different types of the first sense of <em class="italic">vegetable</em> (the hyponyms) and store them in the <span class="No-Break"><strong class="source-inline">word_list</strong></span><span class="No-Break"> variable.</span></li>
				<li>Iterate through the list of words, collect the lemma for each word, and store the lemmas in the <span class="No-Break"><strong class="source-inline">simple_names</strong></span><span class="No-Break"> variable.</span></li>
				<li>Print <span class="No-Break">the words.</span></li>
			</ol>
			<p>We can then generate <a id="_idIndexMarker659"/>some sample data by filling in a text template with each <a id="_idIndexMarker660"/>word, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
text_frame = "can you give me some good recipes for "
for vegetable in range(len(simple_names)):
    print(text_frame + simple_names[vegetable])
can you give me some good recipes for artichoke
can you give me some good recipes for artichoke_heart
can you give me some good recipes for asparagus
can you give me some good recipes for bamboo_shoot</pre>
			<p>The preceding code shows the first few sentences that we can generate from the text frame and the list of vegetables. Of course, in a real application, we would want to have multiple text frames to get a good variety <span class="No-Break">of sentences.</span></p>
			<p>At the beginning of this section, we listed a few ways to apply ontologies in NLP applications; you can probably come up with more if you think of different ways that you could make use of the meanings of words to solve problems in natural <span class="No-Break">language applications.</span></p>
			<p>However, words don’t occur in isolation; they are combined with other words to create sentences with richer and more complex meanings. The next section will move on from analyzing words to the analysis of entire sentences, which we will analyze both syntactically <span class="No-Break">and semantically.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor169"/>Sentence-level analysis</h1>
			<p>Sentences<a id="_idIndexMarker661"/> can be analyzed in terms of<a id="_idIndexMarker662"/> their <strong class="bold">syntax</strong> (the structural relationships among parts of the sentence) or <a id="_idIndexMarker663"/>their <strong class="bold">semantics</strong> (the relationships among the meanings of the parts of the sentence). We’ll look at both of these types of analysis next. Recognizing syntactic relationships is useful on its own for applications such as grammar checking (does the subject of the sentence agree with the verb? Is the <a id="_idIndexMarker664"/>correct form of the verb being used?), while recognizing semantic relationships on their own is useful for applications such as finding the components of a request in chatbots. Recognizing both syntactic and semantic relationships together is an alternative to statistical methods in almost any <span class="No-Break">NLP application.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor170"/>Syntactic analysis</h2>
			<p>The<a id="_idIndexMarker665"/> syntax of sentences and <a id="_idIndexMarker666"/>phrases can be analyzed in a process called <strong class="bold">parsing</strong>. Parsing is <a id="_idIndexMarker667"/>a type of analysis that attempts to match a set of rules, called grammar, to an input text. There are many approaches to parsing. We will not cover this topic in detail since there are many other resources available if you are interested in this topic, such as <a href="https://en.wikipedia.org/wiki/Syntactic_parsing_(computational_linguistics)">https://en.wikipedia.org/wiki/Syntactic_parsing_(computational_linguistics)</a> or <a href="https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_syntactic_analysis.htm">https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_syntactic_analysis.htm</a>. Chart parsing, dependency parsing, and recursive descent parsing are only a few of the many approaches to syntactic parsing. NLTK includes a <strong class="source-inline">parse</strong> package, which includes a variety of parsing algorithms that you can explore. For our examples in this section, we will use the chart parser in the <strong class="source-inline">nltk.parse.ChartParser</strong> class, which is a common and <span class="No-Break">basic approach.</span></p>
			<h3>Context-free grammars and parsing</h3>
			<p>A very common way to define the rules for syntactic parsing is <strong class="bold">context-free grammars</strong> (<strong class="bold">CFGs</strong>). CFGs <a id="_idIndexMarker668"/>can be used in chart parsing as well as many other parsing <a id="_idIndexMarker669"/>algorithms. You may be familiar with this format because it is widely used in computer science for defining formal languages, such as programming languages. CFGs consist of a set of <em class="italic">rules</em>. Each rule consists<a id="_idIndexMarker670"/> of a <strong class="bold">left-hand side</strong> (<strong class="bold">LHS</strong>) and a <strong class="bold">right-hand side</strong> (<strong class="bold">RHS</strong>), typically separated by <a id="_idIndexMarker671"/>a symbol, such as an arrow. The rule is interpreted to mean that the single symbol on the LHS is made up of the components of <span class="No-Break">the RHS.</span></p>
			<p>For example, the context-free rule <strong class="source-inline">S -&gt; NP VP</strong> states that a sentence (<strong class="source-inline">S</strong>) consists of a noun phrase (<strong class="bold">NP</strong>), followed by a <strong class="bold">verb phrase</strong> (<strong class="bold">VP</strong>). An NP can consist of a <strong class="bold">determiner</strong> (<strong class="bold">Det</strong>) such as <em class="italic">an</em>, <em class="italic">my</em>, or <em class="italic">the</em>, followed by one or two <strong class="bold">nouns </strong>(<strong class="bold">Ns</strong>) such as <em class="italic">elephant</em>, possibly followed by a <strong class="bold">prepositional phrase </strong>(<strong class="bold">PP</strong>), or just a <strong class="bold">pronoun</strong> (<strong class="bold">Pro</strong>), and so on. Every rule must be in turn defined with another rule, until the rules end in words (or, more generally, <em class="italic">terminal symbols</em>), which do not appear on the LHS of <span class="No-Break">any rule.</span></p>
			<p>The following shows the code for creating a CFG for a few rules of English. These are <em class="italic">constituency rules</em>, which <a id="_idIndexMarker672"/>show how the parts of the sentence are related to each other. There is another commonly used format, <em class="italic">dependencies</em>, which <a id="_idIndexMarker673"/>shows how the words are related to each other, which we will not explore in this book because the constituency rules are sufficient to illustrate the basic concepts of syntactic grammar and <span class="No-Break">syntactic parsing:</span></p>
			<pre class="source-code">
grammar = nltk.CFG.fromstring("""
S -&gt; NP VP
PP -&gt; P NP
NP -&gt; Det N | Det N N |Det N PP | Pro
Pro -&gt; 'I' |'you'|'we'
VP -&gt; V NP | VP PP
Det -&gt; 'an' | 'my' | 'the'
N -&gt; 'elephant' | 'pajamas' | 'movie' |'family' | 'room' |'children'
V -&gt; 'saw'|'watched'
P -&gt; 'in'
""")</pre>
			<p>This grammar is only able to parse a few sentences, such as <em class="italic">the children watched the movie in the family room</em>. For example, it would not be able to parse a sentence <em class="italic">the children slept</em> because, in this grammar, the VP has to include an object or prepositional phrase in addition to the verb. A full CFG for English would be much larger and more complex than the one in the preceding code. It’s also worth pointing out that the NLTK rules can be annotated with probabilities that indicate the likelihood of each alternative on <span class="No-Break">the RHS.</span></p>
			<p>For example, <em class="italic">rule 4</em> (<strong class="source-inline">Pro <img src="image/011.png" alt=""/> 'I' |'you'|'we'</strong>) in the preceding code could have probabilities for the relative likelihoods of <em class="italic">I</em>, <em class="italic">you</em>, and <em class="italic">we</em>. In practice, this will result in more accurate parses, but it<a id="_idIndexMarker674"/> does not affect the examples we’ll show in this<a id="_idIndexMarker675"/> chapter. <em class="italic">Table 8.1</em> summarizes some <span class="No-Break">CFG terminology:</span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Symbol</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Meaning</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>S</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Sentence</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The children watched <span class="No-Break">the movie</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">NP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Noun phrase</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">The children</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">VP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Verb phrase</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Watched <span class="No-Break">the movie</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">PP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Prepositional phrase</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In the <span class="No-Break">family room</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Pro</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Pronoun</span></p>
						</td>
						<td class="No-Table-Style">
							<p>I, we, you, they, he, <span class="No-Break">she, it</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Det</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Determiner <span class="No-Break">or article</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">The, a</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>V</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Verb</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Watched, saw</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>N</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Noun</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Children, movie, elephant, <span class="No-Break">family, room</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Meanings of CFG terms for the grammar in the CFG code block</p>
			<p><em class="italic">Table 8.2</em> summarizes some syntactic conventions used in <span class="No-Break">NLTK CFGs:</span></p>
			<table id="table002-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Symbol</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Meaning</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>-&gt;</p>
						</td>
						<td class="No-Table-Style">
							<p>Separates the LHS from <span class="No-Break">the RHS</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>|</p>
						</td>
						<td class="No-Table-Style">
							<p>Separates alternate possibilities for RHSs that expand <span class="No-Break">the LHS</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Single quotes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Indicates a word; that is, a <span class="No-Break">terminal symbol</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Initial capitalization</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Indicates a syntactic category; that is, a non-terminal symbol, which is expected to be defined by <span class="No-Break">additional rules</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.2 – CFG syntax</p>
			<p>We can parse and<a id="_idIndexMarker676"/> visualize the sentence <em class="italic">the children watched the movie in the family room</em> with the grammar in the previous code block using the <a id="_idIndexMarker677"/><span class="No-Break">following code:</span></p>
			<pre class="source-code">
# we will need this to tokenize the input
from nltk import word_tokenize
# a package for visualizing parse trees
import svgling
# to use svgling we need to disable NLTK's normal visualization functions
svgling.disable_nltk_png()
# example sentence that can be parsed with the grammar we've defined
sent = nltk.word_tokenize("the children watched the movie in the family room")
# create a chart parser based on the grammar above
parser = nltk.ChartParser(grammar)
# parse the sentence
trees = list(parser.parse(sent))
# print a text-formatted parse tree
print(trees[0])
# print an SVG formatted parse tree
trees[0]</pre>
			<p>We can view the results of parsing in different ways – for example, as a bracketed text format, as in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
(S
  (NP (Det the) (N children))
  (VP
    (VP (V watched) (NP (Det the) (N movie)))
    (PP (P in) (NP (Det the) (N family) (N room)))))</pre>
			<p>Notice that the parse directly reflects the grammar: the overall result is called <em class="italic">S</em>, because it came from the first rule in the grammar, <strong class="source-inline">S -&gt; NP VP</strong>. Similarly, <em class="italic">NP</em> and <em class="italic">VP</em> are connected directly to <em class="italic">S</em>, and their child nodes are listed in parentheses <span class="No-Break">after them.</span></p>
			<p>The preceding<a id="_idIndexMarker678"/> format is useful for possible later stages of processing that may need to be <a id="_idIndexMarker679"/>computer-readable; however, it is a little bit difficult to read. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows a conventional tree diagram of this parse, which is easier to view. As in the case of the preceding text parse, you can see that it aligns directly with the grammar. The words, or terminal symbols, all appear at the bottom, or <em class="italic">leaves</em>, of <span class="No-Break">the tree:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B19005_08_01.jpg" alt="Figure 8.1 – Constituency tree for “the children watched the movie in the family room”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Constituency tree for “the children watched the movie in the family room”</p>
			<p>You can try <a id="_idIndexMarker680"/>parsing other sentences with this grammar, and you <a id="_idIndexMarker681"/>can also try modifying the grammar. For example, try adding a grammar rule that will enable the grammar to parse sentences with verbs that are not followed by NPs or PPs, such as <em class="italic">the </em><span class="No-Break"><em class="italic">children slept</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor171"/>Semantic analysis and slot filling</h2>
			<p>The <a id="_idIndexMarker682"/>previous sections on regular expressions and syntactic analysis dealt only with the structure of sentences, not their meaning. A syntactic<a id="_idIndexMarker683"/> grammar like the one shown in the preceding section can parse nonsense sentences such as <em class="italic">the movie watched the children in the room room</em> as long as they match the grammar. We can see this in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B19005_08_02.jpg" alt="Figure 8.2 – Constituency tree for “the movie watched the children in the room room”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Constituency tree for “the movie watched the children in the room room”</p>
			<p>In most applications, however, we don’t only want to find the syntactic structure of sentences; we also want to extract part or all of their meanings. The process of extracting meaning is called <strong class="bold">semantic analysis</strong>. The particular <a id="_idIndexMarker684"/>sense of <em class="italic">meaning</em> will vary depending on the application. For example, in many of the applications we’ve worked on so far in this <a id="_idIndexMarker685"/>book, the only meaning that needed to be derived from a document was its overall classification. This was the case in the movie review data – the only meaning that we wanted to get from the document was the positive or negative sentiment. The statistical methods that we’ve looked at in previous chapters are very good at doing this kind of <span class="No-Break">coarse-grained processing.</span></p>
			<p>However, there are other applications in which it’s necessary to get more detailed information about the relationships between items in the sentence. While there are machine learning techniques for getting fine-grained information (and we will be discussing them in <em class="italic">Chapters 9</em>, <em class="italic">10</em>, and <em class="italic">11</em>), they work best with large amounts of data. If there is less data available, sometimes rule-based processing can be <span class="No-Break">more effective.</span></p>
			<h3>Basic slot filling</h3>
			<p>For the<a id="_idIndexMarker686"/> next examples, we will be looking in detail at a technique often used in interactive applications, <strong class="bold">slot filling</strong>. This is<a id="_idIndexMarker687"/> a common technique used in voicebots and chatbots, although it is also used in non-interactive applications such as <span class="No-Break"><em class="italic">information extraction</em></span><span class="No-Break">.</span></p>
			<p>As an example, consider a chatbot application that helps a user find a restaurant. The application is designed to expect the user to offer some search criteria, such as the type of cuisine, the atmosphere, and the location. These criteria are the <em class="italic">slots</em> in the application. For example, the user might say, <em class="italic">I’d like to find a nice Italian restaurant near here</em>. The overall user goal, <strong class="bold">Restaurant Search</strong>, is the <em class="italic">intent</em>. We will be focusing on identifying slots in this chapter but will discuss intents in more detail in <span class="No-Break">later chapters.</span></p>
			<p>The design for this application is shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B19005_08_03.jpg" alt="Figure 8.3 – Slots for a restaurant search application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Slots for a restaurant search application</p>
			<p>In processing the user’s utterance, the system has to identify what slots the user has specified, and it has to extract their values. This is all the information the system needs to help the user find a restaurant, so anything else in the sentence is normally ignored. This can lead to errors if the part of the sentence that is ignored is actually relevant, but in most cases, the approach works. This leads to a useful processing strategy in many applications, where the system only looks for information that is relevant to its task. This is in contrast to the syntactic parsing process that we reviewed  previously, which required the system to analyze the <span class="No-Break">entire sentence.</span></p>
			<p>We can use the <a id="_idIndexMarker688"/>rule-based matcher in spaCy to create an application that analyzes a user’s utterance to find values for these slots. The basic approach is to define patterns for the system to find words that specify a slot and to define corresponding tags that label the values with their slot names. The following code shows how to find some of the slots shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em> in sentences (we won’t show the code for all of the <a id="_idIndexMarker689"/>slots in order to keep the <span class="No-Break">example short):</span></p>
			<pre class="source-code">
import spacy
from spacy.lang.en import English
nlp = English()
ruler = nlp.add_pipe("entity_ruler")
cuisine_patterns = [
    {"label": "CUISINE", "pattern": "italian"},
    {"label": "CUISINE", "pattern": "german"},
    {"label": "CUISINE", "pattern": "chinese"}]
price_range_patterns = [
    {"label": "PRICE_RANGE", "pattern": "inexpensive"},
    {"label": "PRICE_RANGE", "pattern": "reasonably priced"},
    {"label": "PRICE_RANGE", "pattern": "good value"}]
atmosphere_patterns = [
    {"label": "ATMOSPHERE", "pattern": "casual"},
    {"label": "ATMOSPHERE", "pattern": "nice"},
    {"label": "ATMOSPHERE", "pattern": "cozy"}]
location_patterns = [
    {"label": "LOCATION", "pattern": "near here"},
    {"label": "LOCATION", "pattern": "walking distance"},
    {"label": "LOCATION", "pattern": "close by"},
    {"label": "LOCATION", "pattern": "a short drive"}]
ruler.add_patterns(cuisine_patterns)
ruler.add_patterns(price_range_patterns)
ruler.add_patterns(atmosphere_patterns)
ruler.add_patterns(location_patterns)
doc = nlp("can you recommend a casual italian restaurant within walking distance")
print([(ent.text, ent.label_) for ent in doc.ents])
[('casual', 'ATMOSPHERE'), ('italian', 'CUISINE'), ('walking distance', 'LOCATION')]</pre>
			<p>The preceding code starts by importing spaCy and the information that we need for processing text in the English language. The rule processor is called <strong class="source-inline">ruler</strong> and is added as a stage in the NLP pipeline. We then define three cuisines (a real application would likely have many more) and label them <strong class="source-inline">CUISINE</strong>. Similarly, we define patterns for recognizing price ranges, atmospheres, and locations. These rules state that if the user’s sentence contains a specific word or phrase, such as <strong class="source-inline">near here</strong>, the <strong class="source-inline">LOCATION</strong> slot should be filled with that word <span class="No-Break">or phrase.</span></p>
			<p>The next step is<a id="_idIndexMarker690"/> to add the patterns to the rule processor (<strong class="source-inline">ruler</strong>) and then <a id="_idIndexMarker691"/>run the NLP processor on a sample sentence, <em class="italic">can you recommend a casual italian restaurant within walking distance?</em>. This process applies the rules to the <a id="_idIndexMarker692"/>document, which results in a set of labeled slots (which are called <strong class="bold">entities</strong> in spaCy) in <strong class="source-inline">doc.ents</strong>. By printing the slots and values, we can see that the processor found three slots, <strong class="source-inline">ATMOSPHERE</strong>, <strong class="source-inline">CUISINE</strong>, and <strong class="source-inline">LOCATION</strong>, with values of <strong class="source-inline">casual</strong>, <strong class="source-inline">Italian</strong>, and <strong class="source-inline">walking distance</strong>, respectively. By trying other sentences, we can confirm the following important characteristics of this approach to <span class="No-Break">slot filling:</span></p>
			<ul>
				<li>Parts of the sentence that don’t match a pattern, such as <em class="italic">can you recommend</em>, are ignored. This also means that the non-matching parts of the sentence can be nonsense, or they could actually be important to the meaning, but when they’re ignored, the system can potentially make a mistake. For example, if the utterance were <em class="italic">can you recommend a casual non-Italian restaurant within walking distance</em>, the system would incorrectly think that the user wanted to find an Italian restaurant by using these rules. Additional rules can be written to take these kinds of cases into account, but in many applications, we just want to accept some inaccuracies as the price of keeping the application simple. This has to be considered on an <span class="No-Break">application-by-application basis.</span></li>
				<li>Slots and values will be recognized wherever they occur in the sentences; they don’t have to occur in any <span class="No-Break">particular order.</span></li>
				<li>If a particular slot doesn’t occur in the sentence, that doesn’t cause any problems. It will just be left out of the resulting list <span class="No-Break">of entities.</span></li>
				<li>If a slot occurs more than once, all occurrences will <span class="No-Break">be recognized.</span></li>
			</ul>
			<p>The names of the<a id="_idIndexMarker693"/> slot labels are up to the developer; they don’t have<a id="_idIndexMarker694"/> to be specific values. For example, we could have said <strong class="source-inline">TYPE_OF_FOOD</strong> instead of <strong class="source-inline">CUISINE</strong>, and the processing would be <span class="No-Break">the same.</span></p>
			<p>We can use the spaCy visualizer, <strong class="source-inline">displacy</strong>, to get a clearer visualization of the result using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from spacy import displacy
colors = {"CUISINE": "#ea7e7e",
          "PRICE_RANGE": "#baffc9",
          "ATMOSPHERE": "#abcdef",
          "LOCATION": "#ffffba"}
options = {"ents": ["CUISINE","PRICE_RANGE","ATMOSPHERE","LOCATION"], "colors": colors}
displacy.render(doc, style="ent", options=options,jupyter = True)</pre>
			<p>We can see the result in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em>, where the text and its slots and values <span class="No-Break">are highlighted:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B19005_08_04.jpg" alt="Figure 8.4 – Slot visualization with displacy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Slot visualization with displacy</p>
			<p>Because our <a id="_idIndexMarker695"/>slots are custom (that is, not built into spaCy), to show colored slots, we have to define colors for the different slots (or <strong class="source-inline">ents</strong>) and assign the colors to the slots. We can then visualize the slots and values using a different color<a id="_idIndexMarker696"/> for each slot. The colors are defined in the <strong class="source-inline">colors</strong> variable in the preceding code. We can assign any colors to the slots that we find useful. The colors don’t need to be different, but it is normally most helpful if they are, and if they are fairly distinctive. The values of the colors in this example are hexadecimal codes, which have standard color interpretations. <a href="https://www.color-hex.com/">https://www.color-hex.com/</a> is a useful website<a id="_idIndexMarker697"/> that shows the hexadecimal values for <span class="No-Break">many colors.</span></p>
			<h3>Using the spaCy id attribute</h3>
			<p>You have probably noticed that<a id="_idIndexMarker698"/> some of the slot values we have defined in this example mean the same thing – for example, <strong class="source-inline">close by</strong> and <strong class="source-inline">near here</strong>. If the slots and values are passed on to a later stage in processing such as database lookup, that later stage will have to have code for handling both <strong class="source-inline">close by</strong> and <strong class="source-inline">near here</strong>, even though the database lookup will be the same. This will complicate the application, so we would like to avoid it. spaCy provides another attribute of <strong class="source-inline">ent</strong>, <strong class="source-inline">ent_id_</strong>,  for this purpose. This <strong class="source-inline">id</strong> attribute can be assigned in the patterns that find the slots, along with the label and pattern. This is accomplished by specifying an <strong class="source-inline">id</strong> attribute in the pattern declarations, which is a modification of the location patterns in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
location_patterns = [
    {"label": "LOCATION", "pattern": "near here", "id":"nearby"},
    {"label": "LOCATION", "pattern": "close by","id":"nearby"},
    {"label": "LOCATION", "pattern": "near me","id":"nearby"},
    {"label": "LOCATION", "pattern": "walking distance", "id":"short_walk"},
    {"label": "LOCATION", "pattern": "short walk", "id":"short_walk"},
    {"label": "LOCATION", "pattern": "a short drive", "id":"short_drive"}]</pre>
			<p>If we print the slots, values, and IDs that result from <em class="italic">can you recommend a casual italian restaurant close by</em>, the result is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
[('casual', 'ATMOSPHERE', ''), ('italian', 'CUISINE', ''), ('close by', 'LOCATION', 'nearby')]</pre>
			<p>Here, we can see that the ID of <strong class="source-inline">close by</strong> is <strong class="source-inline">nearby</strong>, based on the <strong class="source-inline">close </strong><span class="No-Break"><strong class="source-inline">by</strong></span><span class="No-Break"> pattern.</span></p>
			<p>In the preceding code, we can see that the first three location patterns, which have similar meanings, have all been assigned the ID <strong class="source-inline">nearby</strong>. With this ID, the next stage in processing only needs to receive the <strong class="source-inline">ent_id_</strong> value, so it only has to handle <strong class="source-inline">nearby</strong>, and it doesn’t have to have additional cases for <strong class="source-inline">close by</strong> and <span class="No-Break"><strong class="source-inline">near me</strong></span><span class="No-Break">.</span></p>
			<p>Note that in this example, the results for the <strong class="source-inline">CUISINE</strong> and <strong class="source-inline">ATMOSPHERE</strong> slots have empty IDs, because these were not defined in the <strong class="source-inline">CUISINE</strong> and <strong class="source-inline">ATMOSPHERE</strong> patterns. It is nevertheless good practice to define IDs for all patterns, if there are any IDs, in order to keep the<a id="_idIndexMarker699"/> <span class="No-Break">results uniform.</span></p>
			<p>Also note that these patterns reflect some design decisions about what phrases are synonymous and, therefore, should have the same ID, and which phrases are not synonymous and should have <span class="No-Break">different IDs.</span></p>
			<p>In the preceding code, we can see that <strong class="source-inline">short walk</strong> doesn’t have the same ID as <strong class="source-inline">near me</strong>, for example. The design decision that was made here was to consider <strong class="source-inline">short walk</strong> and <strong class="source-inline">near me</strong> to have different meanings, and therefore to require different handling in the later stages of the application. Making decisions about which values are and are not synonymous will depend on the application and how rich the information available in the backend <span class="No-Break">application is.</span></p>
			<p>We have described several useful rule-based approaches to NLP. <em class="italic">Table 8.3</em> summarizes these rule-based <a id="_idIndexMarker700"/>techniques by listing three important properties of <span class="No-Break">rule-based techniques:</span></p>
			<ul>
				<li>The format of <span class="No-Break">the rules</span></li>
				<li>The types of processing that apply the rules <span class="No-Break">to text</span></li>
				<li>How the results <span class="No-Break">are represented</span></li>
			</ul>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B19005_08_Table_01.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Figure">Table 8.3 – Formats, processing, and results for rule-based techniques</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor172"/>Summary</h1>
			<p>In this chapter, we’ve learned several important skills that make use of rules for processing <span class="No-Break">natural language.</span></p>
			<p>We’ve learned how to apply regular expressions to identify fixed-format expressions such as numbers, dates, and addresses. We’ve also learned about the uses of rule-based Python tools such as the NLTK syntactic parsing libraries for analyzing the syntactic structure of sentences and how to apply them. Finally, we’ve learned about rule-based tools for semantics analysis such as spaCy’s <strong class="source-inline">entity_ruler</strong> for analyzing the slot-value semantics <span class="No-Break">of sentences.</span></p>
			<p>The next chapter, <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, will begin the discussion on machine learning by introducing statistical techniques such as classification with Naïve Bayes and <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>), <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), and conditional random fields. In contrast to the rule-based approaches we have discussed in this chapter, statistical approaches are based on models that are learned from training data and then applied to new, previously unseen data. Unlike the all-or-none rule-based systems, statistical systems are based <span class="No-Break">on probabilities.</span></p>
			<p>As we explore these techniques, we’ll also consider how they can be combined with the rule-based techniques discussed in this chapter to create even more powerful and <span class="No-Break">effective systems.</span></p>
		</div>
	</body></html>