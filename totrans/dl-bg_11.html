<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Variational Autoencoders
                </header>
            
            <article>
                
<p>Autoencoders can be really powerful in finding rich latent spaces. They are almost magical, right? What if we told you that <strong>variational autoencoders</strong> (<strong>VAEs</strong>) are even more impressive? Well, they are. They have inherited all the nice things about traditional autoencoders and added the ability to generate data from a parametric distribution. </p>
<p class="mce-root">In this chapter, we will introduce the philosophy behind generative models in the unsupervised deep learning field and their importance in the production of new data. We will present the VAE as a better alternative to a deep autoencoder. At the end of this chapter, you will know where VAEs come from and what their purpose is. You will be able to see the difference between deep and shallow VAE models and you will be able to appreciate the generative property of VAEs.</p>
<p><span>The chapter is organized as follows:</span></p>
<ul>
<li><span>Introducing deep generative models</span></li>
<li>Examining the VAE model</li>
<li>Comparing deep and shallow VAEs on MNIST</li>
<li>Thinking about the ethical implications of generative models</li>
</ul>
<h1 id="uuid-4251f2f4-5e2c-4841-b254-d754facd1157">Introducing deep generative models</h1>
<p>Deep learning has very interesting contributions to the general machine learning community, particularly when it comes to deep discriminative and generative models. We are familiar with what a discriminative model is—for example, a <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>) is one. In a discriminative model, we are tasked with guessing, predicting, or approximating a desired target, <sub><img class="fm-editor-equation" src="assets/2c8c7283-b742-4625-b194-d682a0dae077.png" style="width:0.58em;height:0.83em;"/></sub>, given input data <em><img class="fm-editor-equation" src="assets/36b66f81-9b91-46e6-9cc5-c91817249c39.png" style="width:0.75em;height:0.83em;"/></em>. In statistical theory terms, we are modeling the conditional probability density function, <img src="assets/4af3a90c-1525-4445-a4ec-ddade5cbafce.png" style="width:2.58em;height:1.08em;"/>. On the other hand, by a generative model, this is what most people mean:</p>
<p><em>A model that can generate data <img class="fm-editor-equation" src="assets/36b66f81-9b91-46e6-9cc5-c91817249c39.png" style="width:0.75em;height:0.83em;"/> that follows a particular distribution based on an input or stimulus <img class="fm-editor-equation" src="assets/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png" style="width:0.67em;height:0.83em;"/>.</em></p>
<p>In deep learning, we can build a neural network that can model this generative process very well. In statistical terms, the neural model approximates the conditional probability density function, <img src="assets/8c63f596-488c-4e73-a7aa-7e08c2fa3fac.png" style="width:2.33em;height:1.00em;"/>. While there are several generative models today, in this book, we will talk about three in particular.</p>
<p>First, we will talk about VAEs, which are discussed in the next section. Second, <a href="6ec46669-c8d3-4003-ba28-47114f1515df.xhtml">Chapter 10</a>, <em>Restricted Boltzmann Machines</em>, will introduce a graphical approach and its properties (Salakhutdinov, R., et al. (2007)). The last approach will be covered in <a href="7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml">Chapter 14</a>, <em>Generative Adversarial Networks.</em> These networks are changing the way we think about model robustness and data generation (Goodfellow, I., <em>et al.</em> (2014)).</p>
<p class="mce-root"/>
<h1 id="uuid-eea46287-2f6b-406d-b1fa-0125e358410c">Examining the VAE model</h1>
<p>The VAE is a particular type of autoencoder (<span>Kingma, D. P., &amp; Welling, M. (2013))</span>. It learns specific statistical properties of the dataset derived from a Bayesian approach. First, let's define <sub><img class="fm-editor-equation" src="assets/83389b96-e277-4cf3-8778-7c1450d525ac.png" style="width:2.33em;height:1.17em;"/></sub> as the prior probability density function of a random latent variable, <sub><img class="fm-editor-equation" src="assets/52cbb884-6c5d-4cfc-9d26-f0eb85c61622.png" style="width:3.08em;height:1.17em;"/></sub>. Then, we can describe a conditional probability density function, <sub><img src="assets/a04acbe7-4f47-4fd3-9d94-2a97e8e478b8.png" style="width:2.50em;height:0.92em;"/></sub>, which can be interpreted as a model that can produce data—say, <sub><img src="assets/7b2a4e2f-4402-4293-b890-0ab54f1f0694.png" style="width:5.25em;height:1.42em;"/></sub>. It follows that we can approximate the posterior probability density function in terms of the conditional and prior distributions, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ab9cec77-1f80-41d4-96d0-67a90fd91a22.png" style="width:13.17em;height:1.67em;"/></p>
<p>It turns out that an exact posterior is intractable, but this problem can be solved, approximately, by making a few assumptions and using an interesting idea to compute gradients. To begin with, the prior can be assumed to follow an isotropic Gaussian distribution, <img src="assets/8732332a-8166-4cbc-b4cf-fca9b6f5b0c9.png" style="width:6.58em;height:1.17em;"/>. We can also assume that the conditional distribution, <img src="assets/dc0906a5-b6fc-4f7e-b632-939b6c8585ca.png" style="width:2.92em;height:1.08em;"/>, can be parametrized and modeled using a neural network; that is, given a latent vector <img class="fm-editor-equation" src="assets/e5b5b6ea-6bfd-48d7-bd48-19b814924ab4.png" style="width:0.67em;height:0.83em;"/>, we use a neural network to <em>generate</em> <img class="fm-editor-equation" src="assets/de19cd96-7891-4682-a488-d8f832e9aeee.png" style="width:0.75em;height:0.83em;"/>. The weights of the network, in this case, are denoted as  <sub><img class="fm-editor-equation" src="assets/29d99b0a-5807-4f74-8a55-16b1a1745334.png" style="width:0.33em;height:0.58em;"/></sub>, and the network would be the equivalent of the <em>decoder</em> network. The choice of the parametric distribution could be Gaussian, for outputs where <img class="fm-editor-equation" src="assets/cac94549-d340-466d-b3d0-6e71f6c823f4.png" style="width:0.75em;height:0.83em;"/> can take on a wide variety of values, or Bernoulli, if the output <img class="fm-editor-equation" src="assets/36b66f81-9b91-46e6-9cc5-c91817249c39.png" style="width:0.75em;height:0.83em;"/> is likely to be binary (or Boolean) values. Next, we must go back again to another neural network to approximate the posterior, using <sub><img class="fm-editor-equation" src="assets/80b72202-e128-44ce-ad05-ace625e90357.png" style="width:3.50em;height:1.33em;"/></sub> with separate parameters, <sub><img class="fm-editor-equation" src="assets/160f2e57-8c5d-4a98-9a12-faf7e05313f1.png" style="width:0.50em;height:0.92em;"/></sub>. This network can be interpreted as the <em>encoder</em> network that takes <img class="fm-editor-equation" src="assets/913f83a9-0a47-4629-b4ea-7d16124233b3.png" style="width:0.75em;height:0.83em;"/> as input and generates the latent variable <img class="fm-editor-equation" src="assets/f8da3963-6d86-4faf-90ad-0ce4598b25f8.png" style="width:0.67em;height:0.83em;"/>.</p>
<p>Under this assumption, we can define a loss function as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png" style="width:23.75em;height:3.00em;"/></p>
<p>The full derivation can be followed in <span>Kingma, D. P., &amp; Welling, M. (2013)</span><span>. However, in short, we can say that in the first term, <img src="assets/6c8220c0-ab8a-49f3-8984-c004e5782ac5.png" style="width:2.50em;height:0.92em;"/> is the Kullback–Leibler divergence function, which aims to measure how different the distribution of the prior is, <sub><img class="fm-editor-equation" src="assets/110e5ad7-4d3b-42f3-bd5a-c048eefa31b3.png" style="width:2.50em;height:1.25em;"/></sub>, with respect to the distribution of the posterior, <sub><img class="fm-editor-equation" src="assets/68e048a8-5c70-4a2a-b51e-9ded415b10bd.png" style="width:2.17em;height:0.83em;"/></sub>. This happens in the <em>encoder</em>, and we want to make sure that the prior and posterior of <img class="fm-editor-equation" src="assets/f1af68c2-2332-4405-bd6f-16ec408d80c3.png" style="width:0.67em;height:0.83em;"/> are a close match. The second term is related to the decoder network, which aims to minimize the reconstruction loss based on the negative log likelihood of the conditional distribution, <img src="assets/84214a05-6ba7-43ae-9981-1c3469e9407a.png" style="width:3.17em;height:1.17em;"/>, with respect to the expectation of the posterior, <img src="assets/4e736730-4af1-4524-b200-f64746e7e823.png" style="width:3.17em;height:1.17em;"/>.</span></p>
<p class="mce-root">One last trick to make the VAE learn through gradient descent is to use an idea called <strong>re-parameterization</strong><em>.</em> This trick is necessary because it is impossible to encode one sample, <img class="fm-editor-equation" src="assets/82260ac8-99f8-4fb4-9f7d-509ac5aefc7c.png" style="width:0.75em;height:0.83em;"/>, to approximate an isotropic Gaussian with 0 mean and some variance, and draw a sample <img class="fm-editor-equation" src="assets/79de787c-640f-439e-80f7-bbb776fdf0b1.png" style="width:0.58em;height:0.67em;"/> from that distribution, pause there, and then go on to decode that and calculate the gradients, and then go back and make updates. The re-parametrization trick is simply a method for generating samples from <sub><img class="fm-editor-equation" src="assets/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png" style="width:3.50em;height:1.33em;"/></sub>, while at the same time, it allows gradient calculation. If we say that <sub><img class="fm-editor-equation" src="assets/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png" style="width:5.25em;height:1.25em;"/></sub>, we can express the random variable <img class="fm-editor-equation" src="assets/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png" style="width:0.50em;height:0.58em;"/> in terms of an auxiliary variable, <img class="fm-editor-equation" src="assets/d20928b8-47fc-495e-8597-0c476e80bb92.png" style="width:0.50em;height:0.75em;"/>, with marginal probability density function, <sub><img class="fm-editor-equation" src="assets/800b5609-ccd9-447d-8da9-add870f8b803.png" style="width:1.17em;height:0.83em;"/></sub>, such that, <sub><img class="fm-editor-equation" src="assets/04fa784b-ebe5-4b9c-9b22-2506e12654a9.png" style="width:4.58em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/c07036e2-7876-4882-a7df-c93fcdd8a6b2.png" style="width:2.33em;height:1.25em;"/></sub> is a function parameterized by <sub><img class="fm-editor-equation" src="assets/dbeee19b-aca5-4a8f-b802-64dcd61ec13e.png" style="width:0.75em;height:1.33em;"/></sub> and returns a vector. This allows gradient calculations on parameters <sub><img class="fm-editor-equation" src="assets/6dc7faab-b474-45db-87c2-62cc0e4a321b.png" style="width:0.58em;height:1.00em;"/></sub> (generator or decoder) and updates on both <sub><img class="fm-editor-equation" src="assets/5635e398-101a-4836-9b24-2b94084c779c.png" style="width:0.50em;height:0.92em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/456dbd6d-36b0-42c9-8457-3124c7245a53.png" style="width:0.50em;height:0.83em;"/></sub> using any available gradient descent method.</p>
<div class="packt_infobox">The <em>tilde</em> sign (~) in the <sub><img class="fm-editor-equation" src="assets/1ca0773f-49d4-4a80-8f31-f3e0ae712ee1.png" style="width:5.75em;height:1.25em;"/></sub> equation can be interpreted as <em>follows the distribution of</em>. Thus, the equation can be read as <img class="fm-editor-equation" src="assets/7bc90bd7-3be3-4e8c-99f2-ecbe53e205e2.png" style="width:0.75em;height:0.83em;"/> follows the distribution of the posterior, <sub><img class="fm-editor-equation" src="assets/a5b1d977-263d-4bda-ba53-00039ad43c93.png" style="width:3.17em;height:1.25em;"/></sub>.</div>
<p><em>Figure 9.1</em> depicts the VAE architecture, explicitly showing the pieces involved in the <em>bottlenecks</em> and how the pieces of the network are interpreted:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c6f80791-6557-4162-a6b7-642b28784a67.png" style="width:26.33em;height:33.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.1 – VAE architecture</div>
<p>The preceding figure shows that in an ideal VAE, the parameters of the distributions are learned precisely and perfectly to achieve exact reconstruction. However, this is just an illustration, and in practice, perfect reconstruction can be difficult to achieve.</p>
<div class="packt_infobox"><strong>Bottlenecks </strong>are the latent representations or parameters that are found in neural networks that go from layers with large numbers of neural units to layers with a decreasing number of neural units. These bottlenecks are known to produce interesting feature spaces (Zhang, Y., <em>et al.</em> (2014)).</div>
<p>Now, let's prepare to make our first VAE piece by piece. We will begin by describing the dataset we will use.</p>
<h2 id="uuid-acf5c806-da57-475d-91ec-41a2e583e012">The heart disease dataset revisited</h2>
<p>In <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em>Preparing Data</em>, we described in full the properties of a dataset called the <strong>Cleveland Heart Disease</strong> dataset. A screenshot of two columns from this dataset is depicted in <em>Figure 9.2</em>. <span>Here, we will revisit this dataset with the purpose of reducing the original 13 dimensions of the data down to only two dimensions. Not only that, but we will also try to produce new data from the generator—that is, the decoder:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6e25e8f1-e950-4fd6-a7ec-9ac75ecdd0cb.png" style="width:36.42em;height:31.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.2 – The Cleveland Heart Disease dataset samples on two columns</div>
<p>Our attempts to perform dimensionality reduction can be easily justified by looking at <em>Figures 3.8</em> and <em>3.9</em> in <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em>Preparing Data</em>, and noticing that the data can possibly be processed so as to see whether a neural network can cause the data associated with hearts with no disease to cluster separately from the rest. Similarly, we can justify the generation of new data given that the dataset itself only contains 303 samples.</p>
<p class="mce-root"/>
<p>To download the data, we can simply run the following code:</p>
<pre>#download data<br/>!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data</pre>
<p>Then, to load the data into a data frame and separate the training data and the targets, we can execute the following code:</p>
<pre>import pandas as pd<br/>df = pd.read_csv('processed.cleveland.data', header=None)<br/># this next line deals with possible numeric errors<br/>df = df.apply(pd.to_numeric, errors='coerce').dropna()<br/>X = df[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]].values<br/>y = df[13].values</pre>
<p>The next thing we will need to do is to code the re-parametrization trick so that we can sample random noise during training.</p>
<h2 id="uuid-ae145bf0-2581-4b33-a454-0240e39b96b7">The re-parametrization trick and sampling </h2>
<p>Remember that the re-parametrization trick aims to sample from <sub><img class="fm-editor-equation" src="assets/800b5609-ccd9-447d-8da9-add870f8b803.png" style="width:1.92em;height:1.25em;"/></sub> instead of <sub><img class="fm-editor-equation" src="assets/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png" style="width:3.50em;height:1.33em;"/></sub>. Also, recall the distribution <sub><img class="fm-editor-equation" src="assets/65ff4f78-8d25-4b17-a877-3775474be6a7.png" style="width:6.25em;height:1.17em;"/></sub>. This will allow us to make the learning algorithm learn the parameters of <sub><img class="fm-editor-equation" src="assets/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png" style="width:3.50em;height:1.33em;"/></sub>—that is, <sub><img class="fm-editor-equation" src="assets/3ab31204-0e3f-44a5-97b3-e0829a9a29c0.png" style="width:1.83em;height:0.92em;"/></sub>—and we simply produce a sample from <sub><img class="fm-editor-equation" src="assets/57985506-28ec-4d9a-9bdb-499ecfa47d83.png" style="width:6.08em;height:1.00em;"/></sub>.</p>
<p>To achieve this, we can generate the following method:</p>
<pre>from tensorflow.keras import backend as K<br/><br/>def sampling(z_params):<br/>  z_mean, z_log_var = z_params<br/>  batch = K.shape(z_mean)[0]<br/>  dims = K.int_shape(z_mean)[1]<br/>  epsilon = K.random_normal(shape=(batch, dims))<br/>  return z_mean + K.exp(0.5 * z_log_var) * epsilon</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>sampling()</kbd> method receives the mean and log variance of <img src="assets/c8dc8061-6987-435e-a863-5b8b836b864d.png" style="width:3.83em;height:1.42em;"/> (which are to be learned), and returns a vector that is sampled from this parametrized distribution; <img class="fm-editor-equation" src="assets/e1c06fa7-5aa1-40cb-8903-2ba1bd71dcac.png" style="width:0.67em;height:1.00em;"/> is just random noise from a Gaussian (<kbd>random_normal</kbd>) distribution with 0 mean and unit variance. To make this method fully compatible with mini-batch training, the samples are generated according to the size of the mini-batch.</p>
<h2 id="uuid-c6df3076-855d-4b63-9f32-426f908467d8">Learning the posterior's parameters in the encoder</h2>
<p>The posterior distribution, <sub><img class="fm-editor-equation" src="assets/35226dc4-445f-4f7e-b5ae-e332f3eedb53.png" style="width:3.08em;height:1.17em;"/></sub>, is intractable by itself, but since we are using the re-parametrization trick, we can actually perform sampling based on <img class="fm-editor-equation" src="assets/18a8f5d8-cb9e-484d-8786-64bc62d3bb16.png" style="width:0.67em;height:1.00em;"/>. We will now make a simple encoder to learn these parameters.</p>
<p>For numerical stability, we need to scale the input to make it have 0 mean and unit variance. For this, we can invoke the methodology learned in <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em>Preparing Data</em>:</p>
<pre>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scaler.fit(X)<br/><strong>x_train</strong> = scaler.transform(X)<br/>original_dim = x_train.shape[1]</pre>
<p>The <kbd>x_train</kbd> matrix contains the scaled training data. The following variables will also be useful for designing the encoder of the VAE:</p>
<pre class="mce-root">input_shape = (original_dim, )<br/>intermediate_dim = 13 <br/>batch_size = 18 # comes from ceil(sqrt(x_train.shape[0]))<br/>latent_dim = 2 # useful for visualization<br/>epochs = 500</pre>
<p>These variables are straightforward, except that the batch size is in terms of the square root of the number of samples. This is an empirical value found to be a good start, but on larger datasets, it is not guaranteed to be the best.</p>
<p>Next, we can build the encoder portion, as follows:</p>
<pre class="mce-root">from tensorflow.keras.layers import Lambda, Input, Dense, Dropout, BatchNormalization<br/>from tensorflow.keras.models import Model<br/><br/>inputs = Input(shape=input_shape)<br/>bn = BatchNormalization()(inputs)<br/>dp = Dropout(0.2)(bn)<br/>x = Dense(intermediate_dim, activation='sigmoid')(dp)<br/>x = Dropout(0.2)(x)<br/><strong>z_mean</strong> = Dense(latent_dim)(x)<br/><strong>z_log_var</strong> = Dense(latent_dim)(x)<br/>z_params = [<strong>z_mean</strong>, <strong>z_log_var</strong>]<br/>z = Lambda(sampling, output_shape=(latent_dim,))(z_params)<br/>encoder = Model(inputs, [<strong>z_mean, z_log_var, z</strong>])</pre>
<p>This approach of encoder utilizes the <kbd>Lambda</kbd> class, which is part of the <kbd>tensorflow.keras.layers</kbd> collection. This allows us to use the previously defined <kbd>sampling()</kbd> method (or any arbitrary expression, really) as a layer object. <em>Figure 9.3</em> illustrates the architecture of the full VAE, including the layers of the encoder described in the preceding code block:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c5540457-c546-4297-8aa3-3166a1987073.png" style="width:40.33em;height:36.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.3 – VAE architecture for the Cleveland Heart Disease dataset</div>
<p class="mce-root"/>
<p>The encoder uses batch normalization, followed by <strong>dropout</strong> at the input layers, followed by a dense layer that has <strong>Tanh activation</strong> and <strong>dropout</strong>. From the <strong>dropout</strong>, two dense layers are in charge of modeling the parameters of the distribution of the latent variable, and a sample is drawn from this parametrized distribution. The decoder network is discussed next.</p>
<h2 id="uuid-8a86c916-ad3d-41a1-8fe3-6523e70fe6d5">Modeling the decoder</h2>
<p>The decoder portion of the VAE is very much standard with respect to what you already know about an autoencoder. The decoder takes the latent variable, which in the VAE was produced by a parametric distribution, and then it should reconstruct the input exactly. The decoder can be specified as follows:</p>
<pre>latent_inputs = Input(shape=(latent_dim,))<br/>x = Dense(intermediate_dim, activation='relu')(latent_inputs)<br/>r_outputs = Dense(original_dim)(x)    # reconstruction outputs<br/>decoder = Model(latent_inputs, r_outputs)</pre>
<p>In the preceding code, we simply connect two dense layers—the first contains a ReLU activation, while the second has linear activations in order to map back to the input space. Finally, we can define the complete VAE in terms of inputs and outputs as defined in the encoder and decoder:</p>
<pre class="mce-root">outputs = decoder(encoder(inputs)[2])   # it is index 2 since we want z<br/><strong>vae</strong> = Model(inputs, outputs)</pre>
<p class="mce-root">The VAE model is completed as described here and depicted in <em>Figure 9.3</em>. The next steps leading to the training of this model include the definition of a loss function, which we will discuss next.</p>
<h2 id="uuid-98b0bafa-25e7-4569-b389-21bc950ac9a6">Minimizing the loss</h2>
<p>We explained earlier that the loss function needs to be in terms of the encoder and decoder; this is the equation we discussed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/542e3bf0-5cee-4200-9ed5-1bbbbd27aa4a.png" style="width:25.75em;height:3.25em;"/></p>
<p>If we want to code this loss, we will need to code it in terms of something more practical. Applying all the previous assumptions made on the problem, including the re-parametrization trick, allows us to rewrite an approximation of the loss in simpler terms, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8bddf69c-11f1-415d-be2a-c1766f4a885e.png" style="width:27.00em;height:5.00em;"/></p>
<p>This is for all samples of <img class="fm-editor-equation" src="assets/71d43944-64da-4574-b03d-25570e0ecc86.png" style="width:0.83em;height:0.92em;"/> where <sub><img class="fm-editor-equation" src="assets/445b73e4-f1f8-49d6-953f-301b73f27395.png" style="width:6.42em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/ce34c93c-41c6-4549-99f3-1c8eb8a3152c.png" style="width:5.58em;height:1.33em;"/></sub>. Furthermore, the decoder loss portion can be approximated using any of your favorite reconstruction losses—for example, the <strong>mean squared error</strong> (<strong>MSE</strong>) loss or the binary cross-entropy loss. It has been proven that minimizing any of these losses will also minimize the posterior.</p>
<p>We can define the reconstruction loss in terms of the MSE, as follows:</p>
<pre>from tensorflow.keras.losses import mse<br/>r_loss = <strong>mse</strong>(inputs, outputs)</pre>
<p>Alternatively, we could do so with the binary cross-entropy loss, like so:</p>
<pre>from tensorflow.keras.losses import binary_crossentropy<br/>r_loss = <strong>binary_crossentropy</strong>(inputs, outputs)</pre>
<p>One additional thing we could do, which is optional, is to monitor how important the reconstruction loss <span>is</span><span> </span><span>in comparison to the KL-divergence loss (the term related to the encoder). One typical thing to do is to make the reconstruction loss be multiplied by either the latent dimension or by the input dimension. This effectively makes the loss larger by that factor. If we do the latter, we can penalize the reconstruction loss, as follows:</span></p>
<pre>r_loss = <strong>original_dim</strong> * r_loss</pre>
<p class="mce-root">The KL-divergence loss for the encoder term can now be expressed in terms of the mean and variance, as follows:</p>
<pre>kl_loss = 1 + <strong>z_log_var</strong> - K.square(<strong>z_mean</strong>) - K.exp(<strong>z_log_var</strong>)<br/>kl_loss = 0.5 * K.sum(kl_loss, axis=-1)</pre>
<p class="mce-root">Therefore, we can simply add the overall loss to the model, which becomes the following:</p>
<pre class="mce-root">vae_loss = K.mean(<strong>r_loss</strong> + <strong>kl_loss</strong>)<br/>vae.add_loss(<strong>vae_loss</strong>)</pre>
<p class="mce-root"/>
<p>With this, we are good to go ahead and compile the model and train it, as explained next.</p>
<h2 id="uuid-471295b5-d4b1-4aa9-8598-1b1ed84bd4de">Training a VAE</h2>
<p>The finishing touch is to compile the VAE model, which will put all the pieces together. During the compilation, we will choose an optimizer (the gradient descent method). In this case, we will choose <em>Adam</em> (Kingma, D. P., <em>et al.</em> (2014)).</p>
<div class="packt_infobox">Fun fact: the creator of the VAE is the same person who soon after created Adam. His name is <strong>Diederik P. Kingma</strong> and he is currently a research scientist at Google Brain.</div>
<p>To compile the model and choose the optimizer, we do the following:</p>
<pre class="mce-root">vae.compile(optimizer='<strong>adam</strong>')</pre>
<p class="mce-root">Finally, we train with training data for 500 epochs, using a batch size of 18, like so:</p>
<pre class="mce-root">hist = vae.fit(<strong>x_train</strong>, epochs=<strong>epochs</strong>,<br/>               batch_size=<strong>batch_size</strong>,<br/>               validation_data=(x_train, None))</pre>
<p>Notice that we are using the training set as the validation set. This is not recommended in most settings, but here, it works because the chance of selecting identical mini-batches for training and validation is very low. Furthermore, it would usually be considered cheating to do that; however, the latent representation used in the reconstruction does not directly come from the input; rather, it comes from a distribution similar to the input data. To demonstrate that the training and validation sets yield different results, we plot the training progress across epochs, as shown in <em>Figure 9.4</em>:</p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5bced4db-4f13-4517-8210-33d5982e7a4a.png" style="width:34.50em;height:21.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.4 – VAE training performance across epochs</div>
<p>The preceding figure not only indicates that the model converges quickly, but it also shows that the model does not overfit on the input data. This is a nice property to have, usually. </p>
<p><em>Figure 9.4</em> can be produced with the following code:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], '--', color='#dc267f')<br/>plt.title('Model reconstruction loss')<br/>plt.ylabel('MSE Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Validation Set'], loc='upper right')<br/>plt.show()</pre>
<p>However, note that the results may vary due to the unsupervised nature of the VAE.</p>
<p class="mce-root"/>
<p>Next, if we take a look at the latent representation that is produced by sampling from a random distribution using the parameters that were learned during training, we can see what the data looks like. <em>Figure 9.5</em> depicts the latent representations obtained:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e59a5b95-6a1a-4f10-8cc4-6f568df25c1c.png" style="width:33.33em;height:29.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.5 – VAE's sampled latent representation in two dimensions</div>
<p>The figure clearly suggests that the data corresponding to no indication of heart disease is clustered on the left quadrants, while the samples corresponding to heart disease are clustered on the right quadrant of the latent space. The histogram shown on the top of the figure suggests the presence of two well-defined clusters. This is great! Also, recall that the VAE does not know anything about labels: we can't stress this enough! Compare <em>Figure 9.5</em> here with <em>Figure 3.9</em> in <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em><span>Preparing </span>Data</em>, and you will notice that the performance of the VAE is superior to KPCA. Furthermore, compare this figure with <em>Figure 3.8</em> in <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em><span>Preparing </span>Data,</em> and notice that the performance of the VAE is comparable (if not better) than <strong>Linear Discriminant Analysis</strong> <span>(</span><strong>LDA</strong>), which uses label information to produce low-dimensional representations. In other words, LDA cheats a little bit.</p>
<p class="mce-root"/>
<p>One of the most interesting properties of the VAE is that we can generate data; let's go ahead and see how this is done.</p>
<h2 id="uuid-3840c30a-f42d-4d17-b9dd-1ccc6c69ce98">Generating data from the VAE</h2>
<p>Since the VAE learns the parameters of a parametric distribution over the latent space, which is sampled to reconstruct the input data back, we can use those parameters to draw more samples and reconstruct them. The idea is to generate data for whatever purposes we have in mind.</p>
<p>Let's start by encoding the original dataset and see how close the reconstruction <span>is</span><span> </span><span>to the original. Then, generating data should be straightforward. To encode the input data into the latent space and decode it, we do the following:</span></p>
<pre>encdd = <strong>encoder</strong>.predict(x_train)<br/>x_hat = <strong>decoder</strong>.predict(encdd<strong>[0]</strong>)</pre>
<p>Recall that <kbd>x_train</kbd> is <img class="fm-editor-equation" src="assets/b0c5b964-8583-4007-8807-e18c4b9522ef.png" style="width:0.75em;height:0.83em;"/> and <kbd>x_hat</kbd> is the reconstruction, <img class="fm-editor-equation" src="assets/6476b534-a425-4523-a577-baeeb7b34df3.png" style="width:0.75em;height:1.08em;"/>. Notice that we are using <kbd>encdd[0]</kbd> as the input for the decoder. The reason for this is that the encoder yields a list of three vectors, <kbd>[z_mean, z_log_var, z]</kbd>. Therefore, to use the 0 element in the list is to refer to the mean of the distribution corresponding to the samples. In fact, <kbd><span>encdd[0][10]</span></kbd> would yield a two-dimensional vector corresponding to the mean parameter of the distribution that can produce the 10<sup>th</sup> sample in the dataset—that is, <kbd>x_train[10]</kbd>. If you think about it, the mean could be the best latent representation that we can find since it would be the most likely to reconstruct the input in the decoder.</p>
<p>With this in mind, we can take a look at how good the reconstruction <span>is</span><span> </span><span>by running something like this:</span></p>
<pre class="mce-root">import numpy as np<br/>print(np.around(scaler.inverse_transform(x_train[0])<span>,</span><span> decimals=</span><span>1</span><span>))</span><br/>print(np.around(scaler.inverse_transform(x_hat[0])<span>,</span><span> </span><span>decimals=</span><span>1</span><span>))</span></pre>
<p>This gives the following output:</p>
<pre>[ 63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0 ]<br/>[ 61.2  0.5  3.1  144.1  265.1  0.5  1.4  146.3  0.2  1.4  1.8  1.2  4.8 ]</pre>
<p class="mce-root"/>
<div class="packt_tip">If the output shows scientific notation that is difficult to read, try disabling it temporarily like this:<br/>
<kbd>import numpy as np</kbd><br/>
<kbd>np.set_printoptions(suppress=True)</kbd><br/>
<kbd>print(np.around(scaler.inverse_transform(x_train[0]), decimals=1))</kbd><br/>
<kbd>print(np.around(scaler.inverse_transform(x_hat[0]), decimals=1))</kbd><br/>
<kbd>np.set_printoptions(suppress=False)</kbd></div>
<p>In this example, we are focusing on the first data point in the training set—that is, <span><kbd>x_train[0]</kbd>—in the top row; its reconstruction is the bottom row. Close examination reveals that there are differences between both; however, these differences might be relatively small in terms of the MSE.</span></p>
<p>Another important aspect to point out here is that the data needs to be scaled back to its original input space since it was scaled prior to its use in training the model. Fortunately, the <kbd>StandardScaler()</kbd> class has an <kbd>inverse_transform()</kbd> method that can help in mapping any reconstruction back to the <span>range of values of each dimension in </span>input space.</p>
<p>In order to generate more data at will, we can define a method to do so. The following method produces random noise, uniformly, in the range <kbd>[-2, +2]</kbd>, which comes from an examination of <em>Figure 9.5</em>, which shows the range of the latent space to be within such range:</p>
<pre>def generate_samples(N = 10, latent_dim = 2):<br/>  <strong>noise</strong> = np.random.uniform(<strong>-2.0, 2.0</strong>, (N,latent_dim))<br/>  gen = decoder.predict(noise)<br/>  return gen</pre>
<p>This function would need to be adjusted according to the range of values in the latent space; also, it can be adjusted by looking at the distribution of the data in the latent space. For example, if the latent space seems to be normally distributed, then a normal distribution can be used like this: <kbd>noise = np.random.normal(0.0, 1.0, (N,latent_dim))</kbd>, assuming 0 mean and unit variance.</p>
<p>We can call the function to generate <em>fake</em> data by doing the following:</p>
<pre class="mce-root"><strong>gen</strong> = generate_samples(10, latent_dim)<br/>print(np.around(scaler.inverse_transform(<strong>gen</strong>), decimals=1))</pre>
<p class="mce-root"/>
<p>This gives the following output:</p>
<pre>[[ 43.0  0.7  2.7  122.2  223.8  0.0  0.4  172.2  0.0  0.3  1.2  0.1  3.6]<br/> [ 57.4  0.9  3.9  133.1  247.6  0.1  1.2  129.0  0.8  2.1  2.0  1.2  6.4]<br/> [ 60.8  0.7  3.5  142.5  265.7  0.3  1.4  136.4  0.5  1.9  2.0  1.4  5.6]<br/> [ 59.3  0.6  3.2  137.2  261.4  0.2  1.2  146.2  0.3  1.2  1.7  0.9  4.7]<br/> [ 51.5  0.9  3.2  125.1  229.9  0.1  0.7  149.5  0.4  0.9  1.6  0.4  5.1]<br/> [ 60.5  0.5  3.2  139.9  268.4  0.3  1.3  146.1  0.3  1.2  1.7  1.0  4.7]<br/> [ 48.6  0.5  2.6  126.8  243.6  0.1  0.7  167.3  0.0  0.2  1.1  0.1  3.0]<br/> [ 43.7  0.8  2.9  121.2  219.7  0.0  0.5  163.8  0.1  0.5  1.4  0.1  4.4]<br/> [ 54.0  0.3  2.5  135.1  264.2  0.2  1.0  163.4  0.0  0.3  1.1  0.3  2.7]<br/> [ 52.5  1.0  3.6  123.3  227.8  0.0  0.8  137.7  0.7  1.6  1.8  0.6  6.2]]</pre>
<p>Recall that this data is generated from random noise. You can see how this is a major breakthrough in the deep learning community. You can use this data to augment your dataset and produce as many samples as you wish. We can look at the quality of the generated samples and decide for ourselves whether the quality is good enough for our purposes. </p>
<p>Now, granted that since you and I may not be medical doctors specializing in heart disease, we might not be qualified to determine with certainty that the data generated makes sense; but if we did this correctly, it generally does make sense. To make this clear, the next section uses MNIST images to prove that the generated samples are good since we can <span>all</span><span> </span><span>make a visual assessment of numeral images.</span></p>
<p class="mce-root"/>
<h1 id="uuid-15318064-128a-46ee-89ca-4e7401a9a44f">Comparing a deep and shallow VAE on MNIST</h1>
<p>Comparing shallow and deep models is part of the experimentation process that leads to finding the best models. In this comparison over MNIST images, we will be implementing the architecture shown in <em>Figure 9.6</em> as the shallow model, while the deep model architecture is shown in <em>Figure 9.7</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/066f2ead-8aeb-434c-b6fc-e6b5081790be.png" style="width:43.50em;height:37.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.6 – VAE shallow architecture over MNIST</div>
<p class="mce-root"/>
<p>As you can appreciate, both models are substantially different when it comes to the number of layers involved in each one. The quality of the reconstruction will be different as a consequence:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/2b345ff5-b67c-4f2d-977d-1dd9834d5b0d.png" style="width:51.50em;height:32.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.7 – VAE deep architecture over MNIST</span></div>
<p>These models will be trained using a small number of epochs for the shallow VAE and a much larger number of epochs for the deeper model.</p>
<p>The code to reproduce the shallow encoder can be easily inferred from the example used in the Cleveland Heart Disease dataset; however, the code for the deep VAE will be discussed in the sections that follow.</p>
<h2 id="uuid-608d07d6-b8df-41e7-a18b-a058fb16af6d">Shallow VAE</h2>
<p>One of the first things that we can use to compare the VAE is its learned representations. <em>Figure 9.8</em> depicts the latent space projections of the training set:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5234c729-7ee3-4c7c-82d2-2a97581675a7.png" style="width:34.67em;height:30.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.8 – Shallow VAE latent space projections of the training dataset</div>
<p>From the preceding figure, we can observe clusters of data points that are spreading out from the center coordinates. What we would like to see are well-defined clusters that are ideally separated enough so as to facilitate classification, for example. In this case, we see a little bit of overlap among certain groups, particularly number <span class="packt_screen">4</span> and number <span class="packt_screen">9</span>, which makes a lot of sense.</p>
<p>The next thing to look into is the reconstruction ability of the models. <em>Figure 9.9</em> shows a sample of the input, and <em>Figure 9.10</em> shows the corresponding reconstructions after the model has been trained:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9a20024e-4abf-44aa-ac42-9314e2d61afb.png" style="width:17.50em;height:9.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.9 – Sample input to the VAE</span></div>
<p>The expectation for the shallow model is to perform in a manner that is directly related to the size of the model:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9de3f378-6e48-4e41-b4e2-7d575ea4d98f.png" style="width:19.92em;height:10.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.10 – Shallow VAE reconstructions with respect to the input in Figure 9.9</span></div>
<p>Clearly, there seem to be some issues with the reconstruction of the number 2 and number 8, which is confirmed by observing the great deal of overlap shown between these two numerals in <em>Figure 9.8</em>. </p>
<p class="mce-root"/>
<p>Another thing we can do is to visualize the data generated by the VAE if we draw numbers from the range of the latent space. <em>Figure 9.11</em> shows the latent space as it changes across the two dimensions:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/b394a3ad-b012-4f4e-bd26-5ee5448e1c6a.png" style="width:37.92em;height:37.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.11 – Shallow VAE latent space exploration in the range [-4,4] in both dimensions</span></div>
<p>What we find really interesting in <em>Figure 9.11</em> is that we can see how numerals are transformed into others progressively as the latent space is traversed. If we take the center line going from top to bottom, we can see how we can go from the number 0 to the number 6, then to the number 2, then to the number 8, down to the number 1. We could do the same by tracing a diagonal path, or other directions. Making this type of visualization also allows us to see some artifacts that were not seen in the training dataset that could cause potential problems if we generate data without care.</p>
<p class="mce-root"/>
<p>To see whether the deeper model is any better than this, we will implement it in the next section.</p>
<h2 id="uuid-ed14a54f-ae38-4fac-9c0b-3e46e98e4807">Deep VAE</h2>
<p><em>Figure 9.7</em> depicts a deep VAE architecture that can be implemented in parts—first the encoder, and then the decoder.</p>
<h3 id="uuid-c0cd0aad-868d-415b-b493-f0d634eb96f0">Encoder</h3>
<p>The encoder can be implemented using the functional paradigm, as follows:</p>
<pre>from tensorflow.keras.layers import Lambda, Input, Dense, Dropout<br/>from tensorflow.keras.layers import Activation, BatchNormalization<br/>from tensorflow.keras.models import Model<br/><br/>inpt_dim = 28*28<br/>ltnt_dim = 2<br/><br/>inpt_vec = Input(shape=(<strong>inpt_dim</strong>,))</pre>
<p>Here, <kbd>inpt_dim</kbd> corresponds to the 784 dimensions of a 28*28 MNIST image. Continuing with the rest, we have the following:</p>
<pre>el1 = Dropout(0.1)(inpt_vec)<br/>el2 = Dense(512)(el1)<br/>el3 = Activation('relu')(el2)<br/>el4 = Dropout(0.1)(el3)<br/>el5 = Dense(512)(el4)<br/>el6 = BatchNormalization()(el5)<br/>el7 = Activation('relu')(el6)<br/>el8 = Dropout(0.1)(el7)<br/><br/>el9 = Dense(256)(el8)<br/>el10 = Activation('relu')(el9)<br/>el11 = Dropout(0.1)(el10)<br/>el12 = Dense(256)(el11)<br/>el13 = BatchNormalization()(el12)<br/>el14 = Activation('relu')(el13)<br/>el15 = Dropout(0.1)(el14)<br/><br/>el16 = Dense(128)(el15)<br/>el17 = Activation('relu')(el16)<br/>el18 = Dropout(0.1)(el17)<br/>el19 = Dense(ltnt_dim)(el18)<br/>el20 = BatchNormalization()(el19)<br/>el21 = Activation('sigmoid')(el20)<br/><br/><strong>z_mean</strong> = Dense(ltnt_dim)(el21)<br/><strong>z_log_var</strong> = Dense(ltnt_dim)(el21)<br/><strong>z</strong> = Lambda(sampling)([z_mean, z_log_var])<br/><strong>encoder</strong> = Model(inpt_vec, [<strong>z_mean, z_log_var, z</strong>])</pre>
<p>Note that the encoder model uses dropout layers with a 10% dropout rate. The rest of the layers are all things we have seen before, including batch normalization. The only new thing here is the <kbd>Lambda</kbd> function, which is exactly as defined earlier in this chapter.</p>
<p>Next, we will define the decoder.</p>
<h3 id="uuid-fea8d758-2381-4552-b5a9-8b646cf0b9fc">Decoder</h3>
<p>The decoder is a few layers shorter than the encoder. This choice of layers is simply to show that as long as the number of dense layers is almost equivalent in the encoder and decoder, some of the other layers can be omitted as part of the experiment to look for performance boosts.</p>
<p>This is the design of the decoder:</p>
<pre><strong>ltnt_vec</strong> = Input(shape=(<strong>ltnt_dim</strong>,))<br/>dl1 = Dense(128)(ltnt_vec)<br/>dl2 = BatchNormalization()(dl1)<br/>dl3 = Activation('relu')(dl2)<br/><br/>dl4 = Dropout(0.1)(dl3)<br/>dl5 = Dense(256)(dl4)<br/>dl6 = Activation('relu')(dl5)<br/>dl7 = Dense(256)(dl6)<br/>dl8 = BatchNormalization()(dl7)<br/>dl9 = Activation('relu')(dl8)<br/><br/>dl10 = Dropout(0.1)(dl9)<br/>dl11 = Dense(512)(dl10)<br/>dl12 = Activation('relu')(dl11)<br/>dl13 = Dense(512)(dl12)<br/>dl14 = BatchNormalization()(dl13)<br/>dl15 = Activation('relu')(dl14)<br/>dl16 = Dense(inpt_dim, activation='sigmoid') (dl15)<br/><br/><strong>decoder</strong> = Model(<strong>ltnt_vec</strong>, dl16)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Once again, there is nothing new here that we have not seen before. It is all layers after more layers. Then, we can put all this together in the model, as follows:</p>
<pre>outputs = decoder(encoder(<strong>inpt_vec</strong>)<strong>[2]</strong>)<br/><strong>vae</strong> = Model(<strong>inpt_vec</strong>, <strong>outputs</strong>)</pre>
<p>That is it! After this, we can compile the model, choose our optimizer, and train the model in the exact same way that we did in earlier sections.</p>
<p>If we want to visualize the latent space of the deep VAE in order to compare it with <em>Figure 9.8</em>, we could look at the space shown in <em>Figure 9.12</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/243634d2-d778-484e-8dac-ae717e021a0e.png" style="width:34.92em;height:30.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.12 – Deep VAE latent space projections of the training dataset</span></div>
<p>As you can see, the latent space appears to be different in the way the geometry of it looks. This is most likely the effect of activation functions delimiting the latent space range for specific manifolds. One of the most interesting things to observe is the separation of groups of samples even if there still exists some overlap–for example, with numerals <span class="packt_screen">9</span> and <span class="packt_screen">4</span>. However, the overlaps in this case are less severe in comparison to <em>Figure 9.8</em>.</p>
<p><em>Figure 9.13</em> shows the reconstruction of the same input shown in <em>Figure 9.9</em>, but now using the deeper VAE:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c3c397cf-bc79-42f8-b882-7ffecf4360b5.png" style="width:20.00em;height:10.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.13 – Deep VAE reconstructions with respect to the input in Figure 9.9. Compare to Figure 9.10</span></div>
<p>Clearly, the reconstruction is much more better and robust in comparison to the shallow VAE. To make things even clearer, we can also explore the generator by traversing the latent space by producing random noise in the same range as the latent space. This is shown in <em>Figure 9.14</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/f32052b8-c121-446c-9efe-212cf3ccc400.png" style="width:28.67em;height:28.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9.14 – Deep VAE latent space exploration in the range [-4,4] in both dimensions. Compare to Figure 9.11</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The latent space of the deeper VAE clearly seems to be richer in diversity and more interesting from a visual perspective. If we pick the rightmost line and traverse the space from bottom to top, we can see how numeral 1 becomes a 7 and then a 4 with smaller and progressive changes.</p>
<h2 id="uuid-cc1d264d-06d8-4b97-97e0-f5aaa6070e75">Denoising VAEs</h2>
<p>VAEs are also known to be good in image denoising applications (Im, D. I. J., <em>et al.</em> (2017)). This property is achieved by injecting noise as part of the learning process. To find out more about this, you can search the web for denoising VAEs and you will find resources on that particular topic. We just want you to be aware of them and know that they exist if you need them.</p>
<h1 id="uuid-5725e4e5-8d98-4c98-a133-56d4bfe51281">Thinking about the ethical implications of generative models</h1>
<p>Generative models are one of the most exciting topics in deep learning nowadays. But with great power comes great responsibility. We can use the power of generative models for many good things, such as the following:</p>
<ul>
<li>Augmenting your dataset to make it more complete</li>
<li>Training your model with unseen data to make it more stable</li>
<li>Finding adversarial examples to re-train your model and make it more robust</li>
<li>Creating new images of things that look like other things, such as images of art or vehicles</li>
<li>Creating new sequences of sounds that sound like other sounds, such as people speaking or birds singing</li>
<li>Generating new security codes for data encryption</li>
</ul>
<p>We can go on as our imagination permits. What we must always remember is that these generative models, if not modeled properly, can lead to many problems, such as bias, causing trustworthiness issues on your models. It can be easy to use these models to generate a fake sequence of audio of a person saying something that they did not really say, or producing an image of the face of a person doing something they did not really do, or with a body that does not belong to the face.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Some of the most notable wrongdoings include deepfakes. It is not worth our time going through the ways of achieving such a thing, but suffice to say, our generative models should not be used for malicious purposes. Soon enough, international law will be established to punish those who commit crimes through malicious generative modeling. </p>
<p>But until international laws are set and countries adopt new policies, you must follow the best practices when developing your models:</p>
<ul>
<li>Test your models for the most common types of bias: historical, societal, algorithmic, and so on (Mehrabi, N., <em>et al.</em> (2019)).</li>
<li>Train your models using reasonable training and test sets.</li>
<li>Be mindful of data preprocessing techniques; see <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em>Preparing Data</em>, for more details.</li>
<li>Make sure your models produce output that always respects the dignity and worth of all human beings.</li>
<li>Have your model architectures validated by a peer.</li>
</ul>
<p>With this in mind, go on and be as responsible and creative as you can with this new tool that you now have at your disposal: VAEs.</p>
<h1 id="uuid-18a7c7da-62cb-4446-bd2d-7f1ec723441e">Summary </h1>
<p>This advanced chapter has shown you one of the most interesting and simpler models that is able to generate data from a learned distribution using the configuration of an autoencoder and by applying variational Bayes principles leading to a VAE. We looked at the pieces of the model itself and explained them in terms of input data from the Cleveland dataset. Then, we generated data from the learned parametric distribution, showing that VAEs can easily be used for this purpose. To prove the robustness of VAEs on shallow and deep configurations, we implemented a model over the MNIST dataset. The experiment proved that deeper architectures produce well-defined regions of data distributions as opposed to fuzzy groups in shallow architectures; however, both shallow and deep models are particularly good for the task of learning representations.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>By this point, you should feel confident in identifying the pieces of a VAE and being able to tell the main differences between a traditional autoencoder and a VAE in terms of its motivation, architecture, and capabilities. You should appreciate the generative power of VAEs and feel ready to implement them. After reading this chapter, you should be able to code both basic and deep VAE models and be able to use them for dimensionality reduction and data visualization, as well as to generate data while being mindful of the potential risks. Finally, you should now be familiarized with the usage of the <kbd>Lambda</kbd> functions for general-purpose use in TensorFlow and Keras.</p>
<p>If you have liked learning about unsupervised models so far, stay with me and continue to <a href="6ec46669-c8d3-4003-ba28-47114f1515df.xhtml">Chapter 10</a>,<span> </span><em>Restricted Boltzmann Machines,</em><span> which will present a unique model that is rooted in what is known as graphical models. Graphical models use graph theory mixed with learning theory to perform machine learning. An interesting aspect of restricted Boltzmann machines is that the algorithm can go forward and backward during learning to satisfy connection constraints. Stay tuned!</span></p>
<h1 id="uuid-36e9e947-421e-423c-bc83-df43033ebea2">Questions and answers</h1>
<ol>
<li class="mce-root"><strong>How is data generation possible from random noise?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Since the VAE learns the parameters of a parametric random distribution, we can simply use those parameters to sample from such a distribution. Since random noise usually follows a normal distribution with certain parameters, we can say that we are sampling random noise. The nice thing is that the decoder knows what to do with the noise that follows a particular distribution.</p>
<ol start="2">
<li class="mce-root"><strong>What is the advantage of having a deeper VAE? </strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">It is hard to say what the advantage is (if there is any) without having the data or knowing the application. For the Cleveland Heart Disease dataset, for example, a deeper VAE might not be necessary; while for MNIST or CIFAR, a moderately large model might be beneficial. It depends.</p>
<ol start="3">
<li><strong>Is there a way to make changes to the loss function?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Of course, you can change the loss function, but be careful to preserve the principles on which it is constructed. Let's say that a year from now we found a simpler way of minimizing the negative log likelihood function, then we could (and should) come back and edit the loss to adopt the new ideas.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-da118869-0467-42b0-89fd-2743bdd2ea85">References</h1>
<ul>
<li>Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational Bayes. <em>arXiv preprint</em> arXiv:1312.6114.</li>
<li>Salakhutdinov, R., Mnih, A., &amp; Hinton, G. (2007, June). Restricted Boltzmann machines for collaborative filtering. In <em>Proceedings of the 24th International Conference on Machine Learning</em> (pp. 791-798).</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. (2014). Generative adversarial nets. In <em>Advances in Neural Information Processing Systems</em> (pp. 2672-2680).</li>
<li>Zhang, Y., Chuangsuwanich, E., &amp; Glass, J. (2014, May). Extracting deep neural network bottleneck features using low-rank matrix factorization. In <em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (pp. 185-189). IEEE.</li>
<li>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <em>arXiv preprint</em> arXiv:1412.6980.</li>
<li>Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2019). A survey on bias and fairness in machine learning. <em>arXiv preprint</em> arXiv:1908.09635.</li>
<li>Im, D. I. J., Ahn, S., Memisevic, R., &amp; Bengio, Y. (2017, February). Denoising criterion for variational auto-encoding framework. In <em>Thirty-First AAAI Conference on Artificial Intelligence</em>.</li>
</ul>


            </article>

            
        </section>
    </body></html>