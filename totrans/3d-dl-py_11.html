<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer191">
<h1 class="chapter-number" id="_idParaDest-130" lang="en-GB"><a id="_idTextAnchor134"/>10</h1>
<h1 id="_idParaDest-131" lang="en-GB"><a id="_idTextAnchor135"/>Mesh R-CNN</h1>
<p lang="en-GB">This chapter is dedicated to a state-of-the-art model called Mesh R-CNN, which aims to combine two different but important tasks into one end-to-end model. It is a combination of the well-known image segmentation model Mask R-CNN and a new 3D structure prediction model. These two tasks were researched a lot separately.</p>
<p lang="en-GB">Mask R-CNN is an object detection and instance segmentation algorithm that got the highest precision scores in benchmark datasets. It belongs to the R-CNN family and is a two-stage end-to-end object detection model.</p>
<p lang="en-GB">Mesh R-CNN goes beyond the 2D object detection problem and outputs a 3D mesh of detected objects as well. If we think of the world, people see in 3D, which means the objects are 3D. So, why not have a detection model that outputs objects in 3D as well?</p>
<p lang="en-GB">In this chapter, we are going to understand how Mesh R-CNN works. Moreover, we will dive deeper into understanding different elements and techniques used in models such as voxels, meshes, graph convolutional networks, and Cubify operators.</p>
<p lang="en-GB">Next, we will explore the GitHub repository provided by the authors of the Mesh R-CNN paper. We will try the demo on our image and visualize the results of the prediction.</p>
<p lang="en-GB">Finally, we will discuss how we can reproduce the training and testing of Mesh R-CNN and understand the benchmark of the model accuracy.</p>
<p lang="en-GB">In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li lang="en-GB">Understanding mesh and voxel structures</li>
<li lang="en-GB">Understanding the structure of the model</li>
<li lang="en-GB">Understanding what a graph convolution is</li>
<li lang="en-GB">Trying the demo of Mesh R-CNN</li>
<li lang="en-GB">Understanding the training and testing process of the model</li>
</ul>
<h1 id="_idParaDest-132" lang="en-GB"><a id="_idTextAnchor136"/>Technical requirements</h1>
<p lang="en-GB">To run the example code snippets in this book, ideally, you need to have a computer with a GPU. However, running the code snippets with only CPUs is not impossible.</p>
<p lang="en-GB">The following are the recommended computer configurations:</p>
<ul>
<li lang="en-GB">A GPU from, for example, the NVIDIA GTX series or RTX series with at least 8 GB of memory</li>
<li lang="en-GB">Python 3</li>
<li lang="en-GB">The PyTorch library and PyTorch3D libraries</li>
<li lang="en-GB">Detectron2</li>
<li lang="en-GB">The Mesh R-CNN repository, which can be found at <a href="https://github.com/facebookresearch/meshrcnn">https://github.com/facebookresearch/meshrcnn</a></li>
</ul>
<p lang="en-GB">The code snippets for this chapter can be found at <a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python</a>.</p>
<h1 id="_idParaDest-133" lang="en-GB"><a id="_idTextAnchor137"/>Overview of meshes and voxels</h1>
<p lang="en-GB">As mentioned earlier in this book, meshes<a id="_idIndexMarker450"/> and voxels are two different 3D data representations. Mesh R-CNN uses both representations to get better quality 3D structure predictions.</p>
<p lang="en-GB">A mesh is the surface of a 3D model represented as polygons, where each polygon can be represented as a triangle. Meshes consist of vertices connected by edges. The edge and vertex connection creates faces that have a commonly triangular shape. This representation is good for faster transformations and rendering:</p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<img alt="Figure 10.1: Example of a polygon mesh " height="509" src="image/B18217_10_001.jpg" width="825"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Example of a polygon mesh</p>
<p lang="en-GB">Voxels are the 3D <a id="_idIndexMarker451"/>analogs of 2D pixels. As each image consists of 2D pixels, it is logical to use the same idea to represent 3D data. Each voxel is a cube, and each object is a group of cubes where some of <a id="_idIndexMarker452"/>them are the outer visible parts, and some of them are inside the object. It’s easier to visualize 3D objects with voxels, but it’s not the only use case. In deep learning problems, voxels can be used as input for 3D convolutional neural networks:</p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="Figure 10.2: Example of a voxel " height="525" src="image/B18217_10_002.jpg" width="458"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: Example of a voxel</p>
<p lang="en-GB">Mesh R-CNN uses both types of 3D data representations. Experiments have shown that predicting voxels and then<a id="_idIndexMarker453"/> converting them into the mesh, and then refining the mesh, helps the network learn better.</p>
<p lang="en-GB">Next, we’ll look at the<a id="_idIndexMarker454"/> Mesh R-CNN architecture to see how the aforementioned 3D representations of data are created from image input.</p>
<h1 id="_idParaDest-134" lang="en-GB"><a id="_idTextAnchor138"/>Mesh R-CNN architecture</h1>
<p lang="en-GB">3D shape detection has<a id="_idIndexMarker455"/> captured the interest of many researchers. Many models have been developed that have gotten good accuracy, but they mostly focused on synthetic benchmarks and isolated objects:</p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 10.3: 3D object examples of the ShapeNet dataset " height="662" src="image/B18217_10_003.jpg" width="825"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3: 3D object examples of the ShapeNet dataset</p>
<p lang="en-GB">At the same time, 2D object detection and image segmentation problems have had rapid advances as well. Many models and architectures solve this problem with high accuracy and speed. There are solutions for localizing objects and detecting the bounding boxes and masks. One of them is called Mask R-CNN, which is a model for object detection and instance segmentation. This model is state-of-the-art and has a lot of real-life applications.</p>
<p lang="en-GB">However, we see the world in 3D. The authors of the Mesh R-CNN paper decided to combine these two approaches into a single solution: a model that detects the object on a realistic image and outputs the 3D mesh instead of the mask. The new model takes a state-of-the-art object <a id="_idIndexMarker456"/>detection model, which takes an RGB image as input and outputs the class label, segmentation mask, and 3D mesh of the objects. The authors have added a new branch to Mask R-CNN that is responsible for predicting high-resolution triangle meshes:</p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="Figure 10.5: Mesh R-CNN general structure " height="561" src="image/B18217_10_004.jpg" width="790"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5: Mesh R-CNN general structure</p>
<p lang="en-GB">The authors aimed to create one model that is end-to-end trainable. That is why they took the state-of-the-art Mask R-CNN model and added a new branch for mesh prediction. Before diving deeper into the mesh prediction part, let’s quickly recap Mask R-CNN:</p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 10.6: Mask R-CNN structure (Reference: https://arxiv.org/abs/1703.06870) " height="689" src="image/B18217_10_005Redraw.jpg" width="1597"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6: Mask R-CNN structure (Reference: https://arxiv.org/abs/1703.06870)</p>
<p lang="en-GB">Mask R-CNN takes an RGB image<a id="_idIndexMarker457"/> as input and outputs bounding boxes, category labels, and instance segmentation masks. First, the image passes through the backbone network, which is<a id="_idIndexMarker458"/> typically based on ResNet – for example, ResNet-50-FPN. The backbone network outputs the feature map, which is the input of the next network: the <strong class="bold" lang="">region proposal network</strong> (<strong class="bold" lang="">RPN</strong>). This network outputs proposals. The object classification and mask prediction branches then process the proposals and output classes and masks, respectively.</p>
<p lang="en-GB">This structure of Mask R-CNN is the same for Mesh R-CNN as well. However, in the end, a mesh predictor was added. A mesh predictor is a new module that consists of two branches: the voxel branch and the mesh refinement branch.</p>
<p lang="en-GB">The voxel branch takes proposed and aligned features as input and outputs the coarse voxel predictions. These are then given as input to the mesh refinement branch, which outputs the final mesh. The losses of the voxel branch and mesh refinement branch are added to the box and mask losses and the model is trained end to end:</p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 10.7: Mesh R-CNN architecture " height="440" src="image/B18217_10_006.jpg" width="1277"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7: Mesh R-CNN architecture</p>
<h2 id="_idParaDest-135" lang="en-GB"><a id="_idTextAnchor139"/>Graph convolutions</h2>
<p lang="en-GB">Before we look at the structure <a id="_idIndexMarker459"/>of the mesh predictor, let’s understand what a graph convolution is and how it works.</p>
<p lang="en-GB">Early variants of neural networks were adopted<a id="_idIndexMarker460"/> for structured Euclidean data. However, in the real world, most data is non-Euclidian and has graph structures. Recently, many variants of neural networks have started to adapt to <a id="_idIndexMarker461"/>graph data as well, with one of them being convolutional networks, which are called <strong class="bold" lang="">graph convolutional networks</strong> (<strong class="bold" lang="">GCNs</strong>).</p>
<p lang="en-GB">Meshes have this graph structure, which is why GCNs are applicable in 3D structure prediction problems. The basic operation of a CNN is convolution, which is done using filters. We use the sliding window technique for convolution, and the filters include weights that the model should learn. GCNs use a similar technique for convolution, though the main difference is that the number of nodes can vary, and the nodes are unordered:</p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 10.8: Example of a convolution operation in Euclidian and graph data (Source: https://arxiv.org/pdf/1901.00596.pdf) " height="548" src="image/B18217_10_007Redraw.jpg" width="1266"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8: Example of a convolution operation in Euclidian and graph data (Source: https://arxiv.org/pdf/1901.00596.pdf)</p>
<p lang="en-GB"><em class="italic" lang="">Figure 10.9</em> shows an example of a<a id="_idIndexMarker462"/> convolutional layer. The input of the network is the graph and adjacency matrix, which represents the edges between the nodes in forward propagation. The convolution<a id="_idIndexMarker463"/> layer encapsulates information for each node by aggregating information from its neighborhood. After that, nonlinear transformation is applied. Later, the output of this network can be used in different tasks, such as classification:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 10.9: Example of a convolutional neural network (Source: https://arxiv.org/pdf/1901.00596.pdf) " height="598" src="image/B18217_10_008Redraw.jpg" width="1586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9: Example of a convolutional neural network (Source: https://arxiv.org/pdf/1901.00596.pdf)</p>
<h2 id="_idParaDest-136" lang="en-GB"><a id="_idTextAnchor140"/>Mesh predictor</h2>
<p lang="en-GB">The mesh predictor module aims to detect the 3D structure of an object. It is the logical continuation of the <strong class="source-inline" lang="">RoIAlign</strong> module, and it is responsible for predicting and outputting the final mesh.</p>
<p lang="en-GB">As we get 3D meshes from real-life images, we can’t use fixed mesh templates with fixed mesh topologies. That is why the mesh predictor consists of two branches. The combination of the voxel branch and mesh refinement branch helps reduce the issue with fixed topologies.</p>
<p lang="en-GB">The voxel branch is analogous<a id="_idIndexMarker464"/> to the mask branch from Mask R-CNN. It takes aligned features from <strong class="source-inline" lang="">ROIAlign</strong> and outputs a G x G x G grid of voxel occupancy <a id="_idIndexMarker465"/>probabilities. Next, the Cubify operation is used. It uses a threshold for binarizing voxel occupancy. Each occupied voxel is replaced with a cuboid triangle mesh with 8 vertices, 18 edges, and 12 faces.</p>
<p lang="en-GB">The voxel loss is binary cross-entropy, which minimizes the predicted probabilities of voxel occupancy with ground truth occupancies.</p>
<p lang="en-GB">The mesh refinement branch is a sequence of three different operations: vertex alignment, graph convolution, and vertex refinement. Vertex alignment is similar to ROI alignment; for each mesh vertex, it yields an image-aligned feature.</p>
<p lang="en-GB">Graph convolution takes image-aligned features and propagates information along mesh edges. Vertex refinement updates vertex positions. It aims to update vertex geometry by keeping the topology fixed:</p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 10.10: Mesh refinement branch " height="208" src="image/B18217_10_009.jpg" width="995"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10: Mesh refinement branch</p>
<p lang="en-GB">As shown in <em class="italic" lang="">Figure 10.10</em>, we can have multiple stages of refinement. Each stage consists of vertex alignment, graph convolution, and vertex refinement operations. In the end, we get a more accurate 3D mesh.</p>
<p lang="en-GB">The final important part of the model is the mesh loss function. For this branch, chamfer and normal losses are used. However, these techniques need sampled points from predicted and ground-truth meshes.</p>
<p lang="en-GB">The following mesh sampling method is used: given vertices and faces, the points are uniformly sampled from a <a id="_idIndexMarker466"/>probability distribution of the surface of the mesh. The probability of each face is<a id="_idIndexMarker467"/> proportional to its area.</p>
<p lang="en-GB">Using these sampling techniques, a point cloud from the ground truth, <em class="italic" lang="">Q</em>, and a point cloud from the prediction, <em class="italic" lang="">P</em>, are sampled. Next, we calculate <em class="italic" lang="">Λ</em><span class="subscript" lang="">PQ</span>, which is the set of pairs (<em class="italic" lang="">p</em>,<em class="italic" lang="">q</em>) where <em class="italic" lang="">q</em> is the nearest neighbor of <em class="italic" lang="">p</em> in <em class="italic" lang="">Q</em>.</p>
<p lang="en-GB">Chamfer distance is calculated between <em class="italic" lang="">P</em> and <em class="italic" lang="">Q</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="" height="137" src="image/Formula_10_001.jpg" width="1125"/>
</div>
</div>
<p lang="en-GB">Next, the absolute normal distance is calculated:</p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="" height="137" src="image/Formula_10_002.jpg" width="1122"/>
</div>
</div>
<p lang="en-GB">Here, <em class="italic" lang="">u</em><span class="subscript" lang="">p</span> and <em class="italic" lang="">u</em><span class="subscript" lang="">q</span> are the units normal to points <em class="italic" lang="">p</em> and <em class="italic" lang="">q</em>, respectively.</p>
<p lang="en-GB">However, only these two losses degenerated meshes. This is why, for high-quality mesh production, a shape regularizer was added, which was called edge loss:</p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="" height="122" src="image/Formula_10_003.jpg" width="1032"/>
</div>
</div>
<p lang="en-GB">The final mesh loss is the weighted average of three presented losses: chamfer loss, normal loss, and edge loss.</p>
<p lang="en-GB">In terms of training, two types of experiments were conducted. The first one was to check the mesh predictor branch. Here, the ShapeNet dataset was used, which includes 55 common categories of classes. This is widely used in benchmarking for 3D shape prediction; however, it includes CAD models, which have separate backgrounds. Due to this, the mesh predictor <a id="_idIndexMarker468"/>model reached state-of-the-art status. Moreover, it solves issues regarding objects with holes that previous models couldn’t detect well:</p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 10.11: Mesh predictor on the ShapeNet dataset " height="451" src="image/B18217_10_010.jpg" width="660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11: Mesh predictor on the ShapeNet dataset</p>
<p lang="en-GB">The third row represents<a id="_idIndexMarker469"/> the output of the mesh predictor. We can see that it predicts the 3D shape and that it handles the topology and geometry of objects very well:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 10.12: The output of the end-to-end Mesh R-CNN model " height="427" src="image/B18217_10_011.jpg" width="644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12: The output of the end-to-end Mesh R-CNN model</p>
<p lang="en-GB">The next step is to <a id="_idIndexMarker470"/>perform experiments on real-life images. For this, the Pix3D dataset was used, which includes 395 unique 3D models placed in 10,069 real-life images. In this case, benchmark results <a id="_idIndexMarker471"/>are not available, because the authors were the first to try this technique. However, we can check the output results from the training in <em class="italic" lang="">Figure 10.11</em>.</p>
<p lang="en-GB">With that, we have discussed the Mesh R-CNN architecture. Now, we can get hands-on and use Mesh R-CNN to find objects in test images.</p>
<h1 id="_idParaDest-137" lang="en-GB"><a id="_idTextAnchor141"/>Demo of Mesh R-CNN with PyTorch</h1>
<p lang="en-GB">In this section, we will use the Mesh R-CNN repository to run the demo. We will try the model on our image and<a id="_idIndexMarker472"/> render the output <strong class="source-inline" lang="">.obj</strong> file to see how the model predicts the 3D shape. Moreover, we will discuss the training process of the model.</p>
<p lang="en-GB">Installing Mesh R-CNN is pretty straightforward. You need to install Detectron2 and PyTorch3D first, then build <a id="_idIndexMarker473"/>Mesh R-CNN. <strong class="source-inline" lang="">Detectron2</strong> is a library from Facebook Research that provides state-of-the-art detection and segmentation models. It includes Mask R-CNN as well, the model on which Mesh R-CNN was built. You can install <strong class="source-inline" lang="">detectron2</strong> by running the following command:</p>
<p class="source-code" lang="en-GB">python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'</p>
<p lang="en-GB">If this doesn’t work for you, check the website for alternative ways to install it. Next, you need to install PyTorch3D, as described earlier in this book. When both requirements are ready, you just need to<a id="_idIndexMarker474"/> build Mesh R-CNN:</p>
<p class="source-code" lang="en-GB">git clone https://github.com/facebookresearch/meshrcnn.git</p>
<p class="source-code" lang="en-GB">cd meshrcnn &amp;&amp; pip install -e .</p>
<h2 id="_idParaDest-138" lang="en-GB"><a id="_idTextAnchor142"/>Demo</h2>
<p lang="en-GB">The repository includes a <strong class="source-inline" lang="">demo.py</strong> file, which is used to demonstrate how the end-to-end process of Mesh R-CNN works. The file is located in <strong class="source-inline" lang="">meshrcnn/demo/demo.py</strong>. Let’s look at the code to <a id="_idIndexMarker475"/>understand how the demo is done. The file includes the <strong class="source-inline" lang="">VisualizationDemo</strong> class, which consists of two main methods: <strong class="source-inline" lang="">run_on_image</strong> and <strong class="source-inline" lang="">visualize_prediction</strong>. The method names speak for themselves: the first takes an image as input and outputs predictions of the model, while the other visualizes the detection of the mask, and then saves the final mesh and the image with predictions and confidence:</p>
<p class="source-code" lang="en-GB">python demo/demo.py \</p>
<p class="source-code" lang="en-GB">--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \</p>
<p class="source-code" lang="en-GB">--input /path/to/image \</p>
<p class="source-code" lang="en-GB">--output output_demo \</p>
<p class="source-code" lang="en-GB">--onlyhighest MODEL.WEIGHTS meshrcnn://meshrcnn_R50.pth</p>
<p lang="en-GB">For the demo, you just need to run the preceding command from the terminal. The command has the following parameters:</p>
<ul>
<li lang="en-GB"><strong class="source-inline" lang="">--config-file</strong> takes the path to the config file, which can be found in the <strong class="source-inline" lang="">configs</strong> directory</li>
<li lang="en-GB"><strong class="source-inline" lang="">--input</strong> takes the path to the input image</li>
<li lang="en-GB"><strong class="source-inline" lang="">--output</strong> takes the path to the directory where predictions should be saved</li>
<li lang="en-GB"><strong class="source-inline" lang="">--onlyhighest</strong>, if <strong class="source-inline" lang="">True</strong>, outputs only one mesh and mask that has the highest confidence</li>
</ul>
<p lang="en-GB">Now, let’s run and check the output.</p>
<p lang="en-GB">For the demo, we <a id="_idIndexMarker476"/>will use the image of the apartment that we used in the <a id="_idIndexMarker477"/>previous chapter:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 10.13: The input image for the network " height="767" src="image/B18217_10_012.jpg" width="1125"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13: The input image for the network</p>
<p lang="en-GB">We give the path to this image to <strong class="source-inline" lang="">demo.py</strong>. After prediction, we get the mask visualization and mesh of the image. Since we used the <strong class="source-inline" lang="">--onlyhighest</strong> argument, we only got one mask, which is the prediction of the sofa object. This has an 88.7% confidence score. The mask prediction is correct – it covers almost the entire sofa:</p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 10.14: The output of the demo.py file " height="768" src="image/B18217_10_013.jpg" width="1125"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14: The output of the demo.py file</p>
<p lang="en-GB">Besides the mask, we<a id="_idIndexMarker478"/> also got the mesh in the same directory, which<a id="_idIndexMarker479"/> is a <strong class="source-inline" lang="">.obj</strong> file. Now, we need to render images from the 3D object.</p>
<p lang="en-GB">The following code is from the <strong class="source-inline" lang="">chapt10/viz_demo_results.py</strong> file:</p>
<ol>
<li lang="en-GB">First, let’s import all the libraries used in the code:<p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import numpy as np</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">from pytorch3d.io import load_obj</p><p class="source-code" lang="en-GB">from pytorch3d.structures import Meshes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">   FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,</p><p class="source-code" lang="en-GB">   RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,</p><p class="source-code" lang="en-GB">   SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">import argparse</p></li>
<li lang="en-GB">Next, we are <a id="_idIndexMarker480"/>going to define arguments to run the code:<p class="source-code" lang="en-GB">parser = argparse.ArgumentParser()</p><p class="source-code" lang="en-GB">parser.add_argument('--path_to_mesh', default="./demo_results/0_mesh_sofa_0.887.obj")</p><p class="source-code" lang="en-GB">parser.add_argument('--save_path', default='./demo_results/sofa_render.png')</p><p class="source-code" lang="en-GB">parser.add_argument('--distance', default=1, help = 'distance from camera to the object')</p><p class="source-code" lang="en-GB">parser.add_argument('--elevation', default=150.0,  help = 'angle of elevation in degrees')</p><p class="source-code" lang="en-GB">parser.add_argument('--azimuth', default=-10.0, help = 'rotation of the camera')</p><p class="source-code" lang="en-GB">args = parser.parse_args()</p></li>
</ol>
<p lang="en-GB">We need input for <strong class="source-inline" lang="">path_to_mesh</strong> – that is, the output <strong class="source-inline" lang="">.obj</strong> file of <strong class="source-inline" lang="">demo.py</strong>. We also need to specify the path where the rendered output should be saved, then specify<a id="_idIndexMarker481"/> the distance from the camera, elevation angle, and rotation.</p>
<ol>
<li lang="en-GB" value="3">Next, we must load and initialize the mesh object. First, we must load the <strong class="source-inline" lang="">.obj</strong> file with the <strong class="source-inline" lang="">load_obj</strong> function from <strong class="source-inline" lang="">pytorch3d</strong>. Then, we must make the vertexes white. We will use the <strong class="source-inline" lang="">Meshes</strong> structure from <strong class="source-inline" lang="">pytorch3d</strong> to create a mesh object:<p class="source-code" lang="en-GB"># Load the obj and ignore the textures and materials.</p><p class="source-code" lang="en-GB">verts, faces_idx, _ = load_obj(args.path_to_mesh)</p><p class="source-code" lang="en-GB">faces = faces_idx.verts_idx</p><p class="source-code" lang="en-GB"># Initialize each vertex to be white in color.</p><p class="source-code" lang="en-GB">verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)</p><p class="source-code" lang="en-GB">textures = TexturesVertex(verts_features=verts_rgb.to(device))</p><p class="source-code" lang="en-GB"># Create a Meshes object for the sofa. Here we have only one mesh in the batch.</p><p class="source-code" lang="en-GB">sofa_mesh = Meshes(</p><p class="source-code" lang="en-GB">   verts=[verts.to(device)],</p><p class="source-code" lang="en-GB">   faces=[faces.to(device)],</p><p class="source-code" lang="en-GB">   textures=textures</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">The next step is <a id="_idIndexMarker482"/>to initialize the perspective camera. Then, we <a id="_idIndexMarker483"/>need to set blend parameters that will be used to blend faces. <strong class="source-inline" lang="">sigma</strong> controls opacity, whereas <strong class="source-inline" lang="">gamma</strong> controls the sharpness of edges:<p class="source-code" lang="en-GB">cameras = FoVPerspectiveCameras(device=device)</p><p class="source-code" lang="en-GB">blend_params = BlendParams(sigma=1e-4, gamma=1e-4)</p></li>
<li lang="en-GB">Next, we must define settings for rasterization and shading. We will set the output image size to 256*256 and set <strong class="source-inline" lang="">faces_per_pixel</strong> to 100, which will blend 100 faces for one pixel. Then, we will use rasterization settings to create a silhouette mesh renderer by composing a rasterizer and a shader:<p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">   image_size=256,</p><p class="source-code" lang="en-GB">   blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,</p><p class="source-code" lang="en-GB">   faces_per_pixel=100,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">silhouette_renderer = MeshRenderer(</p><p class="source-code" lang="en-GB">   rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">       cameras=cameras,</p><p class="source-code" lang="en-GB">       raster_settings=raster_settings</p><p class="source-code" lang="en-GB">   ),</p><p class="source-code" lang="en-GB">   shader=SoftSilhouetteShader(blend_params=blend_params)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">We need to<a id="_idIndexMarker484"/> create one more <strong class="source-inline" lang="">RasterizationSettings</strong> object since we will use the Phong renderer as well. It will only need to blend one face per pixel. Again, the image output will be 256. Then, we need to <a id="_idIndexMarker485"/>add a point light in front of the object. Finally, we need to initialize the Phong renderer:<p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">   image_size=256,</p><p class="source-code" lang="en-GB">   blur_radius=0.0,</p><p class="source-code" lang="en-GB">   faces_per_pixel=1,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))</p><p class="source-code" lang="en-GB">phong_renderer = MeshRenderer(</p><p class="source-code" lang="en-GB">   rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">       cameras=cameras,</p><p class="source-code" lang="en-GB">       raster_settings=raster_settings</p><p class="source-code" lang="en-GB">   ),</p><p class="source-code" lang="en-GB">   shader=HardPhongShader(device=device, cameras=cameras, lights=lights)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Now, we must <a id="_idIndexMarker486"/>create the position of the camera based on spheral angles. We will use the <strong class="source-inline" lang="">look_at_view_transform</strong> function and add the <strong class="source-inline" lang="">distance</strong>, <strong class="source-inline" lang="">elevation</strong>, and <strong class="source-inline" lang="">azimuth</strong> parameters that were mentioned previously. Lastly, we must get the rendered output from the silhouette and Phong renderer by giving them <a id="_idIndexMarker487"/>the mesh and camera position as input:<p class="source-code" lang="en-GB">R, T = look_at_view_transform(distance, elevation, azimuth, device=device)</p><p class="source-code" lang="en-GB"># Render the sofa providing the values of R and T.</p><p class="source-code" lang="en-GB">silhouette = silhouette_renderer(meshes_world=sofa_mesh, R=R, T=T)</p><p class="source-code" lang="en-GB">image_ref = phong_renderer(meshes_world=sofa_mesh, R=R, T=T)</p></li>
<li lang="en-GB">The last step is to visualize the results. We will use <strong class="source-inline" lang="">matplotlib</strong> to plot both rendered images:<p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.subplot(1, 2, 1)</p><p class="source-code" lang="en-GB">plt.imshow(silhouette.squeeze()[..., 3])</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.subplot(1, 2, 2)</p><p class="source-code" lang="en-GB">plt.imshow(image_ref.squeeze())</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.savefig(args.save_path)</p></li>
</ol>
<p lang="en-GB">The output of the preceding code will be a <strong class="source-inline" lang="">.png</strong> image that will be saved in the <strong class="source-inline" lang="">save_path</strong> folder <a id="_idIndexMarker488"/>given in the arguments. For this <a id="_idIndexMarker489"/>parameter and the image presented here, the rendered mesh will look like this:</p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 10.16: Rendered 3D output of the model " height="404" src="image/B18217_10_014.jpg" width="836"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16: Rendered 3D output of the model</p>
<p lang="en-GB">As we can see from this angle, the mesh looks very similar to the sofa, not counting some defects on not visible parts. You can play with camera position and lighting to render an image of the object from another point of view.</p>
<p lang="en-GB">The repository also provides an opportunity to run and reproduce the experiments described in the Mesh R-CNN paper. It allows you to run both the Pix3D experiment and the ShapeNet experiment.</p>
<p lang="en-GB">As mentioned earlier, the Pix3D data includes real-life images of different IKEA furniture. This data was used to evaluate the whole Mesh R-NN from end to end.</p>
<p lang="en-GB">To download this data, you need to run the following command:</p>
<p class="source-code" lang="en-GB">datasets/pix3d/download_pix3d.sh</p>
<p lang="en-GB">The data contains two splits named S1 and S2 and the repository provides weights for both splits. After downloading the data, you can reproduce the training by running the following command:</p>
<p class="source-code" lang="en-GB">python tools/train_net.py \</p>
<p class="source-code" lang="en-GB">--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \</p>
<p class="source-code" lang="en-GB">--eval-only MODEL.WEIGHTS /path/to/checkpoint_file</p>
<p lang="en-GB">You just need to be careful with the configs. The original model was distributed and trained on 8 GB of GPU. If you<a id="_idIndexMarker490"/> don’t have that much capacity, it probably<a id="_idIndexMarker491"/> won’t reach the same accuracy, so you need to tune your hyperparameters for better accuracy.</p>
<p lang="en-GB">You can use your trained weights or you can simply run an evaluation on the pre-trained models provided by the authors:</p>
<p class="source-code" lang="en-GB">python tools/train_net.py \</p>
<p class="source-code" lang="en-GB">--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \</p>
<p class="source-code" lang="en-GB">--eval-only MODEL.WEIGHTS /path/to/checkpoint_file</p>
<p lang="en-GB">The preceding command will evaluate the model for the specified checkpoint file. You can find the checkpoints by going to the model’s GitHub repository.</p>
<p lang="en-GB">Next, if you want to run the experiment on ShapeNet, you need to download the data, which can be done by running the following command:</p>
<p class="source-code" lang="en-GB">datasets/shapenet/download_shapenet.sh</p>
<p lang="en-GB">This will download the training, validation, and test sets. The authors have also provided the preprocessing code for the ShapeNet dataset. Preprocessing will reduce the loading time. The following command will output zipped data, which is convenient for training in clusters:</p>
<p class="source-code" lang="en-GB">python tools/preprocess_shapenet.py \</p>
<p class="source-code" lang="en-GB">--shapenet_dir /path/to/ShapeNetCore.v1 \</p>
<p class="source-code" lang="en-GB">--shapenet_binvox_dir /path/to/ShapeNetCore.v1.binvox \</p>
<p class="source-code" lang="en-GB">--output_dir ./datasets/shapenet/ShapeNetV1processed \</p>
<p class="source-code" lang="en-GB">--zip_output</p>
<p lang="en-GB">Next, to reproduce the experiment, you just need to run the <strong class="source-inline" lang="">train_net_shapenet.py</strong> file with corresponding configs. Again, be careful when adjusting the training process to your hardware capacity:</p>
<p class="source-code" lang="en-GB">python tools/train_net_shapenet.py --num-gpus 8 \</p>
<p class="source-code" lang="en-GB">--config-file configs/shapenet/voxmesh_R50.yaml</p>
<p lang="en-GB">Finally, you can always <a id="_idIndexMarker492"/>evaluate your model, or the checkpoints provided<a id="_idIndexMarker493"/> by the authors, by running the following command:</p>
<p class="source-code" lang="en-GB">python tools/train_net_shapenet.py --num-gpus 8 \</p>
<p class="source-code" lang="en-GB">--config-file configs/shapenet/voxmesh_R50.yaml</p>
<p lang="en-GB">You can compare your results with the results provided in the paper. The following chart shows the scale-normalized protocol training results that the authors got:</p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 10.17: The results of evaluation on the ShapeNet dataset " height="757" src="image/B18217_10_015.jpg" width="825"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17: The results of evaluation on the ShapeNet dataset</p>
<p lang="en-GB">The chart includes the <a id="_idIndexMarker494"/>category name, number of instances per <a id="_idIndexMarker495"/>category, chamfer, normal losses, and the F1 scores.</p>
<h1 id="_idParaDest-139" lang="en-GB"><a id="_idTextAnchor143"/>Summary</h1>
<p lang="en-GB">In this chapter, we presented a new way of looking at the object detection task. The 3D world requires solutions that work accordingly, and this is one of the first approaches toward that goal. We learned how Mesh R-CNN works by understanding the architecture and the structure of the model. We dove deeper into some interesting operations and techniques that are used in the model, such as graph convolutional networks, Cubify operations, the mesh predictor structure, and more. Finally, we learned how this model can be used in practice to detect objects on the image that the network has never seen before. We evaluated the results by rendering the 3D object.</p>
<p lang="en-GB">Throughout this book, we have covered 3D deep learning concepts, from the basics to more advanced solutions. First, we learned about the various 3D data types and structures. Then, we delved into different types of models that solve different types of problems such as mesh detection, view synthesis, and more. In addition, we added PyTorch 3D to our computer vision toolbox. By completing this book, you should be ready to tackle real-world problems related to 3D computer vision and much more.</p>
</div>
<div>
<div id="_idContainer192">
</div>
</div>
</div></body></html>