- en: '*Chapter 10*: Implementing DL Explainability with MLflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The importance of **deep learning** (**DL**) explainability is now well established,
    as we learned in the previous chapter. In order to implement DL explainability
    in a real-world project, it is desirable to log the explainer and the explanations
    as artifacts, just like other model artifacts in the MLflow server, so that we
    can easily track and reproduce the explanation. The integration of DL explainability
    tools such as SHAP ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    with MLflow can support different implementation mechanisms, and it is important
    to understand how these integrations can be used for our DL explainability scenarios.
    In this chapter, we will explore several ways to integrate the SHAP explanations
    into MLflow by using different MLflow capabilities. As tools for explainability
    and DL models are both rapidly evolving, we will also highlight the current limitations
    and workarounds when using MLflow for DL explainability implementation. By the
    end of this chapter, you will feel comfortable implementing SHAP explanations
    and explainers using MLflow APIs for scalable model explainability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding current MLflow explainability integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing SHAP explanations using the MLflow artifact logging API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing SHAP explainers using the MLflow pyfunc API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements are necessary to complete this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow full-fledged local server: This is the same one we have been using since
    [*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040), *Tracking Models, Parameters,
    and Metrics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SHAP Python library: [https://github.com/slundberg/shap](https://github.com/slundberg/shap).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark 3.2.1 and PySpark 3.2.1: See the details in the `README.md` file of this
    chapter''s GitHub repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding current MLflow explainability integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLflow has several ways to support explainability integration. When implementing
    explainability, we refer to two types of artifacts: explainers and explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: An explainer is an explainability model, and a common one is a SHAP model that
    could be different kinds of SHAP explainers, such as **TreeExplainer**, **KernelExplainer**,
    and **PartitionExplainer** ([https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html)).
    For computational efficiency, we usually choose **PartitionExplainer** for DL
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An explanation is an artifact that shows some form of output from the explainer,
    which could be text, numerical values, or plots. Explanations can happen in offline
    training or testing, or can happen during online production. Thus, we should be
    able to provide an explainer for offline evaluation or an explainer endpoint for
    online queries if we want to know why the model provides certain predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we give a brief overview of the current capability as of MLflow version
    1.25.1 ([https://pypi.org/project/mlflow/1.25.1/](https://pypi.org/project/mlflow/1.25.1/)).
    There are four different ways to use MLflow for explainability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `mlflow.log_artifact` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact))
    to log relevant explanation artifacts such as bar plots and Shapley values arrays.
    This gives maximum flexibility for logging explanations. This can be used either
    offline as batch processing or online when we automatically log a SHAP bar plot
    for a certain prediction. Note that logging an explanation for each prediction
    during online production scenarios is expensive, so we should provide a separate
    explanation API for on-demand queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `mlflow.pyfunc.PythonModel` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))
    to create an explainer that can be logged and loaded with MLflow's `pyfunc` methods,
    `mlflow.pyfunc.log_model` for logging and `mlflow.pyfunc.load_model` or `mlflow.pyfunc.spark_udf`
    for loading an explainer. This gives us maximum flexibility to create customized
    explainers as MLflow generic `pyfunc` models and can be used for either offline
    batch explanation or online as an **Explanation as a Service** (**EaaS**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `mlflow.shap` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html](https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html)).
    This has some limitations. For example, the `mlflow.shap.log_explainer` method
    only supports scikit-learn and PyTorch models. The `mlflow.shap.log_explanation`
    method only supports `shap.KernelExplainer` ([https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html)).
    This is very computationally intensive, as the computing time grows exponentially
    with respect to the number of features; thus, it is not feasible to compute explanations
    for even a moderate size dataset (see a posted GitHub issue [https://github.com/mlflow/mlflow/issues/4071](https://github.com/mlflow/mlflow/issues/4071)).
    The existing examples provided by MLflow are for classical ML models in scikit-learn
    packages such as linear regression or random forest, with no DL model explainability
    examples ([https://github.com/mlflow/mlflow/tree/master/examples/shap](https://github.com/mlflow/mlflow/tree/master/examples/shap)).
    We will show in later sections of this chapter that this API currently does not
    support the transformers-based SHAP explainers and explanations, thus we will
    not use this API in this chapter. We will highlight some of the issues as we walk
    through our examples in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `mlflow.evaluate` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)).
    This can be used for evaluation after the model is already trained and tested.
    This is an experimental feature and might change in the future. It supports MLflow
    `pyfunc` models. However, it has some limitations in that the evaluation dataset
    label values must be numeric or Boolean, all feature values must be numeric, and
    each feature column must only contain scalar values ([https://www.mlflow.org/docs/latest/models.html#model-evaluation](https://www.mlflow.org/docs/latest/models.html#model-evaluation)).
    Again, existing examples provided by MLflow are only for classical ML models in
    scikit-learn packages ([https://github.com/mlflow/mlflow/tree/master/examples/evaluation](https://github.com/mlflow/mlflow/tree/master/examples/evaluation)).
    We could use this API to just log the classifier metrics for an NLP sentiment
    model, but the explanation part will be skipped automatically by this API because
    it requires a feature column containing scalar values (an NLP model input is a
    text input). Thus, this is not applicable to the DL model explainability we need.
    So, we will not use this API in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that some of these APIs are still experimental and are still evolving,
    users should be aware of the limitations and workarounds to successfully implement
    explainability with MLflow. For DL model explainability, as we will learn in this
    chapter, it is quite challenging to implement using MLflow as the MLflow integration
    with SHAP is still a work-in-progress as of MLflow version 1.25.1\. In the following
    sections, we will learn when and how to use these different APIs to implement
    explanations and log and load explainers for DL models.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a SHAP explanation using the MLflow artifact logging API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLflow has a generic tracking API that can log any artifact: `mlflow.log_artifact`.
    However, the examples given in the MLflow documentation usually use scikit-learn
    and tabular numerical data for training, testing, and explaining. Here, we want
    to show how to use `mlflow.log_artifact` for an NLP sentimental DL model to log
    relevant artifacts, such as Shapley value arrays and Shapley value bar plots.
    You can check out the Python VS Code notebook, `shap_mlflow_log_artifact.py`,
    in this chapter''s GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py))
    to follow along with the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have the prerequisites, including a local full-fledged MLflow
    server and the conda virtual environment, ready. Follow the instructions in the
    `README.md` ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))
    file in the [*Chapter 10*](B18120_10_ePub.xhtml#_idTextAnchor127) folder to get
    these ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure you activate the `chapter10-dl-explain` virtual environment as follows
    before you start running any code in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the relevant libraries at the beginning of the notebook as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to set up some environment variables. The first three environment
    variables are for the local MLflow URIs, and the fourth is for disabling a Hugging
    Face warning that arises due to a known Hugging Face tokenization issue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also need to set up the MLflow experiment and show the MLflow experiment
    ID as an output on the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you have been running the notebook, you should see an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the MLflow experiment ID for the experiment name `dl_explain_chapter10`
    is `14`. Note that, you could also set the MLflow tracking URI as an environment
    variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use MLflow's `mlflow.set_tracking_uri` API to define the URI location
    instead. Either way is fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create a DL model to classify a sentence into either positive or
    negative sentiment using Hugging Face''s transformer pipeline API. Since this
    is already fine-tuned, we will focus on how to get the explainer and explanation
    for the model, rather than focusing on how to train or finetune a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code snippets create a sentiment analysis model, `dl_model`, and then create
    a SHAP `explainer` for this model. Then we provide a list of two sentences for
    this explainer to get the `shap_values` object. This will be used for logging
    in MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the `shap_values` object, we can now start a new MLflow run and log both
    the Shapley values and the bar plot that we saw in the previous chapter ([*Chapter
    9*](B18120_09_ePub.xhtml#_idTextAnchor112)*, Fundamentals of Deep Learning Explainability*).
    The first line of code makes sure all active MLflow runs are ended. This is useful
    if we want to rerun this block of code multiple times interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define two constants. One, `artifact_root_path`, is for the root path
    in the MLflow artifact store, which will be used to store all the SHAP explanation
    objects. The other, `shap_bar_plot`, is for the artifact filename, which will
    be used for the bar plot figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We then start a new MLflow run, under which we will generate and log three
    SHAP files into the MLflow artifact store under the path `model_explanations_shap`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to have a temporary local directory, as shown in the preceding
    code snippet to first save the SHAP files, and then log those files to the MLflow
    server. If you have run the notebook up to this point, you should see a temporary
    directory in the output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to generate the SHAP files and save them. The first one is
    the bar plot, which is a little bit tricky to save and log. Let''s walk through
    the following code to understand how we do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are using `matplotlib.pyplot`, which was imported as `plt` to first
    clear the figure using `plt.clf()` and then create a subplot with some adjustments.
    Here, we define `bottom=0.2`, which means the position of the bottom edge of the
    subplots is at 20% of the figure height. Similarly, we adjust the left edge of
    the subplot. Then we use the `shap.plots.bar` SHAP API to plot the bar plot for
    the first sentence's feature contribution to the prediction, but with the `show`
    parameter to be `False`. This means, we will not see the plot in the interactive
    run, but the figure is stored in the pyplot `plt` variable, which can then be
    saved using `plt.savefig` to a local temporary directory with the filename prefix
    `shap_bar_plot`. `pyplot` will automatically add the file extension `.png` to
    the file once it is saved. So, this will save a local image file called `shap_bar_plot.png`
    in the temporary folder. The last statement calls MLflow's `mlflow.log_artifact`
    to upload this PNG file to the MLflow tracking server's artifact store in the
    root folder, `model_explanations_shap`. We also need to make sure that we close
    the current figure by calling `plt.close(plt.gcf())`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to logging the `shap_bar_plot.png` to the MLflow server, we also
    want to log the Shapley `base_values` array and `shap_values` array as NumPy arrays
    into the MLflow track server. This can be done through the following statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will first save a local copy of `shap_values.npy` and `base_values.npy`
    in the local temporary folder and then upload it to the MLflow tracking server's
    artifact store.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you followed the notebook up until here, you should be able to verify in
    the local MLflow server whether these artifacts are successfully stored. Go to
    the MLflow UI at the localhost – `http://localhost/` and then find the experiment
    `dl_explain_chapter10`. You should then be able to find the experiment you just
    ran. It should look something like *Figure 10.1*, where you can find three files
    in the `model_explanations_shap` folder: `base_values.npy`, `shap_bar_plot.png`,
    and `shap_values.npy`. *Figure 10.1* shows the bar plot of feature contribution
    of different tokens or words for the prediction result of the sentence – `Not
    a good movie to spend time on`. The URL for this experiment page is something
    like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image'
  prefs: []
  type: TYPE_NORMAL
- en: in the MLflow tracking server
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image in
    the MLflow tracking server
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also use code to programmatically download these files
    stored in the MLflow tracking server and check them locally. We provide such code
    in the last cell of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the last cell block of the notebook code, which is to download the
    three files from the MLflow server we just saved and print them out, you should
    be able to see the following output, as displayed in *Figure 10.2*. The mechanism
    to download artifacts from the MLflow tracking server is to use the `MlflowClient().download_artifacts`
    API, where you provide the MLflow run ID (in our example, it is `10f0655189f740aeb813a015f1f6e115`
    ) and the artifact root path `model_explanations_shap` as the parameters to the
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will download all files in `model_explanations_shap` on the MLflow tracking
    server to a local path, which is the return variable `downloaded_local_path`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Download the SHAP base_values and shap_values array from the
    MLflow tracking server to a local path and display them'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow
    tracking server to a local path and display them
  prefs: []
  type: TYPE_NORMAL
- en: 'To display the two NumPy arrays, we need to call NumPy''s `load` API to load
    them and then print them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to set the `allow_pickle` parameter to `True` when calling
    the `np.load` API so that NumPy can correctly load these files back into memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can run this notebook interactively in the VS Code environment, you
    can also run it in the command line as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will produce all the output in the console and log all the artifacts into
    the MLflow server as we have seen in our interactive running of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: If you have run the code so far, congratulations on the successful completion
    of implementing logging SHAP explanations to the MLflow tracking server using
    MLflow's `mlflow.log_artifact` API!
  prefs: []
  type: TYPE_NORMAL
- en: Although the process of logging all the explanations seems a little bit long,
    this approach does have the advantage of having no dependency on what kind of
    explainer is used since the explainer is defined outside of the MLflow artifact
    logging API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to use the built-in `mlflow.pyfunc.PythonModel`
    API to log a SHAP explainer as an MLflow model and then deploy as an endpoint
    or use it in a batch mode as if it is a generic MLflow `pyfunc` model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a SHAP explainer using the MLflow pyfunc API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know from the previous section, a SHAP explainer can be used offline whenever
    needed by creating a new instance of an explainer using SHAP APIs. However, as
    the underlying DL models are often logged into the MLflow server, it is desirable
    to also log the corresponding explainer into the MLflow server, so that we not
    only keep track of the DL models, but also their explainers. In addition, we can
    use the generic MLflow pyfunc model logging and loading APIs for the explainer,
    thus unifying access to DL models and their explainers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will learn step-by-step how to implement a SHAP explainer
    as a generic MLflow pyfunc model and how to use it for offline and online explanation.
    We will break the process up into three subsections:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and logging an MLflow pyfunc explainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an MLflow pyfunc explainer for an EaaS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an MLflow pyfunc explainer for batching explanation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the first subsection on creating and logging a MLflow pyfunc
    explainer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and logging an MLflow pyfunc explainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to follow this section, please check out `nlp_sentiment_classifier_explainer.py`
    in the GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, by subclassing `mlflow.pyfunc.PythonModel`, we can create a customized
    MLflow model that encapsulates a SHAP explainer. So, let''s declare this class
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to instantiate an explainer. Instead of creating an explainer
    in the `init` method of this class, we will use the `load_context` method to load
    a SHAP explainer for the Hugging Face NLP sentiment analysis classifier, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create a SHAP explainer whenever this `SentimentAnalysisExplainer`
    class is executed. Note that the sentiment classifier is a Hugging Face pipeline
    object, with the `return_all_scores` parameter set to `True`. This means that
    this will return the label and probability score for both positive and negative
    sentiment of each input text.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Runtime Errors for SHAP explainers
  prefs: []
  type: TYPE_NORMAL
- en: If we implement `self.explainer` in the `init` method in this class, we will
    encounter a runtime error related to the SHAP package's `_masked_model.py` file,
    which complains about `init` method will be serialized by MLflow, so it is clear
    that this runtime error comes from MLflow's serialization. However, implementing
    `self.explainer` in the `load_context` function avoids MLflow's serialization,
    and works correctly when invoking this explainer at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then implement the `sentiment_classifier_explanation` method, which
    takes an input of a pandas DataFrame row and produces a pickled `shap_values`
    output as an explanation for a single row of text input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we need to use a pair of square brackets to enclose the `row['text']`
    value so that it becomes a list not just a single value. This is because this
    SHAP explainer expects a list of texts, not just a single string. If we don't
    enclose the value within the square brackets, then the explainer will split the
    entire string character by character, treating each character as if it is a word,
    which is not what we want. Once we get the Shapley values as the output from the
    explainer as `shap_values`, we then need to serialize them using `pickle.dumps`
    before returning to the caller. MLflow pyfunc model input and output signature
    do not support complex object without serialization, so this pickling step makes
    sure that the model output signature is MLflow compliant. We will see the definition
    of this MLflow pyfunc explainer's input and output signature in *step 5* shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to implement the required `predict` method for this class. This
    will apply the `sentiment_classifier_explanation` method to the entire input pandas
    DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce a new column named `shap_values` for each row of the input
    pandas DataFrame in the `text` column. We then drop the `text` column and return
    a single-column `shap_values` DataFrame as the final prediction result: in this
    case, the explanation results as a DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the `SentimentAnalysisExplainer` class implementation, we
    can use the standard MLflow pyfunc model logging API to log this model into the
    MLflow tracking server. Before doing the MLflow logging, let''s make sure we declare
    this explainer''s model signature, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These statements declare that the input is a DataFrame with a single `string`
    type `text` column and the output is a DataFrame with a single `string` type `shap_values`
    column. Recall that this `shap_values` column is a pickled serialized bytes string,
    which contains the Shapley values object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can implement the explainer logging step using the `mlflow.pyfunc.log_model`
    method in a task method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are four parameters in the `log_model` method that we use. The `MODEL_ARTIFACT_PATH`
    is the name of the folder in the MLflow tracking server where the explainer will
    be stored. Here, the value is defined as `nlp_sentiment_classifier_explainer`
    in the Python file you checked out. `CONDA_ENV` is the `conda.yaml` file in this
    chapter's root folder. The `python_model` parameter is the `SentimentAnalysisExplainer`
    class we just implemented, and `signature` is the explainer input and output signature
    we defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to run this whole file as follows in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assuming you have the local MLflow tracking server and environment variables
    set up correctly by following the `README.md` file for this chapter in the GitHub
    repository, this will produce the following two lines in the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This means we have successfully logged the explainer in our local MLflow tracking
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the MLflow web UI at `http://localhost/` in the web browser and click
    the `dl_explain_chapter10` experiment folder. You should be able to find this
    run and the logged explainer in the `Artifacts` folder under `nlp_sentiment_classifier_explainer`,
    which should look as shown in *Figure 10.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `MLmodel` metadata shown in *Figure 10.3* does not differ much
    from the normal DL inference pipeline that we logged before as an MLflow pyfunc
    model except for the `artifact_path` name and the `signature`. That's the advantage
    of using this approach because now we can use the generic MLflow pyfunc model
    methods to load this explainer or deploy it as a service.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the mlflow.shap.log_explainer API
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, MLflow has a `mlflow.shap.log_explainer` API that
    provides a method to log an explainer. However, this API does not support our
    NLP sentiment classifier explainer because our NLP pipeline is not a known model
    flavor that MLflow currently supports. Thus even though `log_explainer` can write
    this explainer object into the tracking server, when loading the explainer back
    into memory using the `mlflow.shap.load_explainer` API, it will fail with the
    following error message: `mlflow.shap.log_explainer` API in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a logged explainer, we can use it in two ways: deploy it into
    a web service so that we can create an endpoint to establish an EaaS, or load
    the explainer directly through MLflow pyfunc `load_model` or `spark_udf` method
    using the MLflow `run_id`. Let''s start with the web service deployment by setting
    up a local web service.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an MLflow pyfunc explainer for an EaaS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can set up a local EaaS in a standard MLflow way since now the SHAP explainer
    is just like a generic MLflow pyfunc model. Perform the following steps to see
    how this can be implemented locally:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following MLflow command to set up a local web service for the explainer
    we just logged. The `run_id` in this example is `ad1edb09e5ea4d8ca0332b8bc2f5f6c9`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 10.4 – SHAP EaaS console output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – SHAP EaaS console output
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that in *Figure 10.4*, the default underlying pretrained language model
    is loaded after the `gunicore` HTTP server is up and running. This is because
    our implementation of the explainer was inside the `load_context` method, which
    is exactly what is to be expected: loading the explainer immediately after the
    web service is up and running.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a different terminal window, type the following command to invoke the explainer
    web service at port `5000` of localhost with two sample texts as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS
  prefs: []
  type: TYPE_NORMAL
- en: Note that in *Figure 10.5*, the column name is `shap_values`, while the values
    are pickled bytes hexadecimal data. These are not human readable, but can be converted
    back to the original `shap_values` using `pickle.loads` method at the caller side.
    So, if you see a response output like *Figure 10.5*, congratulations for setting
    up a local EaaS! You can deploy this explainer service just like other MLflow
    service deployments, as described in [*Chapter 8*](B18120_08_ePub.xhtml#_idTextAnchor095)*,
    Deploying a DL Inference Pipeline at Scale*, since this explainer now can be called
    just like a generic MLflow pyfunc model service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how to use the MLflow pyfunc explainer for batch explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Using an MLflow pyfunc explainer for batch explanation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two ways to implement offline batch explanation using an MLflow pyfunc
    explainer:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the pyfunc explainer as an MLflow pyfunc model to explain a given pandas
    DataFrame input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the pyfunc explainer as a PySpark UDF to explain a given PySpark DataFrame
    input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with loading the explainer as an MLflow pyfunc model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the MLflow pyfunc explainer as an MLflow pyfunc model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we have already mentioned, another way to consume an MLflow logged explainer
    is to load the explainer in a local Python code using MLflow''s pyfunc `load_model`
    method directly, instead of deploying it into a web service. This is very straightforward,
    and we will show you how it can be done. You can check out the code in the `shap_mlflow_pyfunc_explainer.py`
    file in the GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the logged explainer back into memory. The following
    code does this using `mlflow.pyfunc.load_model` and the explainer `run_id` URI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should load the explainer as if it is just a generic MLflow pyfunc model.
    We can print out the metadata of the explainer by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This means this is a `mlflow.pyfunc.model` flavor, which is great news, since
    we can use the same MLflow pyfunc API to use this explainer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will get some example data to test the newly loaded explainer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will load the IMDb test dataset, truncate each review text to 500 characters,
    and pick the first 20 rows to make a pandas DataFrame for explanation in the next
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the explainer as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will run the SHAP partition explainer for the input DataFrame `df_test`.
    It will show the following output for each row of the DataFrame when it is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The result will be a pandas DataFrame with a single column, `shap_values`. This
    may take a few minutes as it needs to tokenize each row, execute the explainer,
    and serialize the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the explainer execution is done, we can check the results by deserializing
    the row content. Here is the code to check the first output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print out the first row''s `shap_values`. *Figure 10.6* shows a partial
    screenshot of the output of `shap_values`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Partial output of the deserialized shap_values from the explanation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Partial output of the deserialized shap_values from the explanation
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 10.6*, the output of `shap_values` is no different
    from what we learned in [*Chapter 9*](B18120_09_ePub.xhtml#_idTextAnchor112)*,
    Fundamentals of Deep Learning Explainability*, when we did not use MLflow to log
    and load the explainer. We can also generate Shapley text plots to highlight the
    contribution of the texts to the predicted sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following statement in the notebook to see the Shapely text plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate a plot displayed in *Figure 10.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow
    logged explainer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow
    logged explainer
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 10.7*, this review has a positive sentiment and the
    keywords or phrases that contribute to the predicted sentiment are `good`, `love`,
    and some other phrases highlighted in red. When you see this Shapley text plot,
    you should give yourself a round of applause, as you have finished learning how
    to use an MLflow logged explainer to generate batch explanation.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned during the step-by-step implementation of this batch explanation,
    it is a little slow to do a large batch explanation using this pyfunc model approach.
    Luckily, we have another way to implement the batch explanation using the PySpark
    UDF function, which we will explain in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the pyfunc explainer as a PySpark UDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For scalable batch explanation, we can use Spark''s distributed computing capability,
    which is supported by loading the pyfunc explainer as a PySpark UDF. There is
    no extra work to use this capability, since this is provided by the MLflow pyfunc
    API already through the `mlflow.pyfunc.spark_udf` method. We will show you how
    to implement this at-scale explanation step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: First, make sure you have worked through the `README.md` file ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))
    to install Spark, create and activate the `chapter10-dl-pyspark-explain` virtual
    environment, and set up all the environment variables before you run the PySpark
    UDF code to do the explanation at scale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then you can start running the VS Code notebook, `shap_mlflow_pyspark_explainer.py`,
    which you can check out in the GitHub repository: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py).
    Run the following command at `chapter10/notebooks/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the final output displayed in *Figure 10.8*, among quite a few
    lines of output preceding these final few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – PySpark UDF explainer''s output of the first two rows of text''s
    shap_values along with their input texts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_10_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's
    shap_values along with their input texts
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen in *Figure 10.8*, the PySpark UDF explainer''s output is a PySpark
    DataFrame that has two columns: `text` and `shap_values`. The `text` column is
    the original input text, while the `shap_values` column contains the pickled serialized
    Shapley values, just like we saw in the previous subsection when we used the pyfunc
    explainer for the pandas DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see what's happening in the code. We will explain the key code blocks
    in the `shap_mlflow_pyspark_explainer.py` file. Since this is a VS Code notebook,
    you can run it either in the command line as we just did or interactively inside
    the VS Code IDE window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first key code block is to load the explainer using the `mflow.pyfunc.spark_udf`
    method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first statement is to initialize a `SparkSession` variable and then use
    `run_id` to load the logged explainer into memory. Run the explainer to get the
    metadata as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This means we now have a SHAP explainer wrapped as a Spark UDF function. This
    allows us to directly apply the SHAP explainer for an input PySpark DataFrame
    in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the IMDb test dataset as before to get a list of `short_data`, and
    then create a PySpark DataFrame for the top 20 rows of the test dataset for explanation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note the last statement, which uses PySpark's `withColumn` function to add a
    new `shap_values` column to the input DataFrame, `spark_df`, which originally
    contained only one column, `text`. This is a natural way to use Spark's parallel
    and distributed computing capability. If you have run both the previous non-Spark
    approach using the MLflow pyfunc `load_model` method and the current PySpark UDF
    one, you will notice that the Spark approach runs much faster, even on a local
    computer. This allows us to do SHAP explanation at scale for many instances of
    input texts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to verify the results, we show the `spark_df` DataFrame's top two rows,
    which was illustrated in *Figure 10.8*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By now, with MLflow's pyfunc Spark UDF wrapped SHAP explainer, we can confidently
    do large-scale batch explanation. Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: Let's now summarize what we have learned in this chapter in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we first reviewed the existing approaches in the MLflow APIs
    that could be used for implementing explainability. Two existing MLflow APIs,
    `mlflow.shap` and `mlflow.evaluate`, have limitations, thus cannot be used for
    the complex DL models and pipelines explainability scenarios we need. We then
    focused on two main approaches to implement SHAP explanations and explainers within
    the MLflow API framework: `mlflow.log_artifact` for logging explanations and `mlflow.pyfunc.PythonModel`
    for logging a SHAP explainer. Using the `log_artifact` API can allow us to log
    Shapley values and explanation plots into the MLflow tracking server. Using `mlflow.pyfunc.PythonModel`
    allows us to log a SHAP explainer as a MLflow pyfunc model, thus opening doors
    to deploy a SHAP explainer as a web service to create an EaaS endpoint. It also
    opens doors to use SHAP explainers through the MLflow pyfunc `load_model` or `spark_udf`
    API for large-scale offline batch explanation. This enables us to confidently
    implement explainability at scale for DL models.'
  prefs: []
  type: TYPE_NORMAL
- en: As the field of explainability continues to evolve, MLflow's integration with
    SHAP and other explainability toolboxes will also continue to improve. Interested
    readers are encouraged to continue their learning journey through the links provided
    in the further reading section. Happy continuous learning and growing!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shapley Values at Scale: [https://neowaylabs.github.io/data-science/shapley-values-at-scale/](https://neowaylabs.github.io/data-science/shapley-values-at-scale/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling SHAP Calculations With PySpark and Pandas UDF: [https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html](https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speeding up Shapley value computation using Ray, a distributed computing system:
    [https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/](https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpreting an NLP model with LIME and SHAP: [https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4](mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Evaluation in MLflow: [https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html](https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
