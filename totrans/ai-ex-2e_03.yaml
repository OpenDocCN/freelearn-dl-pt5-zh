- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Intelligence – Evaluation Functions and Numerical Convergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two issues appear when a reward matrix (*R*)-driven MDP produces results. These issues
    can be summed up in two principles.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principle 1**: AI algorithms often surpass humans in classification, prediction,
    and decision-making areas.'
  prefs: []
  type: TYPE_NORMAL
- en: The key executive function of human intelligence, decision-making, relies on
    the ability to evaluate a situation. No decision can be made without measuring
    the pros and cons and factoring the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Humanity takes great pride in its ability to evaluate. However, in many cases,
    a machine can do better. Chess represents our pride in our thinking ability. A chessboard
    is often present in movies to symbolize human intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Today, not a single chess player can beat the best chess engines. One of the
    extraordinary core capabilities of a chess engine is the evaluation function;
    it takes many parameters into account more precisely than humans.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principle 2**: Principle 1 leads to a very tough consequence. It is sometimes
    possible and other times impossible for a human to verify the results that an
    AI algorithm produces, let alone ensemble meta-algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Principle 1 has been difficult to detect because of the media hype surrounding
    face and object recognition. It is easy for a human to check whether the face
    or object the ML algorithm was supposed to classify was correctly classified.
  prefs: []
  type: TYPE_NORMAL
- en: However, in a decision-making process involving many features, principle 2 rapidly
    appears. In this chapter, we will identify what results and convergence to measure
    and decide how to measure it. We will also explore measurement and evaluation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of the episodes of a learning session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical convergence measurements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to numerical gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree supervised learning as an evaluation method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thing is to set evaluation goals. To do this, we will decide what
    to measure and how.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking down what to measure and deciding how to measure it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now tackle the tough task of finding the factors that can make a system
    go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model built in the previous chapters can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: From *l*[v], the availability vector (capacity in a warehouse, for example),
    to *R*, the process creates the reward matrix from the raw data (*Chapter 2*,
    *Building a Reward Matrix – Designing Your Datasets*) required for the MDP reinforcement
    learning program (*Chapter 1*, *Getting Started with Next-Generation Artificial
    Intelligence through Reinforcement Learning*). As described in the previous chapter,
    a softmax(*l*[v]) function is applied to *l*[v]. In turn, a one-hot(softmax(*l*[v]))
    is applied, which is then converted into the reward value *R*, which will be used
    for the *Q* (Q-learning) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The MDP-driven Bellman equation then runs from reading *R* (the reward matrix)
    to the results. Gamma is the learning parameter, *Q* is the Q-learning function,
    and the results represent the final value of the states of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters to be measured are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The company's input data. Ready-to-use datasets such as MNIST are designed to
    be efficient for an exploration phase. These ready-made datasets often contain
    some noise (unreliable data) to make them realistic. The same process must be
    achieved with raw company data. The only problem is that you cannot download a
    corporate dataset from somewhere. You have to build time-consuming datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights and biases that will be applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function (a logistic function or other).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choices to make after the one-hot process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Episode management through convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A verification process through interactive random checks and independent algorithms
    such as supervised learning to control unsupervised algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real-life company projects, a system will not be approved until tens of thousands
    of results have been produced. In some cases, a corporation will approve the system
    only after hundreds of datasets with millions of data samples have been tested
    to be sure that all scenarios are accurate. Each dataset represents a scenario
    that consultants can work on with parameter scripts. The consultant introduces
    parameter scenarios that are tested by the system and measured. In decision-making
    systems with up to 200 parameters, a consultant will remain necessary for months
    in an industrial environment. A reinforcement learning program will be on its
    own to calculate events. However, even then, consultants are needed to manage
    the hyperparameters. In real-life systems, with high financial stakes, quality
    control will always remain essential.
  prefs: []
  type: TYPE_NORMAL
- en: Measurement should thus apply to generalization more than simply applying to
    a single or few datasets. Otherwise, you will have a natural tendency to control
    the parameters and overfit your model in a too-good-to-be-true scenario.
  prefs: []
  type: TYPE_NORMAL
- en: For example, say you wake up one morning and look at the sky. The weather is
    clear, the sun is shining, and there are no clouds. The next day, you wake up
    and you see the same weather. You write this down in a dataset and send it off
    to a customer for weather prediction. Every time the customer runs the program,
    it predicts clear sunny skies! That what overfitting leads to! This explains why
    we need large datasets to fully understand how to use an algorithm or illustrate
    how a machine learning program works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the reward matrix, the reinforcement program in the first chapter had
    a learning parameter ![](img/B15438_03_003.png), shown in `mdp03.py`, which is
    used for this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The ![](img/B15438_03_004.png) learning parameter in itself needs to be closely
    monitored because it introduces uncertainty into the system. This means that the
    learning process will always remain a probability, never a certainty. One might
    wonder why this parameter is not just taken out. Paradoxically, that will lead
    to even more global uncertainty. The more the ![](img/B15438_03_004.png) learning
    parameter tends to 1, the more you risk overfitting your results. Overfitting
    means that you are pushing the system to think it's learning well when it isn't.
    It's exactly like a teacher who gives high grades to everyone in the class all
    the time. The teacher would be overfitting the grade-student evaluation process,
    and nobody would know whether the students have learned anything.
  prefs: []
  type: TYPE_NORMAL
- en: The results of the reinforcement program need to be measured as they go through
    episodes. The range of the learning process itself must be measured.
  prefs: []
  type: TYPE_NORMAL
- en: All of these measurements will have a significant effect on the results obtained.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to start is by measuring the quality of convergence of the system.
  prefs: []
  type: TYPE_NORMAL
- en: If the system provides good convergence, you might avoid the headache of having
    to go back and check everything.
  prefs: []
  type: TYPE_NORMAL
- en: Convergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convergence measures the distance between the present state of a training session
    and the goal of the training session. In a reinforcement learning program, an
    MDP, for example, there is no training data, so there is no target data to compare
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, two methods are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implicit convergence**: In this case, we run the training for a large number
    of episodes, 50,000, for example. We know through trial and error that the program
    will reach a solution by then.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Numerically controlled gradient descent**: We measure the training progress
    at each episode and stop when it is safe to do so.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implicit convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last part of `mdp01.py` in the first chapter, a range of 50,000 was implemented.
    In this chapter, we will run `mdp03.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last part of `mdp01.py`, the idea was to set the number of episodes
    at such a level that meant that convergence was certain. In the following code,
    the range (`50000`) is a constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Convergence, in this case, will be defined as the point at which no matter how
    long you run the system, the `Q` result matrix will not change anymore.
  prefs: []
  type: TYPE_NORMAL
- en: By setting the range to `50000`, you can test and verify this. As long as the
    reward matrices remain homogeneous, this will work. If the reward matrices strongly
    vary from one scenario to another, this model will produce unstable results.
  prefs: []
  type: TYPE_NORMAL
- en: Try to run the program with different ranges. Lower the ranges until you see
    that the results are not optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Numerically controlled gradient descent convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will use `mdp03.py`, a modified version of `mdp01.py` explored
    in *Chapter 1*, with an additional function: numerically controlled gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Letting the MDP train for 50,000 will produce a good result but consume unnecessary
    CPU. Using a numerically controlled gradient descent evaluation function will
    save a lot of episodes. Let's see how many.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to define the gradient descent function based on a derivative.
    Let's have a quick review of what a derivative is.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: '*h* is the value of the step of the function. Imagine that *h* represents each
    line of a bank account statement. If we read the statement line by line, *h* =
    1\. If we read two lines at a time, *h* = 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the present line of the bank account statement = *f*(*x*) = *a certain
    amount*.
  prefs: []
  type: TYPE_NORMAL
- en: When you read the next line of the bank account, the function is (*f* + *h*)
    = *the amount after f*(*x*). If you had 100 units of currency in your bank account
    at *f*(*x*) and spent 10 units of currency, on the next line, *x* + *h*, you would
    have *f*(*x* + *h*) = *90 units of currency* left.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient provides the direction of your slope: up, down, or constant. In
    this case, we can say that the slope, the **gradient**, is doing downward, as
    shown in the following graph, which illustrates the decreasing values of *y*(cost,
    loss) as *x* increases (training episodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Plotting the decreasing cost/loss values as training episodes increase'
  prefs: []
  type: TYPE_NORMAL
- en: We also need to know by how much your bank account is changing – how much the
    **derivative** is worth. In this case, derivative means by how much the balance
    of your bank account is changing on each line of your bank statement. In this
    case, you spent 10 units of currency in one bank statement line, so the derivative
    at this value of *x* (line in your bank account) = –10.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code of the Bellman equation as seen in *Chapter 1*, *Getting
    Started with Next-Generation Artifcial Intelligence through Reinforcement Learning*,
    the step of the loop is 1 as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since *i* = 1, *h* = 1 in our gradient descent calculation can be simplified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now define *f*(*x*) in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`conv` is the sum of the 6×6 `Q` matrix that is slowly filling up as the MDP
    training progresses. Thus *f*(*x*) = `conv=Q.sum()` = sum of `Q`. The function
    adds up all the values in `Q` to have a precise value of the state of the system
    at each *i*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = the state of the system at *i* – 1'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x* + 1) is the value of the system at *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We must remember that the `Q` matrix is increasing progressively as the MDP
    process continues to train. We measure the distance between two steps, *h*. This
    distance will decrease. Now we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x* + 1) – *f*(*x*) = `-Q.sum()+conv`'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we implement additional variables for our evaluation function, which
    uses gradient descent at line 83 of `mdp01.py`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`nc=1` activates the evaluation function, and `ci` begins to count the episodes
    it will take with this function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the first episode, `i==1`, *f*(*x*)= `Q.sum()` as planned:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*f*(*x* + 1) = `-Q.sum()+conv` is applied:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The distance, the absolute value of the derivative, is displayed and stored
    because we will be using it to plot a figure with Matplotlib:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`xi=100` plays a critical role in this numerically controlled gradient descent
    function. Every `xi`, the process stops to check the status of the training process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are two possible cases: **a)** and **b)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case a)** As long as the local derivative is >0 at each episode, the MDP
    continues its training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display varying local derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Case b)** When the derivative value reaches a constant value for `xi` episodes,
    the MDP has been trained and the training can now stop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display a constant derivate, `xi`, before the training stops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When the training is over, the number of training episodes is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 2,099 is a lot less than the 50,000 implicit convergence episodes, which proves
    the efficiency of this numerically controlled gradient descent method.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the learning process, you can display a Matplotlib figure containing
    the convergence level of each episode that we had stored in `cq=ql.zeros((2500,
    1))`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The figure is displayed with a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Une image contenant capture d’écran  Description générée automatiquement](img/B15438_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: A plot demonstrating numerical convergence'
  prefs: []
  type: TYPE_NORMAL
- en: This graph shows the numerical convergence. As you can see in the graph, the
    cost or loss decreases as the number of training episodes increases, as explained
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note the following properties of this gradient descent method:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of episodes will vary from one training session to another because
    the MDP is a random process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training curve at local episodes is sometimes erratic because of the random
    nature of the training process. Sometimes, the curve will go up instead of down
    locally. In the end, it will reach 0 and stay there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the training curve increases locally, there is nothing you can do. An MDP
    does no backpropagation to modify weights, parameters, or strategies, as we will
    see when we look at artificial neural networks (ANNs), for example, in *Chapter
    8*, *Solving the XOR Problem with a Feedforward Neural Network*. No action is
    required in an MDP process. You can try to change the learning rate or go back
    and check your reward matrix and the preprocessing phase implemented on the raw
    datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the training curve does not reach 0 and stay there, check the learning parameters,
    the reward matrix, and the preprocessing phase implemented on the raw datasets.
    You might even have to go back and check the noise (defective data or missing
    data) in the initial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the MDP training is over, do some random tests using the functionality
    provided at line 145 and explained in *Chapter 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, when prompted for an input, enter `1` and see if the result is
    correct, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This random test verification method will work efficiently with a relatively
    small reward matrix.
  prefs: []
  type: TYPE_NORMAL
- en: However, this approach will prove difficult with a size 25×25 reward matrix,
    for example. The machine easily provides a result. But how can we evaluate it?
    In that case, we have reached the limit of human analytic capacity. In the preceding
    code, we entered a starting point and obtained an answer. With a small reward
    matrix, it is easy to visually check and see if the answer is correct. When analyzing
    25 × 25 = 625 cells, it would take days to verify the results. For the record,
    bear in mind that when Andrey Markov invented his approach over 100 years ago,
    he used a pen and paper! However, we have computers, so we must use an evaluation
    algorithm to evaluate the results of our MDP process.
  prefs: []
  type: TYPE_NORMAL
- en: The increasing volumes of data and parameters in a global world have made it
    impossible for humans to outperform the ever-growing intelligence of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating beyond human analytic capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An efficient manager has a high evaluation quotient. A machine often has a better
    one in an increasing number of fields. The problem for a human is to understand
    the evaluation machine intelligence has produced.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes a human will say "that's a good machine thinking result" or "that's
    a bad result," without being able to explain why or determine whether there is
    a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation is one of the major keys to efficient decision-making in all fields:
    from chess, production management, rocket launching, and self-driving cars to
    data center calibration, software development, and airport schedules.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore a chess scenario to illustrate the limits of human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Chess engines are not high-level deep learning-based software. They rely heavily
    on evaluations and calculations. They evaluate much better than humans, and there
    is a lot to learn from them. The question now is to know whether any human can
    beat a chess engine or not. The answer is no.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate a position in chess, you need to examine all the pieces, their quantitative
    value, their qualitative value, the cooperation between pieces, who owns each
    of the 64 squares, the king's safety, the bishop pairs, the knight positioning,
    and many other factors.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a position in a chess game shows why machines are surpassing humans
    in quite a few decision-making fields.
  prefs: []
  type: TYPE_NORMAL
- en: The following scenario is after move 23 in the Kramnik-Bluebaum 2017 game. It cannot
    be correctly evaluated by humans. It contains too many parameters to analyze and
    too many possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_03_4-1-298x300.png](img/B15438_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Chess example scenario'
  prefs: []
  type: TYPE_NORMAL
- en: It is white's turn to play, and a close analysis shows that both players are
    lost at this point. In a tournament like this, they must each continue to keep
    a poker face. They often look at their position with a confident face to hide
    their dismay. Some even shorten their thinking time to make their opponent think
    they know where they are going.
  prefs: []
  type: TYPE_NORMAL
- en: These unsolvable positions for humans are painless to solve with chess engines,
    even for cheap, high-quality chess engines on a smartphone. This can be generalized
    to all human activity that has become increasingly complex, unpredictable, and
    chaotic. Decision-makers will increasingly rely on AI to help them make the right
    choices.
  prefs: []
  type: TYPE_NORMAL
- en: No human can play chess and evaluate the way a chess engine does by simply calculating
    the positions of the pieces, their squares of liberty, and many other parameters.
    A chess engine generates an evaluation matrix with millions of calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The following table is the result of an evaluation of only one position among
    many others (real and potential).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Position evaluated** | **0,3** |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **White** | **34** |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Initial position** | **Position** | **Value** |  | **Quality Value**
    | **Total Value** |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | a2 | a2 | 1 | a2-b2 small pawn island | 0,05 | 1,05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | b2 | b2 | 1 | a2-b2 small pawn island | 0,05 | 1,05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | c2 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | d2 | d4 | 1 | Occupies center, defends Be5 | 0,25 | 1,25 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | e2 | e2 | 1 | Defends Qf3 | 0,25 | 1,25 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | f2 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | g2 | g5 | 1 | Unattacked, attacking 2 squares | 0,3 | 1,3 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | h2 | h3 | 1 | Unattacked, defending g4 | 0,1 | 1,1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rook** | a1 | c1 | 5 | Occupying c-file, attacking b7 with Nd5-Be5 | 1
    | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **Knight** | b1 | d5 | 3 | Attacking Nb6, 8 squares | 0,5 | 3,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **BishopDS** | c1 | e5 | 3 | Central position, 10 squares, attacking c7 |
    0,5 | 3,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **Queen** | d1 | f3 | 9 | Battery with Bg2, defending Ne5, X-Ray b7 | 1 |
    11 |'
  prefs: []
  type: TYPE_TB
- en: '| **King** | e1 | h1 | 0 | X-rayed by Bb6 on a7-g1 diagonal | -0,5 | -0,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **BishopWS** | f1 | g2 | 3 | Supporting Qf3 in defense and attack | 0,5 |
    3,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **Knight** | g1 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rook** | h1 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | 29 |  | 5 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  | White: 34 |'
  prefs: []
  type: TYPE_TB
- en: The value of the position of white is 34.
  prefs: []
  type: TYPE_NORMAL
- en: '| **White** | **34** |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Black** | **33,7** |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Initial position** | **Position** | **Value** |  | **Quality Value**
    | **Total Value** |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | a7 | a7 | 1 | a7-b7 small pawn island | 0,05 | 1,05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | b7 | b7 | 1 | a7-b7 small pawn island | 0,05 | 1,05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | c7 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | d7 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | e7 | f5 | 1 | Doubled, 2 squares | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | f7 | f7 | 1 |  | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | g7 | g6 | 1 | Defending f5 but abandoning Kg8 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Pawn** | h7 | h5 | 1 | Well advanced with f5,g6 | 0,1 | 1,1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rook** | a8 | d8 | 5 | Semi-open d-file attacking Nd5 | 2 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| **Knight** | b8 | x | 0 | Captured | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BishopDS** | c8 | b6 | 3 | Attacking d4, 3 squares | 0,5 | 3,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **Queen** | d8 | e6 | 9 | Attacking d4,e5, a bit cramped | 1,5 | 10,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **King** | e8 | g8 | 0 | f6,h6, g7,h8 attacked | -1 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| **BishopWS** | f8 | x | 0 | Captured, white lost bishop pair | 0,5 | 0,5
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Knight** | g8 | e8 | 3 | Defending c7,f6,g7 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rook** | h8 | f8 | 5 | Out of play | -2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | 31 |  | 2,7 | Black: 33,7 |'
  prefs: []
  type: TYPE_TB
- en: The value of black is 33.7.
  prefs: []
  type: TYPE_NORMAL
- en: So white is winning by 34 – 33.7 = 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation system can easily be represented with two McCulloch-Pitts neurons,
    one for black and one for white. Each neuron would have 30 weights = {*w*[1],*w*[2]
    … *w*[30]}, as shown in the previous table. The sum of both neurons requires an
    activation function that converts the evaluation into 1/100th of a pawn, which
    is the standard measurement unit in chess. Each weight will be the output of squares
    and piece calculations. Then the MDP can be applied to Bellman's equation with
    a random generator of possible positions.
  prefs: []
  type: TYPE_NORMAL
- en: Present-day chess engines contain this type of brute calculation approach. They
    don't need more to beat humans.
  prefs: []
  type: TYPE_NORMAL
- en: No human, not even world champions, can calculate these positions with this
    accuracy. The number of parameters to take into account overwhelms them each time
    they reach positions like these. They then play more or less randomly with a possibly
    good idea in mind. The chances of success against a chess engine resemble a lottery
    sometimes. Chess experts discover this when they run human-played games with powerful
    chess engines to see how the game plays out. The players themselves now tend to
    reveal their incapacity to provide a deep analysis when asked why they made a
    questionable move. It often takes hours to go through a game, its combinations
    and find the reasons of a bad move. In the end, the players will often use a machine
    to help them understand what happened.
  prefs: []
  type: TYPE_NORMAL
- en: The positions analyzed here represent only one possibility. A chess engine will
    test millions of possibilities. Humans can test only a few.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring a result like this has nothing to do with natural human thinking.
    Only machines can think like that. Not only do chess engines solve the problem,
    but they are also impossible to beat.
  prefs: []
  type: TYPE_NORMAL
- en: '*Principle 1: At one point, there are problems humans face that only machines
    can solve.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Principle 2: Sometimes, it will be possible to verify the result of an ML
    system, sometimes not. However, we must try to find ways to check the result.*'
  prefs: []
  type: TYPE_NORMAL
- en: One solution to solve the problem of principle 2 is to verify an unsupervised
    algorithm with a supervised algorithm through random samples.
  prefs: []
  type: TYPE_NORMAL
- en: Using supervised learning to evaluate a result that surpasses human analytic
    capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, an AI solution exceeds a human's capacity to analyze a
    situation in detail. It is often too difficult for a human to understand the millions
    of calculations a machine made to reach a conclusion and explain it. To solve
    that problem, another AI, ML, or DL algorithm will provide assisted AI capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The raw data preprocessed by the neural approach of *Chapter 2*, *Building a
    Reward Matrix – Designing Your Datasets*, works fine. The reward matrix looks fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MDP-driven Bellman equation provides good reinforcement training results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convergence function and values work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results on this dataset look satisfactory but the results are questioned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A manager or user will always come up with a killer question: how can you prove
    that this will work with other datasets in the future and confirm 100% that the
    results are reliable?'
  prefs: []
  type: TYPE_NORMAL
- en: The only way to be sure that this whole system works is to run thousands of
    datasets with hundreds of thousands of product flows.
  prefs: []
  type: TYPE_NORMAL
- en: The idea now is to use supervised learning to create an independent way of checking
    the results. One method is to use a decision tree to visualize some key aspects
    of the solution and be able to reassure the users and yourself that the system
    is reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees provide a white box approach with powerful functionality. In
    this section, we will limit the exploration to an intuitive approach. In *Chapter
    5*, *How to Use Decision Trees to Enhance K-Means Clustering*, we will go into
    the theory of decision trees and random trees and explore more complex examples.
  prefs: []
  type: TYPE_NORMAL
- en: In this model, the features of the input are analyzed so that we can classify
    them. The analysis can be transformed into decision trees depending on real-time
    data, to create a distribution representation to predict future outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, you can run the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Decision_Tree_Priority_classifier.py`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or the following Jupyter notebook on Google Colaboratory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DTCH03.ipynb`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Colaboratory might have the two following packages installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This could help you avoid installing them locally, which might take some time
    if you get a Graphviz requirement message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both programs produce the same decision tree image:'
  prefs: []
  type: TYPE_NORMAL
- en: '`warehouse_example_decision_tree.png`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuitive description of this decision tree approach runs in 5 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Represent the features of the incoming orders to store in a warehouse
    – for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we will limit the model to three properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Priority/location, which is the most important property in a warehouse flow
    in this model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volumes to transport
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing priority – the financial and customer satisfaction property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: Provide priority labels for the learning dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3**: Providing the dataset input matrix, which is the output matrix
    of the reinforcement learning program. The values have been approximated but are
    enough to run the model. They simulate some of the intermediate decisions and
    transformations that occur during the decision process (ratios applied, uncertainty
    factors added, and other parameters). The input matrix is `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The features in step 1 apply to each column.
  prefs: []
  type: TYPE_NORMAL
- en: The values in step 2 apply to every line.
  prefs: []
  type: TYPE_NORMAL
- en: The values of the third column [0,1] are discrete indicators for the training
    session.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: Run a standard decision tree classifier. This classifier will distribute
    the representations (distributed representations) into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: The properties of high-priority orders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The properties of low-priority orders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many types of algorithms. In this case, a standard `sklearn` function
    is called to do the job, as shown in the following source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5**: Visualization separates the orders into priority groups. Visualizing
    the tree is optional but provides a trendy white box approach. You will have to
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import collections`, a Python container library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import pydotplus`, a Python interface to Graphviz''s dot language. You can
    choose to use Graphviz directly with other variations of this source code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The source code will take the nodes and edges of the decision tree, draw them,
    and save the image in a file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The file will contain this intuitive decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant signe, texte  Description générée automatiquement](img/B15438_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: A decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image produces the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree represented as a graph that has nodes (the boxes) and edges
    (the lines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When *gini*=0, this box is a **leaf**; the tree will grow no further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*gini* means **Gini impurity**. At an intuitive level, Gini impurity will focus
    on the highest values of Gini impurity to classify the samples. We will go into
    the theory of Gini impurity in *Chapter 5*, *How to Use Decision Trees to Enhance
    K-Means Clustering*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*samples* = 6\. There are six samples in the training dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Priority/location <=360.0 is the largest division point that can be visualized:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: The false arrow points out the two values that are not <=360\. The ones that
    are classified as `True` are considered as low-priority values.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After a few runs, the user will get used to visualizing the decision process
    as a white box and trust the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each ML tool suits a special need in a specific situation. In the next chapter,
    *Optimizing Your Solutions with K-Means Clustering*, we will explore another machine
    learning algorithm: *k-means clustering*.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter drew a distinction between machine intelligence and human intelligence.
    Solving a problem like a machine means using a chain of mathematical functions
    and properties. Machine intelligence surpasses humans in many fields.
  prefs: []
  type: TYPE_NORMAL
- en: The further you get in machine learning and deep learning, the more you will
    find mathematical functions that solve the core problems. Contrary to the astounding
    amount of hype, mathematics relying on CPUs is replacing humans, not some form
    of mysterious conscious intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: The power of machine learning reaches beyond human *mathematical reasoning*.
    It makes ML generalization to other fields easier. A mathematical model, without
    the complexity of humans entangled in emotions, makes it easier to deploy the
    same model in many fields. The models of the first three chapters of this book
    can be used for self-driving vehicles, drones, robots in a warehouse, scheduling
    priorities, and much more. Try to imagine as many fields you can apply these to
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and measurement are at the core of machine learning and deep learning.
    The key factor is constantly monitoring convergence between the results the system
    produces and the goal it must attain. The door is open to the constant adaptation
    of the parameters of algorithms to reach their objectives.
  prefs: []
  type: TYPE_NORMAL
- en: When a human is surpassed by an unsupervised reinforcement learning algorithm,
    a decision tree, for example, can provide invaluable assistance to human intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter, *Optimizing Your Solutions with K-Means Clustering*, goes
    a step further into machine intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can a human beat a chess engine? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Humans can estimate decisions better than machines with intuition when it comes
    to large volumes of data. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a reinforcement learning program with a Q function is a feat in itself.
    Using the results afterward is useless. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supervised learning decision tree functions can be used to verify that the result
    of the unsupervised learning process will produce reliable, predictable results.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results of a reinforcement learning program can be used as input to a scheduling
    system by providing priorities. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can artificial intelligence software think like humans? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more on decision trees: [https://youtu.be/NsUqRe-9tb4](https://youtu.be/NsUqRe-9tb4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more on chess analysis by experts such as Zoran Petronijevic, with whom
    I discussed this chapter: [https://chessbookreviews.wordpress.com/tag/zoran-petronijevic/](https://chessbookreviews.wordpress.com/tag/zoran-petronijevic/),
    [https://www.chess.com/fr/member/zoranp](https://www.chess.com/fr/member/zoranp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more on AI chess programs: [https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
