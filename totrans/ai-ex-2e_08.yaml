- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Solving the XOR Problem with a Feedforward Neural Network
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用前馈神经网络解决 XOR 问题
- en: In the course of a corporate project, there always comes the point when a problem that
    seems impossible to solve hits you. At that point, you try everything you've learned,
    but it doesn't work for what's asked of you. Your team or customer begins to look
    elsewhere. It's time to react.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业项目的过程中，总会有某一时刻，遇到一个看似无法解决的问题。此时，你会尝试所有学到的东西，但对方的要求还是无法满足。你的团队或客户开始寻找其他解决方案。是时候采取行动了。
- en: In this chapter, an impossible-to-solve business case regarding material optimization
    will be resolved successfully with a hand-made version of a **feedforward neural
    network** (**FNN**) with backpropagation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，关于材料优化的一个无法解决的商业案例将通过手工制作的**前馈神经网络**（**FNN**）与反向传播成功解决。
- en: 'Feedforward networks are one of the key building blocks of deep learning. The
    battle around the XOR function perfectly illustrates how deep learning regained
    popularity in corporate environments. XOR is an exclusive OR function that we
    will explore later in this chapter. The XOR FNN illustrates one of the critical
    functions of neural networks: **classification**. Once information becomes classified
    into subsets, it opens the doors to **prediction** and many other functions of
    neural networks, such as representation learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络是深度学习的关键构建模块之一。围绕 XOR 函数的争论完美地说明了深度学习如何在企业环境中重新获得流行。XOR 是一种排他性“或”函数，我们将在本章后面探讨。XOR
    FNN 说明了神经网络的一个关键功能：**分类**。一旦信息被分类为子集，它就为**预测**和神经网络的许多其他功能打开了大门，如表征学习。
- en: An XOR FNN will be built from scratch to demystify deep learning from the start.
    A vintage, start-from-scratch method will be applied, blowing the deep learning
    hype off the table.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 XOR FNN 将从零开始构建，以从一开始就解密深度学习。我们将应用一种复古的、从头开始的方法，揭开深度学习的炒作面纱。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Explaining the XOR problem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 XOR 问题
- en: How to hand-build an FNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何手动构建 FNN
- en: Solving XOR with an FNN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 FNN 解决 XOR 问题
- en: Classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Backpropagation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: A cost function
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数
- en: Cost function optimization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数优化
- en: Error loss
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误损失
- en: Convergence
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛
- en: Before we begin building an FNN, we'll first introduce XOR and its limitations
    in the first artificial neural model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建 FNN 之前，我们首先会介绍 XOR 及其在第一个人工神经模型中的局限性。
- en: The original perceptron could not solve the XOR function
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始感知器无法解决 XOR 函数
- en: The original perceptron was designed in the 1950s and improved in the late 1970s.
    The original perceptron contained one neuron that could not solve the XOR function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 原始感知器是在1950年代设计的，并在1970年代末期得到了改进。原始感知器包含一个神经元，无法解决 XOR 函数。
- en: An XOR function means that you have to choose an exclusive OR (XOR).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 函数意味着你必须选择一个排他性的“或” (XOR)。
- en: 'This can be difficult to grasp, as we''re not used to thinking about the way
    in which we use *or* in our everyday lives. In truth, we use *or* interchangeably
    as either inclusive or exclusive all of the time. Take this simple example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很难理解，因为我们不习惯以我们日常生活中使用 *或* 的方式来思考。事实上，我们总是交替使用 *或*，既有包容性也有排他性。举个简单的例子：
- en: If a friend were to come and visit me, I may ask them, "Would you like tea or
    coffee?" This is basically the offer of tea XOR coffee; I would not expect my
    friend to ask for both tea and coffee! My friend will choose one or the other.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个朋友来拜访我，我可能会问他们：“你想要茶还是咖啡？”这基本上是提供茶 XOR 咖啡；我不会期望我的朋友同时要求茶和咖啡！我的朋友会选择其中之一。
- en: I may follow up my question with, "Would you like milk or sugar?" In this case,
    I would not be surprised if my friend wanted both. This is an inclusive *or*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能会跟进我的问题，问：“你想要加牛奶还是加糖？”在这种情况下，如果我的朋友想要两者，我不会感到惊讶。这是一个包容性的 *或*。
- en: XOR, therefore, means "You can have one or the other, but not both."
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，XOR 意味着“你可以选择一个或另一个，但不能同时选择两者。”
- en: We will develop these concepts in the chapter through more examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章通过更多示例来展开这些概念。
- en: To solve this XOR function, we will build an FNN.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个 XOR 函数，我们将构建一个 FNN。
- en: Once the feedforward network for solving the XOR problem is built, it will be
    applied to an optimization example. The material optimizing example will choose
    the best combinations of dimensions among billions to minimize the use of corporate
    resources with the generalization of the XOR function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: First, a solution to the XOR limitation of a perceptron must be clarified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: XOR and linearly separable models
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the late 1960s, it was mathematically proven that a perceptron could *not*
    solve an XOR function. Fortunately, today, the perceptron and its neocognitron
    version form the core model for neural networking.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: You may be tempted to think, *so what?* However, the entire field of neural
    networks relies on solving problems such as this to classify patterns. Without
    pattern classification, images, sounds, and words mean nothing to a machine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Linearly separable models
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The McCulloch-Pitts 1943 neuron (see *Chapter 2*, *Building a Reward Matrix
    – Designing Your Datasets*) led to Rosenblatt's 1957-59 perceptron and the 1960
    Widrow-Hoff adaptive linear element (Adaline).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: These models are linear models based on an *f*(*x*, *w*) function that requires
    a line to separate results. A perceptron cannot achieve this goal and thus cannot
    classify many objects it faces.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard linear function can separate values. **Linear separability** can
    be represented in the following graph:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_01-1.png](img/B15438_08_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Linearly separable patterns'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the line separating the preceding dots and the part under it represent
    a picture that needs to be represented by a machine learning or deep learning
    application. The dots above the line represent *clouds* in the sky; the dots below
    the line represent *trees* on a hill. The line represents the slope of that hill.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: To be linearly separable, a function must be able to separate the *clouds* from
    the *trees* to classify them. The prerequisite to classification is **separability**
    of some sort, linear or nonlinear.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The XOR limit of a linear model, such as the original perceptron
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A linear model cannot solve the XOR problem expressed as follows in a table:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value of x1** | **Value of x2** | **Output** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: 'Lines 3 and 4 show an exclusive OR (XOR). Imagine that you are offering a child
    a piece of cake OR a piece of candy (1 or 1):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: The child answers: "I want candy or nothing at all!" (0 or 1).
    That''s exclusive OR (XOR)!'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 2**: The child answers: "I want a cake or nothing at all!" (1 or 0).
    That''s an exclusive OR (XOR) as well!'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following graph shows the linear inseparability of the XOR function represented
    by one perceptron:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_08_02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Linearly inseparable patterns'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The values of the table represent the Cartesian coordinates in this graph. The
    circles with a cross at (1, 1) and (0, 0) cannot be separated from the circles
    at (1, 0) and (0, 1). That's a huge problem. It means that Frank Rosenblatt's
    *f*(*x*, *w*) perceptron cannot separate, and thus can not classify, these dots
    into *clouds* and *trees*. Thus, in many cases, the perceptron cannot identify
    values that require linear separability.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Having invented the most powerful neural concept of the twentieth century—a
    neuron that can learn—Frank Rosenblatt had to bear with this limitation through
    the 1960s.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: As explained with the preceding cake-OR-candy example, the absence of an XOR
    function limits applications in which you must choose exclusively between two
    options. There are many "it's-either-that-or-nothing" situations in real-life
    applications. For a self-driving car, it could be either turn left or turn right,
    but don't swerve back and forth while making the decision!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We will solve this limitation with a vintage solution, starting by building,
    and later implementing, an FNN.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Building an FNN from scratch
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's perform a mind experiment. Imagine we are in 1969\. We have today's knowledge
    but nothing to prove it. We know that a perceptron cannot implement the exclusive
    OR function XOR.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: We have an advantage because we now know a solution exists. To start our experiment,
    we only have a pad, a pencil, a sharpener, and an eraser waiting for us. We're
    ready to solve the XOR problem from scratch on paper before programming it. We
    have to find a way to classify those dots with a neural network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – defining an FNN
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have to be unconventional to solve this problem. We must forget the complicated
    words and theories of the twenty-first century.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a neural network layer in high-school format. A hidden layer will
    be:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *x* * *w*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'OK. Now we have one layer. A layer is merely a function. This function can
    be expressed as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*, *w*)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In which *x* is the input value, and *w* is some value to multiply *x* by. Hidden
    means that the computation is not visible, just as *x* = 2 and *x* + 2 is the
    hidden layer that leads to 4.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have defined a neural network in three lines:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Input *x*.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some function that changes its value, like 2 × 2 = 4, which transformed 2\.
    That is a layer. And if the result is superior to 2, for example, then great!
    The output is 1, meaning yes or true. Since we don't see the computation, this
    is the *hidden* layer.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f*(*x*, *w*) is the building block of any neural network. "Feedforward" means
    that we will be going from layer 1 to layer 2, moving forward in a sequence.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know that basically any neural network is built with values transformed
    by an operation to become an output of something, we need some logic to solve
    the XOR problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – an example of how two children can solve the XOR problem every day
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An example follows of how two children can solve the XOR problem using a straightforward
    everyday example. I strongly recommend this method. I have taken very complex
    problems, broken them down into small parts to a child's level, and often solved
    them in a few minutes. Then, you get the sarcastic answer from others such as
    "Is that all you did?" But, the sarcasm vanishes when the solution works over
    and over again in high-level corporate projects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s convert the XOR problem into a candy problem in a store. Two
    children go to the store and want to buy candy. However, they only have enough
    money to buy one pack of candy. They have to agree on a choice between two packs
    of different candy. Let''s say pack one is chocolate and the other is chewing
    gum. Then, during the discussion between these two children, 1 means yes, 0 means
    no. Their budget limits the options of these two children:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Going to the store and not buying any chocolate **or** chewing gum = (no, no)
    = (0, 0). That's not an option for these children! So the answer is false.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to the store and buying both chocolate **and** chewing gum = (yes, yes)
    = (1, 1). That would be fantastic, but that's not possible. It's too expensive.
    So, the answer is, unfortunately, false.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to the store and either buying chocolate **or** chewing gum = (1, 0 or
    0, 1) = (yes or no) or (no or yes). That's possible. So, the answer is true.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine the two children. The eldest one is reasonable. The younger one doesn't
    really know how to count yet and wants to buy both packs of candy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'We express this on paper:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[1] (eldest child''s decision, yes or no, 1 or 0) * *w*[1] (what the elder
    child thinks). The elder child is thinking this, or:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[1] * *w*[1] or *h*[1] = *x*[1] * *w*[1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The elder child weighs a decision like we all do every day, such as purchasing
    a car (*x* = 0 or 1) multiplied by the cost (*w*[1]).
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*x*[2] (the younger child''s decision, yes or no, 1 or 0) * *w*[3] (what the
    younger child thinks). The younger child is also thinking this, or:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[2] * *w*[3] or *h*[2] = *x*[2] * *w*[3]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Theory**: *x*[1] and *x*[2] are the inputs. *h*[1] and *h*[2] are neurons
    (the result of a calculation). Since *h*[1] and *h*[2] contain calculations that
    are not visible during the process, they are hidden neurons. *h*[1] and *h*[2]
    thus form a hidden layer. *w*[1] and *w*[3] are weights that represent how we
    "weigh" a decision, stating that something is more important than something else.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine the two children talking to each other.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Hold it a minute! This means that now, each child is communicating with the
    other:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[1] (the elder child) says *w*[2] to the younger child. Thus, *w*[2] = this
    is what I think and am telling you:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[1] * *w*[2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*x*[2] (the younger child) says, "please add my views to your decision," which
    is represented by *w*[4][:]'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[2] * *w*[4]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now have the first two equations expressed in high-school-level code. It''s
    what one thinks plus what one says to the other, asking the other to take that
    into account:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`h1` sums up what is going on in one child''s mind: personal opinion + the
    other child''s opinion.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '`h2` sums up what is going on in the other child''s mind and conversation:
    personal opinion + the other child''s opinion.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory**: The calculation now contains two input values and one hidden layer.
    Since, in the next step, we are going to apply calculations to `h1` and `h2`,
    we are in a feedforward neural network. We are moving from the input to another
    layer, which will lead us to another layer, and so on. This process of going from
    one layer to another is the basis of deep learning. The more layers you have,
    the deeper the network is. The reason `h1` and `h2` form a hidden layer is that
    their output is just the input of another layer.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we don''t need complicated numbers in an activation function
    such as logistic sigmoid, so we state whether the output values are less than
    1 or not:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: if *h*[1] + *h*[2] >= 1 then *y*[1] = 1
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: if *h*[1] + *h*[2] < 1 then *y*[2] = 0
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory**: *y*[1] and *y*[2] form a second hidden layer. These variables can
    be scalars, vectors, or matrices. They are neurons.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now, a problem comes up. Who is right? The elder child or the younger child?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The only way seems to be to play around, with the weights *W* representing all
    the weights. Weights in a neural network work like weights in our everyday lives.
    We *weigh* decisions all the time. For example, there are two books to purchase,
    and we will "weigh" our decisions. If one is interesting and cheaper, it will
    weigh more or less in our decision, for example.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The children in our case agree on purchasing at least something, so from now
    on, *w*[3] = *w*[2], *w*[4] = *w*[1]. The younger and elder child will thus share
    some of the decision weights.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Now, somebody has to be an influencer. Let's leave this hard task to the elder
    child. The elder child, being more reasonable, will continuously deliver the bad
    news. You have to subtract something from your choice, represented by a minus
    (–) sign.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Each time they reach the point *h*[i], the eldest child applies a critical negative
    view on purchasing packs of candy. It's –*w* of everything comes up to be sure
    not to go over the budget. The opinion of the elder child is biased, so let's
    call the variable a bias, *b*[1]. Since the younger child's opinion is biased
    as well, let's call this view a bias too, *b*[2]. Since the eldest child's view
    is always negative, –*b*[1] will be applied to all of the eldest child's thoughts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'When we apply this decision process to their view, we obtain:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *y*[1] * –*b*[1]'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[2] = *y*[2] * *b*[2]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we just have to use the same result. If the result is >=1, then the threshold
    has been reached. The threshold is calculated as shown in the following function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *h*[1] + *h*[2]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first start effectively finding the weights, starting by setting the
    weights and biases to 0.5, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[1] = 0.2; *w*[2] = 0.5; *b*[1] = 0.5'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[3] = *w*[2]; *w*[4] = *w*[1]; *b*[2] = *b*[1]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: It's not a full program yet, but its theory is done.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Only the communication going on between the two children is making the difference;
    we will focus on only modifying *w*[2] and *b*[1] after a first try. It works
    on paper after a few tries.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'We now write the basic mathematical function, which is, in fact, the program
    itself on paper:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let's go from the solution on paper to Python.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Why wasn't this deceivingly simple solution found in 1969? Because *it seems
    simple today but wasn't so at that time*, like all inventions found by our genius
    predecessors. Nothing is easy at all in artificial intelligence and mathematics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll stick with the solution proposed here, and implement
    it in Python.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a vintage XOR solution in Python with an FNN and backpropagation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To stay in the spirit of a 1969 vintage solution, we will not use NumPy, TensorFlow,
    Keras, or any other high-level library. Writing a vintage FNN with backpropagation
    written in high-school mathematics is fun.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: If you break a problem down into very elementary parts, you understand it better
    and provide a solution to that specific problem. You don't need to use a huge
    truck to transport a loaf of bread.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, by thinking through the minds of children, we went against running
    20,000 or more episodes in modern CPU-rich solutions to solve the XOR problem.
    The logic used proves that both inputs can have the same parameters as long as
    one bias is negative (the elder reasonable critical child) to make the system
    provide a reasonable answer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic Python solution quickly reaches a result in a few iterations, approximately
    10 iterations (epochs or episodes), depending on how we think it through. An epoch
    can be related to a try. Imagine looking at somebody practicing basketball:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The person throws the ball toward the hoop but misses. That was an epoch (an
    episode can be used as well).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The person thinks about what happened and changes the way the ball will be thrown.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This improvement is what makes it a learning epoch (or episode). It is not a
    simple memoryless try. Something is really happening to improve performance.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The person throws the ball again (next epoch) and again (next epochs) until
    the overall performance has improved. This is how a neural network improves over
    epochs.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FNN_XOR_vintage_tribute.py` contains (at the top of the code) a result matrix
    with four columns.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Each element of the matrix represents the status (`1` = correct, `0` = false)
    of the four predicates to solve:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `train` variable is the number of predicates to solve: (0, 0), (1, 1),
    (1, 0), (0, 1). The variable of the predicate to solve is `pred`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the program is practically a copy of the sheet of paper we wrote,
    as in the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`pred` is an argument of the function from `1` to `4`. The four predicates
    are represented in the following table:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predicate (pred)** | **x[1]** | **x[2]** | **Expected result** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0 | 1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 1 | 1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: That is why *y* must be <1 for predicates 0 and 1\. Then, *y* must be >=1 for
    predicates 2 and 3.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to call the following function limiting the training to 50 epochs,
    which are more than enough:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At the first epoch, the weights and biases are all set to `0.5`. No use thinking!
    Let the program do the job. As explained previously, the weight and bias of `x2`
    are equal.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the hidden layers and `y` calculation function are called four times,
    one for each predicate to train, as shown in the following code snippet:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, the system must train. To do that, we need to measure the number of predictions,
    1 to 4, that are correct at each iteration and decide how to change the weights/biases
    until we obtain proper results. We'll do that in the following section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: A simplified version of a cost function and gradient descent
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Slightly more complex gradient descent will be described in the next chapter.
    In this chapter, only a one-line equation will do the job. The only thing to bear
    in mind as an unconventional thinker is: *so what?* The concept of gradient descent
    is minimizing loss or errors between the present result and a goal to attain.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: First, a cost function is needed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We need to
    find out how many are correctly trained at each epoch.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The cost function will measure the difference between the training goal (4)
    and the result of this epoch or training iteration (result).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: When 0 convergence is reached, it means the training has succeeded.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '`result[0,0,0,0]` contains a `0` for each value if none of the four predicates
    have been trained correctly. `result[1,0,1,0]` means two out of the four predicates
    are correct. `result[1,1,1,1]` means that all four predicates have been trained
    and that the training can stop. `1`, in this case, means that the correct training
    result was obtained. It can be `0` or `1`. The `result` array is the result counter.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The cost function will express this training by having a value of `4`, `3`,
    `2`, `1`, or `0` as the training goes down the slope to 0.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent measures the value of the descent to find the direction of
    the slope: up, down, or 0\. Then, once you have that slope and the steepness of
    it, you can optimize the weights. A derivative is a way to know whether you are
    going up or down a slope.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Each time we move up or down the slope, we check to see whether we are moving
    in the right direction. We will assume that we will go one step at a time. So
    if we change directions, we will change our pace by one step. That one step value
    is our **learning rate**. We will measure our progression at each step. However,
    if we feel comfortable with our results, we might walk 10 steps at a time and
    only check to see if we are on the right track every 10 steps. Our learning rate
    will thus have increased to 10 steps.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we hijacked the concept and used it to set the learning rate
    to `0.05` with a one-line function. Why not? It helped to solve gradient descent
    optimization in one line:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By applying the vintage children-buying-candy logic to the whole XOR problem,
    we found that only `w2` needed to be optimized. That's why `b1=w2`. That's because
    `b1` is doing the tough job of saying something negative (`-`) all the time, which
    completely changes the course of the resulting outputs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The rate is set at `0.05`, and the program finishes training in 10 epochs:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a logical *yes* or *no* problem. The way the network is built is pure
    logic. Nothing can stop us from using whatever training rates we wish. In fact,
    that's what gradient descent is about. There are many gradient descent methods.
    If you invent your own and it works for your solution, that is fine.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'This one-line code is enough, in this case, to see whether the slope is going
    down. As long as the slope is negative, the function is going downhill to *cost*
    = 0:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following diagram sums up the whole process:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_03.png](img/B15438_08_03.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: A feedforward neural network model (FNN)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all of the arrows of the layers go forward in this "feedforward"
    neural network. However, the arrow that stems from the *y* node and goes backward
    can seem confusing. This line represents a change in weights to train the model.
    This means that we go back to changing the weights and running the network for
    another epoch (or episode). The system is adjusting its weights epoch by epoch
    until the overall result is correct.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Too simple? Well, it works, and that's all that counts in real-life development.
    If your code is bug-free and does the job, then that's what matters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Finding a simple development tool means nothing more than that. It's just another
    tool in the toolbox. We can get this XOR function to work on a neural network
    and generate income.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Companies are not interested in how smart you are but how efficient (profitable)
    you can be.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'A company''s survival relies on multiple constraints: delivering on time, offering
    good prices, providing a product with a reasonable quality level, and many more
    factors besides.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: When we come up with a solution, it is useless to show how smart we can be writing
    tons of code. Our company or customers expect an efficient solution that will
    run well and is easy to maintain. In short, focus on efficiency. Once we have
    a good solution, we need to show that it works. In this case, we proved that linear
    separability was achieved.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Linear separability was achieved
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bear in mind that the whole purpose of this feedforward network with backpropagation
    through a cost function was to transform a linear non-separable function into
    a linearly separable function to implement the classification of features presented
    to the system. In this case, the features had a `0` or `1` value.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: One of the core goals of a layer in a neural network is to make the input make
    sense, meaning to be able to separate one kind of information from another.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '`h1` and `h2` will produce the Cartesian coordinate linear separability training
    axis, as implemented in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running the program provides a view of the nonlinear input values once the
    hidden layers have trained them. The nonlinear values then become linear values
    in a linearly separable function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The intermediate result and goal are not a bunch of numbers on a screen to
    show that the program works. The result is a set of Cartesian values that can
    be represented in the following linearly separated graph:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_04-1.png](img/B15438_08_04.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Linearly separable patterns'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We have now obtained a separation between the top values, representing the intermediate
    values of the (1, 0) and (0, 1) inputs, and the bottom values, representing the
    (1, 1) and (0, 0) inputs. The top values are separated from the bottom values
    by a clear line. We now have *clouds* on top and *trees* below the line that separates
    them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers of the neural network have transformed nonlinear values into linearly
    separable values, making classification possible through standard separation equations,
    such as the one in the following code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability of a neural network to make non-separable information separable
    and classifiable represents one of the core powers of deep learning. From this
    technique, many operations can be performed on data, such as subset optimization.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at a practical application for our FNN XOR solution.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Applying the FNN XOR function to optimizing subsets of data
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are more than 7.5 billion people breathing air on this planet. In 2050,
    there might be 2.5 billion more of us. All of these people need to wear clothes
    and eat. Just those two activities involve classifying data into subsets for industrial
    purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouping** is a core concept for any production. Production relating to producing
    clothes and food requires grouping to optimize production costs. Imagine not grouping
    and delivering one T-shirt at a time from one continent to another instead of
    grouping T-shirts in a container and grouping many containers (not just two on
    a ship). Let''s focus on clothing, for example.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: A chain of stores needs to replenish the stock of clothing in each store as
    the customers purchase their products. In this case, the corporation has 10,000
    stores. The brand produces jeans, for example. Their average product is a faded
    jean. This product sells a slow 50 units a month per store. That adds up to 10,000
    stores × 50 units = 500,000 units or stock-keeping units (SKUs) per month. These
    units are sold in all sizes, grouped into average, small, and large. The sizes
    sold per month are random.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The main factory for this product has about 2,500 employees producing those
    jeans at an output of about 25,000 jeans per day. The employees work in the following
    main fields: cutting, assembling, washing, lasering, packaging, and warehousing.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The first difficulty arises with the purchase and use of fabric. The fabric
    for this brand is not cheap. Large amounts are necessary. Each pattern (the form
    of pieces of the pants to be assembled) needs to be cut by wasting as little fabric
    as possible.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an empty box you want to fill up to optimize the volume. If
    you only put soccer balls in it, there will be a lot of space. If you slip tennis
    balls in the empty spaces, space will decrease. If, on top of that, you fill the
    remaining empty spaces with ping pong balls, you will have optimized the available
    space in the box.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Building optimized subsets can be applied to containers, warehouse flows and
    storage, truckload optimizing, and almost all human activities.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: In the apparel business, if 1% to 10% of the fabric is wasted while manufacturing
    jeans, the company will survive the competition. At over 10%, there is a real
    problem to solve. Losing 20% of all the fabric consumed in manufacturing jeans
    can bring the company down and force it into bankruptcy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The main rule is to combine larger pieces and smaller pieces to make optimized
    cutting patterns.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of space through larger and smaller objects can be applied to cutting
    the forms, which are the patterns of the jeans, for example. Once they are cut,
    they will be assembled at the sewing stations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem can be summed up as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Creating subsets of the 500,000 SKUs to optimize the cutting process for the
    month to come in a given factory
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sure that each subset contains smaller sizes and larger sizes to minimize
    the loss of fabric by choosing 6 sizes per day to build 25,000 unit subsets per
    day
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating cut plans of an average of 3 to 6 sizes per subset per day for a
    production of 25,000 units per day
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In mathematical terms, this means trying to find subsets of sizes among 500,000
    units for a given day.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is to find 6 well-matched sizes among 500,000 units, as shown in the
    following combination formula:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_08_001.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: At this point, most people abandon the idea and find some easy way out of this,
    even if it means wasting fabric.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The first reaction we all have is that this is more than the number of stars
    in the universe and all that hype. However, that's not the right way to look at
    it at all. The right way is to look exactly in the opposite direction.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The key to this problem is to observe the particle at a microscopic level, at
    the **bits of information** level. Analyzing detailed data is necessary to obtain
    reliable results. This is a fundamental concept of machine learning and deep learning.
    Translated into our field, it means that to process an image, ML and DL process
    pixels.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'So, even if the pictures to process represent large quantities, it will come
    down to small units of information to analyze:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '| yottabyte (YB) | 10^(24) | yobibyte (YiB) | 2^(80) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: It might be surprising to see these large numbers appear suddenly! However,
    when trying to combine thousands of elements, the combinations become exponential.
    When you extend this to the large population that major apparel brands have to
    deal with, it becomes rapidly exponential as well.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Today, Google, Facebook, Amazon, and others have yottabytes of data to classify
    and make sense of. Using the term **big data** doesn't mean much. It's just a
    lot of data, and so what?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: You do not need to analyze the individual positions of each data point in a
    dataset but use the probability distribution.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: To understand that, let's go to a store to buy some jeans for a family. One
    of the parents wants a pair of jeans, and so does a teenager in that family. They
    both go and try to find their size in the pair of jeans they want. The parent
    finds 10 pairs of jeans in size *x*. All of the jeans are part of the production
    plan. The parent picks one at *random*, and the teenager does the same. Then they
    pay for them and take them home.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Some systems work fine with random choices: random transportation (taking jeans
    from the store to home) of particles (jeans, other product units, pixels, or whatever
    is to be processed), making up that fluid (a dataset).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Translated into our factory, this means that a stochastic (random) process can
    be introduced to solve the problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'All that was required is that small and large sizes were picked at random among
    the 500,000 units to produce. If 6 sizes from 1 to 6 were to be picked per day,
    the sizes could be classified as follows in a table:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '*Smaller sizes* = *S* = {1, 2, 3}'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '*Larger sizes* = *L* = {4, 5, 6}'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting this into numerical subset names, *S* = 1 and *L* = 6\. By selecting
    large and small sizes to produce at the same time, the fabric will be optimized,
    as shown in the following table:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '| **Size of choice 1** | **Size of choice 2** | **Output** |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6 | 0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 1 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 1 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: You will notice that the first two lines contain the same value. This will not
    optimize fabric consumption. If you put only large size 6 products together, there
    will be "holes" in the pattern. If you only put small size 1 products together,
    then they will fill up all of the space and leave no room for larger products.
    Fabric cutting is optimal when large and small sizes are present on the same roll
    of fabric.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Doesn't this sound familiar? It looks exactly like our vintage FNN, with 1 instead
    of 0 and 6 instead of 1.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: All that has to be done is to stipulate that subset *S* = *value* 0, and subset
    *L* = *value* 1; and the previous code can be generalized.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`FFN_XOR_generalization.py` is the program that generalizes the previous code,
    as shown in the following snippet.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'If this works, then smaller and larger sizes will be chosen to send to the
    cut planning department, and the fabric will be optimized. Applying the randomness
    concept of Bellman''s equation, a stochastic process is applied, choosing customer
    unit orders at random (each order is one size and a unit quantity of 1):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The weights and bias are now constants obtained by the result of the XOR training
    FNN. The training is over; the FNN is now used to provide results. Bear in mind
    that the word *learning* in machine learning and deep learning doesn't mean you
    have to train systems forever. In stable environments, training is run only when
    the datasets change. At one point in a project, you are hopefully using deep *trained*
    systems and not simply exploring the training phase of a deep *learning* process.
    The goal is not to spend all corporate resources on learning but on using trained
    models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning architecture must rapidly become deep trained models to produce
    a profit.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'For this prototype validation, the size of a given order is random. `0` means
    the order fits in the *S* subset; `1` means the order fits in the *L* subset.
    The data generation function reflects the random nature of consumer behavior in
    the following six-size jeans consumption model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once two customer orders have been chosen at random in the correct size category,
    the FNN is activated and runs like the previous example. Only the `result` array
    has been changed since we are using the same core program. Only a yes (`1`) or
    no (`0`) is expected, as shown in the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The number of subsets to produce needs to be calculated to determine the volume
    of positive results required.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The choice is made of 6 sizes among 500,000 units. But, the request is to produce
    a daily production plan for the factory. The daily production target is 25,000\.
    Also, each subset can be used about 20 times. There is always, on average, 20
    times the same size in a given pair of jeans available.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Six sizes are required to obtain good fabric optimization. This means that
    after three choices, the result represents one subset of potential optimized choices:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '*R* = 120 × 3 subsets of two sizes = 360'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The magic number has been found. For every 3 choices, the goal of producing
    6 sizes multiplied by a repetition of 20 will be reached.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'The production-per-day request is 25,000:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The number of subsets requested = 25000/3=8333\. 333
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'The system can run 8,333 products as long as necessary to produce the volume
    of subsets requested. In this case, the range is set to a sample of 1,000,000
    products. It can be extended or reduced when needed. The system is filtering the
    correct subsets through the following function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When the 8,333 subsets have been found respecting the smaller-larger size distribution,
    the system stops, as shown in the following output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This example proves the point. *Simple solutions can solve very complex problems.*
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main functions, among some minor ones, must be added:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: After each choice, the orders chosen must be removed from the 500,000-order
    dataset. When an order has been selected, processing it again will generate errors
    in the global results. This will preclude choosing the same order twice and reduce
    the number of choices to be made.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimization function to regroup the results for production purposes, for
    example. The idea is not to run through the records randomly, but to organize
    them by sets. This way, each set can be controlled independently.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application information:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The core calculation part of the application is fewer than 50 lines long.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a few control functions and arrays, the program might reach 200 lines maximum.
    The goal of the control functions is to check and see whether the results reach
    the overall goal. For example, every 1,000 records, a local result could be checked
    to see whether it fits the overall goal.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in easy maintenance for a team.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the number of lines of code to create a powerful application can
    prove to be very efficient for many business problems.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a small neural network from scratch provides a practical view of the
    elementary properties of a neuron. We saw that a neuron requires an input that
    can contain many variables. Then, weights are applied to the values with biases.
    An activation function then transforms the result and produces an output.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks, even one- or two-layer networks, can provide real-life solutions
    in a corporate environment. A real-life business case was implemented using complex
    theory broken down into small functions. Then, these components were assembled
    to be as minimal and profitable as possible.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: It takes talent to break a problem down into elementary parts and find a simple,
    powerful solution. It requires more effort than just typing hundreds to thousands
    of lines of code to make things work. A well-thought through algorithm will always
    be more profitable, and software maintenance will prove more cost-effective.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Customers expect quick-win solutions. Artificial intelligence provides a large
    variety of tools that satisfy that goal. When solving a problem for a customer,
    do not look for the best theory, but the simplest and fastest way to implement
    a profitable solution, no matter how unconventional it seems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an enhanced FNN perceptron solved a complex business problem.
    In the next chapter, we will explore a convolutional neural network (CNN). We
    will build a CNN with TensorFlow 2.x, layer by layer, to classify images.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can the perceptron alone solve the XOR problem? (Yes | No)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the XOR function linearly non-separable? (Yes | No)
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the main goals of layers in a neural network is classification. (Yes
    | No)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is deep learning the only way to classify data? (Yes | No)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A cost function shows the increase in the cost of a neural network. (Yes | No)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can simple arithmetic be enough to optimize a cost function? (Yes | No)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feedforward network requires inputs, layers, and an output. (Yes | No)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feedforward network always requires training with backpropagation. (Yes |
    No)
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In real-life applications, solutions are only found by following existing theories.
    (Yes | No)
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear separability: [http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
