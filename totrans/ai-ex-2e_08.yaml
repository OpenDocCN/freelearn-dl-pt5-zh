- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solving the XOR Problem with a Feedforward Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the course of a corporate project, there always comes the point when a problem that
    seems impossible to solve hits you. At that point, you try everything you've learned,
    but it doesn't work for what's asked of you. Your team or customer begins to look
    elsewhere. It's time to react.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, an impossible-to-solve business case regarding material optimization
    will be resolved successfully with a hand-made version of a **feedforward neural
    network** (**FNN**) with backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedforward networks are one of the key building blocks of deep learning. The
    battle around the XOR function perfectly illustrates how deep learning regained
    popularity in corporate environments. XOR is an exclusive OR function that we
    will explore later in this chapter. The XOR FNN illustrates one of the critical
    functions of neural networks: **classification**. Once information becomes classified
    into subsets, it opens the doors to **prediction** and many other functions of
    neural networks, such as representation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: An XOR FNN will be built from scratch to demystify deep learning from the start.
    A vintage, start-from-scratch method will be applied, blowing the deep learning
    hype off the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the XOR problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to hand-build an FNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving XOR with an FNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost function optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we begin building an FNN, we'll first introduce XOR and its limitations
    in the first artificial neural model.
  prefs: []
  type: TYPE_NORMAL
- en: The original perceptron could not solve the XOR function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original perceptron was designed in the 1950s and improved in the late 1970s.
    The original perceptron contained one neuron that could not solve the XOR function.
  prefs: []
  type: TYPE_NORMAL
- en: An XOR function means that you have to choose an exclusive OR (XOR).
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be difficult to grasp, as we''re not used to thinking about the way
    in which we use *or* in our everyday lives. In truth, we use *or* interchangeably
    as either inclusive or exclusive all of the time. Take this simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: If a friend were to come and visit me, I may ask them, "Would you like tea or
    coffee?" This is basically the offer of tea XOR coffee; I would not expect my
    friend to ask for both tea and coffee! My friend will choose one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: I may follow up my question with, "Would you like milk or sugar?" In this case,
    I would not be surprised if my friend wanted both. This is an inclusive *or*.
  prefs: []
  type: TYPE_NORMAL
- en: XOR, therefore, means "You can have one or the other, but not both."
  prefs: []
  type: TYPE_NORMAL
- en: We will develop these concepts in the chapter through more examples.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this XOR function, we will build an FNN.
  prefs: []
  type: TYPE_NORMAL
- en: Once the feedforward network for solving the XOR problem is built, it will be
    applied to an optimization example. The material optimizing example will choose
    the best combinations of dimensions among billions to minimize the use of corporate
    resources with the generalization of the XOR function.
  prefs: []
  type: TYPE_NORMAL
- en: First, a solution to the XOR limitation of a perceptron must be clarified.
  prefs: []
  type: TYPE_NORMAL
- en: XOR and linearly separable models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the late 1960s, it was mathematically proven that a perceptron could *not*
    solve an XOR function. Fortunately, today, the perceptron and its neocognitron
    version form the core model for neural networking.
  prefs: []
  type: TYPE_NORMAL
- en: You may be tempted to think, *so what?* However, the entire field of neural
    networks relies on solving problems such as this to classify patterns. Without
    pattern classification, images, sounds, and words mean nothing to a machine.
  prefs: []
  type: TYPE_NORMAL
- en: Linearly separable models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The McCulloch-Pitts 1943 neuron (see *Chapter 2*, *Building a Reward Matrix
    – Designing Your Datasets*) led to Rosenblatt's 1957-59 perceptron and the 1960
    Widrow-Hoff adaptive linear element (Adaline).
  prefs: []
  type: TYPE_NORMAL
- en: These models are linear models based on an *f*(*x*, *w*) function that requires
    a line to separate results. A perceptron cannot achieve this goal and thus cannot
    classify many objects it faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard linear function can separate values. **Linear separability** can
    be represented in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_01-1.png](img/B15438_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Linearly separable patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the line separating the preceding dots and the part under it represent
    a picture that needs to be represented by a machine learning or deep learning
    application. The dots above the line represent *clouds* in the sky; the dots below
    the line represent *trees* on a hill. The line represents the slope of that hill.
  prefs: []
  type: TYPE_NORMAL
- en: To be linearly separable, a function must be able to separate the *clouds* from
    the *trees* to classify them. The prerequisite to classification is **separability**
    of some sort, linear or nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: The XOR limit of a linear model, such as the original perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A linear model cannot solve the XOR problem expressed as follows in a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value of x1** | **Value of x2** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Lines 3 and 4 show an exclusive OR (XOR). Imagine that you are offering a child
    a piece of cake OR a piece of candy (1 or 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: The child answers: "I want candy or nothing at all!" (0 or 1).
    That''s exclusive OR (XOR)!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 2**: The child answers: "I want a cake or nothing at all!" (1 or 0).
    That''s an exclusive OR (XOR) as well!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following graph shows the linear inseparability of the XOR function represented
    by one perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Linearly inseparable patterns'
  prefs: []
  type: TYPE_NORMAL
- en: The values of the table represent the Cartesian coordinates in this graph. The
    circles with a cross at (1, 1) and (0, 0) cannot be separated from the circles
    at (1, 0) and (0, 1). That's a huge problem. It means that Frank Rosenblatt's
    *f*(*x*, *w*) perceptron cannot separate, and thus can not classify, these dots
    into *clouds* and *trees*. Thus, in many cases, the perceptron cannot identify
    values that require linear separability.
  prefs: []
  type: TYPE_NORMAL
- en: Having invented the most powerful neural concept of the twentieth century—a
    neuron that can learn—Frank Rosenblatt had to bear with this limitation through
    the 1960s.
  prefs: []
  type: TYPE_NORMAL
- en: As explained with the preceding cake-OR-candy example, the absence of an XOR
    function limits applications in which you must choose exclusively between two
    options. There are many "it's-either-that-or-nothing" situations in real-life
    applications. For a self-driving car, it could be either turn left or turn right,
    but don't swerve back and forth while making the decision!
  prefs: []
  type: TYPE_NORMAL
- en: We will solve this limitation with a vintage solution, starting by building,
    and later implementing, an FNN.
  prefs: []
  type: TYPE_NORMAL
- en: Building an FNN from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's perform a mind experiment. Imagine we are in 1969\. We have today's knowledge
    but nothing to prove it. We know that a perceptron cannot implement the exclusive
    OR function XOR.
  prefs: []
  type: TYPE_NORMAL
- en: We have an advantage because we now know a solution exists. To start our experiment,
    we only have a pad, a pencil, a sharpener, and an eraser waiting for us. We're
    ready to solve the XOR problem from scratch on paper before programming it. We
    have to find a way to classify those dots with a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – defining an FNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have to be unconventional to solve this problem. We must forget the complicated
    words and theories of the twenty-first century.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a neural network layer in high-school format. A hidden layer will
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *x* * *w*'
  prefs: []
  type: TYPE_NORMAL
- en: 'OK. Now we have one layer. A layer is merely a function. This function can
    be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*, *w*)'
  prefs: []
  type: TYPE_NORMAL
- en: In which *x* is the input value, and *w* is some value to multiply *x* by. Hidden
    means that the computation is not visible, just as *x* = 2 and *x* + 2 is the
    hidden layer that leads to 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have defined a neural network in three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: Input *x*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some function that changes its value, like 2 × 2 = 4, which transformed 2\.
    That is a layer. And if the result is superior to 2, for example, then great!
    The output is 1, meaning yes or true. Since we don't see the computation, this
    is the *hidden* layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f*(*x*, *w*) is the building block of any neural network. "Feedforward" means
    that we will be going from layer 1 to layer 2, moving forward in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know that basically any neural network is built with values transformed
    by an operation to become an output of something, we need some logic to solve
    the XOR problem.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – an example of how two children can solve the XOR problem every day
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An example follows of how two children can solve the XOR problem using a straightforward
    everyday example. I strongly recommend this method. I have taken very complex
    problems, broken them down into small parts to a child's level, and often solved
    them in a few minutes. Then, you get the sarcastic answer from others such as
    "Is that all you did?" But, the sarcasm vanishes when the solution works over
    and over again in high-level corporate projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s convert the XOR problem into a candy problem in a store. Two
    children go to the store and want to buy candy. However, they only have enough
    money to buy one pack of candy. They have to agree on a choice between two packs
    of different candy. Let''s say pack one is chocolate and the other is chewing
    gum. Then, during the discussion between these two children, 1 means yes, 0 means
    no. Their budget limits the options of these two children:'
  prefs: []
  type: TYPE_NORMAL
- en: Going to the store and not buying any chocolate **or** chewing gum = (no, no)
    = (0, 0). That's not an option for these children! So the answer is false.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to the store and buying both chocolate **and** chewing gum = (yes, yes)
    = (1, 1). That would be fantastic, but that's not possible. It's too expensive.
    So, the answer is, unfortunately, false.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going to the store and either buying chocolate **or** chewing gum = (1, 0 or
    0, 1) = (yes or no) or (no or yes). That's possible. So, the answer is true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine the two children. The eldest one is reasonable. The younger one doesn't
    really know how to count yet and wants to buy both packs of candy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We express this on paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[1] (eldest child''s decision, yes or no, 1 or 0) * *w*[1] (what the elder
    child thinks). The elder child is thinking this, or:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[1] * *w*[1] or *h*[1] = *x*[1] * *w*[1]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The elder child weighs a decision like we all do every day, such as purchasing
    a car (*x* = 0 or 1) multiplied by the cost (*w*[1]).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*x*[2] (the younger child''s decision, yes or no, 1 or 0) * *w*[3] (what the
    younger child thinks). The younger child is also thinking this, or:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[2] * *w*[3] or *h*[2] = *x*[2] * *w*[3]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Theory**: *x*[1] and *x*[2] are the inputs. *h*[1] and *h*[2] are neurons
    (the result of a calculation). Since *h*[1] and *h*[2] contain calculations that
    are not visible during the process, they are hidden neurons. *h*[1] and *h*[2]
    thus form a hidden layer. *w*[1] and *w*[3] are weights that represent how we
    "weigh" a decision, stating that something is more important than something else.'
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine the two children talking to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hold it a minute! This means that now, each child is communicating with the
    other:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[1] (the elder child) says *w*[2] to the younger child. Thus, *w*[2] = this
    is what I think and am telling you:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[1] * *w*[2]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*x*[2] (the younger child) says, "please add my views to your decision," which
    is represented by *w*[4][:]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[2] * *w*[4]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now have the first two equations expressed in high-school-level code. It''s
    what one thinks plus what one says to the other, asking the other to take that
    into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`h1` sums up what is going on in one child''s mind: personal opinion + the
    other child''s opinion.'
  prefs: []
  type: TYPE_NORMAL
- en: '`h2` sums up what is going on in the other child''s mind and conversation:
    personal opinion + the other child''s opinion.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory**: The calculation now contains two input values and one hidden layer.
    Since, in the next step, we are going to apply calculations to `h1` and `h2`,
    we are in a feedforward neural network. We are moving from the input to another
    layer, which will lead us to another layer, and so on. This process of going from
    one layer to another is the basis of deep learning. The more layers you have,
    the deeper the network is. The reason `h1` and `h2` form a hidden layer is that
    their output is just the input of another layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we don''t need complicated numbers in an activation function
    such as logistic sigmoid, so we state whether the output values are less than
    1 or not:'
  prefs: []
  type: TYPE_NORMAL
- en: if *h*[1] + *h*[2] >= 1 then *y*[1] = 1
  prefs: []
  type: TYPE_NORMAL
- en: if *h*[1] + *h*[2] < 1 then *y*[2] = 0
  prefs: []
  type: TYPE_NORMAL
- en: '**Theory**: *y*[1] and *y*[2] form a second hidden layer. These variables can
    be scalars, vectors, or matrices. They are neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, a problem comes up. Who is right? The elder child or the younger child?
  prefs: []
  type: TYPE_NORMAL
- en: The only way seems to be to play around, with the weights *W* representing all
    the weights. Weights in a neural network work like weights in our everyday lives.
    We *weigh* decisions all the time. For example, there are two books to purchase,
    and we will "weigh" our decisions. If one is interesting and cheaper, it will
    weigh more or less in our decision, for example.
  prefs: []
  type: TYPE_NORMAL
- en: The children in our case agree on purchasing at least something, so from now
    on, *w*[3] = *w*[2], *w*[4] = *w*[1]. The younger and elder child will thus share
    some of the decision weights.
  prefs: []
  type: TYPE_NORMAL
- en: Now, somebody has to be an influencer. Let's leave this hard task to the elder
    child. The elder child, being more reasonable, will continuously deliver the bad
    news. You have to subtract something from your choice, represented by a minus
    (–) sign.
  prefs: []
  type: TYPE_NORMAL
- en: Each time they reach the point *h*[i], the eldest child applies a critical negative
    view on purchasing packs of candy. It's –*w* of everything comes up to be sure
    not to go over the budget. The opinion of the elder child is biased, so let's
    call the variable a bias, *b*[1]. Since the younger child's opinion is biased
    as well, let's call this view a bias too, *b*[2]. Since the eldest child's view
    is always negative, –*b*[1] will be applied to all of the eldest child's thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we apply this decision process to their view, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *y*[1] * –*b*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[2] = *y*[2] * *b*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we just have to use the same result. If the result is >=1, then the threshold
    has been reached. The threshold is calculated as shown in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *h*[1] + *h*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first start effectively finding the weights, starting by setting the
    weights and biases to 0.5, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[1] = 0.2; *w*[2] = 0.5; *b*[1] = 0.5'
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[3] = *w*[2]; *w*[4] = *w*[1]; *b*[2] = *b*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: It's not a full program yet, but its theory is done.
  prefs: []
  type: TYPE_NORMAL
- en: Only the communication going on between the two children is making the difference;
    we will focus on only modifying *w*[2] and *b*[1] after a first try. It works
    on paper after a few tries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now write the basic mathematical function, which is, in fact, the program
    itself on paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let's go from the solution on paper to Python.
  prefs: []
  type: TYPE_NORMAL
- en: Why wasn't this deceivingly simple solution found in 1969? Because *it seems
    simple today but wasn't so at that time*, like all inventions found by our genius
    predecessors. Nothing is easy at all in artificial intelligence and mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll stick with the solution proposed here, and implement
    it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a vintage XOR solution in Python with an FNN and backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To stay in the spirit of a 1969 vintage solution, we will not use NumPy, TensorFlow,
    Keras, or any other high-level library. Writing a vintage FNN with backpropagation
    written in high-school mathematics is fun.
  prefs: []
  type: TYPE_NORMAL
- en: If you break a problem down into very elementary parts, you understand it better
    and provide a solution to that specific problem. You don't need to use a huge
    truck to transport a loaf of bread.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, by thinking through the minds of children, we went against running
    20,000 or more episodes in modern CPU-rich solutions to solve the XOR problem.
    The logic used proves that both inputs can have the same parameters as long as
    one bias is negative (the elder reasonable critical child) to make the system
    provide a reasonable answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic Python solution quickly reaches a result in a few iterations, approximately
    10 iterations (epochs or episodes), depending on how we think it through. An epoch
    can be related to a try. Imagine looking at somebody practicing basketball:'
  prefs: []
  type: TYPE_NORMAL
- en: The person throws the ball toward the hoop but misses. That was an epoch (an
    episode can be used as well).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The person thinks about what happened and changes the way the ball will be thrown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This improvement is what makes it a learning epoch (or episode). It is not a
    simple memoryless try. Something is really happening to improve performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The person throws the ball again (next epoch) and again (next epochs) until
    the overall performance has improved. This is how a neural network improves over
    epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FNN_XOR_vintage_tribute.py` contains (at the top of the code) a result matrix
    with four columns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each element of the matrix represents the status (`1` = correct, `0` = false)
    of the four predicates to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train` variable is the number of predicates to solve: (0, 0), (1, 1),
    (1, 0), (0, 1). The variable of the predicate to solve is `pred`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the program is practically a copy of the sheet of paper we wrote,
    as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`pred` is an argument of the function from `1` to `4`. The four predicates
    are represented in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predicate (pred)** | **x[1]** | **x[2]** | **Expected result** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: That is why *y* must be <1 for predicates 0 and 1\. Then, *y* must be >=1 for
    predicates 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to call the following function limiting the training to 50 epochs,
    which are more than enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At the first epoch, the weights and biases are all set to `0.5`. No use thinking!
    Let the program do the job. As explained previously, the weight and bias of `x2`
    are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the hidden layers and `y` calculation function are called four times,
    one for each predicate to train, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, the system must train. To do that, we need to measure the number of predictions,
    1 to 4, that are correct at each iteration and decide how to change the weights/biases
    until we obtain proper results. We'll do that in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: A simplified version of a cost function and gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Slightly more complex gradient descent will be described in the next chapter.
    In this chapter, only a one-line equation will do the job. The only thing to bear
    in mind as an unconventional thinker is: *so what?* The concept of gradient descent
    is minimizing loss or errors between the present result and a goal to attain.'
  prefs: []
  type: TYPE_NORMAL
- en: First, a cost function is needed.
  prefs: []
  type: TYPE_NORMAL
- en: There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We need to
    find out how many are correctly trained at each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: The cost function will measure the difference between the training goal (4)
    and the result of this epoch or training iteration (result).
  prefs: []
  type: TYPE_NORMAL
- en: When 0 convergence is reached, it means the training has succeeded.
  prefs: []
  type: TYPE_NORMAL
- en: '`result[0,0,0,0]` contains a `0` for each value if none of the four predicates
    have been trained correctly. `result[1,0,1,0]` means two out of the four predicates
    are correct. `result[1,1,1,1]` means that all four predicates have been trained
    and that the training can stop. `1`, in this case, means that the correct training
    result was obtained. It can be `0` or `1`. The `result` array is the result counter.'
  prefs: []
  type: TYPE_NORMAL
- en: The cost function will express this training by having a value of `4`, `3`,
    `2`, `1`, or `0` as the training goes down the slope to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent measures the value of the descent to find the direction of
    the slope: up, down, or 0\. Then, once you have that slope and the steepness of
    it, you can optimize the weights. A derivative is a way to know whether you are
    going up or down a slope.'
  prefs: []
  type: TYPE_NORMAL
- en: Each time we move up or down the slope, we check to see whether we are moving
    in the right direction. We will assume that we will go one step at a time. So
    if we change directions, we will change our pace by one step. That one step value
    is our **learning rate**. We will measure our progression at each step. However,
    if we feel comfortable with our results, we might walk 10 steps at a time and
    only check to see if we are on the right track every 10 steps. Our learning rate
    will thus have increased to 10 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we hijacked the concept and used it to set the learning rate
    to `0.05` with a one-line function. Why not? It helped to solve gradient descent
    optimization in one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By applying the vintage children-buying-candy logic to the whole XOR problem,
    we found that only `w2` needed to be optimized. That's why `b1=w2`. That's because
    `b1` is doing the tough job of saying something negative (`-`) all the time, which
    completely changes the course of the resulting outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rate is set at `0.05`, and the program finishes training in 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is a logical *yes* or *no* problem. The way the network is built is pure
    logic. Nothing can stop us from using whatever training rates we wish. In fact,
    that's what gradient descent is about. There are many gradient descent methods.
    If you invent your own and it works for your solution, that is fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This one-line code is enough, in this case, to see whether the slope is going
    down. As long as the slope is negative, the function is going downhill to *cost*
    = 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram sums up the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_03.png](img/B15438_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: A feedforward neural network model (FNN)'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all of the arrows of the layers go forward in this "feedforward"
    neural network. However, the arrow that stems from the *y* node and goes backward
    can seem confusing. This line represents a change in weights to train the model.
    This means that we go back to changing the weights and running the network for
    another epoch (or episode). The system is adjusting its weights epoch by epoch
    until the overall result is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Too simple? Well, it works, and that's all that counts in real-life development.
    If your code is bug-free and does the job, then that's what matters.
  prefs: []
  type: TYPE_NORMAL
- en: Finding a simple development tool means nothing more than that. It's just another
    tool in the toolbox. We can get this XOR function to work on a neural network
    and generate income.
  prefs: []
  type: TYPE_NORMAL
- en: Companies are not interested in how smart you are but how efficient (profitable)
    you can be.
  prefs: []
  type: TYPE_NORMAL
- en: 'A company''s survival relies on multiple constraints: delivering on time, offering
    good prices, providing a product with a reasonable quality level, and many more
    factors besides.'
  prefs: []
  type: TYPE_NORMAL
- en: When we come up with a solution, it is useless to show how smart we can be writing
    tons of code. Our company or customers expect an efficient solution that will
    run well and is easy to maintain. In short, focus on efficiency. Once we have
    a good solution, we need to show that it works. In this case, we proved that linear
    separability was achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Linear separability was achieved
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bear in mind that the whole purpose of this feedforward network with backpropagation
    through a cost function was to transform a linear non-separable function into
    a linearly separable function to implement the classification of features presented
    to the system. In this case, the features had a `0` or `1` value.
  prefs: []
  type: TYPE_NORMAL
- en: One of the core goals of a layer in a neural network is to make the input make
    sense, meaning to be able to separate one kind of information from another.
  prefs: []
  type: TYPE_NORMAL
- en: '`h1` and `h2` will produce the Cartesian coordinate linear separability training
    axis, as implemented in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the program provides a view of the nonlinear input values once the
    hidden layers have trained them. The nonlinear values then become linear values
    in a linearly separable function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The intermediate result and goal are not a bunch of numbers on a screen to
    show that the program works. The result is a set of Cartesian values that can
    be represented in the following linearly separated graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_04-1.png](img/B15438_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Linearly separable patterns'
  prefs: []
  type: TYPE_NORMAL
- en: We have now obtained a separation between the top values, representing the intermediate
    values of the (1, 0) and (0, 1) inputs, and the bottom values, representing the
    (1, 1) and (0, 0) inputs. The top values are separated from the bottom values
    by a clear line. We now have *clouds* on top and *trees* below the line that separates
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers of the neural network have transformed nonlinear values into linearly
    separable values, making classification possible through standard separation equations,
    such as the one in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The ability of a neural network to make non-separable information separable
    and classifiable represents one of the core powers of deep learning. From this
    technique, many operations can be performed on data, such as subset optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at a practical application for our FNN XOR solution.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the FNN XOR function to optimizing subsets of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are more than 7.5 billion people breathing air on this planet. In 2050,
    there might be 2.5 billion more of us. All of these people need to wear clothes
    and eat. Just those two activities involve classifying data into subsets for industrial
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouping** is a core concept for any production. Production relating to producing
    clothes and food requires grouping to optimize production costs. Imagine not grouping
    and delivering one T-shirt at a time from one continent to another instead of
    grouping T-shirts in a container and grouping many containers (not just two on
    a ship). Let''s focus on clothing, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: A chain of stores needs to replenish the stock of clothing in each store as
    the customers purchase their products. In this case, the corporation has 10,000
    stores. The brand produces jeans, for example. Their average product is a faded
    jean. This product sells a slow 50 units a month per store. That adds up to 10,000
    stores × 50 units = 500,000 units or stock-keeping units (SKUs) per month. These
    units are sold in all sizes, grouped into average, small, and large. The sizes
    sold per month are random.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main factory for this product has about 2,500 employees producing those
    jeans at an output of about 25,000 jeans per day. The employees work in the following
    main fields: cutting, assembling, washing, lasering, packaging, and warehousing.'
  prefs: []
  type: TYPE_NORMAL
- en: The first difficulty arises with the purchase and use of fabric. The fabric
    for this brand is not cheap. Large amounts are necessary. Each pattern (the form
    of pieces of the pants to be assembled) needs to be cut by wasting as little fabric
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an empty box you want to fill up to optimize the volume. If
    you only put soccer balls in it, there will be a lot of space. If you slip tennis
    balls in the empty spaces, space will decrease. If, on top of that, you fill the
    remaining empty spaces with ping pong balls, you will have optimized the available
    space in the box.
  prefs: []
  type: TYPE_NORMAL
- en: Building optimized subsets can be applied to containers, warehouse flows and
    storage, truckload optimizing, and almost all human activities.
  prefs: []
  type: TYPE_NORMAL
- en: In the apparel business, if 1% to 10% of the fabric is wasted while manufacturing
    jeans, the company will survive the competition. At over 10%, there is a real
    problem to solve. Losing 20% of all the fabric consumed in manufacturing jeans
    can bring the company down and force it into bankruptcy.
  prefs: []
  type: TYPE_NORMAL
- en: The main rule is to combine larger pieces and smaller pieces to make optimized
    cutting patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of space through larger and smaller objects can be applied to cutting
    the forms, which are the patterns of the jeans, for example. Once they are cut,
    they will be assembled at the sewing stations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem can be summed up as:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating subsets of the 500,000 SKUs to optimize the cutting process for the
    month to come in a given factory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sure that each subset contains smaller sizes and larger sizes to minimize
    the loss of fabric by choosing 6 sizes per day to build 25,000 unit subsets per
    day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating cut plans of an average of 3 to 6 sizes per subset per day for a
    production of 25,000 units per day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In mathematical terms, this means trying to find subsets of sizes among 500,000
    units for a given day.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is to find 6 well-matched sizes among 500,000 units, as shown in the
    following combination formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, most people abandon the idea and find some easy way out of this,
    even if it means wasting fabric.
  prefs: []
  type: TYPE_NORMAL
- en: The first reaction we all have is that this is more than the number of stars
    in the universe and all that hype. However, that's not the right way to look at
    it at all. The right way is to look exactly in the opposite direction.
  prefs: []
  type: TYPE_NORMAL
- en: The key to this problem is to observe the particle at a microscopic level, at
    the **bits of information** level. Analyzing detailed data is necessary to obtain
    reliable results. This is a fundamental concept of machine learning and deep learning.
    Translated into our field, it means that to process an image, ML and DL process
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, even if the pictures to process represent large quantities, it will come
    down to small units of information to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '| yottabyte (YB) | 10^(24) | yobibyte (YiB) | 2^(80) |'
  prefs: []
  type: TYPE_TB
- en: It might be surprising to see these large numbers appear suddenly! However,
    when trying to combine thousands of elements, the combinations become exponential.
    When you extend this to the large population that major apparel brands have to
    deal with, it becomes rapidly exponential as well.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Google, Facebook, Amazon, and others have yottabytes of data to classify
    and make sense of. Using the term **big data** doesn't mean much. It's just a
    lot of data, and so what?
  prefs: []
  type: TYPE_NORMAL
- en: You do not need to analyze the individual positions of each data point in a
    dataset but use the probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: To understand that, let's go to a store to buy some jeans for a family. One
    of the parents wants a pair of jeans, and so does a teenager in that family. They
    both go and try to find their size in the pair of jeans they want. The parent
    finds 10 pairs of jeans in size *x*. All of the jeans are part of the production
    plan. The parent picks one at *random*, and the teenager does the same. Then they
    pay for them and take them home.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some systems work fine with random choices: random transportation (taking jeans
    from the store to home) of particles (jeans, other product units, pixels, or whatever
    is to be processed), making up that fluid (a dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: Translated into our factory, this means that a stochastic (random) process can
    be introduced to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that was required is that small and large sizes were picked at random among
    the 500,000 units to produce. If 6 sizes from 1 to 6 were to be picked per day,
    the sizes could be classified as follows in a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Smaller sizes* = *S* = {1, 2, 3}'
  prefs: []
  type: TYPE_NORMAL
- en: '*Larger sizes* = *L* = {4, 5, 6}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting this into numerical subset names, *S* = 1 and *L* = 6\. By selecting
    large and small sizes to produce at the same time, the fabric will be optimized,
    as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Size of choice 1** | **Size of choice 2** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: You will notice that the first two lines contain the same value. This will not
    optimize fabric consumption. If you put only large size 6 products together, there
    will be "holes" in the pattern. If you only put small size 1 products together,
    then they will fill up all of the space and leave no room for larger products.
    Fabric cutting is optimal when large and small sizes are present on the same roll
    of fabric.
  prefs: []
  type: TYPE_NORMAL
- en: Doesn't this sound familiar? It looks exactly like our vintage FNN, with 1 instead
    of 0 and 6 instead of 1.
  prefs: []
  type: TYPE_NORMAL
- en: All that has to be done is to stipulate that subset *S* = *value* 0, and subset
    *L* = *value* 1; and the previous code can be generalized.
  prefs: []
  type: TYPE_NORMAL
- en: '`FFN_XOR_generalization.py` is the program that generalizes the previous code,
    as shown in the following snippet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If this works, then smaller and larger sizes will be chosen to send to the
    cut planning department, and the fabric will be optimized. Applying the randomness
    concept of Bellman''s equation, a stochastic process is applied, choosing customer
    unit orders at random (each order is one size and a unit quantity of 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The weights and bias are now constants obtained by the result of the XOR training
    FNN. The training is over; the FNN is now used to provide results. Bear in mind
    that the word *learning* in machine learning and deep learning doesn't mean you
    have to train systems forever. In stable environments, training is run only when
    the datasets change. At one point in a project, you are hopefully using deep *trained*
    systems and not simply exploring the training phase of a deep *learning* process.
    The goal is not to spend all corporate resources on learning but on using trained
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning architecture must rapidly become deep trained models to produce
    a profit.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this prototype validation, the size of a given order is random. `0` means
    the order fits in the *S* subset; `1` means the order fits in the *L* subset.
    The data generation function reflects the random nature of consumer behavior in
    the following six-size jeans consumption model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once two customer orders have been chosen at random in the correct size category,
    the FNN is activated and runs like the previous example. Only the `result` array
    has been changed since we are using the same core program. Only a yes (`1`) or
    no (`0`) is expected, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The number of subsets to produce needs to be calculated to determine the volume
    of positive results required.
  prefs: []
  type: TYPE_NORMAL
- en: The choice is made of 6 sizes among 500,000 units. But, the request is to produce
    a daily production plan for the factory. The daily production target is 25,000\.
    Also, each subset can be used about 20 times. There is always, on average, 20
    times the same size in a given pair of jeans available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Six sizes are required to obtain good fabric optimization. This means that
    after three choices, the result represents one subset of potential optimized choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R* = 120 × 3 subsets of two sizes = 360'
  prefs: []
  type: TYPE_NORMAL
- en: The magic number has been found. For every 3 choices, the goal of producing
    6 sizes multiplied by a repetition of 20 will be reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'The production-per-day request is 25,000:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of subsets requested = 25000/3=8333\. 333
  prefs: []
  type: TYPE_NORMAL
- en: 'The system can run 8,333 products as long as necessary to produce the volume
    of subsets requested. In this case, the range is set to a sample of 1,000,000
    products. It can be extended or reduced when needed. The system is filtering the
    correct subsets through the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When the 8,333 subsets have been found respecting the smaller-larger size distribution,
    the system stops, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This example proves the point. *Simple solutions can solve very complex problems.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main functions, among some minor ones, must be added:'
  prefs: []
  type: TYPE_NORMAL
- en: After each choice, the orders chosen must be removed from the 500,000-order
    dataset. When an order has been selected, processing it again will generate errors
    in the global results. This will preclude choosing the same order twice and reduce
    the number of choices to be made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimization function to regroup the results for production purposes, for
    example. The idea is not to run through the records randomly, but to organize
    them by sets. This way, each set can be controlled independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application information:'
  prefs: []
  type: TYPE_NORMAL
- en: The core calculation part of the application is fewer than 50 lines long.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a few control functions and arrays, the program might reach 200 lines maximum.
    The goal of the control functions is to check and see whether the results reach
    the overall goal. For example, every 1,000 records, a local result could be checked
    to see whether it fits the overall goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in easy maintenance for a team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the number of lines of code to create a powerful application can
    prove to be very efficient for many business problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a small neural network from scratch provides a practical view of the
    elementary properties of a neuron. We saw that a neuron requires an input that
    can contain many variables. Then, weights are applied to the values with biases.
    An activation function then transforms the result and produces an output.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks, even one- or two-layer networks, can provide real-life solutions
    in a corporate environment. A real-life business case was implemented using complex
    theory broken down into small functions. Then, these components were assembled
    to be as minimal and profitable as possible.
  prefs: []
  type: TYPE_NORMAL
- en: It takes talent to break a problem down into elementary parts and find a simple,
    powerful solution. It requires more effort than just typing hundreds to thousands
    of lines of code to make things work. A well-thought through algorithm will always
    be more profitable, and software maintenance will prove more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Customers expect quick-win solutions. Artificial intelligence provides a large
    variety of tools that satisfy that goal. When solving a problem for a customer,
    do not look for the best theory, but the simplest and fastest way to implement
    a profitable solution, no matter how unconventional it seems.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, an enhanced FNN perceptron solved a complex business problem.
    In the next chapter, we will explore a convolutional neural network (CNN). We
    will build a CNN with TensorFlow 2.x, layer by layer, to classify images.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can the perceptron alone solve the XOR problem? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the XOR function linearly non-separable? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the main goals of layers in a neural network is classification. (Yes
    | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is deep learning the only way to classify data? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A cost function shows the increase in the cost of a neural network. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can simple arithmetic be enough to optimize a cost function? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feedforward network requires inputs, layers, and an output. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feedforward network always requires training with backpropagation. (Yes |
    No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In real-life applications, solutions are only found by following existing theories.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear separability: [http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
