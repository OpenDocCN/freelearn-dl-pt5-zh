["```py\nDef _build_tf_config():\n    hosts = json.loads(os.getenv(\"SM_HOSTS\"))\n    current_host = os.getenv(\"SM_CURRENT_HOST\")\n    workers = hosts\n    def host_addresses(hosts, port=7777):\n        return [\"{}:{}\".format(host, port) for host in hosts]\n    tf_config = {\"cluster\": {}, \"task\": {}}\n    tf_config[\"cluster\"][\"worker\"] = host_addresses(workers)\n    tf_config[\"task\"] = {\"index\": workers.index(current_host), \"type\": \"worker\"}\n    os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\n```", "```py\n{\n    \"cluster\": \n    {\n        \"worker\": [\"algo-1:7777\", \"algo-2:7777\"]},\n        \"task\": {\"index\": 0, \"type\": \"worker\"\n    }\n}\n```", "```py\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(\n    communication_options=tf.distribute.experimental.CommunicationOptions(\n        implementation=tf.distribute.experimental.CollectiveCommunication.AUTO\n    )\n)\n```", "```py\n    with strategy.scope():\n        multi_worker_model = build_and_compile_cnn_model()\n```", "```py\nglobal_batch_size = args.batch_size_per_device * _get_world_size()\nmulti_worker_dataset = mnist_dataset(global_batch_size)\n```", "```py\nfrom sagemaker.tensorflow import TensorFlow\nps_instance_type = 'ml.p2.xlarge'\nps_instance_count = 2\nhyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\nestimator_ms = TensorFlow(\n                       source_dir='1_sources',\n                       entry_point='train_ms.py', \n                       role=role,\n                       framework_version='2.8',\n                       py_version='py39',\n                       disable_profiler=True,\n                       debugger_hook_config=False,\n                       hyperparameters=hyperparameters,\n                       instance_count=ps_instance_count, \n                       instance_type=ps_instance_type,\n                       )\nestimator_ms.fit()\n```", "```py\ndistribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\n```", "```py\n    def _initiate_hvd():\n        hvd.init()\n        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        if gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n    ```", "```py\n    train_dataset = train_dataset.shard(hvd.size(), hvd.rank())\n    ```", "```py\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001 * hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    ```", "```py\n        hvd_model.fit(\n            shareded_by_rank_dataset,\n            epochs=args.epochs,\n            steps_per_epoch=args.steps_per_epoch // hvd.size(),\n            callbacks=callbacks,\n        )\n    ```", "```py\nfrom sagemaker.tensorflow import TensorFlow\nps_instance_type = 'ml.p2.xlarge'\nps_instance_count = 2\ndistribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\nhyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\nestimator_hvd = TensorFlow(\n                       source_dir='1_sources',\n                       entry_point='train_hvd.py', \n                       role=role,\n                       framework_version='2.8',\n                       py_version='py39',\n                       disable_profiler=True,\n                       debugger_hook_config=False,\n                       hyperparameters=hyperparameters,\n                       instance_count=ps_instance_count, \n                       instance_type=ps_instance_type,\n                       distribution=distribution\n                       )\nestimator_hvd.fit()\n```", "```py\n        nodes = json.loads(os.getenv(\"SM_HOSTS\"))\n        nnodes = len(nodes)\n        node_rank = nodes.index(os.getenv(\"SM_CURRENT_HOST\"))\n        nproc_per_node = os.getenv(\"SM_NUM_GPUS\", 1)\n    ```", "```py\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"torch.distributed.run\",\n            f\"--nproc_per_node={nproc_per_node}\",\n            f\"--nnodes={str(nnodes)}\",\n            f\"--node_rank={node_rank}\",\n            f\"--rdzv_id={os.getenv('SAGEMAKER_JOB_NAME')}\",\n            \"--rdzv_backend=c10d\",\n            f\"--rdzv_endpoint={nodes[0]}:{RDZV_PORT}\",\n            distr_args.train_script,\n        ]\n        # Adding training hyperparameters which will then be passed in training script\n        cmd.extend(training_hyperparameters)\n    ```", "```py\n        process = subprocess.Popen(cmd, env=os.environ)\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n    ```", "```py\n    dist.init_process_group(\n        backend=\"nccl\",\n        rank=int(os.getenv(\"RANK\", 0)),\n        world_size=int(os.getenv(\"WORLD_SIZE\", 1)),\n    )\n    ```", "```py\n    torch.cuda.set_device(os.getenv(\"LOCAL_RANK\"))\n    device = torch.device(\"cuda\")\n    model = model.to(device)\n    ```", "```py\n    model = DDP(model)\n    args.batch_size //= dist.get_world_size()\n    args.batch_size = max(args.batch_size, 1)\n    ```", "```py\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            image_datasets[\"train\"], num_replicas=args.world_size, rank=args.rank\n        )\n        train_loader = torch.utils.data.DataLoader(\n            image_datasets[\"train\"],\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=0,\n            pin_memory=True,\n            sampler=train_sampler,\n        ) \n    ```", "```py\nfrom sagemaker.pytorch import PyTorch\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\nhyperparameters = {\n  'train-script': 'train_ddp.py',\n  'epochs': 25,\n  }\nestimator_ms = PyTorch(\n                       source_dir='2_sources',\n                       entry_point='launcher.py', \n                       role=role,\n                       framework_version='1.9',\n                       py_version='py38',\n                       disable_profiler=True,\n                       debugger_hook_config=False,\n                       hyperparameters=hyperparameters,\n                       instance_count=ps_instance_count, \n                       instance_type=ps_instance_type,\n                       )\nestimator_ms.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"})\n```", "```py\ndistribution = { \n    \"smdistributed\": { \n        \"dataparallel\": {\n            \"enabled\": True, \n            \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\"\n        }\n    }\n}\n```", "```py\nimport smdistributed.dataparallel.torch.torch_smddp\nimport torch.distributed as dist\ndist.init_process_group(backend='smddp')\n```", "```py\nfrom sagemaker.pytorch import PyTorch\ninstance_type = 'ml.p3.16xlarge'\ninstance_count = 2\ndistribution = { \n    \"smdistributed\": { \n        \"dataparallel\": {\n            \"enabled\": True, \n            \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\"\n        }\n    }\n}\nsm_dp_estimator = PyTorch(\n          entry_point=\"train_sm_dp.py\",\n          source_dir='3_sources',\n          role=role,\n          instance_type=instance_type,\n          sagemaker_session=sagemaker_session,\n          framework_version='1.10',\n          py_version='py38',\n          instance_count=2,\n          hyperparameters={\n              \"batch-size\":64,\n              \"epochs\":25,\n          },\n          disable_profiler=True,\n          debugger_hook_config=False,\n          distribution=distribution,\n          base_job_name=\"SM-DP\",\n      )\n```", "```py\nsmd_mp_estimator = PyTorch(\n# ... other job parameters are reducted for brevity\ninstance_count=2,\ninstance_type= 'ml.p3.16xlarge',          \ndistribution={\n                  \"modelparallel\": {\n                      \"enabled\":True,\n                      \"parameters\": {\n                          \"microbatches\": 8, \n                          \"placement_strategy\": \"cluster\", \n                          \"pipeline\": \"interleaved\",\n                          \"optimize\": \"speed\", \n                          \"partitions\": 2,\n                          \"auto_partition\": True,\n                          \"ddp\": True,\n                      }\n                  }\n              },\n            \"mpi\": {\n                    \"enabled\": True,\n                    \"processes_per_host\": 8,\n                    \"custom_mpi_options\": mpioptions \n              }\n```", "```py\n    import smdistributed.modelparallel.torch as smp\n    smp.init()\n    ```", "```py\n    logger.debug(\n    f\"Hello from global rank {smp.rank()}. \"\n          f\"Local rank {smp.local_rank()} and local size {smp.local_size()}. \"\n          f\"List of ranks where current model is stored {smp.get_mp_group()}. \"\n          f\"List of ranks with different replicas of the same model {smp.get_dp_group()}. \"\n          f\"Current MP rank {smp.mp_rank()} and MP size is {smp.mp_size()}. \"\n            f\"Current DP rank {smp.dp_rank()} and DP size is {smp.dp_size()}.\"\n        )\n    ```", "```py\n    [1,mpirank:0,algo-1]:INFO:__main__:Hello from global rank 0\\. Local rank 0 and local size 8\\. List of ranks where current model is stored [0, 1]. List of ranks with different replicas of the same model [0, 2, 4, 6, 8, 10, 12, 14]. Current MP rank 0 and MP size is 2\\. Current DP rank 0 and DP size is 8.\n    ```", "```py\n    torch.cuda.set_device(smp.local_rank())\n    device = torch.device(\"cuda\")\n    ```", "```py\n    model = smp.DistributedModel(model)\n    optimizer = smp.DistributedOptimizer(optimizer)\n    ```", "```py\n        dataloaders_dict = {}\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            image_datasets[\"train\"], num_replicas=sdmp_args.dp_size, rank=sdmp_args.dp_rank)\n        dataloaders_dict[\"train\"] = torch.utils.data.DataLoader(\n            image_datasets[\"train\"],\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=0,\n            pin_memory=True,\n            sampler=train_sampler,\n            drop_last=True,\n        )\n        dataloaders_dict[\"val\"] = torch.utils.data.DataLoader(\n            image_datasets[\"val\"],\n            batch_size=args.batch_size,\n            shuffle=False,\n            drop_last=True,\n        )\n    ```", "```py\n    @smp.step\n    def train_step(model, data, target, criterion):\n        output = model(data)\n        loss = criterion(output, target)\n        model.backward(loss)  #  instead of PyTorch loss.backward()\n        return output, loss\n    @smp.step\n    def test_step(model, data, target, criterion):\n        output = model(data)\n        loss = criterion(output, target)\n        return output, loss\n    ```", "```py\n    for epoch in range(num_epochs):\n            for phase in [\"train\", \"val\"]:\n                if phase == \"train\":\n                    model.train()  # Set model to training mode\n                else:\n                    model.eval()  # Set model to evaluate mode\n                for inputs, labels in dataloaders[phase]:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == \"train\"):\n                        if phase == \"train\":\n                            outputs, loss_mb = train_step(model, inputs, labels, criterion)\n                            loss = loss_mb.reduce_mean()\n                            optimizer.step()\n                        else:\n                            outputs, loss_mb = test_step(model, inputs, labels, criterion)\n                            loss = loss_mb.reduce_mean()\n    ```", "```py\n        if smp.dp_rank() == 0:\n            model_file_path = os.path.join(\n                os.environ[\"SM_MODEL_DIR\"], f\"finetuned-{args.model_name}-checkpoint.pt\"\n            )\n            model_dict = model.state_dict()  # save the full model\n            opt_dict = optimizer.state_dict()  # save the full optimizer state\n            smp.save(\n                {\"model_state_dict\": model_dict, \"optimizer_state_dict\": opt_dict},\n                model_file_path,\n                partial=False,\n            )\n    ```", "```py\n    model_state = torch.load('finetuned-resnet-checkpoint.pt')['model_state_dict']\n    model_ft = models.resnet18(pretrained=False)\n    num_ftrs = model_ft.fc.in_features\n    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n    model_ft.load_state_dict(model_state)\n    outputs = model_ft(inputs)\n    ```"]