["```py\nfrom sklearn.neural_network import BernoulliRBM\nrbm = BernoulliRBM()\n```", "```py\nfrom sklearn.neural_network import BernoulliRBM\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_size = x_train.shape[1]\noriginal_dim = image_size * image_size\nx_train = np.reshape(x_train, [-1, original_dim])\nx_test = np.reshape(x_test, [-1, original_dim])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\nrbm = BernoulliRBM(verbose=True)\n\nrbm.n_components = 100\nrbm.fit(x_train)\n```", "```py\n[BernoulliRBM] Iteration 1, pseudo-likelihood = -104.67, time = 12.84s\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -102.20, time = 13.70s\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -97.95, time = 13.99s\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -99.79, time = 13.86s\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -96.06, time = 14.03s\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -97.08, time = 14.06s\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -95.78, time = 14.02s\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -99.94, time = 13.92s\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -93.65, time = 14.10s\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -96.97, time = 14.02s\n```", "```py\nr = rbm.transform(x_test)\n```", "```py\nimport matplotlib.pyplot as plt\nimport umap\n\ny_ = list(map(int, y_test))\nX_ = rbm.transform(x_test)\n\nX_ = umap.UMAP().fit_transform(X_)\n\nplt.figure(figsize=(10,8))\nplt.title('UMAP of 100 RBM Learned Components on MNIST')\nplt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')\nplt.xlabel('$z_1$')\nplt.ylabel('$z_2$')\nplt.colorbar()\n```", "```py\nv = rbm.components_\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure()\ncnt = 1\nfor i in range(10):    #we look into the first ten digits of test set\n  x = x_test[i]\n  for j in range(10):  #we project and reuse as input ten times\n    plt.subplot(10, 10, cnt)\n    plt.imshow(x.reshape((28, 28)), cmap='gray')\n    x = rbm.gibbs(x) #here use current as input and use as input again\n    cnt += 1\nplt.show()\n```", "```py\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\ninpt_dim = 28*28    # 784 dimensions\nltnt_dim = 100      # 100 components\n\ninpt_vec = Input(shape=(inpt_dim,))\nencoder = Dense(ltnt_dim, activation='sigmoid') (inpt_vec)\nlatent_ncdr = Model(inpt_vec, encoder)\ndecoder = Dense(inpt_dim, activation='sigmoid') (encoder)\nautoencoder = Model(inpt_vec, decoder)\n\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\nautoencoder.fit(x_train, x_train, epochs=200, batch_size=1000)\n```", "```py\nimport matplotlib.pyplot as plt\nimport umap\n\ny_ = list(map(int, y_test))\nX_ = latent_ncdr.predict(x_test)\n\nX_ = umap.UMAP().fit_transform(X_)\n\nplt.figure(figsize=(10,8))\nplt.title('UMAP of 100 AE Learned Components on MNIST')\nplt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')\nplt.xlabel('$z_1$')\nplt.ylabel('$z_2$')\nplt.colorbar()\n```", "```py\nlatent_ncdr.get_weights()[0]\n```"]