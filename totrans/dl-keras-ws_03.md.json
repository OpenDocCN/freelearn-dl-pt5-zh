["```py\nInitialize all the weights (w) and biases (b) arbitrarily\nRepeat Until converge {\nCompute loss given w and b\nCompute derivatives of loss with respect to w (dw), and with respect to b (db) using backpropagation\nUpdate w to w – alpha * dw\nUpdate b to b – alpha * db\n}\n```", "```py\nimport numpy as np\nimport pandas as pd\nX = pd.read_csv('../data/tree_class_feats.csv')\ny = pd.read_csv('../data/tree_class_target.csv')\n# Print the sizes of the dataset\nprint(\"Number of Examples in the Dataset = \", X.shape[0])\nprint(\"Number of Features for each example = \", X.shape[1]) \nprint(\"Possible Output Classes = \", np.unique(y))\n```", "```py\nNumber of Examples in the Dataset = 10000\nNumber of Features for each example = 10\nPossible Output Classes = [0 1]\n```", "```py\n    from keras.models import Sequential\n    from tensorflow import random\n    np.random.seed(42)\n    random.set_seed(42)\n    model = Sequential()\n    ```", "```py\n    from keras.layers import Dense, Activation\n    model.add(Dense(10, activation='tanh', input_dim=10))\n    ```", "```py\n    model.add(Dense(5, activation='tanh'))\n    ```", "```py\n    model.add(Dense(1, activation='sigmoid'))\n    ```", "```py\n    model.compile(optimizer='sgd', loss='binary_crossentropy', \\\n                  metrics=['accuracy'])\n    model.summary()\n    ```", "```py\n    history = model.fit(X, y, epochs=100, batch_size=5, \\\n                        verbose=1, validation_split=0.2, \\\n                        shuffle=False)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    ```", "```py\n    y_predicted = model.predict(X.iloc[0:10,:])\n    ```", "```py\n     # print the predicted classes\n    print(\"Predicted probability for each of the \"\\\n          \"examples belonging to class 1: \"),\n    print(y_predicted)\n    print(\"Predicted class label for each of the examples: \"), \n    print(np.round(y_predicted))\n    ```", "```py\n    Predicted probability for each of the examples belonging to class 1:\n    [[0.00354007]\n     [0.8302744 ]\n     [0.00316998]\n     [0.95335543]\n     [0.99479216]\n     [0.00334176]\n     [0.43222323]\n     [0.00391936]\n     [0.00332899]\n     [0.99759173]\n    Predicted class label for each of the examples:\n    [[0.]\n     [1.]\n     [0.]\n     [1.]\n     [1.]\n     [0.]\n     [0.]\n     [0.]\n     [0.]\n     [1.]]\n    ```", "```py\n    # import required packages from Keras\n    from keras.models import Sequential \n    from keras.layers import Dense, Activation \n    import numpy as np\n    import pandas as pd\n    from tensorflow import random\n    from sklearn.model_selection import train_test_split\n    # import required packages for plotting\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline \n    import matplotlib.patches as mpatches\n    # import the function for plotting decision boundary\n    from utils import plot_decision_boundary\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    seed = 1\n    ```", "```py\n    feats = pd.read_csv('outlier_feats.csv')\n    target = pd.read_csv('outlier_target.csv')\n    print(\"X size = \", feats.shape)\n    print(\"Y size = \", target.shape)\n    print(\"Number of examples = \", feats.shape[0])\n    ```", "```py\n    plt.scatter(feats[:,0], feats[:,1], \\\n                s=40, c=Y, cmap=plt.cm.Spectral)\n    ```", "```py\n    plot_decision_boundary(lambda x: model.predict(x), \\\n                           X_train, y_train)\n    ```", "```py\nmodel = Sequential()\nmodel.add(Dense(8, activation='tanh', input_dim=2))\nmodel.add(Dense(4, activation='tanh'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='sgd', loss='binary_crossentropy')\nmodel.fit(X, y, epochs=epochs, batch_size=batch_size)\n```", "```py\nmodel.evaluate(X, y, batch_size=None, verbose=0)\n```", "```py\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', \\\n              metrics=['accuracy'])\nmodel.evaluate(X, y, batch_size=None, verbose=0)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, \\\ny_train, y_test = train_test_split(X, y, test_size=0.3, \\\n                                   random_state=None)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(8, activation='tanh', input_dim=2))\nmodel.add(Dense(4, activation='tanh'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='sgd', \\\n              loss='binary_crossentropy')\nmodel.fit(X_train, y_train, epochs=epochs, \\\n          batch_size=batch_size)\n```", "```py\nmodel.evaluate(X_train, y_train, batch_size=None, \\\n               verbose=0)\nmodel.evaluate(X_test, y_test, batch_size=None, \\\n               verbose=0)\n```", "```py\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test))\n```", "```py\nimport matplotlib.pyplot as plt \nimport matplotlib\n# plot training loss\nplt.plot(history.history['loss'])\n# plot test loss\nplt.plot(history.history['val_loss'])\n```", "```py\n    X = pd.read_csv('../data/HCV_feats.csv')\n    y = pd.read_csv('../data/HCV_target.csv')\n    ```"]