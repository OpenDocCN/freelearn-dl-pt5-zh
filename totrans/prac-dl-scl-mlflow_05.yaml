- en: '*Chapter 3*: Tracking Models, Parameters, and Metrics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that MLflow can support multiple scenarios through the life cycle of DL
    models, it is common to use MLflow's capabilities incrementally. Usually, people
    start with MLflow tracking since it is easy to use and can handle many scenarios
    for reproducibility, provenance tracking, and auditing purposes. In addition,
    tracking the history of a model from cradle to sunset not only goes beyond the
    data science experiment management domain but is also important for model governance
    in the enterprise, where business and regulatory risks need to be managed for
    using models in production. While the precise business values of tracking models
    in production are still evolving, the need for tracking a model's entire life
    cycle is unquestionable and growing. For us to be able to do this, we will begin
    this chapter by setting up a full-fledged local MLflow tracking server.
  prefs: []
  type: TYPE_NORMAL
- en: We will then take a deep dive into how we can track a model, along with its
    parameters and metrics, using MLflow's tracking and registry APIs. By the end
    of this chapter, you should feel comfortable using MLflow's tracking and registry
    APIs for various reproducibility and auditing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a full-fledged local MLflow tracking server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking model provenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking model metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the requirements you will need to follow the instructions
    provided in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Desktop: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch `lightning-flash`: 0.5.0.: [https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0](https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VS Code with the Jupyter Notebook extension: [https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following GitHub URL for the code for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WSL2: If you are a Microsoft Windows user, it is recommended to install WSL2
    to run the Bash scripts provided in this book: [https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a full-fledged local MLflow tracking server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027), *Getting Started with
    MLflow for Deep Learning*, we gained hands-on experience working with a local
    filesystem-based MLflow tracking server and inspecting the components of the MLflow
    experiment. However, there are limitations with a default local filesystem-based
    MLflow server as the model registry functionality is not available. The benefit
    of having a model registry is that we can register the model, version control
    the model, and prepare for model deployment into production. Therefore, this model
    registry will bridge the gap between offline experimentation and an online deployment
    production scenario. Thus, we need a full-fledged MLflow tracking server with
    the following stores to track the complete life cycle of a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backend store**: A relational database backend is needed to support MLflow''s
    storage of metadata (metrics, parameters, and many others) about the experiment.
    This also allows the query capability of the experiment to be used. We will use
    a MySQL database as a local backend store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact store**: An object store that can store arbitrary types of objects,
    such as serialized models, vocabulary files, figures, and many others. In a production
    environment, a popular choice is the AWS S3 store. We will use **MinIO** ([https://min.io/](https://min.io/)),
    a multi-cloud object store, as a local artifact store, which is fully compatible
    with the AWS S3 store API but can run on your laptop without you needing to access
    the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make this local setup as easy as possible, we will use the `docker-compose`
    ([https://docs.docker.com/compose/](https://docs.docker.com/compose/)) tool with
    one line of command to start and stop a local full-fledged MLflow tracking server,
    as described in the following steps. Note that Docker Desktop ([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/))
    must be installed and running on the machine before you can follow these steps.
    Docker helps build and share containerized applications and microservices. The
    following steps will launch the local MLflow tracking server inside your local
    Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the `chapter03` code repository for your local development environment:
    [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change directory to the `mlflow_docker_setup` subfolder, which can be found
    under the `chapter03` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the command is successful, you should see an output similar to the following
    on your screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A local full-fledged MLflow tracking server is up and running'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – A local full-fledged MLflow tracking server is up and running
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to `http://localhost/` to see the MLflow UI web page. Then, click the **Models**
    tab in the UI (*Figure 3.2*). Note that this tab would not work if you only had
    a local filesystem as the backend store for the MLflow tracking server. Hence,
    the MLflow UI''s backend is now running on the Docker container service you just
    started, not a local filesystem. Since this is a brand-new server, there are no
    registered models yet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – MLflow model registry UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – MLflow model registry UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to `http://localhost:9000/`,and the following screen (*Figure 3.3*) should
    appear for the MinIO artifact store web UI. Enter `minio` for `minio123` for `.env`
    file, under the `mlflow_docker_setup` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – MinIO Web UI login page and browser page after logging in'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – MinIO Web UI login page and browser page after logging in
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should have a full-fledged local MLflow tracking server
    running successfully! If you want to stop the server, simply type the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Docker-based MLflow tracking server will stop. We are now ready to use this
    local MLflow server to track model provenance, parameters, and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model provenance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Provenance** tracking for digital artifacts has been long studied in the
    literature. For example, when you''re using a piece of patient diagnosis data
    in the biomedical industry, people usually want to know where it comes from, what
    kind of processing and cleaning has been done to the data, who owns the data,
    and other history and lineage information about the data. The rise of ML/DL models
    for industrial and business scenarios in production makes provenance tracking
    a required functionality. The different granularities of provenance tracking are
    critical for operationalizing and managing not just the data science offline experimentation,
    but also before/during/after the model is deployed in production. So, what needs
    to be tracked for provenance?'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the open provenance tracking framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at a general provenance tracking framework to understand the big
    picture of why provenance tracking is a major effort. The following diagram is
    based on the **Open Provenance Model Vocabulary Specification** ([http://open-biomed.sourceforge.net/opmv/ns.html](http://open-biomed.sourceforge.net/opmv/ns.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – An open provenance tracking framework'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – An open provenance tracking framework
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, there are three important items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Artifacts**: Things that are produced or used by processes (**A1** and **A2**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processes**: Actions that are performed by using or producing artifacts (**P1**
    and **P2**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Causal relationships**: Edges or relationships between artifacts and processes,
    such as *used*, *wasGeneratedBy*, and *wasDerivedFrom* in the preceding diagram
    (**R1**, **R2**, and **R3**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intuitively, this **open provenance model** (**OPM**) framework allows us to
    ask the following 5W1H (five Ws and one H) questions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Types of provenance questions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Types of provenance questions
  prefs: []
  type: TYPE_NORMAL
- en: Having a systematic provenance framework and a set of questions will help us
    learn how to track model provenance and provide answers to these questions. This
    will motivate us when we implement MLflow model tracking in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MLflow model tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use an MLflow tracking server to answer most of these types of provenance
    questions if we implement both MLflow logging and registry for the DL model we
    use. First, let''s review what MLflow provides in terms of model provenance tracking.
    MLflow provides two sets of APIs for model provenance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logging API**: This allows each run of the experiment or a model pipeline
    to log the model artifact into the artifact store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Registry API**: This allows a centralized location to track the version of
    the model and the stages of the model''s life cycle (**None**, **Archived**, **Staging**,
    or **Production**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difference between Model Logging and Model Registry
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although every run of the experiment needs to be logged and the model needs
    to be saved in the artifact store, not every instance of the model needs to be
    registered in the model registry. That's because, for many early exploratory model
    experimentations, the model might not be good. Thus, it is not necessarily registered
    to track the version. Only when a model has good offline performance and becomes
    a candidate for promoting to production do we need to register it in the model
    registry to go through the model promotion process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although MLflow's official API documentation separates logging and registry
    into two components, we will refer to them together as model tracking functionality
    in MLflow in this book.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We already saw MLflow''s auto-logging for the DL model we built in [*Chapter
    2*](B18120_02_ePub.xhtml#_idTextAnchor027), *Getting Started with MLflow for Deep
    Learning*. Although auto-logging is powerful, there are two issues with the current
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not automatically register the model to the model registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not work out of the box for the logged model to work directly with the
    original input data (in our case, an English sentence) if you just follow MLflow's
    suggestion to use the `mlflow.pyfunc.load_model` API to load the logged model.
    This is a limitation that's probably due to the experimental nature of the current
    auto-logging APIs in MLflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s walk through an example to review MLflow''s capabilities and auto-logging''s
    limitations and how we can solve them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the following environment variables in your Bash terminal, where your
    MinIO and MySQL-based Docker component is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are using the same
    values that were defined in the `.env` file, under the `mlflow_docker_setup` folder.
    This is done to make sure that we are using the MLflow server that we set up previously.
    Since these environmental variables are session-based, we can also set up the
    following environment variables in the notebook''s code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding three lines of code can be found in this chapter''s notebook
    file, just after importing the required Python packages. Before you execute the
    notebook, make sure that you run the following commands to initialize the virtual
    environment, `dl_model`, which now has additional required packages defined in
    the `requirements.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you set up the `dl_model` virtual environment in the previous chapters, you
    can skip the first line on creating a virtual environment called `dl_model`. However,
    you still need to activate `dl_model` as the currently active virtual environment
    and then run `pip install -r requirements.txt` to install all the required Python
    packages. Once the `dl_model` virtual environment has been set up successfully,
    you may proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along with this model tracking implementation, check out the `dl_model_tracking.ipynb`
    notebook file in VS Code by going to this chapter''s GitHub repository: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that, in the fourth cell of the `dl_model_tracking.ipynb` notebook, we
    need to point it to the correct and new MLflow tracking URI that we just set up
    in the Docker and define a new experiment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will still use the auto-logging capabilities provided by MLflow but we will
    assign the run with a variable name, `dl_model_tracking_run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`dl_model_tracking_run` allows us to get the `run_id` parameter and other metadata
    about this run programmatically, as we will see in the next step. Once this code
    cell has been executed, we will have a trained model logged in the MLflow tracking
    server with all the required parameters and metrics. However, the model hasn''t
    been registered yet. We can find the logged experiment in the MLflow web UI, along
    with all the relevant parameters and metrics, at http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47\.
    You can find the model artifacts in the `http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/`
    to see the storage UI, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Model artifacts logged In the MinIO storage backend'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Model artifacts logged In the MinIO storage backend
  prefs: []
  type: TYPE_NORMAL
- en: The folder structure is similar to what we saw in [*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027),
    *Getting Started with MLflow for Deep Learning*, when we used a plain filesystem
    to store the model artifacts. However, here, we are using a **MinIO** bucket to
    store these model artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve the `run_id` parameter from `dl_model_tracking_run`, as well as other
    metadata, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print out something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the logged model by defining the logged model URI. This will allow
    us to reload the logged model at this specific location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `mlflow.pytorch.load_model` and the following `logged_model` URI to load
    the model back into memory and make a new prediction for a given input sentence,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will output a model prediction label, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: mlflow.pytorch.load_model versus mlflow.pyfunc.load_model
  prefs: []
  type: TYPE_NORMAL
- en: By default, and in the MLflow experiment tracking page's artifact section, if
    you have a logged model, MLflow will recommend using `mlflow.pyfunc.load_model`
    to load back a logged model for prediction. However, this only works for inputs
    such as a pandas DataFrame, NumPy array, or tensor; this does not work for an
    NLP text input. Since auto-logging for PyTorch lightning uses `mlflow.pytorch.log_model`
    to save the model, the correct way to load a logged model back is to use `mlflow.pytorch.load_model`,
    as we have shown here. This is because MLflow's default design is to use `mlflow.pyfunc.load_model`
    with standardization and a known limitation that can only accept input formats
    in terms of numbers. For text and image data, it requires a tokenization step
    as a preprocessing step. However, since the PyTorch model we saved here already
    performs tokenization as part of the serialized model, we can use the native `mlflow.pytorch.load_model`
    to directly load the model that accepts text as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully logged the model and loaded the model back to
    make a prediction. If we think this model is performing well enough, then we can
    register it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s register the model by using the `mlflow.register_model` API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Model registration success message'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Model registration success message
  prefs: []
  type: TYPE_NORMAL
- en: This shows that the model has been successfully registered as version 1 in the
    model registry, under the name `nlp_dl_model`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also find this registered model in the MLflow web UI by clicking `http://localhost/#/models/nlp_dl_model/versions/1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – MLflow tracking server web UI showing the newly registered model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – MLflow tracking server web UI showing the newly registered model
  prefs: []
  type: TYPE_NORMAL
- en: By default, a newly registered model's stage is **None**, as shown in the preceding
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: By having a model registered with a version number and stage label, we have
    laid the foundation for deployment to staging (also known as pre-production) and
    then production. We will discuss how to perform model deployment based on registered
    models later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have solved the two issues we raised at the beginning of
    this section regarding the limitations of auto-logging:'
  prefs: []
  type: TYPE_NORMAL
- en: How to load a logged DL PyTorch model using the `mlflow.pytorch.load_model`
    API instead of the `mlflow.pyfunc.load_model` API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to register a logged DL PyTorch model using the `mlflow.register_model`
    API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choices of MLflow DL Model Logging APIs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For DL models, the auto-logging for PyTorch only works for `mlflow.pyfunc.log_model`
    to log the model, especially when we need to have multi-step DL model pipelines.
    We will implement such custom MLflow model flavors later in this book. If you
    don''t want to use auto-logging for PyTorch, then you can directly use `mlflow.pytorch.log_model`.
    PyTorch''s auto-logging uses `mlflow.pytorch.log_model` inside its implementation
    (see the official MLflow open source implementation here: [https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we don''t want to use auto-logging, then we can use MLflow''s model logging
    API directly. This also gives us an alternative way to simultaneously register
    the model in one call. You can use the following line of code to both log and
    register the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that this line of code does not log any parameters or metrics of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we have not only logged many experiments and models in the tracking
    server for offline experimentation but also registered performant models for production
    deployment in the future with version control and provenance tracking. We can
    now answer some of the provenance questions that we posted at the beginning of
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Answers to model provenance questions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Answers to model provenance questions
  prefs: []
  type: TYPE_NORMAL
- en: The why and where provenance questions are yet to be fully answered but will
    be done so later in this book. This is because the why provenance question for
    the production model can only be tracked and logged when the model is ready for
    deployment, where we need to add comments and reasons to justify the model's deployment.
    The where provenance question can be answered fully when we have a multiple-step
    model pipeline. However, here, we only have a single-step pipeline, which is the
    simplest case. A multi-step pipeline contains explicitly separate modulized code
    to specify which step performs what functionality so that we can easily change
    the detailed implementation of any of the steps without changing the flow of the
    pipeline. In the next two sections, we will investigate how we can track metrics
    and the parameters of models without using auto-logging.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The default metric for the text classification model in the PyTorch `lightning-flash`
    package is **Accuracy**. If we want to change the metric to **F1 score** (a harmonic
    mean of precision and recall), which is a very common metric for measuring a classifier''s
    performance, then we need to change the configuration of the classifier model
    before we start the model training process. Let''s learn how to make this change
    and then use MLflow''s non-auto-logging API to log the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When defining the classifier variable, instead of using the default metric,
    we will pass a metric function called `torchmetrics.F1` as a variable, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This uses the built-in metrics function of `torchmetrics`, the `F1` module,
    along with the number of classes in the data we need to classify as a parameter.
    This makes sure that the model is trained and tested using this new metric. You
    will see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that the model training and testing were using the F1 score as the
    metric, not the default accuracy metric. For more information on how you can use
    `torchmetrics` for customized metrics, please consult its documentation site:
    [https://torchmetrics.readthedocs.io/en/latest/](https://torchmetrics.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to log all the metrics to the MLflow tracking server, including
    the training, validation, and testing metrics, we need to get all the current
    metrics by calling the trainer''s callback function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to cast all the metric values to `float` to make sure that they
    are compatible with the MLflow `log_metrics` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call MLflow''s `log_metrics` to log all the metrics in the tracking
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see the following metrics after using the F1 score as the classifier''s
    metric, which will be logged in MLflow''s tracking server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Using MLflow's `log_metrics` API gives us more control with additional lines
    of code, but if we are satisfied with its auto-logging capabilities, then the
    only thing we need to change is what metric we want to use for the model training
    and testing processes. In this case, we only need to define a new metric to use
    when declaring a new DL model (that is, use the F1 score instead of the default
    accuracy metric).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to track multiple model metrics simultaneously, such as the F1
    score, accuracy, precision, and recall, then the only thing you need to do is
    define a Python list of metrics you want to compute and track, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, in the model initialization statement, instead of passing a single metric
    to the `metrics` parameter, you can just pass the `list_of_metrics` Python list
    that we just defined, above the `metrics` parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'No more changes need to be made to the rest of the code. So, in the `dl_model-non-auto-tracking.ipynb`
    notebook (https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb),
    you will notice that the preceding line is commented out by default. However,
    you can uncomment it and then comment out the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, when you run the rest of the notebook, you will get the model testing
    reports, along with the following metrics, in the notebook''s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that the numbers for accuracy, F1, precision, and recall are
    the same. This is because, by default, `torchmetrics` uses a `torchmetrics` does
    not support a `none` method, which computes the metric for each class and returns
    the metric for each class, even for a binary classification model. So, this does
    not produce a single scalar number. However, you can always call scikit-learn's
    metrics API to compute an F1-score or other metrics based on the binary average
    method by passing two lists of values. Here, we can use `y_true` and `y_predict`,
    where `y_true` is the list of ground truth label values and `y_predict` is the
    list of model predicted label values. This can be a good exercise for you to try
    out as this is a common practice for all ML models, not special treatment for
    a DL model.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already seen, there are lots of benefits of using auto-logging in
    MLflow, but if we want to track additional model parameters, we can either use
    MLflow to log additional parameters on top of what auto-logging records, or directly
    use MLflow to log all the parameters we want without using auto-logging at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through a notebook without using MLflow auto-logging. If we want
    to have full control of what parameters will be logged by MLflow, we can use two
    APIs: `mlflow.log_param` and `mlflow.log_params`. The first one logs a single
    pair of key-value parameters, while the second logs an entire dictionary of key-value
    parameters. So, what kind of parameters might we be interested in tracking? The
    following answers this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`log_params` API to record them in the experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model parameters**: These parameters are learned during the model training
    process. For a DL model, these usually refer to the neural network weights that
    are learned during training. We don''t need to log these weight parameters individually
    since they are already in the logged DL model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s log these hyperparameters using MLflow''s `log_params` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note that here, we log the maximal number of epochs, the trainer's first optimizer's
    name, the optimizer's default parameters, and the overall classifier's hyperparameters
    (`classifier_model.hparams`). The one-line piece of code `mlflow.log_params(params)`
    logs all the key-value parameters in the `params` dictionary to the MLflow tracking
    server. If you see the following hyperparameters in the MLflow tracking server,
    then it means it works!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_03_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – MLflow tracking server web UI showing the logged model hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this list of parameters is more than what the auto-logger logs as
    we added additional hyperparameters to log in the experiment. If you want to log
    any other customized parameters, you can follow the same pattern in your experiment.
    The complete notebook, without the use of auto-logging, can be checked out in
    this chapter's GitHub repository at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: If you have reached this point in this chapter, then you have successfully implemented
    an MLflow tracking model and its metrics and parameters!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we set up a local MLflow development environment that has full
    support for backend storage and artifact storage using MySQL and the MinIO object
    store. This will be very useful for us when we develop MLflow-supported DL models
    in this book. We started by presenting the open provenance tracking framework
    and asked model provenance tracking questions that are of interest. We worked
    on addressing the issues of auto-logging and successfully registered a trained
    model by loading a trained model from a logged model in MLflow for prediction
    using the `mlflow.pytorch.load_model` API. We also experimented on how to directly
    use MLflow's `log_metrics`, `log_params`, and `log_model` APIs without auto-logging,
    which gives us more control and flexibility over how we can log additional or
    customized metrics and parameters. We were able to answer many of the provenance
    questions by performing model provenance tracking, as well as by providing a couple
    of the questions that require further study of using MLflow to track multi-step
    model pipelines and their deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue our learning journey in the next chapter and learn how to perform
    code and data tracking using MLflow, which will give us additional power to answer
    data and code-related provenance questions.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow Docker setup reference: [https://github.com/sachua/mlflow-docker-compose](https://github.com/sachua/mlflow-docker-compose)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow PyTorch autologging implementation: [https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/_pytorch_autolog.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow PyTorch model logging, loading, and registry documentation: [https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow parameters and metrics logging documentation: [https://www.mlflow.org/docs/latest/python_api/mlflow.html](https://www.mlflow.org/docs/latest/python_api/mlflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow model registry documentation: [https://www.mlflow.org/docs/latest/model-registry.html](https://www.mlflow.org/docs/latest/model-registry.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Digging into big provenance (with SPADE): [https://queue.acm.org/detail.cfm?id=3476885](https://queue.acm.org/detail.cfm?id=3476885)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to utilize `torchmetrics` and `lightning-flash`: [https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are precision, recall, and F1 score equal when using micro averaging in
    a multi-class problem? [https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
