- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Managing Deep Learning Datasets
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理深度学习数据集
- en: '**Deep learning** models usually require a considerable amount of training
    data to learn useful patterns. In many real-life applications, new data is continuously
    collected, processed, and added to the training dataset, so your models can be
    periodically retrained so that they can adjust to changing real-world conditions.
    In this chapter, we will look into SageMaker capabilities and other AWS services
    to help you manage your training data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**模型通常需要大量的训练数据来学习有用的模式。在许多现实应用中，新数据会持续收集、处理，并添加到训练数据集中，以便您的模型可以定期进行重新训练，从而适应不断变化的现实环境。在本章中，我们将探讨SageMaker的能力以及其他AWS服务，帮助您管理训练数据。'
- en: SageMaker provides a wide integration capability where you can use AWS general-purpose
    data storage services such as Amazon S3, Amazon EFS, and Amazon FSx for Lustre.
    Additionally, SageMaker has purpose-built storage for **machine learning** (**ML**)
    called SageMaker Feature Store. We will discuss when to choose one or another
    storage solution, depending on the type of data, consumption, and ingestion patterns.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了广泛的集成功能，您可以使用AWS通用数据存储服务，如Amazon S3、Amazon EFS和Amazon FSx for Lustre。此外，SageMaker还有专为**机器学习**（**ML**）设计的存储解决方案——SageMaker
    Feature Store。我们将讨论根据数据类型、消费和摄取模式选择存储解决方案的时机。
- en: In many cases, before you can use training data, you need to pre-process it.
    For instance, data needs to be converted into a specific format or datasets need
    to be augmented with modified versions of samples. In this chapter, we will review
    SageMaker Processing and how it can be used to process large-scale ML datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，在使用训练数据之前，您需要对其进行预处理。例如，数据需要转换成特定格式，或数据集需要通过修改后的样本版本进行增强。本章将回顾SageMaker
    Processing，以及如何使用它来处理大规模的机器学习（ML）数据集。
- en: We will close this chapter by looking at advanced techniques on how to optimize
    the data retrieval process for TensorFlow and PyTorch models using AWS data streaming
    utilities.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后，我们将探讨一些先进的技术，如何利用AWS数据流工具优化TensorFlow和PyTorch模型的数据检索过程。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Selecting storage solutions for ML datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为ML数据集选择存储解决方案
- en: Processing data at scale
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模处理数据
- en: Optimizing data storage and retrieval
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据存储和检索
- en: After reading this chapter, you will know how to organize your DL dataset’s
    life cycle for training and inference on SageMaker. We will also run through some
    hands-on examples for data processing and data retrieval to help you gain practical
    skills in those areas.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将知道如何组织您的深度学习（DL）数据集的生命周期，以便在SageMaker上进行训练和推理。我们还将通过一些动手示例来帮助您获得数据处理和数据检索方面的实际技能。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will provide code samples so that you can develop your practical
    skills. The full code examples are available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供代码示例，以便您可以开发实际技能。完整的代码示例可以在[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/)查看。
- en: 'To follow this code, you will need the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这段代码，您需要以下内容：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS账户和具有管理Amazon SageMaker资源权限的IAM用户
- en: A SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible
    environment established
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已建立的SageMaker笔记本、SageMaker Studio笔记本或本地兼容SageMaker环境
- en: Selecting storage solutions for ML datasets
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为ML数据集选择存储解决方案
- en: 'AWS Cloud provides a wide range of storage solutions that can be used to store
    inference and training data. When choosing an optimal storage solution, you may
    consider the following factors:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Cloud提供了一系列可以用来存储推理和训练数据的存储解决方案。在选择最佳存储解决方案时，您可以考虑以下因素：
- en: Data volume and velocity
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量和速度
- en: Data types and associated metadata
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型及相关元数据
- en: Consumption patterns
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费模式
- en: Backup and retention requirements
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和保留要求
- en: Security and audit requirements
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性和审计要求
- en: Integration capabilities
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成能力
- en: Price to store, write, and read data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储、写入和读取数据的价格
- en: Carefully analyzing your specific requirements may suggest the right solution
    for your use case. It’s also typical to combine several storage solutions for
    different stages of your data life cycle. For instance, you could store data used
    for inference consumption with lower latency requirements in faster but more expensive
    storage; then, you could move the data to cheaper and slower storage solutions
    for training purposes and long-term retention.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of common storage types with different characteristics:
    filesystems, object storage and block storage solutions. Amazon provides managed
    services for each type of storage. We will review their characteristics and how
    to use them in your SageMaker workloads in the following subsections. Then, we
    will focus on Amazon SageMaker Feature Store as it provides several unique features
    specific to ML workloads and datasets.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EBS – high-performance block storage
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Block storage solutions are designed for quick data retrieval and manipulation.
    The data is broken into blocks on the physical device for efficient utilization.
    Block storage allows you to abstract and decouple data from the runtime environment.
    At data retrieval time, blocks are reassembled by the storage solution and returned
    to users.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EBS is a fully managed block storage solution that supports a wide range
    of use cases for different read-write patterns, throughput, and latency requirements.
    A primary use case of Amazon EBS is to serve as data volumes attached to Amazon
    EC2 compute nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker provides seamless integration with EBS. In the following example,
    we are provisioning a training job with four nodes; each node will have an EBS
    volume with 100 GB attached to it. The training data will be downloaded and stored
    on EBS volumes by SageMaker:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that you cannot customize the type of EBS volume that’s used. Only **general-purpose
    SSD** volumes are supported. Once the training job is completed, all instances
    and attached EBS volumes will be purged.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3 – industry-standard object storage
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object storage implements a flat structure, where each file object has a unique
    identifier (expressed as a path) and associated data object. A flat structure
    allows you to scale object storage solutions linearly and sustain high-throughput
    data reads and writes while keeping costs low.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Object storage can handle objects of different types and sizes. Object storage
    also allows you to store metadata associated with each object. Data reads and
    writes are typically done via HTTP APIs, which allows for ease of integration.
    However, note that object storage solutions are generally slower than filesystems
    or block storage solutions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3 was the first petabyte-scale cloud object storage service. It offers
    durability, availability, performance, security, and virtually unlimited scalability
    at very low costs. Many object storage solutions follow the Amazon S3 API. Amazon
    S3 is used to store customer data, but it’s also used for many internal AWS features
    and services where data needs to be persisted.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 S3 是首个 PB 级别的云对象存储服务。它提供持久性、可用性、性能、安全性和几乎无限的可扩展性，同时成本非常低廉。许多对象存储解决方案都遵循亚马逊
    S3 的 API。亚马逊 S3 用于存储客户数据，但它也用于许多 AWS 内部功能和服务，其中数据需要持久化。
- en: 'SageMaker provides seamless integration with Amazon S3 for storing input and
    output objects, such as datasets, log streams, job outputs, and model artifacts.
    Let’s look at an example of a training job and how to define where we will store
    our inputs and outputs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 提供了与 Amazon S3 的无缝集成，用于存储输入和输出对象，如数据集、日志流、作业输出和模型工件。让我们看一个训练作业示例，以及如何定义我们将存储输入和输出的位置：
- en: The `model_uri` parameter specifies an S3 location of model artifacts, such
    as pre-trained weights, tokenizers, and others. SageMaker automatically downloads
    these artifacts to each training node.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_uri` 参数指定了模型工件的 S3 位置，例如预训练权重、分词器等。SageMaker 会自动将这些工件下载到每个训练节点。'
- en: '`checkpoint_s3_uri` defines the S3 location where training checkpoints will
    be uploaded during training. Note that it is the developer’s responsibility to
    implement checkpointing functionality in the training script.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint_s3_uri` 定义了训练过程中上传训练检查点的 S3 位置。请注意，实现检查点功能由开发者在训练脚本中负责。'
- en: '`output_path` specifies the S3 destination of all output artifacts that SageMaker
    will upload from training nodes after training job completion.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_path` 指定了 SageMaker 在训练作业完成后，从训练节点上传的所有输出工件的 S3 目标位置。'
- en: '`tensorboard_output_config` defines where to store TensorBoard logs on S3\.
    Note that SageMaker continuously uploads these logs during training job execution,
    so you can monitor your training progress in near real time in TensorBoard:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorboard_output_config` 定义了在 S3 上存储 TensorBoard 日志的位置。请注意，SageMaker 会在训练作业执行期间持续上传这些日志，因此你可以在
    TensorBoard 中实时监控训练进度：'
- en: '[PRE1]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We also use S3 to store our training dataset. Take a look at the following
    `estimator.fit()` method, which defines the location of our training data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 S3 来存储我们的训练数据集。请看以下 `estimator.fit()` 方法，它定义了我们训练数据的位置：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the `"train"` and `"test"` parameters are called `/opm/ml/input/data/{channel_name}`
    directory. Additionally, the training toolkit will create `SM_CHANNEL_{channel_name}`
    environment variables, which you can use in your training script to access model
    artifacts locally.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`"train"` 和 `"test"` 参数被称为 `/opm/ml/input/data/{channel_name}` 目录。此外，训练工具包将创建
    `SM_CHANNEL_{channel_name}` 环境变量，您可以在训练脚本中使用这些变量来本地访问模型工件。
- en: As shown in the preceding code block, Amazon S3 can be used to store the input
    and output artifacts of SageMaker training jobs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面代码块所示，Amazon S3 可用于存储 SageMaker 训练作业的输入和输出工件。
- en: File, FastFile, and Pipe modes
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件、快速文件和管道模式
- en: S3 storage is a common place to store your training datasets. By default, when
    working with data stored on S3, all objects matching the path will be downloaded
    to each compute node and stored on its EBS volumes. This is known as **File**
    mode.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储是一个常用的训练数据集存储位置。默认情况下，当使用存储在 S3 上的数据时，所有与路径匹配的对象将被下载到每个计算节点，并存储在其 EBS 卷中。这被称为
    **文件**模式。
- en: However, in many scenarios, training datasets can be hundreds of gigabytes or
    larger. Downloading such large files will take a considerable amount of time,
    even before your training begins. To reduce the time needed to start a training
    job, SageMaker supports **Pipe** mode, which allows you to stream data from the
    S3 location without fully downloading it. This allows you to start training jobs
    immediately and fetch data batches as needed during the training cycle.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多场景中，训练数据集可能有数百 GB 或更大。即使在训练开始之前，下载如此大的文件也需要相当长的时间。为了减少启动训练作业所需的时间，SageMaker
    支持 **管道**模式，允许你从 S3 位置流式传输数据，而无需完全下载。这使你能够立即启动训练作业，并在训练过程中根据需要获取数据批次。
- en: One of the drawbacks of Pipe mode is that it requires using framework-specific
    implementations of data utilities to stream data. The recently introduced **FastFile**
    mode addresses this challenge. FastFile mode allows you to stream data directly
    from S3 without the need to implement any specific data loaders. In your training
    or processing scripts, you can treat **FastFiles** as if they are regular files
    stored on disk; Amazon SageMaker will take care of the read and write operations
    for you.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: We will develop practical skills on how to organize training code for S3 streaming
    using **FastFile** and **Pipe** modes in the *Optimizing data storage and retrieval*
    section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: FullyReplicated and ShardedByKey
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many training and data processing tasks, we want to parallelize our job across
    multiple compute nodes. In scenarios where we have many data objects, we can split
    our tasks by splitting our full set of objects into unique subsets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: To implement such a scenario, SageMaker supports **ShardedByKey** mode, which
    attempts to evenly split all matching objects and deliver a unique subset of objects
    to each node. For instance, if you have *n* objects in your dataset and *k* compute
    nodes in your job, then each compute node will get a unique set of *n/k* objects.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, the default mode, **FullyReplicated**, is used when
    SageMaker downloads all matching objects to all nodes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: We will acquire practical skills on how to distribute data processing tasks
    in the *Distributed data processing* section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EFS – general-purpose shared filesystem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon EFS is a managed file storage service that is easy to set up and automatically
    scales up to petabytes. It provides a filesystem interface and file semantics
    such as file locking and strong consistency. Unlike Amazon EBS, which allows you
    to attach storage to a single compute node, EFS can be simultaneously used by
    hundreds or thousands of compute nodes. This allows you to organize efficient
    data sharing between nodes without the need to duplicate and distribute data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker allows you to use EFS to store training datasets. The following
    code shows an example of how to use Amazon EFS in the training job configuration
    using the `FileSystemInput` class. Note that in this case, we have configured
    read-only access to the data (the `ro` flag of the `file_system_access_mode` parameter),
    which is typically the case for the training job. However, you can also specify
    read-write permissions by setting `file_system_access_mode` to `rw`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, you can control other EFS resources. Depending on the latency requirements
    for data reads and writes, you can choose from several modes that define the latency
    and concurrency characteristics of your filesystem. At the time of writing this
    book, EFS can sustain 10+ GB per second throughput and scale up to thousands of
    connected compute nodes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Amazon FSx for Lustre – high-performance filesystem
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon FSx for Lustre is a file storage service optimized for ML and **high-performance
    computing** (**HPC**) workloads. It is designed for sub-millisecond latency for
    read and write operations and can provide hundreds of GB/s throughput. You can
    also choose to store data in S3 and synchronize it with the Amazon FSx for Lustre
    filesystem. In this case, an FSx system presents S3 objects as files and allows
    you to update data back to the S3 origin.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker supports storing training data in the FSx for Lustre filesystem.
    Training job configuration is similar to using the EFS filesystem; the only difference
    is that the `file_system_type` parameter is set to `FSxLustre`. The following
    code shows a sample training job:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that when provisioning your Lustre filesystem, you may choose either SSD
    or HDD storage. You should choose SSD for latency-sensitive workloads; HDD is
    a better fit for workloads with high-throughput requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Feature Store – purpose-built ML storage
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve discussed general-purpose file and object storage services that
    can be used to store data in your SageMaker workloads. However, real-life ML workflows
    may present certain challenges when it comes to feature engineering and data management,
    such as the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Managing the data ingestion pipeline to keep data up to date
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing data usage between different teams in your organization and eliminating
    duplicative efforts
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing data between inference and training workloads when needed
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing dataset consistency, its metadata and versioning
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ad hoc analysis of data
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these challenges, SageMaker provides an ML-specific data storage
    solution called **Feature Store**. It allows you to accelerate data processing
    and curation by reducing repetitive steps and providing a set of APIs to ingest,
    transform, and consume data for inference and model training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Its central concept is a **feature** – a single attribute of a data record.
    Each data record consists of one or many features. Additionally, data records
    contain metadata such as record update time, unique record ID, and status (deleted
    or not). The feature can be of the string, integer, or fractional type. Data records
    and their associated features can be organized into logical units called **feature
    groups**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review the key features of SageMaker Feature Store.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Online and offline storage
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature Store supports several storage options for different use cases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Offline storage** is designed to store your data in scenarios where data
    retrieval latency is not critical, such as storing data for training or batch
    inference. Your dataset resides in S3 storage and can be queried using the Amazon
    Athena SQL engine.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online storage** allows you to retrieve a single or batch of records with
    millisecond latency for real-time inference use cases.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offline and online storage** allows you to store the same data in both forms
    of storage and use it in both inference and training scenarios.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingestion Interfaces
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 导入接口
- en: There are several ways to get your data in Feature Store. One way is using Feature
    Store’s `PutRecord` API, which allows you to write either a single or a batch
    of records. This will write the records in both offline and online storage.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以将数据导入 Feature Store。其中一种方法是使用 Feature Store 的 `PutRecord` API，它允许您写入单条或批量记录。这将把记录写入离线存储和在线存储。
- en: Another option is to use a Spark connector. This is a convenient way to ingest
    data if you already have your Spark-based data processing pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用 Spark 连接器。如果您已经有基于 Spark 的数据处理管道，这是一种方便的数据导入方式。
- en: Analytical queries
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析查询
- en: When data is stored in offline storage, you can use Athena SQL to query the
    dataset using SQL syntax. This is helpful when you have a diverse team that has
    different levels of coding skills. As Feature Store contains useful metadata fields,
    such as **Event Time** and **Status**, you can use these times to run *time travel*
    queries, for instance, to get a historical snapshot of your dataset at a given
    point in time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据存储在离线存储中时，可以使用 Athena SQL 通过 SQL 语法查询数据集。这对于拥有不同编码技能水平的多元化团队非常有帮助。由于 Feature
    Store 包含有用的元数据字段，如**事件时间**和**状态**，您可以使用这些时间进行*时间旅行*查询，例如，获取数据集在某一时刻的历史快照。
- en: Feature discovery
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征发现
- en: 'Once data has been ingested into Feature Store, you can use SageMaker Studio
    to review and analyze datasets via an intuitive UI component without the need
    to write any code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被导入 Feature Store，您可以通过 SageMaker Studio 使用直观的 UI 组件查看和分析数据集，而无需编写任何代码：
- en: '![Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI ](img/B17519_04_01.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 通过 SageMaker Studio UI 发现 Feature Store 数据集](img/B17519_04_01.jpg)'
- en: Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 通过 SageMaker Studio UI 发现 Feature Store 数据集
- en: Now that we understand the value proposition of Feature Store compared to more
    general-purpose storage solutions, let’s see how it can be used for a typical
    DL scenario when we want to have tokenized text available next to its original
    form.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了 Feature Store 相比于更通用的存储解决方案的价值主张，接下来让我们看看它如何在典型的深度学习场景中使用，当我们希望将分词文本与原始文本并排存放时。
- en: Using Feature Store for inference and training
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行推理和训练
- en: In this practical example, we will develop skills on how to use SageMaker Feature
    Store to ingest, process, and consume datasets that contain IMDb reviews. We will
    take the original dataset that contains the reviews and run a custom BERT tokenizer
    to convert unstructured text into a set of integer tokens. Then, we will ingest
    the dataset with the tokenized text feature into Feature Store so that you don’t
    have to tokenize the dataset the next time we want to use it. After that, we will
    train our model to categorize positive and negative reviews.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实际示例中，我们将学习如何使用 SageMaker Feature Store 导入、处理和消费包含 IMDb 评论的数据集。我们将采用包含评论的原始数据集，并运行自定义
    BERT 分词器将非结构化文本转换为一组整数令牌。然后，我们将把包含分词文本特征的数据集导入 Feature Store，这样下次使用时就无需再进行分词了。之后，我们将训练模型以分类正面和负面评论。
- en: We will use SageMaker Feature Store SDK to interact with Feature Store APIs.
    We will use the HuggingFace Datasets ([https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/))
    and Transformers ([https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index))
    libraries to tokenize the text and run training and inference. Please make sure
    that these libraries are installed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 SageMaker Feature Store SDK 与 Feature Store APIs 进行交互。我们将使用 HuggingFace
    Datasets（[https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)）和
    Transformers（[https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)）库来对文本进行分词，并进行训练和推理。请确保已安装这些库。
- en: Preparing the data
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Follow these steps to prepare the data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤准备数据：
- en: 'The first step is to acquire the initial dataset that contains the IMDb reviews:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是获取包含 IMDb 评论的初始数据集：
- en: '[PRE5]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we must convert the dataset into a pandas DataFrame that is compatible
    with `EventTime` and `ID`. Both are required by Feature Store to support fast
    retrieval and feature versioning:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须将数据集转换为与`EventTime`和`ID`兼容的pandas DataFrame。这两个字段是Feature Store所必需的，以支持快速检索和特征版本控制：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let’s run the downloaded pre-trained tokenizer for the `Distilbert` model
    and add a new attribute, `tokenized-text`, to our dataset. Note that we cast `tokenized-text`
    to a string as SageMaker Feature Store doesn’t support collection data types such
    as arrays or maps:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行下载的 `Distilbert` 模型预训练的分词器，并向数据集添加一个新属性 `tokenized-text`。请注意，我们将 `tokenized-text`
    转换为字符串，因为 SageMaker Feature Store 不支持集合数据类型，如数组或映射：
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As a result, we have a pandas DataFrame object that contains the features we
    are looking to ingest into Feature Store.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个 pandas DataFrame 对象，里面包含了我们希望摄取到 Feature Store 中的特征。
- en: Ingesting the data
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据摄取
- en: 'The next step is to provision Feature Store resources and prepare them for
    ingestion. Follow these steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为 Feature Store 配置资源并准备摄取。请按照以下步骤操作：
- en: 'We will start by configuring the feature group and preparing feature definitions.
    Note that since we stored our dataset in a pandas DataFrame, Feature Store can
    use this DataFrame to infer feature types:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从配置特征组并准备特征定义开始。请注意，由于我们将数据集存储在 pandas DataFrame 中，Feature Store 可以使用该 DataFrame
    推断特征类型：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have prepared the feature group configuration, we are ready to
    create it. This may take several minutes, so let’s add a `Waiter`. Since we are
    planning to use both online and offline storage, we will set the `enable_online_store`
    flag to `True`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了特征组配置，接下来可以创建它。这个过程可能需要几分钟，所以我们来加一个 `Waiter`。由于我们打算同时使用在线存储和离线存储，我们将
    `enable_online_store` 标志设置为 `True`：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the group is available, we are ready to ingest data. Since we have a full
    dataset available, we will use a batch ingest API, as shown here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦组可用，我们就可以准备摄取数据。由于我们有完整的数据集，我们将使用批量摄取 API，如下所示：
- en: '[PRE10]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the data has been ingested, we can run some analytical queries. For example,
    we can check if we are dealing with a balanced or imbalanced dataset. As mentioned
    previously, Feature Store supports querying data using the Amazon Athena SQL engine:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被摄取后，我们可以运行一些分析查询。例如，我们可以检查数据集是平衡的还是不平衡的。如前所述，Feature Store 支持使用 Amazon Athena
    SQL 引擎查询数据：
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It will take a moment to run, but in the end, you should get a count of the
    labels in our dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个过程需要一些时间，但最终你应该能得到我们数据集中标签的数量。
- en: Using Feature Store for training
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行训练
- en: Now that we have data available, let’s train our binary classification model.
    Since data in Feature Store is stored in Parquet format ([https://parquet.apache.org/](https://parquet.apache.org/))
    in a designated S3 location, we can directly use Parquet files for training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了可用的数据，接下来让我们训练二分类模型。由于 Feature Store 中的数据以 Parquet 格式存储在指定的 S3 位置（[https://parquet.apache.org/](https://parquet.apache.org/)），我们可以直接使用
    Parquet 文件进行训练。
- en: 'To handle Parquet files, we need to make sure that our data reader is aware
    of the format. For this, we can use the pandas `.read_parquet()` method. Then,
    we can convert the pandas DataFrame object into the HuggingFace dataset and select
    the attributes of interest – `tokenized-text` and `label`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理 Parquet 文件，我们需要确保数据读取器能够识别这种格式。为此，我们可以使用 pandas 的 `.read_parquet()` 方法。然后，我们可以将
    pandas DataFrame 对象转换为 HuggingFace 数据集，并选择我们关心的属性——`tokenized-text` 和 `label`：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we need to convert `tokenized-text` from a string into a list of integers:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将 `tokenized-text` 从字符串转换为整数列表：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The rest of the training script is the same. You can find the full code at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的训练脚本是相同的。你可以在 [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py)
    找到完整代码。
- en: 'Now that we’ve modified the training script, we are ready to run our training
    job:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修改了训练脚本，准备好运行训练任务了：
- en: 'First, we must get the location of the dataset:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须获取数据集的位置：
- en: '[PRE14]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we must pass it to our `Estimator` object:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须将它传递给我们的 `Estimator` 对象：
- en: '[PRE15]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After some time (depending on how many epochs or steps you use), the model should
    be trained to classify reviews based on the input text.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间（取决于你使用的 epoch 或步骤数），模型应该已经训练好，可以根据输入的文本对评论进行分类。
- en: Using Feature Store for inference
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行推理
- en: 'For inference, we can use the Feature Store runtime client from the Boto3 library
    to fetch a single record or a batch:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，我们可以使用 Boto3 库中的 Feature Store 运行时客户端来获取单个记录或批量记录：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that you need to know the unique IDs of the records to retrieve them:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, you can send this request for inference to your deployed model. Refer
    to the following notebook for an end-to-end example: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed the options you can use to store your ML data for
    inference and training needs. But it’s rarely the case that data can be used “as-is.”
    In many scenarios, you need to continuously process data at scale before using
    it in your ML workloads. SageMaker Processing provides a scalable and flexible
    mechanism to process your data at scale. Let’s take a look.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Processing data at scale
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Processing allows you to run containerized code in the cloud. This
    is useful for scenarios such as data pre and post-processing, feature engineering,
    and model evaluation. SageMaker Processing can be useful for ad hoc workloads
    as well as recurrent jobs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of a training job, Amazon SageMaker provides a managed experience
    for underlying compute and data infrastructure. You will need to provide a processing
    job configuration, code, and the container you want to use, but SageMaker will
    take care of provisioning the instances and deploying the containerized code,
    as well as running and monitoring the job and its progress. Once your job reaches
    the terminal state (success or failure), SageMaker will upload the resulting artifacts
    to the S3 storage and deprovision the cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker Processing provides two pre-built containers:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A PySpark container with dependencies to run Spark computations
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scikit-learn container
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When selecting which built-in processing container, note that the PySpark container
    supports distributed Spark jobs. It allows you to coordinate distributed data
    processing in a Spark cluster, maintain it globally across the dataset, and visualize
    processing jobs via the Spark UI. At the same time, the scikit-learn container
    doesn’t support a shared global state, so each processing node runs independently.
    Limited task coordination can be done by sharding datasets into sub-datasets and
    processing each sub-dataset independently.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also provide a **Bring-Your-Own** (**BYO**) processing container with
    virtually any runtime configuration to run SageMaker Processing. This flexibility
    allows you to easily move your existing processing code so that it can run on
    SageMaker Processing with minimal effort:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – SageMaker Processing node ](img/B17519_04_02.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – SageMaker Processing node
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to build a container for processing and run a multi-node processing
    job to augment the image dataset for further training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting image data using SageMaker Processing
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will download the 325 Bird Species dataset from Kaggle
    ([https://www.kaggle.com/gpiosenka/100-bird-species/](https://www.kaggle.com/gpiosenka/100-bird-species/)).
    Then, we will augment this dataset with modified versions of the images (rotated,
    cropped, resized) to improve the performance of downstream image classification
    tasks. For image transformation, we will use the Keras library. Then, we will
    run our processing job on multiple nodes to speed up our job. Follow these steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by building a custom processing container. Note that SageMaker
    runs processing containers using the `docker run image_uri` command, so we need
    to specify the entry point in our Dockerfile. We are using the official Python
    3.7 container with a basic Debian version:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will start by building a custom processing container.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to provide our processing code. We will use `keras.utils` to load
    the original dataset into memory and specify the necessary transformations:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since the Keras generator operates in memory, we need to save the generated
    images to disk:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We save the augmented images in a similar directory hierarchy, where labels
    are defined by directory name.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the BYO container and processing code, we are ready to schedule
    the processing job. First, we need to instantiate the `Processor` object with
    basic job configurations, such as the number and type of instances and container
    images:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To start the job, we must execute the `.run()` method. This method allows us
    to provide additional configuration parameters. For instance, to distribute tasks
    evenly, we need to split datasets into chunks. This is easy to do using the `ShardedByKey`
    distribution type. In this case, SageMaker will attempt to evenly distribute objects
    between our processing nodes. SageMaker Processing allows you to pass your custom
    script configuration via the `arguments` collection. You will need to make sure
    that your processing script can parse these command-line arguments properly:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For the full processing code, please refer to [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: This example should give you an intuition of how SageMaker Processing can be
    used for your data processing needs. At the same time, SageMaker Processing is
    flexible enough to run any arbitrary tasks, such as batch inference, data aggregation
    and analytics, and others.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how to optimize data storage and retrieval
    for large DL datasets.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing data storage and retrieval
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training **SOTA DL** models, you typically need a large dataset for a model
    to train. It can be expensive to store and retrieve such large datasets. For instance,
    the popular computer vision dataset **COCO2017** is approximately 30 GB, while
    the **Common Crawl** dataset for NLP tasks has a size of hundreds of TB. Dealing
    with such large datasets requires careful consideration of where to store the
    dataset and how to retrieve it at inference or training time. In this section,
    we will discuss some of the optimization strategies you can use when choosing
    storage and retrieval strategies.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a storage solution
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When choosing an optimal storage solution, you may consider the following factors,
    among others:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The cost of storage and data retrieval
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latency and throughput requirements for data retrieval
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data partitioning
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How frequently data is refreshed
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the pros and cons of various storage solutions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon S3** provides the cheapest storage solution among those considered.
    However, you should be aware that Amazon S3 also charges for data transfer and
    data requests. In scenarios where your dataset consists of a large number of small
    files, you may incur considerable costs associated with **PUT** and **GET** records.
    You may consider batching small objects into large objects to reduce this cost.
    Note that there are additional charges involved when retrieving data stored in
    another AWS region. It could be reasonable to collocate your workload and data
    in the same AWS region to avoid these costs. S3 is also generally the slowest
    storage solution. By default, Amazon SageMaker downloads all objects from S3 before
    the training begins. This initial download time can take minutes and will add
    to the general training time. For instance, in the case of the **COCO2017** dataset,
    it takes ~20 minutes to download it on training nodes from S3\. Amazon provides
    several mechanisms to stream data from S3 and eliminates download time. We will
    discuss these in this section.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon EFS** storage is generally more expensive than Amazon S3\. However,
    unlike Amazon S3, Amazon EFS doesn’t have any costs associated with read and write
    operations. Since EFS provides a filesystem interface, compute nodes can directly
    mount to the EFS directory that contains the dataset and use it immediately without
    the need to download the dataset. Amazon EFS provides an easy mechanism to share
    reusable datasets between workloads or teams.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon FSx for Lustre** provides the lowest latency for data retrieval but
    also the most expensive storage price. Like Amazon EFS, it doesn’t require any
    download time. One of the common use cases for scenarios is to store your data
    in S3\. When you need to run your set of experiments, you can provision FSx for
    Lustre with synchronization from S3, which seamlessly copies data from S3 to your
    filesystem. After that, you can run your experiments and use FSx for Lustre as
    a data source, leveraging the lowest latency for data retrieval. Once experimentation
    is done, you can de-provision the Lustre filesystem to avoid any additional costs
    and keep the original data in S3.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Feature Store** has the most out-of-the-box ML-specific features;
    however, it has its shortcomings and strong assumptions. Since its offline storage
    is backed by S3, it has a similar cost structure and latency considerations. Online
    storage adds additional storage, read, and write costs. SageMaker Feature Store
    fits well into scenarios when you need to reuse the same dataset for inference
    and training workloads. Another popular use case for Feature Store is when you
    need to have audit requirements or run analytical queries against your datasets.
    Note that since Feature Store supports only a limited amount of data types (for
    example, it doesn’t support any collections), you may need to do type casting
    when consuming data from Feature Store.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS provides a wide range of storage solutions and at times, it may not be obvious
    which solution to choose. As always, it’s important to start by understanding
    your use case requirements and success criteria (for instance, lowest possible
    latency, highest throughput, or most cost-optimal solution).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Streaming datasets
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon S3 is a popular storage solution for large ML datasets, given its low
    cost, high durability, convenient API, and integration with other services, such
    as SageMaker. As we discussed in the previous section, one of the downsides of
    using S3 to store training datasets is that you need to download the dataset to
    your training nodes before training can start.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: You can choose to use the **ShardedByKey** distribution strategy, which will
    reduce the amount of data downloaded to each training node. However, that approach
    only reduces the amount of data that needs to be downloaded to your training nodes.
    For large datasets (100s+ GB), it solves the problem only partially. You will
    also need to ensure that your training nodes have enough EBS volume capacity to
    store data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach to reduce training time is to stream data from Amazon
    S3 without downloading it upfront. Several implementations of S3 data streaming
    are provided by Amazon SageMaker:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Framework-specific streaming implementations, such as `PipeModeDataset` for
    TensorFlow and Amazon S3 Plugin for PyTorch
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framework-agnostic FastFile mode
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review the benefits of these approaches.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: PipeModeDataset for TensorFlow
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`PipeModeDataset`, your training program can read from S3 without managing
    access to S3 objects. When using `PipeModeDataset`, you need to ensure that you
    are using a matching version of TensorFlow.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Pipe mode is enabled when configuring a SageMaker training job. You
    can map multiple datasets to a single pipe if you’re storing multiple datasets
    under the same S3 path. Note that SageMaker supports up to 20 pipes. If you need
    more than 20 pipes, you may consider using Augmented Manifest files, which allow
    you to explicitly list a set of S3 objects to be streamed. During training, SageMaker
    will read objects from the manifest file and stream them into the pipe.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '`PipeModeDataset` supports the following dataset formats: text line, RecordIO,
    and TFRecord. If you have a dataset in a different format (for instance, as separate
    image files) you will have to convert your dataset. Note that the performance
    of `PipeModeDataset` performance on the number and size of the files. It’s generally
    recommended to keep the file size around 100 to 200 MB for optimal performance.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Since `PipeModeDataset` implements the TensorFlow Dataset API, you can use familiar
    methods to manipulate your datasets, such as `.apply(), .map()`. `PipeModeDataset`
    also can be passed to TensorFlow Estimator directly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several differences between `PipeModeDataset` and TensorFlow Dataset
    that you should consider:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`PipeModeDataset` reads data sequentially in files. SageMaker supports the
    `ShuffleConfig` ([https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml))
    parameter, which shuffles the order of the files to read. You can also call the
    `.shuffle()` method to further shuffle records.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PipeModeDataset` supports only three data types, all of which require data
    to be converted into one of the supported formats.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PipeModeDataset` has limited controls when it comes to manipulating data at
    training time. For instance, if you need to boost the underrepresented class in
    your classification dataset, you will need to use a separate pipe to stream samples
    of the underrepresented file and handle the boosting procedure in your training
    script.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PipeModeDataset` doesn’t support SageMaker Local mode, so it can be tricky
    to debug your training program. When using SageMaker Pipe mode, you don’t have
    access to the internals of how SageMaker streams your data objects into pipes.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at how `PipeModeDataset` can be used. In this example, for training
    purposes, we will convert the CIFAR-100 dataset into TFRecords and then stream
    this dataset at training time using `PipeModeDataset`. We will provide a redacted
    version for brevity instead of listing the entire example. The full source is
    available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb).
    Follow these steps:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by converting our dataset into TFRecord format. In the following
    code block, there is a method that iterates over a batch of files, converts a
    pair of images and labels into a TensorFlow `Example` class, and writes a batch
    of `Example` objects into a single `TFRecord` file:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have our datasets in TFRecord format, we need to create our training
    script. It will largely follow a typical TensorFlow training script, with the
    only difference being that we will use `PipeModeDataset` instead of `TFRecordDataset`.
    You can use the following code to configure `PipeModeDataset`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When configuring the SageMaker training job, we need to explicitly specify
    that we want to use Pipe mode:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that since the CIFAF100 dataset is relatively small, you may be not able
    to see any considerable decrease in the training start time. However, with bigger
    datasets such as COCO2017, you can expect the training time to reduce by at least
    several minutes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3 Plugin for PyTorch
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon S3 Plugin for PyTorch allows you to stream data directly from S3 objects
    with minimal changes to your existing PyTorch training script. Under the hood,
    S3 Plugin uses `TransferManager` from the AWS SDK for C++ to fetch files from
    S3 and utilizes S3 multipart download for optimal data throughput and reliability.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'S3 Plugin provides two implementations of PyTorch dataset APIs: a map-style
    `S3Dataset` and an iterable-style `S3IterableDataset`. In the following section,
    we will discuss when to use one or another.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Map-style S3Dataset
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`S3Dataset` represents a mapping of indexes and data records and implements
    the `__getitem__()` method. It allows you to randomly access data records based
    on their indices. A map-style dataset works best when each file has a single data
    record. You can use PyTorch’s distributed sampler to further partition the dataset
    between training nodes.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using `S3Dataset` for images stored on S3:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the dataset class that inherits from the parent `S3Dataset`.
    Then, we will define the data processing pipeline using PyTorch functions:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will create a PyTorch-native `Dataloader` object that can be passed
    to any training script:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Iterable-style S3IterableDataset
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`S3IterableDataset` represents iterable objects and implements Python’s `__iter__()`
    method. Generally, you use an iterable-style dataset when random reads (such as
    in a map-style dataset) are expensive or impossible. You should use an iterable-style
    dataset when you have a batch of data records stored in a single file object.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: When using `S3IterableDataset`, it’s important to control your file sizes. If
    your dataset is represented by a large number of files, accessing each file will
    come with overhead. In such scenarios, it’s recommended to merge data records
    into larger file objects.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '`S3IterableDataset` doesn’t restrict what file types can be used. A full binary
    blob of the file object is returned, and you are responsible to provide parsing
    logic. You can shuffle the URLs of file objects by setting the `shuffle_urls`
    flag to true. Note that if you need to shuffle records within the same data objects,
    you can use `ShuffleDataset` accumulates data records across multiple file objects
    and returns a random sample from it.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '`S3IterableDataset` takes care of sharding data between training nodes when
    running distributed training. You can wrap `S3IterableDataset` with PyTorch’s
    `DataLoader` for parallel data loading and pre-processing.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example of how to construct an iterable-style dataset from
    several TAR archives stored on S3 and apply data transformations:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining a custom dataset class using PyTorch’s native `IterableDataset`.
    As part of the class definition, we use `S3IterableDataset` to fetch data from
    S3 and data transformations that will be applied to individual data records:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we define a transformation to normalize images and then instantiate a
    dataset instance with the ability to stream images from S3:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, let’s look at FastFile mode.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: FastFile mode
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In late 2021, Amazon announced a new approach for streaming data directly from
    S3 called FastFile mode. It combines the benefits of streaming data from S3 with
    the convenience of working with local files. In **FastFile** mode, each file will
    appear to your training program as a POSIX filesystem mount. Hence, it will be
    indistinguishable from any other local files, such as the ones stored on the mounted
    EBS volume.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: When reading file objects in FastFile mode, SageMaker retrieves chunks of the
    file if the file format supports chunking; otherwise, a full file is retrieved.
    FastFile mode performs optimally if data is read sequentially. Please note that
    there is an additional overhead on retrieving each file object. So, fewer files
    will usually result in a lower startup time for your training job.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the previously discussed framework-specific streaming plugins,
    FastFile mode has several benefits:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Avoids any framework-specific implementations for data streaming. You can use
    your PyTorch or TensorFlow native data utilities and share datasets between frameworks.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, you have more granular control over data inputs using your framework
    utilities to perform operations such as shuffling, dynamic boosting, and data
    processing *on the fly*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no restrictions on the file format.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easier to debug your training program as you can use SageMaker Local mode
    to test and debug your program locally first.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To use FastFile mode, you need to supply an appropriate `input_mode` value
    when configuring your SageMaker `Estimator` object. The following code shows an
    example of a TensorFlow training job:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: FastFile mode can be a good starting choice, given its ease of use and versatility.
    If, for some reason, you are not happy with its performance, you can always consider
    tuning the configuration of your dataset (file format, file size, data processing
    pipeline, parallelism, and so on) or reimplement the use of one of the framework-specific
    implementations. It may also be a good idea to compare FastFile mode’s performance
    of streaming data from S3 using other methods such as Pipe mode and S3 Plugin
    for PyTorch.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed the available storage solutions for storing and
    managing DL datasets and discussed their pros and cons in detail, along with their
    usage scenarios. We walked through several examples of how to integrate your SageMaker
    training scripts with different storage services. Later, we learned about various
    optimization strategies for storing data and discussed advanced mechanisms for
    optimizing data retrieval for training tasks. We also looked into SageMaker Processing
    and how it can be used to scale your data processing efficiently.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: This chapter closes the first part of this book, which served as an introduction
    to using DL models on SageMaker. Now, we will move on to advanced topics. In the
    next chapter, we will discuss the advanced training capabilities that SageMaker
    offers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Building and Training Deep Learning Models'
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will learn how to train DL models using SageMaker-managed capabilities,
    outlining available software frameworks to distribute training processes across
    many nodes, optimizing hardware utilization, and monitoring your training jobs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17519_05.xhtml#_idTextAnchor083), *Considering Hardware for
    Deep Learning Training*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17519_06.xhtml#_idTextAnchor097), *Engineering Distributed Training*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17519_07.xhtml#_idTextAnchor110), *Operationalizing Deep Learning
    Training*'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
