<html><head></head><body>
<div id="_idContainer279">
<h1 class="chapter-number" id="_idParaDest-27" lang="en-GB"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-28" lang="en-GB"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.2.1">Neural Networks</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.5.1">, we introduced a number of basic </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">machine learning</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">ML</span></strong><span class="koboSpan" id="kobo.9.1">) concepts and techniques. </span><span class="koboSpan" id="kobo.9.2">We went through the main ML paradigms, as well as some popular classic ML algorithms, and we finished on </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">neural networks</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">NN</span></strong><span class="koboSpan" id="kobo.13.1">). </span><span class="koboSpan" id="kobo.13.2">In this chapter, we will formally introduce what NNs are, discuss their mathematical foundations, describe in detail how their building blocks work, see how we can stack many layers to create a deep feedforward NN, and then learn how to </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">train them.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.15.1">In this chapter, we will cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">main topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.17.1">The need </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">for NNs</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.19.1">The math </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">of NNs</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.21.1">An introduction </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">to NNs</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">Training NNs</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.24.1">The link between NNs and the human brain</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.25.1">Initially, NNs</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.26.1"> were inspired by the biological brain (hence the name). </span><span class="koboSpan" id="kobo.26.2">Over time, however, we’ve stopped trying to emulate how the brain works, and instead, we’re focused on finding the correct configurations for specific tasks, including computer vision, natural language processing, and speech recognition. </span><span class="koboSpan" id="kobo.26.3">You can think of it this way – for a long time, we were inspired by the flight of birds, but, in the end, we created airplanes, which are quite different. </span><span class="koboSpan" id="kobo.26.4">We are still far from matching the potential of the brain. </span><span class="koboSpan" id="kobo.26.5">Perhaps the machine learning algorithms of the future will resemble the brain more, but that’s not the case now. </span><span class="koboSpan" id="kobo.26.6">Hence, for the rest of this book, we won’t try to create analogies between the brain and NNs. </span><span class="koboSpan" id="kobo.26.7">To follow this train</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.27.1"> of thought, we’ll call the smallest building NN building blocks </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">units</span></strong><span class="koboSpan" id="kobo.29.1">, instead of neurons, as they were </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">originally known.</span></span></p>
<h1 id="_idParaDest-29" lang="en-GB"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.31.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.32.1">We’ll implement the example in this chapter using Python. </span><span class="koboSpan" id="kobo.32.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.32.3">You can find the code examples in the book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02"><span class="No-Break"><span class="koboSpan" id="kobo.34.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter02</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.35.1">.</span></span></p>
<h1 id="_idParaDest-30" lang="en-GB"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.36.1">The need for NNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.37.1">NNs have been around</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.38.1"> for many years, and they’ve gone through several periods, during which they’ve fallen in and out of favor. </span><span class="koboSpan" id="kobo.38.2">However, recently, they have steadily gained ground over many other competing machine learning</span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.39.1"> algorithms. </span><span class="koboSpan" id="kobo.39.2">This resurgence is due to computers getting faster, the use of </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">graphical processing units</span></strong><span class="koboSpan" id="kobo.41.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.42.1">GPUs</span></strong><span class="koboSpan" id="kobo.43.1">) versus the most traditional use of </span><strong class="bold"><span class="koboSpan" id="kobo.44.1">central processing units</span></strong><span class="koboSpan" id="kobo.45.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.46.1">CPUs</span></strong><span class="koboSpan" id="kobo.47.1">), better algorithms and NN design, and increasingly</span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.48.1"> larger datasets, which we’ll look at in this book. </span><span class="koboSpan" id="kobo.48.2">To get an idea of their success, let’s look at the ImageNet</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.49.1"> Large Scale Visual Recognition Challenge (</span><a href="http://image-net.org/challenges/LSVRC/"><span class="koboSpan" id="kobo.50.1">http://image-net.org/challenges/LSVRC/</span></a><span class="koboSpan" id="kobo.51.1">, or just </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">ImageNet</span></strong><span class="koboSpan" id="kobo.53.1">). </span><span class="koboSpan" id="kobo.53.2">The participants train their algorithms using the ImageNet</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.54.1"> database. </span><span class="koboSpan" id="kobo.54.2">It contains more than 1 million high-resolution color images in over 1,000 categories (one category may be images of cars, another of people, trees, and so on). </span><span class="koboSpan" id="kobo.54.3">One of the tasks in the challenge is to classify unknown images into these categories. </span><span class="koboSpan" id="kobo.54.4">In 2011, the winner achieved a top-five accuracy of 74.2%. </span><span class="koboSpan" id="kobo.54.5">In 2012, Alex Krizhevsky and his team entered the competition with a convolutional network (a special type of deep network). </span><span class="koboSpan" id="kobo.54.6">That year, they won with a top-five accuracy of 84.7%. </span><span class="koboSpan" id="kobo.54.7">Since then, the winning algorithms have always been NNs, and the current top-five accuracy is around 99%. </span><span class="koboSpan" id="kobo.54.8">However, deep learning algorithms have excelled in other areas – for example, both Google Now and Apple’s Siri assistants rely on deep networks for speech recognition and Google’s use of deep learning for their translation engines. </span><span class="koboSpan" id="kobo.54.9">Recent image and text generation systems such as Stability AI’s Stable Diffusion and OpenAI’s DALL-E and ChatGPT are implemented </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">with NNs.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.56.1">We’ll talk about these exciting advances in the following chapters, but for now, we’ll focus on the mathematical foundations of NNs. </span><span class="koboSpan" id="kobo.56.2">To help us with this task, we’ll use simple networks with one or two layers. </span><span class="koboSpan" id="kobo.56.3">You can think of these as toy examples that are not deep networks, but understanding how they work is important. </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">Here’s why:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.58.1">Knowing the theory of NNs will help you understand the rest of the book because a large majority of NNs in use today share common principles. </span><span class="koboSpan" id="kobo.58.2">Understanding simple networks means that you’ll understand deep </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">networks too.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.60.1">Having some fundamental knowledge is always good. </span><span class="koboSpan" id="kobo.60.2">It will help you a lot when you face some new material (even material not included in </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">this book).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.62.1">I hope these arguments will convince</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.63.1"> you of the importance of this chapter. </span><span class="koboSpan" id="kobo.63.2">As a small consolation, we’ll talk about deep learning in depth (pun intended) in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.64.1">Chapter 3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.65.1">.</span></span></p>
<h1 id="_idParaDest-31" lang="en-GB"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.66.1">The math of NNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.67.1">In the following few sections, we’ll discuss</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.68.1"> the mathematical principles of NNs. </span><span class="koboSpan" id="kobo.68.2">This way, we’ll be able to explain NNs through these very principles in a fundamental and </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">structured way.</span></span></p>
<h2 id="_idParaDest-32" lang="en-GB"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.70.1">Linear algebra</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.71.1">Linear algebra</span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.72.1"> deals with objects</span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.73.1"> such as vectors and matrices, linear transformations, and linear equations such </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">as </span></span><span class="No-Break"><span class="koboSpan" id="kobo.75.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/32.png" style="vertical-align:-0.340em;height:1.051em;width:12.305em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.77.1">Linear algebra identifies the following </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">mathematical objects:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.79.1">Scalar</span></strong><span class="koboSpan" id="kobo.80.1">: A </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">single number.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.82.1">Vector</span></strong><span class="koboSpan" id="kobo.83.1">: A one-dimensional array of numbers (also known as components or </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">scalars</span></strong><span class="koboSpan" id="kobo.85.1">), where each element</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.86.1"> has an index. </span><span class="koboSpan" id="kobo.86.2">We can denote</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.87.1"> vectors either</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.88.1"> with a superscript arrow (</span><span class="koboSpan" id="kobo.89.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;→&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/33.png" style="vertical-align:-0.012em;height:1.073em;width:0.564em"/></span><span class="koboSpan" id="kobo.90.1">) or in bold (</span><strong class="bold"><span class="koboSpan" id="kobo.91.1">x</span></strong><span class="koboSpan" id="kobo.92.1">), but we’ll mostly use the bold notation throughout the book. </span><span class="koboSpan" id="kobo.92.2">The following is an example of </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">a vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.94.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/34.png" style="vertical-align:-0.390em;height:1.451em;width:8.367em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.95.1">We can represent a </span><em class="italic"><span class="koboSpan" id="kobo.96.1">n</span></em><span class="koboSpan" id="kobo.97.1">-dimensional vector as the coordinates of a point in an </span><em class="italic"><span class="koboSpan" id="kobo.98.1">n</span></em><span class="koboSpan" id="kobo.99.1">-dimensional Euclidean space, </span><span class="koboSpan" id="kobo.100.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/35.png" style="vertical-align:-0.000em;height:0.661em;width:1.099em"/></span><span class="koboSpan" id="kobo.101.1">. </span><span class="koboSpan" id="kobo.101.2">Think of Euclidean space as a coordinate system – the vector starts at the center of that system, and each of the vector’s elements represents the coordinate of the point along its corresponding coordinate axis. </span><span class="koboSpan" id="kobo.101.3">The following figure shows a vector in a three-dimensional coordinate </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">system, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.103.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/36.png" style="vertical-align:-0.000em;height:0.703em;width:1.076em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.105.1"><img alt="Figure 2.1 – Vector representation in a three-dimensional space" src="image/B19627_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.106.1">Figure 2.1 – Vector representation in a three-dimensional space</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.107.1">The figure</span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.108.1"> can also help us define</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.109.1"> two additional properties of </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">the vector:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.111.1">Magnitude</span></strong><span class="koboSpan" id="kobo.112.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">or </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.114.1">length</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">):</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.116.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfenced open=&quot;|&quot; close=&quot;|&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/37.png" style="vertical-align:-0.380em;height:1.253em;width:9.490em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.117.1">Think of the magnitude as a generalization of the Pythagorean theorem for an </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">n-dimensional space.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.119.1">Direction</span></strong><span class="koboSpan" id="kobo.120.1">: The angle between the vector and each axis of the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">vector space.</span></span></li>
</ul>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.122.1">Matrix</span></strong><span class="koboSpan" id="kobo.123.1">: A two-dimensional array of scalars, where each element</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.124.1"> is identified by a row and a column. </span><span class="koboSpan" id="kobo.124.2">We’ll denote a matrix with a bold capital letter – for example, </span><strong class="bold"><span class="koboSpan" id="kobo.125.1">A</span></strong><span class="koboSpan" id="kobo.126.1">. </span><span class="koboSpan" id="kobo.126.2">Conversely, we’ll denote the matrix elements with the small matrix letter, with the row and column as subscript indices – for example, </span><span class="koboSpan" id="kobo.127.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/38.png" style="vertical-align:-0.483em;height:0.931em;width:0.749em"/></span><span class="koboSpan" id="kobo.128.1">. </span><span class="koboSpan" id="kobo.128.2">We can see an example of the matrix notation in the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">following formula:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.130.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋱&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋮&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mo&gt;⋯&lt;/mml:mo&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;m&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/39.png" style="vertical-align:-2.090em;height:4.687em;width:10.170em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.131.1">We can represent a vector</span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.132.1"> as either a single-row </span><em class="italic"><span class="koboSpan" id="kobo.133.1">1×n</span></em><span class="koboSpan" id="kobo.134.1"> matrix (</span><strong class="bold"><span class="koboSpan" id="kobo.135.1">row matrix</span></strong><span class="koboSpan" id="kobo.136.1">) or a single-column </span><em class="italic"><span class="koboSpan" id="kobo.137.1">n×1</span></em><span class="koboSpan" id="kobo.138.1"> matrix (</span><strong class="bold"><span class="koboSpan" id="kobo.139.1">column matrix</span></strong><span class="koboSpan" id="kobo.140.1">). </span><span class="koboSpan" id="kobo.140.2">Transformed like this, the vector can participate in different</span><a id="_idIndexMarker141"/> <span class="No-Break"><span class="koboSpan" id="kobo.141.1">matrix operations.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.142.1">Tensor</span></strong><span class="koboSpan" id="kobo.143.1">: The term </span><em class="italic"><span class="koboSpan" id="kobo.144.1">tensor</span></em><span class="koboSpan" id="kobo.145.1"> originates from mathematics</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.146.1"> and physics, where it existed </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.147.1">long before we started using it in ML. </span><span class="koboSpan" id="kobo.147.2">Its definition in these fields differs from the one in ML. </span><span class="koboSpan" id="kobo.147.3">Fortunately, the tensor in the context of ML is just a multi-dimensional array with the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">following properties:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.149.1">Rank</span></strong><span class="koboSpan" id="kobo.150.1">: The number of array dimensions. </span><span class="koboSpan" id="kobo.150.2">Vectors and matrices are special cases of tensors. </span><span class="koboSpan" id="kobo.150.3">A tensor of rank 0 is a scalar, a tensor of rank 1 is a vector, and a tensor of rank 2 is a matrix. </span><span class="koboSpan" id="kobo.150.4">There is no limit on the number of dimensions, and some types of NNs can use tensors of rank 4 </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">or more.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.152.1">Shape</span></strong><span class="koboSpan" id="kobo.153.1">: The size of each of the </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">tensor’s dimensions.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.155.1">Data type</span></strong><span class="koboSpan" id="kobo.156.1"> of the tensor values. </span><span class="koboSpan" id="kobo.156.2">In practice, the data types include 16-, 32-, and 64-bit floats and 8-, 16-, 32-, and </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">64-bit integers.</span></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.158.1">The tensor is the main data structure</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.159.1"> of libraries such as PyTorch, Keras, </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">and TensorFlow.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.161.1">The nature of tensors</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.162.1">You can find a detailed</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.163.1"> discussion on the nature of tensors here: </span><a href="https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors"><span class="koboSpan" id="kobo.164.1">https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors</span></a><span class="koboSpan" id="kobo.165.1">. </span><span class="koboSpan" id="kobo.165.2">You can also compare</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.166.1"> this with the PyTorch (</span><a href="https://pytorch.org/docs/stable/tensors.html"><span class="koboSpan" id="kobo.167.1">https://pytorch.org/docs/stable/tensors.html</span></a><span class="koboSpan" id="kobo.168.1">) and TensorFlow (</span><a href="https://www.tensorflow.org/guide/tensor"><span class="koboSpan" id="kobo.169.1">https://www.tensorflow.org/guide/tensor</span></a><span class="koboSpan" id="kobo.170.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">tensor</span></span><span class="No-Break"><a id="_idIndexMarker147"/></span><span class="No-Break"><span class="koboSpan" id="kobo.172.1"> definitions.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.173.1">Now that we’ve introduced vectors, matrices, and tensors, let’s continue with some of the linear operations they can </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">participate in.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.175.1">Vector and matrix operations</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.176.1">We’ll focus on the operations</span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.177.1"> that relate to NNs, starting</span><a id="_idIndexMarker149"/> <span class="No-Break"><span class="koboSpan" id="kobo.178.1">with vectors:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.179.1">Vector addition</span></strong><span class="koboSpan" id="kobo.180.1">: Adds two or more </span><em class="italic"><span class="koboSpan" id="kobo.181.1">n</span></em><span class="koboSpan" id="kobo.182.1">-dimensional</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.183.1"> vectors, </span><strong class="bold"><span class="koboSpan" id="kobo.184.1">a</span></strong><span class="koboSpan" id="kobo.185.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.186.1">b</span></strong><span class="koboSpan" id="kobo.187.1"> (and so on) to a </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">new vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.189.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/40.png" style="vertical-align:-0.390em;height:1.151em;width:13.427em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.190.1">Dot (or scalar) product</span></strong><span class="koboSpan" id="kobo.191.1">: Combines two </span><em class="italic"><span class="koboSpan" id="kobo.192.1">n</span></em><span class="koboSpan" id="kobo.193.1">-dimensional</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.194.1"> vectors, </span><strong class="bold"><span class="koboSpan" id="kobo.195.1">a</span></strong><span class="koboSpan" id="kobo.196.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">b</span></strong><span class="koboSpan" id="kobo.198.1">, into a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.199.1">scalar value</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.201.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/41.png" style="vertical-align:-0.062em;height:0.814em;width:7.044em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.202.1">Here, the angle between the two vectors is </span><em class="italic"><span class="koboSpan" id="kobo.203.1">θ</span></em><span class="koboSpan" id="kobo.204.1">, and </span><span class="koboSpan" id="kobo.205.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/42.png" style="vertical-align:-0.062em;height:0.565em;width:0.975em"/></span><span class="koboSpan" id="kobo.206.1"> and </span><span class="koboSpan" id="kobo.207.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/43.png" style="vertical-align:-0.062em;height:0.814em;width:1.043em"/></span><span class="koboSpan" id="kobo.208.1"> are their magnitudes. </span><span class="koboSpan" id="kobo.208.2">For example, if the vectors are </span><em class="italic"><span class="koboSpan" id="kobo.209.1">two</span></em><span class="koboSpan" id="kobo.210.1">-dimensional and their components are </span><span class="koboSpan" id="kobo.211.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/44.png" style="vertical-align:-0.340em;height:1.051em;width:6.860em"/></span><span class="koboSpan" id="kobo.212.1">, the preceding formula becomes </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">the following:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.214.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/45.png" style="vertical-align:-0.340em;height:1.051em;width:11.973em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.215.1">The following diagram illustrates the dot product of </span><strong class="bold"><span class="koboSpan" id="kobo.216.1">a</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.217.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.218.1">b</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.220.1"><img alt="Figure 2.2 – The dot product of vectors – top: vector components, and bottom: the dot product of the two vectors" src="image/B19627_02_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.221.1">Figure 2.2 – The dot product of vectors – top: vector components, and bottom: the dot product of the two vectors</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.222.1">We can think of the dot</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.223.1"> product as a similarity measure between</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.224.1"> the two vectors, where the angle </span><em class="italic"><span class="koboSpan" id="kobo.225.1">θ</span></em><span class="koboSpan" id="kobo.226.1"> indicates how similar they are. </span><span class="koboSpan" id="kobo.226.2">If </span><em class="italic"><span class="koboSpan" id="kobo.227.1">θ</span></em><span class="koboSpan" id="kobo.228.1"> is small (the vectors have similar directions), then their dot product</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.229.1"> will be higher because </span><span class="koboSpan" id="kobo.230.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/46.png" style="vertical-align:-0.012em;height:0.723em;width:1.669em"/></span><span class="koboSpan" id="kobo.231.1"> will converge toward 1. </span><span class="koboSpan" id="kobo.231.2">In this context, we can define a </span><strong class="bold"><span class="koboSpan" id="kobo.232.1">cosine similarity</span></strong><span class="koboSpan" id="kobo.233.1"> between two vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.235.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;a&lt;/mi&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mfenced open=&quot;|&quot; close=&quot;|&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;a&lt;/mi&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;|&quot; close=&quot;|&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/47.png" style="vertical-align:-0.590em;height:1.622em;width:5.038em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.236.1">Cross (or vector) product</span></strong><span class="koboSpan" id="kobo.237.1">: A combination of two vectors, </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">a</span></strong><span class="koboSpan" id="kobo.239.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.240.1">b</span></strong><span class="koboSpan" id="kobo.241.1">, in a new vector, which</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.242.1"> is perpendicular to both initial vectors. </span><span class="koboSpan" id="kobo.242.2">The magnitude</span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.243.1"> of the output vector is equal to </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">the following:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.245.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/48.png" style="vertical-align:-0.062em;height:0.814em;width:7.281em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.246.1">We can see an example of a cross product of two-dimensional vectors in the </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.248.1"><img alt="Figure 2.3 – A cross product of two two-dimensional vectors" src="image/B19627_02_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.249.1">Figure 2.3 – A cross product of two two-dimensional vectors</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.250.1">The output vector is perpendicular (or </span><strong class="bold"><span class="koboSpan" id="kobo.251.1">normal</span></strong><span class="koboSpan" id="kobo.252.1">) to the plane, which contains the input vectors. </span><span class="koboSpan" id="kobo.252.2">The output vector</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.253.1"> magnitude is equal to the area</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.254.1"> of a parallelogram</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.255.1"> with sides, the </span><strong class="bold"><span class="koboSpan" id="kobo.256.1">a</span></strong><span class="koboSpan" id="kobo.257.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.258.1">b</span></strong><span class="koboSpan" id="kobo.259.1"> vectors (denoted with light blue in the </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">preceding diagram).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.261.1">Now, let’s focus</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.262.1"> on the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">matrix</span></span><span class="No-Break"><a id="_idIndexMarker161"/></span><span class="No-Break"><span class="koboSpan" id="kobo.264.1"> operations:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.265.1">Matrix transpose</span></strong><span class="koboSpan" id="kobo.266.1">: Flip the matrix along its main diagonal, represented by the set of all matrix elements, </span><span class="koboSpan" id="kobo.267.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/49.png" style="vertical-align:-0.483em;height:0.931em;width:0.789em"/></span><span class="koboSpan" id="kobo.268.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.269.1">i=j</span></em><span class="koboSpan" id="kobo.270.1">. </span><span class="koboSpan" id="kobo.270.2">We’ll denote the transpose operation with </span><span class="koboSpan" id="kobo.271.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/50.png" style="vertical-align:-0.000em;height:0.661em;width:0.684em"/></span><span class="koboSpan" id="kobo.272.1"> in superscript form. </span><span class="koboSpan" id="kobo.272.2">The cell </span><span class="koboSpan" id="kobo.273.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/51.png" style="vertical-align:-0.483em;height:0.931em;width:0.788em"/></span><span class="koboSpan" id="kobo.274.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.275.1">A</span></strong><span class="koboSpan" id="kobo.276.1"> is equal to the cell </span><span class="koboSpan" id="kobo.277.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/49.png" style="vertical-align:-0.483em;height:0.931em;width:0.789em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.278.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.279.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/53.png" style="vertical-align:-0.000em;height:0.718em;width:1.155em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.281.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/54.png" style="vertical-align:-0.483em;height:1.251em;width:4.803em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.282.1">The transpose of an </span><em class="italic"><span class="koboSpan" id="kobo.283.1">m×n</span></em><span class="koboSpan" id="kobo.284.1"> matrix is an </span><em class="italic"><span class="koboSpan" id="kobo.285.1">n×m</span></em><span class="koboSpan" id="kobo.286.1"> matrix, as we can see with the </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">following examples:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.288.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;23&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;33&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⇒&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;23&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;33&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/55.png" style="vertical-align:-1.416em;height:3.340em;width:18.922em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.289.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowspacing=&quot;1.0000ex&quot; rowalign=&quot;baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;13&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;23&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo&gt;⇒&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em&quot; columnwidth=&quot;auto auto&quot; columnalign=&quot;center center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;13&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;23&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/56.png" style="vertical-align:-1.409em;height:3.326em;width:16.423em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.290.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto&quot; columnalign=&quot;center center center&quot; rowalign=&quot;baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;13&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mo&gt;⇒&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable columnwidth=&quot;auto&quot; columnalign=&quot;center&quot; rowspacing=&quot;1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mn&gt;13&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/57.png" style="vertical-align:-1.409em;height:3.326em;width:14.353em"/></span><br/></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.291.1">Matrix-scalar multiplication</span></strong><span class="koboSpan" id="kobo.292.1">: Multiplication of a matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.293.1">A</span></strong><span class="koboSpan" id="kobo.294.1">, by a scalar, </span><em class="italic"><span class="koboSpan" id="kobo.295.1">y</span></em><span class="koboSpan" id="kobo.296.1">, into a new matrix with the same size as </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">the original:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.298.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/58.png" style="vertical-align:-0.830em;height:2.168em;width:15.317em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.299.1">Matrix-matrix addition</span></strong><span class="koboSpan" id="kobo.300.1">: Element-wise addition of two or more matrices, </span><strong class="bold"><span class="koboSpan" id="kobo.301.1">A</span></strong><span class="koboSpan" id="kobo.302.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.303.1">B</span></strong><span class="koboSpan" id="kobo.304.1"> (and so on), into a new matrix. </span><span class="koboSpan" id="kobo.304.2">All input matrices must be the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">same size:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.306.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/59.png" style="vertical-align:-1.060em;height:2.626em;width:22.263em"/></span><br/></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.307.1">Matrix-vector multiplication</span></strong><span class="koboSpan" id="kobo.308.1">: Multiplication of a matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.309.1">A</span></strong><span class="koboSpan" id="kobo.310.1">, by a vector, </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">x</span></strong><span class="koboSpan" id="kobo.312.1">, into a </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">new vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.314.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/60.png" style="vertical-align:-1.467em;height:3.440em;width:16.361em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.315.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/61.png" style="vertical-align:-0.797em;height:2.100em;width:12.369em"/></span><br/></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.316.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;17&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;39&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/62.png" style="vertical-align:-0.667em;height:1.842em;width:17.839em"/></span><br/></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.317.1">The number of matrix columns</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.318.1"> must be equal to the vector size. </span><span class="koboSpan" id="kobo.318.2">The result</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.319.1"> of an </span><em class="italic"><span class="koboSpan" id="kobo.320.1">m×n</span></em><span class="koboSpan" id="kobo.321.1"> matrix, multiplied by an </span><em class="italic"><span class="koboSpan" id="kobo.322.1">n</span></em><span class="koboSpan" id="kobo.323.1">-dimensional vector, is an </span><em class="italic"><span class="koboSpan" id="kobo.324.1">m</span></em><span class="koboSpan" id="kobo.325.1">-dimensional vector. </span><span class="koboSpan" id="kobo.325.2">We can assume that the rows of the matrix are </span><em class="italic"><span class="koboSpan" id="kobo.326.1">n</span></em><span class="koboSpan" id="kobo.327.1">-dimensional vectors. </span><span class="koboSpan" id="kobo.327.2">Then, each value of the output vector is the dot product between the corresponding matrix row vector </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.329.1">x</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.331.1">Matrix multiplication</span></strong><span class="koboSpan" id="kobo.332.1">: A binary operation, which represents the multiplication of two matrices, </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">A</span></strong><span class="koboSpan" id="kobo.334.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.335.1">B</span></strong><span class="koboSpan" id="kobo.336.1">, into a single output matrix. </span><span class="koboSpan" id="kobo.336.2">We can think of it as multiple matrix-vector multiplications, where each column of the second matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">a vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.338.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;23&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;13&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;11&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;23&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;31&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;21&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;23&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;32&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/63.png" style="vertical-align:-1.804em;height:4.115em;width:32.909em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.339.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;18&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;15&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;30&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;8&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;20&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;36&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;22&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;28&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;49&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;64&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/64.png" style="vertical-align:-1.200em;height:2.908em;width:27.172em"/></span><br/></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.340.1">If we represent the two vectors as matrices, their dot product, </span><span class="koboSpan" id="kobo.341.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/65.png" style="vertical-align:-0.012em;height:0.730em;width:5.016em"/></span><span class="koboSpan" id="kobo.342.1">, is equivalent</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.343.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">matrix-matrix</span></span><span class="No-Break"><a id="_idIndexMarker165"/></span><span class="No-Break"><span class="koboSpan" id="kobo.345.1"> multiplication.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.346.1">You can now breathe a sigh of relief because we’ve concluded our introduction to linear algebra. </span><span class="koboSpan" id="kobo.346.2">Not all is well though, as we’ll focus on probability </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">theory next.</span></span></p>
<h2 id="_idParaDest-33" lang="en-GB"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.348.1">An introduction to probability</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.349.1">In this section, we’ll introduce</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.350.1"> some basic concepts</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.351.1"> of probability theory. </span><span class="koboSpan" id="kobo.351.2">They will help us later in the book when we discuss NN training algorithms and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">language processing.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.353.1">We’ll start with the concept of a </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">statistical experiment</span></strong><span class="koboSpan" id="kobo.355.1">, which has the </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">following</span></span><span class="No-Break"><a id="_idIndexMarker168"/></span><span class="No-Break"><span class="koboSpan" id="kobo.357.1"> properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.358.1">It is composed of multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">independent trials</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.360.1">The outcome of each trial is determined by chance (it </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">is </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.362.1">non-deterministic</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">)</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.364.1">It has multiple</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.365.1"> possible outcomes, known </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">as </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.367.1">events</span></strong></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.368.1">We know in advance all possible </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">experiment outcomes</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.370.1">Examples of statistical experiments include a coin toss with two possible outcomes (heads or tails) and a dice roll with six possible outcomes (1 </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">to 6).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.372.1">The likelihood</span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.373.1"> that some event, </span><em class="italic"><span class="koboSpan" id="kobo.374.1">е</span></em><span class="koboSpan" id="kobo.375.1">, would occur is known as </span><strong class="bold"><span class="koboSpan" id="kobo.376.1">probability</span></strong><span class="koboSpan" id="kobo.377.1"> P(</span><em class="italic"><span class="koboSpan" id="kobo.378.1">е</span></em><span class="koboSpan" id="kobo.379.1">). </span><span class="koboSpan" id="kobo.379.2">It is a value in the range of [0, 1]. </span><span class="koboSpan" id="kobo.379.3">P(</span><em class="italic"><span class="koboSpan" id="kobo.380.1">e</span></em><span class="koboSpan" id="kobo.381.1">) = 0.5 indicates a 50–50 chance</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.382.1"> that the event will occur, P(</span><em class="italic"><span class="koboSpan" id="kobo.383.1">e</span></em><span class="koboSpan" id="kobo.384.1">) = 0 indicates that the event cannot occur, and P(</span><em class="italic"><span class="koboSpan" id="kobo.385.1">e</span></em><span class="koboSpan" id="kobo.386.1">) = 1 indicates that it will </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">always occur.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.388.1">We can approach probability in </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">two ways:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.390.1">Theoretical</span></strong><span class="koboSpan" id="kobo.391.1">: All events are equally likely to occur </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.392.1">and the probability of the event (outcome) we’re interested in is </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">as follows:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.394.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;e&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;N&lt;/mtext&gt;&lt;mtext&gt;umber of successful outcomes&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;T&lt;/mtext&gt;&lt;mtext&gt;otal number of outcomes&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/66.png" style="vertical-align:-0.513em;height:1.553em;width:14.077em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.395.1">The theoretical probability of the two possible outcomes in the coin toss example is P(heads) = P(tails) = 1/2. </span><span class="koboSpan" id="kobo.395.2">In the dice roll example, we have P(each side of the dice) = </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">1/6.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.397.1">Empirical</span></strong><span class="koboSpan" id="kobo.398.1">: This is the number</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.399.1"> of times an event, </span><em class="italic"><span class="koboSpan" id="kobo.400.1">e</span></em><span class="koboSpan" id="kobo.401.1">, occurs in relation to </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">all trials:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.403.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;N&lt;/mtext&gt;&lt;mtext&gt;umber of times&lt;/mtext&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mtext&gt;occurs&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;T&lt;/mtext&gt;&lt;mtext&gt;otal number of trials&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/67.png" style="vertical-align:-0.513em;height:1.553em;width:12.013em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.404.1">The empirical result of the experiment may show that the events aren’t equally likely. </span><span class="koboSpan" id="kobo.404.2">For example, if we toss a coin 100 times and observe heads 47 times, the empirical probability for heads is P(heads) = 47 / 100 = 0.47. </span><span class="koboSpan" id="kobo.404.3">The law of large numbers tells us that we’ll calculate the probability more accurately with a higher number </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">of trials.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.406.1">Now, we’ll discuss probability in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">of sets.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.408.1">Probability and sets</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.409.1">In this section, we’ll introduce</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.410.1"> sets and their properties. </span><span class="koboSpan" id="kobo.410.2">We’ll also see how to apply these properties</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.411.1"> in probability theory. </span><span class="koboSpan" id="kobo.411.2">Let’s start with </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">some definitions:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.413.1">Sample space</span></strong><span class="koboSpan" id="kobo.414.1">: The </span><strong class="bold"><span class="koboSpan" id="kobo.415.1">set</span></strong><span class="koboSpan" id="kobo.416.1"> (get it?) of all possible events (outcomes) of an experiment. </span><span class="koboSpan" id="kobo.416.2">We’ll denote</span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.417.1"> it with a capital letter. </span><span class="koboSpan" id="kobo.417.2">Like Python, we’ll list all events in the sample space with {}. </span><span class="koboSpan" id="kobo.417.3">For example, the sample spaces of coin toss and dice roll events are </span><span class="koboSpan" id="kobo.418.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;S&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;{&quot; close=&quot;}&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;heads,&lt;/mtext&gt;&lt;mtext&gt;tails&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/68.png" style="vertical-align:-0.340em;height:1.101em;width:7.229em"/></span><span class="koboSpan" id="kobo.419.1"> and </span><span class="koboSpan" id="kobo.420.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;3,4&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;5,6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/69.png" style="vertical-align:-0.340em;height:1.004em;width:1.902em"/></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.421.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;3,4&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;5,6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/70.png" style="vertical-align:-0.167em;height:0.857em;width:4.963em"/></span></span> <span class="No-Break"><span class="koboSpan" id="kobo.422.1">respectively.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.423.1">Sample point</span></strong><span class="koboSpan" id="kobo.424.1">: A single event (for example, tails) of the</span><a id="_idIndexMarker177"/> <span class="No-Break"><span class="koboSpan" id="kobo.425.1">sample space.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.426.1">Event</span></strong><span class="koboSpan" id="kobo.427.1">: A single sample point or a combination (</span><strong class="bold"><span class="koboSpan" id="kobo.428.1">subset</span></strong><span class="koboSpan" id="kobo.429.1">) of sample points of the sample space. </span><span class="koboSpan" id="kobo.429.2">For example, a combined event is for the dice to land on an odd </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">number, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.431.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,3&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/71.png" style="vertical-align:-0.340em;height:1.024em;width:5.052em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.433.1">Let’s assume that we have</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.434.1"> a set (sample space), S = {1, 2, 3, 4, 5}, and two subsets (combined events), A = {1, 2, 3} and B = {3, 4, 5}. </span><span class="koboSpan" id="kobo.434.2">We’ll use them to define the following </span><a id="_idIndexMarker179"/><span class="No-Break"><span class="koboSpan" id="kobo.435.1">set operations:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.436.1">Intersection</span></strong><span class="koboSpan" id="kobo.437.1">: A set of elements that exist in both A </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">and B:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.439.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/72.png" style="vertical-align:-0.062em;height:0.746em;width:5.112em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.440.1">If the intersection of A and B is an empty set {}, they </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">are </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.442.1">disjoint</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.444.1">Complement</span></strong><span class="koboSpan" id="kobo.445.1">: A set of all elements that aren’t included in A </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">or B:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.447.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;{&quot; close=&quot;}&quot;&gt;&lt;mn&gt;4,5&lt;/mn&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;{&quot; close=&quot;}&quot;&gt;&lt;mn&gt;1,2&lt;/mn&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/73.png" style="vertical-align:-0.167em;height:0.851em;width:11.678em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.448.1">Union</span></strong><span class="koboSpan" id="kobo.449.1">: A set of all elements that exist in either A </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">or B:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.451.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;mml:mo&gt;∪&lt;/mml:mo&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;3,4&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/74.png" style="vertical-align:-0.167em;height:0.851em;width:8.194em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.452.1">The following Venn diagrams illustrate </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">these operations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<span class="koboSpan" id="kobo.454.1"><img alt="Figure 2.4 – Venn diagrams of the possible set relationships" src="image/B19627_02_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.455.1">Figure 2.4 – Venn diagrams of the possible set relationships</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.456.1">Now, let’s see how to transfer the set operations in the probability domain. </span><span class="koboSpan" id="kobo.456.2">We’ll work with independent events – that is, the occurrence of one event doesn’t affect the probability of the occurrence of another. </span><span class="koboSpan" id="kobo.456.3">For example, the outcomes of the different coin tosses are independent of one another. </span><span class="koboSpan" id="kobo.456.4">With that, let’s define the set operations in terms of probability </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">and events:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.458.1">The intersection of two events</span></strong><span class="koboSpan" id="kobo.459.1">: A set of sample points that exist</span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.460.1"> in both events. </span><span class="koboSpan" id="kobo.460.2">The probability</span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.461.1"> of the intersection is called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.462.1">joint probability</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.464.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P(A∩B)=P(A)&lt;/mml:mtext&gt;&lt;mml:mtext&gt;×&lt;/mml:mtext&gt;&lt;mml:mtext&gt;P(B)&lt;/mml:mtext&gt;&lt;/mml:math&gt;" src="image/75.png" style="vertical-align:-0.169em;height:0.877em;width:8.102em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.465.1">Let’s say that we want to compute the probability of a card being simultaneously spades and an ace (more poetically, the ace of spades). </span><span class="koboSpan" id="kobo.465.2">The probability for spades is P(spades) = 13/52 = ¼, and the probability of an ace is P(ac</span><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.466.1">e) = 4/52 = 1/13. </span><span class="koboSpan" id="kobo.466.2">The joint probability of the two is P(ace, spades) = (1/13) </span><span class="koboSpan" id="kobo.467.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/76.png" style="vertical-align:-0.000em;height:0.483em;width:0.586em"/></span><span class="koboSpan" id="kobo.468.1"> (1/4) = 1/52. </span><span class="koboSpan" id="kobo.468.2">We can intuitively</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.469.1"> validate this result because the ace of spades is a unique card, and its probability would be 1/52. </span><span class="koboSpan" id="kobo.469.2">Since we draw a single card, the two events occur at the same time and are independent. </span><span class="koboSpan" id="kobo.469.3">Had they occurred successively – for example, two card draws, where one is a black and the other is an ace – we would </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.470.1">fall in the domain of </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">conditional probability.</span></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.472.1">The probability</span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.473.1"> of the occurrence of a single event, P(A), is also known as </span><strong class="bold"><span class="koboSpan" id="kobo.474.1">marginal probability</span></strong><span class="koboSpan" id="kobo.475.1"> (as opposed to </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">joint probability).</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.477.1">Disjoint (or mutually exclusive) events</span></strong><span class="koboSpan" id="kobo.478.1">: Two or more events that don’t share any outcomes. </span><span class="koboSpan" id="kobo.478.2">In other </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.479.1">words, their respective sample space subsets are disjoint. </span><span class="koboSpan" id="kobo.479.2">For example, the events of odd or even dice rolls are disjoint. </span><span class="koboSpan" id="kobo.479.3">The following is true for </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">disjoint events:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.481.1">The joint probability for these events to occur together is P(A</span><span class="koboSpan" id="kobo.482.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/77.png" style="vertical-align:-0.031em;height:0.566em;width:0.619em"/></span><span class="koboSpan" id="kobo.483.1">B) = 0</span></li><li lang="en-GB"><span class="koboSpan" id="kobo.484.1">The sum of the probabilities of disjoint events </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">is </span></span><span class="No-Break"><span class="koboSpan" id="kobo.486.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow /&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;disjoint&lt;/mtext&gt;&lt;mtext&gt;events&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/78.png" style="vertical-align:-0.333em;height:1.095em;width:10.175em"/></span></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.487.1">Jointly exhaustive events</span></strong><span class="koboSpan" id="kobo.488.1">: The subsets of such events contain the whole sample space</span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.489.1"> between themselves. </span><span class="koboSpan" id="kobo.489.2">For example, events A = {1, 2, 3} and B = {4, 5, 6} are jointly exhaustive because, together, they cover the whole sample space S = {1, 2, 3, 4, 5, 6}. </span><span class="koboSpan" id="kobo.489.3">The following is true for the probability of jointly </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">exhaustive events:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.491.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow /&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;jointly exhaustive events&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/79.png" style="vertical-align:-0.387em;height:1.428em;width:14.191em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.492.1">Complement events</span></strong><span class="koboSpan" id="kobo.493.1">: Two or more events that are disjoint and jointly exhaustive at the same</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.494.1"> time. </span><span class="koboSpan" id="kobo.494.2">For example, odd and even dice roll events </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">are complementary.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.496.1">Union of events</span></strong><span class="koboSpan" id="kobo.497.1">: A set of events coming from either A or B (not necessarily in both). </span><span class="koboSpan" id="kobo.497.2">The probability</span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.498.1"> of this </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">union is:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.500.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P(A&lt;/mml:mtext&gt;&lt;mml:mtext&gt;∪&lt;/mml:mtext&gt;&lt;mml:mtext&gt;B)=P(A)+P(B)-P(A∩B)&lt;/mml:mtext&gt;&lt;/mml:math&gt;" src="image/80.png" style="vertical-align:-0.169em;height:0.877em;width:11.612em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.501.1">So far, we have discussed</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.502.1"> independent events. </span><span class="koboSpan" id="kobo.502.2">Now, let’s see what happens if the events</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.503.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">not independent.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.505.1">Conditional probability and the Bayes rule</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.506.1">If event A occurs before B and the occurrence of A changes the probability of the occurrence of B, then the two events are dependent. </span><span class="koboSpan" id="kobo.506.2">To understand this, let’s imagine that we draw consecutive cards from the deck. </span><span class="koboSpan" id="kobo.506.3">When the deck is full, the probability of drawing a spade is P(spade) = 13/52 = 0.25. </span><span class="koboSpan" id="kobo.506.4">However, once we’ve drawn the first spade, the probability</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.507.1"> of picking a spade on the second turn changes. </span><span class="koboSpan" id="kobo.507.2">Now, we only have 51 cards and 1 less spade. </span><span class="koboSpan" id="kobo.507.3">The probability of the second draw is called </span><strong class="bold"><span class="koboSpan" id="kobo.508.1">conditional probability</span></strong><span class="koboSpan" id="kobo.509.1">, P(B|A). </span><span class="koboSpan" id="kobo.509.2">This is the probability of event B (the second draw of a spade) to occur, given that event A (the first draw of a spade) has occurred. </span><span class="koboSpan" id="kobo.509.3">The probability of picking a spade on the second draw is P(spade2|spade1) = 12/51 = </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">0.235.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.511.1">We can extend the joint probability formula (introduced in the preceding section) for </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">dependent events:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.513.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P(A∩B)=P(A)&lt;/mml:mtext&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P(B|A)&lt;/mml:mtext&gt;&lt;/mml:math&gt;" src="image/81.png" style="vertical-align:-0.250em;height:1.000em;width:9.556em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.514.1">However, this formula is just a special case for two events. </span><span class="koboSpan" id="kobo.514.2">We can extend it even further</span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.515.1"> for multiple events, </span><span class="koboSpan" id="kobo.516.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/82.png" style="vertical-align:-0.340em;height:1.004em;width:4.415em"/></span><span class="koboSpan" id="kobo.517.1">. </span><span class="koboSpan" id="kobo.517.2">This new generic formula is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.518.1">chain rule </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.519.1">of probability</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.521.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;n&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;n&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;n-1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;n-1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/83.png" style="vertical-align:-0.383em;height:1.183em;width:24.256em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.522.1">For example, the chain rule for three events is </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.524.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;3&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;3&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;3&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;2&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;1&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/84.png" style="vertical-align:-0.390em;height:1.189em;width:35.512em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.525.1">We can use this property to derive the formula for the conditional </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">probability </span><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.527.1">itself:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.528.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P(B|A)&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A∩B&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mtext&gt;P(A)&lt;/mtext&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/85.png" style="vertical-align:-0.644em;height:1.745em;width:7.107em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.529.1">Let’s discuss the intuition </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">behind this:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.531.1">P(A </span></strong><strong class="bold"><span class="koboSpan" id="kobo.532.1">∩</span></strong><strong class="bold"><span class="koboSpan" id="kobo.533.1"> B)</span></strong><span class="koboSpan" id="kobo.534.1"> indicates that we’re only interested in the occurrences of B, if A has already occurred – that is, we’re interested in the joint occurrence of the events, hence the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">joint probability.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.536.1">P(A)</span></strong><span class="koboSpan" id="kobo.537.1"> indicates that we’re interested only in the subset of outcomes when event A has occurred. </span><span class="koboSpan" id="kobo.537.2">We already know that A has occurred, and therefore, we restrict our observations to </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">these outcomes.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.539.1">The following is true for </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">dependent events:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.541.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/86.png" style="vertical-align:-0.300em;height:1.098em;width:11.100em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.542.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;mml:mo&gt;∩&lt;/mml:mo&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;A&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/87.png" style="vertical-align:-0.300em;height:1.098em;width:10.997em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.543.1">We can use this rule to replace</span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.544.1"> the value of P(A∩B) in the conditional probability formula to derive what is known as the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.545.1">Bayes rule</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.547.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A∩B&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mtext&gt;=P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mtext&gt;=P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mtext&gt;⇔&lt;/mtext&gt;&lt;mtext&gt;  &lt;/mtext&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A∩B&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;B&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;A&lt;/mtext&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/88.png" style="vertical-align:-0.548em;height:1.906em;width:30.522em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.548.1">The Bayes rule makes</span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.549.1"> it possible to compute the conditional</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.550.1"> probability, P(B|A), if we know the opposite conditional probability, P(A|B). </span><span class="koboSpan" id="kobo.550.2">P(A) and P(B|A) are known as </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">prior probability and posterior </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.552.1">probability</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">, respectively.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.554.1">We can illustrate the Bayes rule with a classic example from the realm of medical testing. </span><span class="koboSpan" id="kobo.554.2">A patient is administered a medical test for a disease, which comes out positive. </span><span class="koboSpan" id="kobo.554.3">Most tests have a sensitivity value, which is the percentage chance of the test being positive when administered to people with a particular disease. </span><span class="koboSpan" id="kobo.554.4">Using this information, we’ll apply the Bayes rule to compute the actual probability of the patient having the disease, given that the test</span><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.555.1"> is positive. </span><span class="koboSpan" id="kobo.555.2">We get </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">the following:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.557.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;has disease&lt;/mtext&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;test&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtext&gt;has disease&lt;/mtext&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;test&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mtext&gt;has disease&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;P&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;test&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/89.png" style="vertical-align:-0.740em;height:2.101em;width:29.280em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.558.1">We can think of P(has disease) as the probability of the disease in the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">general population.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.560.1">Now, we’ll make some assumptions about the disease and the sensitivity of </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">the test:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.562.1">The test is 98% </span><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.563.1">sensitive – that is, it will detect only 98% of all positive cases: P(test=positive|has disease) = </span><a id="_idTextAnchor058"/><span class="No-Break"><span class="koboSpan" id="kobo.564.1">0.98</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.565.1">Two per cent of the people under 50 have this kind of disease: P(has disease) = </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">0.02</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.567.1">The test is positive for 3.9% of the population when administered to people under 50: P(test=positive) = </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">0.039</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.569.1">We can ask the following question: if a test is 98% sensitive and a 45-year-old person is administered the test, which turns</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.570.1"> out to be positive, what is the probability that they</span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.571.1"> have the disease? </span><span class="koboSpan" id="kobo.571.2">We can calculate it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">Bayes rule:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.573.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;has disease&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;test&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;has disease&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;test&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;has disease&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;P&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;test&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.02&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;0.98&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.039&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/90.png" style="vertical-align:-0.740em;height:2.101em;width:37.328em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.574.1">This example can serve as an introduction to the next section, where we’ll introduce the </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">confusion matrix.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.576.1">Confusion matrix</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.577.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.578.1">confusion matrix</span></strong><span class="koboSpan" id="kobo.579.1"> is used to evaluate the performance of a binary</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.580.1"> classification algorithm, similar to the medical test we introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.581.1">Conditional probability and the Bayes </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.582.1">rule</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.583.1"> section:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.584.1"><img alt="Figure 2.5 – Confusion matrix" src="image/B19627_02_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.585.1">Figure 2.5 – Confusion matrix</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.586.1">The relationship between</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.587.1"> the actual condition (</span><strong class="bold"><span class="koboSpan" id="kobo.588.1">P</span></strong><span class="koboSpan" id="kobo.589.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">N</span></strong><span class="koboSpan" id="kobo.591.1">) and the predicted outcome (</span><strong class="bold"><span class="koboSpan" id="kobo.592.1">PP</span></strong><span class="koboSpan" id="kobo.593.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.594.1">PN</span></strong><span class="koboSpan" id="kobo.595.1">) allows us to place the prediction in one of </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">four categories:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.597.1">True positive (TP)</span></strong><span class="koboSpan" id="kobo.598.1">: The actual and predicted values are </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">both true</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.600.1">True negative (TN)</span></strong><span class="koboSpan" id="kobo.601.1">: The actual and predicted values are </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">both false</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.603.1">False positive (FP)</span></strong><span class="koboSpan" id="kobo.604.1">: The actual value is negative, but the classification algorithm has predicted a </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">positive value</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.606.1">False negative (FN)</span></strong><span class="koboSpan" id="kobo.607.1">: The actual value is positive, but the algorithm has predicted a </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">negative value</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.609.1">Based on these categories, we’ll introduce some measures that evaluate different aspects of the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">the algorithm:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.611.1">Accuracy</span></strong><span class="koboSpan" id="kobo.612.1"> = </span><span class="koboSpan" id="kobo.613.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;TP&lt;/mtext&gt;&lt;mtext&gt;+&lt;/mtext&gt;&lt;mtext&gt;TN&lt;/mtext&gt;&lt;/mrow&gt;&lt;mtext&gt;TP + FP + FN+TN&lt;/mtext&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/91.png" style="vertical-align:-0.195em;height:0.917em;width:4.405em"/></span><span class="koboSpan" id="kobo.614.1">: The fraction of correct predictions among </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">all cases.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.616.1">Precision</span></strong><span class="koboSpan" id="kobo.617.1"> = </span><span class="koboSpan" id="kobo.618.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TP&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TP + FP&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/92.png" style="vertical-align:-0.190em;height:0.907em;width:1.928em"/></span><span class="koboSpan" id="kobo.619.1">: The fraction of positive predictions that were </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">actually correct.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.621.1">Recall</span></strong><span class="koboSpan" id="kobo.622.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">sensitivity</span></strong><span class="koboSpan" id="kobo.624.1">) = </span><span class="koboSpan" id="kobo.625.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TP&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TP + FN&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/93.png" style="vertical-align:-0.195em;height:0.912em;width:2.004em"/></span><span class="koboSpan" id="kobo.626.1">: The fraction of actual positive cases that were </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">predicted correctly.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.628.1">Specificity</span></strong><span class="koboSpan" id="kobo.629.1"> = </span><span class="koboSpan" id="kobo.630.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TN&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;TN + FP&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/94.png" style="vertical-align:-0.195em;height:0.917em;width:2.033em"/></span><span class="koboSpan" id="kobo.631.1">: The fraction of actual negative cases that were </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">predicted correctly.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.633.1">F1 score</span></strong><span class="koboSpan" id="kobo.634.1"> = </span><span class="koboSpan" id="kobo.635.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;precision&lt;/mml:mtext&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mtext&gt;recall&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;precision+recall&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/95.png" style="vertical-align:-0.351em;height:1.230em;width:5.295em"/></span><span class="koboSpan" id="kobo.636.1">: Represents the balance between precision and recall. </span><span class="koboSpan" id="kobo.636.2">Because of the multiplication of the two measures, the F1 score will have a high value when</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.637.1"> both measures have </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">high values.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.639.1">In the following section, we’ll discuss the field of differential calculus, which will help us to </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">train NNs.</span></span></p>
<h2 id="_idParaDest-34" lang="en-GB"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.641.1">Differential calculus</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.642.1">We can think</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.643.1"> of an ML algorithm</span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.644.1"> as a mathematical function with inputs and parameters (which is the case for NNs). </span><span class="koboSpan" id="kobo.644.2">Our goal is to adjust these parameters in a way that will allow the ML function to closely approximate some other target function. </span><span class="koboSpan" id="kobo.644.3">To do this, we need to know how the output</span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.645.1"> of the ML function changes when we change some of its parameters (called weights). </span><span class="koboSpan" id="kobo.645.2">Fortunately, differential calculus can help us here – it deals with the rate of change of a function with respect to a variable (parameter) that the function depends on. </span><span class="koboSpan" id="kobo.645.3">To understand how this works, we’ll start with a function, </span><em class="italic"><span class="koboSpan" id="kobo.646.1">f(x)</span></em><span class="koboSpan" id="kobo.647.1">, with a single parameter, </span><em class="italic"><span class="koboSpan" id="kobo.648.1">x</span></em><span class="koboSpan" id="kobo.649.1">, which has the </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">following graph:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.651.1"><img alt="Figure 2.6 – A sample graph of a function, f(x), with a single parameter, x. The function graph is denoted with an uninterrupted blue line; the slope is denoted with an interrupted red line" src="image/B19627_02_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.652.1">Figure 2.6 – A sample graph of a function, f(x), with a single parameter, x. </span><span class="koboSpan" id="kobo.652.2">The function graph is denoted with an uninterrupted blue line; the slope is denoted with an interrupted red line</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.653.1">We can approximate how </span><em class="italic"><span class="koboSpan" id="kobo.654.1">f(x)</span></em><span class="koboSpan" id="kobo.655.1"> changes with respect to </span><em class="italic"><span class="koboSpan" id="kobo.656.1">x</span></em><span class="koboSpan" id="kobo.657.1"> for any value of </span><em class="italic"><span class="koboSpan" id="kobo.658.1">x</span></em><span class="koboSpan" id="kobo.659.1"> by calculating the slope of the function</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.660.1"> for that value. </span><span class="koboSpan" id="kobo.660.2">If the slope</span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.661.1"> is positive, the function increases, and it decreases if the slope is negative. </span><span class="koboSpan" id="kobo.661.2">The steepness of the slope indicates the rate of change of the function for that value. </span><span class="koboSpan" id="kobo.661.3">We can calculate the slope with the </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.663.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;slope&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/96.png" style="vertical-align:-0.473em;height:1.724em;width:11.481em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.664.1">T</span><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.665.1">he idea here is simple – we calculate the difference between the two values of </span><em class="italic"><span class="koboSpan" id="kobo.666.1">f</span></em><span class="koboSpan" id="kobo.667.1"> at </span><em class="italic"><span class="koboSpan" id="kobo.668.1">x</span></em><span class="koboSpan" id="kobo.669.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.670.1">x+Δx</span></em><span class="koboSpan" id="kobo.671.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.672.1">Δx</span></em><span class="koboSpan" id="kobo.673.1"> is a very small value) – </span><em class="italic"><span class="koboSpan" id="kobo.674.1">Δy = f(x + Δx) - f(x).</span></em><span class="koboSpan" id="kobo.675.1"> The ratio between </span><em class="italic"><span class="koboSpan" id="kobo.676.1">Δy</span></em><span class="koboSpan" id="kobo.677.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.678.1">Δx</span></em><span class="koboSpan" id="kobo.679.1"> gives us the slope. </span><span class="koboSpan" id="kobo.679.2">But why is </span><em class="italic"><span class="koboSpan" id="kobo.680.1">Δx</span></em><span class="koboSpan" id="kobo.681.1"> required to be small? </span><span class="koboSpan" id="kobo.681.2">If </span><em class="italic"><span class="koboSpan" id="kobo.682.1">Δx</span></em><span class="koboSpan" id="kobo.683.1"> is too big, the part of the function graph between </span><em class="italic"><span class="koboSpan" id="kobo.684.1">x</span></em><span class="koboSpan" id="kobo.685.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.686.1">x+Δx</span></em><span class="koboSpan" id="kobo.687.1"> may change significantly, and the slope measurement would be inaccurate. </span><span class="koboSpan" id="kobo.687.2">When </span><em class="italic"><span class="koboSpan" id="kobo.688.1">Δx</span></em><span class="koboSpan" id="kobo.689.1"> converges to 0, we’ll assume that our slope approximates the actual slope at a single point of the graph. </span><span class="koboSpan" id="kobo.689.2">In this case, we call the slope the </span><strong class="bold"><span class="koboSpan" id="kobo.690.1">first derivative</span></strong><span class="koboSpan" id="kobo.691.1"> of </span><em class="italic"><span class="koboSpan" id="kobo.692.1">f(x)</span></em><span class="koboSpan" id="kobo.693.1">. </span><span class="koboSpan" id="kobo.693.2">We can express this in mathematical terms via the </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">following equation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.695.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/97.png" style="vertical-align:-0.513em;height:1.764em;width:12.234em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.696.1">Here, </span><span class="koboSpan" id="kobo.697.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mtext&gt;Δ&lt;/mtext&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/98.png" style="vertical-align:-0.455em;height:1.166em;width:1.482em"/></span><span class="koboSpan" id="kobo.698.1">is the mathematical concept of the limit (</span><em class="italic"><span class="koboSpan" id="kobo.699.1">Δx</span></em><span class="koboSpan" id="kobo.700.1"> approaches 0), and </span><em class="italic"><span class="koboSpan" id="kobo.701.1">f’(x)</span></em><span class="koboSpan" id="kobo.702.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.703.1">dy/dx</span></em><span class="koboSpan" id="kobo.704.1"> are Lagrange’s and Leibniz’s not</span><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.705.1">ations</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.706.1"> for derivatives, respectively. </span><span class="koboSpan" id="kobo.706.2">The process of finding the derivative of </span><em class="italic"><span class="koboSpan" id="kobo.707.1">f</span></em><span class="koboSpan" id="kobo.708.1"> is called </span><strong class="bold"><span class="koboSpan" id="kobo.709.1">differentiation</span></strong><span class="koboSpan" id="kobo.710.1">. </span><span class="koboSpan" id="kobo.710.2">The following diagram shows slopes at different values </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.712.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.714.1"><img alt="Figure 2.7 – Slopes at different values of x" src="image/B19627_02_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.715.1">Figure 2.7 – Slopes at different values of x</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.716.1">The points, where </span><em class="italic"><span class="koboSpan" id="kobo.717.1">f</span></em><span class="koboSpan" id="kobo.718.1"> neither</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.719.1"> increases nor decreases as we change </span><em class="italic"><span class="koboSpan" id="kobo.720.1">x</span></em><span class="koboSpan" id="kobo.721.1">, are called </span><strong class="bold"><span class="koboSpan" id="kobo.722.1">saddle points</span></strong><span class="koboSpan" id="kobo.723.1">. </span><span class="koboSpan" id="kobo.723.2">The values of </span><em class="italic"><span class="koboSpan" id="kobo.724.1">f</span></em><span class="koboSpan" id="kobo.725.1"> at the saddle point</span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.726.1">s are called the </span><strong class="bold"><span class="koboSpan" id="kobo.727.1">local minimum</span></strong><span class="koboSpan" id="kobo.728.1"> and the </span><strong class="bold"><span class="koboSpan" id="kobo.729.1">local maximum</span></strong><span class="koboSpan" id="kobo.730.1">. </span><span class="koboSpan" id="kobo.730.2">Conversely, the slopes of </span><em class="italic"><span class="koboSpan" id="kobo.731.1">f</span></em><span class="koboSpan" id="kobo.732.1"> at the saddle</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.733.1"> points</span><a id="_idIndexMarker209"/> <span class="No-Break"><span class="koboSpan" id="kobo.734.1">are 0.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.735.1">So far, we have discussed a function with a single parameter, </span><em class="italic"><span class="koboSpan" id="kobo.736.1">x</span></em><span class="koboSpan" id="kobo.737.1">. </span><span class="koboSpan" id="kobo.737.2">Now, let’</span><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.738.1">s focus on a function with multiple</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.739.1"> parameters, </span><span class="koboSpan" id="kobo.740.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/99.png" style="vertical-align:-0.390em;height:1.101em;width:4.453em"/></span><span class="koboSpan" id="kobo.741.1">. </span><span class="koboSpan" id="kobo.741.2">The derivative of </span><em class="italic"><span class="koboSpan" id="kobo.742.1">f</span></em><span class="koboSpan" id="kobo.743.1"> with respect</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.744.1"> to any of the parameters, </span><span class="koboSpan" id="kobo.745.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/100.png" style="vertical-align:-0.340em;height:0.788em;width:0.613em"/></span><span class="koboSpan" id="kobo.746.1">, is called a </span><strong class="bold"><span class="koboSpan" id="kobo.747.1">partial derivative</span></strong><span class="koboSpan" id="kobo.748.1"> and is denoted by </span><span class="koboSpan" id="kobo.749.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/101.png" style="vertical-align:-0.257em;height:0.968em;width:2.184em"/></span><span class="koboSpan" id="kobo.750.1">. </span><span class="koboSpan" id="kobo.750.2">To compute the partial derivative, we will assume</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.751.1"> that all the other parameters, </span><span class="koboSpan" id="kobo.752.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;≠&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/102.png" style="vertical-align:-0.483em;height:0.996em;width:2.744em"/></span><span class="koboSpan" id="kobo.753.1">, are constants. </span><span class="koboSpan" id="kobo.753.2">We’ll denote the partial derivatives of the components of a vector </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">with </span></span><span class="No-Break"><span class="koboSpan" id="kobo.755.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∇&lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/103.png" style="vertical-align:-0.458em;height:1.253em;width:5.670em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.757.1">Finally, let’s discuss some useful </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">different</span><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.759.1">iation rules:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.760.1">Chain rule</span></strong><span class="koboSpan" id="kobo.761.1">: </span><em class="italic"><span class="koboSpan" id="kobo.762.1">f</span></em><span class="koboSpan" id="kobo.763.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.764.1">g</span></em><span class="koboSpan" id="kobo.765.1"> are functions</span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.766.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.767.1">h(x)= f(g(x))</span></em><span class="koboSpan" id="kobo.768.1">. </span><span class="koboSpan" id="kobo.768.2">The derivative of </span><em class="italic"><span class="koboSpan" id="kobo.769.1">f</span></em><span class="koboSpan" id="kobo.770.1"> with respect to </span><em class="italic"><span class="koboSpan" id="kobo.771.1">x</span></em><span class="koboSpan" id="kobo.772.1"> for any </span><em class="italic"><span class="koboSpan" id="kobo.773.1">x</span></em><span class="koboSpan" id="kobo.774.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">as follows:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.776.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;or&lt;/mml:mtext&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/104.png" style="vertical-align:-0.721em;height:1.761em;width:26.042em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.777.1">Sum rule</span></strong><span class="koboSpan" id="kobo.778.1">: </span><em class="italic"><span class="koboSpan" id="kobo.779.1">f</span></em><span class="koboSpan" id="kobo.780.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.781.1">g</span></em><span class="koboSpan" id="kobo.782.1"> are some functions</span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.783.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.784.1">h(x) = f(x) + g(x)</span></em><span class="koboSpan" id="kobo.785.1">. </span><span class="koboSpan" id="kobo.785.2">The sum rule states </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">the following:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.787.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⟹&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/105.png" style="vertical-align:-0.307em;height:1.067em;width:24.152em"/></span></p>
<ul>
<li lang="en-GB"><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.788.1">Common functions</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.790.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/106.png" style="vertical-align:-0.012em;height:0.689em;width:2.681em"/></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.791.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/107.png" style="vertical-align:-0.134em;height:0.811em;width:3.873em"/></span><span class="koboSpan" id="kobo.792.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.793.1">a</span></em><span class="koboSpan" id="kobo.794.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">a</span></span><span class="No-Break"><a id="_idIndexMarker215"/></span><span class="No-Break"><span class="koboSpan" id="kobo.796.1"> scalar</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.797.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/108.png" style="vertical-align:-0.012em;height:0.689em;width:2.698em"/></span><span class="koboSpan" id="kobo.798.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.799.1">a</span></em><span class="koboSpan" id="kobo.800.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">a scalar</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.802.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/109.png" style="vertical-align:-0.012em;height:0.715em;width:3.331em"/></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.803.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/110.png" style="vertical-align:-0.062em;height:0.739em;width:3.913em"/></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.804.1">The mathematical foundations of NNs and NNs themselves form a kind of knowledge hierarchy. </span><span class="koboSpan" id="kobo.804.2">Think of the topics we discussed in </span><em class="italic"><span class="koboSpan" id="kobo.805.1">The math of NNs</span></em><span class="koboSpan" id="kobo.806.1"> section as the building blocks of NNs. </span><span class="koboSpan" id="kobo.806.2">They represent an important step toward a comprehensive understanding of NNs, which</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.807.1"> will help us throughout this book and beyond. </span><span class="koboSpan" id="kobo.807.2">Now, we have the necessary preparation</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.808.1"> to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">full-fledged NNs.</span></span></p>
<h1 id="_idParaDest-35" lang="en-GB"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.810.1">An introduction to NNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.811.1">We can describe NNs as a mathematical model</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.812.1"> for information processing. </span><span class="koboSpan" id="kobo.812.2">As discussed in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.813.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.814.1">, this is a good way to describe any ML algorithm, but in this chapter, it has a specific meaning in the context of NNs. </span><span class="koboSpan" id="kobo.814.2">An NN is not a fixed program but rather a model, a system that processes information, or inputs. </span><span class="koboSpan" id="kobo.814.3">The characteristics</span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.815.1"> of an NN are </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.817.1">Information processing</span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.818.1"> occurs in its simplest form, over simple elements </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.820.1">units</span></strong></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.821.1">Units are connected, and they exchange signals between them through </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">connection links</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.823.1">Connection links between units can be stronger or weaker, and this determines how information </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">is processed</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.825.1">Each unit has an internal state that is determined by all the incoming connections from </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">other units</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.827.1">Each unit has a different </span><strong class="bold"><span class="koboSpan" id="kobo.828.1">activation function</span></strong><span class="koboSpan" id="kobo.829.1"> that is calculated on its state and determines</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.830.1"> its </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">output</span></span><span class="No-Break"><a id="_idIndexMarker222"/></span><span class="No-Break"><span class="koboSpan" id="kobo.832.1"> signal</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.833.1">A more general description of an NN would be as a computational graph of mathematical operations, but we will learn more about </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">that later.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.835.1">We can identify two main characteristics</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.836.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">an NN:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.838.1">Neural net architecture</span></strong><span class="koboSpan" id="kobo.839.1">: This describes the set of connections – namely, feedforward, recurrent, multi- or single-layered, and so on – between the units, the number of layers, and the number of units in </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">each layer.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.841.1">Learning</span></strong><span class="koboSpan" id="kobo.842.1">: This describes what is commonly defined as training. </span><span class="koboSpan" id="kobo.842.2">The most common but not exclusive way to train an NN is with </span><strong class="bold"><span class="koboSpan" id="kobo.843.1">gradient descent</span></strong><span class="koboSpan" id="kobo.844.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.845.1">GD</span></strong><span class="koboSpan" id="kobo.846.1">) and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.847.1">backpropagation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.848.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.849.1">BP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.851.1">We’ll start our discussion from the smallest building block of the NN – </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">the unit.</span></span></p>
<h2 id="_idParaDest-36" lang="en-GB"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.853.1">Units – the smallest NN building block</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.854.1">Units are mathematical functions</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.855.1"> that can be defined</span><a id="_idIndexMarker225"/> <span class="No-Break"><span class="koboSpan" id="kobo.856.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.857.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/111.png" style="vertical-align:-0.817em;height:2.177em;width:7.775em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.858.1">Here, we do </span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">the following:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.860.1">We compute the weighted sum </span><span class="koboSpan" id="kobo.861.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow /&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/112.png" style="vertical-align:-0.343em;height:1.105em;width:4.287em"/></span><span class="koboSpan" id="kobo.862.1"> (also known as an activation value). </span><span class="koboSpan" id="kobo.862.2">Let’s focus on the components</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.863.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">this sum:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.865.1">The inputs </span><span class="koboSpan" id="kobo.866.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/113.png" style="vertical-align:-0.340em;height:0.788em;width:0.670em"/></span><span class="koboSpan" id="kobo.867.1"> are numerical values that represent either the outputs of other units of the network, or the values of the input </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">data itself</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.869.1">The weights </span><span class="koboSpan" id="kobo.870.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/114.png" style="vertical-align:-0.340em;height:0.788em;width:0.835em"/></span><span class="koboSpan" id="kobo.871.1"> are numerical values that represent either the strength of the inputs or the strength of the connections between </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">the units</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.873.1">The weight </span><em class="italic"><span class="koboSpan" id="kobo.874.1">b</span></em><span class="koboSpan" id="kobo.875.1"> is a special weight called </span><strong class="bold"><span class="koboSpan" id="kobo.876.1">bias</span></strong><span class="koboSpan" id="kobo.877.1">, which represents an always-on input unit</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.878.1"> with a value </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">of 1</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.880.1">Alternatively, we can substitute </span><span class="koboSpan" id="kobo.881.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/115.png" style="vertical-align:-0.340em;height:0.788em;width:0.624em"/></span><span class="koboSpan" id="kobo.882.1"> and </span><span class="koboSpan" id="kobo.883.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/15.png" style="vertical-align:-0.340em;height:0.788em;width:0.822em"/></span><span class="koboSpan" id="kobo.884.1"> with their vector representations, where </span><span class="_-----MathTools-_Math_Variable_v-bold"><span class="koboSpan" id="kobo.885.1">x</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.886.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.887.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.888.1">→</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.889.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.890.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.891.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.892.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.893.1">[</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.894.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.895.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.896.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.897.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.898.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.899.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.900.1">2</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.901.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.902.1">…</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.903.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.904.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.905.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.906.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.907.1">]</span></span><span class="koboSpan" id="kobo.908.1"> and </span><span class="koboSpan" id="kobo.909.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/117.png" style="vertical-align:-0.390em;height:1.451em;width:10.162em"/></span><span class="koboSpan" id="kobo.910.1">. </span><span class="koboSpan" id="kobo.910.2">Here, the formula will use the dot product of the </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">two vectors:</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.912.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/118.png" style="vertical-align:-0.257em;height:1.368em;width:12.844em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.913.1">2.	</span><span class="koboSpan" id="kobo.913.2">The sum </span><span class="koboSpan" id="kobo.914.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow /&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/119.png" style="vertical-align:-0.343em;height:1.105em;width:4.625em"/></span><span class="koboSpan" id="kobo.915.1"> serves as input to the </span><strong class="bold"><span class="koboSpan" id="kobo.916.1">activation function</span></strong> <em class="italic"><span class="koboSpan" id="kobo.917.1">f</span></em><span class="koboSpan" id="kobo.918.1"> (also known as </span><strong class="bold"><span class="koboSpan" id="kobo.919.1">transfer function</span></strong><span class="koboSpan" id="kobo.920.1">). </span><span class="koboSpan" id="kobo.920.2">The output of </span><em class="italic"><span class="koboSpan" id="kobo.921.1">f</span></em><span class="koboSpan" id="kobo.922.1"> is a single </span><strong class="bold"><span class="koboSpan" id="kobo.923.1">numerical value</span></strong><span class="koboSpan" id="kobo.924.1">, which represents the output</span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.925.1"> of the unit itself. </span><span class="koboSpan" id="kobo.925.2">The activation function</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.926.1"> has the </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">following</span></span><span class="No-Break"><a id="_idIndexMarker230"/></span><span class="No-Break"><span class="koboSpan" id="kobo.928.1"> properties:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.929.1">Non-linear</span></strong><span class="koboSpan" id="kobo.930.1">: </span><em class="italic"><span class="koboSpan" id="kobo.931.1">f</span></em><span class="koboSpan" id="kobo.932.1"> is the source of non-linearity in an NN – if the NN was entirely linear, it would only be able to approximate other </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">linear functions</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.934.1">Differentiable</span></strong><span class="koboSpan" id="kobo.935.1">: This makes it possible to train the network with GD </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">and BP</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.937.1">Don’t worry if you don’t understand</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.938.1"> everything – we’ll discuss activation functions in detail later in </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">the chapter.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.940.1">The following diagram (left) shows </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">a unit:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<span class="koboSpan" id="kobo.942.1"><img alt="Figure 2.8 – Left: a unit and its equivalent formula, and right: a geometric representation of a perceptron" src="image/B19627_02_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.943.1">Figure 2.8 – Left: a unit and its equivalent formula, and right: a geometric representation of a perceptron</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.944.1">The input vector </span><strong class="bold"><span class="koboSpan" id="kobo.945.1">x</span></strong><span class="koboSpan" id="kobo.946.1"> will be perpendicular to the weight vector </span><strong class="bold"><span class="koboSpan" id="kobo.947.1">w</span></strong><span class="koboSpan" id="kobo.948.1"> if </span><span class="koboSpan" id="kobo.949.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/120.png" style="vertical-align:-0.012em;height:0.646em;width:3.970em"/></span><span class="koboSpan" id="kobo.950.1">. </span><span class="koboSpan" id="kobo.950.2">Therefore, all </span><strong class="bold"><span class="koboSpan" id="kobo.951.1">x</span></strong><span class="koboSpan" id="kobo.952.1"> vectors, where </span><span class="koboSpan" id="kobo.953.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/121.png" style="vertical-align:-0.012em;height:0.646em;width:3.978em"/></span><span class="koboSpan" id="kobo.954.1">, define a hyperplane in vector space </span><span class="koboSpan" id="kobo.955.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/122.png" style="vertical-align:-0.000em;height:0.661em;width:1.110em"/></span><span class="koboSpan" id="kobo.956.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.957.1">n</span></em><span class="koboSpan" id="kobo.958.1"> is the dimension of </span><strong class="bold"><span class="koboSpan" id="kobo.959.1">x</span></strong><span class="koboSpan" id="kobo.960.1">. </span><span class="koboSpan" id="kobo.960.2">In the case of two-dimensional input </span><span class="koboSpan" id="kobo.961.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/123.png" style="vertical-align:-0.383em;height:0.880em;width:2.534em"/></span><span class="koboSpan" id="kobo.962.1">, we can represent the hyperplane as a line. </span><span class="koboSpan" id="kobo.962.2">This could</span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.963.1"> be illustrated with the perceptron (or </span><strong class="bold"><span class="koboSpan" id="kobo.964.1">binary classifier</span></strong><span class="koboSpan" id="kobo.965.1">) – a unit with a </span><strong class="bold"><span class="koboSpan" id="kobo.966.1">threshold activation function</span></strong><span class="koboSpan" id="kobo.967.1">, </span><span class="koboSpan" id="kobo.968.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mtext&gt;else&lt;/mtext&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/124.png" style="vertical-align:-0.257em;height:0.968em;width:4.262em"/></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.969.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mtext&gt;else&lt;/mtext&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/125.png" style="vertical-align:-0.012em;height:0.723em;width:5.036em"/></span></span><span class="koboSpan" id="kobo.970.1">, that classifies</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.971.1"> its input in one of the two classes. </span><span class="koboSpan" id="kobo.971.2">The geometric representation of the perceptron</span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.972.1"> with two inputs </span><span class="koboSpan" id="kobo.973.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/126.png" style="vertical-align:-0.383em;height:0.880em;width:2.515em"/></span><span class="koboSpan" id="kobo.974.1"> is a line (or </span><strong class="bold"><span class="koboSpan" id="kobo.975.1">decision boundary</span></strong><span class="koboSpan" id="kobo.976.1">) separating the two classes (to the right in the </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">preceding diagram).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.978.1">In the preceding diagram, we can also see that the role of the bias, </span><em class="italic"><span class="koboSpan" id="kobo.979.1">b</span></em><span class="koboSpan" id="kobo.980.1">, is to allow the hyperplane to shift away from the center of the coordinate system. </span><span class="koboSpan" id="kobo.980.2">If we don’t use bias, the unit will have limited </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">representation power.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.982.1">The unit is a generalization of several algorithms we introduced in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.983.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.984.1">:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.985.1">A unit with an identity activation function </span><em class="italic"><span class="koboSpan" id="kobo.986.1">f(x) = x</span></em><span class="koboSpan" id="kobo.987.1"> is equivalent to </span><strong class="bold"><span class="koboSpan" id="kobo.988.1">multiple </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.989.1">linear regression</span></strong></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.990.1">A unit with a </span><strong class="bold"><span class="koboSpan" id="kobo.991.1">sigmoid activation</span></strong><span class="koboSpan" id="kobo.992.1"> function is equivalent to </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">logistic regression</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.994.1">A unit with a threshold activation function is equivalent to </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">a perceptron</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.996.1">We already know from </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.997.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.998.1"> that the perceptron (hence the unit) only works with linearly separable classes, and now we know that is because it defines a hyperplane. </span><span class="koboSpan" id="kobo.998.2">This imposes a serious limitation</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.999.1"> on the unit because</span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.1000.1"> it cannot classify linearly inseparable problems – even simple ones such as </span><strong class="bold"><span class="koboSpan" id="kobo.1001.1">exclusive or</span></strong><span class="koboSpan" id="kobo.1002.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1003.1">XOR</span></strong><span class="koboSpan" id="kobo.1004.1">). </span><span class="koboSpan" id="kobo.1004.2">To overcome this limitation, we’ll need</span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.1005.1"> to organize the units in an NN. </span><span class="koboSpan" id="kobo.1005.2">However, before we discuss full-fledged NNs, we’ll focus on the next NN building block – </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">the layers.</span></span></p>
<h2 id="_idParaDest-37" lang="en-GB"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.1007.1">Layers as operations</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1008.1">An NN can have an indefinite number</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.1009.1"> of units, which are organized in interconnected layers. </span><span class="koboSpan" id="kobo.1009.2">A layer has the </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1011.1">It combines the scalar outputs of multiple units in a single output vector. </span><span class="koboSpan" id="kobo.1011.2">A unit can convey limited information because its output is a scalar. </span><span class="koboSpan" id="kobo.1011.3">By combining the unit outputs, instead of a single activation, we can now consider the vector in its entirety. </span><span class="koboSpan" id="kobo.1011.4">This way, we can convey a lot more information, not only because the vector has multiple values but also because the relative ratios between them carry </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">additional meaning.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1013.1">The units of one layer can be connected to the units of other layers, but not to other units of the same layer. </span><span class="koboSpan" id="kobo.1013.2">Because of this, we can parallelize the computation of the outputs of all units in a single layer (thereby increasing the computational speed). </span><span class="koboSpan" id="kobo.1013.3">This ability is one of the major reasons for the success of DL in </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">recent years.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1015.1">We can generalize multivariate regression to a layer, as opposed to only linear or logistic regression to a single unit. </span><span class="koboSpan" id="kobo.1015.2">In other words, we can approximate multiple values with a layer as opposed to a single value with a unit. </span><span class="koboSpan" id="kobo.1015.3">This happens in the case of classification output, where each output unit represents the probability the input belongs to a </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">certain class.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1017.1">In classical NNs (that is, NNs before DL, when</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.1018.1"> they were just one of many ML algorithms), the primary type of layer is the </span><strong class="bold"><span class="koboSpan" id="kobo.1019.1">fully c</span><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.1020.1">onnected</span></strong><span class="koboSpan" id="kobo.1021.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1022.1">FC</span></strong><span class="koboSpan" id="kobo.1023.1">) layer. </span><span class="koboSpan" id="kobo.1023.2">In this layer, every unit receives weighted input from all the components of the input vector, </span><strong class="bold"><span class="koboSpan" id="kobo.1024.1">x</span></strong><span class="koboSpan" id="kobo.1025.1">. </span><span class="koboSpan" id="kobo.1025.2">This can represent either the output of another layer in the network or a sample of the input dataset. </span><span class="koboSpan" id="kobo.1025.3">Let’s assume that the size of the input vector is </span><em class="italic"><span class="koboSpan" id="kobo.1026.1">m</span></em><span class="koboSpan" id="kobo.1027.1">, and that the FC layer has </span><em class="italic"><span class="koboSpan" id="kobo.1028.1">n</span></em><span class="koboSpan" id="kobo.1029.1"> units and an activation function, </span><em class="italic"><span class="koboSpan" id="kobo.1030.1">f</span></em><span class="koboSpan" id="kobo.1031.1">, which is the same for all the units. </span><span class="koboSpan" id="kobo.1031.2">Each of the </span><em class="italic"><span class="koboSpan" id="kobo.1032.1">n</span></em><span class="koboSpan" id="kobo.1033.1"> units will have </span><em class="italic"><span class="koboSpan" id="kobo.1034.1">m</span></em><span class="koboSpan" id="kobo.1035.1"> weights – one for each of the </span><em class="italic"><span class="koboSpan" id="kobo.1036.1">m</span></em><span class="koboSpan" id="kobo.1037.1"> inputs. </span><span class="koboSpan" id="kobo.1037.2">The following is a formula we can use for the output of a single unit, </span><em class="italic"><span class="koboSpan" id="kobo.1038.1">j</span></em><span class="koboSpan" id="kobo.1039.1">, of an FC layer. </span><span class="koboSpan" id="kobo.1039.2">It’s the same as the formula we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1040.1">Units – the smallest NN building block</span></em><span class="koboSpan" id="kobo.1041.1"> section, but we’ll include the unit </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">index here:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1043.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/127.png" style="vertical-align:-0.817em;height:2.177em;width:8.182em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1044.1">Here, </span><span class="koboSpan" id="kobo.1045.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/128.png" style="vertical-align:-0.483em;height:0.931em;width:1.014em"/></span><span class="koboSpan" id="kobo.1046.1"> is the weight between the </span><em class="italic"><span class="koboSpan" id="kobo.1047.1">j</span></em><span class="koboSpan" id="kobo.1048.1">-th layer unit and the </span><em class="italic"><span class="koboSpan" id="kobo.1049.1">i</span></em><span class="koboSpan" id="kobo.1050.1">-th value of the input vector, </span><strong class="bold"><span class="koboSpan" id="kobo.1051.1">x</span></strong><span class="koboSpan" id="kobo.1052.1">. </span><span class="koboSpan" id="kobo.1052.2">We can represent the weights connecting the elements of </span><strong class="bold"><span class="koboSpan" id="kobo.1053.1">x</span></strong><span class="koboSpan" id="kobo.1054.1"> to the units as an </span><em class="italic"><span class="koboSpan" id="kobo.1055.1">m×n</span></em><span class="koboSpan" id="kobo.1056.1"> matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.1057.1">W</span></strong><span class="koboSpan" id="kobo.1058.1">. </span><span class="koboSpan" id="kobo.1058.2">Each column of </span><strong class="bold"><span class="koboSpan" id="kobo.1059.1">W</span></strong><span class="koboSpan" id="kobo.1060.1"> represents the weight vector of all the inputs to a single unit of the layer. </span><span class="koboSpan" id="kobo.1060.2">In this case, the output vector of the layer, </span><strong class="bold"><span class="koboSpan" id="kobo.1061.1">y</span></strong><span class="koboSpan" id="kobo.1062.1">, is the result of </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">matrix-vector multiplication.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1064.1">We can also combine multiple input</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.1065.1"> sample vectors, </span><span class="koboSpan" id="kobo.1066.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/129.png" style="vertical-align:-0.000em;height:0.734em;width:1.083em"/></span><span class="koboSpan" id="kobo.1067.1">, in an input matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.1068.1">X</span></strong><span class="koboSpan" id="kobo.1069.1">, where each input data vector, </span><span class="koboSpan" id="kobo.1070.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/130.png" style="vertical-align:-0.000em;height:0.734em;width:1.249em"/></span><span class="koboSpan" id="kobo.1071.1">, is represented by a row in </span><strong class="bold"><span class="koboSpan" id="kobo.1072.1">X</span></strong><span class="koboSpan" id="kobo.1073.1">. </span><span class="koboSpan" id="kobo.1073.2">The matrix itself is referred to as a </span><strong class="bold"><span class="koboSpan" id="kobo.1074.1">batch</span></strong><span class="koboSpan" id="kobo.1075.1">. </span><span class="koboSpan" id="kobo.1075.2">Then, we’ll simultaneously compute all output</span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.1076.1"> vectors, </span><span class="koboSpan" id="kobo.1077.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/131.png" style="vertical-align:-0.244em;height:0.978em;width:1.077em"/></span><span class="koboSpan" id="kobo.1078.1">, corresponding to the input samples, </span><span class="koboSpan" id="kobo.1079.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/132.png" style="vertical-align:-0.000em;height:0.734em;width:1.094em"/></span><span class="koboSpan" id="kobo.1080.1">. </span><span class="koboSpan" id="kobo.1080.2">In this case, we will have matrix-matrix multiplication, </span><strong class="bold"><span class="koboSpan" id="kobo.1081.1">XW</span></strong><span class="koboSpan" id="kobo.1082.1">, and the layer output is also a </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">matrix, </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1084.1">Y</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1086.1">The following diagram shows an example of an FC layer, as well as its equivalent formulas in the batch and single </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">sample scenarios:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<span class="koboSpan" id="kobo.1088.1"><img alt="Figure 2.9 – An FC layer with vector/matrix inputs and outputs and its equivalent formulas" src="image/B19627_02_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1089.1">Figure 2.9 – An FC layer with vector/matrix inputs and outputs and its equivalent formulas</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1090.1">We have explicitly separated the bias and input weight matrices, but in practice, the underlying implementation may use a shared weight matrix and append an additional row of 1s to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">input data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1092.1">So far, we represented the input data</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.1093.1"> samples as vectors, which we can combine in a matrix. </span><span class="koboSpan" id="kobo.1093.2">However, the input data can have more dimensions. </span><span class="koboSpan" id="kobo.1093.3">For example, we can represent an RGB image with three dimensions – three two-dimensional channels (one channel for each color). </span><span class="koboSpan" id="kobo.1093.4">To combine multiple</span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.1094.1"> images in a batch, we’ll need a fourth dimension. </span><span class="koboSpan" id="kobo.1094.2">In such cases, we can use input/output </span><strong class="bold"><span class="koboSpan" id="kobo.1095.1">tensors</span></strong><span class="koboSpan" id="kobo.1096.1"> instead </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">of matrices.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1098.1">We’ll also use different types of layers to process multidimensional data. </span><span class="koboSpan" id="kobo.1098.2">One such type is the convolutional layer, which we’ll discuss in </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1099.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.1100.1">. </span><span class="koboSpan" id="kobo.1100.2">We have many other layer types, such as attention, pooling, and so on. </span><span class="koboSpan" id="kobo.1100.3">Some of the layers have trainable weights (FC, attention, convolutional), while others don’t (pooling). </span><span class="koboSpan" id="kobo.1100.4">We can also use the terms functions or operations interchangeably with the layer. </span><span class="koboSpan" id="kobo.1100.5">For example, in TensorFlow and PyTorch, the FC layer we just described is a combination of two sequential operations. </span><span class="koboSpan" id="kobo.1100.6">First, we perform the weighted sum of the weights and inputs, and then we feed the result as an input to the activation function operation. </span><span class="koboSpan" id="kobo.1100.7">In practice (that is, when working with DL libraries), the basic building block of an NN is not the unit but an operation that takes one or more tensors as input and outputs one or </span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">more tensors:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<span class="koboSpan" id="kobo.1102.1"><img alt="Figure 2.10 – A function (or operation) with input and output tensors" src="image/B19627_02_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1103.1">Figure 2.10 – A function (or operation) with input and output tensors</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1104.1">Finally, we have all the necessary information</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.1105.1"> to discuss NNs in their </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">full glory.</span></span></p>
<h2 id="_idParaDest-38" lang="en-GB"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.1107.1">Multi-layer NNs</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1108.1">As we have mentioned</span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.1109.1"> several times, single-layer neural nets</span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.1110.1"> can only classify linearly separable classes. </span><span class="koboSpan" id="kobo.1110.2">However, there is nothing that prevents us from introducing more layers</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.1111.1"> between the input and the output. </span><span class="koboSpan" id="kobo.1111.2">These extra layers are called hidden layers. </span><span class="koboSpan" id="kobo.1111.3">The following diagram demonstrates a three-layer fully connected NN with two </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">hidden layers:</span></span></p>
<p class="IMG---Figure" lang="en-GB"> </p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<span class="koboSpan" id="kobo.1113.1"><img alt="Figure 2.11 – Multi-layer feed-forward network" src="image/B19627_02_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1114.1">Figure 2.11 – Multi-layer feed-forward network</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1115.1">The input layer has </span><em class="italic"><span class="koboSpan" id="kobo.1116.1">k</span></em><span class="koboSpan" id="kobo.1117.1"> input units, the first hidden</span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.1118.1"> layer has </span><em class="italic"><span class="koboSpan" id="kobo.1119.1">n</span></em><span class="koboSpan" id="kobo.1120.1"> hidden units, and the second hidden</span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.1121.1"> layer has </span><em class="italic"><span class="koboSpan" id="kobo.1122.1">m</span></em><span class="koboSpan" id="kobo.1123.1"> hidden units. </span><span class="koboSpan" id="kobo.1123.2">On top is the always-on bias unit. </span><span class="koboSpan" id="kobo.1123.3">The output, in this example, is the two units, </span><span class="koboSpan" id="kobo.1124.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/133.png" style="vertical-align:-0.333em;height:0.795em;width:0.660em"/></span><span class="koboSpan" id="kobo.1125.1"> and </span><span class="koboSpan" id="kobo.1126.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/134.png" style="vertical-align:-0.333em;height:0.795em;width:0.725em"/></span><span class="koboSpan" id="kobo.1127.1">, where each unit represents one of two possible classes. </span><span class="koboSpan" id="kobo.1127.2">The output unit with the highest activation value will determine the NN’s class prediction for the given input sample. </span><span class="koboSpan" id="kobo.1127.3">Each of the hidden units has a non-linear</span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.1128.1"> activation function, and the outputs have a special activation function called </span><strong class="bold"><span class="koboSpan" id="kobo.1129.1">softmax</span></strong><span class="koboSpan" id="kobo.1130.1">, which we’ll discuss in the </span><em class="italic"><span class="koboSpan" id="kobo.1131.1">Activation functions</span></em><span class="koboSpan" id="kobo.1132.1"> section. </span><span class="koboSpan" id="kobo.1132.2">A unit from one layer is connected to all units from the previous and following layers (hence fully connected). </span><span class="koboSpan" id="kobo.1132.3">Each connection has its own weight, </span><em class="italic"><span class="koboSpan" id="kobo.1133.1">w</span></em><span class="koboSpan" id="kobo.1134.1">, which is not depicted for reasons </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">of simplicity.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1136.1">As we mentioned in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1137.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.1138.1">, we can think of the hidden layers as the NN’s internal representation of the input data. </span><span class="koboSpan" id="kobo.1138.2">This is the way the NN understands the input sample with its own internal logic. </span><span class="koboSpan" id="kobo.1138.3">However, this internal representation is non-interpretable by humans. </span><span class="koboSpan" id="kobo.1138.4">To bridge the gap between the network’s representation and the actual data we’re interested in, we need the output layer. </span><span class="koboSpan" id="kobo.1138.5">You can think of this as a translator; we use it to understand the network’s logic, and at the same time, we can convert it to the actual target values that we are </span><span class="No-Break"><span class="koboSpan" id="kobo.1139.1">interested in.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1140.1">However, we are not limited </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.1141.1">to single-path networks with sequential layers, as shown in the preceding diagram. </span><span class="koboSpan" id="kobo.1141.2">The layers (or operations in general) form </span><strong class="bold"><span class="koboSpan" id="kobo.1142.1">directed acyclic graphs</span></strong><span class="koboSpan" id="kobo.1143.1">. </span><span class="koboSpan" id="kobo.1143.2">In such a graph, the information cannot pass twice through the same layer (no loops) and it flows in only one direction, from the input to the output. </span><span class="koboSpan" id="kobo.1143.3">The network in the preceding diagram is just a special case of a graph whose layers are connected sequentially. </span><span class="koboSpan" id="kobo.1143.4">The following diagram also depicts a valid NN with two input layers, a single output layer, and randomly interconnected hidden layers. </span><span class="koboSpan" id="kobo.1143.5">The layers are represented as operations </span><span class="koboSpan" id="kobo.1144.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/135.png" style="vertical-align:-0.257em;height:0.991em;width:2.060em"/></span><span class="koboSpan" id="kobo.1145.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1146.1">i</span></em><span class="koboSpan" id="kobo.1147.1"> is an index that helps us differentiate between </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">multiple operations):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<span class="koboSpan" id="kobo.1149.1"><img alt="Figure 2.12 – NN as a graph of operations" src="image/B19627_02_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1150.1">Figure 2.12 – NN as a graph of operations</span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1151.1">Recurrent networks</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1152.1">There is a special</span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.1153.1"> class of NNs called </span><strong class="bold"><span class="koboSpan" id="kobo.1154.1">recurrent networks</span></strong><span class="koboSpan" id="kobo.1155.1">, which represent a </span><strong class="bold"><span class="koboSpan" id="kobo.1156.1">directed cyclic graph</span></strong><span class="koboSpan" id="kobo.1157.1"> (they can have loops). </span><span class="koboSpan" id="kobo.1157.2">We’ll discuss them</span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.1158.1"> in detail in </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1159.1">Chapter 6</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1161.1">In this section, we introduced the most basic type</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.1162.1"> of NN – that is, the unit – gradually</span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.1163.1"> expanded it to layers, and then generalized it as a graph of operations. </span><span class="koboSpan" id="kobo.1163.2">We can also think of it in another way. </span><span class="koboSpan" id="kobo.1163.3">The operations have precise mathematical definitions. </span><span class="koboSpan" id="kobo.1163.4">Therefore, the NN, as a composition of functions is also a mathematical function, where the input data represents the function arguments, and the set of network weights, </span><em class="italic"><span class="koboSpan" id="kobo.1164.1">θ</span></em><span class="koboSpan" id="kobo.1165.1"> (a set of all weight matrices, </span><strong class="bold"><span class="koboSpan" id="kobo.1166.1">W</span></strong><span class="koboSpan" id="kobo.1167.1">), are its parameters. </span><span class="koboSpan" id="kobo.1167.2">We’ll denote it with either </span><span class="koboSpan" id="kobo.1168.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/136.png" style="vertical-align:-0.340em;height:1.051em;width:1.676em"/></span><span class="koboSpan" id="kobo.1169.1"> or </span><span class="koboSpan" id="kobo.1170.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/137.png" style="vertical-align:-0.257em;height:1.017em;width:2.265em"/></span><span class="koboSpan" id="kobo.1171.1">. </span><span class="koboSpan" id="kobo.1171.2">Let’s assume that, when an operation receives input from more than one source (input data or other operations), we use the elementwise sum to combine the multiple input tensors. </span><span class="koboSpan" id="kobo.1171.3">Then, we can represent the NN as a series of nested functions/operations. </span><span class="koboSpan" id="kobo.1171.4">The equivalent formula for the feed-forward network on the left is </span><span class="No-Break"><span class="koboSpan" id="kobo.1172.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1173.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/138.png" style="vertical-align:-0.533em;height:1.415em;width:28.067em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1174.1">Now that </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.1175.1">we’re familiar</span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.1176.1"> with the full NN architecture, let’s discuss the different types of </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">activation functions.</span></span></p>
<h2 id="_idParaDest-39" lang="en-GB"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.1178.1">Activation functions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1179.1">We now know that multi-layer networks</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.1180.1"> can classify linearly</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.1181.1"> inseparable classes, but to do this, they need to satisfy one more condition. </span><span class="koboSpan" id="kobo.1181.2">If the units don’t have activation functions, their output would be the weighted sum of the inputs, </span><span class="koboSpan" id="kobo.1182.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow /&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/139.png" style="vertical-align:-0.343em;height:1.105em;width:4.275em"/></span><span class="koboSpan" id="kobo.1183.1">, which is a linear function. </span><span class="koboSpan" id="kobo.1183.2">Then, the entire NN – that is, a composition of units – becomes a composition of linear functions, which is also a linear function. </span><span class="koboSpan" id="kobo.1183.3">This means that even if we add hidden layers, the network will still be equivalent to a simple linear regression model, with all its limitations. </span><span class="koboSpan" id="kobo.1183.4">To turn the network into a non-linear function, we’ll use non-linear activation functions for the units. </span><span class="koboSpan" id="kobo.1183.5">Usually, all units in the same layer have the same activation function, but different layers may have different activation functions. </span><span class="koboSpan" id="kobo.1183.6">We’ll start with three popular activation functions. </span><span class="koboSpan" id="kobo.1183.7">The first two are from the </span><em class="italic"><span class="koboSpan" id="kobo.1184.1">classic</span></em><span class="koboSpan" id="kobo.1185.1"> period of NNs, while the third </span><span class="No-Break"><span class="koboSpan" id="kobo.1186.1">is contemporary:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1187.1">Sigmoid</span></strong><span class="koboSpan" id="kobo.1188.1">: Its output is bounded between 0 and 1 and can be interpreted</span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.1189.1"> stochastically as the probability of the unit being active. </span><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.1190.1">Because of these properties, the sigmoid was the most popular activation function for a long time. </span><span class="koboSpan" id="kobo.1190.2">However, it also has some less desirable properties (more on that later), which led to its decline in use. </span><span class="koboSpan" id="kobo.1190.3">The following diagram shows the sigmoid function, its derivative, and their graphs (the derivative will be useful when we </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">discuss BP):</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer175">
<span class="koboSpan" id="kobo.1192.1"><img alt="Figure 2.13 – The sigmoid activation function" src="image/B19627_02_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1193.1">Figure 2.13 – The sigmoid activation function</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1194.1">Hyperbolic tangent (tanh)</span></strong><span class="koboSpan" id="kobo.1195.1">: The name speaks for itself. </span><span class="koboSpan" id="kobo.1195.2">The principal difference</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.1196.1"> with the sigmoid is that the tanh is in the (-1, 1) range. </span><span class="koboSpan" id="kobo.1196.2">The following diagram</span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.1197.1"> shows the tanh function, its derivative, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">their graphs:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer176">
<span class="koboSpan" id="kobo.1199.1"><img alt="Figure 2.14 – The hyperbolic tangent activation function" src="image/B19627_02_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1200.1">Figure 2.14 – The hyperbolic tangent activation function</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1201.1">Rectified Linear Unit (ReLU)</span></strong><span class="koboSpan" id="kobo.1202.1">: This is the new kid on the block (that is, compared to the </span><em class="italic"><span class="koboSpan" id="kobo.1203.1">veterans</span></em><span class="koboSpan" id="kobo.1204.1">). </span><span class="koboSpan" id="kobo.1204.2">ReLU </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.1205.1">was first successfully </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.1206.1">used in 2011 (see </span><em class="italic"><span class="koboSpan" id="kobo.1207.1">Deep Sparse Rectifier Neural Networks</span></em><span class="koboSpan" id="kobo.1208.1"> at </span><a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf"><span class="koboSpan" id="kobo.1209.1">http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf</span></a><span class="koboSpan" id="kobo.1210.1">). </span><span class="koboSpan" id="kobo.1210.2">The following diagram shows the ReLU function, its derivative, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">their graphs:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer177">
<span class="koboSpan" id="kobo.1212.1"><img alt="Figure 2.15 – The ReLU activation function" src="image/B19627_02_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1213.1">Figure 2.15 – The ReLU activation function</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1214.1">As we can see, the ReLU repeats its input when x &gt; 0 and stays at 0 otherwise. </span><span class="koboSpan" id="kobo.1214.2">This activation has several important advantages over sigmoid and tanh, which make it possible to train NNs</span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.1215.1"> with more hidden layers (that is, deeper networks). </span><span class="koboSpan" id="kobo.1215.2">We’ll discuss these advantages, as well as other types</span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.1216.1"> of activation</span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.1217.1"> functions, in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1218.1">Chapter 3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1219.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1220.1">In the next section, we’ll demonstrate how NNs can approximate </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">any function.</span></span></p>
<h2 id="_idParaDest-40" lang="en-GB"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.1222.1">The universal approximation theorem</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1223.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.1224.1">Multi-layer NNs</span></em><span class="koboSpan" id="kobo.1225.1"> section, we defined</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.1226.1"> the NN as a function, </span><span class="koboSpan" id="kobo.1227.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/140.png" style="vertical-align:-0.340em;height:1.051em;width:1.654em"/></span><span class="koboSpan" id="kobo.1228.1">, where </span><strong class="bold"><span class="koboSpan" id="kobo.1229.1">x</span></strong><span class="koboSpan" id="kobo.1230.1"> is the input data (most often a vector</span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.1231.1"> or a tensor) and </span><em class="italic"><span class="koboSpan" id="kobo.1232.1">θ</span></em><span class="koboSpan" id="kobo.1233.1"> is the NN weights. </span><span class="koboSpan" id="kobo.1233.2">Conversely, the training dataset is a collection of input samples and labels, which represents another, real-world, function </span><span class="koboSpan" id="kobo.1234.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/141.png" style="vertical-align:-0.257em;height:0.829em;width:1.593em"/></span><span class="koboSpan" id="kobo.1235.1">. </span><span class="koboSpan" id="kobo.1235.2">The NN function </span><span class="koboSpan" id="kobo.1236.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/142.png" style="vertical-align:-0.340em;height:1.051em;width:1.684em"/></span> <strong class="bold"><span class="koboSpan" id="kobo.1237.1">approximates</span></strong><span class="koboSpan" id="kobo.1238.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.1239.1">function </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/141.png" style="vertical-align:-0.257em;height:0.829em;width:1.593em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1242.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/144.png" style="vertical-align:-0.340em;height:1.051em;width:4.824em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1243.1">The universal approximation theorem states that any continuous function on compact subsets of </span><span class="koboSpan" id="kobo.1244.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/122.png" style="vertical-align:-0.000em;height:0.661em;width:1.107em"/></span><span class="koboSpan" id="kobo.1245.1"> can be approximated to an arbitrary degree of accuracy by a feedforward NN, with at least one hidden layer with a finite number of units and a non-linear activation. </span><span class="koboSpan" id="kobo.1245.2">This is significant because it tells us that there are no theoretical insurmountable limitations in terms of NNs. </span><span class="koboSpan" id="kobo.1245.3">In practice, an NN with a single hidden layer will perform poorly in many tasks, but at least we can aspire to a bright future with all-powerful NNs. </span><span class="koboSpan" id="kobo.1245.4">We can understand the universal approximation theorem with an </span><span class="No-Break"><span class="koboSpan" id="kobo.1246.1">intuitive example.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1247.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1248.1">The idea for the following</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.1249.1"> example was inspired by Michael A. </span><span class="koboSpan" id="kobo.1249.2">Nielsen’s book </span><em class="italic"><span class="koboSpan" id="kobo.1250.1">Neural Networks and Deep </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1251.1">Learning</span></em></span><span class="No-Break"> </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">(</span></span><a href="http://neuralnetworksanddeeplearning.com/"><span class="No-Break"><span class="koboSpan" id="kobo.1253.1">http://neuralnetworksanddeeplearnin</span></span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">g.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1256.1">We’ll design an NN</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.1257.1"> with one hidden layer that approximates the </span><strong class="bold"><span class="koboSpan" id="kobo.1258.1">boxcar function</span></strong><span class="koboSpan" id="kobo.1259.1"> (shown on the right in the following diagram). </span><span class="koboSpan" id="kobo.1259.2">This is a type of step function, which is 0 across </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.1260.1">all input values, except in a narrow range, where it is equal to a constant value, </span><em class="italic"><span class="koboSpan" id="kobo.1261.1">A</span></em><span class="koboSpan" id="kobo.1262.1">. </span><span class="koboSpan" id="kobo.1262.2">A series of </span><strong class="bold"><span class="koboSpan" id="kobo.1263.1">translated</span></strong><span class="koboSpan" id="kobo.1264.1"> step functions can approximate any continuous function on a compact subset of </span><span class="koboSpan" id="kobo.1265.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/146.png" style="vertical-align:-0.000em;height:0.661em;width:0.768em"/></span><span class="koboSpan" id="kobo.1266.1">, as shown in the left figure of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1267.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<span class="koboSpan" id="kobo.1268.1"><img alt="Figure 2.16 – Left: a continuous function approximation with a series of step functions, and right: a single boxcar step function" src="image/B19627_02_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1269.1">Figure 2.16 – Left: a continuous function approximation with a series of step functions, and right: a single boxcar step function</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1270.1">We’ll start building our boxcar NN with a single unit, with a single scalar input, </span><em class="italic"><span class="koboSpan" id="kobo.1271.1">x</span></em><span class="koboSpan" id="kobo.1272.1">, and sigmoid activation. </span><span class="koboSpan" id="kobo.1272.2">The following figure shows the unit, as well as its </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">equivalent formula:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<span class="koboSpan" id="kobo.1274.1"><img alt="Figure 2.17 – A unit with single input and sigmoid activation" src="image/B19627_02_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1275.1">Figure 2.17 – A unit with single input and sigmoid activation</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1276.1">In the following diagrams, we can see the unit output</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.1277.1"> for different values of </span><em class="italic"><span class="koboSpan" id="kobo.1278.1">b</span></em><span class="koboSpan" id="kobo.1279.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1280.1">w</span></em><span class="koboSpan" id="kobo.1281.1"> with inputs, </span><em class="italic"><span class="koboSpan" id="kobo.1282.1">x</span></em><span class="koboSpan" id="kobo.1283.1">, in the range [-</span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">10: 10]:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<span class="koboSpan" id="kobo.1285.1"><img alt="Figure 2.18 – The unit output based on diﬀerent values of w and b. The network input, x, is represented on the x axis" src="image/B19627_02_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1286.1">Figure 2.18 – The unit output based on diﬀerent values of w and b. </span><span class="koboSpan" id="kobo.1286.2">The network input, x, is represented on the x axis</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1287.1">We can see that the weight, </span><em class="italic"><span class="koboSpan" id="kobo.1288.1">w</span></em><span class="koboSpan" id="kobo.1289.1">, determines the steepness of the sigmoid</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.1290.1"> function. </span><span class="koboSpan" id="kobo.1290.2">We can also see that the formula, </span><em class="italic"><span class="koboSpan" id="kobo.1291.1">t = -b/w</span></em><span class="koboSpan" id="kobo.1292.1">, determines</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.1293.1"> the translation of the function along the </span><em class="italic"><span class="koboSpan" id="kobo.1294.1">x</span></em><span class="koboSpan" id="kobo.1295.1"> axis. </span><span class="koboSpan" id="kobo.1295.2">Let’s cover the different graphs in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1296.1">preceding diagram:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1297.1">Top-left</span></strong><span class="koboSpan" id="kobo.1298.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">Regular sigmoid</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1300.1">Top-right</span></strong><span class="koboSpan" id="kobo.1301.1">: A large weight, </span><em class="italic"><span class="koboSpan" id="kobo.1302.1">w</span></em><span class="koboSpan" id="kobo.1303.1">, amplifies the input, </span><em class="italic"><span class="koboSpan" id="kobo.1304.1">x</span></em><span class="koboSpan" id="kobo.1305.1">, to a point where the sigmoid output resembles </span><span class="No-Break"><span class="koboSpan" id="kobo.1306.1">threshold activation</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1307.1">Bottom-left</span></strong><span class="koboSpan" id="kobo.1308.1">: The bias, </span><em class="italic"><span class="koboSpan" id="kobo.1309.1">b</span></em><span class="koboSpan" id="kobo.1310.1">, translates the unit activation along the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1311.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1"> axis</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1313.1">Bottom-right</span></strong><span class="koboSpan" id="kobo.1314.1">: We can simultaneously translate the activation along the </span><em class="italic"><span class="koboSpan" id="kobo.1315.1">x</span></em><span class="koboSpan" id="kobo.1316.1"> axis with the bias, </span><em class="italic"><span class="koboSpan" id="kobo.1317.1">b</span></em><span class="koboSpan" id="kobo.1318.1">, and reverse the activation with a negative value of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">weight, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1320.1">w</span></em></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1321.1">We can intuitively see that the unit can implement all the pieces of the box function. </span><span class="koboSpan" id="kobo.1321.2">However, to create a full box function, we’ll have to combine two such units in an NN with one hidden layer. </span><span class="koboSpan" id="kobo.1321.3">The following diagram shows the NN architecture, along with the weights and biases of the units, as well as the box function that’s produced by </span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">the network:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<span class="koboSpan" id="kobo.1323.1"><img alt="Figure 2.19 – A box function approximation NN" src="image/B19627_02_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1324.1">Figure 2.19 – A box function approximation NN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1325.1">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.1326.1">it works:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1327.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/147.png" style="vertical-align:-0.012em;height:0.634em;width:2.820em"/></span><span class="koboSpan" id="kobo.1328.1">: the NN output </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">is 0.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1330.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/148.png" style="vertical-align:-0.012em;height:0.593em;width:1.786em"/></span><span class="koboSpan" id="kobo.1331.1">5: The top unit activates for the upper step of the function and stays active for all values </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1333.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/149.png" style="vertical-align:-0.012em;height:0.634em;width:2.491em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1334.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1335.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/150.png" style="vertical-align:-0.012em;height:0.646em;width:2.500em"/></span><span class="koboSpan" id="kobo.1336.1">0: The bottom unit activates for the bottom step of the function and stays active for all </span><span class="koboSpan" id="kobo.1337.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/151.png" style="vertical-align:-0.012em;height:0.646em;width:2.986em"/></span><span class="koboSpan" id="kobo.1338.1">. </span><span class="koboSpan" id="kobo.1338.2">The outputs of the two hidden units cancel each other out because of the weights in the output layer, which are the same but with </span><span class="No-Break"><span class="koboSpan" id="kobo.1339.1">opposite signs.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1340.1">The weights of the output layer determine the constant value, </span><em class="italic"><span class="koboSpan" id="kobo.1341.1">A = 5</span></em><span class="koboSpan" id="kobo.1342.1">, of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1343.1">boxcar function.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1344.1">The output of this network is 5 in the [-5:5] interval and 0 for all other inputs. </span><span class="koboSpan" id="kobo.1344.2">We can approximate additional boxes by adding more units to the hidden layer in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1345.1">similar manner.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1346.1">Now that we’re familiar with the structure</span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.1347.1"> of an NN, let’s focus</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.1348.1"> on their </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">training process.</span></span></p>
<h1 id="_idParaDest-41" lang="en-GB"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.1350.1">Training NNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1351.1">The NN function </span><span class="koboSpan" id="kobo.1352.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/152.png" style="vertical-align:-0.340em;height:1.051em;width:1.652em"/></span> <strong class="bold"><span class="koboSpan" id="kobo.1353.1">approximates</span></strong><span class="koboSpan" id="kobo.1354.1"> the function </span><span class="koboSpan" id="kobo.1355.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/153.png" style="vertical-align:-0.257em;height:0.829em;width:1.570em"/></span><span class="koboSpan" id="kobo.1356.1">: </span><span class="koboSpan" id="kobo.1357.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/154.png" style="vertical-align:-0.340em;height:1.051em;width:4.741em"/></span><span class="koboSpan" id="kobo.1358.1">. </span><span class="koboSpan" id="kobo.1358.2">The goal of the training</span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.1359.1"> is to find parameters, </span><em class="italic"><span class="koboSpan" id="kobo.1360.1">θ</span></em><span class="koboSpan" id="kobo.1361.1">, such that </span><span class="koboSpan" id="kobo.1362.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/155.png" style="vertical-align:-0.340em;height:1.051em;width:1.961em"/></span><span class="koboSpan" id="kobo.1363.1"> will best approximate </span><span class="koboSpan" id="kobo.1364.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/156.png" style="vertical-align:-0.257em;height:0.829em;width:1.675em"/></span><span class="koboSpan" id="kobo.1365.1">. </span><span class="koboSpan" id="kobo.1365.2">First, we’ll see how to do that for a </span><br/><span class="koboSpan" id="kobo.1366.1">single-layer network, using</span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.1367.1"> an optimization algorithm called GD. </span><span class="koboSpan" id="kobo.1367.2">Then, we’ll extend it to a deep feedforward network with the help </span><span class="No-Break"><span class="koboSpan" id="kobo.1368.1">of BP.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1369.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1370.1">We should note that an NN and its training algorithm are two separate things. </span><span class="koboSpan" id="kobo.1370.2">This means we can adjust the weights of a network in some way other than GD and BP, but this is the most popular and efficient way to do so and is, ostensibly, the only way that is currently used </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">in practice.</span></span></p>
<h2 id="_idParaDest-42" lang="en-GB"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.1372.1">GD</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1373.1">For the purposes</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.1374.1"> of this </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.1375.1">section, we’ll train a simple NN using the </span><strong class="bold"><span class="koboSpan" id="kobo.1376.1">mean square error</span></strong><span class="koboSpan" id="kobo.1377.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1378.1">MSE</span></strong><span class="koboSpan" id="kobo.1379.1">) cost function. </span><span class="koboSpan" id="kobo.1379.2">It measures the difference (known as </span><strong class="bold"><span class="koboSpan" id="kobo.1380.1">error</span></strong><span class="koboSpan" id="kobo.1381.1">) between the network output</span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.1382.1"> and the training data labels</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.1383.1"> of all </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">training samples:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1385.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/157.png" style="vertical-align:-0.767em;height:2.077em;width:10.876em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1386.1">At first, this might look scary, but fear not! </span><span class="koboSpan" id="kobo.1386.2">Behind the scenes, it’s very simple and straightforward mathematics (I know that sounds even scarier!). </span><span class="koboSpan" id="kobo.1386.3">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.1387.1">its components:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1388.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/158.png" style="vertical-align:-0.340em;height:1.124em;width:2.336em"/></span><span class="koboSpan" id="kobo.1389.1">: The output of the NN, where </span><em class="italic"><span class="koboSpan" id="kobo.1390.1">θ</span></em><span class="koboSpan" id="kobo.1391.1"> is the set of all network weights. </span><span class="koboSpan" id="kobo.1391.2">For the remainder of this section, we’ll denote the individual weights with </span><span class="koboSpan" id="kobo.1392.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/159.png" style="vertical-align:-0.483em;height:1.194em;width:0.619em"/></span><span class="koboSpan" id="kobo.1393.1"> (unlike the </span><em class="italic"><span class="koboSpan" id="kobo.1394.1">w</span></em><span class="koboSpan" id="kobo.1395.1"> notation in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">the sections).</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.1397.1">n</span></em><span class="koboSpan" id="kobo.1398.1">: The total number of samples in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1399.1">training set.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1400.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/132.png" style="vertical-align:-0.000em;height:0.734em;width:1.115em"/></span><span class="koboSpan" id="kobo.1401.1">: The vector representation of a training sample, where the superscript </span><em class="italic"><span class="koboSpan" id="kobo.1402.1">i</span></em><span class="koboSpan" id="kobo.1403.1"> indicates the </span><em class="italic"><span class="koboSpan" id="kobo.1404.1">i</span></em><span class="koboSpan" id="kobo.1405.1">-th sample of the dataset. </span><span class="koboSpan" id="kobo.1405.2">We use superscript because </span><span class="koboSpan" id="kobo.1406.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/132.png" style="vertical-align:-0.000em;height:0.734em;width:1.094em"/></span><span class="koboSpan" id="kobo.1407.1"> is a vector, and the subscript is reserved for each of the vector components. </span><span class="koboSpan" id="kobo.1407.2">For example, </span><span class="koboSpan" id="kobo.1408.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/162.png" style="vertical-align:-0.483em;height:1.217em;width:1.043em"/></span><span class="koboSpan" id="kobo.1409.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.1410.1">j</span></em><span class="koboSpan" id="kobo.1411.1">-th component of the </span><em class="italic"><span class="koboSpan" id="kobo.1412.1">i</span></em><span class="koboSpan" id="kobo.1413.1">-th </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">training sample.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1415.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/163.png" style="vertical-align:-0.012em;height:0.746em;width:0.940em"/></span><span class="koboSpan" id="kobo.1416.1">: The label associated with the training </span><span class="No-Break"><span class="koboSpan" id="kobo.1417.1">sample, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/132.png" style="vertical-align:-0.000em;height:0.734em;width:1.094em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1419.1">.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1420.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1421.1">In this example, we will use MSE, but in practice, there are different types of cost functions. </span><span class="koboSpan" id="kobo.1421.2">We’ll discuss them in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1422.1">Chapter 3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1423.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1424.1">First, GD computes</span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.1425.1"> the derivative (gradient) of </span><em class="italic"><span class="koboSpan" id="kobo.1426.1">J(θ)</span></em><span class="koboSpan" id="kobo.1427.1"> with respect to all the network weights. </span><span class="koboSpan" id="kobo.1427.2">The gradient gives us an indication of how </span><em class="italic"><span class="koboSpan" id="kobo.1428.1">J(θ)</span></em><span class="koboSpan" id="kobo.1429.1"> changes </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.1430.1">with respect to each weight. </span><span class="koboSpan" id="kobo.1430.2">Then, the algorithm uses this information to update the weights in a way that will minimize </span><em class="italic"><span class="koboSpan" id="kobo.1431.1">J(θ)</span></em><span class="koboSpan" id="kobo.1432.1"> in future occurrences of the same input/target pairs. </span><span class="koboSpan" id="kobo.1432.2">The goal is to gradually reach the </span><strong class="bold"><span class="koboSpan" id="kobo.1433.1">global minimum</span></strong><span class="koboSpan" id="kobo.1434.1"> of the cost function, where the gradient is 0. </span><span class="koboSpan" id="kobo.1434.2">The following is a visualization of GD for MSE with respect to a single </span><span class="No-Break"><span class="koboSpan" id="kobo.1435.1">NN weight:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<span class="koboSpan" id="kobo.1436.1"><img alt="Figure 2.20 – An MSE diagram" src="image/B19627_02_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1437.1">Figure 2.20 – An MSE diagram</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1438.1">Let’s go over the execution</span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.1439.1"> of GD step </span><span class="No-Break"><span class="koboSpan" id="kobo.1440.1">by step:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1441.1">Initialize the network weights, </span><em class="italic"><span class="koboSpan" id="kobo.1442.1">θ</span></em><span class="koboSpan" id="kobo.1443.1">, with </span><span class="No-Break"><span class="koboSpan" id="kobo.1444.1">random values.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1445.1">Repeat until the cost function, </span><em class="italic"><span class="koboSpan" id="kobo.1446.1">J(θ)</span></em><span class="koboSpan" id="kobo.1447.1">, falls below a </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">certain threshold:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1449.1">Forward pass</span></strong><span class="koboSpan" id="kobo.1450.1">: Compute the MSE </span><em class="italic"><span class="koboSpan" id="kobo.1451.1">J(θ)</span></em><span class="koboSpan" id="kobo.1452.1"> cost function for all the samples of the training set using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">preceding formula</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1454.1">Backward pass</span></strong><span class="koboSpan" id="kobo.1455.1">: Compute the partial derivatives (gradients) of </span><em class="italic"><span class="koboSpan" id="kobo.1456.1">J(θ)</span></em><span class="koboSpan" id="kobo.1457.1"> with respect to all the network weights </span><span class="koboSpan" id="kobo.1458.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/165.png" style="vertical-align:-0.483em;height:1.194em;width:0.588em"/></span><span class="koboSpan" id="kobo.1459.1">, using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1460.1">chain rule:</span></span></li></ul></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1461.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/166.png" style="vertical-align:-0.913em;height:2.452em;width:30.433em"/></span><br/></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1462.1">Let’s analyze the partial derivative </span><span class="koboSpan" id="kobo.1463.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/167.png" style="vertical-align:-0.483em;height:1.245em;width:3.677em"/></span><span class="koboSpan" id="kobo.1464.1">. </span><em class="italic"><span class="koboSpan" id="kobo.1465.1">J</span></em><span class="koboSpan" id="kobo.1466.1"> is a function of </span><span class="koboSpan" id="kobo.1467.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/168.png" style="vertical-align:-0.483em;height:1.194em;width:0.646em"/></span><span class="koboSpan" id="kobo.1468.1"> by being a function of the network output. </span><span class="koboSpan" id="kobo.1468.2">Therefore, it is also a function of the NN function itself – that is, </span><em class="italic"><span class="koboSpan" id="kobo.1469.1">J(f(θ))</span></em><span class="koboSpan" id="kobo.1470.1">. </span><span class="koboSpan" id="kobo.1470.2">Then, by following the chain rule, we get the </span><span class="No-Break"><span class="koboSpan" id="kobo.1471.1">following: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/169.png" style="vertical-align:-0.493em;height:1.468em;width:8.982em"/></span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1473.1">Use these gradient values to update each of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">network weights:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1475.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/170.png" style="vertical-align:-0.913em;height:2.366em;width:19.586em"/></span><br/></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1476.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1477.1">η</span></em><span class="koboSpan" id="kobo.1478.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.1479.1">learning rate</span></strong><span class="koboSpan" id="kobo.1480.1">, which determines the step size at which the optimizer makes updates to the weights during training. </span><span class="koboSpan" id="kobo.1480.2">Let’s note that as we move closer to the global minimum, the gradient will get smaller and we’ll update the weights in </span><span class="No-Break"><span class="koboSpan" id="kobo.1481.1">finer steps.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1482.1">To better understand how GD</span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.1483.1"> works, we’ll use linear regression</span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.1484.1"> as an example. </span><span class="koboSpan" id="kobo.1484.2">Let’s recall that linear regression is equivalent to a single NN unit with an identity activation function, </span><em class="italic"><span class="koboSpan" id="kobo.1485.1">f(x) = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1486.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1488.1">The linear regression is represented by the function </span><span class="koboSpan" id="kobo.1489.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/171.png" style="vertical-align:-0.486em;height:1.248em;width:6.195em"/></span><span class="koboSpan" id="kobo.1490.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1491.1">m</span></em><span class="koboSpan" id="kobo.1492.1"> is the dimension of the input vector (equal to the number </span><span class="No-Break"><span class="koboSpan" id="kobo.1493.1">of weights)</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1494.1">Then, we have the MSE cost function – </span><span class="koboSpan" id="kobo.1495.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/172.png" style="vertical-align:-0.588em;height:1.448em;width:13.384em"/></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1496.1">Then, we compute the partial derivative </span><span class="koboSpan" id="kobo.1497.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/173.png" style="vertical-align:-0.483em;height:1.243em;width:3.597em"/></span><span class="koboSpan" id="kobo.1498.1"> with respect to a single network weight </span><span class="koboSpan" id="kobo.1499.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/174.png" style="vertical-align:-0.483em;height:1.194em;width:0.606em"/></span><span class="koboSpan" id="kobo.1500.1"> using the chain rule and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1501.1">sum rule:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1502.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msubsup&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/175.png" style="vertical-align:-0.913em;height:2.569em;width:12.691em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1503.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/munderover&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msubsup&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/176.png" style="vertical-align:-0.840em;height:2.569em;width:10.306em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1504.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/177.png" style="vertical-align:-0.941em;height:2.424em;width:11.983em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1505.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/178.png" style="vertical-align:-0.767em;height:2.077em;width:9.189em"/></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1506.1">Now that we have the gradient, </span><span class="koboSpan" id="kobo.1507.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/179.png" style="vertical-align:-0.483em;height:1.243em;width:3.483em"/></span><span class="koboSpan" id="kobo.1508.1">, we can update the weight </span><span class="koboSpan" id="kobo.1509.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/180.png" style="vertical-align:-0.483em;height:1.194em;width:0.582em"/></span><span class="koboSpan" id="kobo.1510.1"> using the learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1511.1">rate </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1512.1">η</span></em></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1513.1">So far, we’ve discussed a GD</span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.1514.1"> that works with NNs with multiple</span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.1515.1"> weights. </span><span class="koboSpan" id="kobo.1515.2">However, for the sake of simplicity, the preceding diagram illustrates the relationship between the cost function and a single-weight NN. </span><span class="koboSpan" id="kobo.1515.3">Let’s remedy this by introducing a more complex cost function for an NN with two weights, </span><span class="koboSpan" id="kobo.1516.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/181.png" style="vertical-align:-0.333em;height:1.044em;width:0.711em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.1517.1">and </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1518.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/182.png" style="vertical-align:-0.333em;height:1.044em;width:0.711em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1519.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer226">
<span class="koboSpan" id="kobo.1520.1"><img alt="Figure 2.21 – The cost function J with respect to two weights" src="image/B19627_02_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1521.1">Figure 2.21 – The cost function J with respect to two weights</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1522.1">The function has a local and a global minimum. </span><span class="koboSpan" id="kobo.1522.2">Nothing prevents GD from converging to the local minimum, instead of the global one, thus finding a sub-optimal approximation to the target function. </span><span class="koboSpan" id="kobo.1522.3">We can try to mitigate this by increasing the learning rate </span><em class="italic"><span class="koboSpan" id="kobo.1523.1">η</span></em><span class="koboSpan" id="kobo.1524.1">. </span><span class="koboSpan" id="kobo.1524.2">The idea is that even if GD converges toward the local minimum, the larger </span><em class="italic"><span class="koboSpan" id="kobo.1525.1">η</span></em><span class="koboSpan" id="kobo.1526.1"> will help us </span><em class="italic"><span class="koboSpan" id="kobo.1527.1">jump</span></em><span class="koboSpan" id="kobo.1528.1"> over the saddle and converge toward the global maximum. </span><span class="koboSpan" id="kobo.1528.2">The risk is that the opposite could happen – if GD correctly converges toward the global maximum, the larger learning rate could make it </span><em class="italic"><span class="koboSpan" id="kobo.1529.1">jump</span></em><span class="koboSpan" id="kobo.1530.1"> over to the local </span><span class="No-Break"><span class="koboSpan" id="kobo.1531.1">minimum instead.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1532.1">A more elegant way to prevent this issue is to use </span><strong class="bold"><span class="koboSpan" id="kobo.1533.1">momentum</span></strong><span class="koboSpan" id="kobo.1534.1">. </span><span class="koboSpan" id="kobo.1534.2">This extends vanilla GD by adjusting the current weight</span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.1535.1"> update with the values of the previous weight updates – that is, if the weight update at step t-1 was big, it will also increase the weight update of step t. </span><span class="koboSpan" id="kobo.1535.2">We can explain momentum with an analogy. </span><span class="koboSpan" id="kobo.1535.3">Think of the loss function surface as the surface of a hill. </span><span class="koboSpan" id="kobo.1535.4">Now, imagine that we are holding a ball at the top of the hill (maximum). </span><span class="koboSpan" id="kobo.1535.5">If we drop the ball, thanks to the Earth’s gravity, it will start rolling toward the bottom of the hill (minimum). </span><span class="koboSpan" id="kobo.1535.6">The more distance it travels, the more its speed will increase. </span><span class="koboSpan" id="kobo.1535.7">In other words, it will gain momentum (hence the name of </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">the optimization).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1537.1">Now, let’s look at how to implement</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.1538.1"> momentum in the weight</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.1539.1"> update rule </span><span class="koboSpan" id="kobo.1540.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/183.png" style="vertical-align:-0.483em;height:1.243em;width:7.879em"/></span><span class="koboSpan" id="kobo.1541.1">. </span><span class="koboSpan" id="kobo.1541.2">We’ll assume that we are at step t of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">training process:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1543.1">First, we’ll calculate the current weight update value </span><span class="koboSpan" id="kobo.1544.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/184.png" style="vertical-align:-0.340em;height:0.788em;width:0.653em"/></span><span class="koboSpan" id="kobo.1545.1"> by also including the velocity of the previous update </span><span class="koboSpan" id="kobo.1546.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/185.png" style="vertical-align:-0.340em;height:0.788em;width:1.263em"/></span><span class="koboSpan" id="kobo.1547.1">: </span><span class="koboSpan" id="kobo.1548.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/186.png" style="vertical-align:-0.483em;height:1.243em;width:9.428em"/></span><span class="koboSpan" id="kobo.1549.1">. </span><span class="koboSpan" id="kobo.1549.2">Here, μ is a hyperparameter</span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.1550.1"> in the [0:1] range called the momentum rate. </span><span class="koboSpan" id="kobo.1551.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/187.png" style="vertical-align:-0.340em;height:0.788em;width:0.614em"/></span><span class="koboSpan" id="kobo.1552.1"> is initialized as 0 during the </span><span class="No-Break"><span class="koboSpan" id="kobo.1553.1">first iteration.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1554.1">Then, we perform the actual weight update – </span><span class="koboSpan" id="kobo.1555.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/188.png" style="vertical-align:-0.483em;height:1.194em;width:4.482em"/></span><span class="koboSpan" id="kobo.1556.1">.</span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1557.1">Finding the best values of the learning rate </span><em class="italic"><span class="koboSpan" id="kobo.1558.1">η</span></em><span class="koboSpan" id="kobo.1559.1"> and the momentum rate </span><em class="italic"><span class="koboSpan" id="kobo.1560.1">μ</span></em><span class="koboSpan" id="kobo.1561.1"> is an empirical task. </span><span class="koboSpan" id="kobo.1561.2">They can depend on the NN architecture, the type and size of a dataset, and other factors. </span><span class="koboSpan" id="kobo.1561.3">In addition, we might have to adjust them during training. </span><span class="koboSpan" id="kobo.1561.4">Since the NN weights are randomly initialized, we usually start with a larger </span><em class="italic"><span class="koboSpan" id="kobo.1562.1">η</span></em><span class="koboSpan" id="kobo.1563.1"> so that GD can quickly advance while the initial value of the cost function (error) is large. </span><span class="koboSpan" id="kobo.1563.2">Once the decrease in the cost function starts plateauing, we can decrease the learning rate. </span><span class="koboSpan" id="kobo.1563.3">In this way, GD can find minimums that would have been </span><em class="italic"><span class="koboSpan" id="kobo.1564.1">jumped over</span></em><span class="koboSpan" id="kobo.1565.1"> with the larger </span><span class="No-Break"><span class="koboSpan" id="kobo.1566.1">learning rate.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1567.1">Alternatively, we can use an adaptive learning rate algorithm such as </span><strong class="bold"><span class="koboSpan" id="kobo.1568.1">Adam</span></strong><span class="koboSpan" id="kobo.1569.1"> (see </span><em class="italic"><span class="koboSpan" id="kobo.1570.1">Adam: A Method for Stochastic Optimization</span></em><span class="koboSpan" id="kobo.1571.1"> at </span><a href="https://arxiv.org/abs/1412.6980"><span class="koboSpan" id="kobo.1572.1">https://arxiv.org/abs/1412.6980</span></a><span class="koboSpan" id="kobo.1573.1">). </span><span class="koboSpan" id="kobo.1573.2">It calculates individual and adaptive learning rates for every weight, based on previous weight </span><span class="No-Break"><span class="koboSpan" id="kobo.1574.1">updates (momentum).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1575.1">The GD we just described is called </span><strong class="bold"><span class="koboSpan" id="kobo.1576.1">batch gradient descent</span></strong><span class="koboSpan" id="kobo.1577.1"> because it accumulates the error across </span><em class="italic"><span class="koboSpan" id="kobo.1578.1">all</span></em><span class="koboSpan" id="kobo.1579.1"> training samples</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.1580.1"> and then performs a single weight update. </span><span class="koboSpan" id="kobo.1580.2">This is fine for small datasets but could become impractical for large ones, as the training would take a long time with such sporadic updates. </span><span class="koboSpan" id="kobo.1580.3">In practice, we would use </span><span class="No-Break"><span class="koboSpan" id="kobo.1581.1">two modifications:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1582.1">Stochastic (or online) gradient descent</span></strong><span class="koboSpan" id="kobo.1583.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1584.1">SGD</span></strong><span class="koboSpan" id="kobo.1585.1">): Updates the weights after every </span><span class="No-Break"><span class="koboSpan" id="kobo.1586.1">training</span></span><span class="No-Break"><a id="_idIndexMarker296"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1587.1"> sample.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1588.1">Mini-batch gradient descent</span></strong><span class="koboSpan" id="kobo.1589.1">: Accumulates the error over batches of </span><em class="italic"><span class="koboSpan" id="kobo.1590.1">k</span></em><span class="koboSpan" id="kobo.1591.1"> samples (called </span><strong class="bold"><span class="koboSpan" id="kobo.1592.1">mini-batches</span></strong><span class="koboSpan" id="kobo.1593.1">) and performs one weight update after each mini-batch. </span><span class="koboSpan" id="kobo.1593.2">It is a hybrid</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.1594.1"> between online and batch GD. </span><span class="koboSpan" id="kobo.1594.2">In practice, we’ll almost</span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.1595.1"> always use mini-batch GD over the </span><span class="No-Break"><span class="koboSpan" id="kobo.1596.1">other modifications.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1597.1">The next step in our learning journey</span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.1598.1"> is to understand how to apply</span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.1599.1"> GD to train networks with more than </span><span class="No-Break"><span class="koboSpan" id="kobo.1600.1">one layer.</span></span></p>
<h2 id="_idParaDest-43" lang="en-GB"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.1601.1">Backpropagation</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1602.1">In this section, we’ll discuss how to combine GD</span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.1603.1"> with the BP algorithm to update</span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.1604.1"> the weights of networks with more than one layer. </span><span class="koboSpan" id="kobo.1604.2">As we demonstrated in the </span><em class="italic"><span class="koboSpan" id="kobo.1605.1">GD</span></em><span class="koboSpan" id="kobo.1606.1"> section, this means finding the derivative of the cost function </span><em class="italic"><span class="koboSpan" id="kobo.1607.1">J(θ)</span></em><span class="koboSpan" id="kobo.1608.1"> with respect to each network weight. </span><span class="koboSpan" id="kobo.1608.2">We already took a step in this direction with the help of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1609.1">chain rule:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1610.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/189.png" style="vertical-align:-0.913em;height:2.288em;width:12.033em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1611.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1612.1">f(θ)</span></em><span class="koboSpan" id="kobo.1613.1"> is the network output and </span><span class="koboSpan" id="kobo.1614.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/190.png" style="vertical-align:-0.483em;height:1.194em;width:0.555em"/></span><span class="koboSpan" id="kobo.1615.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.1616.1">j</span></em><span class="koboSpan" id="kobo.1617.1">-th network weight. </span><span class="koboSpan" id="kobo.1617.2">In this section, we’ll push the envelope further, and we’ll learn how to derive the NN function itself for all the network weights (hint – the chain rule). </span><span class="koboSpan" id="kobo.1617.3">We’ll do this by propagating the error gradient backward through the network (hence the name). </span><span class="koboSpan" id="kobo.1617.4">Let’s start with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1618.1">few assumptions:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1619.1">For the sake of simplicity, we’ll work with a sequential feedforward NN. </span><em class="italic"><span class="koboSpan" id="kobo.1620.1">Sequential</span></em><span class="koboSpan" id="kobo.1621.1"> means that each layer takes input from the preceding layer and sends its output to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1622.1">following layer.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1623.1">We’ll define </span><span class="koboSpan" id="kobo.1624.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/191.png" style="vertical-align:-0.483em;height:0.931em;width:0.955em"/></span><span class="koboSpan" id="kobo.1625.1"> as the weight between the </span><em class="italic"><span class="koboSpan" id="kobo.1626.1">i</span></em><span class="koboSpan" id="kobo.1627.1">-th unit of layer </span><em class="italic"><span class="koboSpan" id="kobo.1628.1">l</span></em><span class="koboSpan" id="kobo.1629.1"> and the </span><em class="italic"><span class="koboSpan" id="kobo.1630.1">j</span></em><span class="koboSpan" id="kobo.1631.1">-th unit of the subsequent layer </span><em class="italic"><span class="koboSpan" id="kobo.1632.1">l+1</span></em><span class="koboSpan" id="kobo.1633.1">. </span><span class="koboSpan" id="kobo.1633.2">In a multi-layer network, </span><em class="italic"><span class="koboSpan" id="kobo.1634.1">l</span></em><span class="koboSpan" id="kobo.1635.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1636.1">l+1</span></em><span class="koboSpan" id="kobo.1637.1"> can be any two consecutive layers, including input, hidden, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1638.1">output layers.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1639.1">We’ll denote the output of the </span><em class="italic"><span class="koboSpan" id="kobo.1640.1">i</span></em><span class="koboSpan" id="kobo.1641.1">-th unit of layer </span><em class="italic"><span class="koboSpan" id="kobo.1642.1">l</span></em><span class="koboSpan" id="kobo.1643.1"> with </span><span class="koboSpan" id="kobo.1644.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/192.png" style="vertical-align:-0.340em;height:1.116em;width:1.060em"/></span><span class="koboSpan" id="kobo.1645.1"> and the output of the </span><em class="italic"><span class="koboSpan" id="kobo.1646.1">j</span></em><span class="koboSpan" id="kobo.1647.1">-th unit of layer </span><em class="italic"><span class="koboSpan" id="kobo.1648.1">l+1</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1649.1">with </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1650.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/193.png" style="vertical-align:-0.483em;height:1.259em;width:1.704em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1651.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1652.1">We’ll denote the input to the activation function (that is, the weighted sum of the inputs before activation) of unit </span><em class="italic"><span class="koboSpan" id="kobo.1653.1">j</span></em><span class="koboSpan" id="kobo.1654.1"> of layer </span><em class="italic"><span class="koboSpan" id="kobo.1655.1">l</span></em><span class="koboSpan" id="kobo.1656.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1657.1">with </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1658.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/194.png" style="vertical-align:-0.483em;height:1.259em;width:1.134em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1659.1">.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1660.1">The following diagram shows all the notations </span><span class="No-Break"><span class="koboSpan" id="kobo.1661.1">we introduced:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer239">
<span class="koboSpan" id="kobo.1662.1"><img alt="Figure 2.22 – Layer l represents the input, layer l+1 represents the output, and w connects the y activation in layer l to the inputs of the j-th unit of layer l+1" src="image/B19627_02_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1663.1">Figure 2.22 – Layer l represents the input, layer l+1 represents the output, and w connects the y activation in layer l to the inputs of the j-th unit of layer l+1</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1664.1">Armed with this useful</span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.1665.1"> knowledge, let’s get down</span><a id="_idIndexMarker304"/> <span class="No-Break"><span class="koboSpan" id="kobo.1666.1">to business:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1667.1">First, we’ll assume that </span><em class="italic"><span class="koboSpan" id="kobo.1668.1">l</span></em><span class="koboSpan" id="kobo.1669.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1670.1">l+1</span></em><span class="koboSpan" id="kobo.1671.1"> are the second-to-last and the last (output) network layers, respectively. </span><span class="koboSpan" id="kobo.1671.2">Knowing this, the derivative of </span><em class="italic"><span class="koboSpan" id="kobo.1672.1">J</span></em><span class="koboSpan" id="kobo.1673.1"> with respect to </span><span class="koboSpan" id="kobo.1674.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/195.png" style="vertical-align:-0.483em;height:0.931em;width:0.969em"/></span><span class="koboSpan" id="kobo.1675.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.1676.1">as follows:</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1677.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/196.png" style="vertical-align:-0.968em;height:2.464em;width:9.210em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1678.1">2.	</span><span class="koboSpan" id="kobo.1678.2">Let’s focus on </span><span class="koboSpan" id="kobo.1679.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/197.png" style="vertical-align:-0.483em;height:1.259em;width:4.283em"/></span><span class="koboSpan" id="kobo.1680.1">. </span><span class="koboSpan" id="kobo.1680.2">Here, we compute the partial derivative of the weighted sum of the output of layer </span><em class="italic"><span class="koboSpan" id="kobo.1681.1">l</span></em><span class="koboSpan" id="kobo.1682.1"> with respect to one of the weights, </span><span class="koboSpan" id="kobo.1683.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/198.png" style="vertical-align:-0.483em;height:0.931em;width:1.039em"/></span><span class="koboSpan" id="kobo.1684.1">. </span><span class="koboSpan" id="kobo.1684.2">As we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1685.1">Differential calculus</span></em><span class="koboSpan" id="kobo.1686.1"> section, in partial derivatives, we’ll consider all the function parameters except </span><span class="koboSpan" id="kobo.1687.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/128.png" style="vertical-align:-0.483em;height:0.931em;width:1.009em"/></span><span class="koboSpan" id="kobo.1688.1"> constants. </span><span class="koboSpan" id="kobo.1688.2">When we derive </span><span class="koboSpan" id="kobo.1689.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/200.png" style="vertical-align:-0.012em;height:0.788em;width:1.695em"/></span><span class="koboSpan" id="kobo.1690.1">, they all become 0, and we’re only left with </span><span class="koboSpan" id="kobo.1691.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/201.png" style="vertical-align:-0.533em;height:1.360em;width:7.788em"/></span><span class="koboSpan" id="kobo.1692.1">. </span><br/><span class="koboSpan" id="kobo.1693.1">Therefore, we get </span><span class="No-Break"><span class="koboSpan" id="kobo.1694.1">the following:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1695.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/202.png" style="vertical-align:-0.911em;height:2.406em;width:4.651em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1696.1">3.	</span><span class="koboSpan" id="kobo.1696.2">The formula from point 1 holds for any two consecutive hidden layers, </span><em class="italic"><span class="koboSpan" id="kobo.1697.1">l</span></em><span class="koboSpan" id="kobo.1698.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1699.1">l+1</span></em><span class="koboSpan" id="kobo.1700.1">, of the network. </span><span class="koboSpan" id="kobo.1700.2">We know that </span><span class="koboSpan" id="kobo.1701.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/203.png" style="vertical-align:-0.533em;height:1.360em;width:8.034em"/></span><span class="koboSpan" id="kobo.1702.1">, and we also know that </span><span class="koboSpan" id="kobo.1703.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/204.png" style="vertical-align:-0.483em;height:1.259em;width:4.943em"/></span><span class="koboSpan" id="kobo.1704.1"> is the derivative of the activation function, which we can calculate (see the </span><em class="italic"><span class="koboSpan" id="kobo.1705.1">Activation functions</span></em><span class="koboSpan" id="kobo.1706.1"> section). </span><span class="koboSpan" id="kobo.1706.2">All we need to do is calculate the derivative </span><span class="koboSpan" id="kobo.1707.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/205.png" style="vertical-align:-0.483em;height:1.259em;width:3.408em"/></span><span class="koboSpan" id="kobo.1708.1"> (recall that, here, </span><em class="italic"><span class="koboSpan" id="kobo.1709.1">l+1</span></em><span class="koboSpan" id="kobo.1710.1"> is a hidden layer). </span><span class="koboSpan" id="kobo.1710.2">Let’s note that this is the derivative of the error with respect to the activation function in layer </span><em class="italic"><span class="koboSpan" id="kobo.1711.1">l+1</span></em><span class="koboSpan" id="kobo.1712.1">. </span><span class="koboSpan" id="kobo.1712.2">We can now calculate all the derivatives, starting from the last layer and moving backward, because the </span><span class="No-Break"><span class="koboSpan" id="kobo.1713.1">following apply:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1714.1">We can calculate this derivative for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1715.1">last layer</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1716.1">We have a formula that allows us to calculate the derivative for one layer, assuming that we can calculate the derivative for </span><span class="No-Break"><span class="koboSpan" id="kobo.1717.1">the next</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1718.1">4.	</span><span class="koboSpan" id="kobo.1718.2">With these points</span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.1719.1"> in mind, we get the following</span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.1720.1"> equation by applying the </span><span class="No-Break"><span class="koboSpan" id="kobo.1721.1">chain rule:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1722.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/206.png" style="vertical-align:-0.914em;height:2.483em;width:17.332em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1723.1">The sum over j reflects the fact that, in the feedforward part of the network, the output </span><span class="koboSpan" id="kobo.1724.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/207.png" style="vertical-align:-0.340em;height:1.116em;width:1.100em"/></span><span class="koboSpan" id="kobo.1725.1"> is fed to all the units in layer </span><em class="italic"><span class="koboSpan" id="kobo.1726.1">l+1</span></em><span class="koboSpan" id="kobo.1727.1">. </span><span class="koboSpan" id="kobo.1727.2">Therefore, they all contribute to </span><span class="koboSpan" id="kobo.1728.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/208.png" style="vertical-align:-0.340em;height:1.116em;width:0.964em"/></span><span class="koboSpan" id="kobo.1729.1"> when the error is </span><span class="No-Break"><span class="koboSpan" id="kobo.1730.1">propagated backward.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1731.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1732.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.1733.1">Layers as operations</span></em><span class="koboSpan" id="kobo.1734.1"> section, we discussed that in the forward pass, we can simultaneously compute all outputs of layer </span><em class="italic"><span class="koboSpan" id="kobo.1735.1">l+1</span></em><span class="koboSpan" id="kobo.1736.1"> as a matrix-matrix multiplication </span><span class="koboSpan" id="kobo.1737.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/209.png" style="vertical-align:-0.011em;height:0.787em;width:3.866em"/></span><span class="koboSpan" id="kobo.1738.1">. </span><span class="koboSpan" id="kobo.1738.2">Here, </span><span class="koboSpan" id="kobo.1739.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/210.png" style="vertical-align:-0.000em;height:0.776em;width:1.259em"/></span><span class="koboSpan" id="kobo.1740.1"> is the layer output of layer </span><em class="italic"><span class="koboSpan" id="kobo.1741.1">l</span></em><span class="koboSpan" id="kobo.1742.1">, and </span><span class="koboSpan" id="kobo.1743.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/211.png" style="vertical-align:-0.011em;height:0.787em;width:2.459em"/></span><span class="koboSpan" id="kobo.1744.1"> is the weight matrix between layers </span><em class="italic"><span class="koboSpan" id="kobo.1745.1">l</span></em><span class="koboSpan" id="kobo.1746.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1747.1">l+1</span></em><span class="koboSpan" id="kobo.1748.1">. </span><span class="koboSpan" id="kobo.1748.2">In the forward pass, one column of </span><span class="koboSpan" id="kobo.1749.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/212.png" style="vertical-align:-0.011em;height:0.787em;width:2.473em"/></span><span class="koboSpan" id="kobo.1750.1"> represents the weights from all units of the input layer </span><em class="italic"><span class="koboSpan" id="kobo.1751.1">l</span></em><span class="koboSpan" id="kobo.1752.1"> to a single unit of the output layer </span><em class="italic"><span class="koboSpan" id="kobo.1753.1">l+1</span></em><span class="koboSpan" id="kobo.1754.1">. </span><span class="koboSpan" id="kobo.1754.2">We can also represent the backward pass as a matrix-matrix multiplication by using the transpose weight matrix </span><span class="koboSpan" id="kobo.1755.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/213.png" style="vertical-align:-0.061em;height:0.888em;width:8.002em"/></span><span class="koboSpan" id="kobo.1756.1">. </span><span class="koboSpan" id="kobo.1756.2">A column of the transposed matrix represents the weights of all units of </span><em class="italic"><span class="koboSpan" id="kobo.1757.1">l</span></em><span class="koboSpan" id="kobo.1758.1">, whi</span><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.1759.1">ch contributed to a particular unit of </span><em class="italic"><span class="koboSpan" id="kobo.1760.1">l+1</span></em><span class="koboSpan" id="kobo.1761.1"> during the </span><span class="No-Break"><span class="koboSpan" id="kobo.1762.1">forward phase.</span></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1763.1">Once again, we can calculate </span><span class="koboSpan" id="kobo.1764.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/214.png" style="vertical-align:-0.483em;height:1.259em;width:4.999em"/></span><span class="koboSpan" id="kobo.1765.1">. </span><span class="koboSpan" id="kobo.1765.2">Using the same logic that we followed in </span><em class="italic"><span class="koboSpan" id="kobo.1766.1">step 3</span></em><span class="koboSpan" id="kobo.1767.1">, we can compute that </span><span class="koboSpan" id="kobo.1768.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/215.png" style="vertical-align:-0.483em;height:1.260em;width:6.902em"/></span><span class="koboSpan" id="kobo.1769.1">. </span><span class="koboSpan" id="kobo.1769.2">Therefore, once we know </span><span class="koboSpan" id="kobo.1770.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/216.png" style="vertical-align:-0.483em;height:1.259em;width:3.461em"/></span><span class="koboSpan" id="kobo.1771.1">, we can calculate </span><span class="koboSpan" id="kobo.1772.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/217.png" style="vertical-align:-0.144em;height:0.853em;width:1.715em"/></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1773.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/218.png" style="vertical-align:-0.340em;height:1.117em;width:36.655em"/></span></span><span class="koboSpan" id="kobo.1774.1">. </span><br/><span class="koboSpan" id="kobo.1775.1">Since we can calculate </span><span class="koboSpan" id="kobo.1776.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/219.png" style="vertical-align:-0.483em;height:1.259em;width:3.442em"/></span><span class="koboSpan" id="kobo.1777.1"> for the last layer, we can move backward and calculate </span><span class="koboSpan" id="kobo.1778.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/220.png" style="vertical-align:-0.340em;height:1.117em;width:2.818em"/></span><span class="koboSpan" id="kobo.1779.1"> for any layer and, therefore, </span><span class="koboSpan" id="kobo.1780.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/221.png" style="vertical-align:-0.483em;height:1.192em;width:2.726em"/></span><span class="koboSpan" id="kobo.1781.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.1782.1">any layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1783.1">5.	</span><span class="koboSpan" id="kobo.1783.2">To summarize, let’s say</span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.1784.1"> we have a sequence</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.1785.1"> of layers where the </span><span class="No-Break"><span class="koboSpan" id="kobo.1786.1">following applies:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1787.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;→&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/222.png" style="vertical-align:-0.483em;height:1.017em;width:5.122em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1788.1">Here, we have the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1789.1">fundamental equations:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1790.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/196.png" style="vertical-align:-0.968em;height:2.464em;width:9.210em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1791.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/206.png" style="vertical-align:-0.914em;height:2.483em;width:17.332em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1792.1">By using these two equations, we can calculate the derivatives for the cost with respect to </span><span class="No-Break"><span class="koboSpan" id="kobo.1793.1">each layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1794.1">6.	</span><span class="koboSpan" id="kobo.1794.2">If we set </span><span class="koboSpan" id="kobo.1795.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/225.png" style="vertical-align:-0.531em;height:1.588em;width:5.796em"/></span><span class="koboSpan" id="kobo.1796.1">, then </span><span class="koboSpan" id="kobo.1797.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/226.png" style="vertical-align:-0.012em;height:0.788em;width:1.652em"/></span><span class="koboSpan" id="kobo.1798.1"> represents the variation in cost with respect to the activation value, and we can think of </span><span class="koboSpan" id="kobo.1799.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/227.png" style="vertical-align:-0.012em;height:0.788em;width:1.594em"/></span><span class="koboSpan" id="kobo.1800.1"> as the error at unit </span><span class="koboSpan" id="kobo.1801.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/228.png" style="vertical-align:-0.257em;height:1.033em;width:1.604em"/></span><span class="koboSpan" id="kobo.1802.1">. </span><span class="koboSpan" id="kobo.1802.2">We can rewrite these equations </span><span class="No-Break"><span class="koboSpan" id="kobo.1803.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1804.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;msubsup&gt;&lt;mi&gt;δ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/229.png" style="vertical-align:-0.914em;height:2.483em;width:22.848em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1805.1">Following this, we can write the </span><span class="No-Break"><span class="koboSpan" id="kobo.1806.1">following equation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1807.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;δ&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;msubsup&gt;&lt;mi&gt;δ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/230.png" style="vertical-align:-0.964em;height:2.338em;width:9.418em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1808.1">These two equations</span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.1809.1"> provide us with an alternative view of BP, since</span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.1810.1"> there is a variation in cost with respect to the activation value. </span><span class="koboSpan" id="kobo.1810.2">They provide us with a way to calculate the variation for any layer </span><em class="italic"><span class="koboSpan" id="kobo.1811.1">l</span></em><span class="koboSpan" id="kobo.1812.1"> once we know the variation for the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1813.1">layer, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1814.1">l+1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1815.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1816.1">7.	</span><span class="koboSpan" id="kobo.1816.2">We can combine these equations to show </span><span class="No-Break"><span class="koboSpan" id="kobo.1817.1">the following:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1818.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/231.png" style="vertical-align:-0.911em;height:2.406em;width:11.193em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1819.1">8.	</span><span class="koboSpan" id="kobo.1819.2">The updated rule for the weights of each layer is given by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1820.1">following equation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1821.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/232.png" style="vertical-align:-0.483em;height:1.259em;width:8.169em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1822.1">Now that we’re familiar</span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.1823.1"> with GD and BP, let’s implement</span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.1824.1"> them </span><span class="No-Break"><span class="koboSpan" id="kobo.1825.1">in Python.</span></span></p>
<h2 id="_idParaDest-44" lang="en-GB"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.1826.1">A code example of an NN for the XOR function</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1827.1">In this section, we’ll create a simple network</span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.1828.1"> with one hidden layer, which</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.1829.1"> solves the XOR function. </span><span class="koboSpan" id="kobo.1829.2">Let’s recall that XOR is a linearly inseparable problem, hence the need for a hidden layer. </span><span class="koboSpan" id="kobo.1829.3">The source code will allow you to easily modify the number of layers and the number of units per layer, so you can try a number of different scenarios. </span><span class="koboSpan" id="kobo.1829.4">We won’t use any ML libraries. </span><span class="koboSpan" id="kobo.1829.5">Instead, we’ll implement them from scratch, with only the help of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1830.1">numpy</span></strong><span class="koboSpan" id="kobo.1831.1">. </span><span class="koboSpan" id="kobo.1831.2">We’ll also use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1832.1">matplotlib</span></strong><span class="koboSpan" id="kobo.1833.1"> to visualize </span><span class="No-Break"><span class="koboSpan" id="kobo.1834.1">the results:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1835.1">Let’s start by importing </span><span class="No-Break"><span class="koboSpan" id="kobo.1836.1">these libraries:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1837.1">
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import ListedColormap</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1838.1">Then, we will define the activation function and its derivative. </span><span class="koboSpan" id="kobo.1838.2">In this example, we will </span><span class="No-Break"><span class="koboSpan" id="kobo.1839.1">use </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1840.1">tanh(x)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1841.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1842.1">
def tanh(x):
    return (1.0 - np.exp(-2 * x)) / (1.0 + np.exp(-2 * x))
def tanh_derivative(x):
    return (1 + tanh(x)) * (1 - tanh(x))</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1843.1">Then, we will start</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.1844.1"> the definition of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1845.1">NeuralNetwork</span></strong><span class="koboSpan" id="kobo.1846.1"> class and its constructor (note that all its methods and properties</span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.1847.1"> have to be </span><span class="No-Break"><span class="koboSpan" id="kobo.1848.1">properly indented):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1849.1">
class NeuralNetwork:
    # net_arch consists of a list of integers, indicating
    # the number of units in each layer
    def __init__(self, net_arch):
        self.activation_func = tanh
        self.activation_derivative = tanh_derivative
        self.layers = len(net_arch)
        self.steps_per_epoch = 1000
        self.net_arch = net_arch
        # initialize the weights with random values in the range (-1,1)
        self.weights = []
        for layer in range(len(net_arch) - 1):
            w = 2 * np.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1
            self.weights.append(w)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1850.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1851.1">net_arch</span></strong><span class="koboSpan" id="kobo.1852.1"> is a one-dimensional array containing the number of units for each layer. </span><span class="koboSpan" id="kobo.1852.2">For example </span><strong class="source-inline"><span class="koboSpan" id="kobo.1853.1">[2, 4, 1]</span></strong><span class="koboSpan" id="kobo.1854.1"> means an input layer</span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.1855.1"> with two units, a hidden layer</span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.1856.1"> with four units, and an output layer with one unit. </span><span class="koboSpan" id="kobo.1856.2">Since we are studying the XOR function, the input layer will have two units, and the output layer will only have one unit. </span><span class="koboSpan" id="kobo.1856.3">However, the number of hidden units </span><span class="No-Break"><span class="koboSpan" id="kobo.1857.1">can vary.</span></span></p><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1858.1">To conclude the constructor, we will initialize the network weights with random values in the range (-</span><span class="No-Break"><span class="koboSpan" id="kobo.1859.1">1, 1).</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1860.1">Now, we need to define the fit function, which will train </span><span class="No-Break"><span class="koboSpan" id="kobo.1861.1">our network:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1862.1">
def fit(self, data, labels, learning_rate=0.1, epochs=10):</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1863.1">We will start the implementation by concatenating </span><strong class="source-inline"><span class="koboSpan" id="kobo.1864.1">bias</span></strong><span class="koboSpan" id="kobo.1865.1"> to the training </span><strong class="source-inline"><span class="koboSpan" id="kobo.1866.1">data</span></strong><span class="koboSpan" id="kobo.1867.1"> in a new variable, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1868.1">input_data</span></strong><span class="koboSpan" id="kobo.1869.1"> (the source code is indented within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1870.1">method definition):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1871.1">
bias = np.ones((1, data.shape[0]))
input_data = np.concatenate((bias.T, data), axis=1)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1872.1">Then, we’ll run the training for a number </span><span class="No-Break"><span class="koboSpan" id="kobo.1873.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1874.1">epochs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1875.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1876.1">
for k in range(epochs * self.steps_per_epoch):</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1877.1">Within the loop, we’ll visualize the epoch number and the prediction output of the NN at the start of </span><span class="No-Break"><span class="koboSpan" id="kobo.1878.1">each epoch:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1879.1">
print('epochs: {}'.format(k / self.steps_per_epoch))
for s in data:
    print(s, nn.predict(s))</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1880.1">Within the loop, we select a random</span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.1881.1"> sample from the training</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.1882.1"> set and propagate it forward through the hidden </span><span class="No-Break"><span class="koboSpan" id="kobo.1883.1">network layers:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1884.1">
sample = np.random.randint(data.shape[0])
y = [input_data[sample]]
for i in range(len(self.weights) - 1):
    activation = np.dot(y[i], self.weights[i])
    activation_f = self.activation_func(activation)
    # add the bias for the next layer
    activation_f = np.concatenate((np.ones(1),
        np.array(activation_f)))
    y.append(activation_f)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1885.1">Outside the loop, we will calculate</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.1886.1"> the last layer </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.1887.1">output and </span><span class="No-Break"><span class="koboSpan" id="kobo.1888.1">the error:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1889.1">
# last layer
activation = np.dot(y[-1], self.weights[-1])
activation_f = self.activation_func(activation)
y.append(activation_f)
# error for the output layer
error = y[-1] - labels[sample]
delta_vec = [error * self.activation_derivative(y[-1])]</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1890.1">Then, we will propagate the error back (</span><span class="No-Break"><span class="koboSpan" id="kobo.1891.1">backward pass):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1892.1">
# we need to begin from the back from the next to last layer
for i in range(self.layers - 2, 0, -1):
    error = delta_vec[-1].dot(self.weights[i][1:].T)
    error = error * self.activation_derivative(y[i][1:])
    delta_vec.append(error)
# reverse
# [level3(output)-&gt;level2(hidden)] =&gt; [level2(hidden)-&gt;level3(output)]
delta_vec.reverse()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1893.1">Finally, we will update</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.1894.1"> the weights based on the errors</span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.1895.1"> we just computed. </span><span class="koboSpan" id="kobo.1895.2">We will multiply its output delta and input activation to get the gradient of the weight. </span><span class="koboSpan" id="kobo.1895.3">Then, we will update the weight using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1896.1">learning rate:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1897.1">
for i in range(len(self.weights)):
    layer = y[i].reshape(1, nn.net_arch[i] + 1)
    delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])
    self.weights[i] -= learning_rate * layer.T.dot(delta)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1898.1">This concludes the implementation of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1899.1">fit</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1900.1"> method.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1901.1">We’ll now write a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1902.1">predict</span></strong><span class="koboSpan" id="kobo.1903.1"> function to check the results, which returns the network output for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1904.1">given input:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1905.1">
def predict(self, x):
    val = np.concatenate((np.ones(1).T, np.array(x)))
    for i in range(0, len(self.weights)):
        val = self.activation_func(
            np.dot(val, self.weights[i]))
        al = np.concatenate((np.ones(1).T,
            np.array(val)))
    return val[1]</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1906.1">Finally, we’ll write the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1907.1">plot_decision_regions</span></strong><span class="koboSpan" id="kobo.1908.1"> method, which plots</span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.1909.1"> the hyperplane separating</span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.1910.1"> the classes (in our case, represented as lines), based on the input variables. </span><span class="koboSpan" id="kobo.1910.2">We will create a two-dimensional grid with one axis for each input variable. </span><span class="koboSpan" id="kobo.1910.3">We will plot the NN predictions for all input value combinations across the whole grid. </span><span class="koboSpan" id="kobo.1910.4">We will take the network output larger than 0.5 to be </span><strong class="source-inline"><span class="koboSpan" id="kobo.1911.1">true</span></strong><span class="koboSpan" id="kobo.1912.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1913.1">false</span></strong><span class="koboSpan" id="kobo.1914.1"> otherwise (we’ll see the plots at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.1915.1">the section):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1916.1">
def plot_decision_regions(self, X, y, points=200):
    markers = ('o', '^')
    colors = ('red', 'blue')
    cmap = ListedColormap(colors)
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)
    xx1, xx2 = np.meshgrid(np.arange(x1_min,
        x1_max, resolution),
        np.arange(x2_min, x2_max, resolution))
    input = np.array([xx1.ravel(), xx2.ravel()]).T
    Z = np.empty(0)
    for i in range(input.shape[0]):
        val = nn.predict(np.array(input[i]))
        if val &lt; 0.5:
            val = 0
        if val &gt;= 0.5:
            val = 1
        Z = np.append(Z, val)
    Z = Z.reshape(xx1.shape)
    plt.pcolormesh(xx1, xx2, Z, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    # plot all samples
    classes = ["False", "True"]
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0],
            y=X[y == cl, 1],
            alpha=1.0,
            c=colors[idx],
            edgecolors='black',
            marker=markers[idx],
            s=80,
            label=classes[idx])
    plt.xlabel('x1)
    plt.ylabel('x2')
    plt.legend(loc='upper left')
    plt.show()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1917.1">This concludes</span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.1918.1"> the implementation</span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.1919.1"> of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1920.1">NeuralNetwork</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1921.1"> class.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1922.1">Finally, we can run the program with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1923.1">following code:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1924.1">
np.random.seed(0)
# Initialize the NeuralNetwork with 2 input, 2 hidden, and 1 output units
nn = NeuralNetwork([2, 2, 1])
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])
y = np.array([0, 1, 1, 0])
nn.fit(X, y, epochs=10)
print("Final prediction")
for s in X:
    print(s, nn.predict(s))
nn.plot_decision_regions(X, y)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1925.1">We will build the default</span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.1926.1"> network, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1927.1">nn = NeuralNetwork([2,2,1])</span></strong><span class="koboSpan" id="kobo.1928.1">. </span><span class="koboSpan" id="kobo.1928.2">The first and last values (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1929.1">2</span></strong><span class="koboSpan" id="kobo.1930.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1931.1">1</span></strong><span class="koboSpan" id="kobo.1932.1">) represent the input</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.1933.1"> and output layers and cannot be modified, but we can add different numbers of hidden layers with different numbers of units. </span><span class="koboSpan" id="kobo.1933.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1934.1">([2,4,3,1])</span></strong><span class="koboSpan" id="kobo.1935.1"> will represent a three-layer NN, with four units in the first hidden layer and three units in the second </span><span class="No-Break"><span class="koboSpan" id="kobo.1936.1">hidden layer.</span></span></p><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1937.1">We will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1938.1">numpy.random.seed(0)</span></strong><span class="koboSpan" id="kobo.1939.1"> to ensure that the weight initialization is consistent across runs, so we can compare their results. </span><span class="koboSpan" id="kobo.1939.2">This is a common practice when </span><span class="No-Break"><span class="koboSpan" id="kobo.1940.1">training NNs.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1941.1">Now, we will define the training XOR data and labels in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1942.1">x</span></strong><span class="koboSpan" id="kobo.1943.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1944.1">y</span></strong><span class="koboSpan" id="kobo.1945.1"> respectively. </span><span class="koboSpan" id="kobo.1945.2">We will run the training for 10 epochs. </span><span class="koboSpan" id="kobo.1945.3">Finally, we will plot </span><span class="No-Break"><span class="koboSpan" id="kobo.1946.1">the result.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1947.1">In the following diagrams, you can see how the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1948.1">nn.plot_decision_regions</span></strong><span class="koboSpan" id="kobo.1949.1"> function method plots the hyperplanes, which separate the classes. </span><span class="koboSpan" id="kobo.1949.2">The circles represent the network output for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1950.1">(true, true)</span></strong><span class="koboSpan" id="kobo.1951.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1952.1">(false, false)</span></strong><span class="koboSpan" id="kobo.1953.1"> inputs, while the triangles represent the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1954.1">(true, false)</span></strong><span class="koboSpan" id="kobo.1955.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1956.1">(false, true)</span></strong><span class="koboSpan" id="kobo.1957.1"> inputs for the XOR function. </span><span class="koboSpan" id="kobo.1957.2">To the left, we can see the hyperplane of an NN with two hidden units, and to the right, we can see an NN with four </span><span class="No-Break"><span class="koboSpan" id="kobo.1958.1">hidden units:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<span class="koboSpan" id="kobo.1959.1"><img alt="Figure 2.23 – Left: the hyperplane learned by an NN with two hidden units, and right: the hyperplane of an NN with four hidden units" src="image/B19627_02_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1960.1">Figure 2.23 – Left: the hyperplane learned by an NN with two hidden units, and right: the hyperplane of an NN with four hidden units</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1961.1">Networks with different architectures can produce different separating regions. </span><span class="koboSpan" id="kobo.1961.2">In the preceding figure, we can see that while the network finds the right solution, the curves separating the regions</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.1962.1"> will be different, depending</span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.1963.1"> on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1964.1">chosen architecture.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1965.1">We are now ready to start looking more closely at what deep neural nets are and </span><span class="No-Break"><span class="koboSpan" id="kobo.1966.1">their applications.</span></span></p>
<h1 id="_idParaDest-45" lang="en-GB"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.1967.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1968.1">In this chapter, we introduced NNs in detail, and we mentioned their success vis-à-vis other competing algorithms. </span><span class="koboSpan" id="kobo.1968.2">NNs are comprised of interconnected units, where the weights of the connections characterize the strength of the communication between different units. </span><span class="koboSpan" id="kobo.1968.3">We discussed different network architectures, how an NN can have many layers, and why inner (hidden) layers are important. </span><span class="koboSpan" id="kobo.1968.4">We explained how information flows from the input to the output by passing from one layer to the next, based on weights and the activation function. </span><span class="koboSpan" id="kobo.1968.5">Finally, we showed how to train NNs – that is, how to adjust their weights using GD </span><span class="No-Break"><span class="koboSpan" id="kobo.1969.1">and BP.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1970.1">In the following chapter, we’ll continue discussing deep NNs. </span><span class="koboSpan" id="kobo.1970.2">We’ll explain in particular the meaning of </span><em class="italic"><span class="koboSpan" id="kobo.1971.1">deep</span></em><span class="koboSpan" id="kobo.1972.1"> in deep learning, and that it not only refers to the number of hidden layers in a network but to the quality of the learning of the network. </span><span class="koboSpan" id="kobo.1972.2">For this purpose, we’ll show how NNs learn to recognize features and compile them as representations of larger objects. </span><span class="koboSpan" id="kobo.1972.3">We’ll also describe a few important deep learning libraries and, finally, provide a concrete example of applying NNs to handwritten </span><span class="No-Break"><span class="koboSpan" id="kobo.1973.1">digit recognition.</span></span></p>
</div>
</body></html>