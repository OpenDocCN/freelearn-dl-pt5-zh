- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization in Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore another popular field of deep learning – computer
    vision. **Computer vision** is a large field with many tasks, from classification
    through generative models to object detection. Even though we can’t cover all
    of them, we will supply methods that can apply to all tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a **convolutional neural** **network** (**CNN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a CNN with vanilla **neural network** (**NN**) methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a CNN with transfer learning for object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation using transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this chapter, you will be able to handle several computer vision
    tasks such as image classification, object detection, instance segmentation, and
    semantic segmentation. You will be able to apply several tools to regularize the
    trained models, such as architecture, transfer learning, and freezing weights
    for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will train CNNs, object detection, and semantic segmentation
    models, requiring the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchvision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultralytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation-models-pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, after reviewing the fundamental components of CNN, we will train
    one on a classification task – the CIFAR10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer vision is a special field for many reasons. The data handled in computer
    vision projects is usually rather large, multidimensional, and unstructured. However,
    its most specific aspect is arguably its spatial structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'With its spatial structure comes a lot of potential difficulties, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aspect ratio**: Some images come with different aspect ratios depending on
    their source, such as 16/9, 4/3, 1/1, and 9/16'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occlusion**: An object can be occluded by another one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deformation**: An object can be deformed, either because of perspective or
    physical deformation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point of view**: Depending on the point of view, an object can look totally
    different'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Illumination**: A picture can be taken in many light environments that may
    alter the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of these difficulties are summarized in *Figure 10**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Examples of difficulties specific to computer vision](img/B19629_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Examples of difficulties specific to computer vision
  prefs: []
  type: TYPE_NORMAL
- en: Due to the spatial structure of data, models are needed to process it. While
    recurrent neural networks are well suited to sequential data, CNNs are well suited
    to spatially structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to properly build a CNN, we need to introduce two new types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s quickly explain them both.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **convolutional layer** is a layer made of convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: In a fully connected layer, a weighted sum of the input features (or the input
    activation of the previous layer) is computed, with the weights being learned
    while training.
  prefs: []
  type: TYPE_NORMAL
- en: In a convolutional layer, a convolution is applied to the input features (or
    the input activation of the previous layer), with the values of the convolution
    kernel being learned while training. It means the NN will learn the kernel through
    training to extract the most relevant features from the input images.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN can be fine-tuned with several hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stride size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of output channels (i.e., the number of kernels to learn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: See the *There’s more…* subsection for more information about kernels and other
    hyperparameters of the CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **pooling layer** allows you to reduce the dimensionality of images and is
    commonly used in CNNs. For example, a max pooling layer with a 2x2 kernel will
    reduce the dimension of an image by 4 (a factor of 2 both in width and height),
    as shown in *Figure 10**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – On the left is an input image of 4x4, at the top right is the
    result of 2x2 max pooling, and at the bottom right is the result of a 2x2 average
    pooling](img/B19629_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – On the left is an input image of 4x4, at the top right is the
    result of 2x2 max pooling, and at the bottom right is the result of a 2x2 average
    pooling
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of pooling, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max pooling**: Computing the maximum value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average pooling**: Computing the average value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global average pooling**: Computing a global average value for all channels
    (commonly used before fully connected layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeNet-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**LeNet-5** was one of the first proposed CNN architectures, by Yann Le Cun,
    for handwritten digit recognition. Its architecture is shown in *Figure 10**.3*,
    taken from Yann’s paper *Gradient-Based Learning Applied to* *Document Recognition*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – LeNet-5’s original architecture](img/B19629_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – LeNet-5’s original architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe it in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image of dimension 32x32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C1**: A convolution layer with a 5x5 kernel and 6 output channels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S2**: A pooling layer with a 2x2 kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C3**: A convolution layer with a 5x5 kernel and 16 output channels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S4**: A pooling layer with a 2x2 kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C5**: A fully connected layer with 120 units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F6**: A fully connected layer with 84 units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: An output layer with 10 units for 10 classes (0 to 9 digits)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will implement this network in this recipe on the CIFAR-10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this recipe, the needed libraries can be installed with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will train a CNN for image classification on the CIFAR-10
    dataset. The CIFAR-10 dataset is a dataset of 32x32 RGB images, made of 10 classes
    – `plane`, `car`, `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, and `truck`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: matplotlib for visualization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy for data manipulation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Several torch modules and classes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset and transformation module from torchvision
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the transformation to apply to images. Here, it’s a simple two-step
    transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert data to a torch tensor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalize with `0.5` mean and standard deviation values:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data and instantiate the data loaders. The previously defined transformation
    is applied directly at loading as an argument of the `CIFAR10` constructor. The
    data loaders are here instantiated with a batch size of `64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, we can visualize a few images to check what the inputs are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – 64 random images from the CIFAR-10 dataset. The images are
    blurry, but most are clear enough for humans to classify correctly](img/B19629_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – 64 random images from the CIFAR-10 dataset. The images are blurry,
    but most are clear enough for humans to classify correctly
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the `LeNet5` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This implementation has almost the same layers as the original paper. Here
    are a few interesting points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.Conv2d` is the 2D convolution layer in `torch`, having as hyperparameters
    the output dimension, kernel size, stride, and padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nn.MaxPool2d` is the max pooling layer in torch, having as a hyperparameter
    the kernel size (and, optionally, the stride, defaulting to the kernel size)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the ReLU activation function, even if it was not the function used in
    the original paper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.flatten` allows us to flatten a 2D tensor to a 1D tensor so that we
    can apply fully connected layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instantiate the model, and make sure that it works well with a random input
    tensor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the loss and optimizer – a cross-entropy loss for multiclass classification,
    with an Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a helper function, `epoch_step_cifar`, that computes forward propagation,
    backpropagation (in the case of the training set), loss, and accuracy for an epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a helper function, `train_cifar_classifier`, that trains the model
    on a given number of epochs and returns the loss and accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the helper function, train the model on 50 epochs and store the loss
    and accuracy for both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last line of the output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the loss for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the resulting graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Cross-entropy loss for the train and test sets](img/B19629_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Cross-entropy loss for the train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: After less than 10 epochs, the curves start diverging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the accuracy as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the graph that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Accuracy as a function of the epoch for both train and test
    sets](img/B19629_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Accuracy as a function of the epoch for both train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: After about 20 epochs, the accuracy reaches a plateau of around 60% accuracy,
    while the train accuracy keeps growing, suggesting overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, let’s have a quick recap of some necessary tools and concepts
    to properly understand CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: How to store an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel and convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to store an image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An image is nothing but a spatially arranged array of pixels. For example, a
    1-million-pixel grayscale square image is an array of 1,000x1,000 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Each pixel is usually stored as an 8-bit value and can be represented as an
    unsigned integer in the range of [0, 255]. So, ultimately, such an image can be
    represented in Python as a NumPy array of `uint8` with a shape of (1000, 1000).
  prefs: []
  type: TYPE_NORMAL
- en: We can go one step further with color images. A color image is commonly stored
    with three channels – **Red, Green, and Blue** (**RGB**). Each of these channels
    is stored as an 8-bit integer, so a squared 1M pixels color image can be stored
    as a NumPy array of a shape of (3, 1000, 1000), assuming the channel is stored
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: There are many other ways to describe a colored image – **hue, saturation, value**
    (**HSV**), CIELAB, transparency, and so on. However, in this book, and many computer
    vision cases, RGB color space is enough.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already used padding in earlier chapters for NLP processing. It consists
    of adding “space” around an image, basically by adding layers of values around
    an image. An example of padding on a 4x4 matrix is given in *Figure 10**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – An example of a 4x4 matrix padded with one layer of zeros](img/B19629_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – An example of a 4x4 matrix padded with one layer of zeros
  prefs: []
  type: TYPE_NORMAL
- en: 'Padding can take several arguments, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers of padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding method – a given value, repetition, mirror, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, zero padding is used, but sometimes, more sophisticated padding
    can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel and convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A convolution is a mathematical operation, between an image and a kernel, that
    outputs another image. It can be simply schematized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution (input image, kernel) → Output image
  prefs: []
  type: TYPE_NORMAL
- en: A kernel is just a smaller matrix of predefined values, allowing us to get a
    property from an image through convolution. For example, with the right kernel,
    a convolution on an image allows us to blur an image, sharpen an image, detect
    edges, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation of a convolution is quite simple and can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Spatially match the kernel and the image from the top-left corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the weights sum of all the image pixels, with the corresponding kernel
    value as the weight, and store this value as the top-left output image pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go one pixel to the right and repeat; if you reach the rightmost edge of the
    image, go back to the leftmost pixel and one pixel down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This may look complicated, but it gets much easier with a diagram, as shown
    in *Figure 10**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – An example of an image convolution by a kernel.Note that the
    resulting image is smaller in dimension](img/B19629_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – An example of an image convolution by a kernel.Note that the resulting
    image is smaller in dimension
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 10**.8*, the output image is slightly smaller than
    the input image. Indeed, the larger the kernel, the smaller the output image.
  prefs: []
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One more useful concept about convolutions is the concept of **stride**. The
    stride is the number of step pixels to take between two convolutional operations.
    In the example in *Figure 10**.8*, we implicitly considered a stride of 1 – the
    kernel is moved by one pixel each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it’s possible to consider a larger stride – we can have a step of
    any number, as shown in *Figure 10**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – The stride effect on convolution – the larger the stride, the
    smaller the output image](img/B19629_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – The stride effect on convolution – the larger the stride, the
    smaller the output image
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a larger stride will mainly have several, related, consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: Since the convolution skips more pixels, less information remains in the output
    image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output image is smaller, allowing a reduction in dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computation time is lower
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the needs, having a stride larger than one can be an efficient
    way to have lower computation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we can control several aspects of a convolution with three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stride
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They all affect the size of the output image, following this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula can be broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I* is the input image size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is the kernel size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* is the padding size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is the stride size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*O* is the output image size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to this formula, we can efficiently choose the required parameters in
    any case.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Documentation about CNN layers: [https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.xhtml#torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.xhtml#torch.nn.Conv2d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation about pooling layers: [https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.xhtml#torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.xhtml#torch.nn.MaxPool2d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A paper about LeNet-5: [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The amazing Stanford course about deep learning for computer vision: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a CNN with vanilla NN methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since CNNs are a special kind of NNs, most vanilla NN optimization methods
    can be applied to them. A non-exhaustive list of regularization techniques we
    can use with CNNs is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected number of units (if any)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will apply batch normalization to add regularization, reusing
    the LeNet-5 model on the CIFAR-10 dataset, but any other method may work as well.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization is a simple yet very effective method that can help NNs
    regularize and converge faster. The idea of batch normalization is to normalize
    the activation values of a hidden layer for a given batch. The method is very
    similar to a standard scaler for data preparation of quantitative data, but there
    are some differences. Let’s have a look at how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to compute the mean value µ and the standard deviation ![](img/Formula_10_002.png)
    of the activation values ![](img/Formula_10_003.png) of a given layer. Assuming
    ![](img/Formula_10_004.png) is the activation value of the I-th unit of the layer
    and the layer has *n* units, here are the formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_005.jpg)![](img/Formula_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like with a standard scaler, it is now possible to compute the rescaled
    the activation values ![](img/Formula_10_007.png) with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_10_009.png) is just a small value to avoid division by
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, unlike a standard scaler, there is one more step that allows the model
    to learn what is the best distribution with a scale and shift approach, thanks
    to two new learnable parameters, β and ![](img/Formula_10_010.png). They are used
    to compute the final batch normalization output ![](img/Formula_10_011.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_10_013.png) allows us to adjust the scale, while β allows
    us to adjust the shift. These two parameters are learned during training, like
    any other parameter of the NN. This allows the model to adjust the distribution
    if required to improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: For a more visual example, we can see in *Figure 10**.10* a possible distribution
    of activation values on the left for a three-unit layer – the values are skewed
    and with a large standard deviation. After batch normalization, on the right part
    of *Figure 10**.10*, the distributions are now close to normal distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Possible activation distributions of three units of a layer
    before (left) and after (right) batch normalization](img/B19629_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Possible activation distributions of three units of a layer before
    (left) and after (right) batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this method, NNs tend to converge faster and generalize better, as
    we will see in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will reuse torch and its integrated CIFAR-10 dataset so
    that all the needed libraries can be installed with the following command line
    (if not already installed in the previous recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we will reuse the same data and almost the same network as in the previous
    recipe, we will assume the imports and instantiated classes can be reused:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the regularized model. Here, we will mostly reuse the LeNet-5 architecture,
    with added batch normalization at each step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the code, batch normalization can be simply added as a layer with
    `nn.BatchNorm1d` (or `nn.BatchNorm2d` for the convolutional part), which takes
    as an argument the following input dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of units for fully connected layers and `BatchNorm1d`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of kernels for convolution layers and `BatchNorm2d`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Placing batch normalization after the activation function is arguable, and some
    people would rather place it before the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the model, with the loss as cross-entropy and the optimizer as
    Adam:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model over 20 epochs by reusing the `train_cifar_classifier` helper
    function of the previous recipe. Note that the model converges faster than without
    batch normalization in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the loss as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Cross-entropy loss as a function of the epoch](img/B19629_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting starts appearing after only a few epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the accuracy for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Accuracy as a function of the epoch. The test accuracy climbs
    to 66%, compared to 61% without batch normalization](img/B19629_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Accuracy as a function of the epoch. The test accuracy climbs
    to 66%, compared to 61% without batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we can see that while there is still some overfitting, the test
    accuracy improved significantly from 61% to 66%, thanks to batch normalization.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What’s interesting about CNNs is that we can have a look at what they learn
    from data. One way to do so is to look at the learned kernels. This can be done
    using the `visualize_kernels` function, as defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now apply this function to visualize the learned kernels of the `C1`
    and `C3` layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the C1 layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19629_10_13-A.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the C3 layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Top – the learned kernels of the C1 layer, and bottom – the
    learned kernels of the C3 layer](img/B19629_10_13-B.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Top – the learned kernels of the C1 layer, and bottom – the learned
    kernels of the C3 layer
  prefs: []
  type: TYPE_NORMAL
- en: Displaying kernels is not always helpful, but, depending on the task, they can
    give hints on what shapes a model recognizes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The torch documentation about batch normalization: [https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The batch normalization paper: [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A very well-written blog post about batch normalization: [https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a CNN with transfer learning for object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will perform another typical task in computer vision – object
    detection. Before taking advantage of the power of transfer learning to help get
    better performances using a **You Only Look Once** (**YOLO**) model (a widely
    used class of models for object detection), we will give insights about what object
    detection is, the main methods and metrics, as well as the COCO dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Object detection** is a computer vision task, involving both the identification
    and localization of objects of a given class (for example, a car, phone, person,
    or dog). As shown in *Figure 10**.14*, the objects are usually localized, thanks
    to predicted bounding boxes, as well as predicted classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – An example of an image with object detection. Objects are
    detected with a bounding box and a class](img/B19629_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – An example of an image with object detection. Objects are detected
    with a bounding box and a class
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers have proposed many methods to help solve object detection problems,
    some of which are heavily used in many industries. There are several groups of
    methods for object detection, but perhaps the two most widely used groups of methods
    are currently the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One-stage methods, such as YOLO and SSD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-stage methods, based on **Region-Based** **CNN** (**R-CNN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods based on R-CNN are powerful and usually more accurate than one-stage
    methods. On the other hand, one-stage methods are usually less computationally
    expensive and can run in real time, but they may fail at detecting small objects
    more often.
  prefs: []
  type: TYPE_NORMAL
- en: Mean average precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is a specific task, a specific metric is needed to assess the performances
    of such models – the **mean Average Precision** (**mAP**). Let’s get an overview
    of what mAP is. For that, we need to introduce several concepts, such as the **Intersection
    over Union** (**IoU**) and precision and recall in the context of object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'When an object is detected, it comes with three pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: A predicted class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bounding box (usually four points, either center plus width and height, or
    top-left and bottom-right locations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A confidence level or probability that the box contains an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To consider an object successfully detected, the classes must match, and the
    bounding box must be well localized. While knowing whether the classes match is
    trivial, the bounding box localization is computed using a metric called IoU.
  prefs: []
  type: TYPE_NORMAL
- en: Having an explicit name, the IoU can be computed as the intersection of the
    ground truth and the predicted boxes, over the union of those two same boxes,
    as shown in *Figure 10**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – A representation of the IoU metric](img/B19629_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – A representation of the IoU metric
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two bounding boxes – for example, *A* and *B* – IoU can be mathematically
    described with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'IoU has several advantages for a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: The values are between 0 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of 0 means the two boxes don’t overlap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of 1 means the two boxes perfectly match
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A threshold is then applied to the IoU. If the IoU is above the threshold, it
    is considered a **True Positive** (**TP**); otherwise, it is considered a **False
    Positive** (**FP**), allowing us to effectively compute the precision. Finally,
    a **False Negative** (**FN**) is an object that was not detected, given the IoU
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Using these definitions of TP, FP, and FN, it is then possible to compute the
    precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, the precision *P* and recall *R* formulas are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_015.jpg)![](img/Formula_10_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using P and R, it is possible to plot the **Precision-Recall curve** (**PR curve**),
    with P as a function of R for various confidence-level thresholds, from 0 to 1\.
    Using this PR curve, it is possible to compute the **Average Precision** (**AP**)
    for a given class by averaging P for different values of R (for example, averaging
    the interpolated P for R values in [0, 0.1, 0.2... 1]).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The average recall metric can be computed reciprocally using the same method,
    by reversing R and P.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the mAP is simply computed by averaging the AP over all the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: See the *See also* subsection for a link to a great blog post that explains
    in detail the mAP computation.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback of this AP computation is that we considered only one IoU threshold,
    considering the same way almost perfect boxes with an IoU of 0.95 and not-so-good
    boxes with an IoU of 0.5\. This is why some evaluation metrics average the AP
    for several IoU thresholds – for example, from 0.5 to 0.95 with a step of 0.05,
    usually noted as `AP@[IoU=0.5:0.95`] or `AP50-95`.
  prefs: []
  type: TYPE_NORMAL
- en: COCO dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Common Objects in Context** (**COCO**) dataset is a widely used dataset
    in object detection, having the following nice features:'
  prefs: []
  type: TYPE_NORMAL
- en: Hundreds of thousands of images with labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 80 classes of objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexible terms of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wide community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a standard dataset when working with object detection. Thanks to that,
    most standard object detection models come with a set of pre-trained weights on
    the COCO dataset, allowing us to take advantage of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the YOLO algorithm, proposed by Ultralytics. **YOLO**
    stands for **You Only Look Once**, referring to the fact the method operates in
    a single stage, enabling real-time execution on devices with powerful enough power.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is a popular object detection algorithm that was first proposed in 2015\.
    It has had a lot of new versions with improvements since then; version 8 is currently
    being developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be installed simply with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: 'We will train an object detection algorithm on a vehicles dataset available
    on Kaggle. It can be downloaded and prepared with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset using the Kaggle API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rename the folder for simplicity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `datasets` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move the dataset to this folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a result, you should now have a folder dataset with the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is split into `train`, `val`, and `test` sets, with respectively
    738, 185, and 278 images. As we will see in the next subsection, these are typical
    road traffic images. The labels have seven classes – `Car`, `Number Plate`, `Blur
    Number Plate`, `Two-Wheeler`, `Auto`, `Bus`, and `Truck`. We can now proceed to
    train the object detection model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first have to quickly explore the dataset and then train and evaluate
    a YOLO model on this data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`matplotlib` and `cv2` for image loading and visualization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YOLO` for the model training'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glob` as `util` to list the files:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now explore the dataset. First, we will list the images in the `train`
    folder using `glob`, and then we will display eight of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – A patchwork of eight images from the train set of the traffic
    dataset](img/B19629_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – A patchwork of eight images from the train set of the traffic
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, these are mostly traffic-related images of different shapes and
    aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a look at the labels by reading a file, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: 'The labels are an object per line, so here, we have three labeled objects in
    the image. Each line contains five numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: The class number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box center *x* coordinate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box center *y* coordinate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box height
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that all the box information is relative to the size of the image, so they
    are represented as floats in [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: There are other data formats for boxes in images such as the COCO and the Pascal
    VOC formats. More information about can be found in the *See* *also* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even plot this image with the boxes of the labels, using the `plot_labels`
    function implemented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – An example of an image and its labeled bounding boxes](img/B19629_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – An example of an image and its labeled bounding boxes
  prefs: []
  type: TYPE_NORMAL
- en: In this photo, we have labels for two cars and one plate. Let’s go on to the
    next step to train a model on this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a `.yaml` file, expected by the YOLO model, containing the
    dataset location and classes. Create and edit a file named `dataset.yaml` in the
    current directory with your favorite editor, and then fill it with the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now instantiate a new model. This will instantiate a YOLOv8 nano model.
    The YOLO model comes in five sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`''yolov8n.yaml''` for the smallest model with 3.2 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''yolov8s.yaml''` with 11.2 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''yolov8m.yaml''` with 25.9 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''yolov8l.yaml''` with 43.7 million parameters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''yolov8x.yaml''` for the largest model with 68.2 million parameters:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model, providing the dataset with the previously created `dataset.yaml`
    file, the number of epochs, and the name (optional):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: A lot of information is displayed when a model trains in memory, losses, and
    metrics. There’s nothing too complicated if you want to look at it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The name is optional but allows us to easily find where the results and output
    are stored – in the `runs/detect/<name>` folder. If the folder already exists,
    it is simply incremented and not overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this folder, several useful files can be found, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`weights/best.pt`: The weights of the epoch that has the best validation loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`results.csv` with the logged results for each epoch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several curves and information about the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Display the results. Here, we will display the automatically saved results
    image, `results.png`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – A results summary of the YOLO model trained from scratch after
    100 epochs](img/B19629_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – A results summary of the YOLO model trained from scratch after
    100 epochs
  prefs: []
  type: TYPE_NORMAL
- en: Several train and validation losses are displayed, as well as several losses
    – precision (P), recall (R), mAP50, and mAP50-95.
  prefs: []
  type: TYPE_NORMAL
- en: The results are encouraging considering the small dataset – we see a decreasing
    loss and a mAP50 increasing to 0.7, meaning the model is learning well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s display the results as an example of the test set. For that, we first
    need to implement a function that allows us to display the image and the predicted
    boxes and classes, `plot_results_one_image`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE247]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE248]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then compute the inference and display the results on an image from
    the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE250]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE252]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE254]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – An image from the test set and the predicted detections from
    the trained model](img/B19629_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – An image from the test set and the predicted detections from
    the trained model
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, our YOLO model has already learned to detect and correctly classify
    several classes. However, there is still room for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: The boxes do not perfectly match the objects; they are either too large or too
    small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An object may have two classes (even if the difference between the `Blur Number
    Plate` and `Number Plate` classes is arguable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that the direct output of the YOLO model usually contains
    many more bounding boxes. A postprocessing step, called the **non-max suppression**
    algorithm, has been applied here. This algorithm only keeps bounding boxes with
    a high enough confidence level, and small enough overlapping (computed with IoU)
    with other boxes of the same class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to fix this using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Training with transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now train another model on this exact same dataset, with the same number
    of epochs. However, instead of using a model with random weights, we will load
    a model that was trained on the COCO dataset, allowing us to take advantage of
    transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate and train a pre-trained model. Instead of instantiating the model
    with `yolov8n.yaml`, we only need to instantiate it with `yolov8n.pt`; this will
    automatically download the pretrained weights and load them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE258]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now display the results of this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE263]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20 – A results summary of the YOLO model, with pretrained weights
    on the COCO dataset after 100 epochs](img/B19629_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – A results summary of the YOLO model, with pretrained weights
    on the COCO dataset after 100 epochs
  prefs: []
  type: TYPE_NORMAL
- en: Using transfer learning, all metrics have better performances – the mAP50 now
    climbs up to 0.8 against 0.7 previously, which is a significant improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now display the results in the same image as we did previously so that
    we can compare them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE265]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.21 – An image from the test set and the predicted detections from
    the model with the pretrained weights](img/B19629_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 – An image from the test set and the predicted detections from
    the model with the pretrained weights
  prefs: []
  type: TYPE_NORMAL
- en: This single image already shows several improvements – not only do the bounding
    boxes now perfectly fit the objects, but also no two objects are detected for
    a single number plate anymore. Thanks to transfer learning, we were able to efficiently
    help the model generalize.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we focused on the object detection task, but YOLO models can
    do much more than that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same library, it is also possible to train models for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these models also come with pretrained weights so that transfer learning
    can be leveraged to get good performances, even with small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A blog post explaining the mAP metric computation: [https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/](https://pyimagesearch.com/2022/05/02/mean-average-precision-map-using-the-coco-evaluator/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The COCO dataset website, which allows you to easily browse and display the
    dataset: [https://cocodataset.org/#home](https://cocodataset.org/#home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A link to the original YOLO paper: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A link to the ultralytics documentation: [https://docs.ultralytics.com/usage/python/](https://docs.ultralytics.com/usage/python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The YOLOv8 GitHub repo: [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A clear and concise post from Albumentations about the main bounding boxes
    formats: [https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation using transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will take advantage of transfer learning and the fine-tuning
    of pretrained models to undertake a specific task of computer vision – the semantic
    segmentation of drone images.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and instance segmentation are about detecting objects in an
    image – an object is delimited by a bounding box, as well as a polygon in the
    case of instance segmentation. Alternatively, **semantic segmentation** is about
    classifying all the pixels of an image in a class.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 10**.22*, all pixels have a given color so that each
    one is attributed a class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.22 – An example of annotation of semantic segmentation. On the
    left is the original image, and on the right is the labeled image – there is one
    class of object per color, and each pixel is assigned to a given class](img/B19629_10_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 – An example of annotation of semantic segmentation. On the left
    is the original image, and on the right is the labeled image – there is one class
    of object per color, and each pixel is assigned to a given class
  prefs: []
  type: TYPE_NORMAL
- en: Even if it may look similar to instance segmentation, we will see in this recipe
    that the concepts and methods used are quite different. We will review the possible
    metrics, losses, architectures, and encoders to solve a semantic segmentation
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since semantic segmentation can be seen as a multiclass classification of each
    pixel, the most intuitive metric is the averaged accuracy score – the pixel accuracy
    averaged over the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it can be used to sometimes yield solid results. However, most of the
    time in semantic segmentation, some classes are far less present than others –
    for example, in urban pictures, it is likely that there will be a lot of pixels
    of roads and buildings, and much less of persons or bikes. It is then likely to
    have good accuracy but not offer a model that accurately segments underrepresented
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the limitation of the accuracy metric, many other metrics were proposed.
    One of the most used metrics in semantic segmentation is the IoU, already explained
    in the previous recipe. The IoU can be computed for each class independently and
    then averaged to compute a single metric (other averaging methods exist and are
    explored in more detail in *There’s more…* subsection). The IoU is sometimes referred
    to as the **Jaccard index**.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more frequently used metric is the **Dice coefficient**. Given two sets
    of pixels, A (for example, the predictions for a class) and B (for example, the
    ground truth for a class), the Dice coefficient can be computed with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, |A| is simply the number of pixels in A, sometimes called the cardinality
    of A. The Dice coefficient is usually compared to the F1 score and is mathematically
    equivalent. Just like the IoU, the Dice coefficient can be averaged over all the
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, other metrics exist and can be used, but they are outside the scope
    of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Losses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several losses were developed over the years to improve the performance of semantic
    segmentation models. Again, if we just think of semantic segmentation as a classification
    task over many pixels, cross-entropy loss is an intuitive choice. However, just
    like the accuracy score, cross-entropy loss is not a good choice in the case of
    imbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is common to simply use the Dice loss, which directly reuses
    the Dice coefficient. Dice loss is usually better in case of class imbalance,
    but it sometimes has bumpy training losses.
  prefs: []
  type: TYPE_NORMAL
- en: Many other losses were proposed, such as the focal loss and the Tversky loss,
    and all have strengths and improvements. A paper summarizing the most widely used
    losses is cited in the *See* *also* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semantic segmentation is a very specific task, in the sense that unlike object
    detection, the input and output are both images. Indeed, for a given input image
    of size 480x640 (purposefully omitting the RGB channels), the output image is
    expected to have the exact same dimension of 480x640, since each pixel must have
    a predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, for an *N*-class semantic segmentation task, the output dimension
    would be 480x640x*N*, having for each pixel a set of *N* probabilities as the
    output of a softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectures to deal with such problems are usually based on the encoder-decoder
    principle:'
  prefs: []
  type: TYPE_NORMAL
- en: An encoder computes describing features on the input image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder decodes those encoded features in order to have the expected outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the most famous architectures for semantic segmentation is the U-Net
    architecture, shown in *Figure 10**.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.23 – The U-Net architecture as presented in the original paper
    U-Net: Convolutional Networks for Biomedical Image Segmentation](img/B19629_10_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.23 – The U-Net architecture as presented in the original paper U-Net:
    Convolutional Networks for Biomedical Image Segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in *Figure 10**.23*, the U-Net architecture can be broken down
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The input image is at the top left of the diagram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input image is sequentially encoded, as we can move down to the bottom of
    the diagram with convolutional and pooling layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we go back up to the top right of the diagram, the output of the encoder
    is decoded and concatenated with the previous output of the encoder with convolutional
    and upscaling layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, an output of the same width and height as the input image is predicted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One strength of U-Net is that it encodes and then decodes, and it also concatenates
    the intermediate encodings to have efficient predictions. It is now a standard
    architecture when it comes to semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other architectures exist, some of which are widely used too, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Pyramid** **Networks** (**FPN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U-Net++, a proposed improvement of U-Net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the original U-Net paper, as we can see in *Figure 10**.23*, the encoder
    part is a specific one, made of convolution and pooling layers. In practice, however,
    it is common to use famous networks as encoders, pretrained on ImageNet or the
    COCO dataset, so that we can take advantage of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the needs and constraints, several encoders may be used, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet – a light encoder, developed for fast inference on the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual Geometry Group** (**VGG**) architectures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet and ResNet-based architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientNet architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SMP library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Segmentation Models PyTorch** (**SMP**) library is an open source library
    allowing us to do all we need, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing architectures such as U-Net, FPN, or U-Net++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing encoders such as VGG or MobileNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Already implemented losses such as the Dice loss and the focal loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helper functions to compute metrics such as Dice and the IoU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use this library in this recipe to train semantic segmentation models
    on a drone dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to download a dataset containing 400 images and
    associated labels. It can be downloaded with the Kaggle API using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: We end up with three folders, containing several datasets. We will use the one
    in `classes_dataset`, a five-classes dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to install the required libraries with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first train a U-Net model with a MobileNet encoder with transfer learning
    on our task, and then we will do the same with fine-tuning techniques by freezing
    layers and gradually decreasing the learning rate, in order to improve the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training with ImageNet weights and unfreezing all weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will first train a pretrained model on ImageNet in a regular fashion, with
    all the weights trainable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first make the required imports for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `DroneDataset` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE302]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE307]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE308]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `__init__` method just reads all the available image and mask files. It
    also takes a Boolean variable for the train versus test dataset, allowing you
    to select only the first 80% or the last 20% of the files.
  prefs: []
  type: TYPE_NORMAL
- en: The `__getitem__` method simply loads an image from a path and returns the transformed
    image as well as the mask as tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the transformation to apply it to images – here, it’s simply a
    tensor conversion and a normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE309]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a few constants – the batch size, learning rate, classes, and device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the datasets and data loaders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE327]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display an image with an overlay of the associated labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE337]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE338]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE339]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE340]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE341]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE342]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE343]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.24 – An image of the Drone dataset with its mask overlay, made
    of five colors for five classes](img/B19629_10_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 – An image of the Drone dataset with its mask overlay, made of
    five colors for five classes
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, there are several colors overlayed on the image:'
  prefs: []
  type: TYPE_NORMAL
- en: Yellow for `'landing-zones'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dark green for `'soft-surfaces'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue for `'water'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Purple for `'obstacles'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light green for `'moving-objects'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instantiate the model – a U-Net architecture, with EfficientNet as an encoder
    (more specifically, the `''efficientnet-b5''` encoder), pretrained on `imagenet`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE344]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE345]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE346]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE347]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE348]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE349]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the Adam optimizer and the loss as the Dice loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE350]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE351]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE352]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE353]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a helper function, `compute_metrics`, that will help compute the
    IoU and F1-score (equivalent to the Dice coefficient):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE354]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE355]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE356]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE357]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE358]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE359]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE360]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE361]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE362]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE363]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a helper function, `epoch_step_unet`, that will compute forward propagation,
    backpropagation if needed, the loss function, and metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE364]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE365]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE366]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE367]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE368]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE369]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE370]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE371]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE372]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE373]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE374]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE375]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE376]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE377]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE378]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE379]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE380]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE381]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE382]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE383]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE384]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE385]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a `train_unet` function, allowing us to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE386]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE387]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE388]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE389]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE390]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE391]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE392]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE393]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE394]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE395]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE396]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE397]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE398]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE399]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE400]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE401]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE402]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE403]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE404]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE405]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE406]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE407]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE408]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE409]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE410]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE411]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE412]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE413]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE414]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE415]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE416]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE417]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE418]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE419]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE420]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE421]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE422]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE423]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE424]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE425]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE426]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE427]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE428]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train_unet` function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Trains the model on the train set, and compute the evaluation metrics (the IoU
    and F1-score)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluates the model on the test set with the evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a learning rate scheduler is provided, applies a step (see the *There’s more*
    subsection for more about this)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displays in the standard output the train and test losses and IoU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the train and test metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train the model for 50 epochs and store the output train and test metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE429]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE430]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE431]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the metrics for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE432]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE433]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE434]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE435]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE436]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE437]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE438]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE439]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE440]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE441]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE442]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE443]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE444]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE445]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE446]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE447]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE448]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE449]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.25 – The Dice loss (top), IoU (middle), and F1-score (bottom) as
    a function of the epoch for the train and test sets](img/B19629_10_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 – The Dice loss (top), IoU (middle), and F1-score (bottom) as a
    function of the epoch for the train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the IoU goes up to 87% on the test set and seems to reach a plateau
    after about 30 epochs. Also, the test set metrics are bumpy and unstable, which
    can be because of a learning rate that is too high, as well as a model too large.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now try to do the same with the freezing layer and gradually the decrease
    the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a pretrained model by freezing layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now train a pretrained model in two stages – first, we will freeze
    most of the layers of the model for 20 epochs, then only unfreeze all the layers,
    and train 30 more epochs to fine-tune the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define two helper functions to freeze and unfreeze layers. The
    `freeze_encoder` function will freeze all the layers of the encoder up to a given
    block level, provided by the `max_level` argument. If no `max_level` is given,
    all weights of the encoder will be frozen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE450]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE451]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE452]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE453]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE454]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE455]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE456]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE457]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE458]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE459]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE460]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE461]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a new model, which is the same as before, and print the number
    of trainable parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE462]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE463]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE464]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE465]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE466]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE467]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE468]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE469]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, this model is made of ~31.2 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Le’’s now freeze part of the encoder – the first three blocks, which are basically
    most of the weights of the encoder, as we will see – and print the number of trainable
    parameters left over:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE470]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE471]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE472]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  prefs: []
  type: TYPE_PRE
- en: We now have only ~3.9 million trainable parameters left. Almost 27.3 million
    parameters from the encoder are now frozen, out of ~28 million parameters in the
    encoder – the remaining parameters are from the decoder. This means we will mostly
    train the decoder first and use the pretrained encoder as a feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a new optimizer for training, as well as a scheduler. We will use
    an `ExponentialLR` scheduler here, with a gamma value of `0.95` – this means that
    at each epoch, the learning rate will be multiplied by 0.95:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE474]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE475]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE476]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE477]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model with frozen layers on 20 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE478]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE479]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE480]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE481]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, after 20 epochs only, the IoU on the test set already reaches
    88%, slightly higher than without freezing and without any learning rate decay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the decoder and last layers of the encoder are warmed up against this
    dataset, let’s unfreeze all the parameters before training for more epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE482]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE483]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE484]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the trainable parameters are now back at 31.2 million, meaning
    that all the parameters are trainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model on 30 more epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE485]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE486]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE487]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE488]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the results by concatenating the results with and without freezing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE489]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE490]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE491]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE492]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE493]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE494]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE495]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE496]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE497]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE498]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE499]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE500]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE501]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE502]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE503]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE504]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE505]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE506]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.26 – The Dice loss (top), IoU (middle), and F1-score (bottom) as
    a function of the epoch for train and test sets with fine-tuning – after a drop
    when unfreezing the weights, the metrics improve and are stable again](img/B19629_10_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.26 – The Dice loss (top), IoU (middle), and F1-score (bottom) as a
    function of the epoch for train and test sets with fine-tuning – after a drop
    when unfreezing the weights, the metrics improve and are stable again
  prefs: []
  type: TYPE_NORMAL
- en: We can see that as soon as we unfreeze all the parameters at epoch 20, the curves
    get a bit bumpy. However, after 10 more epochs at around epoch 30, the metrics
    become stable again.
  prefs: []
  type: TYPE_NORMAL
- en: After 50 epochs in total, the IoU reaches almost 90%, against only 87% earlier
    without the fine-tuning techniques (freezing and the learning rate decay).
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of curiosity, we can also plot the learning rate as a function of the epoch,
    to look at the decrease:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE507]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE508]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE509]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE510]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.27 – The learning rate value as a function of the epoch for a torch
    ExponentialLR class, with a gamma value of 0.95](img/B19629_10_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.27 – The learning rate value as a function of the epoch for a torch
    ExponentialLR class, with a gamma value of 0.95
  prefs: []
  type: TYPE_NORMAL
- en: As expected, after 50 epochs, the initial learning rate of 0.05 is divided by
    almost 13, down to roughly 0.0003, since ![](img/Formula_10_018.png).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several ways to compute the metrics such as the IoU or Dice coefficient
    in semantic segmentation. In this recipe, as implemented in the `compute_metrics`
    function, the `''micro''` option was chosen with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  prefs: []
  type: TYPE_PRE
- en: First, we can define the TP, FP, FN, and TN for each pixel just like in any
    other classification task. The metrics are then computed based on those values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on that, the most common computation methods are available and well summarized
    in the SMP documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''micro''`: Sum the TP, FP, FN, and TN pixels over all images and classes
    and then only compute the score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''macro''`: Sum the TP, FP, FN, and TN pixels over all images for each label,
    compute the score for each label, and then average over the labels. If there is
    an imbalanced class (which is usually the case in semantic segmentation), this
    method will not take it into account and should be avoided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''weighted''`: The same as `''macro''` but with a weighted average over the
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''micro-imagewise''`, `''macro-imagewise''`, and `''weighted-imagewise''`:
    The same as `''micro''`, `''macro''`, and `''weighted''`, respectively, but they
    compute the score for each image independently before averaging over the images.
    This can be useful when images in a dataset do not have the same dimensions, for
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, a `'micro'` or `'weighted'` approach works fine, but it’s
    always useful to understand the differences and to be able to play with them.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A paper proposing a review of several losses used in semantic segmentation:
    [https://arxiv.org/pdf/2006.14822.pdf](https://arxiv.org/pdf/2006.14822.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper proposing the U-Net architecture: [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GitHub repo of the SMP library: [https://github.com/qubvel/segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A link to the Kaggle dataset: [https://www.kaggle.com/datasets/santurini/semantic-segmentation-drone-dataset](https://www.kaggle.com/datasets/santurini/semantic-segmentation-drone-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
