<html><head></head><body>
<div id="_idContainer054">
<p lang="en-GB"><a id="_idTextAnchor015"/></p>
<h1 class="chapter-number" id="_idParaDest-15" lang="en-GB"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-16" lang="en-GB"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.2.1">Machine Learning – an Introduction</span></h1>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.3.1">Machine learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">ML</span></strong><span class="koboSpan" id="kobo.6.1">) techniques are being applied in a variety of fields, and data scientists</span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.7.1"> are being sought after in many different industries. </span><span class="koboSpan" id="kobo.7.2">With ML, we identify the processes through which we gain knowledge that is not readily apparent from data to make decisions. </span><span class="koboSpan" id="kobo.7.3">Applications of ML techniques may vary greatly and are found in disciplines as diverse as medicine, finance, </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">and advertising.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.9.1">In this chapter, we’ll present different ML approaches, techniques, and some of their applications to real-world problems, and we’ll also introduce one of the major open source packages available in Python for ML, PyTorch. </span><span class="koboSpan" id="kobo.9.2">This will lay the foundation for later chapters in which we’ll focus on a particular type of ML approach</span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.10.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">neural networks </span></strong><span class="koboSpan" id="kobo.12.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.13.1">NNs</span></strong><span class="koboSpan" id="kobo.14.1">). </span><span class="koboSpan" id="kobo.14.2">In particular, we will focus on </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">deep learning</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.17.1">DL</span></strong><span class="koboSpan" id="kobo.18.1">). </span><br/><span class="koboSpan" id="kobo.19.1">DL makes use of more advanced NNs than those used previously. </span><span class="koboSpan" id="kobo.19.2">This is not only a result of recent developments in the theory but also advancements in computer hardware. </span><span class="koboSpan" id="kobo.19.3">This chapter will summarize what ML is and what it can do, preparing you to better understand how DL differentiates itself from popular traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">ML techniques.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.21.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">main topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.23.1">Introduction </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">to ML</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.25.1">Different </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">ML approaches</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.27.1">Neural networks </span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.28.1">Introduction </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">to PyTorch</span></span></li>
</ul>
<h1 id="_idParaDest-17" lang="en-GB"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.30.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.31.1">We’ll implement the example in this chapter using Python and PyTorch. </span><span class="koboSpan" id="kobo.31.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.31.3">You can find the code examples in the book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01"><span class="No-Break"><span class="koboSpan" id="kobo.33.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter01</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.34.1">.</span></span></p>
<h1 id="_idParaDest-18" lang="en-GB"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.35.1">Introduction to ML</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.36.1">ML is often associated with terms such as </span><strong class="bold"><span class="koboSpan" id="kobo.37.1">big data</span></strong><span class="koboSpan" id="kobo.38.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.40.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.41.1">AI</span></strong><span class="koboSpan" id="kobo.42.1">). </span><span class="koboSpan" id="kobo.42.2">However, both are quite</span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.43.1"> different from ML. </span><span class="koboSpan" id="kobo.43.2">To understand</span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.44.1"> what ML is and</span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.45.1"> why it’s useful, it’s important to understand what big data is and how ML applies </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">to it.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.47.1">Big data is a term used to describe huge datasets that are created as the result of large increases in data that is gathered and stored. </span><span class="koboSpan" id="kobo.47.2">For example, this may be through cameras, sensors, or internet </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">social sites.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.49.1">How much data do we create daily?</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.50.1">It’s estimated</span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.51.1"> that Google alone processes over 20 petabytes of information per day, and this number is only going to increase. </span><span class="koboSpan" id="kobo.51.2">A few years ago, Forbes estimated that every day, 2.5 quintillion bytes of data are created and that 90% of all the data in the world has been created in the last </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">two years.</span></span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.53.1">(</span><a href="https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/"><span class="No-Break"><span class="koboSpan" id="kobo.54.1">https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.55.1">)</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.56.1">Humans alone are unable to grasp, let alone analyze, such huge amounts of data, and ML techniques are used to make sense of these very large datasets. </span><span class="koboSpan" id="kobo.56.2">ML is the tool that’s used for large-scale data processing. </span><span class="koboSpan" id="kobo.56.3">It is well suited to complex datasets that have huge numbers of variables and features. </span><span class="koboSpan" id="kobo.56.4">One of the strengths of many ML techniques, and DL in particular, is that they perform best when used on large datasets, thus improving their analytic and predictive power. </span><span class="koboSpan" id="kobo.56.5">In other words, ML techniques, and DL NNs in particular, learn best when they can access large datasets where they can discover patterns and regularities hidden in </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">the data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.58.1">On the other hand, ML’s predictive ability can be successfully adapted to AI systems. </span><span class="koboSpan" id="kobo.58.2">ML can be thought of as the brain of an AI system. </span><span class="koboSpan" id="kobo.58.3">AI can be defined (though this definition may not be unique) as a system that can interact with its environment. </span><span class="koboSpan" id="kobo.58.4">Also, AI machines are endowed with sensors that enable them to know the environment they are in and tools with which they can relate to the environment. </span><span class="koboSpan" id="kobo.58.5">Therefore, ML is the brain that allows the machine to analyze the data ingested through its sensors to formulate an appropriate answer. </span><span class="koboSpan" id="kobo.58.6">A simple example is Siri on an iPhone. </span><span class="koboSpan" id="kobo.58.7">Siri hears the command through its microphone and outputs an answer through its speakers or its display, but to do so, it needs to understand what it’s being told. </span><span class="koboSpan" id="kobo.58.8">Similarly, driverless cars will be equipped with cameras, GPS systems, sonars, and LiDAR, but all this information needs to be processed to provide a correct answer. </span><span class="koboSpan" id="kobo.58.9">This may include whether to accelerate, brake, or turn. </span><span class="koboSpan" id="kobo.58.10">ML is the information-processing method that leads to </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">the answer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.60.1">We’ve explained</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.61.1"> what ML is, but what about DL? </span><span class="koboSpan" id="kobo.61.2">For now, let’s just say that DL is a subfield of ML. </span><span class="koboSpan" id="kobo.61.3">DL methods share some special common features. </span><span class="koboSpan" id="kobo.61.4">The most popular representatives of such methods are </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">deep NNs.</span></span></p>
<h1 id="_idParaDest-19" lang="en-GB"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.63.1">Different ML approaches</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.64.1">As we have seen, the term ML</span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.65.1"> is used in a very general way and refers to the general techniques that are used to extrapolate patterns from large sets, or it is the ability to make predictions on new data based on what is learned by analyzing available known data. </span><span class="koboSpan" id="kobo.65.2">ML techniques can roughly be divided into two core classes, while one more class is often added. </span><span class="koboSpan" id="kobo.65.3">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">the classes:</span></span></p>
<ul>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.67.1">Supervised learning</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.68.1">Unsupervised learning</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.69.1">Reinforcement learning</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.70.1">Let’s take a </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">closer look.</span></span></p>
<h2 id="_idParaDest-20" lang="en-GB"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.72.1">Supervised learning</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.73.1">Supervised learning</span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.74.1"> algorithms are a class of ML algorithms</span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.75.1"> that use previously labeled data to learn its features, so they can classify similar but unlabeled data. </span><span class="koboSpan" id="kobo.75.2">Let’s use an example to understand this </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">concept better.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.77.1">Let’s assume that a user receives many emails every day, some of which are important business emails and some</span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.78.1"> of which are unsolicited junk emails, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">spam</span></strong><span class="koboSpan" id="kobo.80.1">. </span><span class="koboSpan" id="kobo.80.2">A supervised machine algorithm will be presented with a large body of emails that have already been labeled by a teacher as spam or not spam (this is called </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">training data</span></strong><span class="koboSpan" id="kobo.82.1">). </span><span class="koboSpan" id="kobo.82.2">For each sample, the machine will try to predict</span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.83.1"> whether the email is spam or not, and it will compare the prediction with the original target label. </span><span class="koboSpan" id="kobo.83.2">If the prediction differs from the target, the machine will adjust its internal parameters in such a way that the next time it encounters this sample, it will classify it correctly. </span><span class="koboSpan" id="kobo.83.3">Conversely, if the prediction is correct, the parameters will stay the same. </span><span class="koboSpan" id="kobo.83.4">The more training data we feed to the algorithm, the better it becomes (this rule has caveats, as we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">see next).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.85.1">In the example we used, the emails had only two classes (spam or not spam), but the same principles apply to tasks with arbitrary numbers of classes (or categories). </span><span class="koboSpan" id="kobo.85.2">For example, Gmail, the free email service by Google, allows the user to select up to five categories, which are labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.87.1">Primary</span></strong><span class="koboSpan" id="kobo.88.1">: Includes </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">person-to-person</span></span><span class="No-Break"><a id="_idIndexMarker012"/></span><span class="No-Break"><span class="koboSpan" id="kobo.90.1"> conversations</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.91.1">Promotions</span></strong><span class="koboSpan" id="kobo.92.1">: Includes marketing emails, offers, </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">and discounts</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.94.1">Social</span></strong><span class="koboSpan" id="kobo.95.1">: Includes messages from social networks and </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">media-sharing sites</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.97.1">Updates</span></strong><span class="koboSpan" id="kobo.98.1">: Includes bills, bank statements, </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">and receipts</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.100.1">Forums</span></strong><span class="koboSpan" id="kobo.101.1">: Includes messages from online groups and </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">mailing lists</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.103.1">To summarize, the ML task, which maps</span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.104.1"> a set of input values to a finite number of classes, is </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.106.1">classification</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.108.1">In some cases, the outcome may not necessarily be discrete, and we may not have a finite number of classes to classify our data into. </span><span class="koboSpan" id="kobo.108.2">For example, we may try to predict the life expectancy of a group of people based on their predetermined health parameters. </span><span class="koboSpan" id="kobo.108.3">In this case, the outcome</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.109.1"> is a numerical value, and we don’t talk about classification but </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">rather </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.111.1">regression</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.113.1">One way to think of supervised learning</span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.114.1"> is to imagine we are building a function, </span><em class="italic"><span class="koboSpan" id="kobo.115.1">f</span></em><span class="koboSpan" id="kobo.116.1">, defined over a dataset, which</span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.117.1"> comprises information organized by </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">features</span></strong><span class="koboSpan" id="kobo.119.1">. </span><span class="koboSpan" id="kobo.119.2">In the case of email classification, the features can be specific words that may appear more frequently than others in spam emails. </span><span class="koboSpan" id="kobo.119.3">The use of explicit sex-related words will most likely identify a spam email rather than a business/work email. </span><span class="koboSpan" id="kobo.119.4">On the contrary, words such as </span><em class="italic"><span class="koboSpan" id="kobo.120.1">meeting</span></em><span class="koboSpan" id="kobo.121.1">, </span><em class="italic"><span class="koboSpan" id="kobo.122.1">business</span></em><span class="koboSpan" id="kobo.123.1">, or </span><em class="italic"><span class="koboSpan" id="kobo.124.1">presentation</span></em><span class="koboSpan" id="kobo.125.1"> are more likely to describe a work email. </span><span class="koboSpan" id="kobo.125.2">If we have access to metadata, we may also use the sender’s information as a feature. </span><span class="koboSpan" id="kobo.125.3">Each email will then have an associated set of features, and each feature will have a value (in this case, how many times the specific word is present in the email’s body). </span><span class="koboSpan" id="kobo.125.4">The ML algorithm will then seek to map those values to a discrete range that represents the set of classes, or a real value in the case of regression. </span><span class="koboSpan" id="kobo.125.5">The definition of the </span><em class="italic"><span class="koboSpan" id="kobo.126.1">f</span></em><span class="koboSpan" id="kobo.127.1"> function is </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.129.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;mtext&gt;space&lt;/mtext&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mtext&gt;features&lt;/mtext&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mtext&gt;classes&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mtext&gt;(discrete&lt;/mtext&gt;&lt;mtext&gt;values&lt;/mtext&gt;&lt;mtext&gt;or&lt;/mtext&gt;&lt;mtext&gt;real&lt;/mtext&gt;&lt;mtext&gt;values)&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/1.png" style="vertical-align:-0.257em;height:0.968em;width:24.400em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.130.1">In later chapters, we’ll see several examples</span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.131.1"> of either classification</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.132.1"> or regression problems. </span><span class="koboSpan" id="kobo.132.2">One such problem we’ll discuss is classifying handwritten digits of the </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">Modified National Institute of Standards and Technology</span></strong><span class="koboSpan" id="kobo.134.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.135.1">MNIST</span></strong><span class="koboSpan" id="kobo.136.1">) database (</span><a href="http://yann.lecun.com/exdb/mnist/"><span class="koboSpan" id="kobo.137.1">http://yann.lecun.com/exdb/mnist/</span></a><span class="koboSpan" id="kobo.138.1">). </span><span class="koboSpan" id="kobo.138.2">When given a set of images representing 0 to 9, the ML algorithm</span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.139.1"> will try to classify each image in one of the 10 classes, wherein each class corresponds to one of the 10 digits. </span><span class="koboSpan" id="kobo.139.2">Each image is 28×28 (= 784) pixels in size. </span><span class="koboSpan" id="kobo.139.3">If we think of each pixel as one feature, then the algorithm will use a 784-dimensional feature space to classify </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">the digits.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.141.1">The following figure depicts the handwritten digits from the </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">MNIST dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<span class="koboSpan" id="kobo.143.1"><img alt="Figure 1.1 – An example of handwritten digits from the MNIST dataset" src="image/B19627_01_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.144.1">Figure 1.1 – An example of handwritten digits from the MNIST dataset</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.145.1">In the next sections, we’ll talk about some of the most popular classical supervised algorithms. </span><span class="koboSpan" id="kobo.145.2">The following</span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.146.1"> is by no means an exhaustive list or a thorough description</span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.147.1"> of each ML method. </span><span class="koboSpan" id="kobo.147.2">We recommend referring to the book </span><em class="italic"><span class="koboSpan" id="kobo.148.1">Python Machine Learning</span></em><span class="koboSpan" id="kobo.149.1">, by Sebastian Raschka (</span><a href="https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750"><span class="koboSpan" id="kobo.150.1">https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750</span></a><span class="koboSpan" id="kobo.151.1">). </span><span class="koboSpan" id="kobo.151.2">It’s a simple review meant to provide you with a flavor of the different ML techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">in Python.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.153.1">Linear and logistic regression</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.154.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">regression</span></strong><span class="koboSpan" id="kobo.156.1"> algorithm is a type</span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.157.1"> of supervised</span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.158.1"> algorithm that uses features</span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.159.1"> of the input data to predict a numeric value, such as the cost of a house, given certain features, such as size, age, number of bathrooms, number of floors, and location. </span><span class="koboSpan" id="kobo.159.2">Regression analysis tries to find the value of the parameters for the function that best fits an </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">input dataset.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.161.1">In a </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">linear regression</span></strong><span class="koboSpan" id="kobo.163.1"> algorithm, the goal is to minimize a </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">cost function</span></strong><span class="koboSpan" id="kobo.165.1"> by finding appropriate parameters</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.166.1"> for the function over the input data that best approximates the target values. </span><span class="koboSpan" id="kobo.166.2">A cost function</span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.167.1"> is a function of the error – that is, how far we are from getting</span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.168.1"> a correct result. </span><span class="koboSpan" id="kobo.168.2">A popular cost function is the </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">mean squared error</span></strong><span class="koboSpan" id="kobo.170.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.171.1">MSE</span></strong><span class="koboSpan" id="kobo.172.1">), where we take the square of the difference between the expected value and the predicted result. </span><span class="koboSpan" id="kobo.172.2">The sum of all the input examples gives us the error of the algorithm and represents the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">cost function.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.174.1">Say we have a 100-square-meter house that was built 25 years ago with three bathrooms and two floors. </span><span class="koboSpan" id="kobo.174.2">Let’s also assume that the city is divided into 10 different neighborhoods, which we’ll denote with integers from 1 to 10, and say this house is located in the area denoted by 7. </span><span class="koboSpan" id="kobo.174.3">We can parameterize this house with a five-dimensional vector, </span><span class="koboSpan" id="kobo.175.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;100,25,3&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mn&gt;2,7&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/2.png" style="vertical-align:-0.390em;height:1.074em;width:15.417em"/></span><span class="koboSpan" id="kobo.176.1">. </span><span class="koboSpan" id="kobo.176.2">Say that we also know that this house has an estimated value of $100,000 (in today’s world, this would be enough for just a tiny shack near the North Pole, but let’s pretend). </span><span class="koboSpan" id="kobo.176.3">What we want is to create a function, </span><em class="italic"><span class="koboSpan" id="kobo.177.1">f</span></em><span class="koboSpan" id="kobo.178.1">, such </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">that </span></span><span class="No-Break"><span class="koboSpan" id="kobo.180.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;100000&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/3.png" style="vertical-align:-0.257em;height:0.968em;width:5.864em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.182.1">A note of encouragement</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.183.1">Don’t worry If you don’t fully understand some of the terms in this section. </span><span class="koboSpan" id="kobo.183.2">We’ll discuss vectors, cost functions, linear regression, and gradient descent in more detail in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.184.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.185.1">. </span><span class="koboSpan" id="kobo.185.2">We will also see that training NNs and linear/logistic regressions have a lot in common. </span><span class="koboSpan" id="kobo.185.3">For now, you can think of a vector as an array. </span><span class="koboSpan" id="kobo.185.4">We’ll denote vectors with boldface font – for example, </span><span class="koboSpan" id="kobo.186.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/4.png" style="vertical-align:-0.000em;height:0.441em;width:0.496em"/></span><span class="koboSpan" id="kobo.187.1">. </span><span class="koboSpan" id="kobo.187.2">We’ll denote the vector elements with italic font and subscript – for </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">example, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.189.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/5.png" style="vertical-align:-0.340em;height:0.788em;width:0.624em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.191.1">In linear</span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.192.1"> regression, this means</span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.193.1"> finding a vector of weights, </span><span class="koboSpan" id="kobo.194.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/6.png" style="vertical-align:-0.390em;height:0.887em;width:9.011em"/></span><span class="koboSpan" id="kobo.195.1">, such that the dot product of the vectors, </span><span class="koboSpan" id="kobo.196.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;100000&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/7.png" style="vertical-align:-0.012em;height:0.646em;width:6.426em"/></span><span class="koboSpan" id="kobo.197.1">, would be </span><span class="koboSpan" id="kobo.198.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mn&gt;100&lt;/mml:mn&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;25&lt;/mml:mn&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;7&lt;/mml:mn&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;100000&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/8.png" style="vertical-align:-0.340em;height:0.974em;width:18.202em"/></span><span class="koboSpan" id="kobo.199.1"> or </span><span class="koboSpan" id="kobo.200.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;100000&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/9.png" style="vertical-align:-0.343em;height:1.105em;width:7.330em"/></span><span class="koboSpan" id="kobo.201.1">. </span><span class="koboSpan" id="kobo.201.2">If we had 1,000 houses, we could repeat</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.202.1"> the same process for every house, and ideally, we would</span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.203.1"> like to find a single vector, </span><strong class="bold"><span class="koboSpan" id="kobo.204.1">w</span></strong><span class="koboSpan" id="kobo.205.1">, that can predict the correct value that is close enough for every house. </span><span class="koboSpan" id="kobo.205.2">The most common way to train a linear regression model can be seen in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">pseudocode block:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.207.1">
Initialize the vector w with some random values
repeat:
  E = 0 # initialize the cost function E with 0
  for every pair </span><span class="koboSpan" id="kobo.208.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/10.png" style="vertical-align:-0.179em;height:0.972em;width:4.815em"/></span><span class="koboSpan" id="kobo.209.1"> of the traini</span><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.210.1">ng set:
      </span><span class="koboSpan" id="kobo.211.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/11.png" style="vertical-align:-0.066em;height:1.017em;width:10.121em"/></span><span class="koboSpan" id="kobo.212.1"> # here </span><span class="koboSpan" id="kobo.213.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/12.png" style="vertical-align:-0.016em;height:0.759em;width:1.724em"/></span><span class="koboSpan" id="kobo.214.1"> is the real house price
  </span><span class="koboSpan" id="kobo.215.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/13.png" style="vertical-align:-0.311em;height:0.928em;width:11.647em"/></span><span class="koboSpan" id="kobo.216.1"> # Mean Square Error
  use </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">gradient descent</span></strong><span class="koboSpan" id="kobo.218.1"> to update the weights </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">w</span></strong><span class="koboSpan" id="kobo.220.1"> based on MSE until MSE falls below threshold</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.221.1">First, we iterate over the training data</span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.222.1"> to compute the cost function, MSE. </span><span class="koboSpan" id="kobo.222.2">Once we know the value of MSE, we’ll use the </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">gradient descent</span></strong><span class="koboSpan" id="kobo.224.1"> algorithm to update the weights of the vector, </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">w</span></strong><span class="koboSpan" id="kobo.226.1">. </span><span class="koboSpan" id="kobo.226.2">To do this, we’ll calculate the derivatives of the cost function concerning each weight, </span><span class="koboSpan" id="kobo.227.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/14.png" style="vertical-align:-0.340em;height:0.788em;width:0.818em"/></span><span class="koboSpan" id="kobo.228.1">. </span><span class="koboSpan" id="kobo.228.2">In this way, we’ll know how the cost function changes (increase or decrease) concerning </span><span class="koboSpan" id="kobo.229.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/15.png" style="vertical-align:-0.340em;height:0.788em;width:0.823em"/></span><span class="koboSpan" id="kobo.230.1">. </span><span class="koboSpan" id="kobo.230.2">Then we’ll update that weight’s </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">value accordingly.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.232.1">Previously, we demonstrated how to solve a regression problem with linear regression. </span><span class="koboSpan" id="kobo.232.2">Now, let’s take a classification task: trying to determine whether a house is overvalued or undervalued. </span><span class="koboSpan" id="kobo.232.3">In this case, the target data would be categorical [1, 0] – 1 for overvalued and 0 for undervalued. </span><span class="koboSpan" id="kobo.232.4">The price of the house will be an input parameter instead of the target value</span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.233.1"> as before. </span><span class="koboSpan" id="kobo.233.2">To solve the task, we’ll use logistic regression. </span><span class="koboSpan" id="kobo.233.3">This is similar to linear regression</span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.234.1"> but with one difference: in linear regression, the output is </span><span class="koboSpan" id="kobo.235.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/16.png" style="vertical-align:-0.011em;height:0.452em;width:1.820em"/></span><span class="koboSpan" id="kobo.236.1">. </span><span class="koboSpan" id="kobo.236.2">However, here, the output will be a special logistic function (</span><a href="https://en.wikipedia.org/wiki/Logistic_function"><span class="koboSpan" id="kobo.237.1">https://en.wikipedia.org/wiki/Logistic_function</span></a><span class="koboSpan" id="kobo.238.1">), </span><span class="koboSpan" id="kobo.239.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/17.png" style="vertical-align:-0.137em;height:0.702em;width:3.239em"/></span><span class="koboSpan" id="kobo.240.1">. </span><span class="koboSpan" id="kobo.240.2">This will squash the value of </span><span class="koboSpan" id="kobo.241.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/18.png" style="vertical-align:-0.011em;height:0.452em;width:2.026em"/></span><span class="koboSpan" id="kobo.242.1"> in the (0:1) interval. </span><span class="koboSpan" id="kobo.242.2">You can think of the logistic function as a probability, and the closer the result is to 1, the more chance there is that the house is overvalued, and vice versa. </span><span class="koboSpan" id="kobo.242.3">Training is the same as with linear regression, but the output of the function is in the (0:1) interval, and the labels are either 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">or 1.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.244.1">Logistic regression is not a classification</span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.245.1"> algorithm, but we can</span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.246.1"> turn it into one. </span><span class="koboSpan" id="kobo.246.2">We just have to introduce a rule that determines the class based on the logistic function’s output. </span><span class="koboSpan" id="kobo.246.3">For example, we can say that a house is overvalued if the value of </span><span class="koboSpan" id="kobo.247.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/19.png" style="vertical-align:-0.137em;height:0.771em;width:5.818em"/></span><span class="koboSpan" id="kobo.248.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">undervalued otherwise.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.250.1">Multivariate regression</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.251.1">The regression</span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.252.1"> examples in this section have a single numerical output. </span><span class="koboSpan" id="kobo.252.2">A regression analysis can have more than one output. </span><span class="koboSpan" id="kobo.252.3">We’ll refer to such analysis as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.253.1">multivariate regression</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.255.1">Support vector machines</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.256.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.257.1">support vector machine</span></strong><span class="koboSpan" id="kobo.258.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.259.1">SVM</span></strong><span class="koboSpan" id="kobo.260.1">) is a supervised ML algorithm that’s mainly used for classification. </span><span class="koboSpan" id="kobo.260.2">It is the most </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.261.1">popular member of the kernel method</span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.262.1"> class of algorithms. </span><span class="koboSpan" id="kobo.262.2">An SVM tries to find a hyperplane, which separates the samples in </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">the dataset.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.264.1">Hyperplanes</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.265.1">A hyperplane</span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.266.1"> is a plane in a high-dimensional space. </span><span class="koboSpan" id="kobo.266.2">For example, a hyperplane in a one-dimensional space is a point, and in a two-dimensional space, it would just be a line. </span><span class="koboSpan" id="kobo.266.3">In three-dimensional space, the hyperplane would be a plane, and we can’t visualize the hyperplane in four-dimensional space, but we know that </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">it exists.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.268.1">We can think of classification</span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.269.1"> as the process of trying to find a hyperplane that will separate different groups of data points. </span><span class="koboSpan" id="kobo.269.2">Once we have defined our features, every sample (in our case, an email) in the dataset can be thought of as a point in the multidimensional space of features. </span><span class="koboSpan" id="kobo.269.3">One dimension of that space represents all the possible values of one feature. </span><span class="koboSpan" id="kobo.269.4">The coordinates of a point (a sample) are the specific values of each feature for that sample. </span><span class="koboSpan" id="kobo.269.5">The ML algorithm task will be to draw a hyperplane to separate points with different classes. </span><span class="koboSpan" id="kobo.269.6">In our case, the hyperplane would separate spam from </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">non-spam emails.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.271.1">In the following diagram, at the top and bottom, we can see two classes of points (red and blue) that are in a two-dimensional feature space (the </span><em class="italic"><span class="koboSpan" id="kobo.272.1">x</span></em><span class="koboSpan" id="kobo.273.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.274.1">y</span></em><span class="koboSpan" id="kobo.275.1"> axes). </span><span class="koboSpan" id="kobo.275.2">If both the </span><em class="italic"><span class="koboSpan" id="kobo.276.1">x</span></em><span class="koboSpan" id="kobo.277.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.278.1">y</span></em><span class="koboSpan" id="kobo.279.1"> values of a point are below 5, then the point is blue. </span><span class="koboSpan" id="kobo.279.2">In all other cases, the point is red. </span><span class="koboSpan" id="kobo.279.3">In this case, the classes are </span><strong class="bold"><span class="koboSpan" id="kobo.280.1">linearly separable</span></strong><span class="koboSpan" id="kobo.281.1">, meaning we can separate them with a hyperplane. </span><span class="koboSpan" id="kobo.281.2">Conversely, the classes</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.282.1"> in the image on the right are </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">linearly inseparable:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.284.1"><img alt="Figure 1.2 – A linearly separable set of points (left) and a linearly inseparable set (right)" src="image/B19627_01_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.285.1">Figure 1.2 – A linearly separable set of points (left) and a linearly inseparable set (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.286.1">The SVM tries to find a hyperplane that maximizes the distance between itself and the points. </span><span class="koboSpan" id="kobo.286.2">In other words, from all possible hyperplanes that can separate the samples, the SVM finds the one that has the maximum distance from all points. </span><span class="koboSpan" id="kobo.286.3">In addition, SVMs can deal with data that is not linearly</span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.287.1"> separable. </span><span class="koboSpan" id="kobo.287.2">There are two methods for this: introducing soft margins or using the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.288.1">kernel trick</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.290.1">Soft margins work by allowing a few misclassified elements while retaining the most predictive ability of the algorithm. </span><span class="koboSpan" id="kobo.290.2">In practice, it’s better not to overfit the ML model, and we could do so by relaxing some of the </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">SVM hypotheses.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.292.1">The kernel trick solves the same problem differently. </span><span class="koboSpan" id="kobo.292.2">Imagine that we have a two-dimensional feature space, but the classes are linearly inseparable. </span><span class="koboSpan" id="kobo.292.3">The kernel trick uses a kernel function that transforms the data by adding more dimensions to it. </span><span class="koboSpan" id="kobo.292.4">In our case, after the transformation, the data will be three-dimensional. </span><span class="koboSpan" id="kobo.292.5">The linearly inseparable classes in the two-dimensional space will become linearly separable in the three dimensions and our problem </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">is solved:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.294.1"><img alt="Figure 1.3 – A non-linearl﻿y separable set before the kernel was applied (left) and the same dataset after the kernel has been applied, and the data can be linearly separated (right)" src="image/B19627_01_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.295.1">Figure 1.3 – A non-linearly separable set before the kernel was applied (left) and the same dataset after the kernel has been applied, and the data can be linearly separated (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.296.1">Lets move</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.297.1"> to the last one in our list, </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">decision</span></span><span class="No-Break"><a id="_idIndexMarker045"/></span><span class="No-Break"><span class="koboSpan" id="kobo.299.1"> trees.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.300.1">Decision trees</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.301.1">Another popular</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.302.1"> supervised algorithm</span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.303.1"> is the decision tree, which creates a classifier in the form of a tree. </span><span class="koboSpan" id="kobo.303.2">It is composed of decision nodes, where tests on specific attributes are performed, and leaf nodes, which indicate the value of the target attribute. </span><span class="koboSpan" id="kobo.303.3">To classify a new sample, we start at the root of the tree and navigate down the nodes until we reach </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">a leaf.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.305.1">A classic application of this algorithm is the Iris flower dataset (</span><a href="http://archive.ics.uci.edu/ml/datasets/Iris"><span class="koboSpan" id="kobo.306.1">http://archive.ics.uci.edu/ml/datasets/Iris</span></a><span class="koboSpan" id="kobo.307.1">), which contains data from 50 samples of three types of irises </span><br/><span class="koboSpan" id="kobo.308.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.309.1">Iris Setosa</span></strong><span class="koboSpan" id="kobo.310.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">Iris Virginica</span></strong><span class="koboSpan" id="kobo.312.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">Iris Versicolor</span></strong><span class="koboSpan" id="kobo.314.1">). </span><span class="koboSpan" id="kobo.314.2">Ronald Fisher, who created the dataset, measured four different features of </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">these flowers:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.316.1">The length of </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">their sepals</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.318.1">The width of </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">their sepals</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.320.1">The length of </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">their petals</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.322.1">The width of </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">their petals</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.324.1">Based on the different combinations</span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.325.1"> of these features, it’s possible to create a decision tree</span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.326.1"> to decide which species each flower belongs to. </span><span class="koboSpan" id="kobo.326.2">In the following diagram, we have defined a decision tree that will correctly classify almost all the flowers using only two of these features, the petal length </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">and width:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.328.1"><img alt="Figure 1.4 – A decision tree for classifying the Iris dataset" src="image/B19627_01_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.329.1">Figure 1.4 – A decision tree for classifying the Iris dataset</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.330.1">To classify a new sample, we start at the root note of the tree (petal length). </span><span class="koboSpan" id="kobo.330.2">If the sample satisfies the condition, we go left to the leaf, representing the Iris Setosa class. </span><span class="koboSpan" id="kobo.330.3">If not, we go right to a new node (petal width). </span><span class="koboSpan" id="kobo.330.4">This process continues until we reach </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">a leaf.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.332.1">In recent years, decision</span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.333.1"> trees have seen two major improvements. </span><span class="koboSpan" id="kobo.333.2">The first is </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">random forests</span></strong><span class="koboSpan" id="kobo.335.1">, which is an ensemble method that combines the predictions of multiple trees. </span><span class="koboSpan" id="kobo.335.2">The second is a class of algorithms called </span><strong class="bold"><span class="koboSpan" id="kobo.336.1">gradient boosting</span></strong><span class="koboSpan" id="kobo.337.1">, which creates multiple sequential decision</span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.338.1"> trees, where each tree tries to improve the errors made by the previous tree. </span><span class="koboSpan" id="kobo.338.2">Thanks to these improvements, decision trees have become very popular when working with certain types</span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.339.1"> of data. </span><span class="koboSpan" id="kobo.339.2">For example, they are one of the most popular algorithms used</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.340.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">Kaggle competitions.</span></span></p>
<h2 id="_idParaDest-21" lang="en-GB"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.342.1">Unsupervised learning</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.343.1">The second class</span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.344.1"> of ML algorithms is unsupervised learning. </span><span class="koboSpan" id="kobo.344.2">Here, we don’t label the data beforehand; instead, we let the algorithm come to its conclusion. </span><span class="koboSpan" id="kobo.344.3">One of the advantages of unsupervised learning algorithms over supervised ones is that we don’t need labeled data. </span><span class="koboSpan" id="kobo.344.4">Producing labels for supervised algorithms can be costly and slow. </span><span class="koboSpan" id="kobo.344.5">One way to solve this issue is to modify the supervised algorithm so that it uses less labeled data; there are different techniques for this. </span><span class="koboSpan" id="kobo.344.6">But another approach is to use an algorithm, which doesn’t need labels in the first</span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.345.1"> place. </span><span class="koboSpan" id="kobo.345.2">In this section, we’ll discuss some of these </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">unsupervised algorithms.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.347.1">Clustering</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.348.1">One of the most</span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.349.1"> common, and perhaps</span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.350.1"> simplest, examples of unsupervised learning is clustering. </span><span class="koboSpan" id="kobo.350.2">This is a technique that attempts to separate the data </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">into subsets.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.352.1">To illustrate this, let’s view the spam-or-not-spam email classification as an unsupervised learning problem. </span><span class="koboSpan" id="kobo.352.2">In the supervised case, for each email, we had a set of features and a label (spam or not spam). </span><span class="koboSpan" id="kobo.352.3">Here, we’ll use the same set of features, but the emails will not be labeled. </span><span class="koboSpan" id="kobo.352.4">Instead, we’ll ask the algorithm, when given the set of features, to put each sample in one of two separate groups (or clusters). </span><span class="koboSpan" id="kobo.352.5">Then, the algorithm will try to combine the samples in such a way that the intraclass similarity (which is the similarity between samples in the same cluster) is high and the similarity between different clusters is low. </span><span class="koboSpan" id="kobo.352.6">Different clustering algorithms use different metrics to measure similarity. </span><span class="koboSpan" id="kobo.352.7">For some more advanced algorithms, you don’t have to specify the number </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">of clusters.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.354.1">The following graph shows</span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.355.1"> how a set of points can be classified to form </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">three subsets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.357.1"><img alt="Figure 1.5 – Clustering a set of points in three subsets" src="image/B19627_01_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.358.1">Figure 1.5 – Clustering a set of points in three subsets</span></p>
<h4 lang="en-GB"><span class="koboSpan" id="kobo.359.1">K-means</span></h4>
<p lang="en-GB"><span class="koboSpan" id="kobo.360.1">K-means is a clustering algorithm</span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.361.1"> that groups the elements of a dataset into </span><em class="italic"><span class="koboSpan" id="kobo.362.1">k</span></em><span class="koboSpan" id="kobo.363.1"> distinct</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.364.1"> clusters (hence the </span><em class="italic"><span class="koboSpan" id="kobo.365.1">k</span></em><span class="koboSpan" id="kobo.366.1"> in the name). </span><span class="koboSpan" id="kobo.366.2">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">it works:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.368.1">Choose </span><em class="italic"><span class="koboSpan" id="kobo.369.1">k</span></em><span class="koboSpan" id="kobo.370.1"> random points, called </span><strong class="bold"><span class="koboSpan" id="kobo.371.1">centroids</span></strong><span class="koboSpan" id="kobo.372.1">, from the feature space, which will represent</span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.373.1"> the center of each of the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.374.1">k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1"> clusters.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.376.1">Assign each sample of the dataset (that is, each point in the feature space) to the cluster with the </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">closest centroid.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.378.1">For each cluster, we recomputed new centroids by taking the mean values of all the points in </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">the cluster.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.380.1">With the new centroids, we repeat </span><em class="italic"><span class="koboSpan" id="kobo.381.1">Steps 2</span></em><span class="koboSpan" id="kobo.382.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.383.1">3</span></em><span class="koboSpan" id="kobo.384.1"> until the stopping criteria </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">are met.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.386.1">The preceding method is sensitive to the initial choice of random centroids, and it may be a good idea to repeat it with different initial choices. </span><span class="koboSpan" id="kobo.386.2">It’s also possible for some centroids to not be close to any of the points in the dataset, reducing the number of clusters down from </span><em class="italic"><span class="koboSpan" id="kobo.387.1">k</span></em><span class="koboSpan" id="kobo.388.1">. </span><span class="koboSpan" id="kobo.388.2">Finally, it’s worth mentioning that if we used k-means with </span><em class="italic"><span class="koboSpan" id="kobo.389.1">k=3</span></em><span class="koboSpan" id="kobo.390.1"> on the Iris dataset, we may get different distributions of the samples compared to the distribution of the decision tree that we’d introduced. </span><span class="koboSpan" id="kobo.390.2">Once more, this highlights how important it is to carefully choose and use the correct ML method for </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">each problem.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.392.1">Now, let’s discuss a practical example that uses k-means clustering. </span><span class="koboSpan" id="kobo.392.2">Let’s say a pizza delivery place wants to open four new franchises in a city, and they need to choose the locations for the sites. </span><span class="koboSpan" id="kobo.392.3">We can solve this problem </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">with k-means:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.394.1">Find the locations where pizza is ordered most often; these will be our </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">data points.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.396.1">Choose four random points where the sites will </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">be located.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.398.1">By using k-means clustering, we can identify the four best locations that minimize the distance</span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.399.1"> to each </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">delivery</span></span><span class="No-Break"><a id="_idIndexMarker063"/></span><span class="No-Break"><span class="koboSpan" id="kobo.401.1"> place:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.402.1"><img alt="Figure 1.6 – The distribution of points where pizza is delivered most often (left); the round points indicate where the new franchises should be located and their corresponding delivery areas (right)" src="image/B19627_01_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.403.1">Figure 1.6 – The distribution of points where pizza is delivered most often (left); the round points indicate where the new franchises should be located and their corresponding delivery areas (right)</span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.404.1">Self-supervised learning</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.405.1">Self-supervised learning</span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.406.1"> refers to a combination</span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.407.1"> of problems and datasets, which allow us to </span><em class="italic"><span class="koboSpan" id="kobo.408.1">automatically generate</span></em><span class="koboSpan" id="kobo.409.1"> (that is, without human intervention) labeled data from the dataset. </span><span class="koboSpan" id="kobo.409.2">Once we have these labels, we can train a supervised algorithm to solve our task. </span><span class="koboSpan" id="kobo.409.3">To understand this concept better, let’s discuss some </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">use cases:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.411.1">Time series forecasting</span></strong><span class="koboSpan" id="kobo.412.1">: Imagine that we have to predict the future value of a time series</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.413.1"> based on its most recent historical values. </span><span class="koboSpan" id="kobo.413.2">Examples of this include stock (and nowadays crypto) price prediction and weather forecasting. </span><span class="koboSpan" id="kobo.413.3">To generate a labeled data sample, let’s take a window with length </span><em class="italic"><span class="koboSpan" id="kobo.414.1">k</span></em><span class="koboSpan" id="kobo.415.1"> of the historical data that ends at past moment </span><em class="italic"><span class="koboSpan" id="kobo.416.1">t</span></em><span class="koboSpan" id="kobo.417.1">. </span><span class="koboSpan" id="kobo.417.2">We’ll take the historical values in the range </span><em class="italic"><span class="koboSpan" id="kobo.418.1">[t – k; t]</span></em><span class="koboSpan" id="kobo.419.1"> and we’ll use them as input for the supervised algorithm. </span><span class="koboSpan" id="kobo.419.2">We’ll also take the historical value at moment </span><em class="italic"><span class="koboSpan" id="kobo.420.1">t + 1</span></em><span class="koboSpan" id="kobo.421.1"> and we’ll use it as the label for the given input sample. </span><span class="koboSpan" id="kobo.421.2">We can apply this division to the rest of the historical values and generate a labeled training </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">dataset automatically.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.423.1">Natural language processing</span></strong><span class="koboSpan" id="kobo.424.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.425.1">NLP</span></strong><span class="koboSpan" id="kobo.426.1">): Similar to time series, the natural text represents a sequence</span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.427.1"> of words (or tokens). </span><span class="koboSpan" id="kobo.427.2">We can train</span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.428.1"> an NLP algorithm to predict the next word based on the preceding </span><em class="italic"><span class="koboSpan" id="kobo.429.1">k</span></em><span class="koboSpan" id="kobo.430.1"> words in a similar manner to a time series. </span><span class="koboSpan" id="kobo.430.2">However, the natural text does not carry the same strict past/future division as time series do. </span><span class="koboSpan" id="kobo.430.3">Because of this, we can use the whole context around the target word as input – that is, words that come both before and after the target word in the consequence, instead of the preceding words only. </span><span class="koboSpan" id="kobo.430.4">As we’ll see in </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.431.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.432.1">, this technique is foundational to contemporary </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">NLP algorithms.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.434.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.435.1">: This is a special type of NN</span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.436.1"> that tries to reproduce its input. </span><span class="koboSpan" id="kobo.436.2">In other words, the target value (label) of an autoencoder is equal to the input data, </span><span class="koboSpan" id="kobo.437.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/20.png" style="vertical-align:-0.340em;height:0.788em;width:2.914em"/></span><span class="koboSpan" id="kobo.438.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.439.1">i</span></em><span class="koboSpan" id="kobo.440.1"> is the sample index. </span><span class="koboSpan" id="kobo.440.2">We can formally say that it tries to learn an identity function (a function that repeats its input). </span><span class="koboSpan" id="kobo.440.3">Since our labels are just input data, the autoencoder is an unsupervised algorithm. </span><span class="koboSpan" id="kobo.440.4">You might be wondering what the point of an algorithm that tries to predict its input is. </span><span class="koboSpan" id="kobo.440.5">The autoencoder is split into two parts – an encoder and a decoder. </span><span class="koboSpan" id="kobo.440.6">First, the encoder tries to compress the input data into a vector with a smaller size than the input itself. </span><span class="koboSpan" id="kobo.440.7">Next, the decoder tries to reproduce the original input based on this smaller internal state vector. </span><span class="koboSpan" id="kobo.440.8">By setting this limitation, the autoencoder is forced to extract only the most significant features of the input data. </span><span class="koboSpan" id="kobo.440.9">The goal of the autoencoder is to learn a representation of the data that is more efficient or compact than the original representation, while still retaining</span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.441.1"> as much of the original information </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">as possible.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.443.1">Another interesting application of self-supervised learning is in generative models, as opposed to discriminative models. </span><span class="koboSpan" id="kobo.443.2">Let’s discuss the difference between the two. </span><span class="koboSpan" id="kobo.443.3">Given input data, a discriminative model will map it to a certain label (in other words, classification or regression). </span><span class="koboSpan" id="kobo.443.4">A typical example is the classification of MNIST images in 1 of 10-digit classes, where the NN maps input data features (pixel intensities) to the digit label. </span><span class="koboSpan" id="kobo.443.5">We can also say this in another way: a discriminative model gives us the probability of </span><em class="italic"><span class="koboSpan" id="kobo.444.1">y</span></em><span class="koboSpan" id="kobo.445.1"> (class), given </span><em class="italic"><span class="koboSpan" id="kobo.446.1">x</span></em><span class="koboSpan" id="kobo.447.1"> (input) – </span><span class="koboSpan" id="kobo.448.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/21.png" style="vertical-align:-0.191em;height:0.887em;width:4.725em"/></span><span class="koboSpan" id="kobo.449.1">. </span><span class="koboSpan" id="kobo.449.2">In the case of MNIST, this is the probability of the digit when given the pixel intensities of </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">the image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.451.1">On the other hand, a generative model</span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.452.1"> learns how classes</span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.453.1"> are distributed. </span><span class="koboSpan" id="kobo.453.2">You can think of it as the opposite of what the discriminative model does. </span><span class="koboSpan" id="kobo.453.3">Instead of predicting the class probability, </span><em class="italic"><span class="koboSpan" id="kobo.454.1">y</span></em><span class="koboSpan" id="kobo.455.1">, given certain input features, it tries to predict the probability of the input features when given a class, </span><em class="italic"><span class="koboSpan" id="kobo.456.1">y</span></em><span class="koboSpan" id="kobo.457.1"> – </span><span class="koboSpan" id="kobo.458.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/22.png" style="vertical-align:-0.000em;height:0.648em;width:0.553em"/></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.459.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/23.png" style="vertical-align:-0.307em;height:1.121em;width:4.289em"/></span></span><span class="koboSpan" id="kobo.460.1">. </span><span class="koboSpan" id="kobo.460.2">For example, a generative model will be able to create an image of a handwritten digit when given the digit class. </span><span class="koboSpan" id="kobo.460.3">Since we only have 10 classes, it will be able to generate just 10 images. </span><span class="koboSpan" id="kobo.460.4">However, we’ve only used this example to illustrate this concept. </span><span class="koboSpan" id="kobo.460.5">In reality, the class could be an arbitrary tensor of values, and the model would be able to generate an unlimited number of images with different features. </span><span class="koboSpan" id="kobo.460.6">If you don’t understand this now, don’t worry; we’ll discuss this topic again in </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.461.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.462.1">. </span><span class="koboSpan" id="kobo.462.2">In </span><a href="B19627_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.463.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.464.1"> and </span><a href="B19627_09.xhtml#_idTextAnchor236"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.465.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.466.1">, we’ll discuss how transformers (a new type of NN) have been used to produce some impressive generative models. </span><span class="koboSpan" id="kobo.466.2">They have gained popularity both in the research community and the mainstream public because of the attractive results they produce. </span><span class="koboSpan" id="kobo.466.3">Two of the most</span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.467.1"> popular visual models are </span><strong class="bold"><span class="koboSpan" id="kobo.468.1">Stable Diffusion</span></strong><span class="koboSpan" id="kobo.469.1"> (</span><a href="https://github.com/CompVis/stable-diffusion"><span class="koboSpan" id="kobo.470.1">https://github.com/CompVis/stable-diffusion</span></a><span class="koboSpan" id="kobo.471.1">), by Stability AI (</span><a href="https://stability.ai/"><span class="koboSpan" id="kobo.472.1">https://stability.ai/</span></a><span class="koboSpan" id="kobo.473.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.474.1">DALL-E</span></strong><span class="koboSpan" id="kobo.475.1"> (</span><a href="https://openai.com/dall-e-2/"><span class="koboSpan" id="kobo.476.1">https://openai.com/dall-e-2/</span></a><span class="koboSpan" id="kobo.477.1">), by OpenAI, which can create photorealistic or artistic images</span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.478.1"> from a natural language description. </span><span class="koboSpan" id="kobo.478.2">When prompted with the text </span><em class="italic"><span class="koboSpan" id="kobo.479.1">Musician frog playing on a guitar</span></em><span class="koboSpan" id="kobo.480.1">, Stable Diffusion produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.482.1"><img alt="Figure 1.7 – Stable Diffusion output for the prompt Musician frog playing on a guitar" src="image/B19627_01_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.483.1">Figure 1.7 – Stable Diffusion output for the prompt Musician frog playing on a guitar</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.484.1">Another interesting</span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.485.1"> generative model is OpenAI’s </span><strong class="bold"><span class="koboSpan" id="kobo.486.1">ChatGPT</span></strong><span class="koboSpan" id="kobo.487.1"> (GPT stands for </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">Generative Pre-trained Transformer</span></strong><span class="koboSpan" id="kobo.489.1">), which (as its name suggests) acts as a smart</span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.490.1"> chatbot. </span><span class="koboSpan" id="kobo.490.2">ChatGPT can</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.491.1"> answer follow-up questions, admit mistakes, challenge incorrect premises, and reject </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">inappropriate requests.</span></span></p>
<h2 id="_idParaDest-22" lang="en-GB"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.493.1">Reinforcement learning</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.494.1">The third class of ML techniques is called </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.496.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.497.1">RL</span></strong><span class="koboSpan" id="kobo.498.1">). </span><span class="koboSpan" id="kobo.498.2">We will illustrate this with one of the most popular</span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.499.1"> applications of RL: teaching</span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.500.1"> machines how to play games. </span><span class="koboSpan" id="kobo.500.2">The machine (or agent) interacts with the game (or environment). </span><span class="koboSpan" id="kobo.500.3">The goal of the agent is to win the game. </span><span class="koboSpan" id="kobo.500.4">To do this, the agent takes actions that can change the environment’s state. </span><span class="koboSpan" id="kobo.500.5">The environment reacts to the agent’s actions and provides it with reward (or penalty) signals that help the agent to decide its next action. </span><span class="koboSpan" id="kobo.500.6">Winning the game would provide the biggest reward. </span><span class="koboSpan" id="kobo.500.7">In formal terms, the goal of the agent is to maximize the total rewards it receives throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">the game:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.502.1"><img alt="Figure 1.8 – The interaction between diﬀerent elements of ﻿an RL system" src="image/B19627_01_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.503.1">Figure 1.8 – The interaction between diﬀerent elements of an RL system</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.504.1">Let’s imagine a game of chess as an RL problem. </span><span class="koboSpan" id="kobo.504.2">Here, the environment would include the chessboard, along with the locations of the pieces. </span><span class="koboSpan" id="kobo.504.3">The goal of our agent is to beat the opponent. </span><span class="koboSpan" id="kobo.504.4">The agent will then receive a reward when they capture the opponent’s piece, and they will win the biggest reward if they checkmate the opponent. </span><span class="koboSpan" id="kobo.504.5">Conversely, if the opponent captures a piece or checkmates the agent, the reward will be negative. </span><span class="koboSpan" id="kobo.504.6">However, as part of their larger strategies, the players will have to make moves that neither capture a piece nor checkmate the other’s king. </span><span class="koboSpan" id="kobo.504.7">The agent won’t receive any reward then. </span><span class="koboSpan" id="kobo.504.8">If this was a supervised learning problem, we would have to provide a label or a reward for each move. </span><span class="koboSpan" id="kobo.504.9">This is not the case with RL. </span><span class="koboSpan" id="kobo.504.10">In the RL framework, the agent will improvise with a trial-and-error approach to decide its </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">next actions.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.506.1">Let’s take another example, in which sometimes we have to sacrifice a pawn to achieve a more important goal (such as a better position on the chessboard). </span><span class="koboSpan" id="kobo.506.2">In such situations, our humble agent has to be smart enough to take a short-term loss as a long-term gain. </span><span class="koboSpan" id="kobo.506.3">In an even more extreme case, imagine we had the bad luck of playing against Ding Liren, the current world chess champion. </span><span class="koboSpan" id="kobo.506.4">Surely, the agent will lose in this case. </span><span class="koboSpan" id="kobo.506.5">However, how would we know which moves were wrong and led to the agent’s loss? </span><span class="koboSpan" id="kobo.506.6">Chess belongs to a class of problems where the game should be considered in its entirety to reach a successful solution, rather than just looking at the immediate consequences of each action. </span><span class="koboSpan" id="kobo.506.7">RL will give us the framework that will help the agent to navigate and learn in this </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">complex environment.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.508.1">An interesting problem arises from this newfound freedom to take action. </span><span class="koboSpan" id="kobo.508.2">Imagine that the agent has learned one successful chess-playing strategy (or </span><strong class="bold"><span class="koboSpan" id="kobo.509.1">policy</span></strong><span class="koboSpan" id="kobo.510.1">, in RL terms). </span><span class="koboSpan" id="kobo.510.2">After some games, the opponent might guess what that policy is and manage to beat us. </span><span class="koboSpan" id="kobo.510.3">The agent will now face a dilemma with the following decisions: either to follow the current policy and risk becoming predictable, or to experiment with new moves that will surprise the opponent, but also carry the risk of turning out even worse. </span><span class="koboSpan" id="kobo.510.4">In general terms, the agent uses a policy that gives them a certain reward, but their ultimate goal is to maximize the total reward. </span><span class="koboSpan" id="kobo.510.5">A modified policy might be more rewarding, and the agent will be ineffective if they don’t try to find such a policy. </span><span class="koboSpan" id="kobo.510.6">One of the challenges of RL is the trade-off between exploitation (following the current policy) and exploration (trying </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">new moves).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.512.1">So far, we’ve used only games</span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.513.1"> as examples; however, many problems</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.514.1"> can fall into the RL domain. </span><span class="koboSpan" id="kobo.514.2">For example, you can think of autonomous vehicle driving as an RL problem. </span><span class="koboSpan" id="kobo.514.3">The vehicle can get positive rewards if it stays within its lane and observes the traffic rules. </span><span class="koboSpan" id="kobo.514.4">It will gain negative rewards if it crashes. </span><span class="koboSpan" id="kobo.514.5">Another interesting recent application of RL is in managing stock portfolios. </span><span class="koboSpan" id="kobo.514.6">The goal of the agent would be to maximize the portfolio value. </span><span class="koboSpan" id="kobo.514.7">The reward is directly derived from the value of the stocks in </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">the portfolio.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.516.1">On the absence of RL in this edition</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.517.1">The second edition of this book had two chapters on RL. </span><span class="koboSpan" id="kobo.517.2">In this edition, we’ll omit those chapters, and we’ll discuss transformers and their applications instead. </span><span class="koboSpan" id="kobo.517.3">On one hand, RL is a promising field, but at present, training RL models is slow, and their practical applications are limited. </span><span class="koboSpan" id="kobo.517.4">Because of this, RL research is mostly concentrated in well-funded commercial companies and academic institutions. </span><span class="koboSpan" id="kobo.517.5">On the other hand, transformers have represented the next big step in the field of ML, in the same way as GPU-trained deep networks sparked interest in the field in the </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">2009-2012 period.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.519.1">Q-learning</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.520.1">Q-learning is an off-policy temporal-difference RL</span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.521.1"> algorithm. </span><span class="koboSpan" id="kobo.521.2">What a mouthful! </span><span class="koboSpan" id="kobo.521.3">But fear not; let’s not worry about what all this means, and instead just see how the algorithm works. </span><span class="koboSpan" id="kobo.521.4">To do this, we’ll use the game of chess we introduced in the previous section. </span><span class="koboSpan" id="kobo.521.5">As a reminder, the board’s configuration (the locations of the pieces) is the current state of the environment. </span><span class="koboSpan" id="kobo.521.6">Here, the agents can take actions, </span><em class="italic"><span class="koboSpan" id="kobo.522.1">a</span></em><span class="koboSpan" id="kobo.523.1">, by moving pieces, thus changing the state into a new one. </span><span class="koboSpan" id="kobo.523.2">We’ll represent a game of chess as a graph where the different board configurations are the graph’s vertices, and the possible moves from each configuration are the edges. </span><span class="koboSpan" id="kobo.523.3">To make a move, the agent follows the edge</span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.524.1"> from the current state, </span><em class="italic"><span class="koboSpan" id="kobo.525.1">s</span></em><span class="koboSpan" id="kobo.526.1">, to a new state, </span><em class="italic"><span class="koboSpan" id="kobo.527.1">s’</span></em><span class="koboSpan" id="kobo.528.1">. </span><span class="koboSpan" id="kobo.528.2">The basic Q-learning algorithm uses a </span><strong class="bold"><span class="koboSpan" id="kobo.529.1">Q-table</span></strong><span class="koboSpan" id="kobo.530.1"> to help the agent decide which moves to make. </span><span class="koboSpan" id="kobo.530.2">The Q-table contains one row for each board configuration, while the columns of the table </span><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.531.1">are all possible actions that the agent can take (the moves). </span><span class="koboSpan" id="kobo.531.2">A table cell, </span><em class="italic"><span class="koboSpan" id="kobo.532.1">q(s, a)</span></em><span class="koboSpan" id="kobo.533.1">, contains the cumulative expected reward, called a </span><strong class="bold"><span class="koboSpan" id="kobo.534.1">Q-value</span></strong><span class="koboSpan" id="kobo.535.1">. </span><span class="koboSpan" id="kobo.535.2">This is the potential total reward that the agent will receive</span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.536.1"> for the remainder of the game if they take an action, </span><em class="italic"><span class="koboSpan" id="kobo.537.1">a</span></em><span class="koboSpan" id="kobo.538.1">, from their current state, </span><em class="italic"><span class="koboSpan" id="kobo.539.1">s</span></em><span class="koboSpan" id="kobo.540.1">. </span><span class="koboSpan" id="kobo.540.2">At the beginning, the Q-table is initialized with an arbitrary value. </span><span class="koboSpan" id="kobo.540.3">With that knowledge, let’s see how </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">Q-learning works:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.542.1">
Initialize the </span><strong class="bold"><span class="koboSpan" id="kobo.543.1">Q table</span></strong><span class="koboSpan" id="kobo.544.1"> with some arbitrary value
for each episode:
     Observe the initial state </span><strong class="bold"><span class="koboSpan" id="kobo.545.1">s</span></strong><span class="koboSpan" id="kobo.546.1">
     for each step of the episode:
          Select new acti</span><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.547.1">on </span><strong class="bold"><span class="koboSpan" id="kobo.548.1">a</span></strong><span class="koboSpan" id="kobo.549.1"> using Q-table based policy
          Ob</span><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.550.1">serve reward </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">r</span></strong><span class="koboSpan" id="kobo.552.1"> and go to the new state </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">s'</span></strong><span class="koboSpan" id="kobo.554.1">
          Use </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">Bellman eq</span></strong><span class="koboSpan" id="kobo.556.1"> to update </span><strong class="bold"><span class="koboSpan" id="kobo.557.1">q(s, a)</span></strong><span class="koboSpan" id="kobo.558.1"> in the Q-table
     until we reach a terminal state for the episode</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.559.1">An episode starts with a random initial state and finishes when we reach the terminal state. </span><span class="koboSpan" id="kobo.559.2">In our case, one episode would be one full game </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">of chess.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.561.1">The question that arises is, how does the agent’s policy determine what will be the next action? </span><span class="koboSpan" id="kobo.561.2">To do so, the policy has to consider the Q-values of all the possible actions from the current state. </span><span class="koboSpan" id="kobo.561.3">The higher the Q-value, the more attractive the action is. </span><span class="koboSpan" id="kobo.561.4">However, the policy will sometimes ignore the Q-table (exploitation of the existing knowledge) and choose another random action to find higher potential rewards (exploration). </span><span class="koboSpan" id="kobo.561.5">In the beginning, the agent will take random actions because the Q-table doesn’t contain much information (a trial-and-error approach). </span><span class="koboSpan" id="kobo.561.6">As time progresses and the Q-table is gradually filled, the age</span><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.562.1">nt will become more informed in interacting with </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">the environment.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.564.1">We update </span><em class="italic"><span class="koboSpan" id="kobo.565.1">q(s, a)</span></em><span class="koboSpan" id="kobo.566.1"> after each new action by using the </span><strong class="bold"><span class="koboSpan" id="kobo.567.1">Bellman equation</span></strong><span class="koboSpan" id="kobo.568.1">. </span><span class="koboSpan" id="kobo.568.2">The Bellman equation is beyond the</span><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.569.1"> scope</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.570.1"> of this introduction, but it’s enough to know that the updated value, </span><em class="italic"><span class="koboSpan" id="kobo.571.1">q(s, a)</span></em><span class="koboSpan" id="kobo.572.1">, is based on the newly received reward, </span><em class="italic"><span class="koboSpan" id="kobo.573.1">r</span></em><span class="koboSpan" id="kobo.574.1">, as well as the maximum possible Q-value, </span><em class="italic"><span class="koboSpan" id="kobo.575.1">q*(s’, a’)</span></em><span class="koboSpan" id="kobo.576.1">, of the new </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">state, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.578.1">s’</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.580.1">This example was intended to help you understand the basic workings of Q-learning, but you might have noticed an issue with this. </span><span class="koboSpan" id="kobo.580.2">We store the combination of all possible board configurations and moves in the Q-table. </span><span class="koboSpan" id="kobo.580.3">This would make the table huge and impossible to fit in today’s computer memory. </span><span class="koboSpan" id="kobo.580.4">Fortunately, there is a solution for this: we can replace the Q-table with an NN, which will tell the agent what the optimal action is in each state. </span><span class="koboSpan" id="kobo.580.5">In recent</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.581.1"> years, this development has allowed RL algorithms to achieve superhuman performance on tasks such as the games of Go, Dota 2, Doom, </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">and StarCraft.</span></span></p>
<h2 id="_idParaDest-23" lang="en-GB"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.583.1">Components of an ML solution</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.584.1">So far, we’ve discussed three major classes</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.585.1"> of ML algorithms. </span><span class="koboSpan" id="kobo.585.2">However, to solve an ML problem, we’ll need a system in which the ML algorithm is only part of it. </span><span class="koboSpan" id="kobo.585.3">The most important aspects of such a system are </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.587.1">Learner</span></strong><span class="koboSpan" id="kobo.588.1">: This algorithm is used with</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.589.1"> its learning philosophy. </span><span class="koboSpan" id="kobo.589.2">The choice of this algorithm is determined by the problem we’re trying to solve since different problems can be better suited for certain </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">ML algorithms.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.591.1">Training data</span></strong><span class="koboSpan" id="kobo.592.1">: This is the raw dataset that we</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.593.1"> are interested in. </span><span class="koboSpan" id="kobo.593.2">This can be labeled or unlabeled. </span><span class="koboSpan" id="kobo.593.3">It’s important to have enough sample data for the learner to understand the structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">the problem.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.595.1">Representation</span></strong><span class="koboSpan" id="kobo.596.1">: This is how we express the data in terms of the chosen features</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.597.1"> so that we can feed it to the learner. </span><span class="koboSpan" id="kobo.597.2">For example, to classify handwritten images of digits, we’ll represent the image as a two-dimensional array of values, where each cell will contain the color value of one pixel. </span><span class="koboSpan" id="kobo.597.3">A good choice of representation of the data is important for achieving </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">better results.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.599.1">Goal</span></strong><span class="koboSpan" id="kobo.600.1">: This represents the reason</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.601.1"> to learn from the data for the problem at hand. </span><span class="koboSpan" id="kobo.601.2">This is strictly related to the target and helps us define how and what the learner should use and what representation to use. </span><span class="koboSpan" id="kobo.601.3">For example, the goal may be to clean our mailboxes of unwanted emails, and this goal defines what the target of our learner is. </span><span class="koboSpan" id="kobo.601.4">In this case, it is the detection of </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">spam emails.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.603.1">Target</span></strong><span class="koboSpan" id="kobo.604.1">: This represents what is being learned</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.605.1"> as well as the final output. </span><span class="koboSpan" id="kobo.605.2">The target can be a classification of unlabeled data, a representation of input data according to hidden patterns or characteristics, a simulator for future predictions, or a response to an outside stimulus or strategy (in the case </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">of RL).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.607.1">It can never be emphasized</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.608.1"> enough: any ML algorithm can only achieve an approximation of the target and not a perfect numerical description. </span><span class="koboSpan" id="kobo.608.2">ML algorithms are not exact mathematical solutions to problems – they are just approximations. </span><span class="koboSpan" id="kobo.608.3">In the </span><em class="italic"><span class="koboSpan" id="kobo.609.1">Supervised learning section</span></em><span class="koboSpan" id="kobo.610.1">, we defined learning as a function from the space of features (the input) into a range of classes. </span><span class="koboSpan" id="kobo.610.2">Later, we’ll see how certain ML algorithms, such as NNs, can approximate any function to any degree, in theory. </span><span class="koboSpan" id="kobo.610.3">This  is called</span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.611.1"> the universal approximation theorem (</span><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><span class="koboSpan" id="kobo.612.1">https://en.wikipedia.org/wiki/Universal_approximation_theorem</span></a><span class="koboSpan" id="kobo.613.1">), but it does not imply that we can get a precise solution to our problem. </span><span class="koboSpan" id="kobo.613.2">In addition, solutions to the problem can be better achieved by better understanding the </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">training data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.615.1">Typically, a problem that can be solved with classic ML techniques may require a thorough understanding and processing of the training data before deployment. </span><span class="koboSpan" id="kobo.615.2">The steps to solve an ML problem are </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">as follows:</span></span></p>
<ol>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.617.1">Data collection</span></strong><span class="koboSpan" id="kobo.618.1">: This involves gathering as much data</span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.619.1"> as possible. </span><span class="koboSpan" id="kobo.619.2">In the case of supervised learning, this also includes </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">correct labeling.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.621.1">Data processing</span></strong><span class="koboSpan" id="kobo.622.1">: This involves cleaning the data, such as removing redundant or highly correlated</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.623.1"> features, or even filling in missing data, and understanding the features that define the </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">training data.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.625.1">Creation of the test case</span></strong><span class="koboSpan" id="kobo.626.1">: Usually, the data can be divided</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.627.1"> into </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">three sets:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.629.1">Training set</span></strong><span class="koboSpan" id="kobo.630.1">: We use this set to train the ML algorithm. </span><span class="koboSpan" id="kobo.630.2">In most cases, we’ll train</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.631.1"> the algorithm by iterating the whole training set</span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.632.1"> more than once. </span><span class="koboSpan" id="kobo.632.2">We’ll refer to the number of full training set iterations </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">as </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.634.1">epochs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.636.1">Validation set</span></strong><span class="koboSpan" id="kobo.637.1">: We use this set to evaluate the accuracy of the algorithm with unknown</span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.638.1"> data during training. </span><span class="koboSpan" id="kobo.638.2">We’ll train the algorithm for some time on the training set and then we’ll use the validation set to check its performance. </span><span class="koboSpan" id="kobo.638.3">If we are not</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.639.1"> satisfied with the result, we can tune the hyperparameters of the algorithm and repeat the process. </span><span class="koboSpan" id="kobo.639.2">The validation set can also help us determine when to stop the training. </span><span class="koboSpan" id="kobo.639.3">We’ll learn more about this later in </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">this section.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.641.1">Test set</span></strong><span class="koboSpan" id="kobo.642.1">: When we finish tuning the algorithm with the training or validation</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.643.1"> cycle, we’ll use the test set only once for a final evaluation. </span><span class="koboSpan" id="kobo.643.2">The test set is similar to the validation set in the sense that the algorithm hasn’t used it during training. </span><span class="koboSpan" id="kobo.643.3">However, when we strive to improve the algorithm on the validation data, we may inadvertently introduce bias, which can skew the results in favor of the validation set and not reflect the actual performance. </span><span class="koboSpan" id="kobo.643.4">Because we use the test only once, this will provide a more objective measurement of </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">the algorithm.</span></span></li></ul></li>
</ol>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.645.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.646.1">One of the reasons for the success of DL algorithms is that they usually require less data processing than classic methods. </span><span class="koboSpan" id="kobo.646.2">For a classic algorithm, you would have to apply different data processing and extract different features for each problem. </span><span class="koboSpan" id="kobo.646.3">With DL, you can apply the same data processing pipeline for most tasks. </span><span class="koboSpan" id="kobo.646.4">With DL, you can be more productive, and you don’t need as much domain knowledge for the task at hand compared to the classic </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">ML algorithms.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.648.1">There are many valid reasons to create testing and validation datasets. </span><span class="koboSpan" id="kobo.648.2">As mentioned previously, ML techniques can only produce an approximation of the desired result. </span><span class="koboSpan" id="kobo.648.3">Often, we can only include a finite and limited number of variables, and there may be many variables that are outside of our control. </span><span class="koboSpan" id="kobo.648.4">If we only used a single dataset, our model may end up memorizing the data and producing an extremely high accuracy value on the data it has memorized. </span><span class="koboSpan" id="kobo.648.5">However, this result may not be reproducible on other similar but unknown datasets. </span><span class="koboSpan" id="kobo.648.6">One of the key goals of ML algorithms is their ability to generalize. </span><span class="koboSpan" id="kobo.648.7">This is why we create both a validation set used for tuning our model selection during training and a final test set only used at the end of the process to confirm the validity of the </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">selected algorithm.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.650.1">To understand the importance</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.651.1"> of selecting valid features and to avoid memorizing the data, which is also referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.652.1">overfitting</span></strong><span class="koboSpan" id="kobo.653.1"> in the literature (we’ll use this term from now on), let’s use a joke taken from an XKND comic as an </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">example (</span></span><a href="http://xkcd.com/1122):"><span class="No-Break"><span class="koboSpan" id="kobo.655.1">http://xkcd.com/1122</span></span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">):</span></span></a></p>
<p class="author-quote" lang="en-GB"><span class="koboSpan" id="kobo.657.1">“Up until 1996, no democratic US presidential candidate who was an incumbent and with no combat experience had ever beaten anyone whose first name was worth more in Scrabble.”</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.658.1">It’s apparent that such a rule</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.659.1"> is meaningless, but it underscores the importance of selecting valid features, and how the question, “How much is a name worth in Scrabble?” </span><span class="koboSpan" id="kobo.659.2">can bear any relevance while selecting a US president. </span><span class="koboSpan" id="kobo.659.3">Also, this example doesn’t have any predictive power over unknown data. </span><span class="koboSpan" id="kobo.659.4">We’ll call this overfitting, which refers to making predictions that fit the data at hand perfectly but don’t generalize to larger datasets. </span><span class="koboSpan" id="kobo.659.5">Overfitting is the process of trying to make sense of what we’ll call noise (information that does not have any real meaning) and trying to fit the model to </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">small perturbations.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.661.1">To explain this further, let’s try to use ML to predict the trajectory of a ball thrown from the ground up into the air (not perpendicularly) until it reaches the ground again. </span><span class="koboSpan" id="kobo.661.2">Physics teaches us that the trajectory is shaped like a parabola. </span><span class="koboSpan" id="kobo.661.3">We also expect that a good ML algorithm observing thousands of such throws would come up with a parabola as a solution. </span><span class="koboSpan" id="kobo.661.4">However, if we were to zoom into the ball and observe the smallest fluctuations in the air due to turbulence, we might notice that the ball does not hold a steady trajectory but may be subject to small perturbations, which in this case is the noise. </span><span class="koboSpan" id="kobo.661.5">An ML algorithm that tries to model these small perturbations would fail to see the big picture and produce a result that is not satisfactory. </span><span class="koboSpan" id="kobo.661.6">In other words, overfitting is the process that makes the ML algorithm see the trees but forget about </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">the forest:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.663.1"><img alt="Figure 1.9 – A good prediction model (left) and a bad (overﬁtted) prediction model, with the trajectory of a ball thrown from the ground (right)" src="image/B19627_01_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.664.1">Figure 1.9 – A good prediction model (left) and a bad (overﬁtted) prediction model, with the trajectory of a ball thrown from the ground (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.665.1">This is why we separate the training data from the validation and test data; if the algorithm’s accuracy over the test data was not similar to the training data accuracy, that would be a good indication that the model overfits. </span><span class="koboSpan" id="kobo.665.2">We need to make sure that we don’t make the opposite error either – that is, underfitting the model. </span><span class="koboSpan" id="kobo.665.3">In practice, though, if we aim to make our prediction model as accurate as possible on our training data, underfitting</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.666.1"> is much less of a risk, and care is taken to </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">avoid overfitting.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.668.1">The following figure </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">depicts</span></span><span class="No-Break"><a id="_idIndexMarker106"/></span><span class="No-Break"><span class="koboSpan" id="kobo.670.1"> underfitting:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.671.1"><img alt="Figure 1.10 – Underﬁtting can be a problem as well" src="image/B19627_01_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.672.1">Figure 1.10 – Underﬁtting can be a problem as well</span></p>
<h2 id="_idParaDest-24" lang="en-GB"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.673.1">Neural networks</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.674.1">We introduced some of the popular classical ML algorithms</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.675.1"> in the previous sections. </span><span class="koboSpan" id="kobo.675.2">In this section, we’ll talk about NNs, which are the focus of </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">this book.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.677.1">The first example of an NN is called the perceptron, and this was invented by Frank Rosenblatt in 1957. </span><span class="koboSpan" id="kobo.677.2">The perceptron is a classification algorithm that is very similar to logistic regression. </span><span class="koboSpan" id="kobo.677.3">Similar to logistic regression, it has a vector of weights, </span><strong class="bold"><span class="koboSpan" id="kobo.678.1">w</span></strong><span class="koboSpan" id="kobo.679.1">, and its output is a function, </span><span class="koboSpan" id="kobo.680.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/24.png" style="vertical-align:-0.257em;height:0.968em;width:2.825em"/></span><span class="koboSpan" id="kobo.681.1">, of the dot product, </span><span class="koboSpan" id="kobo.682.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/25.png" style="vertical-align:-0.011em;height:0.452em;width:1.950em"/></span><span class="koboSpan" id="kobo.683.1"> (or </span><span class="koboSpan" id="kobo.684.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/26.png" style="vertical-align:-0.393em;height:1.205em;width:3.951em"/></span><span class="koboSpan" id="kobo.685.1">), of the weights </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">and input.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.687.1">The only difference is that </span><em class="italic"><span class="koboSpan" id="kobo.688.1">f</span></em><span class="koboSpan" id="kobo.689.1"> is a simple step function – that is, if </span><span class="koboSpan" id="kobo.690.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;⋅&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/27.png" style="vertical-align:-0.012em;height:0.646em;width:3.942em"/></span><span class="koboSpan" id="kobo.691.1">, then </span><span class="koboSpan" id="kobo.692.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/28.png" style="vertical-align:-0.257em;height:0.968em;width:4.933em"/></span><span class="koboSpan" id="kobo.693.1">, or else </span><br/><span class="koboSpan" id="kobo.694.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;⋅&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/29.png" style="vertical-align:-0.257em;height:0.968em;width:4.773em"/></span><span class="koboSpan" id="kobo.695.1">, wherein we apply a similar logistic regression rule over the output of the logistic function. </span><span class="koboSpan" id="kobo.695.2">The perceptron is an example of a simple one-layer neural </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">feedforward network:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.697.1"><img alt="Figure 1.11 – A simple perceptron with three inputs and one output" src="image/B19627_01_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.698.1">Figure 1.11 – A simple perceptron with three inputs and one output</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.699.1">The perceptron</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.700.1"> was very promising, but it was soon discovered that it has serious limitations as it only works for linearly separable classes. </span><span class="koboSpan" id="kobo.700.2">In 1969, Marvin Minsky and Seymour Paper demonstrated that it could not learn even a simple logical function such as XOR. </span><span class="koboSpan" id="kobo.700.3">This led to a significant decline in the interest </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">in perceptrons.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.702.1">However, other NNs can solve this problem. </span><span class="koboSpan" id="kobo.702.2">A classic multilayer perceptron has multiple interconnected perceptrons, such as units that are organized in different sequential layers (input layer, one or more hidden layers, and an output layer). </span><span class="koboSpan" id="kobo.702.3">Each unit of a layer is connected to all units of the next layer. </span><span class="koboSpan" id="kobo.702.4">First, the information is presented to the input layer, then we use it to compute the output (or activation), </span><span class="koboSpan" id="kobo.703.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/30.png" style="vertical-align:-0.340em;height:0.788em;width:0.593em"/></span><span class="koboSpan" id="kobo.704.1">, for each unit of the first hidden layer. </span><span class="koboSpan" id="kobo.704.2">We propagate forward, with the output as input for the next layers in the network (hence feedforward), and so on until we reach the output. </span><span class="koboSpan" id="kobo.704.3">The most common way to train NNs is by using gradient descent in combination with backpropagation. </span><span class="koboSpan" id="kobo.704.4">We’ll discuss this in detail in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.705.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.706.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.707.1">The following diagram depicts an NN with one </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">hidden layer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.709.1"><img alt="Figure 1.12 – ﻿An NN with one hidden layer" src="image/B19627_01_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.710.1">Figure 1.12 – An NN with one hidden layer</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.711.1">Think of the hidden layers as an abstract representation of the input data. </span><span class="koboSpan" id="kobo.711.2">This is the way the NN understands the features of the data with its internal logic. </span><span class="koboSpan" id="kobo.711.3">However, NNs are non-interpretable models. </span><span class="koboSpan" id="kobo.711.4">This means that if we observe the </span><span class="koboSpan" id="kobo.712.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/31.png" style="vertical-align:-0.340em;height:0.788em;width:0.582em"/></span><span class="koboSpan" id="kobo.713.1"> activations of the hidden layer, we wouldn’t be able to understand them. </span><span class="koboSpan" id="kobo.713.2">For us, they are just a vector of numerical values. </span><span class="koboSpan" id="kobo.713.3">We need the output layer to bridge the gap between the network’s representation and the actual data we’re interested in. </span><span class="koboSpan" id="kobo.713.4">You can think of this as a translator; we use it to understand the network’s logic, and at the same time, we can convert it into the actual target values that we are </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">interested in.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.715.1">The universal approximation theorem tells us that a feedforward network with one hidden layer can represent any function. </span><span class="koboSpan" id="kobo.715.2">It’s good to know that there are no theoretical limits on networks with one hidden layer, but in practice, we can achieve limited success with such architectures. </span><span class="koboSpan" id="kobo.715.3">In </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.716.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.717.1">, we’ll discuss how to achieve better performance with deep NNs, and their advantages</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.718.1"> over the shallow ones. </span><span class="koboSpan" id="kobo.718.2">For now, let’s apply our knowledge by solving a simple classification task with </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">an NN.</span></span></p>
<h2 id="_idParaDest-25" lang="en-GB"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.720.1">Introducing PyTorch</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.721.1">In this section, we’ll introduce</span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.722.1"> PyTorch – an open source Python DL framework that was developed primarily by Facebook that has been gaining momentum recently. </span><span class="koboSpan" id="kobo.722.2">It provides </span><strong class="bold"><span class="koboSpan" id="kobo.723.1">graphics processing unit</span></strong><span class="koboSpan" id="kobo.724.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.725.1">GPU</span></strong><span class="koboSpan" id="kobo.726.1">) accelerated multidimensional array (or tensor) operations, and computational </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.727.1">graphs, which we can use to build NNs. </span><span class="koboSpan" id="kobo.727.2">Throughout this book, we’ll use PyTorch and Keras, and we’ll talk about these libraries and compare them in detail in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.728.1">Chapter 3</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.729.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.730.1">Let’s create a simple NN that will classify the Iris flower dataset. </span><span class="koboSpan" id="kobo.730.2">The steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">as follows:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.732.1">Start by loading </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">the dataset:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.734.1">
import pandas as pd
dat</span><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.735.1">aset = pd.read_csv('</span><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'"><span class="koboSpan" id="kobo.736.1">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'</span></a><span class="koboSpan" id="kobo.737.1">, names=['sepal_length', 'sepal_width'</span><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.738.1">, 'petal_length', 'petal_width', 'species'])
dataset['species'] = pd.Categorical(dataset['species']).codes
dataset = dataset.sample(frac=1, random_state=1234)
train_</span><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.739.1">input = dataset.values[:120, :4]
train_</span><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.740.1">target = dataset.values[:120, 4]
test_input = dataset.values[120:, :4]
test_target = dataset.values[120:, 4]</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.741.1">The preceding</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.742.1"> code is boilerplate code that downloads the Iris dataset’s CSV file</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.743.1"> and then loads it into a </span><strong class="bold"><span class="koboSpan" id="kobo.744.1">pandas DataFrame</span></strong><span class="koboSpan" id="kobo.745.1"> called </span><strong class="source-inline"><span class="koboSpan" id="kobo.746.1">dataset</span></strong><span class="koboSpan" id="kobo.747.1">. </span><span class="koboSpan" id="kobo.747.2">Then, we shuffle the DataFrame’s rows and split the code into NumPy arrays, </span><strong class="source-inline"><span class="koboSpan" id="kobo.748.1">train_input</span></strong><span class="koboSpan" id="kobo.749.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">train_target</span></strong><span class="koboSpan" id="kobo.751.1"> (flower properties/flower class), for the training data and </span><strong class="source-inline"><span class="koboSpan" id="kobo.752.1">test_input</span></strong><span class="koboSpan" id="kobo.753.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">test_target</span></strong><span class="koboSpan" id="kobo.755.1"> for the test data. </span><span class="koboSpan" id="kobo.755.2">We’ll use 120 samples for training and 30 for testing. </span><span class="koboSpan" id="kobo.755.3">If you are not familiar with pandas, think of this as an advanced version </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">of NumPy.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.757.1">Next, define our first NN. </span><span class="koboSpan" id="kobo.757.2">We’ll use a feedforward</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.758.1"> network with one hidden layer with fiv</span><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.759.1">e units, a </span><strong class="bold"><span class="koboSpan" id="kobo.760.1">ReLU</span></strong><span class="koboSpan" id="kobo.761.1"> activation function (this is just another type</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.762.1"> of activation, defined simply as </span><em class="italic"><span class="koboSpan" id="kobo.763.1">f(x) = max(0, x)</span></em><span class="koboSpan" id="kobo.764.1">), and an output layer with three units. </span><span class="koboSpan" id="kobo.764.2">The output layer has three units, and each unit corresponds to one of the three classes of Iris flowers. </span><span class="koboSpan" id="kobo.764.3">The following is the PyTorch definition of </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">the network:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.766.1">
import torch
torch.manual_seed(1234)
hidden_units = 5
net = torch.nn.Sequential(
    torch.nn.Linear(4, hidden_units),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_units, 3)
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.767.1">We’ll use </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">one-hot encoding</span></strong><span class="koboSpan" id="kobo.769.1"> for the target data. </span><span class="koboSpan" id="kobo.769.2">This means that each class of the flower</span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.770.1"> will be represented as an array (</span><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">Iris Setosa = [1, 0, 0]</span></strong><span class="koboSpan" id="kobo.772.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.773.1">Iris Versicolor = [0, 1, 0]</span></strong><span class="koboSpan" id="kobo.774.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.775.1">Iris Virginica = [0, 0, 1]</span></strong><span class="koboSpan" id="kobo.776.1">), and one element of the array will be the target for one unit of the output layer. </span><span class="koboSpan" id="kobo.776.2">When the network classifies a new sample, we’ll determine the class by taking the unit with the highest activation value. </span><strong class="source-inline"><span class="koboSpan" id="kobo.777.1">torch.manual_seed(1234)</span></strong><span class="koboSpan" id="kobo.778.1"> enables us to use the same random data seed every time for the reproducibility </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">of results.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.780.1">Choose the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">loss</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.782.1"> function:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.783.1">
criterion = torch.nn.CrossEntropyLoss()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.784.1">With the </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">criterion</span></strong><span class="koboSpan" id="kobo.786.1"> variable, we define the loss function as </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">cross-entropy loss</span></strong><span class="koboSpan" id="kobo.788.1">. </span><span class="koboSpan" id="kobo.788.2">The loss function will measure</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.789.1"> how different the output of the network is compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">target data.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.791.1">Define the </span><strong class="bold"><span class="koboSpan" id="kobo.792.1">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.793.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.794.1">SGD</span></strong><span class="koboSpan" id="kobo.795.1">) optimizer (a variation of the gradient</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.796.1"> descent algorithm) with a </span><strong class="bold"><span class="koboSpan" id="kobo.797.1">learning rate</span></strong><span class="koboSpan" id="kobo.798.1"> of 0.1 and a </span><strong class="bold"><span class="koboSpan" id="kobo.799.1">momentum</span></strong><span class="koboSpan" id="kobo.800.1"> of 0.9 (we’ll discuss SGD and its parameters in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.801.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.802.1">):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.803.1">
optimizer = torch.optim.SGD(net.parameters(), lr=0.1,
          momentum=0.9)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.804.1">Train </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">the</span></span><span class="No-Break"><a id="_idIndexMarker119"/></span><span class="No-Break"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.806.1"> network:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.807.1">
epochs = 50
for epoch in range(epochs):
    inputs = torch.auto</span><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.808.1">grad.Variable(
        torch.Tensor(train_input).float())
    targets =</span><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.809.1"> torch.autograd.Variable(
        torch.Tensor(train_target).long())
    optimizer.zero_grad()
    out = net(input</span><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.810.1">s)
    loss = criterion(out, targets)
    loss.backward()
    optimizer.step()
    if epoch == 0 or (epoch + 1) % 10 == 0:
        print('Epoch %d Loss: %.4f' % (epoch + 1,
        loss.item()))</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.811.1">We’ll run the training for 50 epochs, which means that we’ll iterate 50 times over the </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">training dataset:</span></span></p><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.813.1">Create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">torch</span></strong><span class="koboSpan" id="kobo.815.1"> variables from the NumPy array – that is, </span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">train_input</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.817.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.818.1">train_target</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.820.1">Zero the gradients</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.821.1"> of the optimizer to prevent accumulation from the previous iterations. </span><span class="koboSpan" id="kobo.821.2">We feed the training data to the NN, </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">net(inputs)</span></strong><span class="koboSpan" id="kobo.823.1">, and we compute the loss function’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">criterion</span></strong><span class="koboSpan" id="kobo.825.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.826.1">out</span></strong><span class="koboSpan" id="kobo.827.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">targets</span></strong><span class="koboSpan" id="kobo.829.1">) between the network output and the </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">target data.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.831.1">Propagate the </span><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">loss</span></strong><span class="koboSpan" id="kobo.833.1"> value back through the network. </span><span class="koboSpan" id="kobo.833.2">We’re doing this so that we can calculate how each network weight affe</span><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.834.1">cts the </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">loss function.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.836.1">The optimizer updates the weights of the network in a way that will reduce the future loss </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">function’s values.</span></span></li></ol><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.838.1">When we run the training, the output will be </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">as follows:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.840.1">
Epoch 1 Loss: 1.2181
Epoch 10 Loss: 0.6745
Epoch 20 Loss: 0.2447
Epoch 30 Loss: 0.1397
Epoch 40 Loss: 0.1001
Epoch 50 Loss: 0.0855</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.841.1">The following graph shows how the loss function decreases with each epoch. </span><span class="koboSpan" id="kobo.841.2">This shows how the network gradually learns the </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">training data:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.843.1"><img alt="Figure 1.13 – The loss function decreases with each epoch" src="image/B19627_01_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.844.1">Figure 1.13 – The loss function decreases with each epoch</span></p>
<ol>
<li lang="en-GB" value="6"><span class="koboSpan" id="kobo.845.1">Let’s see what the</span><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.846.1"> final accuracy</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.847.1"> of our </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">model is:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.849.1">
import numpy as np
inputs = torch.a</span><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.850.1">utograd.Variable(torch.Tensor(test_input).float())
targets = torch.autograd.Variable(torch.Tensor(test_target).long())
optimizer.zero_grad()
out = net(inputs)
_, predicted</span><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.851.1"> = torch.max(out.data, 1)
error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())
print('Errors: %d; Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.852.1">We do this by feeding the test set to the network and computing the error manually. </span><span class="koboSpan" id="kobo.852.2">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">as follows:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.854.1">
Errors: 0; Accuracy: 100%</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.855.1">We were able to classify all 30 test </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">samples correctly.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.857.1">We must also try different hyperparameters</span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.858.1"> of the network and see how the accuracy and loss functions work. </span><span class="koboSpan" id="kobo.858.2">You could try changing the number of units in the hidden layer, the number of epochs we train in the network, as well as the </span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">learning rate.</span></span></p>
<h1 id="_idParaDest-26" lang="en-GB"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.860.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.861.1">This chapter covered what ML is and why it’s so important. </span><span class="koboSpan" id="kobo.861.2">We talked about the main classes of ML techniques and some of the most popular classic ML algorithms. </span><span class="koboSpan" id="kobo.861.3">We also introduced a particular type of ML algorithm, called NNs, which is the basis for DL. </span><span class="koboSpan" id="kobo.861.4">Then, we looked at a coding example where we used a popular ML library to solve a particular </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">classification problem.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.863.1">In the next chapter, we’ll cover NNs in more detail and explore their </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">theoretical justifications.</span></span></p>
</div>
</body></html>