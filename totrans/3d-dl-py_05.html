<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer091">
<h1 class="chapter-number" id="_idParaDest-58" lang="en-GB"><a id="_idTextAnchor059"/>4</h1>
<h1 id="_idParaDest-59" lang="en-GB"><a id="_idTextAnchor060"/>Learning Object Pose Detection and Tracking by Differentiable Rendering</h1>
<p lang="en-GB">In this chapter, we are going to explore an object pose detection and tracking project by using differentiable rendering. In object pose detection, we are interested in detecting the orientation and location of a certain object. For example, we may be given the camera model and object mesh model and need to estimate the object orientation and position from one image of the object. In the approach in this chapter, we are going to formulate such a pose estimation problem as an optimization problem, where the object pose is fitted to the <span class="No-Break" lang="">image observation.</span></p>
<p lang="en-GB">The same approach as the aforementioned can also be used for object pose tracking, where we have already estimated the object pose in the 1, 2,…, up to t-1 time slots and want to estimate the object pose at the <em class="italic" lang="">t</em> time slot, based on one image observation of the object at <span class="No-Break" lang=""><em class="italic" lang="">t</em></span><span class="No-Break" lang=""> time.</span></p>
<p lang="en-GB">One important technique we will use in this chapter is called differentiable rendering, a super-exciting topic currently explored in deep learning. For example, the CVPR 2021 Best Paper Award winner <em class="italic" lang="">GIRAFFE: representing scenes as compositional generative neural feature fields</em> uses differentiable rendering as one important component in <span class="No-Break" lang="">its pipeline.</span></p>
<p lang="en-GB">Rendering is the process of projecting 3D physical models (a mesh model for the object, or a camera model) into 2D images. It is an imitation of the physical process of image formation. Many 3D computer vision tasks can be considered as an inverse of the rendering process – that is, in many computer vision problems, we want to start from 2D images to estimate the 3D physical models (meshes, point cloud segmentation, object poses, or <span class="No-Break" lang="">camera positions).</span></p>
<p lang="en-GB">Thus, a very natural idea that has been discussed in the computer vision community for several decades is that we can formulate many 3D computer vision problems as optimization problems, where the optimization variables are the 3D models (meshes, or point cloud voxels), and the objective functions are certain similarity measures between the rendered images and the <span class="No-Break" lang="">observed images.</span></p>
<p lang="en-GB">To efficiently solve such an optimization problem as the aforementioned, the rendering process should be differentiable. For example, if the rendering is differentiable, we can use the end-to-end approach to train a deep learning model to solve the problem. However, as will be discussed in more detail in the latter sections, conventional rendering processes are not differentiable. Thus, we need to modify the conventional approaches to make them differentiable. We will discuss how we can do that at great length in the <span class="No-Break" lang="">following section.</span></p>
<p lang="en-GB">Thus, in this chapter, we will cover first the <em class="italic" lang="">why differentiable rendering</em> problem and then the <em class="italic" lang="">how differentiable problems</em> problem. We will then talk about what 3D computer vision problems can usually be solved by using differentiable rendering. We will dedicate a significant part of the chapter to a concrete example of using differentiable rendering to solve object pose estimation. We will present coding examples in <span class="No-Break" lang="">the process.</span></p>
<p lang="en-GB">In this chapter, we’re going to cover the following <span class="No-Break" lang="">main topics:</span></p>
<ul>
<li lang="en-GB">Why differentiable renderings <span class="No-Break" lang="">are needed</span></li>
<li lang="en-GB">How to make <span class="No-Break" lang="">rendering differentiable</span></li>
<li lang="en-GB"><a id="_idTextAnchor061"/>What problems can be solved by <span class="No-Break" lang="">differentiable rendering</span></li>
<li lang="en-GB">The object pose <span class="No-Break" lang="">estimation problem</span></li>
</ul>
<h1 id="_idParaDest-60" lang="en-GB"><a id="_idTextAnchor062"/> Technical requirements</h1>
<p lang="en-GB">In order to run the example code snippets in this book, you need to have a computer ideally with a GPU. However, running the code snippets with only CPUs is <span class="No-Break" lang="">not impossible.</span></p>
<p lang="en-GB">The recommended computer configuration includes <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB">A GPU such as the GTX series or RTX series with at least 8 GB <span class="No-Break" lang="">of memory</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Python 3</span></li>
<li lang="en-GB">The PyTorch and <span class="No-Break" lang="">PyTorch3D libraries</span></li>
</ul>
<p lang="en-GB">The code snippets with this chapter can be found <span class="No-Break" lang="">at </span><a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python"><span class="No-Break" lang="">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python</span></a><span class="No-Break" lang="">.</span></p>
<h1 id="_idParaDest-61" lang="en-GB"><a id="_idTextAnchor063"/>Why we want to have differentiable rendering</h1>
<p lang="en-GB">The physical process of image formation is a mapping from 3D models to 2D images. As shown in the example in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.1</em>, depending on the positions of the red and blue spheres in 3D (two possible configurations are shown on the left-hand side), we may get different 2D images (the images corresponding to the two configurations are shown on the <span class="No-Break" lang="">right-hand side).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 4.1: The image formation process is a mapping from the 3D models to 2D images " height="622" src="image/B18217_04_1.jpg" width="1587"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: The image formation process is a mapping from the 3D models to 2D images</p>
<p lang="en-GB">Many 3D computer vision problems are a reversal of image formation. In these problems, we are usually given 2D images and need to estimate the 3D models from the 2D images. For example, in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.2</em>, we are given the 2D image shown on the right-hand side and the question is, <em class="italic" lang="">which 3D model is the one that corresponds to the </em><span class="No-Break" lang=""><em class="italic" lang="">observed image?</em></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 4.2: Many 3D computer vision problems are based on 2D images given to estimate 3D models " height="595" src="image/B18217_04_2.jpg" width="1442"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: Many 3D computer vision problems are based on 2D images given to estimate 3D models</p>
<p lang="en-GB">According to some ideas that were first discussed in the computer vision community decades ago, we can formulate the problem as an optimization problem. In this case, the optimization variables here are the position of two 3D spheres. We want to optimize the two centers, such that the rendered images are like the preceding 2D observed image. To measure similarity precisely, we need to use a cost function – for example, we can use pixel-wise mean-square errors. We then need to compute a gradient from the cost function to the two centers of spheres, so that we can minimize the cost function iteratively by going toward the gradient <span class="No-Break" lang="">descent direction.</span></p>
<p lang="en-GB">However, we can calculate a gradient from the cost function to the optimization variables only under the condition that the mapping from the optimization variables to the cost functions is differentiable, which implies that the rendering process is <span class="No-Break" lang="">also differentiable.</span></p>
<h1 id="_idParaDest-62" lang="en-GB"><a id="_idTextAnchor064"/>How to make rendering differentiable</h1>
<p lang="en-GB">In this section, we are going to discuss why the conventional rendering algorithms are not differentiable. We will discuss the approach used in PyTorch3D, which makes the <span class="No-Break" lang="">rendering differentiable.</span></p>
<p lang="en-GB">Rendering is an imitation of the physical process of image formation. This physical process of image formation itself is differentiable in many cases. Suppose that the surface is normal and the material properties of the object are all smooth. Then, the pixel color in the example is a differentiable function of the positions of <span class="No-Break" lang="">the spheres.</span></p>
<p lang="en-GB">However, there are cases where the pixel color is not a smooth function of the position. This can happen at the occlusion boundaries, for example. This is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.3</em>, where the blue sphere is at a location that would occlude the red sphere at that view if the blue sphere moved up a little bit. The pixel moved at that view is thus not a differentiable function of the sphere <span class="No-Break" lang="">center locations.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 4.3: The image formation is not a smooth function at occlusion boundaries " height="469" src="image/B18217_04_3.jpg" width="1440"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: The image formation is not a smooth function at occlusion boundaries</p>
<p lang="en-GB">When we use conventional rendering algorithms, information about local gradients is lost due to<a id="_idIndexMarker175"/> discretization. As we discussed in the previous chapters, rasterization is a step of rendering where for each pixel on the imaging plane, we find the most relevant mesh face (or decide that no relevant mesh face can <span class="No-Break" lang="">be found).</span></p>
<p lang="en-GB">In conventional rasterization, for each pixel, we generate a ray from the camera center going through the pixel on the imaging plane. We will find all the mesh faces that intersect with this ray. In the conventional approach, the rasterizer will only return the mesh face that is nearest to the camera. The returned mesh face will then be passed to the shader, which is the next step of the rendering pipeline. The shader will then be applied to one of the shading algorithms (such as the Lambertian model or Phong model) to determine the pixel color. This step of choosing the mesh to render is a non-differentiable process, since it is mathematically modeled as a <span class="No-Break" lang="">step function.</span></p>
<p lang="en-GB">There has been a large body of literature in the computer vision community on how to make rendering differentiable. The differentiable rendering implemented in the PyTorch3D library mainly used the approach in <em class="italic" lang="">Soft Rasterizer</em> by Liu, Li, Chen, and <span class="No-Break" lang="">Li (arXiv:1904.01786).</span></p>
<p lang="en-GB">The main idea of differentiable rendering is illustrated in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.4</em>. In the rasterization step, instead of returning only one relevant mesh face, we will find all the mesh faces, such that the distance of the mesh face to the ray is within a certain threshold. In PyTorch3D, this threshold can be set at <strong class="source-inline" lang="">RasterizationSettings.blur_radius</strong>. We can also control the maximal number of faces to be returned by <span class="No-Break" lang="">setting </span><span class="No-Break" lang=""><strong class="source-inline" lang="">RasterizationSettings.faces_per_pixel</strong></span><span class="No-Break" lang="">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 4.4: Differentiable rendering by weighted averaging of all the relevant mesh faces " height="571" src="image/B18217_04_4.jpg" width="1388"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4: Differentiable rendering by weighted averaging of all the relevant mesh faces</p>
<p lang="en-GB">Next, the renderer <a id="_idIndexMarker176"/>needs to calculate a probability map for each mesh face as follows, where <strong class="source-inline" lang="">dist</strong> represents the distance between the mesh face and the ray, and sigma is a hyperparameter. In Pytorch3D, the <strong class="source-inline" lang="">sigma</strong> parameter can be set at <strong class="source-inline" lang="">BlendParams.sigma</strong>. Simply put, this probability map is a probability that this mesh face covers this image pixel. The distance can be negative if the ray intersects the <span class="No-Break" lang="">mesh face.</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="128" src="image/Formula_04_001.png" width="509"/></p>
<p lang="en-GB">Next, the pixel color is determined by the weighted averages of the shading results of all the mesh faces returned by the rasterizer. The weight for each mesh face depends on its inverse depth value, <em class="italic" lang="">z</em>, and probability map, <em class="italic" lang="">D</em>, as shown in the following equation. Because this <em class="italic" lang="">z</em> value is the inverse depth, any mesh faces closer to the camera have larger <em class="italic" lang="">z</em> values than the mesh faces far away from the camera. The weight, w<span class="subscript" lang="">b</span>, is a small weight for the background color. The parameter gamma here is a hyperparameter. In PyTorch3D, this parameter can be set <span class="No-Break" lang="">to </span><span class="No-Break" lang=""><strong class="source-inline" lang="">BlendParams.gamma</strong></span><span class="No-Break" lang="">:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="118" src="image/Formula_04_002.png" width="611"/></p>
<p lang="en-GB">Thus, the final pixel color can be determined by the <span class="No-Break" lang="">following equation:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="136" src="image/Formula_04_003.png" width="432"/></p>
<p lang="en-GB">The PyTorch3D implementation of differential rendering also computes an alpha value for each image pixel. This alpha value represents how likely the image pixel is in the foreground, the ray intersects at least one mesh face, as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.4</em>. We want to compute this alpha value and make it differentiable. In a soft rasterizer, the alpha value is computed from the probability maps, <span class="No-Break" lang="">as follows.</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="147" src="image/Formula_04_004.png" width="363"/></p>
<p lang="en-GB">Now that we<a id="_idIndexMarker177"/> have learned how to make rendering differentiable, we will see how to use it for <span class="No-Break" lang="">various purposes.</span></p>
<h2 id="_idParaDest-63" lang="en-GB"><a id="_idTextAnchor065"/>What problems can be solved by using differentiable rendering</h2>
<p lang="en-GB">As mentioned <a id="_idIndexMarker178"/>earlier, differentiable rendering has been discussed in the computer vision community for decades. In the past, differentiable rendering was used for single-view mesh reconstruction, image-based shape fitting, and more. In the following sections of this chapter, we are going to show a concrete example of using differentiable rendering for rigid object pose estimation <span class="No-Break" lang="">and tracking.</span></p>
<p lang="en-GB">Differentiable rendering is a technique in that we can formulate the estimation problems in 3D computer vision into optimization problems. It can be applied to a wide range of problems. More interestingly, one exciting recent trend is to combine differentiable rendering with deep learning. Usually, differentiable rendering is used as the generator part of the deep learning models. The whole pipeline can thus be trained end <span class="No-Break" lang="">to end.</span></p>
<h1 id="_idParaDest-64" lang="en-GB"><a id="_idTextAnchor066"/>The object pose estimation problem</h1>
<p lang="en-GB">In this section, we <a id="_idIndexMarker179"/>are going to show a concrete example of using differentiable rendering for 3D computer vision problems. The problem is object pose estimation from one single observed image. In addition, we assume that we have the 3D mesh model of <span class="No-Break" lang="">the object.</span></p>
<p lang="en-GB">For example, we assume we have the 3D mesh model for a toy cow and teapot, as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.5</em> and <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.7</em> respectively. Now, suppose we have taken one image of the toy cow and teapot. Thus, we have one RGB image of the toy cow, as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.6</em>, and one silhouette image of the teapot, as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.8</em>. The problem is then to estimate the orientation and location of the toy cow and teapot at the moments when these images <span class="No-Break" lang="">are taken.</span></p>
<p lang="en-GB">Because it is cumbersome to rotate and move the meshes, we choose instead to fix the orientations and locations of the meshes and optimize the orientations and locations of the cameras. By assuming that the camera orientations are always pointing toward the meshes, we can further simplify the problem, such that all we need to optimize is the <span class="No-Break" lang="">camera locations.</span></p>
<p lang="en-GB">Thus, we formulate our optimization problem, such that the optimization variables will be the camera locations. By using differentiable rendering, we can render RGB images and silhouette images for the two meshes. The rendered images are compared with the observed images and, thus, loss functions between the rendered images and observed images can be calculated. Here, we use mean-square errors as the loss function. Because everything is differentiable, we can then compute gradients from the loss functions to the optimization variables. Gradient descent algorithms can then be used to find the best camera positions, such that the rendered images are matched to the <span class="No-Break" lang="">observed images.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 4.5: Mesh model for a toy cow " height="700" src="image/B18217_04_5.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5: Mesh model for a toy cow</p>
<p lang="en-GB">The following<a id="_idIndexMarker180"/> image shows the RGB output for <span class="No-Break" lang="">the cow:</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 4.6: Observed RGB image for the toy cow " height="850" src="image/B18217_04_6.jpg" width="1078"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6: Observed RGB image for the toy cow</p>
<p lang="en-GB">The following figure shows the mesh for <span class="No-Break" lang="">a teapot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 4.7: Mesh model for a teapot " height="452" src="image/B18217_04_7.jpg" width="1516"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7: Mesh model for a teapot</p>
<p lang="en-GB">The following<a id="_idIndexMarker181"/> figure shows the silhouette of <span class="No-Break" lang="">the teapot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 4.8: Observed silhouette of the teapot " height="862" src="image/B18217_04_8.jpg" width="956"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8: Observed silhouette of the teapot</p>
<p lang="en-GB">Now that we know the problem and how to work on it, let’s start coding in the <span class="No-Break" lang="">next section.</span></p>
<h1 id="_idParaDest-65" lang="en-GB"><a id="_idTextAnchor067"/>How it is coded</h1>
<p lang="en-GB">The code is <a id="_idIndexMarker182"/>provided in the repository in the <strong class="source-inline" lang="">chap4</strong> folder as <strong class="source-inline" lang="">diff_render.py</strong>. The mesh model of the teapot is provided in the <strong class="source-inline" lang="">data</strong> subfolder as <strong class="source-inline" lang="">teapot.obj</strong>. We will run through the code <span class="No-Break" lang="">as follows:</span></p>
<ol>
<li lang="en-GB">The code in <strong class="source-inline" lang="">diff_render.py</strong> starts by importing the <span class="No-Break" lang="">needed packages:</span><p class="source-code" lang="en-GB">import os</p><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import numpy as np</p><p class="source-code" lang="en-GB">import torch.nn as nn</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">from skimage import img_as_ubyte</p><p class="source-code" lang="en-GB">from pytorch3d.io import load_obj</p><p class="source-code" lang="en-GB">from pytorch3d.structures import Meshes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,</p><p class="source-code" lang="en-GB">RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,</p><p class="source-code" lang="en-GB">SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">In the next step, we declare a PyTorch device. If you have GPUs, then the device will be created to use GPUs. Otherwise, the device has to <span class="No-Break" lang="">use CPUs:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p><p class="source-code" lang="en-GB">    print("WARNING: CPU only, this will be slow!")</p></li>
<li lang="en-GB">We then define <strong class="source-inline" lang="">output_dir</strong> in the following line. When we run the codes in <strong class="source-inline" lang="">diff_render.py</strong>, the codes will generate some rendered images for each optimization iteration, so that we can see how the optimization works step by step. All the generated rendered images by the code will be put in this <span class="No-Break" lang=""><strong class="source-inline" lang="">output_dir</strong></span><span class="No-Break" lang=""> folder.</span><p class="source-code" lang="en-GB">output_dir = './result_teapot'</p></li>
<li lang="en-GB">We then load the mesh model from the <strong class="source-inline" lang="">./data/teapot.obj</strong> file. Because this mesh model <a id="_idIndexMarker183"/>does not come with textures (material colors), we create an all-one tensor and make the tensor the texture for the mesh model. Eventually, we obtain a mesh model with textures as the <span class="No-Break" lang=""><strong class="source-inline" lang="">teapot_mesh</strong></span><span class="No-Break" lang=""> variable:</span><p class="source-code" lang="en-GB">verts, faces_idx, _ = load_obj("./data/teapot.obj")</p><p class="source-code" lang="en-GB">faces = faces_idx.verts_idx</p><p class="source-code" lang="en-GB">verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)</p><p class="source-code" lang="en-GB">textures = TexturesVertex(verts_features=verts_rgb.to(device))</p><p class="source-code" lang="en-GB">teapot_mesh = Meshes(</p><p class="source-code" lang="en-GB">    verts=[verts.to(device)],</p><p class="source-code" lang="en-GB">    faces=[faces.to(device)],</p><p class="source-code" lang="en-GB">    textures=textures</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we define a camera model in the <span class="No-Break" lang="">following line.</span><p class="source-code" lang="en-GB">cameras = FoVPerspectiveCameras(device=device)</p></li>
<li lang="en-GB">In the next step, we are going to define a differentiable renderer called <strong class="source-inline" lang="">silhouette_renderer</strong>. Each renderer has mainly two components, such as one rasterizer for finding relevant mesh faces for each image pixel, one shader for determining the image pixel colors, and so on. In this example, we actually use a soft silhouette shader, which outputs an alpha value for each image pixel. The alpha value is a real number ranging from 0 to 1, which indicates whether this image pixel is a part of the foreground or background. Note that the hyperparameters for the shader are defined in the <strong class="source-inline" lang="">blend_params</strong> variables, the <strong class="source-inline" lang="">sigma</strong> parameter is for computing the probability maps, and gamma is for computing weights for <span class="No-Break" lang="">mesh faces.</span></li>
</ol>
<p lang="en-GB">Here, we use <strong class="source-inline" lang="">MeshRasterizer</strong> for rasterization. Note that the parameter <strong class="source-inline" lang="">blur_radius</strong> is the threshold for finding relevant mesh faces and <strong class="source-inline" lang="">faces_per_pixel</strong> is <a id="_idIndexMarker184"/>the maximum number of mesh faces that will be returned for each <span class="No-Break" lang="">image pixel:</span></p>
<p class="source-code" lang="en-GB">blend_params = BlendParams(sigma=1e-4, gamma=1e-4)</p>
<p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p>
<p class="source-code" lang="en-GB">    image_size=256,</p>
<p class="source-code" lang="en-GB">    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,</p>
<p class="source-code" lang="en-GB">    faces_per_pixel=100,</p>
<p class="source-code" lang="en-GB">)</p>
<p class="source-code" lang="en-GB">silhouette_renderer = MeshRenderer(</p>
<p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p>
<p class="source-code" lang="en-GB">        cameras=cameras,</p>
<p class="source-code" lang="en-GB">        raster_settings=raster_settings</p>
<p class="source-code" lang="en-GB">    ),</p>
<p class="source-code" lang="en-GB">    shader=SoftSilhouetteShader(blend_params=blend_params)</p>
<p class="source-code" lang="en-GB">)</p>
<ol>
<li lang="en-GB" value="7">We then define <strong class="source-inline" lang="">phong_renderer</strong> as follows. This <strong class="source-inline" lang="">phong_renderer</strong> is mainly used for visualization of the optimization process. Basically, at each optimization iteration, we will render one RGB image according to the camera position at that iteration. Note that this renderer is only used for visualization purposes, thus it is not a differentiable renderer. You can actually tell that <strong class="source-inline" lang="">phong_renderer</strong> is not a differentiable one by noticing <span class="No-Break" lang="">the following:</span><ul><li lang="en-GB">It uses <strong class="source-inline" lang="">HardPhoneShader</strong>, which takes only one mesh face as input for each <span class="No-Break" lang="">image pixel</span></li><li lang="en-GB">It uses <strong class="source-inline" lang="">MeshRenderer</strong> with a <strong class="source-inline" lang="">blur_radius</strong> value of 0.0 and <strong class="source-inline" lang="">faces_per_pixel</strong> is set <span class="No-Break" lang="">to 1</span></li></ul></li>
<li lang="en-GB">A light <a id="_idIndexMarker185"/>source, <strong class="source-inline" lang="">lights</strong>, is then defined with a location of 2.0, 2.0, <span class="No-Break" lang="">and -2.0:</span><p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">    image_size=256,</p><p class="source-code" lang="en-GB">    blur_radius=0.0,</p><p class="source-code" lang="en-GB">    faces_per_pixel=1,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))</p><p class="source-code" lang="en-GB">phong_renderer = MeshRenderer(</p><p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        raster_settings=raster_settings</p><p class="source-code" lang="en-GB">    ),</p><p class="source-code" lang="en-GB">    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we define a camera location and compute the corresponding rotation, <strong class="source-inline" lang="">R</strong>, and displacement, <strong class="source-inline" lang="">T</strong>, of the camera. This rotation and displacement are the target camera position – that is, we are going to generate an image from this camera position and use the image as the observed image in <span class="No-Break" lang="">our problem:</span><p class="source-code" lang="en-GB">distance = 3</p><p class="source-code" lang="en-GB">elevation = 50.0</p><p class="source-code" lang="en-GB">azimuth = 0.0</p><p class="source-code" lang="en-GB">R, T = look_at_view_transform(distance, elevation, azimuth, device=device)</p></li>
<li lang="en-GB">Now, we<a id="_idIndexMarker186"/> generate an image, <strong class="source-inline" lang="">image_ref</strong>, from this camera position. The <strong class="source-inline" lang="">image_ref</strong> function has four channels, <strong class="bold" lang="">RGBA</strong> – <strong class="bold" lang="">R</strong> for red, <strong class="bold" lang="">G</strong> for green, <strong class="bold" lang="">B</strong> for blue, and <strong class="bold" lang="">A</strong> for alpha values. The <strong class="source-inline" lang="">image_ref</strong> function is also saved as <strong class="source-inline" lang="">target_rgb.png</strong> for our <span class="No-Break" lang="">latter inspection:</span><p class="source-code" lang="en-GB">silhouette = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)</p><p class="source-code" lang="en-GB">image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)</p><p class="source-code" lang="en-GB">silhouette = silhouette.cpu().numpy()</p><p class="source-code" lang="en-GB">image_ref = image_ref.cpu().numpy()</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(silhouette.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))</p><p class="source-code" lang="en-GB">plt.close()</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(image_ref.squeeze())</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.savefig(os.path.join(output_dir, 'target_rgb.png'))</p><p class="source-code" lang="en-GB">plt.close()</p></li>
<li lang="en-GB">In the next step, we are going to define a <strong class="source-inline" lang="">Model</strong> class. This <strong class="source-inline" lang="">Model</strong> class is derived <a id="_idIndexMarker187"/>from <strong class="source-inline" lang="">torch.nn.Module</strong>; thus, as with many other PyTorch models, automatic gradient computations can be enabled <span class="No-Break" lang="">for </span><span class="No-Break" lang=""><strong class="source-inline" lang="">Model</strong></span><span class="No-Break" lang="">.</span></li>
</ol>
<p lang="en-GB"><strong class="source-inline" lang="">Model</strong> has an initialization function, <strong class="source-inline" lang="">__init__</strong>, which takes the <strong class="source-inline" lang="">meshes</strong> input for mesh models, <strong class="source-inline" lang="">renderer</strong> for the renderer, and <strong class="source-inline" lang="">image_ref</strong> as the target image that the instance of <strong class="source-inline" lang="">Model</strong> will try to fit. The <strong class="source-inline" lang="">__init__</strong> function creates an <strong class="source-inline" lang="">image_ref</strong> buffer by using the <strong class="source-inline" lang="">torch.nn.Module.register_buffer</strong> function. Just a reminder for those who are not so familiar with this part of PyTorch – a buffer is something that can be saved as a part of <strong class="source-inline" lang="">state_dict</strong> and moved to different devices in <strong class="source-inline" lang="">cuda()</strong> and <strong class="source-inline" lang="">cpu()</strong>, with the rest of the model’s parameters. However, the buffer is not updated by <span class="No-Break" lang="">the optimizer.</span></p>
<p lang="en-GB">The<strong class="source-inline" lang=""> __init__</strong> function also creates a model parameter, <strong class="source-inline" lang="">camera_position</strong>. As a model parameter, the <strong class="source-inline" lang="">camera_position</strong> variable can be updated by the optimizer. Note that the optimization variables now become the <span class="No-Break" lang="">model parameters.</span></p>
<p lang="en-GB">The <strong class="source-inline" lang="">Model</strong> class also has a <strong class="source-inline" lang="">forward</strong> member function, which can do the forward computation and backward gradient propagation. The forward function renders a silhouette image from the current camera position and computes a loss function <a id="_idIndexMarker188"/>between the rendered image with <strong class="source-inline" lang="">image_refer</strong> – the <span class="No-Break" lang="">observed image:</span></p>
<p class="source-code" lang="en-GB">class Model(nn.Module):</p>
<p class="source-code" lang="en-GB">    def __init__(self, meshes, renderer, image_ref):</p>
<p class="source-code" lang="en-GB">        super().__init__()</p>
<p class="source-code" lang="en-GB">        self.meshes = meshes</p>
<p class="source-code" lang="en-GB">        self.device = meshes.device</p>
<p class="source-code" lang="en-GB">        self.renderer = renderer</p>
<p class="source-code" lang="en-GB">        image_ref = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))</p>
<p class="source-code" lang="en-GB">        self.register_buffer('image_ref', image_ref)</p>
<p class="source-code" lang="en-GB">        self.camera_position = nn.Parameter(</p>
<p class="source-code" lang="en-GB">            torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))</p>
<p class="source-code" lang="en-GB">    def forward(self):</p>
<p class="source-code" lang="en-GB">        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)</p>
<p class="source-code" lang="en-GB">        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)</p>
<p class="source-code" lang="en-GB">        image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)</p>
<p class="source-code" lang="en-GB">        loss = torch.sum((image[..., 3] - self.image_ref) ** 2)</p>
<p class="source-code" lang="en-GB">        return loss, image</p>
<ol>
<li lang="en-GB" value="12">Now, we have already defined the <strong class="source-inline" lang="">Model</strong> class. We can then create an instance of the class and define an optimizer. Before running any optimization, we want to render an image to show the starting camera position. This silhouette image for the starting<a id="_idIndexMarker189"/> camera position will be saved <span class="No-Break" lang="">to </span><span class="No-Break" lang=""><strong class="source-inline" lang="">starting_silhouette.png</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">model = Model(meshes=teapot_mesh, renderer=silhouette_renderer, image_ref=image_ref).to(device)</p><p class="source-code" lang="en-GB">optimizer = torch.optim.Adam(model.parameters(), lr=0.05)</p><p class="source-code" lang="en-GB">_, image_init = model()</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.title("Starting Silhouette")</p><p class="source-code" lang="en-GB">plt.savefig(os.path.join(output_dir, 'starting_silhouette.png'))</p><p class="source-code" lang="en-GB">plt.close()</p></li>
<li lang="en-GB">Finally, we can run optimization iterations. During each optimization iteration, we will save the rendered image from the camera position to a file under the <span class="No-Break" lang=""><strong class="source-inline" lang="">output_dir</strong></span><span class="No-Break" lang=""> folder:</span><p class="source-code" lang="en-GB">for i in range(0, 200):</p><p class="source-code" lang="en-GB">    if i%10 == 0:</p><p class="source-code" lang="en-GB">        print('i = ', i)</p><p class="source-code" lang="en-GB">    optimizer.zero_grad()</p><p class="source-code" lang="en-GB">    loss, _ = model()</p><p class="source-code" lang="en-GB">    loss.backward()</p><p class="source-code" lang="en-GB">    optimizer.step()</p><p class="source-code" lang="en-GB">    if loss.item() &lt; 500:</p><p class="source-code" lang="en-GB">        break</p><p class="source-code" lang="en-GB">    R = look_at_rotation(model.camera_position[None, :], device=model.device)</p><p class="source-code" lang="en-GB">    T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)</p><p class="source-code" lang="en-GB">    image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)</p><p class="source-code" lang="en-GB">    image = image[0, ..., :3].detach().squeeze().cpu().numpy()</p><p class="source-code" lang="en-GB">    image = img_as_ubyte(image)</p><p class="source-code" lang="en-GB">    plt.figure()</p><p class="source-code" lang="en-GB">    plt.imshow(image[..., :3])</p><p class="source-code" lang="en-GB">    plt.title("iter: %d, loss: %0.2f" % (i, loss.data))</p><p class="source-code" lang="en-GB">    plt.axis("off")</p><p class="source-code" lang="en-GB">    plt.savefig(os.path.join(output_dir, 'fitting_' + str(i) + '.png'))</p><p class="source-code" lang="en-GB">    plt.close()</p></li>
</ol>
<p lang="en-GB"><span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.9</em> shows the<a id="_idIndexMarker190"/> observed silhouette of the object (in this case, <span class="No-Break" lang="">a teapot):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 4.9: The silhouette of the teapot " height="862" src="image/B18217_04_9.jpg" width="1191"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9: The silhouette of the teapot</p>
<p lang="en-GB">We formulate the fitting problem as an optimization problem. The initial teapot position is illustrated in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><span class="No-Break" lang=""><em class="italic" lang="">.10</em></span><span class="No-Break" lang="">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 4.10: The initial position of the teapot " height="187" src="image/B18217_04_10.jpg" width="582"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10: The initial position of the teapot</p>
<p lang="en-GB">The final optimized<a id="_idIndexMarker191"/> teapot position is illustrated in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><span class="No-Break" lang=""><em class="italic" lang="">.11</em></span><span class="No-Break" lang="">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 4.11: The final position of the teapot " height="310" src="image/B18217_04_11.jpg" width="711"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11: The final position of the teapot</p>
<h2 id="_idParaDest-66" lang="en-GB"><a id="_idTextAnchor068"/>An example of object pose estimation for both silhouette fitting and texture fitting</h2>
<p lang="en-GB">In the <a id="_idIndexMarker192"/>previous <a id="_idIndexMarker193"/>example, we <a id="_idIndexMarker194"/>estimated the object pose by silhouette<a id="_idIndexMarker195"/> fitting. In this section, we will present another example of object pose estimation by using both silhouette fitting and texture fitting. In 3D computer vision, we usually use texture to denote colors. Thus, in this example, we will use differentiable rendering to render RGB images according to the camera positions and optimize the camera position. The code is <span class="No-Break" lang="">in </span><span class="No-Break" lang=""><strong class="source-inline" lang="">diff_render_texture.py</strong></span><span class="No-Break" lang="">:</span></p>
<ol>
<li lang="en-GB" value="1">In this first step, we will import all the <span class="No-Break" lang="">required packages:</span><p class="source-code" lang="en-GB">import os</p><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import numpy as np</p><p class="source-code" lang="en-GB">import torch.nn as nn</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">from skimage import img_as_ubyte</p><p class="source-code" lang="en-GB">from pytorch3d.io import load_objs_as_meshes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,</p><p class="source-code" lang="en-GB">RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,</p><p class="source-code" lang="en-GB">SoftSilhouetteShader, HardPhongShader, PointLights,</p><p class="source-code" lang="en-GB">SoftPhongShader</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we create the PyTorch device using either GPUs <span class="No-Break" lang="">or CPUs:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">    torch.cuda.set_device(device)</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p></li>
<li lang="en-GB">We <a id="_idIndexMarker196"/>set <strong class="source-inline" lang="">output_dir</strong> as <strong class="source-inline" lang="">result_cow</strong>. This <a id="_idIndexMarker197"/>will <a id="_idIndexMarker198"/>be the<a id="_idIndexMarker199"/> folder where we save the <span class="No-Break" lang="">fitting results:</span><p class="source-code" lang="en-GB">output_dir = './result_cow'</p></li>
<li lang="en-GB">We load the mesh model of a toy cow from the <span class="No-Break" lang=""><strong class="source-inline" lang="">cow.obj</strong></span><span class="No-Break" lang=""> file:</span><p class="source-code" lang="en-GB">obj_filename = "./data/cow_mesh/cow.obj"</p><p class="source-code" lang="en-GB">cow_mesh = load_objs_as_meshes([obj_filename], device=device)</p></li>
<li lang="en-GB">We define cameras and light sources <span class="No-Break" lang="">as follows:</span><p class="source-code" lang="en-GB">cameras = FoVPerspectiveCameras(device=device)</p><p class="source-code" lang="en-GB">lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))</p></li>
<li lang="en-GB">Next, we create a <strong class="source-inline" lang="">renderer_silhouette</strong> renderer. This is the differentiable renderer for rendering silhouette images. Note the <strong class="source-inline" lang="">blur_radius</strong> and <strong class="source-inline" lang="">faces_per_pixel</strong> numbers. This renderer is mainly used in <span class="No-Break" lang="">silhouette fitting:</span><p class="source-code" lang="en-GB">blend_params = BlendParams(sigma=1e-4, gamma=1e-4)</p><p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">    image_size=256,</p><p class="source-code" lang="en-GB">    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,</p><p class="source-code" lang="en-GB">    faces_per_pixel=100,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">renderer_silhouette = MeshRenderer(</p><p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        raster_settings=raster_settings</p><p class="source-code" lang="en-GB">    ),</p><p class="source-code" lang="en-GB">    shader=SoftSilhouetteShader(blend_params=blend_params)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we <a id="_idIndexMarker200"/>create<a id="_idIndexMarker201"/> a <strong class="source-inline" lang="">renderer_textured</strong> renderer. This <a id="_idIndexMarker202"/>renderer <a id="_idIndexMarker203"/>is another differentiable renderer, mainly used for rendering <span class="No-Break" lang="">RGB images:</span><p class="source-code" lang="en-GB">sigma = 1e-4</p><p class="source-code" lang="en-GB">raster_settings_soft = RasterizationSettings(</p><p class="source-code" lang="en-GB">    image_size=256,</p><p class="source-code" lang="en-GB">    blur_radius=np.log(1. / 1e-4 - 1.)*sigma,</p><p class="source-code" lang="en-GB">    faces_per_pixel=50,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">renderer_textured = MeshRenderer(</p><p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        raster_settings=raster_settings_soft</p><p class="source-code" lang="en-GB">    ),</p><p class="source-code" lang="en-GB">    shader=SoftPhongShader(device=device,</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        lights=lights)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we create a <strong class="source-inline" lang="">phong_renderer</strong> renderer. This renderer is mainly used for<a id="_idIndexMarker204"/> visualization. The <a id="_idIndexMarker205"/>preceding differentiable <a id="_idIndexMarker206"/>renders tend <a id="_idIndexMarker207"/>to create blurry images. Therefore, it would be nice for us to have a renderer that can create <span class="No-Break" lang="">sharp images:</span><p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">    image_size=256,</p><p class="source-code" lang="en-GB">    blur_radius=0.0,</p><p class="source-code" lang="en-GB">    faces_per_pixel=1,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">phong_renderer = MeshRenderer(</p><p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        raster_settings=raster_settings</p><p class="source-code" lang="en-GB">    ),</p><p class="source-code" lang="en-GB">    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we will define a camera position and the corresponding camera rotation and position. This will be the camera position where the observed image is taken. As in the previous example, we optimize the camera orientation and position instead of the object orientation and position. Also, we assume that the camera is always pointing toward the object. Thus, we only need to optimize the <span class="No-Break" lang="">camera position:</span><p class="source-code" lang="en-GB">distance = 3</p><p class="source-code" lang="en-GB">elevation = 50.0</p><p class="source-code" lang="en-GB">azimuth = 0.0</p><p class="source-code" lang="en-GB">R, T = look_at_view_transform(distance, elevation, azimuth, device=device)</p></li>
<li lang="en-GB">Next, we <a id="_idIndexMarker208"/>create the<a id="_idIndexMarker209"/> observed<a id="_idIndexMarker210"/> images<a id="_idIndexMarker211"/> and save them to <strong class="source-inline" lang="">target_silhouette.png</strong> and <strong class="source-inline" lang="">target_rgb.png</strong>. The images are also stored in the <strong class="source-inline" lang="">silhouette</strong> and <span class="No-Break" lang=""><strong class="source-inline" lang="">image_ref</strong></span><span class="No-Break" lang=""> variables:</span><p class="source-code" lang="en-GB">silhouette = renderer_silhouette(meshes_world=cow_mesh, R=R, T=T)</p><p class="source-code" lang="en-GB">image_ref = phong_renderer(meshes_world=cow_mesh, R=R, T=T)</p><p class="source-code" lang="en-GB">silhouette = silhouette.cpu().numpy()</p><p class="source-code" lang="en-GB">image_ref = image_ref.cpu().numpy()</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(silhouette.squeeze()[..., 3])</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))</p><p class="source-code" lang="en-GB">plt.close()</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(image_ref.squeeze())</p><p class="source-code" lang="en-GB">plt.grid(False)</p><p class="source-code" lang="en-GB">plt.savefig(os.path.join(output_dir, 'target_rgb.png'))</p><p class="source-code" lang="en-GB">plt.close()</p></li>
<li lang="en-GB">We modify the definition for the <strong class="source-inline" lang="">Model</strong> class as follows. The most notable changes from the previous example are that now we will render both the alpha channel image and the RGB images and compare them with the observed images. The <a id="_idIndexMarker212"/>mean-square<a id="_idIndexMarker213"/> losses at the alpha<a id="_idIndexMarker214"/> channel <a id="_idIndexMarker215"/>and RGB channels are weighted to give the final <span class="No-Break" lang="">loss value:</span><p class="source-code" lang="en-GB">class Model(nn.Module):</p><p class="source-code" lang="en-GB">    def __init__(self, meshes, renderer_silhouette, renderer_textured, image_ref, weight_silhouette, weight_texture):</p><p class="source-code" lang="en-GB">        super().__init__()</p><p class="source-code" lang="en-GB">        self.meshes = meshes</p><p class="source-code" lang="en-GB">        self.device = meshes.device</p><p class="source-code" lang="en-GB">        self.renderer_silhouette = renderer_silhouette</p><p class="source-code" lang="en-GB">        self.renderer_textured = renderer_textured</p><p class="source-code" lang="en-GB">        self.weight_silhouette = weight_silhouette</p><p class="source-code" lang="en-GB">        self.weight_texture = weight_texture</p><p class="source-code" lang="en-GB">        image_ref_silhouette = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))</p><p class="source-code" lang="en-GB">        self.register_buffer('image_ref_silhouette', image_ref_silhouette)</p><p class="source-code" lang="en-GB">        image_ref_textured = torch.from_numpy((image_ref[..., :3]).astype(np.float32))</p><p class="source-code" lang="en-GB">        self.register_buffer('image_ref_textured', image_ref_textured)</p><p class="source-code" lang="en-GB">        self.camera_position = nn.Parameter(</p><p class="source-code" lang="en-GB">            torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))</p><p class="source-code" lang="en-GB">    def forward(self):</p><p class="source-code" lang="en-GB">        # Render the image using the updated camera position. Based on the new position of the</p><p class="source-code" lang="en-GB">        # camera we calculate the rotation and translation matrices</p><p class="source-code" lang="en-GB">        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)</p><p class="source-code" lang="en-GB">        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)</p><p class="source-code" lang="en-GB">        image_silhouette = self.renderer_silhouette(meshes_world=self.meshes.clone(), R=R, T=T)</p><p class="source-code" lang="en-GB">        image_textured = self.renderer_textured(meshes_world=self.meshes.clone(), R=R, T=T)</p><p class="source-code" lang="en-GB">        loss_silhouette = torch.sum((image_silhouette[..., 3] - self.image_ref_silhouette) ** 2)</p><p class="source-code" lang="en-GB">        loss_texture = torch.sum((image_textured[..., :3] - self.image_ref_textured) ** 2)</p><p class="source-code" lang="en-GB">        loss = self.weight_silhouette * loss_silhouette + self.weight_texture * loss_texture</p><p class="source-code" lang="en-GB">        return loss, image_silhouette, image_textured</p></li>
<li lang="en-GB">Next, we create an instance of the <strong class="source-inline" lang="">Model</strong> class and create <span class="No-Break" lang="">an optimizer:</span><p class="source-code" lang="en-GB">model = Model(meshes=cow_mesh, renderer_silhouette=renderer_silhouette, renderer_textured = renderer_textured,</p><p class="source-code" lang="en-GB">              image_ref=image_ref, weight_silhouette=1.0, weight_texture=0.1).to(device)</p><p class="source-code" lang="en-GB">optimizer = torch.optim.Adam(model.parameters(), lr=0.05)</p></li>
<li lang="en-GB">Finally, we<a id="_idIndexMarker216"/> run 200 optimization <a id="_idIndexMarker217"/>iterations. The<a id="_idIndexMarker218"/> rendered <a id="_idIndexMarker219"/>images are saved at <span class="No-Break" lang="">each iteration:</span><p class="source-code" lang="en-GB">for i in range(0, 200):</p><p class="source-code" lang="en-GB">    if i%10 == 0:</p><p class="source-code" lang="en-GB">        print('i = ', i)</p><p class="source-code" lang="en-GB">    optimizer.zero_grad()</p><p class="source-code" lang="en-GB">    loss, image_silhouette, image_textured = model()</p><p class="source-code" lang="en-GB">    loss.backward()</p><p class="source-code" lang="en-GB">    optimizer.step()</p><p class="source-code" lang="en-GB">    plt.figure()</p><p class="source-code" lang="en-GB">    plt.imshow(image_silhouette[..., 3].detach().squeeze().cpu().numpy())</p><p class="source-code" lang="en-GB">    plt.title("iter: %d, loss: %0.2f" % (i, loss.data))</p><p class="source-code" lang="en-GB">    plt.axis("off")</p><p class="source-code" lang="en-GB">    plt.savefig(os.path.join(output_dir, 'soft_silhouette_' + str(i) + '.png'))</p><p class="source-code" lang="en-GB">    plt.close()</p><p class="source-code" lang="en-GB">    plt.figure()</p><p class="source-code" lang="en-GB">    plt.imshow(image_textured.detach().squeeze().cpu().numpy())</p><p class="source-code" lang="en-GB">    plt.title("iter: %d, loss: %0.2f" % (i, loss.data))</p><p class="source-code" lang="en-GB">    plt.axis("off")</p><p class="source-code" lang="en-GB">    plt.savefig(os.path.join(output_dir, 'soft_texture_' + str(i) + '.png'))</p><p class="source-code" lang="en-GB">    plt.close()</p><p class="source-code" lang="en-GB">    R = look_at_rotation(model.camera_position[None, :], device=model.device)</p><p class="source-code" lang="en-GB">    T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)</p><p class="source-code" lang="en-GB">    image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)</p><p class="source-code" lang="en-GB">    plt.figure()</p><p class="source-code" lang="en-GB">    plt.imshow(image[..., 3].detach().squeeze().cpu().numpy())</p><p class="source-code" lang="en-GB">    plt.title("iter: %d, loss: %0.2f" % (i, loss.data))</p><p class="source-code" lang="en-GB">    plt.axis("off")</p><p class="source-code" lang="en-GB">    plt.savefig(os.path.join(output_dir, 'hard_silhouette_' + str(i) + '.png'))</p><p class="source-code" lang="en-GB">    plt.close()</p><p class="source-code" lang="en-GB">    image = image[0, ..., :3].detach().squeeze().cpu().numpy()</p><p class="source-code" lang="en-GB">    image = img_as_ubyte(image)</p><p class="source-code" lang="en-GB">    plt.figure()</p><p class="source-code" lang="en-GB">    plt.imshow(image[..., :3])</p><p class="source-code" lang="en-GB">    plt.title("iter: %d, loss: %0.2f" % (i, loss.data))</p><p class="source-code" lang="en-GB">    plt.axis("off")</p><p class="source-code" lang="en-GB">    plt.savefig(os.path.join(output_dir, 'hard_texture_' + str(i) + '.png'))</p><p class="source-code" lang="en-GB">    plt.close()</p><p class="source-code" lang="en-GB">    if loss.item() &lt; 800:</p><p class="source-code" lang="en-GB">        break</p></li>
</ol>
<p lang="en-GB">The<a id="_idIndexMarker220"/> observed <a id="_idIndexMarker221"/>silhouette<a id="_idIndexMarker222"/> image<a id="_idIndexMarker223"/> is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><span class="No-Break" lang=""><em class="italic" lang="">.12</em></span><span class="No-Break" lang="">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 4.12: Observed silhouette image " height="865" src="image/B18217_04_12.jpg" width="1145"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12: Observed silhouette image</p>
<p lang="en-GB">The <a id="_idIndexMarker224"/>RGB<a id="_idIndexMarker225"/> image is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><span class="No-Break" lang=""><em class="italic" lang="">.13</em></span><span class="No-Break" lang="">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 4.13: Observed RGB image " height="850" src="image/B18217_04_13.jpg" width="1182"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13: Observed RGB image</p>
<p lang="en-GB">The<a id="_idIndexMarker226"/> rendered RGB images corresponding <a id="_idIndexMarker227"/>to the initial camera <a id="_idIndexMarker228"/>position <a id="_idIndexMarker229"/>and final fitted camera position are shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 4</em></span><em class="italic" lang="">.14</em> and <span class="No-Break" lang=""><em class="italic" lang="">Figure </em></span><span class="No-Break" lang=""><em class="italic" lang="">4</em></span><span class="No-Break" lang=""><em class="italic" lang="">.15</em></span><span class="No-Break" lang=""> respectively.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 4.14: Image corresponding to the initial camera position " height="162" src="image/B18217_04_14.jpg" width="525"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14: Image corresponding to the initial camera position</p>
<p lang="en-GB">The <a id="_idIndexMarker230"/>image<a id="_idIndexMarker231"/> corresponding <a id="_idIndexMarker232"/>to <a id="_idIndexMarker233"/>the final position is <span class="No-Break" lang="">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 4.15: Image corresponding to the fitted camera position " height="322" src="image/B18217_04_15.jpg" width="633"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15: Image corresponding to the fitted camera position</p>
<h1 id="_idParaDest-67" lang="en-GB"><a id="_idTextAnchor069"/>Summary</h1>
<p lang="en-GB">In this chapter, we started with the question of why differentiable rendering is needed. The answers to this question lie in the fact that rendering can be considered as a mapping from 3D scenes (meshes or point clouds) to 2D images. If rendering is made differentiable, then we can optimize 3D models directly with a properly chosen cost function between the rendered images and <span class="No-Break" lang="">observed images.</span></p>
<p lang="en-GB">We then discussed an approach to make rendering differentiable, which is implemented in the PyTorch3D library. We then discussed two concrete examples of object pose estimation being formulated as an optimization problem, where the object pose is directly optimized to minimize the mean-square errors between the rendered images and <span class="No-Break" lang="">observed images.</span></p>
<p lang="en-GB">We also went through the code examples, where PyTorch3D is used to solve optimization problems. In the next chapter, we will explore more variations of differentiable rendering and where we can <span class="No-Break" lang="">use it.</span></p>
</div>
</div></body></html>