<html><head></head><body>
<div id="_idContainer348">
<h1 class="chapter-number" id="_idParaDest-46" lang="en-GB"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-47" lang="en-GB"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.2.1">Deep Learning Fundamentals</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In this chapter, we will introduce </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">deep learning</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">DL</span></strong><span class="koboSpan" id="kobo.7.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">deep neural networks</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">DNNs</span></strong><span class="koboSpan" id="kobo.11.1">) – that is, </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">neural networks</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.14.1">NNs</span></strong><span class="koboSpan" id="kobo.15.1">) with multiple hidden layers. </span><span class="koboSpan" id="kobo.15.2">You might be wondering what the point of using more than one hidden layer is, given the universal approximation theorem. </span><span class="koboSpan" id="kobo.15.3">This is in no way a naive question, and for a long time, NNs were used in </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">that way.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.17.1">Without going into too much detail, one reason is that approximating a complex function might require a huge number of units in the hidden layer, making it impractical to use. </span><span class="koboSpan" id="kobo.17.2">There is also another, more important, reason for using deep networks, which is not directly related to the number of hidden layers, but to the level of learning. </span><span class="koboSpan" id="kobo.17.3">A deep network does not simply learn to predict output </span><em class="italic"><span class="koboSpan" id="kobo.18.1">Y</span></em><span class="koboSpan" id="kobo.19.1"> given input, </span><em class="italic"><span class="koboSpan" id="kobo.20.1">X</span></em><span class="koboSpan" id="kobo.21.1">; it also understands the basic features of the input. </span><span class="koboSpan" id="kobo.21.2">It’s able to learn abstractions of features of input samples, understand the basic characteristics of the samples, and make predictions based on those characteristics. </span><span class="koboSpan" id="kobo.21.3">This level of abstraction is missing in other basic </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">machine learning</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.24.1">ML</span></strong><span class="koboSpan" id="kobo.25.1">) algorithms and </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">shallow NNs.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.27.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">main topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.29.1">Introduction </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">to DL</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.31.1">Fundamental </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">DL concepts</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.33.1">Deep </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">neural networks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.35.1">Training </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">deep networks</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.37.1">Applications </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">of DL</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.39.1">Introducing popular </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">DL libraries</span></span></li>
</ul>
<h1 id="_idParaDest-48" lang="en-GB"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.41.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.42.1">We’ll implement the example in this chapter using Python, PyTorch, and Keras as part of </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">TF</span></strong><span class="koboSpan" id="kobo.46.1">). </span><span class="koboSpan" id="kobo.46.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.46.3">You can find the code examples in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03"><span class="No-Break"><span class="koboSpan" id="kobo.48.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter03</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.49.1">.</span></span></p>
<h1 id="_idParaDest-49" lang="en-GB"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.50.1">Introduction to DL</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.51.1">In 2012, Alex Krizhevsky, Ilya Sutskever, and </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.52.1">Geoffrey Hinton published a milestone paper titled </span><em class="italic"><span class="koboSpan" id="kobo.53.1">ImageNet Classification with Deep Convolutional Neural Networks</span></em><span class="koboSpan" id="kobo.54.1"> (</span><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"><span class="koboSpan" id="kobo.55.1">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</span></a><span class="koboSpan" id="kobo.56.1">). </span><span class="koboSpan" id="kobo.56.2">The paper describes their use of NNs to win the ImageNet competition of the same year, which we mentioned in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.57.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">At the end of their paper, they noted that the network’s performance degrades even if a single layer is removed. </span><span class="koboSpan" id="kobo.58.3">Their experiments demonstrated that removing any of the middle layers resulted in an about 2% top-1 accuracy loss of the model. </span><span class="koboSpan" id="kobo.58.4">They concluded that network depth is important for the performance of the network. </span><span class="koboSpan" id="kobo.58.5">The basic question is: what makes the network’s depth </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">so important?</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.60.1">A typical English saying is a picture is worth a thousand words. </span><span class="koboSpan" id="kobo.60.2">Let’s use this approach to understand what DL is. </span><span class="koboSpan" id="kobo.60.3">We’ll use images from the highly cited paper </span><em class="italic"><span class="koboSpan" id="kobo.61.1">Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations</span></em><span class="koboSpan" id="kobo.62.1"> (</span><a href="https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf"><span class="koboSpan" id="kobo.63.1">https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf</span></a><span class="koboSpan" id="kobo.64.1">). </span><span class="koboSpan" id="kobo.64.2">Here, the authors trained an NN with pictures of different categories of either objects or animals. </span><span class="koboSpan" id="kobo.64.3">The following figure shows how the different layers of the network learn different characteristics of the input data. </span><span class="koboSpan" id="kobo.64.4">In the first layer, the network learns to detect some small basic features, such as lines and edges, which are common for all images in </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">all categories:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer280">
<span class="koboSpan" id="kobo.66.1"><img alt="Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom) after training" src="image/B19627_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.67.1">Figure 3.1 – The ﬁrst layer weights (top) and the second layer weights (bottom) after training</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.68.1">But the next layers, which we can see in the following figure, combine those lines and edges to compose more complex features that are specific to each category. </span><span class="koboSpan" id="kobo.68.2">In the first row of the bottom-left image, we </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.69.1">can see how the network can detect different features of human faces, such as eyes, noses, and mouths. </span><span class="koboSpan" id="kobo.69.2">In the case of cars, these would be wheels, doors, and so on, as seen in the second image from the left in the following figure. </span><span class="koboSpan" id="kobo.69.3">These</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.70.1"> features are </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">abstract</span></strong><span class="koboSpan" id="kobo.72.1"> – that is, the network has learned the generic shape of a feature (such as a mouth or a nose) and can detect this feature in the input data, despite the variations it </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">might have:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<span class="koboSpan" id="kobo.74.1"><img alt="Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer (bottom) weights learned for a speciﬁc object category (class). Column 5 represents the weights learned for a mixture of four object categories (faces, cars, airplanes, and motorbikes)" src="image/B19627_03_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.75.1">Figure 3.2 – Columns 1 to 4 represent the second-layer (top) and third-layer (bottom) weights learned for a speciﬁc object category (class). </span><span class="koboSpan" id="kobo.75.2">Column 5 represents the weights learned for a mixture of four object categories (faces, cars, airplanes, and motorbikes)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.76.1">In the second row of the preceding figure, we can see how, in the deeper layers, the network combines these features in even more complex ones, such as faces and whole cars. </span><span class="koboSpan" id="kobo.76.2">One strength</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.77.1"> of DNNs is that they can learn these high-level abstract representations by themselves, deducting them from the </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">training data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.79.1">Next, let’s discuss these properties of DNNs in </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">more detail.</span></span></p>
<h1 id="_idParaDest-50" lang="en-GB"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.81.1">Fundamental DL concepts</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.82.1">In 1801, Joseph Marie </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.83.1">Charles invented the </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">Jacquard loom</span></strong><span class="koboSpan" id="kobo.85.1">. </span><span class="koboSpan" id="kobo.85.2">Charles was not a scientist, but simply a merchant. </span><span class="koboSpan" id="kobo.85.3">The Jacquard loom used a set of punched </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.86.1">cards, where each card represented a pattern to be reproduced on the loom. </span><span class="koboSpan" id="kobo.86.2">At the same time, each card was an abstract representation of that pattern. </span><span class="koboSpan" id="kobo.86.3">Punched cards have been used, for example, in the tabulating machine invented by Herman Hollerith in 1890, or in the first computers as a means to input code. </span><span class="koboSpan" id="kobo.86.4">In the tabulating machine, the cards were simply abstractions of samples to be fed into the machine to calculate statistics on a population. </span><span class="koboSpan" id="kobo.86.5">But in the Jacquard loom, their use was subtler, and each card represented the abstraction of a pattern that could be combined with others to create more complex patterns. </span><span class="koboSpan" id="kobo.86.6">The punched card is an abstract representation of a feature of reality, the final </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">weaved design.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.88.1">In a way, the Jacquard loom sowed the seeds of what DL is today, the definition of reality through the representations of its features. </span><span class="koboSpan" id="kobo.88.2">A DNN does not simply recognize what makes a cat a cat, or a squirrel a squirrel, but it understands what features are present in a cat and a squirrel, respectively. </span><span class="koboSpan" id="kobo.88.3">It learns to design a cat or a squirrel using those features. </span><span class="koboSpan" id="kobo.88.4">If we were to design a weaving pattern in the shape of a cat using a Jacquard loom, we would need to use punched cards that have whiskers on the nose, such as those of a cat, and an elegant and slender body. </span><span class="koboSpan" id="kobo.88.5">Conversely, if we were to design a squirrel, we would need to use a punched card that makes a furry tail. </span><span class="koboSpan" id="kobo.88.6">A deep network that learns basic representations of its output can make classifications using the assumptions it has made. </span><span class="koboSpan" id="kobo.88.7">For example, if there is no furry tail, it will probably not be a squirrel, but rather a cat. </span><span class="koboSpan" id="kobo.88.8">In this way, the amount of information the network learns is much more complete and robust, and the most exciting part is that DNNs learn to do </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">this automatically.</span></span></p>
<h2 id="_idParaDest-51" lang="en-GB"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.90.1">Feature learning</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.91.1">To illustrate how </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.92.1">DL works, let’s consider the task of recognizing a </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.93.1">simple geometric figure, such as a cube, as seen in the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer282">
<span class="koboSpan" id="kobo.95.1"><img alt="Figure 3.3 – An abstraction of an NN representing a cube. Diﬀerent layers encode features with diﬀerent levels of abstraction" src="image/B19627_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.96.1">Figure 3.3 – An abstraction of an NN representing a cube. </span><span class="koboSpan" id="kobo.96.2">Diﬀerent layers encode features with diﬀerent levels of abstraction</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.97.1">The cube is composed of edges (or lines), which intersect in vertices. </span><span class="koboSpan" id="kobo.97.2">Let’s say that each possible point in the three-dimensional space is associated with a unit (forget for a moment that this will require an infinite number of units). </span><span class="koboSpan" id="kobo.97.3">All the points/units are in the first (input) layer of a multilayer feedforward network. </span><span class="koboSpan" id="kobo.97.4">An input point/unit is active if the corresponding point lies on a line. </span><span class="koboSpan" id="kobo.97.5">The points/units that lie on a common line (edge) have strong positive connections to a single common edge/unit in the next layer. </span><span class="koboSpan" id="kobo.97.6">Conversely, they have negative connections to all other units in the next layer. </span><span class="koboSpan" id="kobo.97.7">The only exceptions are the units that lie on the vertices. </span><span class="koboSpan" id="kobo.97.8">Each such unit lies simultaneously on three edges and is connected to its three corresponding units in the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">subsequent layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.99.1">Now, we have two hidden layers, with different levels of abstraction – the first for points and the second for edges. </span><span class="koboSpan" id="kobo.99.2">However, this is not enough to encode a whole cube in the network. </span><span class="koboSpan" id="kobo.99.3">Let’s try this with another layer for vertices. </span><span class="koboSpan" id="kobo.99.4">Here, each three active edges/units of the second layer, which form a vertex, have a significant positive connection to a single common vertex/unit of the third layer. </span><span class="koboSpan" id="kobo.99.5">Since an edge of the cube forms two vertices, each edge/unit will have positive connections to two vertices/units and negative connections to all others. </span><span class="koboSpan" id="kobo.99.6">Finally, we’ll introduce the last hidden layer (the cube). </span><span class="koboSpan" id="kobo.99.7">The four vertices/units forming the cube will have positive connections to a single cube/unit from </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">the cube/layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.101.1">This cube representation example is oversimplified, but we can draw several conclusions from it. </span><span class="koboSpan" id="kobo.101.2">One of them is that DNNs lend themselves well to hierarchically organized data. </span><span class="koboSpan" id="kobo.101.3">For example, an image consists of pixels, which </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.102.1">form lines, edges, regions, and so on. </span><span class="koboSpan" id="kobo.102.2">This is also true for speech, where the building blocks are called </span><strong class="bold"><span class="koboSpan" id="kobo.103.1">phonemes</span></strong><span class="koboSpan" id="kobo.104.1">, as well as text, where we </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.105.1">have characters, words, </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">and sentences.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.107.1">In the preceding</span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.108.1"> example, we dedicated layers to specific cube features deliberately, but in practice, we wouldn’t do that. </span><span class="koboSpan" id="kobo.108.2">Instead, a deep network will “discover” features automatically during training. </span><span class="koboSpan" id="kobo.108.3">These features might not be immediately obvious and, in general, wouldn’t be interpretable by humans. </span><span class="koboSpan" id="kobo.108.4">Also, we wouldn’t know the level of the features encoded in the different layers of the network. </span><span class="koboSpan" id="kobo.108.5">Our example is more akin to classic ML algorithms, where the user has to use their own experience to select what they think are the best features. </span><span class="koboSpan" id="kobo.108.6">This process is called </span><strong class="bold"><span class="koboSpan" id="kobo.109.1">feature engineering</span></strong><span class="koboSpan" id="kobo.110.1">, and it can be labor-intensive and time-consuming. </span><span class="koboSpan" id="kobo.110.2">Allowing a network to automatically discover features is not only easier, but those features are highly abstract, which makes them less sensitive to noise. </span><span class="koboSpan" id="kobo.110.3">For example, human vision can recognize objects of different shapes, sizes, in different lighting conditions, and even when their view is partly obscured. </span><span class="koboSpan" id="kobo.110.4">We can recognize people with different haircuts and facial features, even when they wear a hat or a scarf that covers their mouth. </span><span class="koboSpan" id="kobo.110.5">Similarly, the abstract features the network learns will help it recognize faces better, even in more </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">challenging conditions.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.112.1">In the next section, we’ll discuss some of the reasons DL has become </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">so popular.</span></span></p>
<h2 id="_idParaDest-52" lang="en-GB"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.114.1">The reasons for DL’s popularity</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.115.1">If you’ve followed ML for some </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.116.1">time, you may have noticed that many DL algorithms are not new. </span><strong class="bold"><span class="koboSpan" id="kobo.117.1">Multilayer perceptrons</span></strong><span class="koboSpan" id="kobo.118.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.119.1">MLPs</span></strong><span class="koboSpan" id="kobo.120.1">) have been around for nearly </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.121.1">50 years. </span><span class="koboSpan" id="kobo.121.2">Backpropagation was discovered a couple of times but finally gained recognition in 1986. </span><span class="koboSpan" id="kobo.121.3">Yann LeCun, a famous computer scientist, perfected his work on convolutional networks in the 1990s. </span><span class="koboSpan" id="kobo.121.4">In 1997, Sepp Hochreiter and Jürgen Schmidhuber invented long short-term memory, a type of recurrent NN still in use today. </span><span class="koboSpan" id="kobo.121.5">In this section, we’ll try to understand why we have AI summer now, and </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.122.1">why we only had AI winters (</span><a href="https://en.wikipedia.org/wiki/AI_winter"><span class="No-Break"><span class="koboSpan" id="kobo.123.1">https://en.wikipedia.org/wiki/AI_winter</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.124.1">) before.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.125.1">The first reason is that today, we have a lot more data than in the past. </span><span class="koboSpan" id="kobo.125.2">The rise of the internet and software in different industries has generated a lot of computer-accessible data. </span><span class="koboSpan" id="kobo.125.3">We also have more benchmark datasets, such as ImageNet. </span><span class="koboSpan" id="kobo.125.4">With this comes the desire to extract value from that data by analyzing it. </span><span class="koboSpan" id="kobo.125.5">And, as we’ll see later, DL algorithms work better when they are trained with a lot </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">of data.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.127.1">The second reason</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.128.1"> is the increased computing power. </span><span class="koboSpan" id="kobo.128.2">This is most visible in the drastically increased </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.129.1">processing capacity of </span><strong class="bold"><span class="koboSpan" id="kobo.130.1">graphical processing units</span></strong><span class="koboSpan" id="kobo.131.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.132.1">GPUs</span></strong><span class="koboSpan" id="kobo.133.1">). </span><span class="koboSpan" id="kobo.133.2">NNs are organized in such a way as to take advantage of this parallel architecture. </span><span class="koboSpan" id="kobo.133.3">Let’s see why. </span><span class="koboSpan" id="kobo.133.4">As we learned in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.135.1">, units from a network layer are not connected to units from the same layer. </span><span class="koboSpan" id="kobo.135.2">We also learned that we could represent many layer operations as matrix multiplications. </span><span class="koboSpan" id="kobo.135.3">Matrix multiplication is embarrassingly parallel (trust me, this is a term – you can Google it!). </span><span class="koboSpan" id="kobo.135.4">The computation of each output cell is not related to the computation of any other output cell. </span><span class="koboSpan" id="kobo.135.5">Therefore, we can compute all of the outputs in parallel. </span><span class="koboSpan" id="kobo.135.6">Not coincidentally, GPUs are well suited for highly parallel operations like this. </span><span class="koboSpan" id="kobo.135.7">On the one hand, a GPU has a high number of computational cores compared to a </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">central processing unit</span></strong><span class="koboSpan" id="kobo.137.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.138.1">CPU</span></strong><span class="koboSpan" id="kobo.139.1">). </span><span class="koboSpan" id="kobo.139.2">Even though a </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.140.1">CPU core is faster than a GPU one, we can still compute a lot more output cells in parallel. </span><span class="koboSpan" id="kobo.140.2">But what’s even more important is that GPUs are optimized for memory bandwidth, while CPUs are optimized for latency. </span><span class="koboSpan" id="kobo.140.3">This means that a CPU can fetch small chunks of memory very quickly but will be slow when it comes to fetching large chunks. </span><span class="koboSpan" id="kobo.140.4">The GPU does the opposite. </span><span class="koboSpan" id="kobo.140.5">For matrix multiplication in a deep network with a lot of wide layers, bandwidth becomes the bottleneck, not latency. </span><span class="koboSpan" id="kobo.140.6">In addition, the L1 cache of the GPU is much faster than the L1 cache for the CPU and is also larger. </span><span class="koboSpan" id="kobo.140.7">The L1 cache represents the memory of the information that the program is </span><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.141.1">likely to use next, and storing this data can speed up the process. </span><span class="koboSpan" id="kobo.141.2">Much of the memory gets reused in DNNs, which is why L1 cache memory </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">is important.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.143.1">In the next section, </span><em class="italic"><span class="koboSpan" id="kobo.144.1">Deep neural networks</span></em><span class="koboSpan" id="kobo.145.1">, we’ll give a more precise definition of the key NN architectures that will be thoroughly introduced in the </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">coming chapters.</span></span></p>
<h1 id="_idParaDest-53" lang="en-GB"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.147.1">Deep neural networks</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.148.1">We could define DL as a class of ML techniques, where information is processed in hierarchical layers to understand representations and features from data in increasing levels of complexity. </span><span class="koboSpan" id="kobo.148.2">In practice, all DL algorithms are NNs, which share some common basic properties. </span><span class="koboSpan" id="kobo.148.3">They all</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.149.1"> consist of a graph of interconnected operations, which operate with input/output tensors. </span><span class="koboSpan" id="kobo.149.2">Where they differ is network architecture (or the way units are organized in the network), and sometimes in the way they are trained. </span><span class="koboSpan" id="kobo.149.3">With that in mind, let’s look at the main classes of NNs. </span><span class="koboSpan" id="kobo.149.4">The following list is not exhaustive, but it represents most NN types in </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">use today:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.151.1">Multilayer perceptron</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">MLP</span></strong><span class="koboSpan" id="kobo.154.1">): An NN </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.155.1">with feedforward propagation, fully connected layers, and at least one hidden layer. </span><span class="koboSpan" id="kobo.155.2">We introduced MLPs in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.156.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.157.1">.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.158.1">Convolutional neural network</span></strong><span class="koboSpan" id="kobo.159.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.160.1">CNN</span></strong><span class="koboSpan" id="kobo.161.1">): A CNN is a feedforward NN with several types of special layers. </span><span class="koboSpan" id="kobo.161.2">For example, convolutional layers apply a filter to the input image (or sound) by </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.162.1">sliding that filter all across the incoming signal, to produce an </span><em class="italic"><span class="koboSpan" id="kobo.163.1">n</span></em><span class="koboSpan" id="kobo.164.1">-dimensional activation map. </span><span class="koboSpan" id="kobo.164.2">There is some evidence that units in CNNs are organized similarly to how biological cells are organized in the visual cortex of the brain. </span><span class="koboSpan" id="kobo.164.3">We’ve mentioned CNNs several times so far, and that’s not a coincidence – today, they outperform all other ML al</span><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.165.1">gorithms on many computer vision and NLP tasks. </span><span class="koboSpan" id="kobo.165.2">We’ll discuss CNNs in </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.166.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.167.1">.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.168.1">Recurrent neural network</span></strong><span class="koboSpan" id="kobo.169.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.170.1">RNN</span></strong><span class="koboSpan" id="kobo.171.1">): This type of NN has an internal state (or memory), which is based on</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.172.1"> all, or part of, the input data that’s already been fed to the network. </span><span class="koboSpan" id="kobo.172.2">The output of a recurrent network is a combination of its internal state (memory of inputs) and the latest input sample. </span><span class="koboSpan" id="kobo.172.3">At the same time, the internal state changes to incorporate newly input data. </span><span class="koboSpan" id="kobo.172.4">Because of these properties, recurrent networks are good candidates for tasks that work on sequential data, such as text or time series data. </span><span class="koboSpan" id="kobo.172.5">We’ll discuss recurrent networks in </span><a href="B19627_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.173.1">Chapter 6</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.174.1">.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.175.1">Transformer</span></strong><span class="koboSpan" id="kobo.176.1">: Like RNNs, the</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.177.1"> transformer is suited to work with sequential data. </span><span class="koboSpan" id="kobo.177.2">It uses a</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.178.1"> mechanism called </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">attention</span></strong><span class="koboSpan" id="kobo.180.1">, which allows it </span><em class="italic"><span class="koboSpan" id="kobo.181.1">direct simultaneous access</span></em><span class="koboSpan" id="kobo.182.1"> to all elements of the input sequence. </span><span class="koboSpan" id="kobo.182.2">This is unlike an RNN, which processes the sequence elements one by one and updates its internal state after each element. </span><span class="koboSpan" id="kobo.182.3">As we’ll see in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.183.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.184.1">, the attention mechanism has several major advantages over the classic RNNs. </span><span class="koboSpan" id="kobo.184.2">Because of this, in recent years, transformers have superseded RNNs in </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">many tasks.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.186.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.187.1">: As we mentioned in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.188.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.189.1">, autoencoders are a class of unsupervised learning algorithms, in </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.190.1">which the output shape is the same as the input, which allows the network to better learn </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">basic representations.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.192.1">Now that we’ve outlined the major types of DNNs, let’s discuss how to </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">train them.</span></span></p>
<h1 id="_idParaDest-54" lang="en-GB"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.194.1">Training deep neural networks</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.195.1">Historically, the scientific community has understood that deeper networks have greater representational power compared to shallow ones. </span><span class="koboSpan" id="kobo.195.2">However, there were various challenges in training networks </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.196.1">with more than a few hidden layers. </span><span class="koboSpan" id="kobo.196.2">We now know that we can successfully train DNNs using a combination of gradient descent and backpropagation, just as we discussed in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.197.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.198.1">. </span><span class="koboSpan" id="kobo.198.2">In this section, we’ll see how to improve them so that we can solve some of the problems that exist uniquely for DNNs and not </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">shallow NNs.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.200.1">The first edition of this book</span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.201.1"> included networks such as </span><strong class="bold"><span class="koboSpan" id="kobo.202.1">Restricted Boltzmann Machines</span></strong><span class="koboSpan" id="kobo.203.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.204.1">RBMs</span></strong><span class="koboSpan" id="kobo.205.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.206.1">Deep Belief Networks</span></strong><span class="koboSpan" id="kobo.207.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.208.1">DBNs</span></strong><span class="koboSpan" id="kobo.209.1">). </span><span class="koboSpan" id="kobo.209.2">They were popularized by</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.210.1"> Geoffrey Hinton, a Canadian scientist, and one of the most prominent DL researchers. </span><span class="koboSpan" id="kobo.210.2">Back in 1986, he was also one of the inventors of backpropagation. </span><span class="koboSpan" id="kobo.210.3">RBMs are a special type of generative NN, where the units are organized into two layers, namely visible and hidden. </span><span class="koboSpan" id="kobo.210.4">Unlike feedforward networks, the data in an RBM can flow in both directions – from visible to hidden units, and vice versa. </span><span class="koboSpan" id="kobo.210.5">In 2002, Prof. </span><span class="koboSpan" id="kobo.210.6">Hinton introduced </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">contrastive divergence</span></strong><span class="koboSpan" id="kobo.212.1">, which is </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.213.1">an unsupervised algorithm for training RBMs. </span><span class="koboSpan" id="kobo.213.2">In 2006, he introduced </span><strong class="bold"><span class="koboSpan" id="kobo.214.1">deep belief networks</span></strong><span class="koboSpan" id="kobo.215.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.216.1">DBNs</span></strong><span class="koboSpan" id="kobo.217.1">), which are DNNs that are formed by stacking multiple RBMs. </span><span class="koboSpan" id="kobo.217.2">Thanks to their novel training algorithm, it was possible to create a DBN with more hidden layers than had previously been possible. </span><span class="koboSpan" id="kobo.217.3">But even with contrastive divergence, training a DBN is not easy. </span><span class="koboSpan" id="kobo.217.4">It is a </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">two-step process:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.219.1">First, we have to train </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.220.1">each RBM with contrastive divergence, and </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.221.1">gradually stack them on top of each other. </span><span class="koboSpan" id="kobo.221.2">This phase is </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.223.1">pre-training</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.225.1">In effect, pre-training serves as a sophisticated weight initialization algorithm for the next phase, called </span><strong class="bold"><span class="koboSpan" id="kobo.226.1">fine-tuning</span></strong><span class="koboSpan" id="kobo.227.1">. </span><span class="koboSpan" id="kobo.227.2">With</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.228.1"> fine-tuning, we transform the DBN into a regular MLP and continue training it using supervised backpropagation and gradient descent, in the same way as we saw in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.229.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.230.1">.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.231.1">Thanks to some algorithmic advances, it’s now possible to train deep networks using plain old backpropagation, thus effectively eliminating the pre-training phase. </span><span class="koboSpan" id="kobo.231.2">These advances rendered DBNs and RBMs obsolete. </span><span class="koboSpan" id="kobo.231.3">They are, without a doubt, interesting from a research perspective, but they are rarely used in practice anymore and we’ll omit them from </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">this edition.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.233.1">Next, let’s discuss the algorithmic advances that made training of NNs with </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">backpropagation possible.</span></span></p>
<h2 id="_idParaDest-55" lang="en-GB"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.235.1">Improved activation functions</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.236.1">But why is training deep networks so hard? </span><span class="koboSpan" id="kobo.236.2">One of </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.237.1">the main challenges that pre-training solved is the so-called </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">vanishing gradients</span></strong><span class="koboSpan" id="kobo.239.1"> problem. </span><span class="koboSpan" id="kobo.239.2">To understand it, we’ll assume that we’ll use backpropagation</span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.240.1"> to train a regular MLP with multiple hidden layers and logistic sigmoid activation at each layer. </span><span class="koboSpan" id="kobo.240.2">Let’s focus on the sigmoid function (the same applies to tanh). </span><span class="koboSpan" id="kobo.240.3">As a reminder, it is computed </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">as </span></span><span class="No-Break"><span class="koboSpan" id="kobo.242.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/233.png" style="vertical-align:-0.134em;height:0.798em;width:3.975em"/></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.243.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/234.png" style="vertical-align:-0.062em;height:0.746em;width:3.336em"/></span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer285">
<span class="koboSpan" id="kobo.245.1"><img alt="Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted) (left); consecutive sigmoid activations, which “squash” the data (right)" src="image/B19627_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.246.1">Figure 3.4 – Logistic sigmoid (uninterrupted) and its derivative (interrupted) (left); consecutive sigmoid activations, which “squash” the data (right)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.247.1">The vanishing gradients manifest themselves in the </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">following ways:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.249.1">In the forward phase, the outputs of the first sigmoid layer are represented by the blue uninterrupted line (both left and right images in the preceding figure) and fall in the range (0, 1). </span><span class="koboSpan" id="kobo.249.2">The dotted lines on the right image represent the sigmoid activations of each of the consecutive layers after the first. </span><span class="koboSpan" id="kobo.249.3">Even after three layers, we can see that the activation is “squashed” in a narrow range and converges to </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.250.1">around 0.66, regardless of the input value. </span><span class="koboSpan" id="kobo.250.2">For example, if the input value of the first layer is 2, then </span><span class="koboSpan" id="kobo.251.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.881&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/235.png" style="vertical-align:-0.050em;height:0.734em;width:5.384em"/></span><span class="koboSpan" id="kobo.252.1">, </span><span class="koboSpan" id="kobo.253.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.71&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/236.png" style="vertical-align:-0.100em;height:0.834em;width:6.088em"/></span><span class="koboSpan" id="kobo.254.1">, </span><span class="koboSpan" id="kobo.255.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.67&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/237.png" style="vertical-align:-0.150em;height:0.933em;width:7.212em"/></span><span class="koboSpan" id="kobo.256.1">, and so on. </span><span class="koboSpan" id="kobo.256.2">This peculiarity of the sigmoid function acts as an eraser of any information coming from the </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">preceding layers.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.258.1">We now know that to train an NN, we need to compute the derivative of the activation function (along with all the other derivatives) for the backward phase. </span><span class="koboSpan" id="kobo.258.2">The derivative of the sigmoid function is represented by the green interrupted line on the left image in the preceding figure. </span><span class="koboSpan" id="kobo.258.3">We can see that it has a significant value in a very narrow interval, centered around 0, and converges toward 0 in all other cases. </span><span class="koboSpan" id="kobo.258.4">In networks with many layers, the derivative would likely converge to 0 when propagated to the first layers of the network. </span><span class="koboSpan" id="kobo.258.5">Effectively, this means we cannot propagate the error to these layers and we cannot update their weights in a </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">meaningful way.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.260.1">Thankfully, the </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">ReLU</span></strong><span class="koboSpan" id="kobo.262.1"> activation we </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.263.1">introduced in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.265.1"> can solve both of these problems with a single stroke. </span><span class="koboSpan" id="kobo.265.2">To recap, the following figure shows the ReLU graph and </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">its derivative:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer289">
<span class="koboSpan" id="kobo.267.1"><img alt="Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)" src="image/B19627_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.268.1">Figure 3.5 – ReLU activation (uninterrupted) and its derivative (interrupted)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.269.1">ReLU has the following </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">desirable</span></span><span class="No-Break"><a id="_idIndexMarker368"/></span><span class="No-Break"><span class="koboSpan" id="kobo.271.1"> properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.272.1">It is </span><strong class="bold"><span class="koboSpan" id="kobo.273.1">idempotent</span></strong><span class="koboSpan" id="kobo.274.1">. </span><span class="koboSpan" id="kobo.274.2">If we pass a value through an arbitrary number of ReLU activations, it will not change; for example, </span><em class="italic"><span class="koboSpan" id="kobo.275.1">ReLU(2) = 2</span></em><span class="koboSpan" id="kobo.276.1">, </span><em class="italic"><span class="koboSpan" id="kobo.277.1">ReLU(ReLU(2)) = 2</span></em><span class="koboSpan" id="kobo.278.1">, and so on. </span><span class="koboSpan" id="kobo.278.2">This is not the case for a sigmoid. </span><span class="koboSpan" id="kobo.278.3">The idempotence of ReLU makes it theoretically possible to create networks with more layers compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">the sigmoid.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.280.1">We can also see that its derivative is either 0 or 1, regardless of the backpropagated value. </span><span class="koboSpan" id="kobo.280.2">In this way, we can avoid vanishing gradients in the backward pass as well. </span><span class="koboSpan" id="kobo.280.3">Strictly speaking, the derivative ReLU at value 0 is undefined, which makes the ReLU only semi-differentiable (more information about this can be found at </span><a href="https://en.wikipedia.org/wiki/Semi-differentiability"><span class="koboSpan" id="kobo.281.1">https://en.wikipedia.org/wiki/Semi-differentiability</span></a><span class="koboSpan" id="kobo.282.1">). </span><span class="koboSpan" id="kobo.282.2">But in practice, it works </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">well enough.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.284.1">It creates sparse activations. </span><span class="koboSpan" id="kobo.284.2">Let’s assume that the weights of the network are initialized randomly through normal distribution. </span><span class="koboSpan" id="kobo.284.3">Here, there is a 0.5 chance that the input for each ReLU unit is &lt; 0. </span><span class="koboSpan" id="kobo.284.4">Therefore, the output of about half of all activations will also be 0. </span><span class="koboSpan" id="kobo.284.5">The sparse activations have several advantages, which we can roughly summarize as Occam’s razor in the context of NNs – it’s better to achieve the same result with a simpler data representation than a </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">complex one.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.286.1">It’s faster to compute in both the forward and </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">backward passes.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.288.1">Despite these ReLU advantages, during training, the network weights can be updated in such a way that some of the ReLU units in a layer will always receive inputs smaller than 0, which, in turn, will cause them to permanently output 0 as well. </span><span class="koboSpan" id="kobo.288.2">This phenomenon is known as </span><strong class="bold"><span class="koboSpan" id="kobo.289.1">dying ReLUs</span></strong><span class="koboSpan" id="kobo.290.1">. </span><span class="koboSpan" id="kobo.290.2">To solve</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.291.1"> this, several ReLU modifications have been proposed. </span><span class="koboSpan" id="kobo.291.2">The following is a </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">non-exhaustive list:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.293.1">Leaky ReLU</span></strong><span class="koboSpan" id="kobo.294.1">: When the</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.295.1"> input is larger than 0, leaky ReLU repeats its input in the</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.296.1"> same way as the regular ReLU does. </span><span class="koboSpan" id="kobo.296.2">However, when </span><em class="italic"><span class="koboSpan" id="kobo.297.1">x &lt; 0</span></em><span class="koboSpan" id="kobo.298.1">, the leaky ReLU outputs </span><em class="italic"><span class="koboSpan" id="kobo.299.1">x</span></em><span class="koboSpan" id="kobo.300.1"> multiplied by some constant, α (</span><em class="italic"><span class="koboSpan" id="kobo.301.1">0 &lt; α &lt; 1</span></em><span class="koboSpan" id="kobo.302.1">), instead of 0. </span><span class="koboSpan" id="kobo.302.2">The following diagram shows the leaky ReLU formula, its derivative, and their graphs for </span><em class="italic"><span class="koboSpan" id="kobo.303.1">α = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.304.1">0.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer290">
<span class="koboSpan" id="kobo.306.1"><img alt="Figure 3.6 – The leaky ReLU activation function" src="image/B19627_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.307.1">Figure 3.6 – The leaky ReLU activation function</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.308.1">Parametric ReLU</span></strong><span class="koboSpan" id="kobo.309.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.310.1">PReLU</span></strong><span class="koboSpan" id="kobo.311.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.312.1">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</span></em><span class="koboSpan" id="kobo.313.1">, </span><a href="https://arxiv.org/abs/1502.01852):"><span class="koboSpan" id="kobo.314.1">https://arxiv.org/abs/1502.01852):</span></a><span class="koboSpan" id="kobo.315.1"> This</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.316.1"> activation is the same as the leaky ReLU, but </span><em class="italic"><span class="koboSpan" id="kobo.317.1">α</span></em><span class="koboSpan" id="kobo.318.1"> is tunable and is adjusted </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">during training.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.320.1">Exponential linear unit</span></strong><span class="koboSpan" id="kobo.321.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.322.1">ELU</span></strong><span class="koboSpan" id="kobo.323.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.324.1">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</span></em><span class="koboSpan" id="kobo.325.1">, </span><a href="https://arxiv.org/abs/1511.07289):"><span class="koboSpan" id="kobo.326.1">https://arxiv.org/abs/1511.07289):</span></a><span class="koboSpan" id="kobo.327.1"> When the input is larger than 0, ELU </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.328.1">repeats its input in the same way as ReLU does. </span><span class="koboSpan" id="kobo.328.2">However, when </span><em class="italic"><span class="koboSpan" id="kobo.329.1">x &lt; 0</span></em><span class="koboSpan" id="kobo.330.1">, the ELU output becomes </span><span class="koboSpan" id="kobo.331.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/238.png" style="vertical-align:-0.257em;height:0.968em;width:6.468em"/></span><span class="koboSpan" id="kobo.332.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.333.1">α</span></em><span class="koboSpan" id="kobo.334.1"> is a tunable parameter. </span><span class="koboSpan" id="kobo.334.2">The following diagram shows the ELU formula, its derivative, and their graphs for </span><em class="italic"><span class="koboSpan" id="kobo.335.1">α = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.336.1">0.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer292">
<span class="koboSpan" id="kobo.338.1"><img alt="Figure 3.7 – The ELU activation function" src="image/B19627_03_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.339.1">Figure 3.7 – The ELU activation function</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.340.1">Scaled exponential linear unit</span></strong><span class="koboSpan" id="kobo.341.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.342.1">SELU</span></strong><span class="koboSpan" id="kobo.343.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.344.1">Self-Normalizing Neural Networks</span></em><span class="koboSpan" id="kobo.345.1">, </span><a href="https://arxiv.org/abs/1706.02515):"><span class="koboSpan" id="kobo.346.1">https://arxiv.org/abs/1706.02515):</span></a><span class="koboSpan" id="kobo.347.1"> This activation is like ELU, except that the</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.348.1"> output (both smaller </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.349.1">and larger than 0) is scaled with an additional </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.350.1">training parameter, </span><em class="italic"><span class="koboSpan" id="kobo.351.1">λ</span></em><span class="koboSpan" id="kobo.352.1">. </span><span class="koboSpan" id="kobo.352.2">The SELU is part of a</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.353.1"> larger concept called </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">self-normalizing NNs</span></strong><span class="koboSpan" id="kobo.355.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.356.1">SNNs</span></strong><span class="koboSpan" id="kobo.357.1">), which is described in the </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">source paper.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.359.1">Sigmoid Linear Unit (SiLU)</span></strong><span class="koboSpan" id="kobo.360.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.361.1">Gaussian Error Linear Unit</span></strong><span class="koboSpan" id="kobo.362.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.363.1">GELU</span></strong><span class="koboSpan" id="kobo.364.1">; see</span><em class="italic"><span class="koboSpan" id="kobo.365.1"> Gaussian Error Linear Units (GELUs)</span></em><span class="koboSpan" id="kobo.366.1">, </span><a href="https://arxiv.org/abs/1606.08415"><span class="koboSpan" id="kobo.367.1">https://arxiv.org/abs/1606.08415</span></a><span class="koboSpan" id="kobo.368.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.369.1">Swish</span></strong><span class="koboSpan" id="kobo.370.1"> (see </span><em class="italic"><span class="koboSpan" id="kobo.371.1">Searching for Activation Functions</span></em><span class="koboSpan" id="kobo.372.1">, </span><a href="https://arxiv.org/abs/1710.05941"><span class="koboSpan" id="kobo.373.1">https://arxiv.org/abs/1710.05941</span></a><span class="koboSpan" id="kobo.374.1">): This is a</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.375.1"> collection of three similar (but not the same) functions that closely resemble ReLU but are differentiable at the 0 point. </span><span class="koboSpan" id="kobo.375.2">For the sake of simplicity, we’ll only show the SiLU graph (</span><em class="italic"><span class="koboSpan" id="kobo.376.1">σ</span></em><span class="koboSpan" id="kobo.377.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">sigmoid function):</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer293">
<span class="koboSpan" id="kobo.379.1"><img alt="Figure 3.8 – The SiLU activation function" src="image/B19627_03_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.380.1">Figure 3.8 – The SiLU activation function</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.381.1">Finally, we have softmax, which is the activation function of the output layer in classification problems. </span><span class="koboSpan" id="kobo.381.2">Let’s assume that the output of the final network layer is a vector, </span><span class="koboSpan" id="kobo.382.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/239.png" style="vertical-align:-0.390em;height:0.901em;width:5.776em"/></span><span class="koboSpan" id="kobo.383.1">. </span><span class="koboSpan" id="kobo.383.2">Each of the </span><em class="italic"><span class="koboSpan" id="kobo.384.1">n</span></em><span class="koboSpan" id="kobo.385.1"> elements represents one of </span><em class="italic"><span class="koboSpan" id="kobo.386.1">n</span></em><span class="koboSpan" id="kobo.387.1"> classes, to which the input sample might belong. </span><span class="koboSpan" id="kobo.387.2">To determine the network prediction, we’ll take the index, </span><em class="italic"><span class="koboSpan" id="kobo.388.1">i</span></em><span class="koboSpan" id="kobo.389.1">, of the highest value, </span><span class="koboSpan" id="kobo.390.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/240.png" style="vertical-align:-0.340em;height:0.802em;width:0.579em"/></span><span class="koboSpan" id="kobo.391.1">, and</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.392.1"> assign the input sample to the class it represents. </span><span class="koboSpan" id="kobo.392.2">However, we can also interpret the network output as a probability distribution of a discrete random variable – that is, each value, </span><span class="koboSpan" id="kobo.393.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/241.png" style="vertical-align:-0.340em;height:0.802em;width:0.589em"/></span><span class="koboSpan" id="kobo.394.1">, represents the probability that the input sample belongs to that particular class. </span><span class="koboSpan" id="kobo.394.2">To help us with this, we’ll use the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">softmax activation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.396.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/242.png" style="vertical-align:-1.001em;height:2.198em;width:6.902em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.397.1">It has the </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.399.1">The denominator in the formula acts as a normalizer. </span><span class="koboSpan" id="kobo.399.2">This is important for the probability interpretation we </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">just introduced:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.401.1">Every value, </span><span class="koboSpan" id="kobo.402.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/243.png" style="vertical-align:-0.390em;height:1.101em;width:1.370em"/></span><span class="koboSpan" id="kobo.403.1">, is constrained within the [0, 1] range, which allows us to treat it as </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">a probability</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.405.1">The total sum of values of </span><span class="koboSpan" id="kobo.406.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/244.png" style="vertical-align:-0.390em;height:1.101em;width:1.724em"/></span><span class="koboSpan" id="kobo.407.1"> is equal to 1: </span><span class="koboSpan" id="kobo.408.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow /&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/245.png" style="vertical-align:-0.536em;height:1.298em;width:5.241em"/></span><span class="koboSpan" id="kobo.409.1">, which also aligns with the </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">probability interpretation</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.411.1">A bonus (in fact, obligatory) is that the function </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">is differentiable.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.413.1">The softmax activation has one more subtle property. </span><span class="koboSpan" id="kobo.413.2">Before we normalize the data, we transform each vector component exponentially with </span><span class="koboSpan" id="kobo.414.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/246.png" style="vertical-align:-0.012em;height:0.614em;width:0.783em"/></span><span class="koboSpan" id="kobo.415.1">. </span><span class="koboSpan" id="kobo.415.2">Let’s imagine that two of the vector components are </span><span class="koboSpan" id="kobo.416.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/247.png" style="vertical-align:-0.333em;height:0.967em;width:2.673em"/></span><span class="koboSpan" id="kobo.417.1"> a</span><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.418.1">nd </span><span class="koboSpan" id="kobo.419.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/248.png" style="vertical-align:-0.333em;height:0.967em;width:2.776em"/></span><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.420.1">. </span><span class="koboSpan" id="kobo.420.2">Here, we would have exp(1) = 2.7 and exp(2) = 7.39. </span><span class="koboSpan" id="kobo.420.3">As we can see, the ratios between the components before and after the transformation are very different – 0.5 and 0.36. </span><span class="koboSpan" id="kobo.420.4">In effect, the softmax function increases the probability of higher scores compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">lower ones.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.422.1">In practice, </span><strong class="bold"><span class="koboSpan" id="kobo.423.1">softmax</span></strong><span class="koboSpan" id="kobo.424.1"> is often used in </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.425.1">combination </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.426.1">with the </span><strong class="bold"><span class="koboSpan" id="kobo.427.1">cross-entropy loss</span></strong><span class="koboSpan" id="kobo.428.1"> function. </span><span class="koboSpan" id="kobo.428.2">It compares the difference between the estimated class probabilities and the actual class distribution (the</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.429.1"> difference is known as cross-entropy). </span><span class="koboSpan" id="kobo.429.2">We can define the cross-entropy loss for a single training sample </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.431.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/249.png" style="vertical-align:-0.914em;height:2.224em;width:11.764em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.432.1">Here, </span><span class="koboSpan" id="kobo.433.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/250.png" style="vertical-align:-0.483em;height:1.052em;width:1.791em"/></span><span class="koboSpan" id="kobo.434.1"> is the estimated probability of the output belonging to class </span><em class="italic"><span class="koboSpan" id="kobo.435.1">j</span></em><span class="koboSpan" id="kobo.436.1"> (out of </span><em class="italic"><span class="koboSpan" id="kobo.437.1">n</span></em><span class="koboSpan" id="kobo.438.1"> total classes) and </span><span class="koboSpan" id="kobo.439.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/251.png" style="vertical-align:-0.483em;height:1.001em;width:0.622em"/></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.440.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/252.png" style="vertical-align:-0.134em;height:0.702em;width:1.158em"/></span></span><span class="koboSpan" id="kobo.441.1"> is the actual probability. </span><span class="koboSpan" id="kobo.441.2">The actual distribution, </span><em class="italic"><span class="koboSpan" id="kobo.442.1">P(X)</span></em><span class="koboSpan" id="kobo.443.1">, is usually a one-hot-encoded vector, where the real class has a probability of 1, and all others have a probability of 0. </span><span class="koboSpan" id="kobo.443.2">In this case, the cross-entropy loss will only capture the error on the target class and will discard all </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">other errors.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.445.1">Now that we’ve learned how to prevent vanishing gradients and we’re able to interpret the NN output as a probability distribution, we’ll focus on the next challenge in front of DNNs – </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">overfitting.</span></span></p>
<h2 id="_idParaDest-56" lang="en-GB"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.447.1">DNN regularization</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.448.1">So far, we’ve learned that an</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.449.1"> NN can approximate any function. </span><span class="koboSpan" id="kobo.449.2">But with great power comes great responsibility. </span><span class="koboSpan" id="kobo.449.3">The NN may learn to approximate the noise of the target function rather than its useful components. </span><span class="koboSpan" id="kobo.449.4">For example, imagine that we are training an NN to classify whether an image contains a car or not, but for some reason, the training set contains mostly red cars. </span><span class="koboSpan" id="kobo.449.5">It may turn out that the NN will associate the color red with the car, rather than its shape. </span><span class="koboSpan" id="kobo.449.6">Now, if the network sees a green car in inference mode, it may not recognize it as such because the color doesn’t match. </span><span class="koboSpan" id="kobo.449.7">As we discussed in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.450.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.451.1">, this problem is referred to as overfitting and it is central to ML (and even more so in deep networks). </span><span class="koboSpan" id="kobo.451.2">In this </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.452.1">section, we’ll discuss several ways to prevent it. </span><span class="koboSpan" id="kobo.452.2">Such techniques are collectively known </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">as regularization.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.454.1">In the context of NNs, these regularization techniques usually impose some artificial limitations or obstacles on the training process to prevent the network from approximating the target function too closely. </span><span class="koboSpan" id="kobo.454.2">They try to guide the network to learn generic rather than specific approximation of the target function in the hope that this representation will generalize well on previously unseen examples of the test dataset. </span><span class="koboSpan" id="kobo.454.3">Let’s start with regularization techniques that apply to the input data before we feed it to </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">the NN:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.456.1">Min-max normalization</span></strong><span class="koboSpan" id="kobo.457.1">: </span><span class="koboSpan" id="kobo.458.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/253.png" style="vertical-align:-0.256em;height:1.039em;width:4.266em"/></span><span class="koboSpan" id="kobo.459.1">. </span><span class="koboSpan" id="kobo.459.2">Here, </span><em class="italic"><span class="koboSpan" id="kobo.460.1">x</span></em><span class="koboSpan" id="kobo.461.1"> is a single element of the input vector, </span><span class="koboSpan" id="kobo.462.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/254.png" style="vertical-align:-0.340em;height:0.788em;width:1.366em"/></span><span class="koboSpan" id="kobo.463.1"> is the </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.464.1">smallest element of the training dataset, and </span><span class="koboSpan" id="kobo.465.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/255.png" style="vertical-align:-0.340em;height:0.788em;width:1.474em"/></span><span class="koboSpan" id="kobo.466.1"> is the largest element. </span><span class="koboSpan" id="kobo.466.2">This operation scales all the inputs in the [0, 1] range. </span><span class="koboSpan" id="kobo.466.3">For example, a grayscale image will have a min color value of 0 and a max color value of 255. </span><span class="koboSpan" id="kobo.466.4">Then, a pixel with an intensity of 125 would have a scaled value of </span><span class="koboSpan" id="kobo.467.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;125&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;255&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.49&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/256.png" style="vertical-align:-0.062em;height:0.746em;width:12.696em"/></span><span class="koboSpan" id="kobo.468.1">. </span><span class="koboSpan" id="kobo.468.2">Min-max is fast and easy to implement. </span><span class="koboSpan" id="kobo.468.3">One problem with this normalization is that data outliers could have an outsized impact on the result over the whole dataset. </span><span class="koboSpan" id="kobo.468.4">For example, if a single erroneous element has a very</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.469.1"> large value, it will enter the formula as </span><span class="koboSpan" id="kobo.470.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/257.png" style="vertical-align:-0.340em;height:0.788em;width:1.466em"/></span><span class="koboSpan" id="kobo.471.1"> and it will drive all normalized dataset values </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">toward 0.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.473.1">Standard score</span></strong><span class="koboSpan" id="kobo.474.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.475.1">z-score</span></strong><span class="koboSpan" id="kobo.476.1">): </span><span class="koboSpan" id="kobo.477.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/258.png" style="vertical-align:-0.072em;height:0.808em;width:2.996em"/></span><span class="koboSpan" id="kobo.478.1">. </span><span class="koboSpan" id="kobo.478.2">It handles</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.479.1"> data outliers better than min-max. </span><span class="koboSpan" id="kobo.479.2">To understand how, let’s focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">the formula:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.481.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/259.png" style="vertical-align:-0.343em;height:1.105em;width:5.982em"/></span><span class="koboSpan" id="kobo.482.1"> is the mean value of all elements of the dataset, where </span><span class="koboSpan" id="kobo.483.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/115.png" style="vertical-align:-0.340em;height:0.788em;width:0.624em"/></span><span class="koboSpan" id="kobo.484.1"> is a single element of the input vector and </span><em class="italic"><span class="koboSpan" id="kobo.485.1">N</span></em><span class="koboSpan" id="kobo.486.1"> is the total size of </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">the dataset.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.488.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/261.png" style="vertical-align:-0.433em;height:1.365em;width:9.230em"/></span><span class="koboSpan" id="kobo.489.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.490.1">standard deviation</span></strong><span class="koboSpan" id="kobo.491.1"> of all </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.492.1">dataset elements. </span><span class="koboSpan" id="kobo.492.2">It measures how far apart the dataset values are from the mean value. </span><span class="koboSpan" id="kobo.492.3">There</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.493.1"> is also </span><strong class="bold"><span class="koboSpan" id="kobo.494.1">variance,    </span></strong><strong class="bold"><span class="koboSpan" id="kobo.495.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/262.png" style="vertical-align:-0.044em;height:0.747em;width:4.011em"/></span></strong><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.496.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/263.png" style="vertical-align:-0.393em;height:1.155em;width:5.060em"/></span></span><span class="koboSpan" id="kobo.497.1">, which removes the square root from the standard deviation. </span><span class="koboSpan" id="kobo.497.2">The variance is theoretically correct but is less intuitive than standard deviation, which is measured in the same units as the original </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">data, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.499.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">.</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.501.1">Alternatively, we can compute μ and σ per sample if it’s not practical to compute them over the entire dataset. </span><span class="koboSpan" id="kobo.501.2">The standard score maintains the dataset’s mean value close to 0 and its standard deviation close </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">to 1.</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.503.1">Data augmentation</span></strong><span class="koboSpan" id="kobo.504.1">: This is where</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.505.1"> we artificially increase the size of the training set by applying random modifications to the training samples before feeding them to the network. </span><span class="koboSpan" id="kobo.505.2">In the case of images, these would be rotation, skew, scaling, and </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">so on.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.507.1">The next class of regularization techniques are applied within the DNN </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">structure itself:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.509.1">Dropout</span></strong><span class="koboSpan" id="kobo.510.1">: Here, we randomly and</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.511.1"> periodically remove some of the units of a layer (along with their input and output connections) from the network. </span><span class="koboSpan" id="kobo.511.2">During a training mini-batch, each unit has a probability, </span><em class="italic"><span class="koboSpan" id="kobo.512.1">p</span></em><span class="koboSpan" id="kobo.513.1">, of being stochastically dropped. </span><span class="koboSpan" id="kobo.513.2">This is to</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.514.1"> ensure that no unit ends up relying too much on other units and “learns” something useful for the NN instead. </span><span class="koboSpan" id="kobo.514.2">Dropout is only applied during the training phase and all the units in the network fully participate during the inference phase. </span><span class="koboSpan" id="kobo.514.3">In the following figure, we can see a dropout for fully </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">connected layers:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer319">
<span class="koboSpan" id="kobo.516.1"><img alt="Figure 3.9 – An example of dropout on full﻿y connected layers" src="image/B19627_03_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.517.1">Figure 3.9 – An example of dropout on fully connected layers</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.518.1">Batch normalization</span></strong><span class="koboSpan" id="kobo.519.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.520.1">BN</span></strong><span class="koboSpan" id="kobo.521.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.522.1">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</span></em><span class="koboSpan" id="kobo.523.1">, </span><a href="https://arxiv.org/abs/1502.03167):"><span class="koboSpan" id="kobo.524.1">https://arxiv.org/abs/1502.03167):</span></a><span class="koboSpan" id="kobo.525.1"> This is a </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.526.1">way to apply data processing, not unlike the standard score, for the hidden layers of the network. </span><span class="koboSpan" id="kobo.526.2">It </span><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.527.1">normalizes the outputs of the hidden layer for each mini-batch (hence the name) in a</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.528.1"> way that maintains its mean activation value close to 0 (</span><strong class="bold"><span class="koboSpan" id="kobo.529.1">re-centering</span></strong><span class="koboSpan" id="kobo.530.1">) and its standard deviation</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.531.1"> close to 1 (</span><strong class="bold"><span class="koboSpan" id="kobo.532.1">re-scaling</span></strong><span class="koboSpan" id="kobo.533.1">). </span><span class="koboSpan" id="kobo.533.2">The intuition is that as information is propagated through the layers, these values ca</span><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.534.1">n deviate from the desired values. </span><span class="koboSpan" id="kobo.534.2">Let’s say that the mini-batch is represented by an </span><em class="italic"><span class="koboSpan" id="kobo.535.1">m×n</span></em><span class="koboSpan" id="kobo.536.1"> matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.537.1">X</span></strong><span class="koboSpan" id="kobo.538.1">. </span><span class="koboSpan" id="kobo.538.2">Each row of </span><strong class="bold"><span class="koboSpan" id="kobo.539.1">X</span></strong><span class="koboSpan" id="kobo.540.1">, </span><span class="koboSpan" id="kobo.541.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/264.png" style="vertical-align:-0.340em;height:0.781em;width:0.666em"/></span><span class="koboSpan" id="kobo.542.1">, represents a </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.543.1">single input vector (this vector is an output of a preceding layer). </span><span class="koboSpan" id="kobo.544.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/265.png" style="vertical-align:-0.483em;height:0.931em;width:0.772em"/></span><span class="koboSpan" id="kobo.545.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.546.1">j</span></em><span class="koboSpan" id="kobo.547.1">-th element of the </span><em class="italic"><span class="koboSpan" id="kobo.548.1">i</span></em><span class="koboSpan" id="kobo.549.1">-th vector. </span><span class="koboSpan" id="kobo.549.2">We can compute BN for each matrix ele</span><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.550.1">ment in the </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">following way:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.552.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/266.png" style="vertical-align:-0.488em;height:1.250em;width:8.646em"/></span><span class="koboSpan" id="kobo.553.1">: This is the mini-batch mean. </span><span class="koboSpan" id="kobo.553.2">We compute a single </span><em class="italic"><span class="koboSpan" id="kobo.554.1">μ</span></em><span class="koboSpan" id="kobo.555.1"> value over all cells of the </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">mini-batch matrix.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.557.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/267.png" style="vertical-align:-0.538em;height:1.300em;width:11.268em"/></span><span class="koboSpan" id="kobo.558.1">: This is the mini-batch variance. </span><span class="koboSpan" id="kobo.558.2">We compute a single </span><span class="koboSpan" id="kobo.559.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mrow /&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/268.png" style="vertical-align:-0.333em;height:1.036em;width:0.824em"/></span><span class="koboSpan" id="kobo.560.1"> value over all cells of the </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">mini-batch matrix.</span></span></li><li lang="en-GB"> <span class="koboSpan" id="kobo.562.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msqrt&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:msqrt&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/269.png" style="vertical-align:-0.523em;height:1.389em;width:4.108em"/></span><span class="koboSpan" id="kobo.563.1">: We normalize each cell of the matrix. </span><em class="italic"><span class="koboSpan" id="kobo.564.1">ε</span></em><span class="koboSpan" id="kobo.565.1"> is a constant that’s added for numerical stability, so the denominator cannot </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">become 0.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.567.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;γ&lt;/mml:mi&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;≡&lt;/mml:mo&gt;&lt;mml:mtext&gt;B&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;N&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;γ,β&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/270.png" style="vertical-align:-0.533em;height:1.244em;width:10.526em"/></span><span class="koboSpan" id="kobo.568.1">: This formula represents the scale and shift of the original data. </span><em class="italic"><span class="koboSpan" id="kobo.569.1">γ</span></em><span class="koboSpan" id="kobo.570.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.571.1">β</span></em><span class="koboSpan" id="kobo.572.1"> are learnable parameters and we compute them over each location, </span><em class="italic"><span class="koboSpan" id="kobo.573.1">ij</span></em><span class="koboSpan" id="kobo.574.1"> (</span><span class="koboSpan" id="kobo.575.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;γ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/271.png" style="vertical-align:-0.483em;height:0.931em;width:0.798em"/></span><span class="koboSpan" id="kobo.576.1"> and </span><span class="koboSpan" id="kobo.577.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/272.png" style="vertical-align:-0.483em;height:1.194em;width:0.798em"/></span><span class="koboSpan" id="kobo.578.1">), over all cells of the </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">mini-batch matrix.</span></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.580.1">Layer normalization</span></strong><span class="koboSpan" id="kobo.581.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.582.1">LN</span></strong><span class="koboSpan" id="kobo.583.1">; see </span><em class="italic"><span class="koboSpan" id="kobo.584.1">Layer Normalization</span></em><span class="koboSpan" id="kobo.585.1">, </span><a href="https://arxiv.org/abs/1607.06450):"><span class="koboSpan" id="kobo.586.1">https://arxiv.org/abs/1607.06450):</span></a><span class="koboSpan" id="kobo.587.1"> LN is similar to BN, but with one key difference: the mean and variance are </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.588.1">computed separately over each mini-batch sample. </span><span class="koboSpan" id="kobo.588.2">This is unli</span><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.589.1">ke BN, where these values are computed across the whole mini-batch. </span><span class="koboSpan" id="kobo.589.2">As with BN, the mini-batch is an </span><em class="italic"><span class="koboSpan" id="kobo.590.1">m×n</span></em><span class="koboSpan" id="kobo.591.1"> matrix, </span><strong class="bold"><span class="koboSpan" id="kobo.592.1">X</span></strong><span class="koboSpan" id="kobo.593.1">, and each row</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.594.1"> vector, </span><span class="koboSpan" id="kobo.595.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/273.png" style="vertical-align:-0.340em;height:0.781em;width:0.690em"/></span><span class="koboSpan" id="kobo.596.1">, is the output of a preceding layer, and </span><span class="koboSpan" id="kobo.597.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/274.png" style="vertical-align:-0.483em;height:0.931em;width:0.772em"/></span><span class="koboSpan" id="kobo.598.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.599.1">j</span></em><span class="koboSpan" id="kobo.600.1">-th element of the </span><em class="italic"><span class="koboSpan" id="kobo.601.1">i</span></em><span class="koboSpan" id="kobo.602.1">-th vector. </span><span class="koboSpan" id="kobo.602.2">Then, we have the following for the </span><em class="italic"><span class="koboSpan" id="kobo.603.1">i</span></em><span class="koboSpan" id="kobo.604.1">-th </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">input vector:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.606.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/275.png" style="vertical-align:-0.532em;height:1.294em;width:5.497em"/></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.607.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/276.png" style="vertical-align:-0.584em;height:1.346em;width:8.452em"/></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.608.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/277.png" style="vertical-align:-0.638em;height:1.533em;width:4.098em"/></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.609.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mi&gt;γ&lt;/mml:mi&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;≡&lt;/mml:mo&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;N&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;γ,β&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/278.png" style="vertical-align:-0.533em;height:1.244em;width:10.476em"/></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.610.1">Root mean square layer normalization</span></strong><span class="koboSpan" id="kobo.611.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.612.1">RMSNorm</span></strong><span class="koboSpan" id="kobo.613.1">; see </span><a href="https://arxiv.org/abs/1910.07467):"><span class="koboSpan" id="kobo.614.1">https://arxiv.org/abs/1910.07467):</span></a><span class="koboSpan" id="kobo.615.1"> The authors of RMSNorm argue that the main benefit of LN comes just from the re-scaling, rather than the combination of re-centering and re-scaling. </span><span class="koboSpan" id="kobo.615.2">Therefore, RMSNorm is a simplified and faster version of LN, which only</span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.616.1"> applies re-scaling using the root mean square statistic. </span><span class="koboSpan" id="kobo.616.2">We’ll use the same notation as with LN. </span><span class="koboSpan" id="kobo.616.3">So, we can define RMSNorm </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">as follows:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.618.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;RMS&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:msqrt&gt;&lt;/mml:math&gt;" src="image/279.png" style="vertical-align:-0.526em;height:1.458em;width:8.596em"/></span><span class="koboSpan" id="kobo.619.1">.</span></li><li lang="en-GB"><span class="koboSpan" id="kobo.620.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;RMS&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;γ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/280.png" style="vertical-align:-0.483em;height:1.350em;width:4.985em"/></span><span class="koboSpan" id="kobo.621.1">: Here, </span><span class="koboSpan" id="kobo.622.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;γ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/281.png" style="vertical-align:-0.483em;height:0.931em;width:0.742em"/></span><span class="koboSpan" id="kobo.623.1"> is the gain parameter used to re-scale the standardized summed inputs (it is set to 1 at the beginning). </span><span class="koboSpan" id="kobo.623.2">It is equivalent to the </span><em class="italic"><span class="koboSpan" id="kobo.624.1">γ</span></em><span class="koboSpan" id="kobo.625.1"> parameter </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">in BN.</span></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.627.1">The following figure illustrates the difference between BN and LN. </span><span class="koboSpan" id="kobo.627.2">On th</span><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.628.1">e left, we compute single </span><em class="italic"><span class="koboSpan" id="kobo.629.1">μ</span></em><span class="koboSpan" id="kobo.630.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.631.1">σ</span></em><span class="koboSpan" id="kobo.632.1"> values across the whole mini-batch. </span><span class="koboSpan" id="kobo.632.2">To the right, we can see </span><span class="koboSpan" id="kobo.633.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;μ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/282.png" style="vertical-align:-0.340em;height:0.788em;width:0.662em"/></span><span class="koboSpan" id="kobo.634.1"> and </span><span class="koboSpan" id="kobo.635.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/283.png" style="vertical-align:-0.340em;height:0.801em;width:0.625em"/></span><span class="koboSpan" id="kobo.636.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">each row:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer340">
<span class="koboSpan" id="kobo.638.1"><img alt="Figure 3.10 – BN and LN computation of μ and σ" src="image/B19627_03_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.639.1">Figure 3.10 – BN and LN computation of μ and σ</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.640.1">The final type of regularization we’ll </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.641.1">introduce is </span><strong class="bold"><span class="koboSpan" id="kobo.642.1">L2 regularization</span></strong><span class="koboSpan" id="kobo.643.1">. </span><span class="koboSpan" id="kobo.643.2">This technique adds a special regularization term to the cost function. </span><span class="koboSpan" id="kobo.643.3">To understand it, let’s take the MSE cost. </span><span class="koboSpan" id="kobo.643.4">We can add L2 regularization to it in the following way (the underscored part of </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">the formula):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.645.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/284.png" style="vertical-align:-1.064em;height:2.424em;width:15.328em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.646.1">Here, </span><span class="koboSpan" id="kobo.647.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/165.png" style="vertical-align:-0.483em;height:1.194em;width:0.592em"/></span><span class="koboSpan" id="kobo.648.1"> is one of </span><em class="italic"><span class="koboSpan" id="kobo.649.1">m</span></em><span class="koboSpan" id="kobo.650.1"> total network weights and </span><em class="italic"><span class="koboSpan" id="kobo.651.1">λ</span></em><span class="koboSpan" id="kobo.652.1"> is the weight decay coefficient. </span><span class="koboSpan" id="kobo.652.2">The rationale is that if the network weights, </span><span class="koboSpan" id="kobo.653.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/174.png" style="vertical-align:-0.483em;height:1.194em;width:0.594em"/></span><span class="koboSpan" id="kobo.654.1">, are large, then the cost function will also increase. </span><span class="koboSpan" id="kobo.654.2">In effect, weight decay penalizes large weights (hence the name). </span><span class="koboSpan" id="kobo.654.3">This prevents the network from relying too heavily on a few features associated with these weights. </span><span class="koboSpan" id="kobo.654.4">There is less chance of overfitting when the network is forced to work with multiple features. </span><span class="koboSpan" id="kobo.654.5">In practical terms, when</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.655.1"> we compute the derivative of the weight decay cost function (the preceding formula) concerning each weight and then propagate it to the weights themselves, the weight update rule changes from </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">the following:</span></span></p>
<p lang="en-GB"> <span class="koboSpan" id="kobo.657.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/287.png" style="vertical-align:-0.543em;height:1.445em;width:6.491em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.658.1">to </span></span><span class="No-Break"><span class="koboSpan" id="kobo.659.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;←&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/288.png" style="vertical-align:-0.543em;height:1.445em;width:8.772em"/></span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.660.1">With this discussion of DNN regularization, we’ve covered our theoretical base. </span><span class="koboSpan" id="kobo.660.2">Next, let’s see what the real-world applications of </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">DNNs are.</span></span></p>
<h1 id="_idParaDest-57" lang="en-GB"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.662.1">Applications of DL</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.663.1">ML in general, and DL in particular, is producing more and more astonishing results in terms of the quality of predictions, feature</span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.664.1"> detection, and classification. </span><span class="koboSpan" id="kobo.664.2">Many of these recent results have made the news. </span><span class="koboSpan" id="kobo.664.3">Such is the pace of progress that some experts are worrying that machines will soon be more intelligent than humans. </span><span class="koboSpan" id="kobo.664.4">But I hope that any such fears you might have will be alleviated after you have read this book. </span><span class="koboSpan" id="kobo.664.5">For better or worse, we’re still far from machines having </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">human-level intelligence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.666.1">In </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.667.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.668.1">, we mentioned how DL algorithms have occupied the leaderboard of the ImageNet competition. </span><span class="koboSpan" id="kobo.668.2">They are successful enough to make the jump from academia to industry. </span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.669.1">Let’s talk about some real-world use cases </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">of DL:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.671.1">Nowadays, new cars have a suite of safety and convenience features that aim to make the driving experience safer and less stressful. </span><span class="koboSpan" id="kobo.671.2">One such feature is automated emergency braking if the car sees an obstacle. </span><span class="koboSpan" id="kobo.671.3">Another one is lane-keeping assist, which allows the vehicle to stay in its current lane without the driver needing to make corrections with the steering wheel. </span><span class="koboSpan" id="kobo.671.4">To recognize lane markings, other vehicles, pedestrians, and cyclists, these systems use a </span><br/><span class="koboSpan" id="kobo.672.1">forward-facing camera. </span><span class="koboSpan" id="kobo.672.2">One of the </span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.673.1">most prominent suppliers of such systems, Mobileye (</span><a href="https://www.mobileye.com/"><span class="koboSpan" id="kobo.674.1">https://www.mobileye.com/</span></a><span class="koboSpan" id="kobo.675.1">), has produced custom chips that use CNNs to detect these objects on the road ahead. </span><span class="koboSpan" id="kobo.675.2">To give you an idea of the importance of this sector, in 2017, Intel acquired Mobileye for $15.3 billion. </span><span class="koboSpan" id="kobo.675.3">This is not an outlier, and Tesla’s famous Autopilot system also relies on CNNs to achieve the same results. </span><span class="koboSpan" id="kobo.675.4">The former director of AI at Tesla, Andrej Karpathy (</span><a href="https://karpathy.ai/"><span class="koboSpan" id="kobo.676.1">https://karpathy.ai/</span></a><span class="koboSpan" id="kobo.677.1">), is a well-known researcher in the field of DL. </span><span class="koboSpan" id="kobo.677.2">We can speculate that future autonomous vehicles will also use deep networks for </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">computer vision.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.679.1">Both Google’s </span><strong class="bold"><span class="koboSpan" id="kobo.680.1">Vision API</span></strong><span class="koboSpan" id="kobo.681.1"> (</span><a href="https://cloud.google.com/vision/"><span class="koboSpan" id="kobo.682.1">https://cloud.google.com/vision/</span></a><span class="koboSpan" id="kobo.683.1">) and</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.684.1"> Amazon’s </span><strong class="bold"><span class="koboSpan" id="kobo.685.1">Rekognition</span></strong><span class="koboSpan" id="kobo.686.1"> (</span><a href="https://aws.amazon.com/rekognition/"><span class="koboSpan" id="kobo.687.1">https://aws.amazon.com/rekognition/</span></a><span class="koboSpan" id="kobo.688.1">) services </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.689.1">use DL models to provide various computer vision capabilities. </span><span class="koboSpan" id="kobo.689.2">These include recognizing and detecting objects and scenes in images, text recognition, face recognition, content moderation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">so on.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.691.1">If these APIs are not enough, you can run your own models in the cloud. </span><span class="koboSpan" id="kobo.691.2">For example, you can use Amazon’s AWS DL AMIs (short for </span><strong class="bold"><span class="koboSpan" id="kobo.692.1">Amazon Machine Images</span></strong><span class="koboSpan" id="kobo.693.1">; see </span><a href="https://aws.amazon.com/machine-learning/amis/"><span class="koboSpan" id="kobo.694.1">https://aws.amazon.com/machine-learning/amis/</span></a><span class="koboSpan" id="kobo.695.1">), which are virtual machines that come configured with some of the most popular DL libraries. </span><span class="koboSpan" id="kobo.695.2">Google offers a similar service with their Cloud AI (</span><a href="https://cloud.google.com/products/ai/"><span class="koboSpan" id="kobo.696.1">https://cloud.google.com/products/ai/</span></a><span class="koboSpan" id="kobo.697.1">), but </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.698.1">they’</span><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.699.1">ve gone one step further. </span><span class="koboSpan" id="kobo.699.2">They created </span><strong class="bold"><span class="koboSpan" id="kobo.700.1">tensor processing units</span></strong><span class="koboSpan" id="kobo.701.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.702.1">TPUs</span></strong><span class="koboSpan" id="kobo.703.1">; see https://cloud.google.com/tpu/) – microprocessors that are optimized for fast NN operations such as matrix multiplication and </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">activation functions.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.705.1">DL has a lot of potential for medical applications. </span><span class="koboSpan" id="kobo.705.2">However, strict regulatory requirements, as well as patient data confidentiality, have slowed down its adoption. </span><span class="koboSpan" id="kobo.705.3">Nevertheless, we’ll identify several areas in which DL could have a </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">high impact:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.707.1">Medical imaging is an umbrella term for various non-invasive methods of creating visual representations</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.708.1"> of the inside of the body. </span><span class="koboSpan" id="kobo.708.2">Some of these include </span><strong class="bold"><span class="koboSpan" id="kobo.709.1">magnetic resonance images</span></strong><span class="koboSpan" id="kobo.710.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.711.1">MRIs</span></strong><span class="koboSpan" id="kobo.712.1">), ultrasound, </span><strong class="bold"><span class="koboSpan" id="kobo.713.1">computed axial tomography</span></strong><span class="koboSpan" id="kobo.714.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.715.1">CAT</span></strong><span class="koboSpan" id="kobo.716.1">) scans, X-rays, and histology images. </span><span class="koboSpan" id="kobo.716.2">Typically, such an</span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.717.1"> image is analyzed by a medical professional to determine the </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">patient’s condition.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.719.1">Computer-aided diagnosis, and computer vision, </span><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.720.1">in particular, can help specialists by detecting</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.721.1"> and highlighting important features of images. </span><span class="koboSpan" id="kobo.721.2">For example, to determine the degree of malignancy of colon cancer, a pathologist would have to analyze the morphology of the glands using histology imaging. </span><span class="koboSpan" id="kobo.721.3">This is a challenging task because morphology can vary greatly. </span><span class="koboSpan" id="kobo.721.4">A DNN could segment the glands from the image automatically, leaving the pathologist to verify the results. </span><span class="koboSpan" id="kobo.721.5">This would reduce the time needed for analysis, making it cheaper and </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">more accessible.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.723.1">Another medical area that could benefit from DL is the analysis of medical history records. </span><span class="koboSpan" id="kobo.723.2">When a doctor diagnoses a patient’s condition and prescribes treatment, they consult the patient’s medical history first. </span><span class="koboSpan" id="kobo.723.3">A DL algorithm could extract the most relevant and important information from those records, even if they are handwritten. </span><span class="koboSpan" id="kobo.723.4">In this way, the doctor’s job would be made easier, and the risk of errors would also </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">be reduced.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.725.1">One area where DNNs have already had an impact is in protein folding. </span><span class="koboSpan" id="kobo.725.2">Proteins are large, complex molecules, whose function depends on their 3D shape. </span><span class="koboSpan" id="kobo.725.3">The building blocks of proteins are amino acids, and their sequence determines the shape of the protein. </span><span class="koboSpan" id="kobo.725.4">The protein folding problem seeks to understand the relationship between the initial amino acid sequence and the final 3D shape of </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.726.1">the protein. </span><span class="koboSpan" id="kobo.726.2">DeepMind’s </span><strong class="bold"><span class="koboSpan" id="kobo.727.1">AlphaFold 2</span></strong><span class="koboSpan" id="kobo.728.1"> model (believed to be based on transformers; see </span><a href="https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe"><span class="koboSpan" id="kobo.729.1">https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe</span></a><span class="koboSpan" id="kobo.730.1">) has managed to predict 200 million protein structures, which represents almost all known </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">cataloged proteins.</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.732.1">Google’s Neural Machine Translation API (</span><a href="https://arxiv.org/abs/1609.08144"><span class="koboSpan" id="kobo.733.1">https://arxiv.org/abs/1609.08144</span></a><span class="koboSpan" id="kobo.734.1">) uses – you guessed it – DNNs for </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">machine translation.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.736.1">Siri (</span><a href="https://machinelearning.apple.com/2017/10/01/hey-siri.html"><span class="koboSpan" id="kobo.737.1">https://machinelearning.apple.com/2017/10/01/hey-siri.html</span></a><span class="koboSpan" id="kobo.738.1">), Google Assistant, and Amazon Alexa (</span><a href="https://aws.amazon.com/deep-learning/"><span class="koboSpan" id="kobo.739.1">https://aws.amazon.com/deep-learning/</span></a><span class="koboSpan" id="kobo.740.1">) rely on deep networks for </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">speech recognition.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.742.1">AlphaGo</span></strong><span class="koboSpan" id="kobo.743.1"> is an AI machine based </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.744.1">on DL that made the news in 2016 by beating the world Go champion, Lee Sedol. </span><span class="koboSpan" id="kobo.744.2">AlphaGo had already made the news, in January 2016, when it beat the European champion, Fan Hui. </span><span class="koboSpan" id="kobo.744.3">At the time, however, it seemed unlikely</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.745.1"> that it could go on to beat the world champion. </span><span class="koboSpan" id="kobo.745.2">Fast-forward a couple of months and AlphaGo was able to achieve this remarkable feat by sweeping its opponent in a 4-1 victory series. </span><span class="koboSpan" id="kobo.745.3">This was an important milestone because Go has many more possible game variations than other games, such as chess, and it’s impossible to consider every possible move in advance. </span><span class="koboSpan" id="kobo.745.4">Also, unlike chess, in Go, it’s very difficult to even judge the current position or value of a single stone on the board. </span><span class="koboSpan" id="kobo.745.5">In 2017, DeepMind released an</span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.746.1"> updated version of AlphaGo called </span><strong class="bold"><span class="koboSpan" id="kobo.747.1">AlphaZero</span></strong><span class="koboSpan" id="kobo.748.1"> (</span><a href="https://arxiv.org/abs/1712.01815"><span class="koboSpan" id="kobo.749.1">https://arxiv.org/abs/1712.01815</span></a><span class="koboSpan" id="kobo.750.1">), and in 2019, they</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.751.1"> released a further update called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.752.1">MuZero</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.753.1"> (</span></span><a href="https://arxiv.org/abs/1911.08265"><span class="No-Break"><span class="koboSpan" id="kobo.754.1">https://arxiv.org/abs/1911.08265</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.755.1">).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.756.1">Tools such as GitHub Copilot (</span><a href="https://github.com/features/copilot"><span class="koboSpan" id="kobo.757.1">https://github.com/features/copilot</span></a><span class="koboSpan" id="kobo.758.1">) and ChatGPT (</span><a href="https://chat.openai.com/"><span class="koboSpan" id="kobo.759.1">https://chat.openai.com/</span></a><span class="koboSpan" id="kobo.760.1">) utilize generative DNN models to transform natural language requests into source code snippets, functions, and entire programs. </span><span class="koboSpan" id="kobo.760.2">We already mentioned Stable Diffusion (</span><a href="https://stability.ai/blog/stable-diffusion-public-release"><span class="koboSpan" id="kobo.761.1">https://stability.ai/blog/stable-diffusion-public-release</span></a><span class="koboSpan" id="kobo.762.1">) and DALL-E (</span><a href="https://openai.com/dall-e-2/"><span class="koboSpan" id="kobo.763.1">https://openai.com/dall-e-2/</span></a><span class="koboSpan" id="kobo.764.1">), which can generate realistic images based on </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">text description.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.766.1">With this short list, we aimed to cover the main areas in which DL is applied, such as computer vision, NLP, speech</span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.767.1"> recognition, and </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.769.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.770.1">RL</span></strong><span class="koboSpan" id="kobo.771.1">). </span><span class="koboSpan" id="kobo.771.2">This list is not exhaustive, however, as there are many other uses for DL algorithms. </span><span class="koboSpan" id="kobo.771.3">Still, I hope this has been enough to spark your interest. </span><span class="koboSpan" id="kobo.771.4">Next, we’ll formally introduce two of the most popular DL libraries – PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">and Keras.</span></span></p>
<h1 id="_idParaDest-58" lang="en-GB"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.773.1">Introducing popular DL libraries</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.774.1">We already implemented a</span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.775.1"> simple example with PyTorch in </span><a href="B19627_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.776.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.777.1">. </span><span class="koboSpan" id="kobo.777.2">In this section, we’ll introduce this library, and Keras, more systemically. </span><span class="koboSpan" id="kobo.777.3">Let’s start with the common features of most </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">DNN libraries:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.779.1">All libraries </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">use Python.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.781.1">The basic unit for data </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.782.1">storage is the </span><strong class="bold"><span class="koboSpan" id="kobo.783.1">tensor</span></strong><span class="koboSpan" id="kobo.784.1">. </span><span class="koboSpan" id="kobo.784.2">Mathematically, the definition of a tensor is more complex, but in the context of DL libraries, they are multi-dimensional (with an arbitrary number of axes) arrays of </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">base values.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.786.1">NNs are represented as a </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">computational graph</span></strong><span class="koboSpan" id="kobo.788.1"> of operations. </span><span class="koboSpan" id="kobo.788.2">The nodes of the graph represent </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.789.1">the operations (weighted sum, activation function, and so on). </span><span class="koboSpan" id="kobo.789.2">The edges represent the flow of data, which is how the output of one operation serves as an input for the next one. </span><span class="koboSpan" id="kobo.789.3">The inputs and outputs of the operations (including the network inputs and outputs) </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">are tensors.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.791.1">All libraries include </span><strong class="bold"><span class="koboSpan" id="kobo.792.1">automatic differentiation</span></strong><span class="koboSpan" id="kobo.793.1">. </span><span class="koboSpan" id="kobo.793.2">This</span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.794.1"> means that all you need to do is define the network architecture and activation functions, and the library will automatically figure out all of the derivatives required for training </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">with backpropagation.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.796.1">So far, we’ve referred to GPUs in general, but in reality, the vast majority of DL projects work exclusively with NVIDIA GPUs. </span><span class="koboSpan" id="kobo.796.2">This is because of the better software support NVIDIA provides. </span><span class="koboSpan" id="kobo.796.3">These libraries are no exception – to implement GPU operations, they rely on the CUDA Toolkit (</span><a href="https://developer.nvidia.com/cuda-toolkit"><span class="koboSpan" id="kobo.797.1">https://developer.nvidia.com/cuda-toolkit</span></a><span class="koboSpan" id="kobo.798.1">) in combination with the cuDNN library (</span><a href="https://developer.nvidia.com/cudnn"><span class="koboSpan" id="kobo.799.1">https://developer.nvidia.com/cudnn</span></a><span class="koboSpan" id="kobo.800.1">). </span><span class="koboSpan" id="kobo.800.2">cuDNN is an extension of CUDA, built specifically for DL applications. </span><span class="koboSpan" id="kobo.800.3">As mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.801.1">Applications of DL</span></em><span class="koboSpan" id="kobo.802.1"> section, you can also run your DL experiments in </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">the cloud.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.804.1">PyTorch is an independent library, while Keras is built on top of TF and acts as a user-friendly TF interface. </span><span class="koboSpan" id="kobo.804.2">We’ll continue by</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.805.1"> implementing a simple classification example using both PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">and Keras.</span></span></p>
<h2 id="_idParaDest-59" lang="en-GB"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.807.1">Classifying digits with Keras</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.808.1">Keras exists either as a </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.809.1">standalone library with TF as the backend or as a sub-component of TF itself. </span><span class="koboSpan" id="kobo.809.2">You can use it in both flavors. </span><span class="koboSpan" id="kobo.809.3">To use Keras as part of TF, we need </span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.810.1">only to install TF itself. </span><span class="koboSpan" id="kobo.810.2">Once we’ve done this, we can use the library with the </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">following import:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.812.1">
import tensorflow.keras</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.813.1">The standalone Keras supports different backends besides TF, such as Theano. </span><span class="koboSpan" id="kobo.813.2">In this case, we can install Keras itself and then use it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">following import:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.815.1">
import keras</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.816.1">The large majority of Keras’s use is with the TF backend. </span><span class="koboSpan" id="kobo.816.2">The author of Keras recommends using the library as a TF component (the first option) and we’ll do so in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">this book.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.818.1">In this section, we’ll use Keras via TF to classify the images of the MNIST dataset. </span><span class="koboSpan" id="kobo.818.2">It’s comprised of 70,000 examples of digits that have been handwritten by different people. </span><span class="koboSpan" id="kobo.818.3">The first 60,000 are typically used for training and the remaining 10,000 </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">for testing:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer346">
<span class="koboSpan" id="kobo.820.1"><img alt="Figure 3.11 – A sample of digits taken from the MNIST dataset" src="image/B19627_03_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.821.1">Figure 3.11 – A sample of digits taken from the MNIST dataset</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.822.1">We’ll build a simple MLP </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.823.1">with one hidden layer. </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.825.1">One of the advantages of Keras is that it can import this dataset for you without you needing to explicitly download it from the web (it will download it </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">for you):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.827.1">
import tensorflow as tf
(X_train, Y_train), (X_validation, Y_validation) = \
             tf.keras.datasets.mnist.load_data()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.828.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.829.1">(X_train, Y_train)</span></strong><span class="koboSpan" id="kobo.830.1"> is the training images and labels, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.831.1">(X_validation, Y_validation)</span></strong><span class="koboSpan" id="kobo.832.1"> is the test images </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">and labels.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.834.1">We need to modify the</span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.835.1"> data so that we can feed it to the NN. </span><strong class="source-inline"><span class="koboSpan" id="kobo.836.1">X_train</span></strong><span class="koboSpan" id="kobo.837.1"> contains 60,000 28×28 pixel images, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.838.1">X_validation</span></strong><span class="koboSpan" id="kobo.839.1"> contains 10,000. </span><span class="koboSpan" id="kobo.839.2">To feed them to the network as inputs, we want to reshape each sample as a 784-pixel-long array, rather than a 28×28 two-dimensional matrix. </span><span class="koboSpan" id="kobo.839.3">We’ll also normalize them in the [0:1] range. </span><span class="koboSpan" id="kobo.839.4">We can accomplish this with these </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">two lines:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.841.1">
X_train = X_train.reshape(60000, 784) / 255
X_validation = X_validation.reshape(10000, 784) / 255</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.842.1">The labels indicate the value of the digit depicted in the images. </span><span class="koboSpan" id="kobo.842.2">We want to convert this into a 10-entry </span><strong class="bold"><span class="koboSpan" id="kobo.843.1">one-hot-encoded</span></strong><span class="koboSpan" id="kobo.844.1"> vector comprised of 0s and just one 1 in the entry corresponding to the digit. </span><span class="koboSpan" id="kobo.844.2">For example, 4 is mapped to [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. </span><span class="koboSpan" id="kobo.844.3">Conversely, our network will have 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">output units:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.846.1">
classes = 10
Y_train = tf.keras.utils.to_categorical(Y_train,
            classes)
Y_validation = tf.keras.utils.to_categorical(
            Y_validation, classes)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.847.1">Define the NN. </span><span class="koboSpan" id="kobo.847.2">In this case, we’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.848.1">Sequential</span></strong><span class="koboSpan" id="kobo.849.1"> model, where each layer serves as an</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.850.1"> input to the next. </span><span class="koboSpan" id="kobo.850.2">In Keras, </span><strong class="source-inline"><span class="koboSpan" id="kobo.851.1">Dense</span></strong><span class="koboSpan" id="kobo.852.1"> means a fully connected layer. </span><span class="koboSpan" id="kobo.852.2">We’ll use a network with one hidden layer with 100 units, BN, ReLU </span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.853.1">activation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">softmax output:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.855.1">
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Activation
input_size = 784
hidden_units = 100
model = Sequential([
    Dense(
        hidden_units, input_dim=input_size),
    BatchNormalization(),
    Activation('relu'),
    Dense(classes),
    Activation('softmax')
])</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.856.1">Now, we can define our gradient descent parameters. </span><span class="koboSpan" id="kobo.856.2">We’ll use the Adam optimizer and categorical cross-entropy loss (this is cross entropy, optimized for </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">softmax outputs):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.858.1">
model.compile(
    loss='categorical_crossentropy',
    metrics=['accuracy'],
    optimizer='adam')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.859.1">Next, run the training for 100 epochs and a batch size of 100. </span><span class="koboSpan" id="kobo.859.2">In Keras, we can do this with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.860.1">fit</span></strong><span class="koboSpan" id="kobo.861.1"> method, which </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.862.1">iterates over the dataset internally. </span><span class="koboSpan" id="kobo.862.2">Keras will default to GPU training, but if a GPU is not available, it will fall back to </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">the CPU:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.864.1">
model.fit(X_train, Y_train, batch_size=100, epochs=20,
          verbose=1)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.865.1">All that’s left to do is add code to evaluate the network’s accuracy on the </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">test data:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.867.1">
score = model.evaluate(X_validation, Y_validation,
          verbose=1)
print('Validation accuracy:', score[1])</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.868.1">And that’s it. </span><span class="koboSpan" id="kobo.868.2">The validation accuracy will be about 97.7%, which is not a great result, but this </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.869.1">example runs in less than 30 seconds on a CPU. </span><span class="koboSpan" id="kobo.869.2">We can make some simple improvements, such as a larger number of hidden units, or a higher number of epochs. </span><span class="koboSpan" id="kobo.869.3">We’ll leave those experiments to you so that you can familiarize yourself with </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">the code.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.871.1">To see what the network has learned, we can visualize the weights of the hidden layer. </span><span class="koboSpan" id="kobo.871.2">The following code allows us to </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">obtain them:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.873.1">
weights = model.layers[0].get_weights()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.874.1">Reshape the weights for each unit back to a 28×28 two-dimensional array and then </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">display them:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.876.1">
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy
fig = plt.figure()
w = weights[0].T
for unit in range(hidden_units):
    ax = fig.add_subplot(10, 10, unit + 1)
    ax.axis("off")
    ax.imshow(numpy.reshape(w[unit], (28, 28)),
    cmap=cm.Greys_r)
plt.show()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.877.1">We can see the result </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.878.1">in the </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">following figure:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer347">
<span class="koboSpan" id="kobo.880.1"><img alt="Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units" src="image/B19627_03_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.881.1">Figure 3.12 – A composite ﬁgure of what was learned by all the hidden units</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.882.1">Now, let us see the </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.883.1">example </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">for PyTorch.</span></span></p>
<h2 id="_idParaDest-60" lang="en-GB"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.885.1">Classifying digits with PyTorch</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.886.1">In this section, we’ll implement the </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.887.1">same example that we did in the </span><em class="italic"><span class="koboSpan" id="kobo.888.1">Classifying digits with Keras</span></em><span class="koboSpan" id="kobo.889.1"> section but this time with PyTorch. </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.891.1">First, we’ll select the device </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.892.1">we’re using (CPU or GPU). </span><span class="koboSpan" id="kobo.892.2">We’ll try with the GPU first and fall back to the CPU if the GPU is </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">not available:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.894.1">
import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.895.1">Like Keras, PyTorch supports MNIST out of the box. </span><span class="koboSpan" id="kobo.895.2">Here’s how we can instantiate the train and </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">validation sets:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.897.1">
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda, Compose
train_data = datasets.MNIST(
    root='data',
    train=True,
    transform=Compose(
        [ToTensor(),
        Lambda(lambda x: torch.flatten(x))]),
        download=True,
    )
validation_data = datasets.MNIST(
    root='data',
    train=False,
    transform=Compose(
        [ToTensor(),
        Lambda(lambda x: torch.flatten(x))]),
    )</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.898.1">The dataset is automatically downloaded and split into training and validation parts. </span><span class="koboSpan" id="kobo.898.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.899.1">ToTensor()</span></strong><span class="koboSpan" id="kobo.900.1"> transformation converts the images from </span><strong class="source-inline"><span class="koboSpan" id="kobo.901.1">numpy</span></strong><span class="koboSpan" id="kobo.902.1"> arrays into PyTorch tensors and </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.903.1">normalizes them in the [0:1] range (as opposed to [0:255] originally). </span><span class="koboSpan" id="kobo.903.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">torch.flatten</span></strong><span class="koboSpan" id="kobo.905.1"> transform flattens the two-dimensional 28×28 images to a one-dimensional 784 tensor so that we can feed it to </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">the NN.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.907.1">Next, we’ll wrap the</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.908.1"> datasets in </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.909.1">DataLoader</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.910.1"> instances:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.911.1">
from torch.utils.data import DataLoader
train_loader = DataLoader(
    dataset=train_data,
    batch_size=100,
    shuffle=True)
validation_loader = DataLoader(
    dataset=validation_data,
    batch_size=100,
    shuffle=True)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.912.1">The data </span><strong class="source-inline"><span class="koboSpan" id="kobo.913.1">DataLoader</span></strong><span class="koboSpan" id="kobo.914.1"> instance takes care of creating mini-batches and shuffles the data randomly. </span><span class="koboSpan" id="kobo.914.2">They are also iterators, which supply mini-batches one at </span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">a time.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.916.1">Then, we’ll define </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.917.1">the NN </span><strong class="source-inline"><span class="koboSpan" id="kobo.918.1">model</span></strong><span class="koboSpan" id="kobo.919.1">. </span><span class="koboSpan" id="kobo.919.2">We’ll use the same MLP with a single hidden layer, as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">Keras example:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.921.1">
torch.manual_seed(1234)
hidden_units = 100
classes = 10
model = torch.nn.Sequential(
    torch.nn.Linear(28 * 28, hidden_units),
    torch.nn.BatchNorm1d(hidden_units),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_units, classes),
)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.922.1">This definition is like the one in Keras. </span><span class="koboSpan" id="kobo.922.2">One difference is that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.923.1">Linear</span></strong><span class="koboSpan" id="kobo.924.1"> (fully connected) layers require both input and output dimensions since they cannot extract the output </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.925.1">dimension of the preceding layer. </span><span class="koboSpan" id="kobo.925.2">The activations are defined as </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">separate operations.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.927.1">Next, let’s define the cross-entropy loss and the </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">Adam optimizer:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.929.1">
cost_func = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.930.1">Now, we can define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.931.1">train_model</span></strong><span class="koboSpan" id="kobo.932.1"> function, which, as its name suggests, takes care of training the </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.933.1">model. </span><span class="koboSpan" id="kobo.933.2">It takes our predefined </span><strong class="source-inline"><span class="koboSpan" id="kobo.934.1">model</span></strong><span class="koboSpan" id="kobo.935.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.936.1">cost_function</span></strong><span class="koboSpan" id="kobo.937.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.938.1">optimizer</span></strong><span class="koboSpan" id="kobo.939.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.940.1">data_loader</span></strong><span class="koboSpan" id="kobo.941.1"> and runs the training for a </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">single epoch:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.943.1">
def train_model(model, cost_function, optimizer, data_loader):
    # send the model to the GPU
    model.to(device)
    # set model to training mode
    model.train()
    current_loss = 0.0
    current_acc = 0
    # iterate over the training data
    for i, (inputs, labels) in enumerate(data_loader):
        # send the input/labels to the GPU
        inputs = inputs.to(device)
        labels = labels.to(device)
        # zero the parameter gradients
        optimizer.zero_grad()
        with torch.set_grad_enabled(True):
            # forward
            outputs = model(inputs)
            _, predictions = torch.max(outputs, 1)
            loss = cost_function(outputs, labels)
            # backward
            loss.backward()
            optimizer.step()
        # statistics
        current_loss += loss.item() * inputs.size(0)
        current_acc += torch.sum(predictions == labels.data)
    total_loss = current_loss / len(data_loader.dataset)
    total_acc = current_acc.double() / len(data_loader.dataset)
    print('Train Loss: {:.4f}; Accuracy: /
        {:.4f}'.format(total_loss, total_acc))</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.944.1">Unlike Keras and its </span><strong class="source-inline"><span class="koboSpan" id="kobo.945.1">fit</span></strong><span class="koboSpan" id="kobo.946.1"> function, we have to implement the PyTorch training ourselves. </span><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">train_model</span></strong><span class="koboSpan" id="kobo.948.1"> iterates over all mini-batches provided by </span><strong class="source-inline"><span class="koboSpan" id="kobo.949.1">train_loader</span></strong><span class="koboSpan" id="kobo.950.1">. </span><span class="koboSpan" id="kobo.950.2">For each mini-batch, </span><strong class="source-inline"><span class="koboSpan" id="kobo.951.1">optimizer.zero_grad()</span></strong><span class="koboSpan" id="kobo.952.1"> resets the gradients from the</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.953.1"> previous iteration. </span><span class="koboSpan" id="kobo.953.2">Then, we initiate the forward and</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.954.1"> backward passes, and finally the </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">weight updates.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.956.1">We’ll also define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.957.1">test_model</span></strong><span class="koboSpan" id="kobo.958.1"> function, which will run the model in inference mode to check </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">its results:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.960.1">
def test_model(model, cost_function, data_loader):
    # send the model to the GPU
    model.to(device)
    # set model in evaluation mode
    model.eval()
    current_loss = 0.0
    current_acc = 0
    # iterate over    the validation data
    for i, (inputs, labels) in enumerate(data_loader):
        # send the input/labels to the GPU
        inputs = inputs.to(device)
        labels = labels.to(device)
        # forward
        with torch.set_grad_enabled(False):
            outputs = model(inputs)
            _, predictions = torch.max(outputs, 1)
            loss = cost_function(outputs, labels)
        # statistics
        current_loss += loss.item() * inputs.size(0)
        current_acc += torch.sum(predictions == labels.data)
    total_loss = current_loss / len(data_loader.dataset)
    total_acc = current_acc.double() / len(data_loader.dataset)
    print('Test Loss: {:.4f}; Accuracy: /
        {:.4f}'.format(total_loss, total_acc))
    return total_loss, total_acc</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.961.1">BN and </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.962.1">dropout layers are not used in evaluation (only in training), so </span><strong class="source-inline"><span class="koboSpan" id="kobo.963.1">model.eval()</span></strong><span class="koboSpan" id="kobo.964.1"> turns </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.965.1">them off. </span><span class="koboSpan" id="kobo.965.2">We iterate over the validation set, initiate a forward pass, and aggregate the validation loss </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">and accuracy.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.967.1">Let’s run the training for </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">20 epochs:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.969.1">
for epoch in range(20):
    train_model(model, cost_func, optimizer,
        train_loader)
test_model(model, cost_func, validation_loader)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.970.1">This model achieves </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">97.6% accuracy.</span></span></p></li>
</ol>
<h1 id="_idParaDest-61" lang="en-GB"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.972.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.973.1">In this chapter, we explained what DL is and how it’s related to DNNs. </span><span class="koboSpan" id="kobo.973.2">We discussed the different types of DNNs and how to train them, and we paid special attention to various regularization techniques that help with the training process. </span><span class="koboSpan" id="kobo.973.3">We also mentioned many real-world applications of DL and tried to analyze the reasons for its efficiency. </span><span class="koboSpan" id="kobo.973.4">Finally, we introduced two of the most popular DL libraries, namely PyTorch and Keras. </span><span class="koboSpan" id="kobo.973.5">We also implemented identical MNIST classification examples with </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">both libraries.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.975.1">In the next chapter, we’ll discuss how to solve classification tasks over more complex image datasets with the help of convolutional networks – one of the most popular and effective deep network models. </span><span class="koboSpan" id="kobo.975.2">We’ll talk about their structure, building blocks, and what makes them uniquely suited to computer vision tasks. </span><span class="koboSpan" id="kobo.975.3">To spark your interest, let’s recall that convolutional networks have consistently won the popular ImageNet challenge since 2012, delivering top-five accuracy from 74.2% </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">to 99%.</span></span></p>
</div>


<div class="Content" id="_idContainer349">
<h1 id="_idParaDest-62" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.1.1">Part 2: </span><br/><span class="koboSpan" id="kobo.2.1">Deep Neural Networks for Computer Vision</span></h1>
</div>
<div id="_idContainer350">
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In this part, we’ll introduce </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">CNNs</span></strong><span class="koboSpan" id="kobo.7.1">) – a type of neural network suitable for computer vision applications. </span><span class="koboSpan" id="kobo.7.2">Building on top of the first three chapters, we’ll discuss the rationale behind CNNs, their building blocks, and their architecture. </span><span class="koboSpan" id="kobo.7.3">We’ll also outline the most popular CNN models in use today. </span><span class="koboSpan" id="kobo.7.4">Finally, we’ll focus on the advanced applications of CNNs – object detection, image segmentation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">image generation.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.9.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following chapters:</span></span></p>
<ul>
<li lang="en-GB"><a href="B19627_04.xhtml#_idTextAnchor107"><em class="italic"><span class="koboSpan" id="kobo.11.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.12.1">, </span><em class="italic"><span class="koboSpan" id="kobo.13.1">Computer Vision with Convolutional Networks</span></em></li>
<li lang="en-GB"><a href="B19627_05.xhtml#_idTextAnchor146"><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">Advanced Computer Vision Applications</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer351">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer352">
</div>
</div>
</body></html>