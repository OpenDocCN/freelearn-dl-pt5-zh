<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch010.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-5-principled-approaches-for-bayesian-deep-learning" class="level1 chapterHead" data-number="10">
<h1 class="chapterHead" data-number="10"><span class="titlemark">ChapterÂ 5</span><br/>
<span id="x1-600005"></span>Principled Approaches for Bayesian Deep Learning</h1>
<p>Now that weâ€™ve introduced the concept of <strong>Bayesian Neural Networks</strong> (<strong>BNNs</strong>), weâ€™re ready to explore the various ways in which they can be implemented. As we discussed previously, ideal BNNs are computationally intensive, becoming intractable with more sophisticated architectures or larger amounts of data. In recent years, researchers have developed a range of methods that make BNNs tractable, allowing them to be implemented with larger and more sophisticated neural network architectures.</p>
<p>In this chapter, weâ€™ll explore two particularly popular methods: <strong>Probabilistic</strong> <strong>Backpropagation</strong> (<strong>PBP</strong>) and <strong>Bayes by Backprop</strong> (<strong>BBB</strong>). Both methods can be referred to as <em>probabilistic neural network models</em>:<span id="dx1-60001"></span> neural networks designed to learn probabilities over their weights, rather than simply learning point estimates (a fundamental defining feature of BNNs, as we learned in <a href="CH4.xhtml#x1-490004"><em>ChapterÂ 4</em></a>, <a href="CH4.xhtml#x1-490004"><em>Introducing Bayesian Deep Learning</em></a>). Because they explicitly learn distributions over the weights at training time, we refer to them as <em>principled</em> methods; in contrast to the methods weâ€™ll explore in the next chapter, which more loosely approximate Bayesian inference with neural networks. Weâ€™ll cover these topics in the following sections of this chapter:</p>
<ul>
<li><p>Explaining notation</p></li>
<li><p>Familiar probabilistic concepts from deep learning</p></li>
<li><p>Bayesian inference by backpropagation</p></li>
<li><p>Implementing BBB with TensorFlow</p></li>
<li><p>Scalable Bayesian deep learning with PBP</p></li>
<li><p>Implementing PBP</p></li>
</ul>
<p>First, letâ€™s quickly review the technical requirements for this chapter. <span id="x1-60002r113"></span></p>
<section id="technical-requirements-3" class="level2 sectionHead" data-number="10.1">
<h2 class="sectionHead" data-number="10.1" id="sigil_toc_id_54"><span class="titlemark">5.1 </span> <span id="x1-610001"></span>Technical requirements</h2>
<p>To complete the practical tasks in this chapter, you will need a Python 3.8 environment with the Python SciPy stack and the following additional Python packages installed:</p>
<ul>
<li><p>TensorFlow 2.0</p></li>
<li><p>TensorFlow Probability</p></li>
</ul>
<p>All of the code for this book can be found on the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-61001r115"></span></p>
</section>
<section id="explaining-notation" class="level2 sectionHead" data-number="10.2">
<h2 class="sectionHead" data-number="10.2" id="sigil_toc_id_55"><span class="titlemark">5.2 </span> <span id="x1-620002"></span>Explaining notation</h2>
<p>While weâ€™ve introduced much of the notation used throughout the book in the previous chapters, weâ€™ll be introducing more notation associated with BDL in the following chapters. As such, weâ€™ve provided an overview of the notation here for reference:<span id="dx1-62001"></span></p>
<ul>
<li><p><em>Î¼</em>: The mean. To make it easy to cross-reference our chapter with the original Probabilistic Backpropagation paper, this is represented as <em>m</em> when discussing PBP.</p></li>
<li><p><em>Ïƒ</em>: The standard deviation.</p></li>
<li><p><em>Ïƒ</em><sup><span class="cmr-8">2</span></sup>: The variance (meaning the square of the standard deviation). To make it easy to cross-reference our chapter with the paper, this is represented as <em>v</em> when discussing PBP.<span id="dx1-62002"></span></p></li>
<li><p><strong>x</strong>: A single vector input to our model. If considering multiple inputs, weâ€™ll use <strong>X</strong> to represent a matrix comprising multiple vector inputs.</p></li>
<li><p><span class="accenthat"><strong>x</strong></span>: An approximation of our input <strong>x</strong>.</p></li>
<li><p><em>y</em>: A single scalar target. When considering multiple targets, weâ€™ll use <strong>y</strong> to represent a vector of multiple scalar targets.</p></li>
<li><p><em>Å·</em>: A single scalar output from our model. When considering multiple outputs, weâ€™ll use <strong>Å·</strong> to represent a vector of multiple scalar outputs.</p></li>
<li><p><strong>z</strong>: The output of an intermediate layer of our model.</p></li>
<li><p><em>P</em>: Some ideal or target distribution.</p></li>
<li><p><em>Q</em>: An approximate distribution.</p></li>
<li><p><em>KL</em>[<em>Q</em><span class="cmsy-10x-x-109">âˆ¥</span><em>P</em>]: The KL preergence between our target distribution <em>P</em> and our approximate distribution <em>Q</em>.</p></li>
<li><p><span class="cmsy-10x-x-109">â„’</span>: The loss.</p></li>
<li><p>: The expectation.</p></li>
<li><p><em>N</em>(<em>Î¼,Ïƒ</em>): A normal (or Gaussian) distribution parameterized by the mean <em>Î¼</em> and the standard deviation <em>Ïƒ</em>.</p></li>
<li><p><em>ğœƒ</em>: A set of model parameters.</p></li>
<li><p>Î”: A gradient.</p></li>
<li><p><em>âˆ‚</em>: A partial derivative.</p></li>
<li><p><em>f</em>(): Some function (e.g. <em>y</em> = <em>f</em>(<em>x</em>) indicates that <em>y</em> is produced by applying function <em>f</em>() to input <em>x</em>).</p></li>
</ul>
<p>We will encounter different variations of this notation, using different subscripts or variable combinations. <span id="x1-62003r116"></span></p>
</section>
<section id="familiar-probabilistic-concepts-from-deep-learning" class="level2 sectionHead" data-number="10.3">
<h2 class="sectionHead" data-number="10.3" id="sigil_toc_id_56"><span class="titlemark">5.3 </span> <span id="x1-630003"></span>Familiar probabilistic concepts from deep learning</h2>
<p>While this book introduces many concepts that may be unfamiliar, you may find that some ideas discussed here are familiar<span id="dx1-63001"></span>. In particular, <strong>Variational</strong> <strong>Inference</strong> (<strong>VI</strong>)<span id="dx1-63002"></span> is something you may be familiar with due to its use in <strong>Variational Autoencoders</strong> (<strong>VAEs</strong>)<span id="dx1-63003"></span>.</p>
<p>As a quick refresher, VAEs are generative models that learn encodings that can be used to generate plausible data. Much like standard autoencoders, VAEs comprise an encoder-decoder architecture.</p>
<div class="IMG---Figure">
<img src="../media/file107.png" alt="PIC"/> <span id="x1-63004r1"></span> <span id="x1-63005"></span></div>
<p class="IMG---Caption">FigureÂ 5.1: Illustration of autoencoder architecture 
</p>
<p>With a standard autoencoder, the model learns a mapping from the encoder to the latent space, and then from the latent space to the decoder.</p>
<p>As we see here, our output is simply defined as <span class="accenthat"><strong>x</strong></span> = <em>f</em><sub><em>d</em></sub>(<strong>z</strong>), where our encoding <strong>z</strong> is simply: <strong>z</strong> = <em>f</em><sub><em>e</em></sub>(<strong>x</strong>), where <em>f</em><sub><em>e</em></sub>() and <em>f</em><sub><em>d</em></sub>() are our encoder and decoder functions, respectively. If we want to generate new data using values in our latent space, we could simply inject some random values into the input of our decoder; bypassing the encoder and randomly sampling from our latent space:</p>
<div class="IMG---Figure">
<img src="../media/file108.png" alt="PIC"/> <span id="x1-63006r2"></span> <span id="x1-63007"></span></div>
<p class="IMG---Caption">FigureÂ 5.2: Illustration of sampling from the latent space of a standard autoencoder 
</p>
<p>The problem with this approach is that a standard autoencoder doesnâ€™t do a great job of learning the structure of the latent space. This means that while weâ€™re free to randomly sample points in this space, thereâ€™s no guarantee that those points will correspond to something that can be processed by the decoder to generate plausible data.</p>
<p>In a VAE, the latent space is modeled<span id="dx1-63008"></span> as a distribution. Therefore, what was <strong>z</strong> = <em>f</em><sub><em>e</em></sub>(<strong>x</strong>) becomes <strong>z</strong> <span class="cmsy-10x-x-109">â‰ˆğ’©</span>(<em>Î¼</em><sub>x</sub>, <em>Ïƒ</em><sub>x</sub>); that is to say, our latent space <strong>z</strong> now becomes a Gaussian distribution conditioned on our input <strong>x</strong>. Now, when we want to generate data using our trained network, we can do so simply by sampling from a normal distribution.</p>
<p>To achieve this, we need to ensure that the latent space approximates a Gaussian distribution. To do so, we use the <strong>Kullback-Leibler divergence</strong> (or KL divergence)<span id="dx1-63009"></span> during training by incorporating it as a regularization term:</p>
<div class="math-display">
<img src="../media/file109.jpg" class="math-display" alt=" 2 â„’ = âˆ¥xâˆ’ Ë†x âˆ¥ + KL [Q âˆ¥P] "/>
</div>
<p>Here, <em>P</em> is our target distribution (in this case, a multivariate Gaussian distribution), which weâ€™re trying to approximate with <em>Q</em>, which is the distribution associated with our latent space, which in this case is as follows:</p>
<div class="math-display">
<img src="../media/file110.jpg" class="math-display" alt="Q = z â‰ˆ ğ’© (Î¼,Ïƒ) "/><span id="x1-63006r3"></span>
</div>
<p>So, our loss now becomes:</p>
<div class="math-display">
<img src="../media/file111.jpg" class="math-display" alt="â„’ = âˆ¥xâˆ’ Ë†x âˆ¥2 + KL [q(z|x)âˆ¥p(z )] "/>
</div>
<p>We can expand it as follows:</p>
<div class="math-display">
<img src="../media/file112.jpg" class="math-display" alt="â„’ = âˆ¥x âˆ’ Ë†xâˆ¥2 + KL [ğ’© (Î¼,Ïƒ)âˆ¥ğ’© (0,I)] "/>
</div>
<p>Here, <em>I</em> is the identity matrix. This will allow our latent space to converge on our Gaussian prior, while also minimizing the reconstruction loss. The KL divergence can additionally be rewritten as follows:</p>
<div class="math-display">
<img src="../media/file113.jpg" class="math-display" alt="KL [q(z|x )âˆ¥p (z)] =q (z|x) logq(z|x )âˆ’ q(z|x) log p(z) "/>
</div>
<p>The terms on the right hand side of our equation here are the expectation (or mean) of log <em>q</em>(<strong>z</strong><span class="cmsy-10x-x-109">|</span><strong>x</strong>) and log <em>p</em>(<strong>z</strong>). As we know from <a href="CH2.xhtml#x1-250002"><em>ChapterÂ 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of</em> <em>Bayesian Inference</em></a> and <a href="CH4.xhtml#x1-490004"><em>ChapterÂ 4</em></a>, <a href="CH4.xhtml#x1-490004"><em>Introducing Bayesian Deep Learning</em></a>, we can obtain the the expectation of a given distribution by sampling. Thus, as we can see that all terms of our KL divergence are expectations computed with respect to our approximate distribution <em>q</em>(<strong>z</strong><span class="cmsy-10x-x-109">|</span><strong>x</strong>), we can approximate our KL divergence by sampling from <em>q</em>(<strong>z</strong><span class="cmsy-10x-x-109">|</span><strong>x</strong>), which is exactly what weâ€™re about to do!</p>
<p>Now that our encoding is represented by the distribution shown in equation <a href="#x1-63006r3">5.3</a>, our neural network<span id="dx1-63010"></span> structure has to change. We need to learn the mean (<em>Î¼</em>) and standard deviation (<em>Ïƒ</em>) parameters of our distribution:</p>
<div class="IMG---Figure">
<img src="../media/file114.png" alt="PIC"/> <span id="x1-63011r3"></span> <span id="x1-63012"></span></div>
<p class="IMG---Caption">FigureÂ 5.3: Illustration of autoencoder architecture with mean and standard deviation weights 
</p>
<p>The issue with constructing a VAE in this way is that our encoding <em>z</em> is now stochastic, rather than deterministic. This is a problem because we canâ€™t obtain a gradient for stochastic variables â€“ and if we canâ€™t obtain a gradient, we have nothing to backpropagate â€“ so we canâ€™t learn!</p>
<p>We can fix this using something called the <strong>reparameterization trick</strong><span id="dx1-63013"></span>. The reparameterization trick involves modifying how we compute <strong>z</strong>. Instead of sampling <strong>z</strong> from our distribution parameters, we will define it as follows:</p>
<div class="math-display">
<img src="../media/file115.jpg" class="math-display" alt="z = Î¼ + Ïƒ âŠ™ ğœ– "/>
</div>
<p>As you can see, weâ€™ve introduced a new variable, <em>ğœ–</em>, which is sampled from a Gaussian distribution with <em>Î¼</em> = 0 and <em>Ïƒ</em> = 1:</p>
<div class="math-display">
<img src="../media/file116.jpg" class="math-display" alt="ğœ– = ğ’© (0,1) "/>
</div>
<p>Introducing <em>ğœ–</em> has allowed us to move the stochasticity out of our backpropagation path<span id="dx1-63014"></span>. With the stochasticity residing solely in <em>ğœ–</em>, weâ€™re able to backpropagate through our weights as normal:</p>
<div class="IMG---Figure">
<img src="../media/file117.png" alt="PIC"/> <span id="x1-63015r4"></span> <span id="x1-63016"></span></div>
<p class="IMG---Caption">FigureÂ 5.4: Illustration of typical VAE architecture with mean and standard deviation weights, having moved the sampling component out of the backpropagation path 
</p>
<p>This means weâ€™re able to represent our encoding as a distribution while still being able to backpropagate the gradient of <em>z</em>: learning the parameters <em>Î¼</em> and <em>Ïƒ</em>, and using <em>ğœ–</em> to sample from the distribution. Being able to represent <em>z</em> as a distribution means weâ€™re able to use it to compute the KL divergence, allowing us to incorporate our regularization term in equation 5.1, which in turn allows our embedding to converge towards a Gaussian distribution during training.</p>
<p>These are the fundamental steps in variational learning, and are what turn our standard autoencoder into a VAE<span id="dx1-63017"></span>. But this isnâ€™t all about learning. Crucially for VAEs, because weâ€™ve learned a normally distributed latent space, we can now sample effectively from that latent space, enabling us to use our VAE to generate new data according to the data landscape learned during training. Unlike the brittle random sampling we had with a standard autoencoder, our VAE is now able to generate <em>plausible</em> data!</p>
<p>To do this, we sample <em>ğœ–</em> from a normal distribution and multiply <em>Ïƒ</em> by this value. This gives us a sample of <em>z</em> to pass through our decoder, obtaining our generated data, <span class="accenthat"><strong>x</strong></span>, at the output.</p>
<p>Now that weâ€™re familiar with the fundamentals of variational learning, in the next section weâ€™ll see how these principles can be applied to create BNNs. <span id="x1-63018r117"></span></p>
</section>
<section id="bayesian-inference-by-backpropagation" class="level2 sectionHead" data-number="10.4">
<h2 class="sectionHead" data-number="10.4" id="sigil_toc_id_57"><span class="titlemark">5.4 </span> <span id="x1-640004"></span>Bayesian inference by backpropagation</h2>
<p>In their 2015 paper, <em>Weight Uncertainty in Neural Networks</em>, Charles Blundell and his colleagues at DeepMind introduced a method for using variational learning for Bayesian inference with neural networks. Their method, which learned the BNN parameters via standard backpropagation, was appropriately named <strong>Bayes by Backprop</strong> (<strong>BBB</strong>)<span id="dx1-64001"></span>.</p>
<p>In the previous section, we saw how we can use variational learning to estimate the posterior distribution of our encoding, <em>z</em>, learning <em>P</em>(<em>z</em><span class="cmsy-10x-x-109">|</span><em>x</em>). For BBB, weâ€™re going to be doing very much the same thing, except this time itâ€™s not just the encoding we care about. This time we want to learn the posterior distribution over all of the parameters (or weights) of our network: <em>P</em>(<em>ğœƒ</em><span class="cmsy-10x-x-109">|</span><em>D</em>).</p>
<p>You can think of this as having an entire network made up of VAE encoding layers, looking something like this:</p>
<div class="IMG---Figure">
<img src="../media/file118.png" alt="PIC"/> <span id="x1-64002r5"></span> <span id="x1-64003"></span></div>
<p class="IMG---Caption">FigureÂ 5.5: Illustration of BBB 
</p>
<p>As such, itâ€™s logical that the learning strategy is also similar to that which we used for the VAE. We again use the principle of variational learning to learn parameters for <em>Q</em>, and approximation of the true distribution <em>P</em>, but this time weâ€™re looking for the parameters <em>ğœƒ</em><sup><em>â‹†</em></sup> that minimize this:</p>
<div class="math-display">
<img src="../media/file119.jpg" class="math-display" alt="ğœƒâ‹† = ğœƒ KL [q(w |ğœƒ)||P(w |D)] "/>
</div>
<p>Here, <em>D</em> is our data, <strong>w</strong> is our network weights, and <em>ğœƒ</em> is the parameters of our distribution, e.g. <em>Î¼</em> and <em>Ïƒ</em> in the case of a Gaussian distribution. To do this we make use of an important cost function in Bayesian learning: the <strong>Evidence Lower</strong> <strong>Bound</strong><sup><a href="#footnote1" id="footref1" class="footnote-ref">1</a></sup><span id="x1-64004f1"></span> , or <strong>ELBO</strong> <span id="dx1-64005"></span>(also referred to as the variational free energy). We denote this with the following:</p>

<div class="math-display">
<img src="../media/file120.jpg" class="math-display" alt="â„’(D,ğœƒ ) = KL [q(w |ğœƒ)||P (w)]âˆ’ q(w |ğœƒ) [log P(D |w)] "/>
</div>
<p>This looks rather complicated, but itâ€™s really just a generalization of what we saw in equation 5.4. We can break it down as follows:</p>
<ol>
<li><div id="x1-64007x1">
<p>On the left-hand side, we have the KL divergence between our prior <em>P</em>(<strong>w</strong>) and our approximate distribution <em>q</em>(<strong>w</strong><span class="cmsy-10x-x-109">|</span><em>ğœƒ</em>). This is similar to what we saw in equations 5.1-5.4 in the previous section. Incorporating the KL divergence in our loss<span id="dx1-64008"></span> allows us to tune our parameters <em>ğœƒ</em> such that our approximate distribution converges on our prior distribution.</p>
</div></li>
<li><div id="x1-64010x2">
<p>On the right-hand side, we have the expectation of the negative log-likelihood of our data <em>D</em> given our neural network weights <strong>w</strong> with respect to the variational distribution. Minimizing this (because itâ€™s the <em>negative log-likelihood</em>) ensures that we learn parameters that maximize the likelihood of our data given our weights; our network learns to map our inputs to our outputs.</p>
</div></li>
</ol>
<p>Just as with VAEs, BBB<span id="dx1-64011"></span> makes use of the reparameterization trick to allow us to backpropagate gradients through our network parameters. Also as before, we sample from our distribution. Taking the form of the KL divergence introduced in equation 5.5, our loss becomes as follows:</p>
<div class="math-display">
<img src="../media/file121.jpg" class="math-display" alt=" âˆ‘N â„’ (D,ğœƒ) â‰ˆ logq(wi |ğœƒ)âˆ’ log P(wi) âˆ’ log P(D |wi) i=1 "/>
</div>
<p><em>N</em> is the number of samples, and <em>i</em> denotes a particular sample. While weâ€™ll be using Gaussian priors here, an interesting property of this approach is that it can be applied to a wide range of distributions.</p>
<p>The next step is to use our weight samples to train our network:</p>
<ol>
<li><div id="x1-64013x1">
<p>First, just as with VAEs, we sample <em>ğœ–</em> from a Gaussian distribution:</p>
<div class="math-display">
<img src="../media/file122.jpg" class="math-display" alt="ğœ– â‰ˆ ğ’© (0,I) "/>
</div>
</div></li>
<li><div id="x1-64015x2">
<p>Next, we apply <em>ğœ–</em> to the weights in a particular layer, just as with our VAE encoding:</p>
<div class="math-display">
<img src="../media/file123.jpg" class="math-display" alt="w = Î¼ + log(1 + exp(Ï))âŠ™ ğœ– "/>
</div>
<p>Note that in BBB, <em>Ïƒ</em> is parameterized as <em>Ïƒ</em> = log(1 + exp(<em>Ï</em>)). This ensures that it is always non-negative (because a standard deviation cannot be negative!).</p>
</div></li>
<li><div id="x1-64017x3">
<p>With our parameters <em>ğœƒ</em> = (<em>Î¼,Ï</em>), we define our loss, following equation 3.10, as follows:</p>
<div class="math-display">
<img src="../media/file124.jpg" class="math-display" alt="f(w, ğœƒ) = logq(w |ğœƒ )âˆ’ log P (w )P (D |w ) "/>
</div>
</div></li>
<li><div id="x1-64019x4">
<p>Because our neural network is made up of weights for both means and standard deviations, we need to calculate the gradients for them separately. We first calculate the gradient with respect to the mean, <em>Î¼</em>:</p>
<div class="math-display">
<img src="../media/file125.jpg" class="math-display" alt=" âˆ‚f(w,-ğœƒ) âˆ‚f-(w,ğœƒ) Î” Î¼ = âˆ‚w + âˆ‚Î¼ "/>
</div>
<p>Then we calculate the gradient with respect to the standard deviation parameter, <em>Ï</em>:</p>
<div class="math-display">
<img src="../media/file126.jpg" class="math-display" alt=" âˆ‚f(w, ğœƒ) ğœ– âˆ‚f (w, ğœƒ) Î” Ï = --------------------+ -------- âˆ‚w 1 + exp(âˆ’ Ï) âˆ‚Ï "/>
</div>
</div></li>
<li><div id="x1-64021x5">
<p>Now we have all the components necessary to update our weights via backpropagation<span id="dx1-64022"></span>, in a similar fashion to a typical neural network, except we update our mean and variance weights with their respective gradients:</p>
<div class="math-display">
<img src="../media/file127.jpg" class="math-display" alt="Î¼ â† Î¼ âˆ’ Î± Î”Î¼ "/>
</div>
<div class="math-display">
<img src="../media/file128.jpg" class="math-display" alt="Ï â† Ï âˆ’ Î±Î” Ï "/>
</div>
</div></li>
</ol>
<p>You may have noticed that the first terms of the gradient computations in equations 5.14 and 5.15 are the gradients you would compute for backpropagation of a typical neural network; weâ€™re simply augmenting these gradients with <em>Î¼</em>- and <em>Ï</em>-specific update rules.</p>
<p>While that was fairly heavy in terms of mathematical content, we can break it down into a few simple concepts:</p>
<ol>
<li><div id="x1-64024x1">
<p>Just as with the encoding in VAEs, we are working with weights that represent the mean and standard deviation of a multivariate distribution, except this time they make up our entire network, not just the encoding layer.</p>
</div></li>
<li><div id="x1-64026x2">
<p>Because of this, we again use a loss that incorporates the KL divergence: weâ€™re looking to maximize the ELBO.</p>
</div></li>
<li><div id="x1-64028x3">
<p>As we are dealing with mean and standard deviation weights, we update these separately with update rules that use the gradients for the respective set of weights.</p>
</div></li>
</ol>
<p>Now that we understand the core principles behind BBB, weâ€™re ready to see how it all comes together in code! <span id="x1-64029r122"></span></p>
</section>
<section id="implementing-bbb-with-tensorflow" class="level2 sectionHead" data-number="10.5">
<h2 class="sectionHead" data-number="10.5" id="sigil_toc_id_58"><span class="titlemark">5.5 </span> <span id="x1-650005"></span>Implementing BBB with TensorFlow</h2>
<p>In this section, weâ€™ll see how to implement BBB in TensorFlow<span id="dx1-65001"></span>. Some of the code youâ€™ll see will be familiar; the core concepts of layers, loss functions, and optimizers will be very similar to what we covered in <em>Chapter 3, Fundamentals of</em> <em>Deep Learning</em><span id="dx1-65002"></span>. Unlike the examples in <a href="CH3.xhtml#x1-350003"><em>ChapterÂ 3</em></a>, <a href="CH3.xhtml#x1-350003"><em>Fundamentals of Deep</em> <em>Learning</em></a>, weâ€™ll see how we can create neural networks capable of probabilistic inference.</p>
<section id="step-1-importing-packages" class="level4 likesubsubsectionHead" data-number="10.5.0.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.5.0.1"><span id="x1-660005"></span>Step 1: Importing packages</h4>
<p>We start by importing the relevant packages<span id="dx1-66001"></span>. Importantly, we will import <code>tensorflow-probability</code>, which will provide us with the layers of the network that replace the point-estimate with a distribution and implement the reparameterization trick. We also set the global parameter for the number of inferences, which will determine how often we sample from the network later:</p>
<pre id="fancyvrb40" class="fancyvrb"><span id="x1-66010r1"></span> 
<code><span id="textcolor672"><span>import</span></span><span>Â </span><span id="textcolor673"><span>tensorflow</span></span><span>Â </span><span id="textcolor674"><span>as</span></span><span>Â </span><span id="textcolor675"><span>tf</span></span> <span id="x1-66012r2"></span> </code>
<code><span id="textcolor676"><span>import</span></span><span>Â </span><span id="textcolor677"><span>numpy</span></span><span>Â </span><span id="textcolor678"><span>as</span></span><span>Â </span><span id="textcolor679"><span>np</span></span> <span id="x1-66014r3"></span> </code>
<code><span id="textcolor680"><span>import</span></span><span>Â </span><span id="textcolor681"><span>matplotlib.pyplot</span></span><span>Â </span><span id="textcolor682"><span>as</span></span><span>Â </span><span id="textcolor683"><span>plt</span></span> <span id="x1-66016r4"></span> </code>
<code><span id="textcolor684"><span>import</span></span><span>Â </span><span id="textcolor685"><span>tensorflow_probability</span></span><span>Â </span><span id="textcolor686"><span>as</span></span><span>Â </span><span id="textcolor687"><span>tfp</span></span> <span id="x1-66018r5"></span> </code>
<code><span id="x1-66020r6"></span></code>
<code><span>NUM_INFERENCES</span><span>Â </span><span id="textcolor688"><span>=</span></span><span>Â </span><span id="textcolor689"><span>7</span></span></code></pre>
</section>
<section id="step-2-acquiring-data" class="level4 likesubsubsectionHead" data-number="10.5.0.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.5.0.2"><span id="x1-670005"></span>Step 2: Acquiring data</h4>
<p>We then download the MNIST Fashion dataset, which is a dataset that contains images of ten different clothing items<span id="dx1-67001"></span>. We also set the class names and derive the number of training examples and classes:</p>
<pre id="fancyvrb41" class="fancyvrb"><span id="x1-67014r1"></span> 
<code><span id="textcolor690"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â download</span><span class="cmitt-10x-x-109">Â MNIST</span><span class="cmitt-10x-x-109">Â fashion</span><span class="cmitt-10x-x-109">Â data</span><span class="cmitt-10x-x-109">Â set</span></span> <span id="x1-67016r2"></span> </code>
<code><span>fashion_mnist</span><span>Â </span><span id="textcolor691"><span>=</span></span><span>Â tf</span><span id="textcolor692"><span>.</span></span><span>keras</span><span id="textcolor693"><span>.</span></span><span>datasets</span><span id="textcolor694"><span>.</span></span><span>fashion_mnist</span> <span id="x1-67018r3"></span> </code>
<code><span>(train_images,</span><span>Â train_labels),</span><span>Â (test_images,</span><span>Â test_labels)</span><span>Â </span><span id="textcolor695"><span>=</span></span><span>Â fashion_mnist</span><span id="textcolor696"><span>.</span></span><span>load_data()</span> <span id="x1-67020r4"></span> </code>
<code><span id="x1-67022r5"></span></code>
<code><span id="textcolor697"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â set</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â names</span></span> <span id="x1-67024r6"></span> </code>
<code><span>CLASS_NAMES</span><span>Â </span><span id="textcolor698"><span>=</span></span><span>Â [</span><span id="textcolor699"><span>'T-shirt'</span></span><span>,</span><span>Â </span><span id="textcolor700"><span>'Trouser'</span></span><span>,</span><span>Â </span><span id="textcolor701"><span>'Pullover'</span></span><span>,</span><span>Â </span><span id="textcolor702"><span>'Dress'</span></span><span>,</span><span>Â </span><span id="textcolor703"><span>'Coat'</span></span><span>,</span> <span id="x1-67026r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor704"><span>'Sandal'</span></span><span>,</span><span>Â </span><span id="textcolor705"><span>'Shirt'</span></span><span>,</span><span>Â </span><span id="textcolor706"><span>'Sneaker'</span></span><span>,</span><span>Â </span><span id="textcolor707"><span>'Bag'</span></span><span>,</span><span>Â </span><span id="textcolor708"><span>'Ankle</span><span>Â boot'</span></span><span>]</span> <span id="x1-67028r8"></span> </code>
<code><span id="x1-67030r9"></span></code>
<code><span id="textcolor709"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â derive</span><span class="cmitt-10x-x-109">Â number</span><span class="cmitt-10x-x-109">Â training</span><span class="cmitt-10x-x-109">Â examples</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â classes</span></span> <span id="x1-67032r10"></span> </code>
<code><span>NUM_TRAIN_EXAMPLES</span><span>Â </span><span id="textcolor710"><span>=</span></span><span>Â </span><span id="textcolor711"><span>len</span></span><span>(train_images)</span> <span id="x1-67034r11"></span> </code>
<code><span>NUM_CLASSES</span><span>Â </span><span id="textcolor712"><span>=</span></span><span>Â </span><span id="textcolor713"><span>len</span></span><span>(CLASS_NAMES)</span></code></pre>
</section>
<section id="step-3-helper-functions" class="level4 likesubsubsectionHead" data-number="10.5.0.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.5.0.3"><span id="x1-680005"></span>Step 3: Helper functions</h4>
<p>Next, we create a helper function<span id="dx1-68001"></span> that defines our model. As you can see, we use a very simple convolutional neural network structure for image classification that consists of a convolutional layer, followed by a max-pooling layer and a fully connected layer. The convolutional layer and the dense layer are imported from the <code>tensorflow-probability</code> package, as indicated by the prefix <em>tfp</em>. Instead of defining point-estimates for the weights, they will define weight distributions.</p>
<p>As the names <code>Convolution2DReparameterization</code> and <code>DenseReparameterization</code> suggest, these layers will use the reparameterization trick to update the weight parameters during backpropagation:</p>
<pre id="fancyvrb42" class="fancyvrb"><span id="x1-68027r1"></span> 
<code><span id="textcolor715"><span>def</span></span><span>Â </span><span id="textcolor716"><span>define_bayesian_model</span></span><span>():</span> <span id="x1-68029r2"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor717"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â computing</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â KL</span><span class="cmitt-10x-x-109">Â divergence</span></span> <span id="x1-68031r3"></span> </code>
<code><span>Â </span><span>Â kl_divergence_function</span><span>Â </span><span id="textcolor718"><span>=</span></span><span>Â </span><span id="textcolor719"><span>lambda</span></span><span>Â q,</span><span>Â p,</span><span>Â _:</span><span>Â tfp</span><span id="textcolor720"><span>.</span></span><span>distributions</span><span id="textcolor721"><span>.</span></span><span>kl_divergence(</span> <span id="x1-68033r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â q,</span><span>Â p</span> <span id="x1-68035r5"></span> </code>
<code><span>Â </span><span>Â )</span><span>Â </span><span id="textcolor722"><span>/</span></span><span>Â tf</span><span id="textcolor723"><span>.</span></span><span>cast(NUM_TRAIN_EXAMPLES,</span><span>Â dtype</span><span id="textcolor724"><span>=</span></span><span>tf</span><span id="textcolor725"><span>.</span></span><span>float32)</span> <span id="x1-68037r6"></span> </code>
<code><span id="x1-68039r7"></span></code>
<code><span>Â </span><span>Â </span><span id="textcolor726"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â model</span></span> <span id="x1-68041r8"></span> </code>
<code><span>Â </span><span>Â model</span><span>Â </span><span id="textcolor727"><span>=</span></span><span>Â tf</span><span id="textcolor728"><span>.</span></span><span>keras</span><span id="textcolor729"><span>.</span></span><span>models</span><span id="textcolor730"><span>.</span></span><span>Sequential([</span> <span id="x1-68043r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tfp</span><span id="textcolor731"><span>.</span></span><span>layers</span><span id="textcolor732"><span>.</span></span><span>Convolution2DReparameterization(</span> <span id="x1-68045r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor733"><span>64</span></span><span>,</span><span>Â kernel_size</span><span id="textcolor734"><span>=</span></span><span id="textcolor735"><span>5</span></span><span>,</span><span>Â padding</span><span id="textcolor736"><span>=</span></span><span id="textcolor737"><span>'SAME'</span></span><span>,</span> <span id="x1-68047r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â kernel_divergence_fn</span><span id="textcolor738"><span>=</span></span><span>kl_divergence_function,</span> <span id="x1-68049r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â activation</span><span id="textcolor739"><span>=</span></span><span>tf</span><span id="textcolor740"><span>.</span></span><span>nn</span><span id="textcolor741"><span>.</span></span><span>relu),</span> <span id="x1-68051r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor742"><span>.</span></span><span>keras</span><span id="textcolor743"><span>.</span></span><span>layers</span><span id="textcolor744"><span>.</span></span><span>MaxPooling2D(</span> <span id="x1-68053r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â pool_size</span><span id="textcolor745"><span>=</span></span><span>[</span><span id="textcolor746"><span>2</span></span><span>,</span><span>Â </span><span id="textcolor747"><span>2</span></span><span>],</span><span>Â strides</span><span id="textcolor748"><span>=</span></span><span>[</span><span id="textcolor749"><span>2</span></span><span>,</span><span>Â </span><span id="textcolor750"><span>2</span></span><span>],</span> <span id="x1-68055r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â padding</span><span id="textcolor751"><span>=</span></span><span id="textcolor752"><span>'SAME'</span></span><span>),</span> <span id="x1-68057r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor753"><span>.</span></span><span>keras</span><span id="textcolor754"><span>.</span></span><span>layers</span><span id="textcolor755"><span>.</span></span><span>Flatten(),</span> <span id="x1-68059r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tfp</span><span id="textcolor756"><span>.</span></span><span>layers</span><span id="textcolor757"><span>.</span></span><span>DenseReparameterization(</span> <span id="x1-68061r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â NUM_CLASSES,</span><span>Â kernel_divergence_fn</span><span id="textcolor758"><span>=</span></span><span>kl_divergence_function,</span> <span id="x1-68063r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â activation</span><span id="textcolor759"><span>=</span></span><span>tf</span><span id="textcolor760"><span>.</span></span><span>nn</span><span id="textcolor761"><span>.</span></span><span>softmax)</span> <span id="x1-68065r20"></span> </code>
<code><span>Â </span><span>Â ])</span> <span id="x1-68067r21"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor762"><span>return</span></span><span>Â model</span></code></pre>
<p>We also create another helper function<span id="dx1-68068"></span> that compiles the model for us, using <code>Adam</code> as our optimizer and a categorical cross-entropy loss. Provided with this loss and the preceding network structure, <code>tensorflow-probability</code> will automatically add the Kullback-Leibler divergence that is contained in the convolutional and dense layers to the cross-entropy loss. This combination effectively amounts to calculating the ELBO loss that we described in equation 5.9:</p>
<pre id="fancyvrb43" class="fancyvrb"><span id="x1-68081r1"></span> 
<code><span id="textcolor764"><span>def</span></span><span>Â </span><span id="textcolor765"><span>compile_bayesian_model</span></span><span>(model):</span> <span id="x1-68083r2"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor766"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â optimizer</span></span> <span id="x1-68085r3"></span> </code>
<code><span>Â </span><span>Â optimizer</span><span>Â </span><span id="textcolor767"><span>=</span></span><span>Â tf</span><span id="textcolor768"><span>.</span></span><span>keras</span><span id="textcolor769"><span>.</span></span><span>optimizers</span><span id="textcolor770"><span>.</span></span><span>Adam()</span> <span id="x1-68087r4"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor771"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â compile</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â model</span></span> <span id="x1-68089r5"></span> </code>
<code><span>Â </span><span>Â model</span><span id="textcolor772"><span>.</span></span><span>compile(optimizer,</span><span>Â loss</span><span id="textcolor773"><span>=</span></span><span id="textcolor774"><span>'categorical_crossentropy'</span></span><span>,</span> <span id="x1-68091r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â metrics</span><span id="textcolor775"><span>=</span></span><span>[</span><span id="textcolor776"><span>'accuracy'</span></span><span>],</span><span>Â experimental_run_tf_function</span><span id="textcolor777"><span>=</span></span><span id="textcolor778"><span>False</span></span><span>)</span> <span id="x1-68093r7"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor779"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â build</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â model</span></span> <span id="x1-68095r8"></span> </code>
<code><span>Â </span><span>Â model</span><span id="textcolor780"><span>.</span></span><span>build(input_shape</span><span id="textcolor781"><span>=</span></span><span>[</span><span id="textcolor782"><span>None</span></span><span>,</span><span>Â </span><span id="textcolor783"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor784"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor785"><span>1</span></span><span>])</span> <span id="x1-68097r9"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor786"><span>return</span></span><span>Â model</span></code></pre>
</section>
<section id="step-4-model-training" class="level4 likesubsubsectionHead" data-number="10.5.0.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.5.0.4"><span id="x1-690005"></span>Step 4: model training</h4>
<p>Before we can train the model, we first need to convert the labels of the training data from integers to one-hot vectors because this is what TensorFlow expects for the categorical cross-entropy loss<span id="dx1-69001"></span>. For example, if an image shows a t-shirt and the integer label for t-shirts is 1, then this label will be transformed like this: <span class="obeylines-h"><span class="verb"><code>[1,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0,</code><code>Â 0]</code></span></span>:</p>
<pre id="fancyvrb44" class="fancyvrb"><span id="x1-69004r1"></span> 
<code><span>train_labels_dense</span><span>Â </span><span id="textcolor787"><span>=</span></span><span>Â tf</span><span id="textcolor788"><span>.</span></span><span>one_hot(train_labels,</span><span>Â NUM_CLASSES)</span></code></pre>
<p>Now, we are ready to train our model on the training data. We will train for ten epochs:</p>
<pre id="fancyvrb45" class="fancyvrb"><span id="x1-69012r1"></span> 
<code><span id="textcolor789"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â use</span><span class="cmitt-10x-x-109">Â helper</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â architecture</span></span> <span id="x1-69014r2"></span> </code>
<code><span>bayesian_model</span><span>Â </span><span id="textcolor790"><span>=</span></span><span>Â define_bayesian_model()</span> <span id="x1-69016r3"></span> </code>
<code><span id="textcolor791"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â use</span><span class="cmitt-10x-x-109">Â helper</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â compile</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â model</span></span> <span id="x1-69018r4"></span> </code>
<code><span>bayesian_model</span><span>Â </span><span id="textcolor792"><span>=</span></span><span>Â compile_bayesian_model(bayesian_model)</span> <span id="x1-69020r5"></span> </code>
<code><span id="textcolor793"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â initiate</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â training</span></span> <span id="x1-69022r6"></span> </code>
<code><span>bayesian_model</span><span id="textcolor794"><span>.</span></span><span>fit(train_images,</span><span>Â train_labels_dense,</span><span>Â epochs</span><span id="textcolor795"><span>=</span></span><span id="textcolor796"><span>10</span></span><span>)</span></code></pre>
</section>
<section id="step-5-inference" class="level4 likesubsubsectionHead" data-number="10.5.0.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.5.0.5"><span id="x1-700005"></span>Step 5: inference</h4>
<p>We can then use the trained model to perform inference on the test images. Here, we predict the class label for the first 50 images in the test split.<span id="dx1-70001"></span> For every image, we sample seven times from the network (as determined by <code>NUM_INFERENCES</code>), which will give us seven predictions for every image:</p>
<pre id="fancyvrb46" class="fancyvrb"><span id="x1-70008r1"></span> 
<code><span>NUM_SAMPLES_INFERENCE</span><span>Â </span><span id="textcolor797"><span>=</span></span><span>Â </span><span id="textcolor798"><span>50</span></span> <span id="x1-70010r2"></span> </code>
<code><span>softmax_predictions</span><span>Â </span><span id="textcolor799"><span>=</span></span><span>Â tf</span><span id="textcolor800"><span>.</span></span><span>stack(</span> <span id="x1-70012r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â [bayesian_model</span><span id="textcolor801"><span>.</span></span><span>predict(test_images[:NUM_SAMPLES_INFERENCE])</span> <span id="x1-70014r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor802"><span>for</span></span><span>Â _</span><span>Â </span><span id="textcolor803"><span>in</span></span><span>Â </span><span id="textcolor804"><span>range</span></span><span>(NUM_INFERENCES)],axis</span><span id="textcolor805"><span>=</span></span><span id="textcolor806"><span>0</span></span><span>)</span></code></pre>
<p>And thatâ€™s it: we have a working BBB model! Letâ€™s visualize the first image in the test split and the seven different predictions for that image. First, we obtain the class predictions:</p>
<pre id="fancyvrb47" class="fancyvrb"><span id="x1-70025r1"></span> 
<code><span id="textcolor807"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â first</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â test</span><span class="cmitt-10x-x-109">Â set</span></span> <span id="x1-70027r2"></span> </code>
<code><span>image_ind</span><span>Â </span><span id="textcolor808"><span>=</span></span><span>Â </span><span id="textcolor809"><span>0</span></span> <span id="x1-70029r3"></span> </code>
<code><span id="textcolor810"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â collect</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-70031r4"></span> </code>
<code><span>class_predictions</span><span>Â </span><span id="textcolor811"><span>=</span></span><span>Â []</span> <span id="x1-70033r5"></span> </code>
<code><span id="textcolor812"><span>for</span></span><span>Â ind</span><span>Â </span><span id="textcolor813"><span>in</span></span><span>Â </span><span id="textcolor814"><span>range</span></span><span>(NUM_INFERENCES):</span> <span id="x1-70035r6"></span> </code>
<code><span>Â </span><span>Â prediction_this_inference</span><span>Â </span><span id="textcolor815"><span>=</span></span><span>Â np</span><span id="textcolor816"><span>.</span></span><span>argmax(softmax_predictions[ind][image_ind])</span> <span id="x1-70037r7"></span> </code>
<code><span>Â </span><span>Â class_predictions</span><span id="textcolor817"><span>.</span></span><span>append(prediction_this_inference)</span> <span id="x1-70039r8"></span> </code>
<code><span id="textcolor818"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â human-readable</span><span class="cmitt-10x-x-109">Â form</span></span> <span id="x1-70041r9"></span> </code>
<code><span>predicted_classes</span><span>Â </span><span id="textcolor819"><span>=</span></span><span>Â [CLASS_NAMES[ind]</span><span>Â </span><span id="textcolor820"><span>for</span></span><span>Â ind</span><span>Â </span><span id="textcolor821"><span>in</span></span><span>Â class_predictions]</span></code></pre>
<p>Then, we visualize the image along with the predicted class for every inference:</p>
<pre id="fancyvrb48" class="fancyvrb"><span id="x1-70054r1"></span> 
<code><span id="textcolor822"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â caption</span></span> <span id="x1-70056r2"></span> </code>
<code><span>image_caption</span><span>Â </span><span id="textcolor823"><span>=</span></span><span>Â []</span> <span id="x1-70058r3"></span> </code>
<code><span id="textcolor824"><span>for</span></span><span>Â caption</span><span>Â </span><span id="textcolor825"><span>in</span></span><span>Â </span><span id="textcolor826"><span>range</span></span><span>(NUM_INFERENCES):</span> <span id="x1-70060r4"></span> </code>
<code><span>Â </span><span>Â image_caption</span><span id="textcolor827"><span>.</span></span><span>append(</span><span id="textcolor828"><span>f</span></span><span id="textcolor829"><span>"Sample</span><span>Â </span></span><span id="textcolor830"><span>{</span></span><span>caption</span><span id="textcolor831"><span>+</span></span><span id="textcolor832"><span>1</span></span><span id="textcolor833"><span>}</span></span><span id="textcolor834"><span>:</span><span>Â </span></span><span id="textcolor835"><span>{</span></span><span>predicted_classes[caption]</span><span id="textcolor836"><span>}</span></span><span id="textcolor837"><span>\n</span></span><span id="textcolor838"><span>"</span></span><span>)</span> <span id="x1-70062r5"></span> </code>
<code><span>image_caption</span><span>Â </span><span id="textcolor839"><span>=</span></span><span>Â </span><span id="textcolor840"><span>'</span><span>Â '</span></span><span id="textcolor841"><span>.</span></span><span>join(image_caption)</span> <span id="x1-70064r6"></span> </code>
<code><span id="textcolor842"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â visualise</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-70066r7"></span> </code>
<code><span>plt</span><span id="textcolor843"><span>.</span></span><span>figure(dpi</span><span id="textcolor844"><span>=</span></span><span id="textcolor845"><span>300</span></span><span>)</span> <span id="x1-70068r8"></span> </code>
<code><span>plt</span><span id="textcolor846"><span>.</span></span><span>title(</span><span id="textcolor847"><span>f</span></span><span id="textcolor848"><span>"Correct</span><span>Â class:</span><span>Â </span></span><span id="textcolor849"><span>{</span></span><span>CLASS_NAMES[test_labels[image_ind]]</span><span id="textcolor850"><span>}</span></span><span id="textcolor851"><span>"</span></span><span>)</span> <span id="x1-70070r9"></span> </code>
<code><span>plt</span><span id="textcolor852"><span>.</span></span><span>imshow(test_images[image_ind],</span><span>Â cmap</span><span id="textcolor853"><span>=</span></span><span>plt</span><span id="textcolor854"><span>.</span></span><span>cm</span><span id="textcolor855"><span>.</span></span><span>binary)</span> <span id="x1-70072r10"></span> </code>
<code><span>plt</span><span id="textcolor856"><span>.</span></span><span>xlabel(image_caption)</span> <span id="x1-70074r11"></span> </code>
<code><span>plt</span><span id="textcolor857"><span>.</span></span><span>show()</span></code></pre>
<p>Looking at the image in <em>Figure</em> <a href="#x1-70136r7"><em>5.7</em></a>, <span id="dx1-70075"></span>on most samples the network predicts Ankle boot (which is the correct class). For two of the samples, the network also predicted Sneaker, which is somewhat plausible given the image is showing a shoe.</p>
<div class="IMG---Figure">
<img src="../media/file129.png" alt="PIC"/> <span id="x1-70076r6"></span> <span id="x1-70077"></span></div>
<p class="IMG---Caption">FigureÂ 5.6: Class predictions across the seven different samples from the network trained with the BBB approach on the first test image in the MNIST fashion dataset 
</p>
<p>Given that we now have seven predictions per image, we can also calculate the mean variance across these predictions to approximate an uncertainty value:</p>
<pre id="fancyvrb49" class="fancyvrb"><span id="x1-70083r1"></span> 
<code><span id="textcolor858"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â calculate</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â across</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-70085r2"></span> </code>
<code><span>var_predictions</span><span>Â </span><span id="textcolor859"><span>=</span></span><span>Â tf</span><span id="textcolor860"><span>.</span></span><span>reduce_mean(</span> <span id="x1-70087r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor861"><span>.</span></span><span>math</span><span id="textcolor862"><span>.</span></span><span>reduce_variance(softmax_predictions,</span><span>Â axis</span><span id="textcolor863"><span>=</span></span><span id="textcolor864"><span>0</span></span><span>),</span> <span id="x1-70089r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â axis</span><span id="textcolor865"><span>=</span></span><span id="textcolor866"><span>1</span></span><span>)</span></code></pre>
<p>For example, the uncertainty value for the first test image in the MNIST fashion dataset is 0<em>.</em>0000002. To put this uncertainty value into context, letâ€™s load some images from the regular MNIST dataset<span id="dx1-70090"></span>, which contains handwritten digits between 0 and 9, and obtain uncertainty values from the model we have trained. We load the dataset and then perform inference again and obtain the uncertainty values:</p>
<pre id="fancyvrb50" class="fancyvrb"><span id="x1-70107r1"></span> 
<code><span id="textcolor867"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â load</span><span class="cmitt-10x-x-109">Â regular</span><span class="cmitt-10x-x-109">Â MNIST</span><span class="cmitt-10x-x-109">Â data</span><span class="cmitt-10x-x-109">Â set</span></span> <span id="x1-70109r2"></span> </code>
<code><span>(train_images_mnist,</span><span>Â train_labels_mnist),</span> <span id="x1-70111r3"></span> </code>
<code><span>(test_images_mnist,</span><span>Â test_labels_mnist)</span><span>Â </span><span id="textcolor868"><span>=</span></span> <span id="x1-70113r4"></span> </code>
<code><span>tf</span><span id="textcolor869"><span>.</span></span><span>keras</span><span id="textcolor870"><span>.</span></span><span>datasets</span><span id="textcolor871"><span>.</span></span><span>mnist</span><span id="textcolor872"><span>.</span></span><span>load_data()</span> <span id="x1-70115r5"></span> </code>
<code><span id="x1-70117r6"></span></code>
<code><span id="textcolor873"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â MNIST</span><span class="cmitt-10x-x-109">Â data</span></span> <span id="x1-70119r7"></span> </code>
<code><span>softmax_predictions_mnist</span><span>Â </span><span id="textcolor874"><span>=</span></span> <span id="x1-70121r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor875"><span>.</span></span><span>stack([bayesian_model</span><span id="textcolor876"><span>.</span></span><span>predict(</span> <span id="x1-70123r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â test_images_mnist[:NUM_SAMPLES_INFERENCE])</span> <span id="x1-70125r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor877"><span>for</span></span><span>Â _</span><span>Â </span><span id="textcolor878"><span>in</span></span><span>Â </span><span id="textcolor879"><span>range</span></span><span>(NUM_INFERENCES)],</span><span>Â axis</span><span id="textcolor880"><span>=</span></span><span id="textcolor881"><span>0</span></span><span>)</span> <span id="x1-70127r11"></span> </code>
<code><span id="x1-70129r12"></span></code>
<code><span id="textcolor882"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â calculate</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â across</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â MNIST</span><span class="cmitt-10x-x-109">Â data</span></span> <span id="x1-70131r13"></span> </code>
<code><span>var_predictions_mnist</span><span>Â </span><span id="textcolor883"><span>=</span></span><span>Â tf</span><span id="textcolor884"><span>.</span></span><span>reduce_mean(</span> <span id="x1-70133r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor885"><span>.</span></span><span>math</span><span id="textcolor886"><span>.</span></span><span>reduce_variance(softmax_predictions_mnist,</span><span>Â axis</span><span id="textcolor887"><span>=</span></span><span id="textcolor888"><span>0</span></span><span>),</span> <span id="x1-70135r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â axis</span><span id="textcolor889"><span>=</span></span><span id="textcolor890"><span>1</span></span><span>)</span></code></pre>
<p>We can then visualize and compare the uncertainty values between the first 50 images in the fashion MNIST dataset and the regular MNIST dataset.</p>
<p>In <em>Figure</em> <a href="#x1-70136r7"><em>5.7</em></a>, we see that uncertainty values are a lot greater for images from the regular MNIST dataset than for the fashion MNIST dataset. This is expected, given that our model has only seen fashion MNIST images during training and the handwritten digits from the regular MNIST dataset are out-of-distribution for the model we trained.</p>
<div class="IMG---Figure">
<img src="../media/file130.png" alt="PIC"/> <span id="x1-70136r7"></span> <span id="x1-70137"></span></div>
<p class="IMG---Caption">FigureÂ 5.7: Uncertainty values for images in the fashion MNIST dataset (left) versus the regular MNIST dataset (right) 
</p>
<p>BBB is perhaps the most commonly encountered highly principled Bayesian deep learning method<span id="dx1-70138"></span>, but it isnâ€™t the only option for those concerned with better principled methods. In the next section, weâ€™ll introduce another highly principled method and learn about the properties that differentiate it from BBB. <span id="x1-70139r124"></span></p>
</section>
</section>
<section id="scalable-bayesian-deep-learning-with-probabilistic-backpropagation" class="level2 sectionHead" data-number="10.6">
<h2 class="sectionHead" data-number="10.6" id="sigil_toc_id_59"><span class="titlemark">5.6 </span> <span id="x1-710006"></span>Scalable Bayesian Deep Learning with Probabilistic Backpropagation</h2>
<p>BBB provided a great introduction to Bayesian inference with neural networks, but variational methods have one key drawback: their reliance on sampling at training and inference time<span id="dx1-71001"></span><span id="dx1-71002"></span>. Unlike a standard neural network, we need to sample from the weight parameters using a range of <em>ğœ–</em> values in order to produce the distributions necessary for probabilistic training and inference.</p>
<p>At around the same time that BBB was introduced, researchers at Harvard University were working on their own brand of Bayesian inference with neural networks: <strong>Probabilistic Backpropagation</strong>, or <strong>PBP</strong>. Like BBB, PBPâ€™s weights form the parameters of a distribution, in this case mean and variance weights (using variance, <em>Ïƒ</em><sup><span class="cmr-8">2</span></sup>, rather than <em>Ïƒ</em>). In fact, the similarities donâ€™t end here â€“ weâ€™re going to see quite a few similarities to BBB but, crucially, weâ€™re going to end up with a different approach to BNN approximation with its own advantages and disadvantages. So, letâ€™s get started.</p>
<p>To make things simpler, and for parity with the various PBP papers, weâ€™ll stick with individual weights while we work through the core ideas of PBP. Hereâ€™s a visualization of how these weights are related in a small neural network:</p>
<div class="IMG---Figure">
<img src="../media/file131.png" alt="PIC"/> <span id="x1-71003r8"></span> <span id="x1-71004"></span></div>
<p class="IMG---Caption">FigureÂ 5.8: Illustration of neural network weights in PBP 
</p>
<p>Just as before, we see that our network is essentially built from two sub-networks: one for the mean weights, or <em>m</em>, and one for the variance weights, or <em>v</em>. The core idea behind PBP is that, for each weight, we have some distribution <em>P</em>(<em>w</em><span class="cmsy-10x-x-109">|</span><em>D</em>) that weâ€™re trying to approximate:</p>
<div class="math-display">
<img src="../media/file132.jpg" class="math-display" alt="q(w) = ğ’© (w|m, v) "/>
</div>
<p>This notation should be very familiar now, with <em>P</em>() being the true (intractable) distribution, and <em>q</em>() being the approximate distribution. In PBPâ€™s<span id="dx1-71005"></span> case, as demonstrated in equation 5.18, this is a Gaussian distribution parameterized by mean <em>m</em> and variance <em>v</em>.</p>
<p>In BBB, we saw how variational learning via the ELBO used the KL divergence<span id="dx1-71006"></span> to ensure our weight distribution converged towards our prior <em>P</em>(<em>w</em>). In PBP, we will again make use of the KL divergence, although this time weâ€™ll do it indirectly. The way that we achieve this is through the use of a process called <strong>Assumed Density Filtering</strong> (<strong>ADF</strong>).<span id="dx1-71007"></span></p>
<p>ADF was developed as a fast sequential method of minimizing the KL divergence between the true posterior <em>P</em>(<em>w</em><span class="cmsy-10x-x-109">|</span><em>D</em>) and some approximation <em>q</em>(<em>w</em><span class="cmsy-10x-x-109">|</span><em>D</em>). A key point here is the fact that it is a <em>sequential</em> algorithm: just like gradient descent, which we use with standard neural networks, ADF updates its parameters sequentially. This makes it particularly well suited for adapting to a neural network. The ADF algorithm can be described in two key steps:</p>
<ol>
<li><div id="x1-71009x1">
<p>Initialize our parameters, with <em>m</em> = 0 and <em>v</em> = 1; that is, we start with a unit Gaussian <span class="cmsy-10x-x-109">ğ’©</span>(0<em>,</em>1).</p>
</div></li>
<li><div id="x1-71011x2">
<p>Next, we go through each data point <em>x</em><sub><em>i</em></sub> <span class="cmsy-10x-x-109">âˆˆ</span> <strong>x</strong> and update the parameters of our model using a set of specific update equations that update our model parameters <em>m</em> and <em>v</em> separately.</p>
</div></li>
</ol>
<p>While itâ€™s beyond the scope of this book to provide a full derivation of ADF, you should know that as we update our parameters through ADF, we also minimize the KL divergence.</p>
<p>Thus, for PBP, we need to adapt typical neural network update rules so that the weights are updated along the lines of ADF instead. We do this using the following update rules, which are derived from the original ADF equations:</p>
<div class="math-display">
<img src="../media/file133.jpg" class="math-display" alt="mnew = m + v âˆ‚logZ-- âˆ‚m "/>
</div>
<div class="math-display">
<img src="../media/file134.jpg" class="math-display" alt=" [ ( ) ] new 2 âˆ‚-log-Z- âˆ‚-log-Z- v = v âˆ’ v âˆ‚m âˆ’ 2 âˆ‚v "/>
</div>
<p>Here, log <em>Z</em> denotes the Gaussian marginal likelihood, which is defined as follows:</p>
<div class="math-display">
<img src="../media/file135.jpg" class="math-display" alt=" 2 logZ = âˆ’ log p(y|m, v) = âˆ’ 0.5 Ã— log-v +-(y-âˆ’-m-) v "/>
</div>
<p>This is the <strong>negative log-likelihood</strong> (<strong>NLL</strong>).<span id="dx1-71012"></span> Equation 5.21 is crucial to how we learn the parameters of PBP<span id="dx1-71013"></span>, as this is the loss function that weâ€™re trying to optimise - so letâ€™s take some time to understand whatâ€™s going on. Just as with our loss for BBB (equation 5.9), we can see that our log <em>Z</em> loss incorporates a few important pieces of information:</p>
<ol>
<li><div id="x1-71015x1">
<p>In the numerator, we see (<em>y</em><span class="cmsy-10x-x-109">âˆ’</span><em>m</em>)<sup><span class="cmr-8">2</span></sup>. This is similar to a typical loss weâ€™re used to seeing in standard neural network training (the L2 loss). This incorporates the penalty between our target <em>y</em> and our mean estimate for the value, <em>m</em>. <span id="dx1-71016"></span></p>
</div></li>
<li><div id="x1-71018x2">
<p>The whole equation gives us the NLL function, which describes the joint probability of our target <em>y</em> as a function of our distribution parameterised by <em>m</em> and <em>v</em>.</p>
</div></li>
</ol>
<p>This has some important properties, which we can explore through a few simple examples. Letâ€™s look at the loss for some arbitrary parameters <em>m</em> = 0<em>.</em>8 and <em>v</em> = 0<em>.</em>4 for a given target <em>y</em> = 0<em>.</em>6:</p>
<div class="math-display">
<img src="../media/file136.jpg" class="math-display" alt=" 2 2 âˆ’ 0.5Ã— logv-+-(y âˆ’-m)- = âˆ’ 0.5 Ã— log(0.4)-+-(0.6-âˆ’-0.8)- = 1.095 v 0.4 "/>
</div>
<p>Here, we can see that our typical error, in this case the squared error, is (0<em>.</em>6 <span class="cmsy-10x-x-109">âˆ’ </span>0<em>.</em>8)<sup><span class="cmr-8">2</span></sup> = 0<em>.</em>04, and we know that as <em>m</em> converges towards <em>y</em>, this will shrink. In addition to that, the log-likelihood scales our error. This is important, because a well-conditioned model for uncertainty quantification will be <em>more uncertain</em> when itâ€™s wrong, and <em>more confident</em> when itâ€™s right. The likelihood function gives us a way of achieving this, ensuring that our likelihood is greater if weâ€™re uncertain about incorrect predictions and certain about correct predictions.</p>
<p>We can see this in action by substituting another value of <em>v</em> and seeing how this changes the NLL. For example, letâ€™s increase our variance to <em>v</em> = 0<em>.</em>9:</p>
<div class="math-display">
<img src="../media/file137.jpg" class="math-display" alt=" 2 âˆ’ 0.5Ã— log(0.9)+-(0.6âˆ’-0.8)- = 0.036 0.9 "/>
</div>
<p>This significant increase in variance produces a similarly significant reduction in NLL. Similarly, weâ€™ll see our NLL increase again if we have high variance for a correct prediction <em>m</em> = <em>y</em>:</p>
<div class="math-display">
<img src="../media/file138.jpg" class="math-display" alt=" log(0.9)+-(0.8âˆ’-0.8)2 âˆ’ 0.5Ã— 0.9 = 0.059 "/>
</div>
<p>Hopefully, with this example you can see how using the NLL loss translates to well calibrated uncertainty estimates over our outputs.<span id="dx1-71019"></span> In fact, this property â€“ using the variance to scale to our objective function â€“ is a fundamental component of all principled BNN methods: BBB also does this, although itâ€™s a little more difficult to demonstrate on paper as it requires sampling.</p>
<p>There are a few low-level details of PBP that weâ€™ll encounter in the implementation. These relate to the ADF process, and we encourage you to take a look at the articles in the <em>Further reading</em> section for comprehensive derivations of PBP and ADF.<span id="dx1-71020"></span></p>
<p>Now that weâ€™ve covered PBPâ€™s core concepts, letâ€™s take a look at how we implement it with TensorFlow. <span id="x1-71021r132"></span></p>
</section>
<section id="implementing-pbp" class="level2 sectionHead" data-number="10.7">
<h2 class="sectionHead" data-number="10.7" id="sigil_toc_id_60"><span class="titlemark">5.7 </span> <span id="x1-720007"></span>Implementing PBP</h2>
<p>Because PBP is quite complex, weâ€™ll implement it as a class. Doing so will keep our example code tidy and allow us to easily compartmentalize our various blocks of code. It will also make it easier to experiment with, for example, if you want to explore changing the number of units or layers in your network.</p>
<section id="step-1-importing-libraries" class="level4 likesubsubsectionHead" data-number="10.7.0.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.1"><span id="x1-730007"></span>Step 1: Importing libraries</h4>
<p>We begin by importing various libraries. In this example, we will use scikit-learnâ€™s California Housing dataset to predict house prices:<span id="dx1-73001"></span></p>
<pre id="fancyvrb51" class="fancyvrb"><span id="x1-73011r1"></span> 
<code><span id="textcolor891"><span>from</span></span><span>Â </span><span id="textcolor892"><span>typing</span></span><span>Â </span><span id="textcolor893"><span>import</span></span><span>Â List,</span><span>Â Union,</span><span>Â Iterable</span> <span id="x1-73013r2"></span> </code>
<code><span id="textcolor894"><span>import</span></span><span>Â </span><span id="textcolor895"><span>math</span></span> <span id="x1-73015r3"></span> </code>
<code><span id="textcolor896"><span>from</span></span><span>Â </span><span id="textcolor897"><span>sklearn</span></span><span>Â </span><span id="textcolor898"><span>import</span></span><span>Â datasets</span> <span id="x1-73017r4"></span> </code>
<code><span id="textcolor899"><span>from</span></span><span>Â </span><span id="textcolor900"><span>sklearn.model_selection</span></span><span>Â </span><span id="textcolor901"><span>import</span></span><span>Â train_test_split</span> <span id="x1-73019r5"></span> </code>
<code><span id="textcolor902"><span>import</span></span><span>Â </span><span id="textcolor903"><span>tensorflow</span></span><span>Â </span><span id="textcolor904"><span>as</span></span><span>Â </span><span id="textcolor905"><span>tf</span></span> <span id="x1-73021r6"></span> </code>
<code><span id="textcolor906"><span>import</span></span><span>Â </span><span id="textcolor907"><span>numpy</span></span><span>Â </span><span id="textcolor908"><span>as</span></span><span>Â </span><span id="textcolor909"><span>np</span></span> <span id="x1-73023r7"></span> </code>
<code><span id="textcolor910"><span>from</span></span><span>Â </span><span id="textcolor911"><span>tensorflow.python.framework</span></span><span>Â </span><span id="textcolor912"><span>import</span></span><span>Â tensor_shape</span> <span id="x1-73025r8"></span> </code>
<code><span id="textcolor913"><span>import</span></span><span>Â </span><span id="textcolor914"><span>tensorflow_probability</span></span><span>Â </span><span id="textcolor915"><span>as</span></span><span>Â </span><span id="textcolor916"><span>tfp</span></span></code></pre>
<p>To make sure we produce the same output every time, we initialize our seeds:</p>
<pre id="fancyvrb52" class="fancyvrb"><span id="x1-73030r1"></span> 
<code><span>RANDOM_SEED</span><span>Â </span><span id="textcolor917"><span>=</span></span><span>Â </span><span id="textcolor918"><span>0</span></span> <span id="x1-73032r2"></span> </code>
<code><span>np</span><span id="textcolor919"><span>.</span></span><span>random</span><span id="textcolor920"><span>.</span></span><span>seed(RANDOM_SEED)</span> <span id="x1-73034r3"></span> </code>
<code><span>tf</span><span id="textcolor921"><span>.</span></span><span>random</span><span id="textcolor922"><span>.</span></span><span>set_seed(RANDOM_SEED)</span></code></pre>
<p>We can then load our dataset and create train and test splits:</p>
<pre id="fancyvrb53" class="fancyvrb"><span id="x1-73042r1"></span> 
<code><span id="textcolor923"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â load</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â California</span><span class="cmitt-10x-x-109">Â Housing</span><span class="cmitt-10x-x-109">Â dataset</span></span> <span id="x1-73044r2"></span> </code>
<code><span>X,</span><span>Â y</span><span>Â </span><span id="textcolor924"><span>=</span></span><span>Â datasets</span><span id="textcolor925"><span>.</span></span><span>fetch_california_housing(return_X_y</span><span id="textcolor926"><span>=</span></span><span id="textcolor927"><span>True</span></span><span>)</span> <span id="x1-73046r3"></span> </code>
<code><span id="textcolor928"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â split</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â data</span><span class="cmitt-10x-x-109">Â (X)</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â targets</span><span class="cmitt-10x-x-109">Â (y)</span><span class="cmitt-10x-x-109">Â into</span><span class="cmitt-10x-x-109">Â train</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â test</span><span class="cmitt-10x-x-109">Â sets</span></span> <span id="x1-73048r4"></span> </code>
<code><span>X_train,</span><span>Â X_test,</span><span>Â y_train,</span><span>Â y_test</span><span>Â </span><span id="textcolor929"><span>=</span></span><span>Â train_test_split(</span> <span id="x1-73050r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â X,</span><span>Â y,</span><span>Â test_size</span><span id="textcolor930"><span>=</span></span><span id="textcolor931"><span>0.1</span></span><span>,</span><span>Â random_state</span><span id="textcolor932"><span>=</span></span><span id="textcolor933"><span>0</span></span> <span id="x1-73052r6"></span> </code>
<code><span>)</span></code></pre>
</section>
<section id="step-2-helper-functions" class="level4 likesubsubsectionHead" data-number="10.7.0.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.2"><span id="x1-740007"></span>Step 2: Helper functions</h4>
<p>Next, we define two helper functions that ensure that our data is in the correct format, one for the input and another one for the output data:<span id="dx1-74001"></span></p>
<pre id="fancyvrb54" class="fancyvrb"><span id="x1-74019r1"></span> 
<code><span id="textcolor934"><span>def</span></span><span>Â </span><span id="textcolor935"><span>ensure_input</span></span><span>(x,</span><span>Â dtype,</span><span>Â input_shape):</span> <span id="x1-74021r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor936"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â ensure</span><span class="cmitt-10x-x-109">Â that</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â input</span><span class="cmitt-10x-x-109">Â is</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â correct</span><span class="cmitt-10x-x-109">Â shape</span></span> <span id="x1-74023r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â x</span><span>Â </span><span id="textcolor937"><span>=</span></span><span>Â tf</span><span id="textcolor938"><span>.</span></span><span>constant(x,</span><span>Â dtype</span><span id="textcolor939"><span>=</span></span><span>dtype)</span> <span id="x1-74025r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â call_rank</span><span>Â </span><span id="textcolor940"><span>=</span></span><span>Â tf</span><span id="textcolor941"><span>.</span></span><span>rank(tf</span><span id="textcolor942"><span>.</span></span><span>constant(</span><span id="textcolor943"><span>0</span></span><span>,</span><span>Â shape</span><span id="textcolor944"><span>=</span></span><span>input_shape,</span><span>Â dtype</span><span id="textcolor945"><span>=</span></span><span>dtype))</span><span>Â </span><span id="textcolor946"><span>+</span></span><span>Â </span><span id="textcolor947"><span>1</span></span> <span id="x1-74027r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor948"><span>if</span></span><span>Â tf</span><span id="textcolor949"><span>.</span></span><span>rank(x)</span><span>Â </span><span id="textcolor950"><em>&lt;</em></span><span>Â call_rank:</span> <span id="x1-74029r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â x</span><span>Â </span><span id="textcolor951"><span>=</span></span><span>Â tf</span><span id="textcolor952"><span>.</span></span><span>reshape(x,</span><span>Â [</span><span id="textcolor953"><span>-</span></span><span id="textcolor954"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor955"><span>*</span></span><span>Â input_shape</span><span id="textcolor956"><span>.</span></span><span>as_list()])</span> <span id="x1-74031r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor957"><span>return</span></span><span>Â x</span> <span id="x1-74033r8"></span> </code>
<code><span id="x1-74035r9"></span></code>
<code><span id="x1-74037r10"></span></code>
<code><span id="textcolor958"><span>def</span></span><span>Â </span><span id="textcolor959"><span>ensure_output</span></span><span>(y,</span><span>Â dtype,</span><span>Â output_dim):</span> <span id="x1-74039r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor960"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â ensure</span><span class="cmitt-10x-x-109">Â that</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â output</span><span class="cmitt-10x-x-109">Â is</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â correct</span><span class="cmitt-10x-x-109">Â shape</span></span> <span id="x1-74041r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â output_rank</span><span>Â </span><span id="textcolor961"><span>=</span></span><span>Â </span><span id="textcolor962"><span>2</span></span> <span id="x1-74043r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor963"><span>=</span></span><span>Â tf</span><span id="textcolor964"><span>.</span></span><span>constant(y,</span><span>Â dtype</span><span id="textcolor965"><span>=</span></span><span>dtype)</span> <span id="x1-74045r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor966"><span>if</span></span><span>Â tf</span><span id="textcolor967"><span>.</span></span><span>rank(y)</span><span>Â </span><span id="textcolor968"><em>&lt;</em></span><span>Â output_rank:</span> <span id="x1-74047r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor969"><span>=</span></span><span>Â tf</span><span id="textcolor970"><span>.</span></span><span>reshape(y,</span><span>Â [</span><span id="textcolor971"><span>-</span></span><span id="textcolor972"><span>1</span></span><span>,</span><span>Â output_dim])</span> <span id="x1-74049r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor973"><span>return</span></span><span>Â y</span></code></pre>
<p>We will also create a short class to initialize a gamma distribution: <code>ReciprocalGammaInitializer</code>. This distribution is used as the prior for PBPâ€™s precision parameter <em>Î»</em> and the noise parameter <em>Î³</em>.</p>
<pre id="fancyvrb55" class="fancyvrb"><span id="x1-74062r1"></span> 
<code><span id="textcolor974"><span>class</span></span><span>Â </span><span id="textcolor975"><span>ReciprocalGammaInitializer</span></span><span>:</span> <span id="x1-74064r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor976"><span>def</span></span><span>Â </span><span id="textcolor977"><span>__init__</span></span><span>(</span><span id="textcolor978"><span>self</span></span><span>,</span><span>Â alpha,</span><span>Â beta):</span> <span id="x1-74066r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor979"><span>self</span></span><span id="textcolor980"><span>.</span></span><span>Gamma</span><span>Â </span><span id="textcolor981"><span>=</span></span><span>Â tfp</span><span id="textcolor982"><span>.</span></span><span>distributions</span><span id="textcolor983"><span>.</span></span><span>Gamma(concentration</span><span id="textcolor984"><span>=</span></span><span>alpha,</span><span>Â rate</span><span id="textcolor985"><span>=</span></span><span>beta)</span> <span id="x1-74068r4"></span> </code>
<code><span id="x1-74070r5"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor986"><span>def</span></span><span>Â </span><span id="textcolor987"><span>__call__</span></span><span>(</span><span id="textcolor988"><span>self</span></span><span>,</span><span>Â shape:</span><span>Â Iterable,</span><span>Â dtype</span><span id="textcolor989"><span>=</span></span><span id="textcolor990"><span>None</span></span><span>):</span> <span id="x1-74072r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â g</span><span>Â </span><span id="textcolor991"><span>=</span></span><span>Â </span><span id="textcolor992"><span>1.0</span></span><span>Â </span><span id="textcolor993"><span>/</span></span><span>Â </span><span id="textcolor994"><span>self</span></span><span id="textcolor995"><span>.</span></span><span>Gamma</span><span id="textcolor996"><span>.</span></span><span>sample(shape)</span> <span id="x1-74074r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor997"><span>if</span></span><span>Â dtype:</span> <span id="x1-74076r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â g</span><span>Â </span><span id="textcolor998"><span>=</span></span><span>Â tf</span><span id="textcolor999"><span>.</span></span><span>cast(g,</span><span>Â dtype</span><span id="textcolor1000"><span>=</span></span><span>dtype)</span> <span id="x1-74078r9"></span> </code>
<code><span id="x1-74080r10"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1001"><span>return</span></span><span>Â g</span></code></pre>
<p>A thorough treatment of these variables is not required for a general understanding of PBP. For further details on this, please see the PBP paper listed in the <em>Further reading</em> section.</p>
</section>
<section id="step-3-data-preparation" class="level4 likesubsubsectionHead" data-number="10.7.0.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.3"><span id="x1-750007"></span>Step 3: Data preparation</h4>
<p>With these prerequisites implemented, we can normalize our data.<span id="dx1-75001"></span> Here, we normalize to mean zero and unit standard deviation. This is a common pre-processing step that will make it easier for our model to find the right set of weights:</p>
<pre id="fancyvrb56" class="fancyvrb"><span id="x1-75025r1"></span> 
<code><span id="textcolor1002"><span>def</span></span><span>Â </span><span id="textcolor1003"><span>get_mean_std_x_y</span></span><span>(x,</span><span>Â y):</span> <span id="x1-75027r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1004"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â compute</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â means</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â standard</span><span class="cmitt-10x-x-109">Â deviations</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â inputs</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â targets</span></span> <span id="x1-75029r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â std_X_train</span><span>Â </span><span id="textcolor1005"><span>=</span></span><span>Â np</span><span id="textcolor1006"><span>.</span></span><span>std(x,</span><span>Â </span><span id="textcolor1007"><span>0</span></span><span>)</span> <span id="x1-75031r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â std_X_train[std_X_train</span><span>Â </span><span id="textcolor1008"><span>==</span></span><span>Â </span><span id="textcolor1009"><span>0</span></span><span>]</span><span>Â </span><span id="textcolor1010"><span>=</span></span><span>Â </span><span id="textcolor1011"><span>1</span></span> <span id="x1-75033r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â mean_X_train</span><span>Â </span><span id="textcolor1012"><span>=</span></span><span>Â np</span><span id="textcolor1013"><span>.</span></span><span>mean(x,</span><span>Â </span><span id="textcolor1014"><span>0</span></span><span>)</span> <span id="x1-75035r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â std_y_train</span><span>Â </span><span id="textcolor1015"><span>=</span></span><span>Â np</span><span id="textcolor1016"><span>.</span></span><span>std(y)</span> <span id="x1-75037r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1017"><span>if</span></span><span>Â std_y_train</span><span>Â </span><span id="textcolor1018"><span>==</span></span><span>Â </span><span id="textcolor1019"><span>0.0</span></span><span>:</span> <span id="x1-75039r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â std_y_train</span><span>Â </span><span id="textcolor1020"><span>=</span></span><span>Â </span><span id="textcolor1021"><span>1.0</span></span> <span id="x1-75041r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â mean_y_train</span><span>Â </span><span id="textcolor1022"><span>=</span></span><span>Â np</span><span id="textcolor1023"><span>.</span></span><span>mean(y)</span> <span id="x1-75043r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1024"><span>return</span></span><span>Â mean_X_train,</span><span>Â mean_y_train,</span><span>Â std_X_train,</span><span>Â std_y_train</span> <span id="x1-75045r11"></span> </code>
<code><span id="x1-75047r12"></span></code>
<code><span id="textcolor1025"><span>def</span></span><span>Â </span><span id="textcolor1026"><span>normalize</span></span><span>(x,</span><span>Â y,</span><span>Â output_shape):</span> <span id="x1-75049r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1027"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â use</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â means</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â standard</span><span class="cmitt-10x-x-109">Â deviations</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â normalize</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â inputs</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â targets</span></span> <span id="x1-75051r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â x</span><span>Â </span><span id="textcolor1028"><span>=</span></span><span>Â ensure_input(x,</span><span>Â tf</span><span id="textcolor1029"><span>.</span></span><span>float32,</span><span>Â x</span><span id="textcolor1030"><span>.</span></span><span>shape[</span><span id="textcolor1031"><span>1</span></span><span>])</span> <span id="x1-75053r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor1032"><span>=</span></span><span>Â ensure_output(y,</span><span>Â tf</span><span id="textcolor1033"><span>.</span></span><span>float32,</span><span>Â output_shape)</span> <span id="x1-75055r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â mean_X_train,</span><span>Â mean_y_train,</span><span>Â std_X_train,</span><span>Â std_y_train</span><span>Â </span><span id="textcolor1034"><span>=</span></span><span>Â get_mean_std_x_y(x,</span><span>Â y)</span> <span id="x1-75057r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â x</span><span>Â </span><span id="textcolor1035"><span>=</span></span><span>Â (x</span><span>Â </span><span id="textcolor1036"><span>-</span></span><span>Â np</span><span id="textcolor1037"><span>.</span></span><span>full(x</span><span id="textcolor1038"><span>.</span></span><span>shape,</span><span>Â mean_X_train))</span><span>Â </span><span id="textcolor1039"><span>/</span></span><span>Â np</span><span id="textcolor1040"><span>.</span></span><span>full(x</span><span id="textcolor1041"><span>.</span></span><span>shape,</span><span>Â std_X_train)</span> <span id="x1-75059r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor1042"><span>=</span></span><span>Â (y</span><span>Â </span><span id="textcolor1043"><span>-</span></span><span>Â mean_y_train)</span><span>Â </span><span id="textcolor1044"><span>/</span></span><span>Â std_y_train</span> <span id="x1-75061r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1045"><span>return</span></span><span>Â x,</span><span>Â y</span> <span id="x1-75063r20"></span> </code>
<code><span id="x1-75065r21"></span></code>
<code><span id="textcolor1046"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â run</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â normalize()</span><span class="cmitt-10x-x-109">Â function</span><span class="cmitt-10x-x-109">Â on</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â data</span></span> <span id="x1-75067r22"></span> </code>
<code><span>x,</span><span>Â y</span><span>Â </span><span id="textcolor1047"><span>=</span></span><span>Â normalize(X_train,</span><span>Â y_train,</span><span>Â </span><span id="textcolor1048"><span>1</span></span><span>)</span></code></pre>
</section>
<section id="step-4-defining-our-model-class" class="level4 likesubsubsectionHead" data-number="10.7.0.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.4"><span id="x1-760007"></span>Step 4: Defining our model class</h4>
<p>We can now start to define our model. Our model will consist of three layers: two ReLU layers and one linear layer.<span id="dx1-76001"></span> We use Kerasâ€™ <code>Layer</code> to define our layers. The code for this layer is quite long, so we will break it into several subsections.</p>
<p>First, we subclass the <code>Layer</code> to create our own <code>PBPLayer</code> and define our <code>init</code> method. Our initialization method sets the number of units in our layer:</p>
<pre id="fancyvrb57" class="fancyvrb"><span id="x1-76015r1"></span> 
<code><span id="textcolor1049"><span>from</span></span><span>Â </span><span id="textcolor1050"><span>tensorflow.keras.initializers</span></span><span>Â </span><span id="textcolor1051"><span>import</span></span><span>Â HeNormal</span> <span id="x1-76017r2"></span> </code>
<code><span id="x1-76019r3"></span></code>
<code><span id="textcolor1052"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â handle</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â PBP</span><span class="cmitt-10x-x-109">Â layers</span></span> <span id="x1-76021r4"></span> </code>
<code><span id="textcolor1053"><span>class</span></span><span>Â </span><span id="textcolor1054"><span>PBPLayer</span></span><span>(tf</span><span id="textcolor1055"><span>.</span></span><span>keras</span><span id="textcolor1056"><span>.</span></span><span>layers</span><span id="textcolor1057"><span>.</span></span><span>Layer):</span> <span id="x1-76023r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1058"><span>def</span></span><span>Â </span><span id="textcolor1059"><span>__init__</span></span><span>(</span><span id="textcolor1060"><span>self</span></span><span>,</span><span>Â units:</span><span>Â </span><span id="textcolor1061"><span>int</span></span><span>,</span><span>Â dtype</span><span id="textcolor1062"><span>=</span></span><span>tf</span><span id="textcolor1063"><span>.</span></span><span>float32,</span><span>Â </span><span id="textcolor1064"><span>*</span></span><span>args,</span><span>Â </span><span id="textcolor1065"><span>**</span></span><span>kwargs):</span> <span id="x1-76025r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1066"><span>super</span></span><span>()</span><span id="textcolor1067"><span>.</span></span><span id="textcolor1068"><span>__init__</span></span><span>(dtype</span><span id="textcolor1069"><span>=</span></span><span>tf</span><span id="textcolor1070"><span>.</span></span><span>as_dtype(dtype),</span><span>Â </span><span id="textcolor1071"><span>*</span></span><span>args,</span><span>Â </span><span id="textcolor1072"><span>**</span></span><span>kwargs)</span> <span id="x1-76027r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1073"><span>self</span></span><span id="textcolor1074"><span>.</span></span><span>units</span><span>Â </span><span id="textcolor1075"><span>=</span></span><span>Â units</span> <span id="x1-76029r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1076"><span>...</span></span></code></pre>
<p>We then create a <code>build()</code> method that defines the weights of our layer. As we discussed in the previous section, PBP comprises both <em>mean</em> weights and <em>variance</em> weights.<span id="dx1-76031"></span> As a simple MLP is composed of a multiplicative component, or weight, and a bias, weâ€™ll split both our weights and biases into mean and variance variables:</p>
<pre id="fancyvrb58" class="fancyvrb"><span id="x1-76068r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1077"><span>...</span></span> <span id="x1-76070r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1078"><span>def</span></span><span>Â </span><span id="textcolor1079"><span>build</span></span><span>(</span><span id="textcolor1080"><span>self</span></span><span>,</span><span>Â input_shape):</span> <span id="x1-76072r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â input_shape</span><span>Â </span><span id="textcolor1081"><span>=</span></span><span>Â tensor_shape</span><span id="textcolor1082"><span>.</span></span><span>TensorShape(input_shape)</span> <span id="x1-76074r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â last_dim</span><span>Â </span><span id="textcolor1083"><span>=</span></span><span>Â tensor_shape</span><span id="textcolor1084"><span>.</span></span><span>dimension_value(input_shape[</span><span id="textcolor1085"><span>-</span></span><span id="textcolor1086"><span>1</span></span><span>])</span> <span id="x1-76076r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1087"><span>self</span></span><span id="textcolor1088"><span>.</span></span><span>input_spec</span><span>Â </span><span id="textcolor1089"><span>=</span></span><span>Â tf</span><span id="textcolor1090"><span>.</span></span><span>keras</span><span id="textcolor1091"><span>.</span></span><span>layers</span><span id="textcolor1092"><span>.</span></span><span>InputSpec(</span> <span id="x1-76078r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â min_ndim</span><span id="textcolor1093"><span>=</span></span><span id="textcolor1094"><span>2</span></span><span>,</span><span>Â axes</span><span id="textcolor1095"><span>=</span></span><span>{</span><span id="textcolor1096"><span>-</span></span><span id="textcolor1097"><span>1</span></span><span>:</span><span>Â last_dim}</span> <span id="x1-76080r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76082r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1098"><span>self</span></span><span id="textcolor1099"><span>.</span></span><span>inv_sqrtV1</span><span>Â </span><span id="textcolor1100"><span>=</span></span><span>Â tf</span><span id="textcolor1101"><span>.</span></span><span>cast(</span> <span id="x1-76084r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1102"><span>1.0</span></span><span>Â </span><span id="textcolor1103"><span>/</span></span><span>Â tf</span><span id="textcolor1104"><span>.</span></span><span>math</span><span id="textcolor1105"><span>.</span></span><span>sqrt(</span><span id="textcolor1106"><span>1.0</span></span><span>Â </span><span id="textcolor1107"><span>*</span></span><span>Â last_dim</span><span>Â </span><span id="textcolor1108"><span>+</span></span><span>Â </span><span id="textcolor1109"><span>1</span></span><span>),</span><span>Â dtype</span><span id="textcolor1110"><span>=</span></span><span id="textcolor1111"><span>self</span></span><span id="textcolor1112"><span>.</span></span><span>dtype</span> <span id="x1-76086r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76088r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1113"><span>self</span></span><span id="textcolor1114"><span>.</span></span><span>inv_V1</span><span>Â </span><span id="textcolor1115"><span>=</span></span><span>Â tf</span><span id="textcolor1116"><span>.</span></span><span>math</span><span id="textcolor1117"><span>.</span></span><span>square(</span><span id="textcolor1118"><span>self</span></span><span id="textcolor1119"><span>.</span></span><span>inv_sqrtV1)</span> <span id="x1-76090r12"></span> </code>
<code><span id="x1-76092r13"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â over_gamma</span><span>Â </span><span id="textcolor1120"><span>=</span></span><span>Â ReciprocalGammaInitializer(</span><span id="textcolor1121"><span>6.0</span></span><span>,</span><span>Â </span><span id="textcolor1122"><span>6.0</span></span><span>)</span> <span id="x1-76094r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1123"><span>self</span></span><span id="textcolor1124"><span>.</span></span><span>weights_m</span><span>Â </span><span id="textcolor1125"><span>=</span></span><span>Â </span><span id="textcolor1126"><span>self</span></span><span id="textcolor1127"><span>.</span></span><span>add_weight(</span> <span id="x1-76096r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1128"><span>"weights_mean"</span></span><span>,</span><span>Â shape</span><span id="textcolor1129"><span>=</span></span><span>[last_dim,</span><span>Â </span><span id="textcolor1130"><span>self</span></span><span id="textcolor1131"><span>.</span></span><span>units],</span> <span id="x1-76098r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â initializer</span><span id="textcolor1132"><span>=</span></span><span>HeNormal(),</span><span>Â dtype</span><span id="textcolor1133"><span>=</span></span><span id="textcolor1134"><span>self</span></span><span id="textcolor1135"><span>.</span></span><span>dtype,</span><span>Â trainable</span><span id="textcolor1136"><span>=</span></span><span id="textcolor1137"><span>True</span></span><span>,</span> <span id="x1-76100r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76102r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1138"><span>self</span></span><span id="textcolor1139"><span>.</span></span><span>weights_v</span><span>Â </span><span id="textcolor1140"><span>=</span></span><span>Â </span><span id="textcolor1141"><span>self</span></span><span id="textcolor1142"><span>.</span></span><span>add_weight(</span> <span id="x1-76104r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1143"><span>"weights_variance"</span></span><span>,</span><span>Â shape</span><span id="textcolor1144"><span>=</span></span><span>[last_dim,</span><span>Â </span><span id="textcolor1145"><span>self</span></span><span id="textcolor1146"><span>.</span></span><span>units],</span> <span id="x1-76106r20"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â initializer</span><span id="textcolor1147"><span>=</span></span><span>over_gamma,</span><span>Â dtype</span><span id="textcolor1148"><span>=</span></span><span id="textcolor1149"><span>self</span></span><span id="textcolor1150"><span>.</span></span><span>dtype,</span><span>Â trainable</span><span id="textcolor1151"><span>=</span></span><span id="textcolor1152"><span>True</span></span><span>,</span> <span id="x1-76108r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76110r22"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1153"><span>self</span></span><span id="textcolor1154"><span>.</span></span><span>bias_m</span><span>Â </span><span id="textcolor1155"><span>=</span></span><span>Â </span><span id="textcolor1156"><span>self</span></span><span id="textcolor1157"><span>.</span></span><span>add_weight(</span> <span id="x1-76112r23"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1158"><span>"bias_mean"</span></span><span>,</span><span>Â shape</span><span id="textcolor1159"><span>=</span></span><span>[</span><span id="textcolor1160"><span>self</span></span><span id="textcolor1161"><span>.</span></span><span>units],</span> <span id="x1-76114r24"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â initializer</span><span id="textcolor1162"><span>=</span></span><span>HeNormal(),</span><span>Â dtype</span><span id="textcolor1163"><span>=</span></span><span id="textcolor1164"><span>self</span></span><span id="textcolor1165"><span>.</span></span><span>dtype,</span><span>Â trainable</span><span id="textcolor1166"><span>=</span></span><span id="textcolor1167"><span>True</span></span><span>,</span> <span id="x1-76116r25"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76118r26"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1168"><span>self</span></span><span id="textcolor1169"><span>.</span></span><span>bias_v</span><span>Â </span><span id="textcolor1170"><span>=</span></span><span>Â </span><span id="textcolor1171"><span>self</span></span><span id="textcolor1172"><span>.</span></span><span>add_weight(</span> <span id="x1-76120r27"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1173"><span>"bias_variance"</span></span><span>,</span><span>Â shape</span><span id="textcolor1174"><span>=</span></span><span>[</span><span id="textcolor1175"><span>self</span></span><span id="textcolor1176"><span>.</span></span><span>units],</span> <span id="x1-76122r28"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â initializer</span><span id="textcolor1177"><span>=</span></span><span>over_gamma,</span><span>Â dtype</span><span id="textcolor1178"><span>=</span></span><span id="textcolor1179"><span>self</span></span><span id="textcolor1180"><span>.</span></span><span>dtype,</span><span>Â trainable</span><span id="textcolor1181"><span>=</span></span><span id="textcolor1182"><span>True</span></span><span>,</span> <span id="x1-76124r29"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76126r30"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1183"><span>self</span></span><span id="textcolor1184"><span>.</span></span><span>Normal</span><span>Â </span><span id="textcolor1185"><span>=</span></span><span>Â tfp</span><span id="textcolor1186"><span>.</span></span><span>distributions</span><span id="textcolor1187"><span>.</span></span><span>Normal(</span> <span id="x1-76128r31"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loc</span><span id="textcolor1188"><span>=</span></span><span>tf</span><span id="textcolor1189"><span>.</span></span><span>constant(</span><span id="textcolor1190"><span>0.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1191"><span>=</span></span><span id="textcolor1192"><span>self</span></span><span id="textcolor1193"><span>.</span></span><span>dtype),</span> <span id="x1-76130r32"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â scale</span><span id="textcolor1194"><span>=</span></span><span>tf</span><span id="textcolor1195"><span>.</span></span><span>constant(</span><span id="textcolor1196"><span>1.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1197"><span>=</span></span><span id="textcolor1198"><span>self</span></span><span id="textcolor1199"><span>.</span></span><span>dtype),</span> <span id="x1-76132r33"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76134r34"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1200"><span>self</span></span><span id="textcolor1201"><span>.</span></span><span>built</span><span>Â </span><span id="textcolor1202"><span>=</span></span><span>Â </span><span id="textcolor1203"><span>True</span></span> <span id="x1-76136r35"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1204"><span>...</span></span></code></pre>
<p>The <code>weights_m</code> and <code>weights_v</code> variables are our mean and variance weights, forming the very core of our PBP model. We will continue our definition of <code>PBPLayer</code> when we work through our model fitting function. For now, we can subclass this class to create our ReLU layer:</p>
<pre id="fancyvrb59" class="fancyvrb"><span id="x1-76156r1"></span> 
<code><span id="textcolor1205"><span>class</span></span><span>Â </span><span id="textcolor1206"><span>PBdivLULayer</span></span><span>(PBPLayer):</span> <span id="x1-76158r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1207"><span>@tf</span></span><span id="textcolor1208"><span>.</span></span><span>function</span> <span id="x1-76160r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1209"><span>def</span></span><span>Â </span><span id="textcolor1210"><span>call</span></span><span>(</span><span id="textcolor1211"><span>self</span></span><span>,</span><span>Â x:</span><span>Â tf</span><span id="textcolor1212"><span>.</span></span><span>Tensor):</span> <span id="x1-76162r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1213"><span class="cmitt-10x-x-109">"""Calculate</span><span class="cmitt-10x-x-109">Â deterministic</span><span class="cmitt-10x-x-109">Â output"""</span></span> <span id="x1-76164r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1214"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â x</span><span class="cmitt-10x-x-109">Â is</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â shape</span><span class="cmitt-10x-x-109">Â [batch,</span><span class="cmitt-10x-x-109">Â divv_units]</span></span> <span id="x1-76166r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â x</span><span>Â </span><span id="textcolor1215"><span>=</span></span><span>Â </span><span id="textcolor1216"><span>super</span></span><span>()</span><span id="textcolor1217"><span>.</span></span><span>call(x)</span> <span id="x1-76168r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â z</span><span>Â </span><span id="textcolor1218"><span>=</span></span><span>Â tf</span><span id="textcolor1219"><span>.</span></span><span>maximum(x,</span><span>Â tf</span><span id="textcolor1220"><span>.</span></span><span>zeros_like(x))</span><span>Â </span><span>Â </span><span id="textcolor1221"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â [batch,</span><span class="cmitt-10x-x-109">Â units]</span></span> <span id="x1-76170r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1222"><span>return</span></span><span>Â z</span> <span id="x1-76172r9"></span> </code>
<code><span id="x1-76174r10"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1223"><span>@tf</span></span><span id="textcolor1224"><span>.</span></span><span>function</span> <span id="x1-76176r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1225"><span>def</span></span><span>Â </span><span id="textcolor1226"><span>predict</span></span><span>(</span><span id="textcolor1227"><span>self</span></span><span>,</span><span>Â previous_mean:</span><span>Â tf</span><span id="textcolor1228"><span>.</span></span><span>Tensor,</span><span>Â previous_variance:</span><span>Â tf</span><span id="textcolor1229"><span>.</span></span><span>Tensor):</span> <span id="x1-76178r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â ma,</span><span>Â va</span><span>Â </span><span id="textcolor1230"><span>=</span></span><span>Â </span><span id="textcolor1231"><span>super</span></span><span>()</span><span id="textcolor1232"><span>.</span></span><span>predict(previous_mean,</span><span>Â previous_variance)</span> <span id="x1-76180r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â mb,</span><span>Â vb</span><span>Â </span><span id="textcolor1233"><span>=</span></span><span>Â get_bias_mean_variance(ma,</span><span>Â va,</span><span>Â </span><span id="textcolor1234"><span>self</span></span><span id="textcolor1235"><span>.</span></span><span>Normal)</span> <span id="x1-76182r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1236"><span>return</span></span><span>Â mb,</span><span>Â vb</span></code></pre>
<p>You can see that we overwrite two functions: our <code>call()</code> and <code>predict()</code> functions. The <code>call()</code> function calls our regular linear <code>call()</code> function and then applies the ReLU max operation we saw in <a href="CH3.xhtml#x1-350003"><em>ChapterÂ 3</em></a>, <a href="CH3.xhtml#x1-350003"><em>Fundamentals of Deep</em> <em>Learning</em></a>. The <code>predict()</code> function calls our regular <code>predict()</code> function, but then also calls a new function, <code>get_bias_mean_variance()</code>. This function computes the mean and variance of our bias in a numerically stable way, as shown here:</p>
<pre id="fancyvrb60" class="fancyvrb"><span id="x1-76207r1"></span> 
<code><span id="textcolor1237"><span>def</span></span><span>Â </span><span id="textcolor1238"><span>get_bias_mean_variance</span></span><span>(ma,</span><span>Â va,</span><span>Â normal):</span> <span id="x1-76209r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â variance_sqrt</span><span>Â </span><span id="textcolor1239"><span>=</span></span><span>Â tf</span><span id="textcolor1240"><span>.</span></span><span>math</span><span id="textcolor1241"><span>.</span></span><span>sqrt(tf</span><span id="textcolor1242"><span>.</span></span><span>maximum(va,</span><span>Â tf</span><span id="textcolor1243"><span>.</span></span><span>zeros_like(va)))</span> <span id="x1-76211r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha</span><span>Â </span><span id="textcolor1244"><span>=</span></span><span>Â safe_div(ma,</span><span>Â variance_sqrt)</span> <span id="x1-76213r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha_inv</span><span>Â </span><span id="textcolor1245"><span>=</span></span><span>Â safe_div(tf</span><span id="textcolor1246"><span>.</span></span><span>constant(</span><span id="textcolor1247"><span>1.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1248"><span>=</span></span><span>alpha</span><span id="textcolor1249"><span>.</span></span><span>dtype),</span><span>Â alpha)</span> <span id="x1-76215r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha_cdf</span><span>Â </span><span id="textcolor1250"><span>=</span></span><span>Â normal</span><span id="textcolor1251"><span>.</span></span><span>cdf(alpha)</span> <span id="x1-76217r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â gamma</span><span>Â </span><span id="textcolor1252"><span>=</span></span><span>Â tf</span><span id="textcolor1253"><span>.</span></span><span>where(</span> <span id="x1-76219r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â alpha</span><span>Â </span><span id="textcolor1254"><em>&lt;</em></span><span>Â </span><span id="textcolor1255"><span>-</span></span><span id="textcolor1256"><span>30</span></span><span>,</span> <span id="x1-76221r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1257"><span>-</span></span><span>alpha</span><span>Â </span><span id="textcolor1258"><span>+</span></span><span>Â alpha_inv</span><span>Â </span><span id="textcolor1259"><span>*</span></span><span>Â (</span><span id="textcolor1260"><span>-</span></span><span id="textcolor1261"><span>1</span></span><span>Â </span><span id="textcolor1262"><span>+</span></span><span>Â </span><span id="textcolor1263"><span>2</span></span><span>Â </span><span id="textcolor1264"><span>*</span></span><span>Â tf</span><span id="textcolor1265"><span>.</span></span><span>math</span><span id="textcolor1266"><span>.</span></span><span>square(alpha_inv)),</span> <span id="x1-76223r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â safe_div(normal</span><span id="textcolor1267"><span>.</span></span><span>prob(</span><span id="textcolor1268"><span>-</span></span><span>alpha),</span><span>Â alpha_cdf),</span> <span id="x1-76225r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76227r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â vp</span><span>Â </span><span id="textcolor1269"><span>=</span></span><span>Â ma</span><span>Â </span><span id="textcolor1270"><span>+</span></span><span>Â variance_sqrt</span><span>Â </span><span id="textcolor1271"><span>*</span></span><span>Â gamma</span> <span id="x1-76229r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â bias_mean</span><span>Â </span><span id="textcolor1272"><span>=</span></span><span>Â alpha_cdf</span><span>Â </span><span id="textcolor1273"><span>*</span></span><span>Â vp</span> <span id="x1-76231r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â bias_variance</span><span>Â </span><span id="textcolor1274"><span>=</span></span><span>Â bias_mean</span><span>Â </span><span id="textcolor1275"><span>*</span></span><span>Â vp</span><span>Â </span><span id="textcolor1276"><span>*</span></span><span>Â normal</span><span id="textcolor1277"><span>.</span></span><span>cdf(</span><span id="textcolor1278"><span>-</span></span><span>alpha)</span><span>Â </span><span id="textcolor1279"><span>+</span></span><span>Â alpha_cdf</span><span>Â </span><span id="textcolor1280"><span>*</span></span><span>Â va</span><span>Â </span><span id="textcolor1281"><span>*</span></span><span>Â (</span> <span id="x1-76233r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1282"><span>1</span></span><span>Â </span><span id="textcolor1283"><span>-</span></span><span>Â gamma</span><span>Â </span><span id="textcolor1284"><span>*</span></span><span>Â (gamma</span><span>Â </span><span id="textcolor1285"><span>+</span></span><span>Â alpha)</span> <span id="x1-76235r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76237r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1286"><span>return</span></span><span>Â bias_mean,</span><span>Â bias_variance</span></code></pre>
<p>With our layer definitions in place, we can build our network.<span id="dx1-76238"></span> We first create a list of all layers in our network:</p>
<pre id="fancyvrb61" class="fancyvrb"><span id="x1-76252r1"></span> 
<code><span>units</span><span>Â </span><span id="textcolor1287"><span>=</span></span><span>Â [</span><span id="textcolor1288"><span>50</span></span><span>,</span><span>Â </span><span id="textcolor1289"><span>50</span></span><span>,</span><span>Â </span><span id="textcolor1290"><span>1</span></span><span>]</span> <span id="x1-76254r2"></span> </code>
<code><span>layers</span><span>Â </span><span id="textcolor1291"><span>=</span></span><span>Â []</span> <span id="x1-76256r3"></span> </code>
<code><span>last_shape</span><span>Â </span><span id="textcolor1292"><span>=</span></span><span>Â X_train</span><span id="textcolor1293"><span>.</span></span><span>shape[</span><span id="textcolor1294"><span>1</span></span><span>]</span> <span id="x1-76258r4"></span> </code>
<code><span id="x1-76260r5"></span></code>
<code><span id="textcolor1295"><span>for</span></span><span>Â unit</span><span>Â </span><span id="textcolor1296"><span>in</span></span><span>Â units[:</span><span id="textcolor1297"><span>-</span></span><span id="textcolor1298"><span>1</span></span><span>]:</span> <span id="x1-76262r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â layer</span><span>Â </span><span id="textcolor1299"><span>=</span></span><span>Â PBdivLULayer(unit)</span> <span id="x1-76264r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â layer</span><span id="textcolor1300"><span>.</span></span><span>build(last_shape)</span> <span id="x1-76266r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â layers</span><span id="textcolor1301"><span>.</span></span><span>append(layer)</span> <span id="x1-76268r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â last_shape</span><span>Â </span><span id="textcolor1302"><span>=</span></span><span>Â unit</span> <span id="x1-76270r10"></span> </code>
<code><span>layer</span><span>Â </span><span id="textcolor1303"><span>=</span></span><span>Â PBPLayer(units[</span><span id="textcolor1304"><span>-</span></span><span id="textcolor1305"><span>1</span></span><span>])</span> <span id="x1-76272r11"></span> </code>
<code><span>layer</span><span id="textcolor1306"><span>.</span></span><span>build(last_shape)</span> <span id="x1-76274r12"></span> </code>
<code><span>layers</span><span id="textcolor1307"><span>.</span></span><span>append(layer)</span></code></pre>
<p>We then create a <code>PBP</code> class that contains the modelâ€™s <code>fit()</code> and <code>predict()</code> functions, similar to what you see in a model defined with Kerasâ€™s <code>tf.keras.Model</code> class. Next, weâ€™ll see a number of important variables; letâ€™s go through them here:</p>
<ul>
<li><p><code>alpha    </code> and <code>beta    </code>: These are parameters of our gamma distribution</p></li>
<li><p><code>Gamma    </code>: An instance of the <code>tfp.distributions.Gamma()    </code> class for our gamma distributions, which is a hyper-prior on PBPâ€™s precision parameter <em>Î»</em></p></li>
<li><p><code>layers    </code>: This variable specifies the number of layers in the model</p></li>
<li><p><code>Normal    </code>: Here, we instantiate an instance of the <code>tfp.distributions.Normal()    </code> class, which implements a Gaussian probability distribution (in this case, with a mean of 0 and a standard deviation of 1):<span id="dx1-76286"></span></p></li>
</ul>
<pre id="fancyvrb62" class="fancyvrb"><span id="x1-76324r1"></span> 
<code><span id="textcolor1314"><span>class</span></span><span>Â </span><span id="textcolor1315"><span>PBP</span></span><span>:</span> <span id="x1-76326r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1316"><span>def</span></span><span>Â </span><span id="textcolor1317"><span>__init__</span></span><span>(</span> <span id="x1-76328r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1318"><span>self</span></span><span>,</span> <span id="x1-76330r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â layers:</span><span>Â List[tf</span><span id="textcolor1319"><span>.</span></span><span>keras</span><span id="textcolor1320"><span>.</span></span><span>layers</span><span id="textcolor1321"><span>.</span></span><span>Layer],</span> <span id="x1-76332r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â dtype:</span><span>Â Union[tf</span><span id="textcolor1322"><span>.</span></span><span>dtypes</span><span id="textcolor1323"><span>.</span></span><span>DType,</span><span>Â np</span><span id="textcolor1324"><span>.</span></span><span>dtype,</span><span>Â </span><span id="textcolor1325"><span>str</span></span><span>]</span><span>Â </span><span id="textcolor1326"><span>=</span></span><span>Â tf</span><span id="textcolor1327"><span>.</span></span><span>float32</span> <span id="x1-76334r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â ):</span> <span id="x1-76336r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1328"><span>self</span></span><span id="textcolor1329"><span>.</span></span><span>alpha</span><span>Â </span><span id="textcolor1330"><span>=</span></span><span>Â tf</span><span id="textcolor1331"><span>.</span></span><span>Variable(</span><span id="textcolor1332"><span>6.0</span></span><span>,</span><span>Â trainable</span><span id="textcolor1333"><span>=</span></span><span id="textcolor1334"><span>True</span></span><span>,</span><span>Â dtype</span><span id="textcolor1335"><span>=</span></span><span>dtype)</span> <span id="x1-76338r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1336"><span>self</span></span><span id="textcolor1337"><span>.</span></span><span>beta</span><span>Â </span><span id="textcolor1338"><span>=</span></span><span>Â tf</span><span id="textcolor1339"><span>.</span></span><span>Variable(</span><span id="textcolor1340"><span>6.0</span></span><span>,</span><span>Â trainable</span><span id="textcolor1341"><span>=</span></span><span id="textcolor1342"><span>True</span></span><span>,</span><span>Â dtype</span><span id="textcolor1343"><span>=</span></span><span>dtype)</span> <span id="x1-76340r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1344"><span>self</span></span><span id="textcolor1345"><span>.</span></span><span>layers</span><span>Â </span><span id="textcolor1346"><span>=</span></span><span>Â layers</span> <span id="x1-76342r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1347"><span>self</span></span><span id="textcolor1348"><span>.</span></span><span>Normal</span><span>Â </span><span id="textcolor1349"><span>=</span></span><span>Â tfp</span><span id="textcolor1350"><span>.</span></span><span>distributions</span><span id="textcolor1351"><span>.</span></span><span>Normal(</span> <span id="x1-76344r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loc</span><span id="textcolor1352"><span>=</span></span><span>tf</span><span id="textcolor1353"><span>.</span></span><span>constant(</span><span id="textcolor1354"><span>0.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1355"><span>=</span></span><span>dtype),</span> <span id="x1-76346r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â scale</span><span id="textcolor1356"><span>=</span></span><span>tf</span><span id="textcolor1357"><span>.</span></span><span>constant(</span><span id="textcolor1358"><span>1.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1359"><span>=</span></span><span>dtype),</span> <span id="x1-76348r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76350r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1360"><span>self</span></span><span id="textcolor1361"><span>.</span></span><span>Gamma</span><span>Â </span><span id="textcolor1362"><span>=</span></span><span>Â tfp</span><span id="textcolor1363"><span>.</span></span><span>distributions</span><span id="textcolor1364"><span>.</span></span><span>Gamma(</span> <span id="x1-76352r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â concentration</span><span id="textcolor1365"><span>=</span></span><span id="textcolor1366"><span>self</span></span><span id="textcolor1367"><span>.</span></span><span>alpha,</span><span>Â rate</span><span id="textcolor1368"><span>=</span></span><span id="textcolor1369"><span>self</span></span><span id="textcolor1370"><span>.</span></span><span>beta</span> <span id="x1-76354r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76356r17"></span> </code>
<code><span id="x1-76358r18"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1371"><span>def</span></span><span>Â </span><span id="textcolor1372"><span>fit</span></span><span>(</span><span id="textcolor1373"><span>self</span></span><span>,</span><span>Â x,</span><span>Â y,</span><span>Â batch_size:</span><span>Â </span><span id="textcolor1374"><span>int</span></span><span>Â </span><span id="textcolor1375"><span>=</span></span><span>Â </span><span id="textcolor1376"><span>16</span></span><span>,</span><span>Â n_epochs:</span><span>Â </span><span id="textcolor1377"><span>int</span></span><span>Â </span><span id="textcolor1378"><span>=</span></span><span>Â </span><span id="textcolor1379"><span>1</span></span><span>):</span> <span id="x1-76360r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â data</span><span>Â </span><span id="textcolor1380"><span>=</span></span><span>Â tf</span><span id="textcolor1381"><span>.</span></span><span>data</span><span id="textcolor1382"><span>.</span></span><span>Dataset</span><span id="textcolor1383"><span>.</span></span><span>from_tensor_slices((x,</span><span>Â y))</span><span id="textcolor1384"><span>.</span></span><span>batch(batch_size)</span> <span id="x1-76362r20"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1385"><span>for</span></span><span>Â epoch_index</span><span>Â </span><span id="textcolor1386"><span>in</span></span><span>Â </span><span id="textcolor1387"><span>range</span></span><span>(n_epochs):</span> <span id="x1-76364r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1388"><span>print</span></span><span>(</span><span id="textcolor1389"><span>f</span></span><span id="textcolor1390"><span>"</span></span><span id="textcolor1391"><span>{</span></span><span>epoch_index</span><span id="textcolor1392"><span>=}</span></span><span id="textcolor1393"><span>"</span></span><span>)</span> <span id="x1-76366r22"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1394"><span>for</span></span><span>Â x_batch,</span><span>Â y_batch</span><span>Â </span><span id="textcolor1395"><span>in</span></span><span>Â data:</span> <span id="x1-76368r23"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â diff_square,</span><span>Â v,</span><span>Â v0</span><span>Â </span><span id="textcolor1396"><span>=</span></span><span>Â </span><span id="textcolor1397"><span>self</span></span><span id="textcolor1398"><span>.</span></span><span>update_gradients(x_batch,</span><span>Â y_batch)</span> <span id="x1-76370r24"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â alpha,</span><span>Â beta</span><span>Â </span><span id="textcolor1399"><span>=</span></span><span>Â update_alpha_beta(</span> <span id="x1-76372r25"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1400"><span>self</span></span><span id="textcolor1401"><span>.</span></span><span>alpha,</span><span>Â </span><span id="textcolor1402"><span>self</span></span><span id="textcolor1403"><span>.</span></span><span>beta,</span><span>Â diff_square,</span><span>Â v,</span><span>Â v0</span> <span id="x1-76374r26"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76376r27"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1404"><span>self</span></span><span id="textcolor1405"><span>.</span></span><span>alpha</span><span id="textcolor1406"><span>.</span></span><span>assign(alpha)</span> <span id="x1-76378r28"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1407"><span>self</span></span><span id="textcolor1408"><span>.</span></span><span>beta</span><span id="textcolor1409"><span>.</span></span><span>assign(beta)</span> <span id="x1-76380r29"></span> </code>
<code><span id="x1-76382r30"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1410"><span>@tf</span></span><span id="textcolor1411"><span>.</span></span><span>function</span> <span id="x1-76384r31"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1412"><span>def</span></span><span>Â </span><span id="textcolor1413"><span>predict</span></span><span>(</span><span id="textcolor1414"><span>self</span></span><span>,</span><span>Â x:</span><span>Â tf</span><span id="textcolor1415"><span>.</span></span><span>Tensor):</span> <span id="x1-76386r32"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â m,</span><span>Â v</span><span>Â </span><span id="textcolor1416"><span>=</span></span><span>Â x,</span><span>Â tf</span><span id="textcolor1417"><span>.</span></span><span>zeros_like(x)</span> <span id="x1-76388r33"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1418"><span>for</span></span><span>Â layer</span><span>Â </span><span id="textcolor1419"><span>in</span></span><span>Â </span><span id="textcolor1420"><span>self</span></span><span id="textcolor1421"><span>.</span></span><span>layers:</span> <span id="x1-76390r34"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â m,</span><span>Â v</span><span>Â </span><span id="textcolor1422"><span>=</span></span><span>Â layer</span><span id="textcolor1423"><span>.</span></span><span>predict(m,</span><span>Â v)</span> <span id="x1-76392r35"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1424"><span>return</span></span><span>Â m,</span><span>Â v</span> <span id="x1-76394r36"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1425"><span>...</span></span></code></pre>
<p>The <code>PBP</code> class <code>__init__</code> function creates a number of parameters but essentially initializes our <em>Î±</em> and <em>Î²</em> hyper-priors with a normal and a gamma distribution. Furthermore, we save the layers that we created in the previous step.<span id="dx1-76397"></span></p>
<p>The <code>fit()</code> function updates the gradients of our layers and then updates our <em>Î±</em> and <em>Î²</em> parameters. The function for updating gradients is defined as follows:</p>
<pre id="fancyvrb63" class="fancyvrb"><span id="x1-76414r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1427"><span>...</span></span> <span id="x1-76416r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1428"><span>@tf</span></span><span id="textcolor1429"><span>.</span></span><span>function</span> <span id="x1-76418r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1430"><span>def</span></span><span>Â </span><span id="textcolor1431"><span>update_gradients</span></span><span>(</span><span id="textcolor1432"><span>self</span></span><span>,</span><span>Â x,</span><span>Â y):</span> <span id="x1-76420r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â trainables</span><span>Â </span><span id="textcolor1433"><span>=</span></span><span>Â [layer</span><span id="textcolor1434"><span>.</span></span><span>trainable_weights</span><span>Â </span><span id="textcolor1435"><span>for</span></span><span>Â layer</span><span>Â </span><span id="textcolor1436"><span>in</span></span><span>Â </span><span id="textcolor1437"><span>self</span></span><span id="textcolor1438"><span>.</span></span><span>layers]</span> <span id="x1-76422r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1439"><span>with</span></span><span>Â tf</span><span id="textcolor1440"><span>.</span></span><span>GradientTape()</span><span>Â </span><span id="textcolor1441"><span>as</span></span><span>Â tape:</span> <span id="x1-76424r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tape</span><span id="textcolor1442"><span>.</span></span><span>watch(trainables)</span> <span id="x1-76426r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â m,</span><span>Â v</span><span>Â </span><span id="textcolor1443"><span>=</span></span><span>Â </span><span id="textcolor1444"><span>self</span></span><span id="textcolor1445"><span>.</span></span><span>predict(x)</span> <span id="x1-76428r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â v0</span><span>Â </span><span id="textcolor1446"><span>=</span></span><span>Â v</span><span>Â </span><span id="textcolor1447"><span>+</span></span><span>Â safe_div(</span><span id="textcolor1448"><span>self</span></span><span id="textcolor1449"><span>.</span></span><span>beta,</span><span>Â </span><span id="textcolor1450"><span>self</span></span><span id="textcolor1451"><span>.</span></span><span>alpha</span><span>Â </span><span id="textcolor1452"><span>-</span></span><span>Â </span><span id="textcolor1453"><span>1</span></span><span>)</span> <span id="x1-76430r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â diff_square</span><span>Â </span><span id="textcolor1454"><span>=</span></span><span>Â tf</span><span id="textcolor1455"><span>.</span></span><span>math</span><span id="textcolor1456"><span>.</span></span><span>square(y</span><span>Â </span><span id="textcolor1457"><span>-</span></span><span>Â m)</span> <span id="x1-76432r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â logZ0</span><span>Â </span><span id="textcolor1458"><span>=</span></span><span>Â logZ(diff_square,</span><span>Â v0)</span> <span id="x1-76434r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â grad</span><span>Â </span><span id="textcolor1459"><span>=</span></span><span>Â tape</span><span id="textcolor1460"><span>.</span></span><span>gradient(logZ0,</span><span>Â trainables)</span> <span id="x1-76436r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1461"><span>for</span></span><span>Â l,</span><span>Â g</span><span>Â </span><span id="textcolor1462"><span>in</span></span><span>Â </span><span id="textcolor1463"><span>zip</span></span><span>(</span><span id="textcolor1464"><span>self</span></span><span id="textcolor1465"><span>.</span></span><span>layers,</span><span>Â grad):</span> <span id="x1-76438r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â l</span><span id="textcolor1466"><span>.</span></span><span>apply_gradient(g)</span> <span id="x1-76440r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1467"><span>return</span></span><span>Â diff_square,</span><span>Â v,</span><span>Â v0</span></code></pre>
<p>Before we can update our gradients, we need to propagate them forward through the network. To do so, weâ€™ll implement our <code>predict()</code> method:</p>
<pre id="fancyvrb64" class="fancyvrb"><span id="x1-76464r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1468"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â ...</span><span class="cmitt-10x-x-109">Â PBPLayer</span><span class="cmitt-10x-x-109">Â continued</span></span> <span id="x1-76466r2"></span> </code>
<code><span id="x1-76468r3"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1469"><span>@tf</span></span><span id="textcolor1470"><span>.</span></span><span>function</span> <span id="x1-76470r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1471"><span>def</span></span><span>Â </span><span id="textcolor1472"><span>predict</span></span><span>(</span><span id="textcolor1473"><span>self</span></span><span>,</span><span>Â previous_mean:</span><span>Â tf</span><span id="textcolor1474"><span>.</span></span><span>Tensor,</span><span>Â previous_variance:</span><span>Â tf</span><span id="textcolor1475"><span>.</span></span><span>Tensor):</span> <span id="x1-76472r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â mean</span><span>Â </span><span id="textcolor1476"><span>=</span></span><span>Â (</span> <span id="x1-76474r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1477"><span>.</span></span><span>tensordot(previous_mean,</span><span>Â </span><span id="textcolor1478"><span>self</span></span><span id="textcolor1479"><span>.</span></span><span>weights_m,</span><span>Â axes</span><span id="textcolor1480"><span>=</span></span><span>[</span><span id="textcolor1481"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor1482"><span>0</span></span><span>])</span> <span id="x1-76476r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1483"><span>+</span></span><span>Â tf</span><span id="textcolor1484"><span>.</span></span><span>expand_dims(</span><span id="textcolor1485"><span>self</span></span><span id="textcolor1486"><span>.</span></span><span>bias_m,</span><span>Â axis</span><span id="textcolor1487"><span>=</span></span><span id="textcolor1488"><span>0</span></span><span>)</span> <span id="x1-76478r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span><span>Â </span><span id="textcolor1489"><span>*</span></span><span>Â </span><span id="textcolor1490"><span>self</span></span><span id="textcolor1491"><span>.</span></span><span>inv_sqrtV1</span> <span id="x1-76480r9"></span> </code>
<code><span id="x1-76482r10"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â variance</span><span>Â </span><span id="textcolor1492"><span>=</span></span><span>Â (</span> <span id="x1-76484r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1493"><span>.</span></span><span>tensordot(</span> <span id="x1-76486r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â previous_variance,</span><span>Â tf</span><span id="textcolor1494"><span>.</span></span><span>math</span><span id="textcolor1495"><span>.</span></span><span>square(</span><span id="textcolor1496"><span>self</span></span><span id="textcolor1497"><span>.</span></span><span>weights_m),</span><span>Â axes</span><span id="textcolor1498"><span>=</span></span><span>[</span><span id="textcolor1499"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor1500"><span>0</span></span><span>]</span> <span id="x1-76488r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76490r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1501"><span>+</span></span><span>Â tf</span><span id="textcolor1502"><span>.</span></span><span>tensordot(</span> <span id="x1-76492r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1503"><span>.</span></span><span>math</span><span id="textcolor1504"><span>.</span></span><span>square(previous_mean),</span><span>Â </span><span id="textcolor1505"><span>self</span></span><span id="textcolor1506"><span>.</span></span><span>weights_v,</span><span>Â axes</span><span id="textcolor1507"><span>=</span></span><span>[</span><span id="textcolor1508"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor1509"><span>0</span></span><span>]</span> <span id="x1-76494r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76496r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1510"><span>+</span></span><span>Â tf</span><span id="textcolor1511"><span>.</span></span><span>expand_dims(</span><span id="textcolor1512"><span>self</span></span><span id="textcolor1513"><span>.</span></span><span>bias_v,</span><span>Â axis</span><span id="textcolor1514"><span>=</span></span><span id="textcolor1515"><span>0</span></span><span>)</span> <span id="x1-76498r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1516"><span>+</span></span><span>Â tf</span><span id="textcolor1517"><span>.</span></span><span>tensordot(previous_variance,</span><span>Â </span><span id="textcolor1518"><span>self</span></span><span id="textcolor1519"><span>.</span></span><span>weights_v,</span><span>Â axes</span><span id="textcolor1520"><span>=</span></span><span>[</span><span id="textcolor1521"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor1522"><span>0</span></span><span>])</span> <span id="x1-76500r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span><span>Â </span><span id="textcolor1523"><span>*</span></span><span>Â </span><span id="textcolor1524"><span>self</span></span><span id="textcolor1525"><span>.</span></span><span>inv_V1</span> <span id="x1-76502r20"></span> </code>
<code><span id="x1-76504r21"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1526"><span>return</span></span><span>Â mean,</span><span>Â variance</span></code></pre>
<p>Now that we can propagate values through our network, we are ready to implement our loss function. As we saw in the previous section, we use the NLL, which weâ€™ll define here:<span id="dx1-76505"></span></p>
<pre id="fancyvrb65" class="fancyvrb"><span id="x1-76525r1"></span> 
<code><span>pi</span><span>Â </span><span id="textcolor1527"><span>=</span></span><span>Â tf</span><span id="textcolor1528"><span>.</span></span><span>math</span><span id="textcolor1529"><span>.</span></span><span>atan(tf</span><span id="textcolor1530"><span>.</span></span><span>constant(</span><span id="textcolor1531"><span>1.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1532"><span>=</span></span><span>tf</span><span id="textcolor1533"><span>.</span></span><span>float32))</span><span>Â </span><span id="textcolor1534"><span>*</span></span><span>Â </span><span id="textcolor1535"><span>4</span></span> <span id="x1-76527r2"></span> </code>
<code><span>LOG_INV_SQRT2PI</span><span>Â </span><span id="textcolor1536"><span>=</span></span><span>Â </span><span id="textcolor1537"><span>-</span></span><span id="textcolor1538"><span>0.5</span></span><span>Â </span><span id="textcolor1539"><span>*</span></span><span>Â tf</span><span id="textcolor1540"><span>.</span></span><span>math</span><span id="textcolor1541"><span>.</span></span><span>log(</span><span id="textcolor1542"><span>2.0</span></span><span>Â </span><span id="textcolor1543"><span>*</span></span><span>Â pi)</span> <span id="x1-76529r3"></span> </code>
<code><span id="x1-76531r4"></span></code>
<code><span id="x1-76533r5"></span></code>
<code><span id="textcolor1544"><span>@tf</span></span><span id="textcolor1545"><span>.</span></span><span>function</span> <span id="x1-76535r6"></span> </code>
<code><span id="textcolor1546"><span>def</span></span><span>Â </span><span id="textcolor1547"><span>logZ</span></span><span>(diff_square:</span><span>Â tf</span><span id="textcolor1548"><span>.</span></span><span>Tensor,</span><span>Â v:</span><span>Â tf</span><span id="textcolor1549"><span>.</span></span><span>Tensor):</span> <span id="x1-76537r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â v0</span><span>Â </span><span id="textcolor1550"><span>=</span></span><span>Â v</span><span>Â </span><span id="textcolor1551"><span>+</span></span><span>Â </span><span id="textcolor1552"><span>1e-6</span></span> <span id="x1-76539r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1553"><span>return</span></span><span>Â tf</span><span id="textcolor1554"><span>.</span></span><span>reduce_sum(</span> <span id="x1-76541r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1555"><span>-</span></span><span id="textcolor1556"><span>0.5</span></span><span>Â </span><span id="textcolor1557"><span>*</span></span><span>Â (diff_square</span><span>Â </span><span id="textcolor1558"><span>/</span></span><span>Â v0)</span><span>Â </span><span id="textcolor1559"><span>+</span></span><span>Â LOG_INV_SQRT2PI</span><span>Â </span><span id="textcolor1560"><span>-</span></span><span>Â </span><span id="textcolor1561"><span>0.5</span></span><span>Â </span><span id="textcolor1562"><span>*</span></span><span>Â tf</span><span id="textcolor1563"><span>.</span></span><span>math</span><span id="textcolor1564"><span>.</span></span><span>log(v0)</span> <span id="x1-76543r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76545r11"></span> </code>
<code><span id="x1-76547r12"></span></code>
<code><span id="x1-76549r13"></span></code>
<code><span id="textcolor1565"><span>@tf</span></span><span id="textcolor1566"><span>.</span></span><span>function</span> <span id="x1-76551r14"></span> </code>
<code><span id="textcolor1567"><span>def</span></span><span>Â </span><span id="textcolor1568"><span>logZ1_minus_logZ2</span></span><span>(diff_square:</span><span>Â tf</span><span id="textcolor1569"><span>.</span></span><span>Tensor,</span><span>Â v1:</span><span>Â tf</span><span id="textcolor1570"><span>.</span></span><span>Tensor,</span><span>Â v2:</span><span>Â tf</span><span id="textcolor1571"><span>.</span></span><span>Tensor):</span> <span id="x1-76553r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1572"><span>return</span></span><span>Â tf</span><span id="textcolor1573"><span>.</span></span><span>reduce_sum(</span> <span id="x1-76555r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1574"><span>-</span></span><span id="textcolor1575"><span>0.5</span></span><span>Â </span><span id="textcolor1576"><span>*</span></span><span>Â diff_square</span><span>Â </span><span id="textcolor1577"><span>*</span></span><span>Â safe_div(v2</span><span>Â </span><span id="textcolor1578"><span>-</span></span><span>Â v1,</span><span>Â v1</span><span>Â </span><span id="textcolor1579"><span>*</span></span><span>Â v2)</span> <span id="x1-76557r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1580"><span>-</span></span><span>Â </span><span id="textcolor1581"><span>0.5</span></span><span>Â </span><span id="textcolor1582"><span>*</span></span><span>Â tf</span><span id="textcolor1583"><span>.</span></span><span>math</span><span id="textcolor1584"><span>.</span></span><span>log(safe_div(v1,</span><span>Â v2)</span><span>Â </span><span id="textcolor1585"><span>+</span></span><span>Â </span><span id="textcolor1586"><span>1e-6</span></span><span>)</span> <span id="x1-76559r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â )</span></code></pre>
<p>We can now propagate values through the network and obtain our gradient with respect to our loss (as we would with a standard neural network). This means weâ€™re ready to update our gradients by applying the update rules we saw in equations 5.19 and 5.20 for the mean weights and variance weights, respectively:</p>
<pre id="fancyvrb66" class="fancyvrb"><span id="x1-76582r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1587"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â ...</span><span class="cmitt-10x-x-109">Â PBPLayer</span><span class="cmitt-10x-x-109">Â continued</span></span> <span id="x1-76584r2"></span> </code>
<code><span id="x1-76586r3"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1588"><span>@tf</span></span><span id="textcolor1589"><span>.</span></span><span>function</span> <span id="x1-76588r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1590"><span>def</span></span><span>Â </span><span id="textcolor1591"><span>apply_gradient</span></span><span>(</span><span id="textcolor1592"><span>self</span></span><span>,</span><span>Â gradient):</span> <span id="x1-76590r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â dlogZ_dwm,</span><span>Â dlogZ_dwv,</span><span>Â dlogZ_dbm,</span><span>Â dlogZ_dbv</span><span>Â </span><span id="textcolor1593"><span>=</span></span><span>Â gradient</span> <span id="x1-76592r6"></span> </code>
<code><span id="x1-76594r7"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1594"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Weights</span></span> <span id="x1-76596r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1595"><span>self</span></span><span id="textcolor1596"><span>.</span></span><span>weights_m</span><span id="textcolor1597"><span>.</span></span><span>assign_add(</span><span id="textcolor1598"><span>self</span></span><span id="textcolor1599"><span>.</span></span><span>weights_v</span><span>Â </span><span id="textcolor1600"><span>*</span></span><span>Â dlogZ_dwm)</span> <span id="x1-76598r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â new_mean_variance</span><span>Â </span><span id="textcolor1601"><span>=</span></span><span>Â </span><span id="textcolor1602"><span>self</span></span><span id="textcolor1603"><span>.</span></span><span>weights_v</span><span>Â </span><span id="textcolor1604"><span>-</span></span><span>Â (</span> <span id="x1-76600r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1605"><span>.</span></span><span>math</span><span id="textcolor1606"><span>.</span></span><span>square(</span><span id="textcolor1607"><span>self</span></span><span id="textcolor1608"><span>.</span></span><span>weights_v)</span> <span id="x1-76602r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1609"><span>*</span></span><span>Â (tf</span><span id="textcolor1610"><span>.</span></span><span>math</span><span id="textcolor1611"><span>.</span></span><span>square(dlogZ_dwm)</span><span>Â </span><span id="textcolor1612"><span>-</span></span><span>Â </span><span id="textcolor1613"><span>2</span></span><span>Â </span><span id="textcolor1614"><span>*</span></span><span>Â dlogZ_dwv)</span> <span id="x1-76604r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76606r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1615"><span>self</span></span><span id="textcolor1616"><span>.</span></span><span>weights_v</span><span id="textcolor1617"><span>.</span></span><span>assign(non_negative_constraint(new_mean_variance))</span> <span id="x1-76608r14"></span> </code>
<code><span id="x1-76610r15"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1618"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Bias</span></span> <span id="x1-76612r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1619"><span>self</span></span><span id="textcolor1620"><span>.</span></span><span>bias_m</span><span id="textcolor1621"><span>.</span></span><span>assign_add(</span><span id="textcolor1622"><span>self</span></span><span id="textcolor1623"><span>.</span></span><span>bias_v</span><span>Â </span><span id="textcolor1624"><span>*</span></span><span>Â dlogZ_dbm)</span> <span id="x1-76614r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â new_bias_variance</span><span>Â </span><span id="textcolor1625"><span>=</span></span><span>Â </span><span id="textcolor1626"><span>self</span></span><span id="textcolor1627"><span>.</span></span><span>bias_v</span><span>Â </span><span id="textcolor1628"><span>-</span></span><span>Â (</span> <span id="x1-76616r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1629"><span>.</span></span><span>math</span><span id="textcolor1630"><span>.</span></span><span>square(</span><span id="textcolor1631"><span>self</span></span><span id="textcolor1632"><span>.</span></span><span>bias_v)</span> <span id="x1-76618r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1633"><span>*</span></span><span>Â (tf</span><span id="textcolor1634"><span>.</span></span><span>math</span><span id="textcolor1635"><span>.</span></span><span>square(dlogZ_dbm)</span><span>Â </span><span id="textcolor1636"><span>-</span></span><span>Â </span><span id="textcolor1637"><span>2</span></span><span>Â </span><span id="textcolor1638"><span>*</span></span><span>Â dlogZ_dbv)</span> <span id="x1-76620r20"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76622r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1639"><span>self</span></span><span id="textcolor1640"><span>.</span></span><span>bias_v</span><span id="textcolor1641"><span>.</span></span><span>assign(non_negative_constraint(new_bias_variance))</span></code></pre>
<p>As discussed in the previous section, PBP belongs to the class of <strong>Assumed</strong> <strong>Density Filtering</strong> (<strong>ADF</strong>) methods.<span id="dx1-76623"></span> As such, we update the <em>Î±</em> and <em>Î²</em> parameters according to ADFâ€™s update rules:<span id="dx1-76624"></span></p>
<pre id="fancyvrb67" class="fancyvrb"><span id="x1-76647r1"></span> 
<code><span id="textcolor1642"><span>def</span></span><span>Â </span><span id="textcolor1643"><span>update_alpha_beta</span></span><span>(alpha,</span><span>Â beta,</span><span>Â diff_square,</span><span>Â v,</span><span>Â v0):</span> <span id="x1-76649r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha1</span><span>Â </span><span id="textcolor1644"><span>=</span></span><span>Â alpha</span><span>Â </span><span id="textcolor1645"><span>+</span></span><span>Â </span><span id="textcolor1646"><span>1</span></span> <span id="x1-76651r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â v1</span><span>Â </span><span id="textcolor1647"><span>=</span></span><span>Â v</span><span>Â </span><span id="textcolor1648"><span>+</span></span><span>Â safe_div(beta,</span><span>Â alpha)</span> <span id="x1-76653r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â v2</span><span>Â </span><span id="textcolor1649"><span>=</span></span><span>Â v</span><span>Â </span><span id="textcolor1650"><span>+</span></span><span>Â beta</span><span>Â </span><span id="textcolor1651"><span>/</span></span><span>Â alpha1</span> <span id="x1-76655r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â logZ2_logZ1</span><span>Â </span><span id="textcolor1652"><span>=</span></span><span>Â logZ1_minus_logZ2(diff_square,</span><span>Â v1</span><span id="textcolor1653"><span>=</span></span><span>v2,</span><span>Â v2</span><span id="textcolor1654"><span>=</span></span><span>v1)</span> <span id="x1-76657r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â logZ1_logZ0</span><span>Â </span><span id="textcolor1655"><span>=</span></span><span>Â logZ1_minus_logZ2(diff_square,</span><span>Â v1</span><span id="textcolor1656"><span>=</span></span><span>v1,</span><span>Â v2</span><span id="textcolor1657"><span>=</span></span><span>v0)</span> <span id="x1-76659r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â logZ_diff</span><span>Â </span><span id="textcolor1658"><span>=</span></span><span>Â logZ2_logZ1</span><span>Â </span><span id="textcolor1659"><span>-</span></span><span>Â logZ1_logZ0</span> <span id="x1-76661r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Z0Z2_Z1Z1</span><span>Â </span><span id="textcolor1660"><span>=</span></span><span>Â safe_exp(logZ_diff)</span> <span id="x1-76663r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â pos_where</span><span>Â </span><span id="textcolor1661"><span>=</span></span><span>Â safe_exp(logZ2_logZ1)</span><span>Â </span><span id="textcolor1662"><span>*</span></span><span>Â (alpha1</span><span>Â </span><span id="textcolor1663"><span>-</span></span><span>Â safe_exp(</span><span id="textcolor1664"><span>-</span></span><span>logZ_diff)</span><span>Â </span><span id="textcolor1665"><span>*</span></span><span>Â alpha)</span> <span id="x1-76665r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â neg_where</span><span>Â </span><span id="textcolor1666"><span>=</span></span><span>Â safe_exp(logZ1_logZ0)</span><span>Â </span><span id="textcolor1667"><span>*</span></span><span>Â (Z0Z2_Z1Z1</span><span>Â </span><span id="textcolor1668"><span>*</span></span><span>Â alpha1</span><span>Â </span><span id="textcolor1669"><span>-</span></span><span>Â alpha)</span> <span id="x1-76667r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â beta_denomi</span><span>Â </span><span id="textcolor1670"><span>=</span></span><span>Â tf</span><span id="textcolor1671"><span>.</span></span><span>where(logZ_diff</span><span>Â </span><span id="textcolor1672"><em>&gt;</em><span>=</span></span><span>Â </span><span id="textcolor1673"><span>0</span></span><span>,</span><span>Â pos_where,</span><span>Â neg_where)</span> <span id="x1-76669r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â beta</span><span>Â </span><span id="textcolor1674"><span>=</span></span><span>Â safe_div(beta,</span><span>Â tf</span><span id="textcolor1675"><span>.</span></span><span>maximum(beta_denomi,</span><span>Â tf</span><span id="textcolor1676"><span>.</span></span><span>zeros_like(beta)))</span> <span id="x1-76671r13"></span> </code>
<code><span id="x1-76673r14"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha_denomi</span><span>Â </span><span id="textcolor1677"><span>=</span></span><span>Â Z0Z2_Z1Z1</span><span>Â </span><span id="textcolor1678"><span>*</span></span><span>Â safe_div(alpha1,</span><span>Â alpha)</span><span>Â </span><span id="textcolor1679"><span>-</span></span><span>Â </span><span id="textcolor1680"><span>1.0</span></span> <span id="x1-76675r15"></span> </code>
<code><span id="x1-76677r16"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â alpha</span><span>Â </span><span id="textcolor1681"><span>=</span></span><span>Â safe_div(</span> <span id="x1-76679r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1682"><span>.</span></span><span>constant(</span><span id="textcolor1683"><span>1.0</span></span><span>,</span><span>Â dtype</span><span id="textcolor1684"><span>=</span></span><span>alpha_denomi</span><span id="textcolor1685"><span>.</span></span><span>dtype),</span> <span id="x1-76681r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1686"><span>.</span></span><span>maximum(alpha_denomi,</span><span>Â tf</span><span id="textcolor1687"><span>.</span></span><span>zeros_like(alpha)),</span> <span id="x1-76683r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-76685r20"></span> </code>
<code><span id="x1-76687r21"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1688"><span>return</span></span><span>Â alpha,</span><span>Â beta</span></code></pre>
</section>
<section id="step-5-avoiding-numerical-errors" class="level4 likesubsubsectionHead" data-number="10.7.0.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.5"><span id="x1-770007"></span>Step 5: Avoiding numerical errors</h4>
<p>Finally, letâ€™s define a few helper functions to ensure that we avoid numerical errors during fitting:<span id="dx1-77001"></span></p>
<pre id="fancyvrb68" class="fancyvrb"><span id="x1-77017r1"></span> 
<code><span id="textcolor1689"><span>@tf</span></span><span id="textcolor1690"><span>.</span></span><span>function</span> <span id="x1-77019r2"></span> </code>
<code><span id="textcolor1691"><span>def</span></span><span>Â </span><span id="textcolor1692"><span>safe_div</span></span><span>(x:</span><span>Â tf</span><span id="textcolor1693"><span>.</span></span><span>Tensor,</span><span>Â y:</span><span>Â tf</span><span id="textcolor1694"><span>.</span></span><span>Tensor,</span><span>Â eps:</span><span>Â tf</span><span id="textcolor1695"><span>.</span></span><span>Tensor</span><span>Â </span><span id="textcolor1696"><span>=</span></span><span>Â tf</span><span id="textcolor1697"><span>.</span></span><span>constant(</span><span id="textcolor1698"><span>1e-6</span></span><span>)):</span> <span id="x1-77021r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â _eps</span><span>Â </span><span id="textcolor1699"><span>=</span></span><span>Â tf</span><span id="textcolor1700"><span>.</span></span><span>cast(eps,</span><span>Â dtype</span><span id="textcolor1701"><span>=</span></span><span>y</span><span id="textcolor1702"><span>.</span></span><span>dtype)</span> <span id="x1-77023r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1703"><span>return</span></span><span>Â x</span><span>Â </span><span id="textcolor1704"><span>/</span></span><span>Â (tf</span><span id="textcolor1705"><span>.</span></span><span>where(y</span><span>Â </span><span id="textcolor1706"><em>&gt;</em><span>=</span></span><span>Â </span><span id="textcolor1707"><span>0</span></span><span>,</span><span>Â y</span><span>Â </span><span id="textcolor1708"><span>+</span></span><span>Â _eps,</span><span>Â y</span><span>Â </span><span id="textcolor1709"><span>-</span></span><span>Â _eps))</span> <span id="x1-77025r5"></span> </code>
<code><span id="x1-77027r6"></span></code>
<code><span id="x1-77029r7"></span></code>
<code><span id="textcolor1710"><span>@tf</span></span><span id="textcolor1711"><span>.</span></span><span>function</span> <span id="x1-77031r8"></span> </code>
<code><span id="textcolor1712"><span>def</span></span><span>Â </span><span id="textcolor1713"><span>safe_exp</span></span><span>(x:</span><span>Â tf</span><span id="textcolor1714"><span>.</span></span><span>Tensor,</span><span>Â BIG:</span><span>Â tf</span><span id="textcolor1715"><span>.</span></span><span>Tensor</span><span>Â </span><span id="textcolor1716"><span>=</span></span><span>Â tf</span><span id="textcolor1717"><span>.</span></span><span>constant(</span><span id="textcolor1718"><span>20</span></span><span>)):</span> <span id="x1-77033r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1719"><span>return</span></span><span>Â tf</span><span id="textcolor1720"><span>.</span></span><span>math</span><span id="textcolor1721"><span>.</span></span><span>exp(tf</span><span id="textcolor1722"><span>.</span></span><span>math</span><span id="textcolor1723"><span>.</span></span><span>minimum(x,</span><span>Â tf</span><span id="textcolor1724"><span>.</span></span><span>cast(BIG,</span><span>Â dtype</span><span id="textcolor1725"><span>=</span></span><span>x</span><span id="textcolor1726"><span>.</span></span><span>dtype)))</span> <span id="x1-77035r10"></span> </code>
<code><span id="x1-77037r11"></span></code>
<code><span id="x1-77039r12"></span></code>
<code><span id="textcolor1727"><span>@tf</span></span><span id="textcolor1728"><span>.</span></span><span>function</span> <span id="x1-77041r13"></span> </code>
<code><span id="textcolor1729"><span>def</span></span><span>Â </span><span id="textcolor1730"><span>non_negative_constraint</span></span><span>(x:</span><span>Â tf</span><span id="textcolor1731"><span>.</span></span><span>Tensor):</span> <span id="x1-77043r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1732"><span>return</span></span><span>Â tf</span><span id="textcolor1733"><span>.</span></span><span>maximum(x,</span><span>Â tf</span><span id="textcolor1734"><span>.</span></span><span>zeros_like(x))</span></code></pre>
</section>
<section id="step-6-instantiating-our-model" class="level4 likesubsubsectionHead" data-number="10.7.0.6">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.6"><span id="x1-780007"></span>Step 6: Instantiating our model</h4>
<p>And there we have it: the core code for training PBP. Now weâ€™re ready to instantiate our model and train it on some data. Letâ€™s use a small batch size and a single epoch in this example:</p>
<pre id="fancyvrb69" class="fancyvrb"><span id="x1-78004r1"></span> 
<code><span>model</span><span>Â </span><span id="textcolor1735"><span>=</span></span><span>Â PBP(layers)</span> <span id="x1-78006r2"></span> </code>
<code><span>model</span><span id="textcolor1736"><span>.</span></span><span>fit(x,</span><span>Â y,</span><span>Â batch_size</span><span id="textcolor1737"><span>=</span></span><span id="textcolor1738"><span>1</span></span><span>,</span><span>Â n_epochs</span><span id="textcolor1739"><span>=</span></span><span id="textcolor1740"><span>1</span></span><span>)</span></code></pre>
</section>
<section id="step-7-using-our-model-for-inference" class="level4 likesubsubsectionHead" data-number="10.7.0.7">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="10.7.0.7"><span id="x1-790007"></span>Step 7: Using our model for inference</h4>
<p>Now that we have our fitted model, letâ€™s see how well it works on our test set.<span id="dx1-79001"></span><span id="dx1-79002"></span> We first normalize our test set:</p>
<pre id="fancyvrb70" class="fancyvrb"><span id="x1-79015r1"></span> 
<code><span id="textcolor1741"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Compute</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â means</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â standard</span><span class="cmitt-10x-x-109">Â deviations</span></span> <span id="x1-79017r2"></span> </code>
<code><span>mean_X_train,</span><span>Â mean_y_train,</span><span>Â std_X_train,</span><span>Â std_y_train</span><span>Â </span><span id="textcolor1742"><span>=</span></span><span>Â get_mean_std_x_y(</span> <span id="x1-79019r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â X_train,</span><span>Â y_train</span> <span id="x1-79021r4"></span> </code>
<code><span>)</span> <span id="x1-79023r5"></span> </code>
<code><span id="x1-79025r6"></span></code>
<code><span id="textcolor1743"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Normalize</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â inputs</span></span> <span id="x1-79027r7"></span> </code>
<code><span>X_test</span><span>Â </span><span id="textcolor1744"><span>=</span></span><span>Â (X_test</span><span>Â </span><span id="textcolor1745"><span>-</span></span><span>Â np</span><span id="textcolor1746"><span>.</span></span><span>full(X_test</span><span id="textcolor1747"><span>.</span></span><span>shape,</span><span>Â mean_X_train))</span><span>Â </span><span id="textcolor1748"><span>/</span></span> <span id="x1-79029r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â np</span><span id="textcolor1749"><span>.</span></span><span>full(X_test</span><span id="textcolor1750"><span>.</span></span><span>shape,</span><span>Â std_X_train)</span> <span id="x1-79031r9"></span> </code>
<code><span id="x1-79033r10"></span></code>
<code><span id="textcolor1751"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Ensure</span><span class="cmitt-10x-x-109">Â that</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â inputs</span><span class="cmitt-10x-x-109">Â are</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â correct</span><span class="cmitt-10x-x-109">Â shape</span></span> <span id="x1-79035r11"></span> </code>
<code><span>X_test</span><span>Â </span><span id="textcolor1752"><span>=</span></span><span>Â ensure_input(X_test,</span><span>Â tf</span><span id="textcolor1753"><span>.</span></span><span>float32,</span><span>Â X_test</span><span id="textcolor1754"><span>.</span></span><span>shape[</span><span id="textcolor1755"><span>1</span></span><span>])</span></code></pre>
<p>Then we get our model predictions: the mean and variance:</p>
<pre id="fancyvrb71" class="fancyvrb"><span id="x1-79038r1"></span> 
<code><span>m,</span><span>Â v</span><span>Â </span><span id="textcolor1756"><span>=</span></span><span>Â model</span><span id="textcolor1757"><span>.</span></span><span>predict(X_test)</span></code></pre>
<p>Then we post-process these values to make sure they have the right shape and are in the range of the original input data:</p>
<pre id="fancyvrb72" class="fancyvrb"><span id="x1-79053r1"></span> 
<code><span id="textcolor1758"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Compute</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â noise</span><span class="cmitt-10x-x-109">Â -</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â baseline</span><span class="cmitt-10x-x-109">Â variation</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â observe</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â targets</span></span> <span id="x1-79055r2"></span> </code>
<code><span>v_noise</span><span>Â </span><span id="textcolor1759"><span>=</span></span><span>Â (model</span><span id="textcolor1760"><span>.</span></span><span>beta</span><span>Â </span><span id="textcolor1761"><span>/</span></span><span>Â (model</span><span id="textcolor1762"><span>.</span></span><span>alpha</span><span>Â </span><span id="textcolor1763"><span>-</span></span><span>Â </span><span id="textcolor1764"><span>1</span></span><span>)</span><span>Â </span><span id="textcolor1765"><span>*</span></span><span>Â std_y_train</span><span id="textcolor1766"><span>**</span></span><span id="textcolor1767"><span>2</span></span><span>)</span> <span id="x1-79057r3"></span> </code>
<code><span id="x1-79059r4"></span></code>
<code><span id="textcolor1768"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Rescale</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â mean</span><span class="cmitt-10x-x-109">Â values</span></span> <span id="x1-79061r5"></span> </code>
<code><span>m</span><span>Â </span><span id="textcolor1769"><span>=</span></span><span>Â m</span><span>Â </span><span id="textcolor1770"><span>*</span></span><span>Â std_y_train</span><span>Â </span><span id="textcolor1771"><span>+</span></span><span>Â mean_y_train</span> <span id="x1-79063r6"></span> </code>
<code><span id="x1-79065r7"></span></code>
<code><span id="textcolor1772"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Rescale</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â values</span></span> <span id="x1-79067r8"></span> </code>
<code><span>v</span><span>Â </span><span id="textcolor1773"><span>=</span></span><span>Â v</span><span>Â </span><span id="textcolor1774"><span>*</span></span><span>Â std_y_train</span><span id="textcolor1775"><span>**</span></span><span id="textcolor1776"><span>2</span></span> <span id="x1-79069r9"></span> </code>
<code><span id="x1-79071r10"></span></code>
<code><span id="textcolor1777"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Reshape</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â variables</span></span> <span id="x1-79073r11"></span> </code>
<code><span>m</span><span>Â </span><span id="textcolor1778"><span>=</span></span><span>Â np</span><span id="textcolor1779"><span>.</span></span><span>squeeze(m</span><span id="textcolor1780"><span>.</span></span><span>numpy())</span> <span id="x1-79075r12"></span> </code>
<code><span>v</span><span>Â </span><span id="textcolor1781"><span>=</span></span><span>Â np</span><span id="textcolor1782"><span>.</span></span><span>squeeze(v</span><span id="textcolor1783"><span>.</span></span><span>numpy())</span> <span id="x1-79077r13"></span> </code>
<code><span>v_noise</span><span>Â </span><span id="textcolor1784"><span>=</span></span><span>Â np</span><span id="textcolor1785"><span>.</span></span><span>squeeze(v_noise</span><span id="textcolor1786"><span>.</span></span><span>numpy()</span><span id="textcolor1787"><span>.</span></span><span>reshape(</span><span id="textcolor1788"><span>-</span></span><span id="textcolor1789"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor1790"><span>1</span></span><span>))</span></code></pre>
<p>Now that weâ€™ve got our predictions, we can compute how well our modelâ€™s done. Weâ€™ll use a standard error metric, RMSE, as well as the metric we used for our loss: the NLL. We can compute them using the following:<span id="dx1-79078"></span></p>
<pre id="fancyvrb73" class="fancyvrb"><span id="x1-79089r1"></span> 
<code><span>rmse</span><span>Â </span><span id="textcolor1791"><span>=</span></span><span>Â np</span><span id="textcolor1792"><span>.</span></span><span>sqrt(np</span><span id="textcolor1793"><span>.</span></span><span>mean((y_test</span><span>Â </span><span id="textcolor1794"><span>-</span></span><span>Â m)</span><span>Â </span><span id="textcolor1795"><span>**</span></span><span>Â </span><span id="textcolor1796"><span>2</span></span><span>))</span> <span id="x1-79091r2"></span> </code>
<code><span>test_log_likelihood</span><span>Â </span><span id="textcolor1797"><span>=</span></span><span>Â np</span><span id="textcolor1798"><span>.</span></span><span>mean(</span> <span id="x1-79093r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1799"><span>-</span></span><span id="textcolor1800"><span>0.5</span></span><span>Â </span><span id="textcolor1801"><span>*</span></span><span>Â np</span><span id="textcolor1802"><span>.</span></span><span>log(</span><span id="textcolor1803"><span>2</span></span><span>Â </span><span id="textcolor1804"><span>*</span></span><span>Â math</span><span id="textcolor1805"><span>.</span></span><span>pi</span><span>Â </span><span id="textcolor1806"><span>*</span></span><span>Â v)</span> <span id="x1-79095r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1807"><span>-</span></span><span>Â </span><span id="textcolor1808"><span>0.5</span></span><span>Â </span><span id="textcolor1809"><span>*</span></span><span>Â (y_test</span><span>Â </span><span id="textcolor1810"><span>-</span></span><span>Â m)</span><span>Â </span><span id="textcolor1811"><span>**</span></span><span>Â </span><span id="textcolor1812"><span>2</span></span><span>Â </span><span id="textcolor1813"><span>/</span></span><span>Â v</span> <span id="x1-79097r5"></span> </code>
<code><span>)</span> <span id="x1-79099r6"></span> </code>
<code><span>test_log_likelihood_with_vnoise</span><span>Â </span><span id="textcolor1814"><span>=</span></span><span>Â np</span><span id="textcolor1815"><span>.</span></span><span>mean(</span> <span id="x1-79101r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1816"><span>-</span></span><span id="textcolor1817"><span>0.5</span></span><span>Â </span><span id="textcolor1818"><span>*</span></span><span>Â np</span><span id="textcolor1819"><span>.</span></span><span>log(</span><span id="textcolor1820"><span>2</span></span><span>Â </span><span id="textcolor1821"><span>*</span></span><span>Â math</span><span id="textcolor1822"><span>.</span></span><span>pi</span><span>Â </span><span id="textcolor1823"><span>*</span></span><span>Â (v</span><span>Â </span><span id="textcolor1824"><span>+</span></span><span>Â v_noise))</span> <span id="x1-79103r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1825"><span>-</span></span><span>Â </span><span id="textcolor1826"><span>0.5</span></span><span>Â </span><span id="textcolor1827"><span>*</span></span><span>Â (y_test</span><span>Â </span><span id="textcolor1828"><span>-</span></span><span>Â m)</span><span>Â </span><span id="textcolor1829"><span>**</span></span><span>Â </span><span id="textcolor1830"><span>2</span></span><span>Â </span><span id="textcolor1831"><span>/</span></span><span>Â (v</span><span>Â </span><span id="textcolor1832"><span>+</span></span><span>Â v_noise)</span> <span id="x1-79105r9"></span> </code>
<code><span>)</span></code></pre>
<p>Evaluating both of these metrics is good practice for any regression task for which you have model uncertainty estimates. The RMSE gives you your standard error metric, which allows you to compare directly with non-probabilistic methods. The NLL gives you an imdivssion of how well calibrated your method is by evaluating how confident your model is when itâ€™s doing well versus doing poorly, as we discussed earlier in the chapter. Together, these metrics give you a comprehensive imdivssion of a Bayesian modelâ€™s performance, and youâ€™ll see them used time and time again in the literature. <span id="x1-79106r134"></span></p>
</section>
</section>
<section id="summary-4" class="level2 sectionHead" data-number="10.8">
<h2 class="sectionHead" data-number="10.8" id="sigil_toc_id_61"><span class="titlemark">5.8 </span> <span id="x1-800008"></span>Summary</h2>
<p>In this chapter, we learned about two fundamental, well-principled, Bayesian deep learning models. BBB showed us how we can make use of variational inference to efficiently sample from our weight space and produce output distributions, while PBP demonstrated that itâ€™s possible to obtain predictive uncertainties <em>without</em> sampling. This makes PBP more computationally efficient than BBB, but each model has its pros and cons.</p>
<p>In BBBâ€™s case, while itâ€™s less computationally efficient than PBP, itâ€™s also more adaptable (particularly with the tools available in TensorFlow for variational layers). We can apply this to a variety of different DNN architectures with relatively little difficulty. The price is incurred through the sampling required at both inference and training time: we need to do more than just a single forward pass to obtain our output distributions.</p>
<p>Conversely, PBP allows us to obtain our uncertainty estimates with a single pass, but â€“ as weâ€™ve just seen â€“ itâ€™s quite complex to implement. This makes it awkward to adapt to other network architectures, and while it has been done (see the <em>Further reading</em> section), itâ€™s not a particularly practical method to use given the technical overhead of implementation and the relatively marginal gains compared to other methods.</p>
<p>In summary, these methods are excellent if you need robust, well-principled BNN approximations and arenâ€™t constrained in terms of memory or computational overheads at inference. But what if you have limited memory and/or limited compute, such as running on edge devices? In these cases, you may want to turn to more practical methods of obtaining predictive uncertainties.</p>
<p>In <em>Chapter 6, Bayesian Neural Network Approximation Using a Standard Deep</em> <em>Learning Toolbox</em>, weâ€™ll see how we can use more familiar components in TensorFlow to create more practical probabilistic neural network models. <span id="x1-80001r142"></span></p>
</section>
<section id="further-reading-3" class="level2 sectionHead" data-number="10.9">
<h2 class="sectionHead" data-number="10.9" id="sigil_toc_id_62"><span class="titlemark">5.9 </span> <span id="x1-810009"></span>Further reading</h2>
<ul>
<li><p><em>Weight Uncertainty in Neural Networks</em>, Charles Blundell <em>et al.</em>: This is the paper that introduced BBB, and is one of the key pieces of BDL literature.</p></li>
<li><p><em>Practical Variational Inference for Neural Networks</em>, Alex Graves <em>et al.</em>: An influential paper on the use of variational inference for neural networks, this work introduces a straightforward stochastic variational method that can be applied to a variety of neural network architectures.</p></li>
<li><p><em>Probabilistic Backpropagation for Scalable Learning of Bayesian</em> <em>Neural Networks</em>, JosÃ© Miguel HernÃ¡ndez-Lobato <em>et al.</em>: Another important work in BDL literature, this work introduced PBP, demonstrating how Bayesian inference can be achieved via more scalable means.</p></li>
<li><p><em>Practical Considerations for Probabilistic Backpropagation</em>, Matt Benatan <em>et al.</em>: In this work, the authors introduce methods for making PBP more practical for real-world applications.</p></li>
<li><p><em>Fully Bayesian Recurrent Neural Networks for Safe Reinforcement</em> <em>Learning</em>, Matt Benatan <em>et al.</em>: This paper shows how PBP can be adapted to an RNN architecture, and shows how BNNs can be advantageous in safety-critical systems.</p></li>
</ul>
<p><span id="x1-81001r114"></span></p>
</section>
</section>
<p id="footnote1"><strong><a href="#footref1">1</a></strong> It is beyond the scope of this book to guide the reader through the derivation of ELBO, but we encourage the reader to see the Further reading section for texts that provide a more comprehensive overview of ELBO.</p>
</body>
</html>