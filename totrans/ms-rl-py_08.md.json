["```py\nimport ray\nray.init()\n```", "```py\n@ray.remote\ndef remote_function():\n    return 1\n```", "```py\nobject_ids = []\nfor _ in range(4):\n    y_id = remote_function.remote()    \n    object_ids.append(y_id)\n```", "```py\n@ray.remote\ndef remote_chain_function(value):\n    return value + 1\ny1_id = remote_function.remote()\nchained_id = remote_chain_function.remote(y1_id)\n```", "```py\ny = 1\nobject_id = ray.put(y)\n```", "```py\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n    def increment(self):\n        self.value += 1\n        return self.value\n```", "```py\na = Counter.remote()\n```", "```py\nobj_id = a.increment.remote()\nray.get(obj_id) == 1\n```", "```py\n    max_samples = 500000\n    config = {\"env\": \"CartPole-v0\",\n              \"num_workers\": 50,\n              \"eval_num_workers\": 10,\n              \"n_step\": 3,\n              \"max_eps\": 0.5,\n              \"train_batch_size\": 512,\n              \"gamma\": 0.99,\n              \"fcnet_hiddens\": [256, 256],\n              \"fcnet_activation\": \"tanh\",\n              \"lr\": 0.0001,\n              \"buffer_size\": 1000000,\n              \"learning_starts\": 5000,\n              \"timesteps_per_iteration\": 10000,\n              \"grad_clip\": 10}\n```", "```py\n    ray.init()\n    parameter_server = ParameterServer.remote(config)\n    replay_buffer = ReplayBuffer.remote(config)\n    learner = Learner.remote(config, \n                             replay_buffer,\n                             parameter_server)\n```", "```py\nlearner.start_learning.remote()\n```", "```py\n    for i in range(config[\"num_workers\"]):\n        eps = config[\"max_eps\"] * i / config[\"num_workers\"]\n        actor = Actor.remote(\"train-\" + str(i), \n                             replay_buffer, \n                             parameter_server, \n                             config, \n                             eps)\n        actor.sample.remote()\n```", "```py\n    for i in range(config[\"eval_num_workers\"]):\n        eps = 0\n        actor = Actor.remote(\"eval-\" + str(i), \n                             replay_buffer, \n                             parameter_server, \n                             config, \n                             eps, \n                             True)\n```", "```py\n    total_samples = 0\n    best_eval_mean_reward = np.NINF\n    eval_mean_rewards = []\n    while total_samples < max_samples:\n        tsid = replay_buffer.get_total_env_samples.remote()\n        new_total_samples = ray.get(tsid)\n        if (new_total_samples - total_samples\n                >= config[\"timesteps_per_iteration\"]):\n            total_samples = new_total_samples\n            parameter_server.set_eval_weights.remote()\n            eval_sampling_ids = []\n            for eval_actor in eval_actor_ids:\n                sid = eval_actor.sample.remote()\n                eval_sampling_ids.append(sid)\n            eval_rewards = ray.get(eval_sampling_ids)\n            eval_mean_reward = np.mean(eval_rewards)\n            eval_mean_rewards.append(eval_mean_reward)\n            if eval_mean_reward > best_eval_mean_reward:\n                best_eval_mean_reward = eval_mean_reward\n                parameter_server.save_eval_weights.remote()\n```", "```py\n@ray.remote\nclass Actor:\n    def __init__(self,\n                 actor_id,\n                 replay_buffer,\n                 parameter_server,\n                 config,\n                 eps,\n                 eval=False):\n        self.actor_id = actor_id\n        self.replay_buffer = replay_buffer\n        self.parameter_server = parameter_server\n        self.config = config\n        self.eps = eps\n        self.eval = eval\n        self.Q = get_Q_network(config)\n        self.env = gym.make(config[\"env\"])\n        self.local_buffer = []\n        self.obs_shape = config[\"obs_shape\"]\n        self.n_actions = config[\"n_actions\"]\n        self.multi_step_n = config.get(\"n_step\", 1)\n        self.q_update_freq = config.get(\"q_update_freq\", 100)\n        self.send_experience_freq = \\\n                    config.get(\"send_experience_freq\", 100)\n        self.continue_sampling = True\n        self.cur_episodes = 0\n        self.cur_steps = 0\n```", "```py\n    def update_q_network(self):\n        if self.eval:\n            pid = \\\n              self.parameter_server.get_eval_weights.remote()\n        else:\n            pid = \\\n              self.parameter_server.get_weights.remote()\n        new_weights = ray.get(pid)\n        if new_weights:\n            self.Q.set_weights(new_weights)\n```", "```py\n    def sample(self):\n        self.update_q_network()\n        observation = self.env.reset()\n        episode_reward = 0\n        episode_length = 0\n        n_step_buffer = deque(maxlen=self.multi_step_n + 1)\n```", "```py\n        while self.continue_sampling:\n            action = self.get_action(observation)\n            next_observation, reward, \\\n            done, info = self.env.step(action)\n```", "```py\n            n_step_buffer.append((observation, action,\n                                  reward, done))\n            if len(n_step_buffer) == self.multi_step_n + 1:\n                self.local_buffer.append(\n                    self.get_n_step_trans(n_step_buffer))\n```", "```py\n            self.cur_steps += 1\n            episode_reward += reward\n            episode_length += 1\n```", "```py\n            if done:\n                if self.eval:\n                    break\n                next_observation = self.env.reset()\n                if len(n_step_buffer) > 1:\n                    self.local_buffer.append(\n                        self.get_n_step_trans(n_step_buffer))\n                self.cur_episodes += 1\n                episode_reward = 0\n                episode_length = 0\n```", "```py\n            observation = next_observation\n            if self.cur_steps % \\\n                    self.send_experience_freq == 0 \\\n                    and not self.eval:\n                self.send_experience_to_replay()\n            if self.cur_steps % \\\n                    self.q_update_freq == 0 and not self.eval:\n                self.update_q_network()\n        return episode_reward\n```", "```py\n     def get_action(self, observation):\n        observation = observation.reshape((1, -1))\n        q_estimates = self.Q.predict(observation)[0]\n        if np.random.uniform() <= self.eps:\n            action = np.random.randint(self.n_actions)\n        else:\n            action = np.argmax(q_estimates)\n        return action\n```", "```py\n    def get_n_step_trans(self, n_step_buffer):\n        gamma = self.config['gamma']\n        discounted_return = 0\n        cum_gamma = 1\n        for trans in list(n_step_buffer)[:-1]:\n            _, _, reward, _ = trans\n            discounted_return += cum_gamma * reward\n            cum_gamma *= gamma\n        observation, action, _, _ = n_step_buffer[0]\n        last_observation, _, _, done = n_step_buffer[-1]\n        experience = (observation, action, discounted_return,\n                      last_observation, done, cum_gamma)\n        return experience\n```", "```py\n    def send_experience_to_replay(self):\n        rf = self.replay_buffer.add.remote(self.local_buffer)\n        ray.wait([rf])\n        self.local_buffer = []\n```", "```py\n@ray.remote\nclass ParameterServer:\n    def __init__(self, config):\n        self.weights = None\n        self.eval_weights = None\n        self.Q = get_Q_network(config)\n    def update_weights(self, new_parameters):\n        self.weights = new_parameters\n        return True\n    def get_weights(self):\n        return self.weights\n    def get_eval_weights(self):\n        return self.eval_weights\n    def set_eval_weights(self):\n        self.eval_weights = self.weights\n        return True\n    def save_eval_weights(self,\n                          filename=\n                          'checkpoints/model_checkpoint'):\n        self.Q.set_weights(self.eval_weights)\n        self.Q.save_weights(filename)\n        print(\"Saved.\")\n```", "```py\n@ray.remote\nclass ReplayBuffer:\n    def __init__(self, config):\n        self.replay_buffer_size = config[\"buffer_size\"]\n        self.buffer = deque(maxlen=self.replay_buffer_size)\n        self.total_env_samples = 0\n    def add(self, experience_list):\n        experience_list = experience_list\n        for e in experience_list:\n            self.buffer.append(e)\n            self.total_env_samples += 1\n        return True\n    def sample(self, n):\n        if len(self.buffer) > n:\n            sample_ix = np.random.randint(\n                len(self.buffer), size=n)\n            return [self.buffer[ix] for ix in sample_ix]\n    def get_total_env_samples(self):\n        return self.total_env_samples\n```", "```py\ndef get_Q_network(config):\n    obs_input = Input(shape=config[\"obs_shape\"],\n                      name='Q_input')\n    x = Flatten()(obs_input)\n    for i, n_units in enumerate(config[\"fcnet_hiddens\"]):\n        layer_name = 'Q_' + str(i + 1)\n        x = Dense(n_units,\n                  activation=config[\"fcnet_activation\"],\n                  name=layer_name)(x)\n    q_estimate_output = Dense(config[\"n_actions\"],\n                              activation='linear',\n                              name='Q_output')(x)\n    # Q Model\n    Q_model = Model(inputs=obs_input,\n                    outputs=q_estimate_output)\n    Q_model.summary()\n    Q_model.compile(optimizer=Adam(), loss='mse')\n    return Q_model\n```", "```py\ndef masked_loss(args):\n    y_true, y_pred, mask = args\n    masked_pred = K.sum(mask * y_pred, axis=1, keepdims=True)\n    loss = K.square(y_true - masked_pred)\n    return K.mean(loss, axis=-1)\n```", "```py\ndef get_trainable_model(config):\n    Q_model = get_Q_network(config)\n    obs_input = Q_model.get_layer(\"Q_input\").output\n    q_estimate_output = Q_model.get_layer(\"Q_output\").output\n    mask_input = Input(shape=(config[\"n_actions\"],),\n                       name='Q_mask')\n    sampled_bellman_input = Input(shape=(1,),\n                                  name='Q_sampled')\n    # Trainable model\n    loss_output = Lambda(masked_loss,\n                         output_shape=(1,),\n                         name='Q_masked_out')\\\n                        ([sampled_bellman_input,\n                          q_estimate_output,\n                          mask_input])\n    trainable_model = Model(inputs=[obs_input,\n                                    mask_input,\n                                    sampled_bellman_input],\n                            outputs=loss_output)\n    trainable_model.summary()\n    trainable_model.compile(optimizer=\n                            Adam(lr=config[\"lr\"],\n                            clipvalue=config[\"grad_clip\"]),\n                            loss=[lambda y_true,\n                                         y_pred: y_pred])\n    return Q_model, trainable_model\n```", "```py\n@ray.remote\nclass Learner:\n    def __init__(self, config, replay_buffer, parameter_server):\n        self.config = config\n        self.replay_buffer = replay_buffer\n        self.parameter_server = parameter_server\n        self.Q, self.trainable = get_trainable_model(config)\n        self.target_network = clone_model(self.Q)\n```", "```py\n    def optimize(self):\n        samples = ray.get(self.replay_buffer\n                          .sample.remote(self.train_batch_size))\n        if samples:\n            N = len(samples)\n            self.total_collected_samples += N\n            self.samples_since_last_update += N\n            ndim_obs = 1\n            for s in self.config[\"obs_shape\"]:\n                if s:\n                    ndim_obs *= s\n```", "```py\n            n_actions = self.config[\"n_actions\"]\n            obs = np.array([sample[0] for sample \\\n                        in samples]).reshape((N, ndim_obs))\n            actions = np.array([sample[1] for sample \\\n                        in samples]).reshape((N,))\n            rewards = np.array([sample[2] for sample \\\n                        in samples]).reshape((N,))\n            last_obs = np.array([sample[3] for sample \\\n                        in samples]).reshape((N, ndim_obs))\n            done_flags = np.array([sample[4] for sample \\\n                        in samples]).reshape((N,))\n            gammas = np.array([sample[5] for sample \\\n                        in samples]).reshape((N,))\n```", "```py\n            masks = np.zeros((N, n_actions))\n            masks[np.arange(N), actions] = 1\n            dummy_labels = np.zeros((N,))\n```", "```py\n            # double DQN\n            maximizer_a = np.argmax(self.Q.predict(last_obs), axis=1)\n            target_network_estimates = self.target_network.predict(last_obs)\n            q_value_estimates = np.array([target_network_estimates[i,\n                                   maximizer_a[i]]\n                                   for i in range(N)]).reshape((N,))\n            sampled_bellman = rewards + gammas * \\\n                              q_value_estimates * (1 - done_flags)\n            trainable_inputs = [obs, masks,\n                                sampled_bellman]\n            self.trainable.fit(trainable_inputs, dummy_labels, verbose=0)\n            self.send_weights()\n```", "```py\n            if self.samples_since_last_update > 500:\n                self.target_network.set_weights(self.Q.get_weights())\n                self.samples_since_last_update = 0\n            return True\n```", "```py\npython train_apex_dqn.py\n```", "```py\ntensorboard --logdir logs/scalars\n```", "```py\nimport pprint\nfrom ray import tune\nfrom ray.rllib.agents.dqn.apex import APEX_DEFAULT_CONFIG\nfrom ray.rllib.agents.dqn.apex import ApexTrainer\nif __name__ == '__main__':\n    config = APEX_DEFAULT_CONFIG.copy()\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(config)\n    config['env'] = \"CartPole-v0\"\n    config['num_workers'] = 50\n    config['evaluation_num_workers'] = 10\n    config['evaluation_interval'] = 1\n    config['learning_starts'] = 5000\n    tune.run(ApexTrainer, config=config)\n```", "```py\ntensorboard --logdir=~/ray_results\n```", "```py\n{   'adam_epsilon': 1e-08,\n    'batch_mode': 'truncate_episodes',\n    'beta_annealing_fraction': -1,\n    'buffer_size': 2000000,\n    'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n    'clip_actions': True,\n    'clip_rewards': None,\n    'collect_metrics_timeout': 180,\n    'compress_observations': False,\n    'custom_eval_function': None,\n    'custom_resources_per_worker': {},\n    'double_q': True,\n    'dueling': True,\n...\n```"]