- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Reminders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep learning** is the specific domain of machine learning based on neural
    networks. Deep learning is known to be particularly powerful with unstructured
    data, such as text, audio, and image, but can be useful for time series and structured
    data too. In this chapter, we will review the basics of deep learning, from a
    perceptron to training a neural network. We will provide recipes for training
    neural networks for three main use cases: regression, binary classification, and
    multiclass classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network for binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a multiclass classification neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will train a perceptron, as well as several neural networks.
    To do so, the following libraries are required:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchvision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron is arguably the building block of deep learning. Even if the
    perceptron is not directly used in production systems, understanding what it is
    can be an asset for building a strong foundation in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will review what a perceptron is and then train one using
    scikit-learn on the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The perceptron is a machine learning method first proposed to mimic a biological
    neuron. It was first proposed in the 1940s and then implemented in the 1950s.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a high-level point of view, a neuron can be described as a cell that receives
    input signals and fires a signal itself when the sum of the input signals is above
    a given threshold. This is exactly what a perceptron does; all you have to do
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the input signals with features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a weighted sum to those features and apply an activation function to it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the output signal with a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More formally, assuming *n* input features ![](img/B19629_06_01.png) and n
    weights ![](img/formula_06_002.png), the output ŷ of a perceptron is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/formula_06_004.png) is the bias and *g* is the activation function;
    historically, this is the step function, which returns 1 for a positive input
    value, 0 otherwise. So, at the end, for *n* input features, a perceptron is made
    of *n+1* parameters: one parameter per feature plus the bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The step function is also called the **Heaviside function** and is widely used
    in other fields, such as physics.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron forward computation is summarized in *Figure 6**.1*. As you can
    see, given a list of features ![](img/formula_06_005.png) and weights ![](img/formula_06_006.png),
    the forward computation is just the weighted sum, to which an activation function
    is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – A mathematical representation of a perceptron: from input features
    to output, through weights and the activation function](img/B19629_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1 – A mathematical representation of a perceptron: from input features
    to output, through weights and the activation function'
  prefs: []
  type: TYPE_NORMAL
- en: On the practical side, scikit-learn is the only thing required for installation
    for this recipe. It can be installed with the `pip install` `scikit-learn` command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Iris dataset again since the perceptron does not really perform
    well on complex classification tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the required imports from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_iris`: A function to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split`: A function to split the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler`: A class allowing us to rescale the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Perceptron`: The class containing the implementation of the perceptron:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Iris dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets using the `train_test_split` function,
    with `random state` set to `0` for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since all the features are quantitative here, we simply rescale all the features
    with a standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the model with the default parameters and fit it on the training
    set with the `.``fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on both the training and test sets with the `.score()` method
    of the `LinearRegression` class, providing the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Out of curiosity, we can have a look at the weights in `.coef_` and the bias
    in `.intercept_`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There are three sets of four weights and one bias, since scikit-learn handles
    on its own the One-vs-Rest multiclass classification, so we have one perceptron
    per class.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is not only a machine learning model. It can be used to simulate
    logical gates: OR, AND, NOR, NAND, and XOR. Let’s have a look.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily implement a forward propagation for the perceptron with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code does not consider many edge cases, but is used here simply to explain
    and demonstrate simple concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AND gate has the inputs and expected outputs defined in the following truth
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input 1** | **Input 2** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – AND gate truth table
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s reproduce this data with an array `X` that has two features (input 1
    and input 2) and four samples, and an array `y` with the expected outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now find a set of weights and bias that will allow the perceptron to
    act as an AND gate, and check the results to see whether it’s actually working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With the same logic, most basic logic gates can be created out of a perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AND gate: weights [1, 1] and bias -1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OR gate: weights [1, 1] and bias 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NOR gate: weights [-1, -1] and bias 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NAND gate: weights [-1, -1] and bias 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XOR gate: this requires two perceptrons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can guess the weights and bias with a trial-and-error approach. But you
    can also use the truth table of a logic gate to make an educated guess or even
    to solve a set of equations.
  prefs: []
  type: TYPE_NORMAL
- en: This means that using perceptrons, any logic function can be computed.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation of the scikit-learn implementation: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A perceptron is not a powerful and commonly used machine learning model. But
    having many perceptrons employed together in a neural network can become a powerful
    machine learning model. In this recipe, we will review a simple neural network,
    sometimes called a **multi-layer perceptron** or **vanilla neural network**. And
    we will then train such a neural network on a regression task on the California
    housing dataset with PyTorch, a widely used framework in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by reviewing what a neural network is, and how to feed forward a
    neural network from input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network can be divided into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The input layer**, containing the input features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hidden layers**, which can be any number of layers and units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The output layer**, which is defined by the expected output of the neural
    network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both the hidden and output layers, we consider each unit (or neuron) to be
    a perceptron, with its own weights and bias.
  prefs: []
  type: TYPE_NORMAL
- en: These three parts are well represented in *Figure 6**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – A typical representation of a neural network: on the left the
    input layer, in the middle the hidden layers, on the right the output layer](img/B19629_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2 – A typical representation of a neural network: on the left the
    input layer, in the middle the hidden layers, on the right the output layer'
  prefs: []
  type: TYPE_NORMAL
- en: We will note the input features ![](img/formula_06_007.png), the activation
    of the unit *i* of the layer *l*, and ![](img/formula_06_008.png), the we­­­ights
    of the unit *i* of the layer *l*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We consider a neural network to involve deep learning if there is at least one
    hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a neural network in regression is not so different from training a
    linear regression. It is made of the same ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation, from input features and weights to a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A loss function to minimize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An algorithm to update the weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s have a look at those ingredients.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The forward propagation is what allows to compute and output from the input
    features. It must be computed from left to right, from the input layer (the input
    features) to the output layer (the output prediction). Each unit being a perceptron,
    the first hidden layer is fairly easy to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_009.jpg)![](img/formula_06_010.jpg)![](img/formula_06_011.jpg)![](img/formula_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/formula_06_013.png) is the bias, and ![](img/formula_06_014.png)
    is the activation function of layer 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to compute the activations of the second hidden layer ![](img/formula_06_015.png),
    we would use the exact same formulas, but with the activations of the first hidden
    layer as input (![](img/formula_06_016.png)), instead of the input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_017.jpg)![](img/formula_06_018.jpg)![](img/formula_06_019.jpg)![](img/formula_06_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can easily generalize to any number of hidden layers and any number of units
    per layer – the principle remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the output layer would be computed in exactly the same way, except
    in this case we have only one output neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One interesting thing to underline: the activation function is also layer dependent,
    meaning that each layer can have a different activation function. This is particularly
    critical for the output layer, which needs a specific output function depending
    on the task and expected output.'
  prefs: []
  type: TYPE_NORMAL
- en: For a regression task, it is common to have a linear activation function, allowing
    the output values of the neural network to be any number.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation function plays a decisive role in neural networks: it adds non-linearity.
    If we have only linear activation functions for hidden layers, no matter the number
    of layers, it is equivalent to having no hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The loss function in a regression task can be the same as in a linear regression:
    the mean squared error. In our example, if we consider the prediction *ŷ* to be
    our output value ![](img/formula_06_022.png), the loss *L* is simply the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Assuming *j* is the sample index.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Updating the weights is done by trying to minimize the loss function. Again,
    this is almost the same as in a linear regression. The tricky part is that, unlike
    linear regression, we have several layers of units with weights and biases that
    all need to be updated. This is where the so-called backpropagation allows the
    updating of each layer step by step, from the rightmost to the leftmost (following
    the convention in *Figure 6**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: The details of the backpropagation, although useful and interesting, are beyond
    this book’s scope.
  prefs: []
  type: TYPE_NORMAL
- en: Also, just like there are several algorithms to optimize the weights in a logistic
    regression (the `solver` parameter in scikit-learn’s `LogisticRegression`), there
    are several algorithms to train a neural network. They are commonly called **optimizers**.
    Among the most frequently used are the **Stochastic Gradient Descent** (**SGD**)
    and **Adaptive** **momentum** (**Adam**).
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is a widely used framework for deep learning, allowing us to easily
    train and reuse deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is fairly easy to use and can be easily installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: For this recipe, we will also need scikit-learn and matplotlib, which can be
    installed with `pip install` `scikit-learn matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will build and train a neural network on the California
    housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need the required imports. Among the imports are some from scikit-learn
    that we have already used in this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`fetch_california_housing` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` to split the data into training and test sets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` to rescale the quantitative data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score` to evaluate the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For display purposes, we also import matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need some imports from torch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` itself for some functions at the lower level of the library'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` containing many useful classes for building a neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for some useful functions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` and `DataLoader` for handling the data operations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to load the data using the `fetch_california_housing` function and
    return the features and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then split the data into training and test sets using the `train_test_split`
    function. We set a test size of 20% and a random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now rescale the data with a standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we convert the *X* and *y* variables to float32 variables. This is
    necessary to prevent later troubles with PyTorch not properly handling float64
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For PyTorch, we need to create the dataset class. Nothing complicated here
    though; this class requires only the following to work properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has to inherit from the `Dataset` class (imported earlier)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to have a constructor (`__init__` method) that deals with (and optionally
    prepares) the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to have a `__len__` method, so that the number of samples can be fetched
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It has to have a `__getitem__` method, in order to get `X` and `y` for any given
    index
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this for the California dataset, and let’s call our class `CaliforniaDataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If we break this class down, we have the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `init` constructor simply converts `X` and `y` to torch tensors with the
    `torch.from_numpy` function and stores the results as class attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `len` method just returns the length of the `X` attribute; it would work
    equally using the length of the `y` attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getitem` method simply returns a tuple with the given item `idx` of the
    `X` and `y` tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is quite straightforward, and will then allow `pytorch` to know what the
    data is, how many samples are in the dataset, and what the sample `i` is. For
    that, we will need to instantiate a `DataLoader` class.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Rescaling can also be computed in this `CaliforniaDataset` class, as well as
    any preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we instantiate the `CaliforniaDataset` objects for the training and test
    datasets. Then we instantiate the associated loaders using the imported `DataLoader`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data loader instances have a couple of options available. Here, we specify
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: The batch size for training. It may have an impact on the final
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle`: Determines whether to shuffle the data at each epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can finally create the neural network model class. For this class, we only
    need to fill in two methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The constructor with whatever is useful, such as parameters and attributes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `forward` method that computes the forward propagation:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'If we break it down, we have designed a class that takes two input parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_shape` is the input shape of the neural networks – this is basically
    the number of features in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_units` is the number of units in the hidden layers, which defaults
    to 24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The neural network itself comprises the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two hidden layers of `hidden_units` units with ReLU activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One output layer of one unit, since we need to predict one value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More context about ReLU and other activation functions will be given in the
    next *There’s* *more* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now instantiate a neural network and test it on random data of the expected
    shape to check whether the `forward` method is working properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the computation of the forward propagation on the random data
    worked well and returns one single value, as expected. Any error in that step
    would mean we did something wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before being able to train the neural network on the data, we need to define
    the loss function and the optimizer. Fortunately, the mean squared error is already
    implemented and available as `nn.MSELoss()`. There are plenty of optimizers available;
    we decided to use Adam here, but other optimizers can also be tested:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer needs the network parameters as input to its constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can train the neural networks on 10 epochs with the following piece
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Hopefully, the comments are self-explanatory. Basically, there are two nested
    loops:'
  prefs: []
  type: TYPE_NORMAL
- en: 'One outer loop over the epochs: the number of times the model is trained over
    the whole dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One inner loop over the samples: for each step, a batch of `batch_size` samples
    is used to train the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each step in the inner loop, we have the following main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get a batch of the data: both features and labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward propagate on this data and get output predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute the loss: the mean squared error between predictions and labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the weights of the network with backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of each step, we print the loss, which hopefully decreases with each
    epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the loss as a function of the epoch. This is quite visual and lets
    us ensure the network is learning if the loss is decreasing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the resulting graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Resulting MSE loss as a function of the epoch](img/B19629_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Resulting MSE loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We could also keep track of the loss on the test set and display it at this
    step for more information. We will do that in the next recipe to avoid being drowned
    in too much information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finally evaluate the model on both the training and test sets. As we
    did previously in this book with regression tasks, we will use the R2-score. Any
    other relevant metric can be used too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, we have a reasonable R2-score of 0.74 on the training set,
    with minor overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we mentioned activation functions without really explaining
    what they are or why they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, they add non-linearities, allowing the model to learn more complex
    patterns. Indeed, if we had a neural network with no activation function, the
    whole model would be equivalent to a linear model (e.g., a linear regression),
    no matter the number of layers or the number of units. This is summarized in *Figure
    6**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – On the left, a neural network with no activation functions can
    only learn linearly separable decision functions. On the right, a neural network
    with activation functions can learn complex decision functions](img/B19629_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – On the left, a neural network with no activation functions can
    only learn linearly separable decision functions. On the right, a neural network
    with activation functions can learn complex decision functions
  prefs: []
  type: TYPE_NORMAL
- en: There are many available activation functions, but some of the most common ones
    for hidden layers are sigmoid, ReLU, and tanh.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sigmoid function is the same as that used in logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This function’s values range from 0 to 1, and outputs 0.5 if *x =* *0*.
  prefs: []
  type: TYPE_NORMAL
- en: tanh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The tanh or hyperbolic tangent function ranges from -1 to 1, with a value of
    0 if *x* is 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **ReLU** or **Rectified Linear Unit** function just returns 0 for any input
    negative value, and *x* for any positive input value *x*. Unlike sigmoid and tanh,
    it does not plateau and thus limits vanishing gradient problems. Its formula is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/formula_06_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can visualize these three activation functions (sigmoid, tanh, and ReLU)
    together for a more intuitive understanding with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll get this output upon running the previous code, which computes the output
    values of these functions in the [-2, 2] range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions
    in the [-2, 2] input range](img/B19629_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions
    in the [-2, 2] input range
  prefs: []
  type: TYPE_NORMAL
- en: 'For more about the available activation functions in PyTorch, have a look at
    the following link: [https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity.](https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity%0D)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are several links to PyTorch tutorials that can be helpful for gaining
    familiarity with it, along with a deeper understanding of how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the following link is for a very well-written website about deep learning,
    for those who wish to have a better understanding of neural networks, gradient
    descent, and backpropagation: [http://neuralnetworksanddeeplearning.com/.](http://neuralnetworksanddeeplearning.com/%0D)'
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network for binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let’s train our first neural network for a binary classification
    task on the breast cancer dataset. We will also learn more about the impact of
    the learning rate and the optimizer on the optimization, as well as how to evaluate
    the model against the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we will see in this recipe, training a neural network for binary classification
    is not so different from training a neural network for regression. Primarily,
    two changes have to be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer’s activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous recipe for a regression task, the output layer had no activation
    function. Indeed, for a regression, one can expect the prediction to take any
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a binary classification, we expect the output to be a probability, so a
    value between 0 and 1, just like the logistic regression. This is why when doing
    a binary classification, the output layer’s activation function is usually the
    sigmoid function. The resulting predictions will be just like those of a logistic
    regression: a number on which to apply a threshold (e.g., 0.5) above which we
    consider the prediction to be class 1.'
  prefs: []
  type: TYPE_NORMAL
- en: As the labels are 0s and 1s, and the predictions are values between 0 and 1,
    the mean squared error is no longer suited to train such a model. So, just like
    for a logistic regression, we would use binary cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: The required libraries for this recipe are matplotlib, scikit-learn, and PyTorch,
    they can be installed with `pip install matplotlib` `scikit-learn torch`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will train a simple neural network with two hidden layers on a binary classification
    task on the breast cancer dataset. Even though this dataset is not really suited
    for deep learning since it is a small dataset, it allows us to easily understand
    all the steps involved in training a neural network for binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following required imports from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_breast_cancer` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` to split the data into training and test sets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` to rescale the quantitative data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy_score` to evaluate the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need matplotlib for display, and we need the following from torch:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch` itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` containing required classes for building a neural network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for activation functions such as ReLU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` and `DataLoader` for handling the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the features and labels with the `load_breast_cancer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets, specifying the random state for
    reproducibility. Also cast the features and labels for `float32` for later compatibility
    with PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `Dataset` class for handling the data. Note that in this recipe
    we integrate the data rescaling in this step, unlike in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Having the scaler in the class has pros and cons, where a pro is properly handling
    data leakage between train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the training and test sets and loaders. Note that no scaler is
    provided to the training dataset, while the test dataset is given the training
    set scaler to ensure that all the data is processed the same with no data leakage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the neural network. Here, we build a neural network with two hidden layers.
    In the `forward` method, the `torch.sigmoid()` function is applied to the output
    layer before returning the value, ensuring we have a prediction between 0 and
    1\. The only parameter needed to instantiate the model is the input shape, which
    is simply the number of features here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now instantiate the model with the right input shape and check that
    the forward propagation works properly on a given random tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running the previous code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss function and the optimizer. As stated, we will use the binary
    cross-entropy loss, available as `nn.BCELoss()` in PyTorch. The chosen optimizer
    is `Adam`, but other optimizers can be tested too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More explanations about the optimizer are provided in the next *There’s* *more*
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now train the neural network for 50 epochs. We also compute both the
    training and test set loss at each epoch, so we can plot them afterward. To do
    so, we need to switch mode for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before training on the training set, switch to train mode with `model.train()`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before evaluating the test set, switch to the `eval` model with `model.eval()`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE231]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Notice the use of `with` `torch.no_grad()` around the evaluation part. This
    line of code allows us to deactivate the autograd engine and speed up processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we plot the losses for the train and test sets as a function of the epoch,
    using the two computed lists in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Resulting MSE loss for the train and test sets](img/B19629_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Resulting MSE loss for the train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, both losses are decreasing. At first, the train and test losses
    are almost equal, but after 10 epochs, the train loss keeps decreasing while the
    test does not, meaning the model has overfit on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to evaluate the model using the accuracy scores from both the
    training and test sets, via the `accuracy_score` function of scikit-learn. It
    requires a few more steps to compute the predictions, since we have to do the
    following operations before getting actual class predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rescale the data with the scaler used for training, available in the `training_data.x_scaler`
    attribute
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the NumPy data to the torch tensor with `torch.tensor()`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply forward propagation to the model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the output torch tensor back to NumPy with `.detach().numpy()`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a threshold to convert a probability prediction (between 0 and 1) to a
    class prediction with `>` `0.5`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE247]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE248]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE250]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE252]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE254]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: We get an accuracy of 99% on the training and 96% on the test set, proving there
    is indeed overfitting, as expected from the curve of the train and test losses
    as a function of the epoch.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen here, the loss is decreasing over time, meaning the model is
    actually learning.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Even if it’s sometimes a bit noisy with bumps in the loss, as long as the overall
    trend remains good, there is nothing to worry about.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important notions that may alter these results, somewhat related
    to each other: the learning rate and the optimizer. As with logistic regression
    or linear regression, the optimizer’s goal is to find the parameters that provide
    the lowest possible loss value. Therefore, this is a minimization problem and
    can be represented as in *Figure 6**.7*: we seek to find the set of parameters
    that give the lowest possible value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Representation of the loss L as a function of the parameters
    w. The red cross is the optimal point, while the blue cross is a random arbitrary
    set of weights](img/B19629_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Representation of the loss L as a function of the parameters w.
    The red cross is the optimal point, while the blue cross is a random arbitrary
    set of weights
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the learning rate can impact the learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The learning rate is set in PyTorch when instantiating the optimizer, with the
    *lr=0.001* parameter for example. Arguably, we can have four main cases for the
    learning rate value, as presented in *Figure 6**.8*, from a low learning rate
    to a very high learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The four main categories of learning rate: too low learning
    rate, good learning rate, high learning rate, very high learning rate (diverging
    loss)](img/B19629_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8 – The four main categories of learning rate: too low learning rate,
    good learning rate, high learning rate, very high learning rate (diverging loss)'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of loss, the rates can be intuited from *Figure 6**.9*, presenting
    the evolution of the weights and the loss for several epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – A visual interpretation of the four cases of learning rate:
    a) a low learning rate, b) a good learning rate, c) a high learning rate, d) a
    very high learning rate](img/B19629_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9 – A visual interpretation of the four cases of learning rate: a)
    a low learning rate, b) a good learning rate, c) a high learning rate, d) a very
    high learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.9* can be further explained with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A low learning rate (a)**: The loss will decrease over the epochs but too
    slowly and may take a very long time to converge. It may also get the model stuck
    in a local minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A good learning rate (b)**: The loss will decrease steadily until it gets
    close enough to the global minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A slightly too large learning rate (c)**: The loss will decrease steeply
    at first but may soon jump over the global minimum without being able to ever
    reach it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A very high learning rate (d)**: The loss will rapidly diverge, taking learning
    steps that are way too large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning the learning rate may sometimes help to produce the best results. Several
    techniques, such as the so-called learning rate decay, decrease the learning rate
    over time to hopefully more accurately catch the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many robust and useful optimizers in deep learning besides the arguably
    most famous ones (the stochastic gradient descent and Adam). Without getting into
    the details of those optimizers, let’s just give some insight into how they work
    and their differences, summarized in *Figure 6**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** simply computes the gradients from the loss
    for each batch, without any further sophistication. It means that sometimes, the
    optimization of one batch may be almost in the opposite direction of another batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam** uses momentum, meaning that for each batch, not only the gradient
    from this batch is used, but also the momentum of the previously computed gradients.
    This allows Adam to keep an overall more consistent direction, and hopefully to
    converge faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – A visual representation of training towards a global minimum,
    on the left with stochastic gradient descent; on the right with Adam keeping the
    momentum of previous steps](img/B19629_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – A visual representation of training towards a global minimum,
    on the left with stochastic gradient descent; on the right with Adam keeping the
    momentum of previous steps
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization process can be summarized with quite a simple metaphor. This
    is like hiking somewhere up a mountain, and then trying to go down (to the global
    minimum) while surrounded by fog. You can either go down with stochastic gradient
    descent or Adam:'
  prefs: []
  type: TYPE_NORMAL
- en: With stochastic gradient descent, you look around you, choose the direction
    with the steepest slope downward, and take a step in that direction. And then
    do it again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Adam, you do the same as stochastic gradient descent but running. You quickly
    look around you, see the direction with the steepest slope downward, and try to
    take a step in that direction while keeping the inertia from your previous steps
    since you’re running. And then do it again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that in this analogy, the step size would be the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A list of available optimizers on PyTorch: [https://pytorch.org/docs/stable/optim.xhtml#algorithms](https://pytorch.org/docs/stable/optim.xhtml#algorithms)'
  prefs: []
  type: TYPE_NORMAL
- en: Training a multiclass classification neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will have a look at another very common task: multiclass
    classification with neural networks, in this instance using PyTorch. We will work
    on a very iconic dataset in deep learning: **MNIST handwritten digit recognition**.
    This dataset is a set of small grayscale images of 28x28 pixels, depicting handwritten
    digits between 0 and 9, having thus 10 classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In classical machine learning, multiclass classification is usually not handled
    natively. For example, when training logistic regression with scikit-learn on
    a three-class task (e.g., the Iris dataset), scikit-learn will automatically train
    three models, using the one-versus-the-rest method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In deep learning, it is possible for the model to natively handle more than
    two classes. To do so, only a few changes are required compared to binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output layer has as many units as classes: this way, each unit will be
    responsible for predicting the probability of one class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output layer’s activation function is the softmax function, a function such
    that the sum of the units is equal to 1, allowing us to consider it as a probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function is the cross-entropy loss, considering multiple classes, unlike
    the binary cross entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we will need a few other changes in the code that are specific
    to the data itself. Since the input is now an image, some transformations are
    required:'
  prefs: []
  type: TYPE_NORMAL
- en: The image, a 2D (or 3D if RGB color image) array, must be flattened to be 1D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data must be normalized, just like rescaling for quantitative data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do that, we will require the following libraries: torch, torchvision (for
    dataset loading and image transformation), and matplotlib for visualization. They
    can be installed with `pip install torch` `torchvision matplotlib`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will reuse the same pattern as previously in this chapter:
    we will train a two-hidden-layer neural network. But a few things will change,
    though:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data is a grayscale image from the MNIST handwritten digits dataset,
    so it’s a 2D array that needs to be flattened
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output layer will have not one, but ten units for the ten classes of the
    dataset; the loss will change accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will not only compute the training and test losses in the training loop,
    but also the accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how to do that in practice now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries. As in previous recipes, we import several useful
    torch modules and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` itself'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` containing the required classes for building a neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for activation functions such as ReLU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataLoader` for handling the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need some imports from torchvision:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MNIST` for loading the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms` for transforming the dataset, both rescaling and flattening the
    data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE258]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the transformations. We use the `Compose` class, allowing us to
    compose two or more transformations. Here, we compose three transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`transforms.ToTensor()`: Converts the input image to `torch.Tensor` format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Normalize()`: Normalizes the image with a mean value and standard
    deviation. It will subtract the mean (i.e., 0.1307) and then divide by the standard
    deviation (i.e., 0.3081) for each pixel value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Lambda(torch.flatten)`: Flattens the 2D tensor to a 1D tensor:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE263]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE264]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE265]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Images are commonly normalized with a mean and standard deviation of 0.5\. We
    normalize with the specific values used in the preceding code block because the
    dataset is made with specific images, but 0.5 would work fine too. Check the *See
    also* subsection of this recipe for an explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the train and test sets, as well as the train and data loaders. Using
    the `MNIST` class, we both get the train and test sets using the `train` parameter
    `as` `True` and `False`, respectively. We directly apply the previously defined
    transformations while loading the data with the MNIST class too. Then we instantiate
    the data loaders with a batch size of 64:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE267]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE268]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network. We define here by default a neural network with
    2 hidden layers of 24 units. The output layer has 10 units for the 10 classes
    of the data (our digits between 0 and 9). Note that the softmax function is applied
    to the output layer, allowing the sum of the 10 units to be strictly equal to
    1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now instantiate the model with the right input shape of 784 (28x28 pixels),
    and check the forward propagation works properly on a given random tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Note the output is a tensor of 10 values, with a sum of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the loss function as the cross-entropy loss, available as `nn.CrossEntropyLoss()`
    in PyTorch, and the optimizer as Adam:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before training, we implement an `epoch_step` helper function that works for
    both the train and test sets, allowing us to loop over all the data, compute the
    loss and the accuracy, and train the model for the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE307]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE308]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE309]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now train the neural network on 20 epochs. For each epoch, we also compute
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss for both train and test sets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy for both train and test sets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for the previous recipe, before training, the model is switched to train
    mode with `model.train()`, while before evaluating on the test set, it is switched
    to `eval` model with `model.eval()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE327]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the loss for both the train and test sets as a function of the
    epoch, since we stored those values for each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for
    both the train and test sets](img/B19629_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for both
    the train and test sets
  prefs: []
  type: TYPE_NORMAL
- en: Since the loss seems to keep improving on both the **train** and **test** sets
    after 20 epochs, it could be interesting in terms of performance to keep training
    more epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to do the same with the accuracy score, showing the equivalent
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Resulting accuracy as a function of the epoch for the train
    and test sets](img/B19629_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Resulting accuracy as a function of the epoch for the train and
    test sets
  prefs: []
  type: TYPE_NORMAL
- en: At the end, the accuracy is about 97% on the train set and 96% on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained, it is, of course, possible to store it so
    that it can be used on new data directly. There are several ways to save a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`net` class must first be instantiated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saving the entire model**: This saves both the weights and the architecture,
    meaning only the file needs to be loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saving in torchscript format**: This saves the entire model using a more
    efficient representation. This method is more suited for deployment and inference
    at scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For now, let’s just save the `state` dict, reload it, and then compute inferences
    on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now possible to compute inferences using that loaded, already-trained
    model on a given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – The resulting output with six input images and their predictions
    from the trained model](img/B19629_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – The resulting output with six input images and their predictions
    from the trained model
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the loaded model can correctly predict the right number on most
    images.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning is often used in heavily computational tasks requiring a lot
    of resources. In such cases, the use of a GPU is often a necessity. PyTorch of
    course allows us to train and infer models on GPUs. Only a few steps are required
    to do so: declaring a device variable and moving both the model and data to this
    device. Let’s have a quick look at how to do it.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Declaring a device variable to be the GPU can be done with the following Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE338]'
  prefs: []
  type: TYPE_PRE
- en: This line instantiates a `torch.device` object, containing `"cuda"` if CUDA
    is available, else it contains `"cpu"`. Indeed, if CUDA is not installed, or if
    there is no GPU on your hardware, the CPU will be used (which is the default behavior).
  prefs: []
  type: TYPE_NORMAL
- en: If the GPU has been correctly detected, the output of `print(device)` is `"cuda"`.
    Otherwise, the output is `"cpu"`.
  prefs: []
  type: TYPE_NORMAL
- en: Moving the model and data to the GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the device is correctly set to the GPU, both the model and data have to
    be moved to the GPU memory. To do so, you only need to call the`.to(device)` method
    on both the model and the data. For example, the training and evaluation code
    that we used in this recipe would become the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  prefs: []
  type: TYPE_PRE
- en: At the beginning, the model is moved to the GPU device once with `net =` `net.to(device)`.
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration loop for both the training and evaluation, the inputs and
    labels tensors are moved to the device with `tensor =` `tensor.to(device)`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The data can be either fully loaded on the GPU at loading, or done one batch
    at a time during training. However, since only rather small datasets can be fully
    loaded in the GPU memory, we did not present this solution here.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation on saving and loading PyTorch models: [https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml](https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the images are transformed with such specific values for the MNIST
    dataset: [https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
