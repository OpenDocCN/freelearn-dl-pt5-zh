["```py\n# TextCNN Components\n# 1D Convolution\nconv1d = mx.gluon.nn.Conv1D(3, 100, activation='relu')\n# Max-Over-Time Pooling\nmax_over_time_pooling = mx.gluon.nn.GlobalMaxPool1D()\n```", "```py\n# RNNs MXNet Implementation Example\nclass RNNModel(mx.gluon.Block):\n    \"\"\"\n    A basic RNN Model\n    \"\"\"\n    def __init__(self, num_hidden, num_layers, embed_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = mx.gluon.rnn.RNN(\n            num_hidden,\n            num_layers,\n            input_size=embed_size)\n    def forward(self, inputs, hidden):\n        output, hidden = self.rnn(inputs, hidden)\n        return output, hidden\n```", "```py\n# RNN with 3 hidden cells, 1 layer and expecting inputs with 20 embeddings\nrnn = RNNModel(3, 1, 20)\n rnn.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n```", "```py\nrnn_hidden = hidden_initial\noutputs = []\nfor index in range(3):\n    rnn_output, rnn_hidden = rnn(inputs[index], rnn_hidden)\n    outputs.append(rnn_output)\n```", "```py\n# LSTMs MXNet Implementation Example\nclass LSTMModel(mx.gluon.Block):\n    \"\"\"\n    A basic LSTM Model\n    \"\"\"\n    def __init__(self, num_hidden, num_layers, embed_size, **kwargs):\n        super(LSTMModel, self).__init__(**kwargs)\n        self.lstm = mx.gluon.rnn.LSTM(\n            num_hidden,\n            num_layers,\n            input_size=embed_size)\n    def forward(self, inputs, hidden):\n        output, hidden = self.lstm(inputs, hidden)\n        return output, hidden\n```", "```py\n# LSTM with 3 hidden cells, 1 layer and expecting inputs with 20 embeddings\nlstm = LSTMModel(3, 1, 20)\n lstm.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n```", "```py\nlstm_hidden = [hidden_initial, state_initial]\n outputs = []\nfor index in range(3):\n    lstm_output, lstm_hidden = lstm(inputs[index], lstm_hidden)\n    outputs.append(lstm_output)\n```", "```py\n# Transformers MXNet Implementation Example\n# Transformer with 6 layers (encoder and decoder), 2 parallel heads, and expecting inputs with 20 embeddings\ntransformer_encoder, transformer_decoder, _ = nlp.model.transformer.get_transformer_encoder_decoder(\n    num_layers=6,\n    num_heads=2,\n    units=20)\ntransformer_encoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n transformer_decoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n```", "```py\nencoded_inputs, _ = transformer_encoder(inputs[0])\n```", "```py\nreduced_number_headlines = int(0.05 * number_headlines)\n print(reduced_number_headlines)\n62209\n```", "```py\nCluster 0 : ['win', 'world', 'cup', 'final', 'lead', 'set', 'hit', 'face', 'record', 'open', 'year', 'miss', 'test', 'new', 'day']\nCluster 1 : ['govt', 'iraq', 'urg', 'nsw', 'polic', 'continu', 'say', 'australia', 'vic', 'consid', 'qld', 'iraqi', 'forc', 'secur', 'sar']\nCluster 2 : ['plan', 'council', 'new', 'govt', 'fund', 'say', 'boost', 'group', 'water', 'concern', 'health', 'report', 'claim', 'seek', 'warn']\nCluster 3 : ['polic', 'man', 'kill', 'charg', 'court', 'murder', 'crash', 'death', 'attack', 'woman', 'face', 'arrest', 'probe', 'car', 'dead']\n```", "```py\nimport random\nrandom_index = random.randint(0, len(headlines_full)) random_headline = headlines_full[\"headline_text\"][random_index] print(random_index, random_headline)\n```", "```py\n771004 waratahs count cost with loss to cheetahs\n```", "```py\n    def predict(cluster_km, headline):    \"\"\"    This function predicts the cluster  of a headline via K-Means\n        \"\"\"\n        # Cleaning\n        headline_clean = clean_text(headline)\n        headline_pre = process_words(headline_clean)\n        # Embeddings\n        bag_of_words_list = headline_pre.split()\n        number_of_words = len(bag_of_words_list)\n        # Process 1st word (to be able to concatenate)\n        word_embeddings_array = w2v[bag_of_words_list[0]].reshape(1, embedding_features)\n        # To manage headlines with just 1 meaningful word\n        word_index = -1\n        for word_index, word in enumerate(bag_of_words_list[1:]):\n            word_embeddings = w2v[word].reshape(1, embedding_features)\n            word_embeddings_array = mx.nd.concat(word_embeddings_array,\n     word_embeddings, dim=0)\n        assert(number_of_words == word_index + 2)\n        average_embedding_headline_pre = mx.nd.mean(word_embeddings_array, axis=0).reshape(1, embedding_features)\n        # Clustering\n        selected_cluster = cluster_km.predict(average_embedding_headline_pre.asnumpy())\n        return selected_cluster\n    ```", "```py\n    predicted_cluster = predict(cluster_km, random_headline)\n    print(predicted_cluster)\n    ```", "```py\n    [0]\n    ```", "```py\ndef process_words_basic(\n    text,\n    lemmatizer = lemmatizer):\n    words = nltk.tokenize.word_tokenize(text)\n    filtered_words_post = []\n    for word in words:\n        if word.isalpha():\n            filtered_words_post.append(lemmatizer.lemmatize(word))\n    return filtered_words_post\n```", "```py\nbert_model, vocab = nlp.model.get_model(\n    'bert_12_768_12',\n    dataset_name='book_corpus_wiki_en_uncased',\n    use_classifier=False,\n    use_decoder=False,\n    ctx=ctx)\n```", "```py\nI went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n```", "```py\n# Formatting single input as expected for the network\nseq_output, _ = process_dataset_sample(test_dataset[0][0])\n seq_output_reshaped = mx.nd.array(seq_output, ctx=ctx).expand_dims(axis=0)\n```", "```py\n# Retrieve best model from training\ntext_cnn.load_parameters(model_file_name)\nreview_sentiment = text_cnn(seq_output_rehaped)\n# We can omit sigmoid processing, outputs of the network\n# with positive values are positive reviews\nif review_sentiment >= 0:\n    print(review_sentiment, \"The review is positive\")\nelse:\n    print(review_sentiment, \"The review is negative\")\n```", "```py\n[[2.5862172]]\n <NDArray 1x1 @gpu(0)> The review is positive\n```", "```py\nFinal Test Accuracy: 0.87724\n```", "```py\n[[15.462966]]\n <NDArray 1x1 @gpu(0)> The review is positive\n```", "```py\nFinal Test Accuracy: 0.90848\n```", "```py\n# IWSLT2015 Dataset (Train, Validation and Test)\n# Dataset Parameters\nsrc_lang, tgt_lang = \"vi\", \"en\"\nsrc_max_len, tgt_max_len = 50, 50\niwslt_train_text = nlp.data.IWSLT2015(\"train\",\n                                      src_lang=src_lang,\n                                      tgt_lang=tgt_lang)\niwslt_val_text   = nlp.data.IWSLT2015(\"val\",\n                                      src_lang=src_lang,\n                                      tgt_lang=tgt_lang)\niwslt_test_text  = nlp.data.IWSLT2015(\"test\",\n                                      src_lang=src_lang,\n                                      tgt_lang=tgt_lang)\niwslt_src_vocab = iwslt_train_text.src_vocab\niwslt_tgt_vocab = iwslt_train_text.tgt_vocab\n```", "```py\nLength of train set: 133166\nLength of val set  : 1553\nLength of test set : 1268\n```", "```py\n# Bucket scheme\nbucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n```", "```py\nBest model test Loss=2.3807, test ppl=10.8130, test bleu=23.15\n```", "```py\nprint(\"Qualitative Evaluation: Translating from Vietnamese to English\")\nexpected_tgt_seq = \"I like to read books.\"\n print(\"Expected translation:\")\n print(expected_tgt_seq)\n# From Google Translate\nsrc_seq = \"Tôi thích đc sách k thut.\"\n print(\"In Vietnamese (from Google Translate):\")\n print(src_seq)\ntranslation_out = nmt.utils.translate(\n    gnmt_translator,\n    src_seq,\n    iwslt_src_vocab,\n    iwslt_tgt_vocab,\n    ctx)\nprint(\"The English translation is:\")\n print(\" \".join(translation_out[0]))\n```", "```py\nQualitative Evaluation: Translating from Vietnamese to English\nExpected translation:\n I like to read books.\n In Vietnamese (from Google Translate):\n Tôi thích đc sách k thut.\n The English translation is:\n I like to read books .\n```", "```py\nBest model test Loss=2.1171, test ppl=8.3067, test bleu=24.16\n```", "```py\nQualitative Evaluation: Translating from Vietnamese to English\nExpected translation:\n I like to read books.\n In Vietnamese (from Google Translate):\n Tôi thích đc sách k thut.\n The English translation is:\n I like to read books .\n```"]