["```py\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(spacyr)\nlibrary(textmineR)\n```", "```py\ntwenty_newsgroups <- read_csv(\"http://ssc.wisc.edu/~ahanna/20_newsgroups.csv\")\n```", "```py\ntwenty_newsgroups[1,]\n```", "```py\nword_tokens <- twenty_newsgroups %>%\n  unnest_tokens(word, text)\n```", "```py\nword_tokens %>%\n  group_by(word) %>%\n  summarize(word_count = n()) %>%\n  top_n(20) %>%\n  ggplot(aes(x=reorder(word, word_count), word_count)) +\n  xlab(\"word\") +\n geom_col() +\n  coord_flip() \n```", "```py\nword_tokens <- word_tokens %>%\n  filter(!word %in% stop_words$word)\n```", "```py\nword_tokens %>%\n  group_by(word) %>%\n  summarize(word_count = n()) %>%\n  top_n(20) %>%\n  ggplot(aes(x=reorder(word, word_count), word_count)) +\n  xlab(\"word\") +\n  geom_col() +\n  coord_flip() \n```", "```py\nword_tokens <- word_tokens %>%\n  filter(str_detect(word, \"^[a-z]+[a-z]$\"))\n```", "```py\nspacy_install()\n\nspacy_initialize(model = \"en_core_web_sm\")\n\nspacy_parse(twenty_newsgroups$text[1], entity = TRUE, lemma = TRUE)\n```", "```py\ntcm <- CreateTcm(doc_vec = twenty_newsgroups$text,\n                 skipgram_window = 10,\n                 verbose = FALSE,\n                 cpus = 2)\n```", "```py\nembeddings <- FitLdaModel(dtm = tcm,\n                          k = 20,\n                          iterations = 500,\n                          burnin = 200,\n                          calc_coherence = TRUE)\n```", "```py\nembeddings$top_terms <- GetTopTerms(phi = embeddings$phi,\n                                    M = 5)\n```", "```py\nembeddings$summary <- data.frame(topic = rownames(embeddings$phi),\n                                 coherence = round(embeddings$coherence, 3),\n                                 prevalence = round(colSums(embeddings$theta), 2),\n                                 top_terms = apply(embeddings$top_terms, 2, function(x){\n                                   paste(x, collapse = \", \")\n                                 }),\n                                 stringsAsFactors = FALSE)\n```", "```py\nembeddings$summary[order(embeddings$summary$coherence, decreasing = TRUE),][1:5,]\n```", "```py\ntwenty_newsgroups$text[400]\n```", "```py\nsentences <- tibble(text = twenty_newsgroups$text[400]) %>%\n  unnest_tokens(sentence, text, token = \"sentences\") %>%\n```", "```py\n  mutate(id = row_number()) %>%\n  select(id, sentence)\n```", "```py\nwords <- sentences %>%\n  unnest_tokens(word, sentence)\n```", "```py\narticle_summary <- textrank_sentences(data = sentences, terminology = words)\n```", "```py\narticle_summary[[\"sentences\"]] %>%\n  arrange(desc(textrank)) %>% \n  top_n(1) %>%\n  pull(sentence)\n```", "```py\nlibrary(tm)\nlibrary(deepnet)\n```", "```py\ncorpus <- Corpus(VectorSource(twenty_newsgroups$text))\n\ncorpus <- tm_map(corpus, content_transformer(tolower))\ncorpus <- tm_map(corpus, removeNumbers)\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, removeWords, c(\"the\", \"and\", stopwords(\"english\")))\ncorpus <- tm_map(corpus, stripWhitespace)\n\nnews_dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))\nnews_dtm <- removeSparseTerms(news_dtm, 0.95)\n```", "```py\nsplit_ratio <- floor(0.75 * nrow(twenty_newsgroups))\n\nset.seed(614)\ntrain_index <- sample(seq_len(nrow(twenty_newsgroups)), size = split_ratio)\n\ntrain_x <- news_dtm[train_index,]\ntrain_y <- twenty_newsgroups$target[train_index]\ntest_x <- news_dtm[-train_index,]\ntest_y <- twenty_newsgroups$target[-train_index]\n```", "```py\nrbm <- rbm.train(x = as.matrix(train_x), hidden = 20, numepochs = 100)\n```", "```py\ntest_latent_features <- rbm.up(rbm, as.matrix(test_x))\n```", "```py\ngibbs<-function (n, rho) \n{\n  mat <- matrix(ncol = 2, nrow = n)\n  x <- 0\n  y <- 0\n  mat[1, ] <- c(x, y)\n  for (i in 2:n) {\n    x <- rnorm(1, rho * y, sqrt(1 - rho^2))\n    y <- rnorm(1, rho * x, sqrt(1 - rho^2))\n    mat[i, ] <- c(x, y)\n  }\n  mat\n}\n```", "```py\ngibbs(10,0.75)\n```", "```py\ngibbs(10,0.03)\n```", "```py\nspam_vs_ham <- read.csv(\"spam.csv\")\n```", "```py\ny <- if_else(spam_vs_ham$v1 == \"spam\", 1, 0)\nx <- spam_vs_ham$v2 %>% \n  str_replace_all(\"[^a-zA-Z0-9/:-_]|\\r|\\n|\\t\", \" \") %>% \n  str_replace_all(\"\\b[a-zA-Z0-9/:-]{1,2}\\b\", \" \") %>%\n  str_trim(\"both\") %>%\n  str_squish()\n```", "```py\ncorpus <- Corpus(VectorSource(x))\ndtm <- DocumentTermMatrix(corpus)\n```", "```py\nsplit_ratio <- floor(0.75 * nrow(dtm))\n\nset.seed(614)\ntrain_index <- sample(seq_len(nrow(dtm)), size = split_ratio)\n\ntrain_x <- dtm[train_index,]\ntrain_y <- y[train_index]\ntest_x <- dtm[-train_index,]\ntest_y <- y[-train_index]\n```", "```py\nrbm3 <- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 3,numepochs = 5)\nrbm5 <- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 5,numepochs = 5)\nrbm1 <- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 1,numepochs = 5)\n```", "```py\nrbm5$e[1:10]\nrbm3$e[1:10]\nrbm1$e[1:10]\n```", "```py\ntrain_latent_features <- rbm.up(rbm1, as.matrix(train_x))\ntest_latent_features <- rbm.up(rbm1, as.matrix(test_x))\n```", "```py\ndbn <- dbn.dnn.train(x = as.matrix(train_x), y = train_y, hidden = c(100,50,10), cd = 1, numepochs = 5)\n```", "```py\npredictions <- nn.predict(dbn, as.matrix(test_x))\n```", "```py\npred_class <- if_else(predictions > 0.3, 1, 0)\ntable(test_y,pred_class)\n```"]