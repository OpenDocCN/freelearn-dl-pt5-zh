- en: Final Remarks on the Future of Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: We have been through a journey together, and if you have read this far you deserve
    to treat yourself with a nice meal. What you have accomplished deserves recognition.
    Tell your friends, share what you have learned, and remember to always keep on
    learning. Deep learning is a rapidly changing field; you cannot sit still. This
    concluding chapter will briefly present to you some of the new exciting topics
    and opportunities in deep learning. If you want to continue your learning, we
    will recommend other helpful resources from Packt that can help you move forward
    in this field. At the end of this chapter, you will know where to go from here
    after having learned the basics of deep learning; you will know what other resources
    Packt offers for you to continue your training in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking for advanced topics in deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning with more resources from Packt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking for advanced topics in deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future of deep learning is hard to predict at the moment; things are changing
    rapidly. However, I believe that if you invest your time in the present advanced
    topics in deep learning, you might see these areas developing prosperously in
    the near future.
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections discuss some of these advanced topics that have the
    potential of flourishing and being disruptive in our area.
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**DRL**) is an area that has gained a lot
    of attention recently given that deep convolutional networks, and other types
    of deep networks, have offered solutions to problems that were difficult to solve
    in the past. Many of the uses of DRL are in areas where we do not have the luxuryof
    having data on all possible conceivable cases, such as space exploration, playing
    video games, or self-driving cars.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on the latter example. If we were using traditional supervised
    learning to make a self-driving car that can take us from point A to point B without
    crashing, we would not only want to have examples of the positive class with events
    of successful journeys, but we would also needexamples of the negative class with
    bad events such as crashes and terrible driving. Think about this: we would need
    to crash as many cars as we have successful events to keep the dataset balanced.
    This is not acceptable; however, reinforcement learning comes to the rescue.'
  prefs: []
  type: TYPE_NORMAL
- en: DRL aims to **reward** good driving aspects; the models learn that there is
    a reward to be gained, so we don't need negative examples. In contrast, traditional
    learning would need to crash cars in order to **penalize** bad outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: When you use DRL to learn using a simulator, you can get AI that can beat pilots
    on simulated flights ([https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/](https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/)),
    or you can get AI that can win on a video game simulator. The gaming world is
    a perfect test scenario for DRL. Let's say that you want to make a DRL model to
    play the famous game *Space Invaders*, shown in *Figure 15.1*; you can make a
    model that rewards destroying space invaders.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/192d5354-6c0a-4be2-b3c5-248a2160a925.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Space invaders video game simulator
  prefs: []
  type: TYPE_NORMAL
- en: If you make a traditional model to teach the user to **not die**, for example,
    then you will still lose because you will eventually be invaded from space. So,
    the best strategy to prevent invasion is both not dying and destroying space invaders.
    In other words, you reward the actions that lead to survival, which are to destroy
    the space invaders quickly while avoiding being killed by their bombs.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, a new DRL research tool was released, called **Dopamine** (Castro,
    P. S., et al. 2018). Dopamine ([https://github.com/google/dopamine](https://github.com/google/dopamine))
    is meant for fast prototyping of reinforcement learning algorithms. Back in [Chapter
    2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup and Introduction to Deep
    Learning Frameworks*, we asked you to install Dopamine for this moment. We simply
    want to give you an idea of how easy Dopamine is so that you can go ahead and
    experiment with it if you are interested. In the following lines of code, we will
    simply load a pre-trained model (agent) and let it play the game.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will make sure the library is installed, then load the pre-trained agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The sample trained agent, which in this case is called `rainbow`, is provided
    by the authors of Dopamine, but you can also train your own if you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to make the agent run (that is, decide to take actions based
    on the rewards) for a number of steps, say `1024`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This piece of code may take a while to run. Internally, it connects to PyGame,
    which is a resource of game simulators for the Python community. It makes several
    decisions and avoids space invasion (and dying). As shown in *Figure 15.2*, the
    model describes the cumulative rewards obtained across time steps and the estimated
    probability of return for every action taken, such as stop, move left, move right,
    and shoot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06b1145f-dc49-487e-8d6a-2f1471fb009c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2 – Left: Calculated rewards of the model across time steps. Right:
    Estimated probability of returns for every action'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the interesting things about this is that you can visualize the agent
    at any of the time steps (frames) and see what the agent was doing at that specific
    time step using the plot in *Figure 15.2* as a reference to decide which time
    step to visualize. Let''s say that you want to visualize steps 540 or 550; you
    can do that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You substitute `<path to current directory>` with the path to your current working
    directory. This is because we need an absolute path, otherwise we could have used
    a relative path with `./` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this, it is self-evident that all frames are saved as images in the `./agent_viz/SpaceInvaders/rainbow/images/`
    directory. You can display them individually or even make a video. The preceding
    code produces the images shown in *Figure 15.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a661df7f-d0ab-4ab6-a4d6-7af0359e5025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3 – Left: step 540\. Right: step 550'
  prefs: []
  type: TYPE_NORMAL
- en: Dopamine is as simple as that. We hope that you will be inspired by reinforcement
    learning and investigate further.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yann LeCun, one of the 2018 ACM Turing Award winners, said at the AAAI conference
    in 2020: *"the future is self-supervised."* He was implying that this area is
    exciting and has a lot of potential.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervision is a relatively new term used to move away from the term *unsupervision*.
    The term "unsupervised learning" might give the impression that there is no supervision
    when, in fact, unsupervised learning algorithms and models typically use more
    supervisory data than supervised models. Take, for example, the classification
    of MNIST data. It uses 10 labels as supervisory signals. However, in an autoencoder
    whose purpose is perfect reconstruction, every single pixel is a supervisory signal,
    so there are 784 supervisory signals from a 28 x 28 image, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervision is also used to mean models that combine some of the stages
    of unsupervised and supervised learning. For example, if we pipeline a model that
    learns representations unsupervised, then we can attach a model downstream that
    will learn to classify something supervised.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the recent advances in deep learning have been in self-supervision.
    It will be a good investment of your time if you try to learn more about self-supervised
    learning algorithms and models.
  prefs: []
  type: TYPE_NORMAL
- en: System 2 algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The famous economist Daniel Kahneman made popular the theory of dual process
    with his book *Thinking Fast and Slow* (Kahneman, D. 2011). The main idea is that
    there are highly complex tasks that we, as humans, are good at developing relatively
    fast and often without thinking too much; for example, drinking water, eating
    food, or looking at an object and recognizing it. These processes are done by
    *System 1*.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are tasks that are not quite simple for the human mind, tasks
    that require our fully devoted attention, such as driving on an unfamiliar road,
    looking at a strange object that does not belong within the assumed context, or
    understanding an abstract painting. These processes are done by *System 2*. Another
    winner of the 2018 ACM Turing Award, Yoshua Bengio, has made the remark that deep
    learning has been very good at System 1 tasks, meaning that existing models can
    recognize objects and perform highly complex tasks relatively easily. However,
    deep learning has not made much progress on System 2 tasks. That is, the future
    of deep learning will be in solving those tasks that are very complex for human
    beings, which will probably involve combining different models across different
    domains with different learning types. Capsule neural networks might be a good
    alternative solution to System 2 tasks (Sabour, S., et al. 2017*)*.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, System 2 algorithms will probably be the future of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at resources available from Packt that can help in further studying
    these ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Learning with more resources from Packt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following lists of books is not meant to be exhaustive, but a starting point
    for your next endeavor. These titles have come out at a great time when there
    is a lot of interest in the field. Regardless of your choice, you will not be
    disappointed.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Hands-On - Second Edition*, by Maxim Lapan, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Reinforcement Learning Workshop*, by Alessandro Palmas *et al.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Reinforcement Learning for Games*, by Micheal Lanham, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyTorch 1.x Reinforcement Learning Cookbook*, by Yuxi Liu, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Reinforcement Learning*, by Sudharsan Ravichandiran, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reinforcement Learning Algorithms with Python*, by Andrea Lonza, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The Unsupervised Learning Workshop*, by Aaron Jones *et. al.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Applied Unsupervised Learning with Python*, by Benjamin Johnston *et. al.*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Unsupervised Learning with Python*, by Giuseppe Bonaccorso, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This final chapter briefly covered new exciting topics and opportunities in
    deep learning. We discussed reinforcement learning, self-supervised algorithms,
    and System 2 algorithms. We also recommended some further resources from Packt,
    hoping that you will want to continue your learning and move forward in this field.
    At this point, you should know where to go from here, and be inspired by the future
    of deep learning. You should be knowledgeable of other recommended books in the
    area to continue with your learning journey.
  prefs: []
  type: TYPE_NORMAL
- en: You are the future of deep learning, and the future is today. Go ahead and make
    things happen.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. (2018).
    Dopamine: A research framework for deep reinforcement learning. arXiv preprint
    arXiv:1812.06110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahneman, D. (2011). *Thinking, Fast and Slow*. *Macmillan*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules.
    In *Advances in neural information processing systems* (pp. 3856-3866).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
