- en: Final Remarks on the Future of Deep Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: We have been through a journey together, and if you have read this far you deserve
    to treat yourself with a nice meal. What you have accomplished deserves recognition.
    Tell your friends, share what you have learned, and remember to always keep on
    learning. Deep learning is a rapidly changing field; you cannot sit still. This
    concluding chapter will briefly present to you some of the new exciting topics
    and opportunities in deep learning. If you want to continue your learning, we
    will recommend other helpful resources from Packt that can help you move forward
    in this field. At the end of this chapter, you will know where to go from here
    after having learned the basics of deep learning; you will know what other resources
    Packt offers for you to continue your training in deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized into the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Looking for advanced topics in deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning with more resources from Packt
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking for advanced topics in deep learning
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future of deep learning is hard to predict at the moment; things are changing
    rapidly. However, I believe that if you invest your time in the present advanced
    topics in deep learning, you might see these areas developing prosperously in
    the near future.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The following sub-sections discuss some of these advanced topics that have the
    potential of flourishing and being disruptive in our area.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**DRL**) is an area that has gained a lot
    of attention recently given that deep convolutional networks, and other types
    of deep networks, have offered solutions to problems that were difficult to solve
    in the past. Many of the uses of DRL are in areas where we do not have the luxuryof
    having data on all possible conceivable cases, such as space exploration, playing
    video games, or self-driving cars.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on the latter example. If we were using traditional supervised
    learning to make a self-driving car that can take us from point A to point B without
    crashing, we would not only want to have examples of the positive class with events
    of successful journeys, but we would also needexamples of the negative class with
    bad events such as crashes and terrible driving. Think about this: we would need
    to crash as many cars as we have successful events to keep the dataset balanced.
    This is not acceptable; however, reinforcement learning comes to the rescue.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: DRL aims to **reward** good driving aspects; the models learn that there is
    a reward to be gained, so we don't need negative examples. In contrast, traditional
    learning would need to crash cars in order to **penalize** bad outcomes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: When you use DRL to learn using a simulator, you can get AI that can beat pilots
    on simulated flights ([https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/](https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/)),
    or you can get AI that can win on a video game simulator. The gaming world is
    a perfect test scenario for DRL. Let's say that you want to make a DRL model to
    play the famous game *Space Invaders*, shown in *Figure 15.1*; you can make a
    model that rewards destroying space invaders.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用深度强化学习（DRL）通过模拟器进行学习时，你可以获得能够在模拟飞行中击败飞行员的AI（[https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/](https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/)），或者你可以获得能够在视频游戏模拟器中获胜的AI。游戏世界是DRL的一个完美测试场景。假设你想要制作一个DRL模型来玩著名的游戏*太空侵略者*，如*图15.1*所示；你可以创建一个奖励摧毁太空侵略者的模型。
- en: '![](img/192d5354-6c0a-4be2-b3c5-248a2160a925.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/192d5354-6c0a-4be2-b3c5-248a2160a925.png)'
- en: Figure 15.1 – Space invaders video game simulator
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 太空侵略者视频游戏模拟器
- en: If you make a traditional model to teach the user to **not die**, for example,
    then you will still lose because you will eventually be invaded from space. So,
    the best strategy to prevent invasion is both not dying and destroying space invaders.
    In other words, you reward the actions that lead to survival, which are to destroy
    the space invaders quickly while avoiding being killed by their bombs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建一个传统的模型来教用户**不死**，例如，你最终还是会失败，因为你最终会被太空入侵。因此，防止入侵的最佳策略是既不死又摧毁太空入侵者。换句话说，你奖励那些导致生存的行动，也就是在避免被它们的炸弹击中的同时迅速摧毁太空入侵者。
- en: In 2018, a new DRL research tool was released, called **Dopamine** (Castro,
    P. S., et al. 2018). Dopamine ([https://github.com/google/dopamine](https://github.com/google/dopamine))
    is meant for fast prototyping of reinforcement learning algorithms. Back in [Chapter
    2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup and Introduction to Deep
    Learning Frameworks*, we asked you to install Dopamine for this moment. We simply
    want to give you an idea of how easy Dopamine is so that you can go ahead and
    experiment with it if you are interested. In the following lines of code, we will
    simply load a pre-trained model (agent) and let it play the game.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，发布了一个新的DRL研究工具，名为**Dopamine**（Castro, P. S., 等，2018）。Dopamine（[https://github.com/google/dopamine](https://github.com/google/dopamine)）用于快速原型开发强化学习算法。在[第二章](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml)中，*深度学习框架的设置与介绍*，我们要求你为此时安装Dopamine。我们只是想给你一个Dopamine的使用概念，让你可以在感兴趣时继续进行实验。在接下来的几行代码中，我们将简单地加载一个预训练模型（智能体），并让它玩游戏。
- en: 'This will make sure the library is installed, then load the pre-trained agent:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保库已安装，然后加载预训练的智能体：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sample trained agent, which in this case is called `rainbow`, is provided
    by the authors of Dopamine, but you can also train your own if you want.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经过训练的智能体在这里被称为`rainbow`，是Dopamine的作者提供的，但如果你愿意，也可以训练自己的智能体。
- en: 'The next step is to make the agent run (that is, decide to take actions based
    on the rewards) for a number of steps, say `1024`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是让智能体运行（即，根据奖励决定采取的行动）若干步，例如`1024`步：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This piece of code may take a while to run. Internally, it connects to PyGame,
    which is a resource of game simulators for the Python community. It makes several
    decisions and avoids space invasion (and dying). As shown in *Figure 15.2*, the
    model describes the cumulative rewards obtained across time steps and the estimated
    probability of return for every action taken, such as stop, move left, move right,
    and shoot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可能需要一段时间才能运行。内部，它连接到PyGame，这是Python社区的一个游戏模拟器资源。它做出几个决策，并避免太空入侵（以及死亡）。如*图15.2*所示，模型描述了在时间步长中的累积奖励，以及每个动作的回报估计概率，例如停止、向左移动、向右移动和射击：
- en: '![](img/06b1145f-dc49-487e-8d6a-2f1471fb009c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06b1145f-dc49-487e-8d6a-2f1471fb009c.png)'
- en: 'Figure 15.2 – Left: Calculated rewards of the model across time steps. Right:
    Estimated probability of returns for every action'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 – 左：模型在时间步长中的计算奖励。右：每个行动的回报估计概率
- en: 'One of the interesting things about this is that you can visualize the agent
    at any of the time steps (frames) and see what the agent was doing at that specific
    time step using the plot in *Figure 15.2* as a reference to decide which time
    step to visualize. Let''s say that you want to visualize steps 540 or 550; you
    can do that as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You substitute `<path to current directory>` with the path to your current working
    directory. This is because we need an absolute path, otherwise we could have used
    a relative path with `./` instead.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'From this, it is self-evident that all frames are saved as images in the `./agent_viz/SpaceInvaders/rainbow/images/`
    directory. You can display them individually or even make a video. The preceding
    code produces the images shown in *Figure 15.3*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a661df7f-d0ab-4ab6-a4d6-7af0359e5025.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3 – Left: step 540\. Right: step 550'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Dopamine is as simple as that. We hope that you will be inspired by reinforcement
    learning and investigate further.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yann LeCun, one of the 2018 ACM Turing Award winners, said at the AAAI conference
    in 2020: *"the future is self-supervised."* He was implying that this area is
    exciting and has a lot of potential.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervision is a relatively new term used to move away from the term *unsupervision*.
    The term "unsupervised learning" might give the impression that there is no supervision
    when, in fact, unsupervised learning algorithms and models typically use more
    supervisory data than supervised models. Take, for example, the classification
    of MNIST data. It uses 10 labels as supervisory signals. However, in an autoencoder
    whose purpose is perfect reconstruction, every single pixel is a supervisory signal,
    so there are 784 supervisory signals from a 28 x 28 image, for example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervision is also used to mean models that combine some of the stages
    of unsupervised and supervised learning. For example, if we pipeline a model that
    learns representations unsupervised, then we can attach a model downstream that
    will learn to classify something supervised.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Many of the recent advances in deep learning have been in self-supervision.
    It will be a good investment of your time if you try to learn more about self-supervised
    learning algorithms and models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: System 2 algorithms
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The famous economist Daniel Kahneman made popular the theory of dual process
    with his book *Thinking Fast and Slow* (Kahneman, D. 2011). The main idea is that
    there are highly complex tasks that we, as humans, are good at developing relatively
    fast and often without thinking too much; for example, drinking water, eating
    food, or looking at an object and recognizing it. These processes are done by
    *System 1*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: However, there are tasks that are not quite simple for the human mind, tasks
    that require our fully devoted attention, such as driving on an unfamiliar road,
    looking at a strange object that does not belong within the assumed context, or
    understanding an abstract painting. These processes are done by *System 2*. Another
    winner of the 2018 ACM Turing Award, Yoshua Bengio, has made the remark that deep
    learning has been very good at System 1 tasks, meaning that existing models can
    recognize objects and perform highly complex tasks relatively easily. However,
    deep learning has not made much progress on System 2 tasks. That is, the future
    of deep learning will be in solving those tasks that are very complex for human
    beings, which will probably involve combining different models across different
    domains with different learning types. Capsule neural networks might be a good
    alternative solution to System 2 tasks (Sabour, S., et al. 2017*)*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, System 2 algorithms will probably be the future of deep learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at resources available from Packt that can help in further studying
    these ideas.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Learning with more resources from Packt
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following lists of books is not meant to be exhaustive, but a starting point
    for your next endeavor. These titles have come out at a great time when there
    is a lot of interest in the field. Regardless of your choice, you will not be
    disappointed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Hands-On - Second Edition*, by Maxim Lapan, 2020.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Reinforcement Learning Workshop*, by Alessandro Palmas *et al.*, 2020.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Reinforcement Learning for Games*, by Micheal Lanham, 2020.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyTorch 1.x Reinforcement Learning Cookbook*, by Yuxi Liu, 2019.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Reinforcement Learning*, by Sudharsan Ravichandiran, 2019.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reinforcement Learning Algorithms with Python*, by Andrea Lonza, 2019.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-supervised learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The Unsupervised Learning Workshop*, by Aaron Jones *et. al.*, 2020.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Applied Unsupervised Learning with Python*, by Benjamin Johnston *et. al.*,
    2019.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Unsupervised Learning with Python*, by Giuseppe Bonaccorso, 2019.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This final chapter briefly covered new exciting topics and opportunities in
    deep learning. We discussed reinforcement learning, self-supervised algorithms,
    and System 2 algorithms. We also recommended some further resources from Packt,
    hoping that you will want to continue your learning and move forward in this field.
    At this point, you should know where to go from here, and be inspired by the future
    of deep learning. You should be knowledgeable of other recommended books in the
    area to continue with your learning journey.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: You are the future of deep learning, and the future is today. Go ahead and make
    things happen.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. (2018).
    Dopamine: A research framework for deep reinforcement learning. arXiv preprint
    arXiv:1812.06110.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahneman, D. (2011). *Thinking, Fast and Slow*. *Macmillan*.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman, D. (2011). *思考，快与慢*。*麦克米兰出版社*。
- en: Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules.
    In *Advances in neural information processing systems* (pp. 3856-3866).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour, S., Frosst, N., 和 Hinton, G. E. (2017). 胶囊之间的动态路由。载于 *神经信息处理系统的进展*（第3856-3866页）。
