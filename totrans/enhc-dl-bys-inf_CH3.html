<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch008.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-3-fundamentals-of-deep-learning" class="level1 chapterHead" data-number="8">
<h1 class="chapterHead" data-number="8"><span class="titlemark">Chapter 3</span><br/>
<span id="x1-350003"></span>Fundamentals of Deep Learning</h1>
<p>Throughout the book, when studying how to apply Bayesian methods and extensions to neural networks, we will encounter different neural network architectures and applications. This chapter will provide an introduction to common architecture types, thus laying the foundation for introducing Bayesian extensions to these architectures later on. We will also review some of the limitations of such common neural network architectures, in particular their tendency to produce overconfident outputs and their susceptibility to adversarial manipulation of inputs. By the end of this chapter, you should have a good understanding of deep neural network basics and know how to implement the most common neural network architecture types in code. This will help you follow the code examples found in later sections.</p>
<p>The content will be covered in the following sections:</p>
<ul>
<li><p>Introducing the multi-layer perceptron</p></li>
<li><p>Reviewing neural network architectures</p></li>
<li><p>Understanding the problem with typical neural networks</p></li>
</ul>
<p><span id="x1-35001r59"></span></p>
<section id="technical-requirements-1" class="level2 sectionHead" data-number="8.1">
<h2 class="sectionHead" data-number="8.1" id="sigil_toc_id_32"><span class="titlemark">3.1 </span> <span id="x1-360001"></span>Technical requirements</h2>
<p>To complete the practical tasks in this chapter, you will need a Python 3.8 environment with the <code>pandas</code> and <code>scikit-learn</code> stack and the following additional Python packages installed:</p>
<ul>
<li><p>TensorFlow 2.0</p></li>
<li><p>Matplotlib plotting library</p></li>
</ul>
<p>All of the code for this book can be found on the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-36003r61"></span></p>
</section>
<section id="introducing-the-multi-layer-perceptron" class="level2 sectionHead" data-number="8.2">
<h2 class="sectionHead" data-number="8.2" id="sigil_toc_id_33"><span class="titlemark">3.2 </span> <span id="x1-370002"></span>Introducing the multi-layer perceptron</h2>
<p>Deep neural networks are at the core of the <span id="dx1-37001"></span>deep learning revolution. The aim of this section is to introduce basic concepts and building blocks for deep neural networks. To get started, we will review the components of the <strong>multi-layer perceptron</strong> (<strong>MLP</strong>) and implement it using the <code>TensorFlow</code> framework. This will serve as the foundation for other code examples in the book. If you are already familiar with neural networks and know how to implement them in code, feel free to jump ahead to the <em>Understanding</em> <em>the problem with typical NNs</em> section, where we cover the limitations of deep neural networks. This chapter focuses on architectural building blocks and principles and does not cover learning rules and gradients. If you require additional background information for those topics, we recommend Sebastian Raschka’s excellent <em>Python Machine Learning</em> book from Packt Publishing (in particular, <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian</em> <em>Inference</em></a>).</p>
<p>The MLP is a feed-forward, fully connected neural network. Feed-forward means that the information in an MLP is only passed in one direction, from the input to the output layers; there are no backward connections. Fully connected means that each neuron is connected to all the neurons in the previous layer. To understand these concepts a bit better, let’s <span id="dx1-37003"></span>have a look at <em>Figure</em>  <a href="#x1-37007r1">3.1</a>, which gives a diagrammatic overview of an MLP. In this example, the MLP has an<span id="dx1-37004"></span> <strong>input layer</strong> with three neurons (shown in red), two <strong>hidden layers</strong> <span id="dx1-37005"></span>with four neurons each (shown in blue), and one<span id="dx1-37006"></span> <strong>output layer</strong> with a single output node (shown in green). Imagine, for example, that we wanted to build a model that predicts housing prices in London. In this example, the three input neurons would represent values of three input features of our model, such as distance from the city centre, floor area, and the construction year of the house. As indicated by the black connections in the figure, these input values are then passed to and aggregated by each of the neurons of the first hidden layer. The values of these neurons are then, in turn, passed to and aggregated by the neurons in the second hidden layer and, finally, the output neuron, which will represent the house value predicted by our model.</p>
<div class="IMG---Figure">
<img src="../media/file54.png" alt="PIC"/> <span id="x1-37007r1"></span> <span id="x1-37008"></span></div>
<p class="IMG---Caption">Figure 3.1: Diagram of a multi-layer perceptron 
</p>
<p>What does it mean exactly for a neuron to aggregate values? To understand this a bit better, let’s focus on a single neuron and the operations that it performs on the values that are passed to it. In <em>Figure</em> <a href="#x1-37013r2"><em>3.2</em></a>, we have taken the network shown in <em>Figure</em> <a href="#x1-37007r1"><em>3.1</em></a> (left panel) and zoomed in on the first neuron in the first hidden layer and the neurons that pass values to it (central panel). In the right panel of the figure, we have slightly rearranged the neurons and have named the input neurons <em>x</em><sub><span class="cmr-8">1</span></sub>, <em>x</em><sub><span class="cmr-8">2</span></sub>, and <em>x</em><sub><span class="cmr-8">3</span></sub>. We have also made the connections explicit, by naming the weights associated with them <em>w</em><sub><span class="cmr-8">1</span></sub>, <em>w</em><sub><span class="cmr-8">2</span></sub>, and <em>w</em><sub><span class="cmr-8">3</span></sub>, respectively. From the right panel in the figure, we can see that an artificial neuron performs two essential operations:</p>
<ol>
<li><div id="x1-37010x1">
<p>First, it takes a weighted average over its inputs (indicated by the Σ).</p>
</div></li>
<li><div id="x1-37012x2">
<p>Second, it takes the output of the first step and applies a non-linearity to it (indicated by the <em>σ</em>. Note that this does not indicate the standard deviation, which is what we’ll use <em>σ</em> for throughout most of the book), such as a sigmoid function, for example.</p>
</div></li>
</ol>
<p>The first operation can be exdivssed more formally as <em>z</em> = <span class="cmex-10x-x-109">∑</span> <sub><em>n</em><span class="cmr-8">=1</span></sub><sup><span class="cmr-8">3</span></sup><em>x</em><sub><em>n</em></sub><em>w</em><sub><em>n</em></sub>. The second operation can be exdivssed as <em>a</em> = <em>σ</em>(<em>z</em>) = <img src="../media/file55.jpg" class="frac" data-align="middle" alt=" 1 1+e−-z"/>. The activation value of the neuron <em>a</em> = <em>σ</em>(<em>z</em>) is then passed to the neurons in the second hidden layer, where the same operations are repeated.</p>
<div class="IMG---Figure">
<img src="../media/file56.png" alt="PIC"/> <span id="x1-37013r2"></span> <span id="x1-37014"></span></div>
<p class="IMG---Caption">Figure 3.2: Aggregation and transformation performed by an artificial neuron in a neural network 
</p>
<p>Now that we have reviewed the different parts of an<span id="dx1-37015"></span> MLP model, let’s implement one in TensorFlow. First, we need to import all the necessary functions. These include <code>Sequential</code> to build a feed-forward model such as MLP, <code>Input</code> to build the input layer, and <code>Dense</code> to build a fully-connected layer:</p>
<pre id="fancyvrb6" class="fancyvrb"><span id="x1-37021r1"></span> 
<code><span id="textcolor31"><span>from</span></span><span> </span><span id="textcolor32"><span>tensorflow.keras.models</span></span><span> </span><span id="textcolor33"><span>import</span></span><span> Sequential,</span><span> Input,</span><span> Dense</span></code></pre>
<p>Equipped with these tools, implementing the MLP is a simple matter of chaining <code>Input</code> and <code>Dense</code> layers in the right order and with the right number of neurons:</p>
<pre id="fancyvrb7" class="fancyvrb"><span id="x1-37037r1"></span> 
<code><span>multi_layer_perceptron</span><span> </span><span id="textcolor34"><span>=</span></span><span> Sequential(</span> <span id="x1-37039r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> [</span> <span id="x1-37041r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor35"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> input</span><span class="cmitt-10x-x-109"> layer</span><span class="cmitt-10x-x-109"> with</span><span class="cmitt-10x-x-109"> 3</span><span class="cmitt-10x-x-109"> neurons</span></span> <span id="x1-37043r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> Input(shape</span><span id="textcolor36"><span>=</span></span><span>(</span><span id="textcolor37"><span>3</span></span><span>,))</span> <span id="x1-37045r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor38"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> first</span><span class="cmitt-10x-x-109"> hidden</span><span class="cmitt-10x-x-109"> layer</span><span class="cmitt-10x-x-109"> with</span><span class="cmitt-10x-x-109"> 4</span><span class="cmitt-10x-x-109"> neurons</span></span> <span id="x1-37047r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> Dense(</span><span id="textcolor39"><span>4</span></span><span>,</span><span> activation</span><span id="textcolor40"><span>=</span></span><span id="textcolor41"><span>"sigmoid"</span></span><span>),</span> <span id="x1-37049r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor42"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> second</span><span class="cmitt-10x-x-109"> hidden</span><span class="cmitt-10x-x-109"> layer</span><span class="cmitt-10x-x-109"> with</span><span class="cmitt-10x-x-109"> 4</span><span class="cmitt-10x-x-109"> neurons</span></span> <span id="x1-37051r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> Dense(</span><span id="textcolor43"><span>4</span></span><span>,</span><span> activation</span><span id="textcolor44"><span>=</span></span><span id="textcolor45"><span>"sigmoid"</span></span><span>),</span> <span id="x1-37053r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span id="textcolor46"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> output</span><span class="cmitt-10x-x-109"> layer</span></span> <span id="x1-37055r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> Dense(</span><span id="textcolor47"><span>1</span></span><span>,</span><span> activation</span><span id="textcolor48"><span>=</span></span><span id="textcolor49"><span>"sigmoid"</span></span><span>),</span> <span id="x1-37057r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> ]</span> <span id="x1-37059r12"></span> </code>
<code><span>)</span></code></pre>
<p>The aggregation in terms of weighted averaging is automatically handled by TensorFlow when using the <code>Dense</code> layer object. Furthermore, implementing an activation function becomes a simple matter of passing the name of the desired function to the <code>activation</code> parameter of the Dense layer (<code>sigmoid</code> in the preceding example).</p>
<p>Before we turn to other neural network architectures besides the<span id="dx1-37063"></span> MLP, a side note on the word <em>deep</em>. A neural network is considered deep if it has more than one hidden layer. The MLP shown previously, for example, has two hidden layers and can be considered a deep neural network. It is possible to add more and more hidden layers, creating very deep neural network architectures. Training such deep architectures comes with its own set of challenges and the science (or art) of training such deep architectures is called<span id="dx1-37064"></span> <strong>deep learning</strong> (<strong>DL</strong>).</p>
<p>In the next section, we’ll learn about some of the common deep neural network architectures and in the section thereafter, we will look at the practical challenges that come with them. <span id="x1-37065r62"></span></p>
</section>
<section id="reviewing-neural-network-architectures" class="level2 sectionHead" data-number="8.3">
<h2 class="sectionHead" data-number="8.3" id="sigil_toc_id_34"><span class="titlemark">3.3 </span> <span id="x1-380003"></span>Reviewing neural network architectures</h2>
<p>In the previous section, we saw how to implement a fully-connected network in the form of an MLP. While such networks were very popular in the early days of deep learning, over the years, machine learning researchers have developed more sophisticated architectures that work more successfully by including domain-specific knowledge (such as computer vision or <strong>Natural Language</strong> <strong>Processing</strong> (<strong>NLP</strong>))<span id="dx1-38001"></span>. In this section, we will<span id="dx1-38002"></span> review some of the most common of these neural network architectures, including <strong>Convolutional Neural</strong> <strong>Networks</strong> (<strong>CNNs</strong>)<span id="dx1-38003"></span> and <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>), as well as attention mechanisms and transformers. <span id="x1-38004r57"></span></p>
<section id="exploring-cnns" class="level3 subsectionHead" data-number="8.3.1">
<h3 class="subsectionHead" data-number="8.3.1" id="sigil_toc_id_35"><span class="titlemark">3.3.1 </span> <span id="x1-390001"></span>Exploring CNNs</h3>
<p>When<span id="dx1-39001"></span> looking back at the example of trying to predict London housing prices with an MLP model, the input features we used (distance to the city centre, floor area, and construction year of the house) were still ”hand-engineered,” meaning that a human looked at the problem and decided which inputs might be relevant to the model when making price predictions. What might such input features look like if we were trying to build a model that takes in images as input and tries to predict which object is shown in the image? One breakthrough moment for deep learning was the realization that neural networks can directly learn and extract the most useful features for a task from the raw data – in the case of visual object classification, these features are learned directly from the pixels in the image.</p>
<p>What would a neural network architecture need to look like if we wanted to extract the most relevant input features from an image for an object classification task? When trying to answer this question, early machine learning researchers turned to mammalian brains. Object classification is a task that our visual system performs relatively effortlessly. One observation that inspired the development of CNNs was that the visual cortex responsible for object recognition in mammals implements a hierarchy of feature extractors that work with increasingly large receptive fields. A <strong>receptive</strong> <strong>field</strong> <span id="dx1-39002"></span>is the area in the image that a biological neuron responds to. The neurons at the early layers of the visual cortex respond to relatively small regions of an image only, while neurons in layers higher up the hierarchy respond to areas that cover large parts (or even the entirety) of an input image.</p>
<p>Inspired by the cortical hierarchy in the brain, CNNs<span id="dx1-39003"></span> implement a hierarchy of feature extractors with artificial neurons higher up in the hierarchy having larger receptive fields. To understand how that works, let’s look at how CNNs build features based on input images. <em>Figure</em> <a href="#x1-39005r3"><em>3.3</em></a> shows an early convolutional layer in a CNN operating on the input image (shown on the left) to extract features into a feature map (shown on the right). You can imagine the feature map as a matrix with <em>n</em> rows and <em>m</em> columns and every feature in the feature map as a scalar value. The example highlights two instances where the convolutional layer operates on different local regions of the image. In the first instance, the feature in the feature map receives input from the face of the kitten.<span id="dx1-39004"></span> In the second instance, the feature receives inputs from the kitten’s right paw. The final feature map will be the result of repeating this same operation over all regions of the input image, sliding a kernel from left to right and from top to bottom to fill all the values in the feature map.</p>
<div class="IMG---Figure">
<img src="../media/file57.png" alt="PIC"/> <span id="x1-39005r3"></span> <span id="x1-39006"></span></div>
<p class="IMG---Caption">Figure 3.3: Building a feature map from the input image 
</p>
<p>What does such a single operation look like numerically? This is illustrated in <em>Figure</em> <em> </em><a href="#x1-39007r4"><em>3.4</em></a>.</p>
<div class="IMG---Figure">
<img src="../media/file58.png" alt="PIC"/> <span id="x1-39007r4"></span> <span id="x1-39008"></span></div>
<p class="IMG---Caption">Figure 3.4: Numerical operations performed by convolutional layer 
</p>
<p>Here, we have zoomed in on one part of the input image and made its pixel values explicit (left side). You can imagine that the kernel (shown in the middle) slides over the<span id="dx1-39009"></span> input image step by step. In the step that is shown in the figure, the kernel is operating on the upper-left corner of the input image (highlighted in red). Given the values in the input image and the kernel values, the final value in the feature map (<strong>28</strong> in the example) is obtained via weighted averaging: each value in the input image is weighted by the corresponding value in the kernel, which yields 9 <span class="cmsy-10x-x-109">∗ </span>0 + 3 <span class="cmsy-10x-x-109">∗ </span>1 + 1 <span class="cmsy-10x-x-109">∗ </span>0 + 4 <span class="cmsy-10x-x-109">∗ </span>0 + 8 <span class="cmsy-10x-x-109">∗ </span>2 + 5 <span class="cmsy-10x-x-109">∗ </span>0 + 5 <span class="cmsy-10x-x-109">∗ </span>1 + 2 <span class="cmsy-10x-x-109">∗ </span>1 + 2 <span class="cmsy-10x-x-109">∗ </span>1 = 28.</p>
<p>Slightly more formally, let us denote the input image by <em>x</em> and the kernel by <em>w</em>. Convolution in a CNN<span id="dx1-39010"></span> can then be exdivssed as <em>z</em> = <span class="cmex-10x-x-109">∑</span> <sub><em>i</em><span class="cmr-8">=1</span></sub><sup><em>n</em></sup> <span class="cmex-10x-x-109">∑</span> <sub><em>j</em><span class="cmr-8">=1</span></sub><sup><em>m</em></sup><em>x</em><sub><em>i,j</em></sub><em>w</em><sub><em>i,j</em></sub>. This is usually followed by a non-linearity, <em>a</em> = <em>σ</em>(<em>z</em>), just like for the MLP. <em>σ</em> could be the sigmoid function introduced previously, but a more popular choice for CNNs is the <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>)<span id="dx1-39011"></span>, which is defined as <em>ReLU</em>(<em>z</em>) = <em>max</em>(0<em>,z</em>).</p>
<p>In modern CNNs, many of these convolutional layers will be stacked on top of each other, such that the feature map that forms the output of one convolutional layer will serve as the input (image) for the next convolutional layer, and so forth. Putting convolutional layers in sequence like this allows the CNN to build more and more abstract feature representations. When studying feature maps at different positions of the hierarchy, it was shown by Matthew Zeiler et al. (see <em>Further reading</em>) that feature maps at early convolutional layers often show edges and simple textures, while feature maps at later convolutional layers show more complex patterns and parts of objects. Similar to the visual cortical hierarchy, neurons in later convolutional layers will tend to have larger receptive fields because they accumulate input from several earlier neurons, which in turn receive inputs from different local regions of the image.</p>
<p>The number of convolutional layers that are stacked on top of each other will determine the depth of a CNN: the more layers, the deeper the network. Another important dimension for a CNN is its width, which is determined by the number of convolutional kernels per layer. You can imagine that we can apply more than one kernel at a given convolutional layer, which will result in additional feature maps – one for every additional kernel. In this case, the kernels in the subsequent<span id="dx1-39012"></span> convolutional layer will need to be three-dimensional in order to handle the multitude of feature maps in the input, where the third dimension of the kernel will be determined by the number of incoming feature maps.</p>
<p>Along with convolutional layers, another common building block for CNNs is <strong>pooling layers</strong><span id="dx1-39013"></span>, in particular <strong>mean-pooling</strong> and <strong>max-pooling</strong><span id="dx1-39014"></span><span id="dx1-39015"></span> layers. The function of these layers is to sub-sample the input, which reduces the input size of the image and thus the subsequent number of parameters needed in the network<span id="dx1-39016"></span> (and thus reduces the computational load and memory footprint).</p>
<p>How do pooling layers operate? In <em>Figure</em> <em> </em><a href="#x1-39017r5"><em>3.5</em></a>, we see both a mean-pooling (left) and max-pooling (right) layer in operation. We see that, like convolutional layers, they operate on local regions of the input. The operations they perform are straightforward – either they take the mean or the maximum of the pixel values in their receptive field.</p>
<div class="IMG---Figure">
<img src="../media/file59.png" alt="PIC"/> <span id="x1-39017r5"></span> <span id="x1-39018"></span></div>
<p class="IMG---Caption">Figure 3.5: Numerical operations performed by pooling layers 
</p>
<p>In addition to computational and memory considerations, another advantage of pooling layers is that they can make the network more robust to small variations in the input. Imagine, for example, one of the input pixel values in the example changed to 0. This will either affect the output very little (mean-pooling layer) or not at all (max-pooling layer).</p>
<p>Now that we have reviewed the essential operations, let’s implement a CNN in TensorFlow. Importing all the necessary functions includes the already familiar <code>Sequential</code> function to build a feed-forward model as well as the <code>Dense</code> layer. In addition, this time, we also import <code>Conv2D</code> for convolution and <code>MaxPooling2D</code> for max-pooling. With these tools, we can implement a CNN by chaining these layer functions in the right order:</p>
<pre id="fancyvrb8" class="fancyvrb"><span id="x1-39036r1"></span> 
<code><span id="textcolor50"><span>from</span></span><span> </span><span id="textcolor51"><span>tensorflow.keras</span></span><span> </span><span id="textcolor52"><span>import</span></span><span> Sequential</span> <span id="x1-39038r2"></span> </code>
<code><span id="textcolor53"><span>from</span></span><span> </span><span id="textcolor54"><span>tensorflow.keras.layers</span></span><span> </span><span id="textcolor55"><span>import</span></span><span> Flatten,</span><span> Conv2D,</span><span> MaxPooling2D,</span><span> Dense</span> <span id="x1-39040r3"></span> </code>
<code><span id="x1-39042r4"></span></code>
<code><span id="x1-39044r5"></span></code>
<code><span>convolutional_neural_network</span><span> </span><span id="textcolor56"><span>=</span></span><span> Sequential([</span> <span id="x1-39046r6"></span> </code>
<code><span> </span><span> </span><span> </span><span> Conv2D(</span><span id="textcolor57"><span>32</span></span><span>,</span><span> (</span><span id="textcolor58"><span>3</span></span><span>,</span><span id="textcolor59"><span>3</span></span><span>),</span><span> activation</span><span id="textcolor60"><span>=</span></span><span id="textcolor61"><span>"relu"</span></span><span>,</span><span> input_shape</span><span id="textcolor62"><span>=</span></span><span>(</span><span id="textcolor63"><span>28</span></span><span>,</span><span> </span><span id="textcolor64"><span>28</span></span><span>,</span><span> </span><span id="textcolor65"><span>1</span></span><span>)),</span> <span id="x1-39048r7"></span> </code>
<code><span> </span><span> </span><span> </span><span> MaxPooling2D((</span><span id="textcolor66"><span>2</span></span><span>,</span><span id="textcolor67"><span>2</span></span><span>)),</span> <span id="x1-39050r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> Conv2D(</span><span id="textcolor68"><span>64</span></span><span>,</span><span> (</span><span id="textcolor69"><span>3</span></span><span>,</span><span id="textcolor70"><span>3</span></span><span>),</span><span> activation</span><span id="textcolor71"><span>=</span></span><span id="textcolor72"><span>"relu"</span></span><span>),</span> <span id="x1-39052r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> MaxPooling2D((</span><span id="textcolor73"><span>2</span></span><span>,</span><span id="textcolor74"><span>2</span></span><span>)),</span><span> </span><span> </span><span> </span><span> Flatten(),</span> <span id="x1-39054r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> Dense(</span><span id="textcolor75"><span>64</span></span><span>,</span><span> activation</span><span id="textcolor76"><span>=</span></span><span id="textcolor77"><span>"relu"</span></span><span>),</span> <span id="x1-39056r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> Dense(</span><span id="textcolor78"><span>10</span></span><span>)</span> <span id="x1-39058r12"></span> </code>
<code><span>])</span></code></pre>
<p>We have built a CNN<span id="dx1-39059"></span> by chaining a convolutional layer with 32 kernels, followed by a max-pooling operation, followed by a convolutional layer with 64 kernels, and another max-pooling operation. In the end, we add two <code>Dense</code> layers. The final <code>Dense</code> layer will serve to match the number of output neurons to the number of classes in a classification problem. In the preceding example, that number is <code>10</code>. Our network is now ready for us to train.</p>
<p>CNNs<span id="dx1-39063"></span> have become crucial for a broad variety of problems, forming a key component in systems designed for a whole range of problems, from self-driving cars through to medical imaging. They also provided a foundation for other important neural network architectures, such as <strong>graph convolutional</strong> <strong>networks</strong> (<strong>GCNs</strong>)<span id="dx1-39064"></span>. But the field of deep learning wasn’t able to dominate the world of machine learning with CNNs alone. In the next section, we’ll learn about another important architecture: the recurrent neural network, an invaluable method for processing sequential data. <span id="x1-39065r66"></span></p>
</section>
<section id="exploring-rnns" class="level3 subsectionHead" data-number="8.3.2">
<h3 class="subsectionHead" data-number="8.3.2" id="sigil_toc_id_36"><span class="titlemark">3.3.2 </span> <span id="x1-400002"></span>Exploring RNNs</h3>
<p>The neural networks that we have seen so far are what we call feedforward networks: each layer of the network feeds into the next layer of the network; there is no cycle. Moreover, the convolutional neural networks we looked at receive a single input (an image) and output a single output: a label or a score for that label. But there are many cases in which we are working with something more complex than a single input, single output task. In this section, we will focus on a family of models called <strong>recurrent neural networks</strong> (<strong>RNNs</strong>)<span id="dx1-40001"></span><span id="dx1-40002"></span>, which focus on processing sequences of inputs, with some also producing sequential outputs.</p>
<p>A typical example of an RNN task is machine translation. For example, translating the English sentence, ”the apple is green,” to French. For such a task to work, a network needs to consider the relationship between the inputs we feed it. Another task could be video classification, where we need to look at different frames of a video to classify the content of the video. An RNN processes an input one step at a time, where every time step can be denoted as <sub><em>t</em></sub>. At every time step, the model computes a hidden state <em>h</em><sub><em>t</em></sub> and an output <em>y</em><sub><em>t</em></sub>. But to compute <em>h</em><sub><em>t</em></sub>, the model does not only receive the input <em>x</em><sub><em>t</em></sub> but also the hidden state at the previous time step <em>h</em><sub><em>t</em><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub>. For a single time step, a vanilla RNN thus computes the following:</p>
<div class="math-display">
<img src="../media/file60.jpg" class="math-display" alt="ht = f(Wxxt + Whht− 1 + b) "/>
</div>
<p>Where:</p>
<ul>
<li><p><em>W</em><sub><em>x</em></sub> are the weights of the RNN for the input <em>x</em><sub><em>t</em></sub></p></li>
<li><p><em>W</em><sub><em>h</em></sub> are the weights for the hidden layer output from the previous time step <em>h</em><sub><em>t</em><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub></p></li>
<li><p><em>b</em> is the bias term</p></li>
<li><p><em>f</em> is an activation function – in a vanilla RNN a <em>tanh</em> activation function</p></li>
</ul>
<p>This way, at every time step, the model also has awareness of what happened at previous time steps because of the additional input <em>h</em><sub><em>t</em><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub>.</p>
<p>We can visualize the flow of an RNN as follows:</p>
<div class="IMG---Figure">
<img src="../media/file61.png" alt="PIC"/> <span id="x1-40003r6"></span> <span id="x1-40004"></span></div>
<p class="IMG---Caption">Figure 3.6: Example of an RNN 
</p>
<p>We can see that we need an initial hidden state as well at time step zero. This is usually just a vector of zeros.</p>
<p>One important variant of a vanilla neural network is a <strong>sequence-to-sequence</strong> (<strong>seq2seq</strong>)<span id="dx1-40005"></span> neural network, a popular paradigm in machine translation. The idea of this network is, as the name suggests, to take a sequence as input and output another sequence. Importantly, both sequences do not have to be of the same length. This enables the architecture to translate sentences in a more flexible way, which is crucial as different languages do not use the same number of words in sentences with the same meaning. This flexibility is achieved with an encoder-decoder architecture<span id="dx1-40006"></span>. This means that we will have two parts of our NN<span id="dx1-40007"></span>: an initial part that encodes the input up until a single weight (many inputs encoded to one hidden vector) matrix that is then used as input for the decoder of the network to produce multiple outputs (one input to many outputs). The encoder and the decoder have separate weight matrices. For a model with two inputs and two outputs, this can be visualized as follows:</p>
<div class="IMG---Figure">
<img src="../media/file62.png" alt="PIC"/> <span id="x1-40008r7"></span> <span id="x1-40009"></span></div>
<p class="IMG---Caption">Figure 3.7: Example of a sequence-to-sequence network 
</p>
<p>In this figure <em>w</em><sub><em>e</em></sub> are the weights of the encoder and <em>w</em><sub><em>d</em></sub> are the weights of the decoder. We can see that compared to our RNN, we now have a new hidden<span id="dx1-40010"></span> state of the decoder <em>s</em><sub><span class="cmr-8">0</span></sub> and we can also observe <em>c</em>, which is a context vector<span id="dx1-40011"></span>. In standard sequence-to-sequence models, <em>c</em> is equal to the hidden state at the end of the encoder, whereas <em>s</em><sub><span class="cmr-8">0</span></sub>, the initial hidden state of the encoder, is typically computed with one or more feedforward layers. The context vector is an additional input to each part of the decoder; it allows each part to use the information of the encoder. <span id="x1-40012r70"></span></p>
</section>
<section id="attention-mechanisms" class="level3 subsectionHead" data-number="8.3.3">
<h3 class="subsectionHead" data-number="8.3.3" id="sigil_toc_id_37"><span class="titlemark">3.3.3 </span> <span id="x1-410003"></span>Attention mechanisms</h3>
<p>Although recurrent neural network models can be powerful, they have an important disadvantage: all information that the encoder can give to the decoder<span id="dx1-41001"></span> has to be in the hidden bottleneck layer<span id="dx1-41002"></span> – the hidden input state that the decoder receives at the start. That is fine for short sentences, but you can imagine that this becomes more difficult when we want to translate an entire paragraph or a very long sentence. We simply cannot expect that a single vector contains all information that is required to translate a long sentence. This downside is solved by a mechanism called attention. Later on, we will generalize the concept of attention, but let us first see how attention can be applied in the context of a seq2seq model.</p>
<p>Attention allows the decoder of a seq2seq model to ”attend” to hidden states of the encoder according to some attention weights. This means that instead of having to rely on the bottleneck layer to translate the input, the decoder can go back to each hidden state of the encoder and decide how much information it wants to use. This is done via a context vector at every time step of the decoder that now functions as a probability vector, determining how much weight to give to each of the hidden states of the encoder. We can think of attention in this context as the following sequence for every time step of the decoder:</p>
<ul>
<li><p><em>e</em><sub><em>t,i</em></sub> = <em>f</em>(<em>s</em><sub><em>t</em><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub><em>,h</em><sub><em>i</em></sub>) computes <em>alignment scores</em> for every hidden state of the encoder. This computation can be an MLP for every hidden state of the encoder, taking as input the current hidden state of the decoder <em>s</em><sub><em>t</em><span class="cmsy-8">−</span><span class="cmr-8">1</span></sub>, and <em>h</em><sub><em>i</em></sub> the hidden states of the encoder.</p></li>
<li><p><em>e</em><sub><em>t,i</em></sub> gives us alignment scores; they tell us something about the relation of every hidden state in the encoder and a single hidden state of the decoder. But the output of <em>f</em> is a scalar, which makes it impossible to compare different alignment scores. That is why we then take the softmax over all alignment scores to get a probability vector; attention weights: <em>a</em><sub><em>t,i</em></sub> = <em>softmax</em>(<em>e</em><sub><em>t,i</em></sub>). These weights are now values between 0 and 1 and tell us, for a single hidden state in the decoder, how much weight to give to every hidden state of the decoder.</p></li>
<li><p>With our attention weights, we now take a weighted average of the hidden states of the encoder. This produces the context vector <em>c</em><sub><span class="cmr-8">1</span></sub>, which can be used for the first time step of the decoder.</p></li>
</ul>
<p>Because this mechanism computes attention weights for every time step of the decoder, the model is now much more flexible: at every time step, it knows how much weight to give to each part of the encoder. Moreover, because we are using an MLP here to compute the attention weights, this mechanism<span id="dx1-41003"></span> can be trained end to end.</p>
<p>This tells you a way to use attention<span id="dx1-41004"></span> in a sequence-to-sequence model. But the attention mechanism can be generalized to make it even more powerful. This generalization is used as a building block in the most powerful neural network applications you see today. It uses three main components:</p>
<ul>
<li><p>Queries, denoted as <em>Q</em>. You can think of these as the hidden states of the decoder.</p></li>
<li><p>Keys, denoted as <em>K</em>. You can think of the keys as the hidden state of the inputs <em>h</em><sub><em>i</em></sub>.</p></li>
<li><p>Values, denoted as <em>V</em> . In the standard attention mechanism, these are the same as the keys, but just separated as a separate value.</p></li>
</ul>
<p>Together, the queries, keys, and values form the attention mechanism in the form of</p>
<div class="math-display">
<img src="../media/file63.jpg" class="math-display" alt=" QKT Attention (Q, K, V) = softmax (-√---)V dk "/>
</div>
<p>We can distinguish three generalizations:</p>
<ul>
<li><p>Using an MLP to compute the attention weights is a relatively heavy operation for every time step. Instead, we can use something more lightweight that allows us to compute the attention weights for every hidden state of the decoder much faster: we use a scaled dot product of the hidden state of the decoder and the hidden states of the encoder. We scale the dot product by the square root of the dimension of <em>K</em>:</p>
<div class="math-display">
<img src="../media/file64.jpg" class="math-display" alt=" T Q√K--- dk "/>
</div>
<p>This is because of two reasons:</p>
<ul>
<li><p>The softmax operation can lead to extreme values – values very close to zero and very close to one. This makes the optimization process more difficult. By scaling the dot product, we avoid this issue.</p></li>
<li><p>The attention mechanism takes the dot product of vectors with a high dimension. This causes the dot product to be very large. By scaling the dot product, we counteract this tendency.</p></li>
</ul></li>
<li><p>We use the input vectors separately – we separate them as keys and values in different input streams. This gives the model more flexibility to handle them in different ways. Both are learnable matrices, so the model can optimize both in different ways.</p></li>
<li><p>Attention takes a set of inputs as the query vector. This is more computationally efficient; instead of computing the dot product for every single query vector, we can do this for all of them at once.</p></li>
</ul>
<p>These three generalizations make attention a very widely applicable algorithm. You see it in most of today’s most performant models, some of the best image classification models – large language models that generate very realistic text or text-to-image models that can create the most beautiful and creative images. Because of the wide use of the attention mechanism<span id="dx1-41005"></span>, it is easily available in TensorFlow and other deep learning libraries. In TensorFlow, you can use attention like so:</p>
<pre id="fancyvrb9" class="fancyvrb"><span id="x1-41009r1"></span> 
<code><span id="textcolor80"><span>from</span></span><span> </span><span id="textcolor81"><span>tensorflow.keras.layers</span></span><span> </span><span id="textcolor82"><span>import</span></span><span> Attention</span> <span id="x1-41011r2"></span> </code>
<code><span>attention</span><span> </span><span id="textcolor83"><span>=</span></span><span> Attention(use_scale</span><span id="textcolor84"><span>=</span></span><span id="textcolor85"><span>True</span></span><span>,</span><span> score_mode</span><span id="textcolor86"><span>=</span></span><span id="textcolor87"><span>'dot'</span></span><span>)</span></code></pre>
<p>And it can be called with our query, key, and value:</p>
<pre id="fancyvrb10" class="fancyvrb"><span id="x1-41017r1"></span> 
<code><span>context_vector,</span><span> attention_weights</span><span> </span><span id="textcolor88"><span>=</span></span><span> attention(</span> <span id="x1-41019r2"></span> </code>
<code><span> </span><span> </span><span> </span><span> inputs</span><span> </span><span id="textcolor89"><span>=</span></span><span> [query,</span><span> value,</span><span> keys],</span> <span id="x1-41021r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> return_attention_scores</span><span> </span><span id="textcolor90"><span>=</span></span><span> </span><span id="textcolor91"><span>True</span></span><span>,</span> <span id="x1-41023r4"></span> </code>
<code><span>)</span></code></pre>
<p><span id="dx1-41024"></span></p>
<p>In the preceding sections, we discussed some important building blocks of neural networks; we discussed the basic MLP, the concept of convolution,<span id="dx1-41025"></span> recurrent neural networks, and attention. There are other components we did not discuss here, and even more variants and combinations of the components we discussed. If you want to know more about these building blocks, refer to the reading list at the end of this chapter. There are excellent resources available in case you want to dive deeper into neural network architectures and components. <span id="x1-41026r65"></span></p>
</section>
</section>
<section id="understanding-the-problem-with-typical-neural-networks" class="level2 sectionHead" data-number="8.4">
<h2 class="sectionHead" data-number="8.4" id="sigil_toc_id_38"><span class="titlemark">3.4 </span> <span id="x1-420004"></span>Understanding the problem with typical neural networks</h2>
<p>The deep neural networks we discussed in previous sections are extremely powerful and, paired with appropriate training data, have enabled big strides in machine perception. In machine vision, convolutional neural networks enable us to classify images, locate objects in images, segment images into different segments or instances, and even to generate entirely novel images. In natural language processing, recurrent neural networks and transformers have allowed us to classify text, to recognize speech, to generate novel text or, as reviewed previously, to translate between two different languages.</p>
<p>However, these standard types of neural network models also have several limitations<span id="dx1-42001"></span>. In this section, we will explore some of these limitations. We will look at the following:</p>
<ul>
<li><p>How the prediction scores of such neural network models can be overconfident</p></li>
<li><p>How such models can produce very confident predictions on OOD data</p></li>
<li><p>How tiny, imperceptible changes in an input image can cause a model to make completely wrong predictions</p></li>
</ul>
<p><span id="x1-42002r73"></span></p>
<section id="uncalibrated-and-overconfident-predictions" class="level3 subsectionHead" data-number="8.4.1">
<h3 class="subsectionHead" data-number="8.4.1" id="sigil_toc_id_39"><span class="titlemark">3.4.1 </span> <span id="x1-430001"></span>Uncalibrated and overconfident predictions</h3>
<p>One problem with modern vanilla neural networks is that they often produce outputs that are not well-calibrated<span id="dx1-43001"></span>. This means that the confidence scores produced by these networks no longer represent their empirical correctness. To understand better what that means, let’s look at the reliability plot for the ideally-calibrated network that is shown in <em>Figure</em> <em> </em><a href="#x1-43002r8"><em>3.8</em></a>.</p>
<div class="IMG---Figure">
<img src="../media/file65.png" alt="PIC"/> <span id="x1-43002r8"></span> <span id="x1-43003"></span></div>
<p class="IMG---Caption">Figure 3.8: Reliability plot for a well-calibrated neural network. The empirically determined (”actual”) accuracy is consistent with the prediction values output by the network 
</p>
<p>As you can see, the reliability plot shows accuracy (on the <em>y</em> axis) as a function of confidence (on the <em>x</em> axis). The basic idea is that, for a well-calibrated network, the output (or confidence) score associated with a prediction should match its empirical correctness. Here, empirical correctness is defined as the accuracy of a group of samples that all share similar output values and, for that reason, are grouped into the same bin in the reliability plot. So, for example, for the group of samples that all were assigned an output score between 0.7 and 0.8, the expectation, for a well-calibrated network, would be that 75% of these predictions should be correct.</p>
<p>To make this idea a bit more formal, let’s imagine that we have a dataset <strong>X</strong> with <strong>N</strong> data samples. Each data sample <strong>x</strong> has a corresponding target label <strong>y</strong>. In a classification setting, we would have <em>y</em>, which represents the membership to one in <em>K</em> classes. To obtain a reliability plot, we would use a neural network model to run inference over the entire dataset <strong>X</strong> to obtain an output score <em>ŷ</em> per sample <strong>x</strong>. We would then use the output scores to assign each data sample to a bin <em>m</em> in the reliability plot. In the preceding figure, we have opted for <em>M</em> = 10 bins. <em>B</em><sub><em>m</em></sub> would be the set of indices of samples that fall into bin <em>m</em>. Finally, we would measure and plot the average accuracy of the predictions for all the samples in a given bin, which is defined as <em>acc</em>(<em>B</em><sub><em>m</em></sub>) = <img src="../media/file66.jpg" class="frac" data-align="middle" alt="-1-- |Bm |"/><span class="cmex-10x-x-109">∑</span> <sub><em>i</em><span class="cmsy-8">∈</span><em>B</em><sub><span class="cmmi-6">m</span></sub></sub>1(<em>ŷ</em><sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub>).</p>
<p>In the case of a well-calibrated network, the average accuracy of the samples in a given bin should match the confidence values of that bin. In the preceding figure, we can see, for example, that for samples that fell in the bin for output scores between 0.2 and 0.3, we observed a matching average accuracy of 0.25. Now let’s see what happens in the case of an uncalibrated network that is over-confident in its predictions. Figure  <a href="#x1-43004r9">3.9</a> illustrates this scenario, which is representative of the behavior shown by many modern vanilla neural network architectures.</p>
<div class="IMG---Figure">
<img src="../media/file67.png" alt="PIC"/> <span id="x1-43004r9"></span> <span id="x1-43005"></span></div>
<p class="IMG---Caption">Figure 3.9: Reliability plot for an overconfident neural network. The ”actual” empirically determined accuracy (purple bars) is consistently below the accuracy suggested by the prediction values of the network (pink bars and gray dashed line) 
</p>
<p>We can observe that for all the bins, the observed (”actual”) accuracy for the samples in the bin is below the accuracy that is expected based on the output score of the samples. This is the exdivssion of over-confident predictions.<span id="dx1-43006"></span> The network’s output makes us believe that it has a high degree of confidence in its predictions, while in reality the actual performance does not match up.</p>
<p>Over-confident predictions can be very problematic in safety- and mission-critical applications, such as medical decision making, self-driving cars, and legal or financial decisions. Networks with overconfident predictions will lack the ability to indicate to us humans (or other networks) when they are likely to be wrong. This lack of awareness can become dangerous, for example, when a network is used to help decide whether a defendant should be granted bail or not. Assume a network is presented with a defendant’s data (such as past convictions, age, education level) and predicts with 95% confidence score that bail should not be granted. presented with this output, a judge might falsely think that the model can be trusted and base their verdict largely on the model’s output. By contrast, calibrated confidence outputs can indicate to what degree we can trust the model’s output. If the model is uncertain, this indicates that there is something about the input data that isn’t well represented in the model’s training data – indicating that the model is more likely to make a mistake. Thus, well-calibrated uncertainties allow us to decide whether to incorporate the model’s predictions in our decision making, or whether to ignore the model entirely.</p>
<p>Drawing and inspecting reliability plots is useful for visualizing calibration in a few neural networks. However, sometimes we want to compare the calibration performance across several neural networks, possibly each network using more than one configuration. In such cases where we need to compare many networks and settings, it is useful to summarize the calibration of a neural network in a scalar statistic. The <strong>Expected Calibration Error</strong> (<strong>ECE</strong>)<span id="dx1-43007"></span> represents such a summary statistic. For every bin in the reliability plot, it measures the difference between the observed accuracy, <em>acc</em>(<em>B</em><sub><em>m</em></sub>), and the accuracy we would have expected based on the output score of the samples, <em>conf</em>(<em>B</em><sub><em>m</em></sub>), which is defined as <img src="../media/file68.jpg" class="frac" data-align="middle" alt="-1-- |Bm |"/><span class="cmex-10x-x-109">∑</span> <sub><em>i</em><span class="cmsy-8">∈</span><em>B</em><sub><span class="cmmi-6">m</span></sub></sub><em>ŷ</em>.<span id="dx1-43008"></span> Then, it takes a weighted average across all bins, where the weight for each bin is determined by the number of samples in the bin:</p>
<div class="math-display">
<img src="../media/file69.jpg" class="math-display" alt=" M∑ ECE = |Bm-||acc(Bm ) − conf(Bm )| m=1 n "/>
</div>
<p>This provides a first introduction to ECE and how it is measured. We will revisit ECE in more detail in <a href="CH8.xhtml#x1-1320008"><em>Chapter 8</em></a>, <a href="CH8.xhtml#x1-1320008"><em>Applying Bayesian Deep Learning</em></a> by providing a code implementation as part of the <em>Revealing dataset shift with Bayesian</em> <em>methods</em> case study.</p>
<p>In the case of a perfectly calibrated neural network output, the <em>ECE</em> would be zero. The more uncalibrated a neural network is, the larger <em>ECE</em> would become. Let us look at some of the reasons for which neural networks are poorly calibrated and overconfident.</p>
<p>One reason is that the softmax function, which is usually the last operation of a classification network, uses the exponential function to make sure that all values are positive:</p>
<div class="math-display">
<img src="../media/file70.jpg" class="math-display" alt=" zi σ(⃗z)i = ∑--e----- Kj=1ezj "/>
</div>
<p>The result of this is that small changes of the input to the softmax function can lead to substantial changes in its output.</p>
<p>Another reason for overconfidence is the increased model capacity of modern deep neural networks (Chuan Guo, et al. (2017)). Neural network architectures have become deeper (more layers) and wider (more neurons per layer) over the years. Such deep and wide neural networks have high variance and can very flexibly fit large amounts of input data. When experimenting with either the number of layers for a neural network or the number of filters per layer, Chuan Guo et al. observed that mis-calibration (and thus overconfidence) becomes worse with deeper and wider architectures. They also found that using batch normalization or training with less weight decay had a negative impact on calibration. These observations point to the conclusion that increased model capacity for modern deep neural network contributes to their overconfidence.</p>
<p>Finally, overconfident estimates can result from choosing particular neural network components. It has been shown, for example, that fully-connected networks that use ReLU functions lead to continuous piecewise affine classifiers (<span class="cite">[<strong>?</strong>]</span>). This, in turn, implies that it is always possible to find input samples for which the ReLU network will produce high-confidence outputs. This holds even for input samples that are unlike the training data, for which generalization performance might be poor and we would thus expect lower confidence outputs. Such arbitrarily<span id="dx1-43009"></span> high confidence predictions also apply to convolutional networks that use either max- or mean-pooling following the convolutional layers, or any other network that results in a piecewise affine classifier function.</p>
<p>The problem is inherent to such neural network architectures and can only be addressed by changing the architecture itself (<span class="cite">[<strong>?</strong>]</span>). <span id="x1-43010r75"></span></p>
</section>
<section id="predictions-on-out-of-distribution-data" class="level3 subsectionHead" data-number="8.4.2">
<h3 class="subsectionHead" data-number="8.4.2" id="sigil_toc_id_40"><span class="titlemark">3.4.2 </span> <span id="x1-440002"></span>Predictions on out-of-distribution data</h3>
<p>Now that we have seen that models can be overconfident and therefore uncalibrated, let’s look at another problem of neural networks. Neural networks are typically trained under the assumption that our test and train data are drawn from the same distribution<span id="dx1-44001"></span>. In practice, however, that is not always the case. The data that a model sees when it is deployed in the real world can change. We call these changes dataset shifts and they are typically divided into three categories:</p>
<ul>
<li><p><strong>Covariate shift</strong>:<span id="dx1-44002"></span> The feature distribution <em>p</em>(<em>x</em>) changes while <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x</em>) is fixed</p></li>
<li><p><strong>Open-set recognition</strong>: New labels appear at test time</p></li>
<li><p><strong>Label shift</strong>: The distribution of labels <em>p</em>(<em>y</em>) changes while <em>p</em>(<em>x</em><span class="cmsy-10x-x-109">|</span><em>y</em>) is fixed</p></li>
</ul>
<p>Examples of the preceding items include the following:</p>
<ul>
<li><p>Covariate shift: A model is trained to recognize faces. The training data consists of faces of mostly young people. The model at test time sees faces of all ages.</p></li>
<li><p>Open-set recognition:<span id="dx1-44003"></span> A model is trained to classify a limited number of dog breeds. At test time, the model sees more dog breeds than present in the training dataset.</p></li>
<li><p>Label shift:<span id="dx1-44004"></span> A model is trained to predict different diseases, of which some are very rare at the time of training. However, over time, the frequency of a rare disease changes and it becomes one of the most frequently seen diseases at test time.</p></li>
</ul>
<div class="IMG---Figure">
<img src="../media/file71.png" alt="PIC"/> <span id="x1-44005r10"></span> <span id="x1-44006"></span></div>
<p class="IMG---Caption">Figure 3.10: The training data distribution and data in the real world mostly overlap, but we cannot expect our model to perform well on the out-of-distribution points in the top right of the figure. 
</p>
<p>As a result of these changes, a model might be less performant when deployed in the real world if it is confronted with data that was not drawn from the same distribution as the training data. How likely a model is to be confronted with out-of-distribution data depends very much on the environment in which the model is deployed: some environments are more static (lower chance of out-of-distribution data), while others are more dynamic (higher chance of out-of-distribution data).</p>
<p>One reason for the problems of deep learning models with out-of-distribution<span id="dx1-44007"></span> data is that models often have a large number of parameters and can therefore memorize specific patterns and features of the training data instead of robust and meaningful representations of the data that reflect the underlying data distribution. When new data that looks slightly different from the training data presents itself at test time, the model does not actually have the ability to generalize and make the right prediction. One such example is an image of a cow (<em>Figure</em> <a href="#x1-44008r11"><em>3.11</em></a>) on a beach, when cows in the training dataset happened to be on green grasslands. Models often use the context present in the data to make predictions.</p>
<div class="IMG---Figure">
<img src="../media/file72.png" alt="PIC"/> <span id="x1-44008r11"></span> <span id="x1-44009"></span></div>
<p class="IMG---Caption">Figure 3.11: An object in a different environment (here, a cow on a beach) can make it difficult for a model to recognize that the image contains the object 
</p>
<p>Before we go over a practical example of how a simple model handles out-of-distribution data, let’s examine a few approaches that can highlight the problem of out-of-distribution data in typical neural networks. Ideally, we would like our model to exdivss high uncertainty when it encounters data that is different from the distribution it was trained on. If that is the case, out-of-distribution data will not be a big problem when a model is deployed in the real world. For example, in mission-critical systems where errors of a model are costly, there is often a certain confidence threshold that should be met before the prediction is trusted. If a model is well-calibrated and it assigns a low confidence score to out-of-distribution<span id="dx1-44010"></span> inputs, the business logic around the model can throw an exception and not use the model’s output. For example, a self-driving car can alert the driver that it should take over control or it can slow down to avoid an accident.</p>
<p>However, common <em>neural networks do not know when they do not know</em>; they typically do not assign a low confidence score to out-of-distribution data.</p>
<p>An example of this is given in a Google paper titled <em>Can You Trust Your Model’s</em> <em>Uncertainty? Evaluating predictive Uncertainty Under Dataset Shift</em>. The paper shows that if you apply perturbations to a test dataset such as blur or noise, such that the images become more and more out-of-distribution, the accuracy of the model goes down. However, the confidence calibration of the model also decreases. This means that the scores of the model are not trustworthy anymore on out-of-distribution data: they do not accurately indicate the model’s confidence in its predictions. We will explore this behaviour ourselves in the <em>Revealing dataset shift with Bayesian methods</em> case study in <a href="CH8.xhtml#x1-1320008"><em>Chapter 8</em></a>, <a href="CH8.xhtml#x1-1320008"><em>Applying</em> <em>Bayesian Deep Learning</em></a>.</p>
<p>Another way to determine how a model handles out-of-distribution data is by feeding it data that is not just perturbed, but completely different from the dataset it was trained on. The procedure to measure the model’s out-of-distribution detection performance is then as follows:</p>
<ol>
<li><div id="x1-44012x1">
<p>Train a model on an in-distribution dataset.</p>
</div></li>
<li><div id="x1-44014x2">
<p>Save the confidence scores of the model on the in-distribution test set.</p>
</div></li>
<li><div id="x1-44016x3">
<p>Feed a completely different, out-of-distribution dataset to the model and save the corresponding confidence scores of the model.</p>
</div></li>
<li><div id="x1-44018x4">
<p>Now, treat the scores from both datasets as scores from a binary problem: in-distribution or out-of-distribution. Compute binary metrics, such as the <strong>area under the Receiver</strong> <strong>Operating Characteristic</strong> (<strong>AUROC</strong>)<span id="dx1-44019"></span> curve or the area under the precision-recall curve.</p>
</div></li>
</ol>
<p>This strategy tells you how well your model can separate in-distribution from out-of-distribution data. The assumption is that in-distribution data should always receive a higher confidence score than out-of-distribution data; in the ideal scenario, there is no overlap between the two distributions. In practice, however, this is not the case. Models do often give high confidence scores to out-of-distribution data<span id="dx1-44020"></span>. We will explore an example of this in the next section and a few solutions to this problem in later chapters. <span id="x1-44021r78"></span></p>
</section>
<section id="example-of-confident-out-of-distribution-predictions" class="level3 subsectionHead" data-number="8.4.3">
<h3 class="subsectionHead" data-number="8.4.3" id="sigil_toc_id_41"><span class="titlemark">3.4.3 </span> <span id="x1-450003"></span>Example of confident, out-of-distribution predictions</h3>
<p>Let’s see how a vanilla neural net can produce confident predictions on out-of-distribution data<span id="dx1-45001"></span>. In this example, we will first train a model and then feed it out-of-distribution data. To keep things simple, we will use a dataset with different types of dogs and cats and build a binary classifier that predicts whether the image contains a dog or a cat.</p>
<p>We first download our data:</p>
<pre id="fancyvrb11" class="fancyvrb"><span id="x1-45006r1"></span> 
<code><span>curl</span><span> -X</span><span> GET</span><span> https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz</span><span> </span><span> </span><span id="textcolor92"><span>\</span></span> <span id="x1-45008r2"></span> </code>
<code><span>--output</span><span> pets.tgz</span> <span id="x1-45010r3"></span> </code>
<code><span>tar</span><span> -xzf</span><span> pets.tgz</span></code></pre>
<p>We then load our data into a dataframe:</p>
<pre id="fancyvrb12" class="fancyvrb"><span id="x1-45020r1"></span> 
<code><span id="textcolor93"><span>import</span></span><span> </span><span id="textcolor94"><span>pandas</span></span><span> </span><span id="textcolor95"><span>as</span></span><span> </span><span id="textcolor96"><span>pd</span></span> <span id="x1-45022r2"></span> </code>
<code><span id="x1-45024r3"></span></code>
<code><span>df</span><span> </span><span id="textcolor97"><span>=</span></span><span> pd</span><span id="textcolor98"><span>.</span></span><span>read_csv(</span><span id="textcolor99"><span>"oxford-iiit-pet/annotations/trainval.txt"</span></span><span>,</span><span> sep</span><span id="textcolor100"><span>=</span></span><span id="textcolor101"><span>"</span><span> "</span></span><span>)</span> <span id="x1-45026r4"></span> </code>
<code><span>df</span><span id="textcolor102"><span>.</span></span><span>columns</span><span> </span><span id="textcolor103"><span>=</span></span><span> [</span><span id="textcolor104"><span>"path"</span></span><span>,</span><span> </span><span id="textcolor105"><span>"species"</span></span><span>,</span><span> </span><span id="textcolor106"><span>"breed"</span></span><span>,</span><span> </span><span id="textcolor107"><span>"ID"</span></span><span>]</span> <span id="x1-45028r5"></span> </code>
<code><span>df[</span><span id="textcolor108"><span>"breed"</span></span><span>]</span><span> </span><span id="textcolor109"><span>=</span></span><span> df</span><span id="textcolor110"><span>.</span></span><span>breed</span><span id="textcolor111"><span>.</span></span><span>apply(</span><span id="textcolor112"><span>lambda</span></span><span> x:</span><span> x</span><span> </span><span id="textcolor113"><span>-</span></span><span> </span><span id="textcolor114"><span>1</span></span><span>)</span> <span id="x1-45030r6"></span> </code>
<code><span>df[</span><span id="textcolor115"><span>"path"</span></span><span>]</span><span> </span><span id="textcolor116"><span>=</span></span><span> df[</span><span id="textcolor117"><span>"path"</span></span><span>]</span><span id="textcolor118"><span>.</span></span><span>apply(</span> <span id="x1-45032r7"></span> </code>
<code><span> </span><span> </span><span id="textcolor119"><span>lambda</span></span><span> x:</span><span> </span><span id="textcolor120"><span>f</span></span><span id="textcolor121"><span>"/content/oxford-iiit-pet/images/</span></span><span id="textcolor122"><span>{</span></span><span>x</span><span id="textcolor123"><span>}</span></span><span id="textcolor124"><span>.jpg"</span></span> <span id="x1-45034r8"></span> </code>
<code><span>)</span></code></pre>
<p>We can then use scikit-learn’s <code>train_test_val()</code> function to create a training and validation set:</p>
<pre id="fancyvrb13" class="fancyvrb"><span id="x1-45043r1"></span> 
<code><span id="textcolor125"><span>import</span></span><span> </span><span id="textcolor126"><span>tensorflow</span></span><span> </span><span id="textcolor127"><span>as</span></span><span> </span><span id="textcolor128"><span>tf</span></span> <span id="x1-45045r2"></span> </code>
<code><span id="textcolor129"><span>from</span></span><span> </span><span id="textcolor130"><span>sklearn.model_selection</span></span><span> </span><span id="textcolor131"><span>import</span></span><span> train_test_split</span> <span id="x1-45047r3"></span> </code>
<code><span id="x1-45049r4"></span></code>
<code><span>paths_train,</span><span> paths_val,</span><span> labels_train,</span><span> labels_val</span><span> </span><span id="textcolor132"><span>=</span></span><span> train_test_split(</span> <span id="x1-45051r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> df[</span><span id="textcolor133"><span>"path"</span></span><span>],</span><span> df[</span><span id="textcolor134"><span>"breed"</span></span><span>],</span><span> test_size</span><span id="textcolor135"><span>=</span></span><span id="textcolor136"><span>0.2</span></span><span>,</span><span> random_state</span><span id="textcolor137"><span>=</span></span><span id="textcolor138"><span>0</span></span> <span id="x1-45053r6"></span> </code>
<code><span>)</span></code></pre>
<p>We then create our train and validation data. Our <code>divprocess()</code> function loads our download image into memory and formats our label such that our model can process it<span id="dx1-45055"></span>. We use a batch size of 256 and an image size of 160x160 pixels:</p>
<pre id="fancyvrb14" class="fancyvrb"><span id="x1-45086r1"></span> 
<code><span>IMG_SIZE</span><span> </span><span id="textcolor139"><span>=</span></span><span> (</span><span id="textcolor140"><span>160</span></span><span>,</span><span> </span><span id="textcolor141"><span>160</span></span><span>)</span> <span id="x1-45088r2"></span> </code>
<code><span>AUTOTUNE</span><span> </span><span id="textcolor142"><span>=</span></span><span> tf</span><span id="textcolor143"><span>.</span></span><span>data</span><span id="textcolor144"><span>.</span></span><span>AUTOTUNE</span> <span id="x1-45090r3"></span> </code>
<code><span id="x1-45092r4"></span></code>
<code><span id="x1-45094r5"></span></code>
<code><span id="textcolor145"><span>@tf</span></span><span id="textcolor146"><span>.</span></span><span>function</span> <span id="x1-45096r6"></span> </code>
<code><span id="textcolor147"><span>def</span></span><span> </span><span id="textcolor148"><span>divprocess_image</span></span><span>(filename):</span> <span id="x1-45098r7"></span> </code>
<code><span> </span><span> raw</span><span> </span><span id="textcolor149"><span>=</span></span><span> tf</span><span id="textcolor150"><span>.</span></span><span>io</span><span id="textcolor151"><span>.</span></span><span>read_file(filename)</span> <span id="x1-45100r8"></span> </code>
<code><span> </span><span> image</span><span> </span><span id="textcolor152"><span>=</span></span><span> tf</span><span id="textcolor153"><span>.</span></span><span>image</span><span id="textcolor154"><span>.</span></span><span>decode_png(raw,</span><span> channels</span><span id="textcolor155"><span>=</span></span><span id="textcolor156"><span>3</span></span><span>)</span> <span id="x1-45102r9"></span> </code>
<code><span> </span><span> </span><span id="textcolor157"><span>return</span></span><span> tf</span><span id="textcolor158"><span>.</span></span><span>image</span><span id="textcolor159"><span>.</span></span><span>resize(image,</span><span> IMG_SIZE)</span> <span id="x1-45104r10"></span> </code>
<code><span id="x1-45106r11"></span></code>
<code><span id="x1-45108r12"></span></code>
<code><span id="textcolor160"><span>@tf</span></span><span id="textcolor161"><span>.</span></span><span>function</span> <span id="x1-45110r13"></span> </code>
<code><span id="textcolor162"><span>def</span></span><span> </span><span id="textcolor163"><span>divprocess</span></span><span>(filename,</span><span> label):</span> <span id="x1-45112r14"></span> </code>
<code><span> </span><span> </span><span id="textcolor164"><span>return</span></span><span> divprocess_image(filename),</span><span> tf</span><span id="textcolor165"><span>.</span></span><span>one_hot(label,</span><span> </span><span id="textcolor166"><span>2</span></span><span>)</span> <span id="x1-45114r15"></span> </code>
<code><span id="x1-45116r16"></span></code>
<code><span id="x1-45118r17"></span></code>
<code><span>train_dataset</span><span> </span><span id="textcolor167"><span>=</span></span><span> (tf</span><span id="textcolor168"><span>.</span></span><span>data</span><span id="textcolor169"><span>.</span></span><span>Dataset</span><span id="textcolor170"><span>.</span></span><span>from_tensor_slices(</span> <span id="x1-45120r18"></span> </code>
<code><span> </span><span> </span><span> </span><span> (paths_train,</span><span> labels_train)</span> <span id="x1-45122r19"></span> </code>
<code><span> </span><span> )</span><span id="textcolor171"><span>.</span></span><span>map(</span><span id="textcolor172"><span>lambda</span></span><span> x,</span><span> y:</span><span> divprocess(x,</span><span> y))</span> <span id="x1-45124r20"></span> </code>
<code><span> </span><span> </span><span id="textcolor173"><span>.</span></span><span>batch(</span><span id="textcolor174"><span>256</span></span><span>)</span> <span id="x1-45126r21"></span> </code>
<code><span> </span><span> </span><span id="textcolor175"><span>.</span></span><span>divfetch(buffer_size</span><span id="textcolor176"><span>=</span></span><span>AUTOTUNE)</span> <span id="x1-45128r22"></span> </code>
<code><span>)</span> <span id="x1-45130r23"></span> </code>
<code><span id="x1-45132r24"></span></code>
<code><span>validation_dataset</span><span> </span><span id="textcolor177"><span>=</span></span><span> (tf</span><span id="textcolor178"><span>.</span></span><span>data</span><span id="textcolor179"><span>.</span></span><span>Dataset</span><span id="textcolor180"><span>.</span></span><span>from_tensor_slices(</span> <span id="x1-45134r25"></span> </code>
<code><span> </span><span> </span><span> </span><span> (paths_val,</span><span> labels_val))</span> <span id="x1-45136r26"></span> </code>
<code><span> </span><span> </span><span id="textcolor181"><span>.</span></span><span>map(</span><span id="textcolor182"><span>lambda</span></span><span> x,</span><span> y:</span><span> divprocess(x,</span><span> y))</span> <span id="x1-45138r27"></span> </code>
<code><span> </span><span> </span><span id="textcolor183"><span>.</span></span><span>batch(</span><span id="textcolor184"><span>256</span></span><span>)</span> <span id="x1-45140r28"></span> </code>
<code><span> </span><span> </span><span id="textcolor185"><span>.</span></span><span>divfetch(buffer_size</span><span id="textcolor186"><span>=</span></span><span>AUTOTUNE)</span> <span id="x1-45142r29"></span> </code>
<code><span>)</span></code></pre>
<p>We can now create our model. To speed up learning, we can use transfer learning and start with a model that was pre-trained on ImageNet:</p>
<pre id="fancyvrb15" class="fancyvrb"><span id="x1-45157r1"></span> 
<code><span id="textcolor187"><span>def</span></span><span> </span><span id="textcolor188"><span>get_model</span></span><span>():</span> <span id="x1-45159r2"></span> </code>
<code><span> </span><span> IMG_SHAPE</span><span> </span><span id="textcolor189"><span>=</span></span><span> IMG_SIZE</span><span> </span><span id="textcolor190"><span>+</span></span><span> (</span><span id="textcolor191"><span>3</span></span><span>,)</span> <span id="x1-45161r3"></span> </code>
<code><span> </span><span> base_model</span><span> </span><span id="textcolor192"><span>=</span></span><span> tf</span><span id="textcolor193"><span>.</span></span><span>keras</span><span id="textcolor194"><span>.</span></span><span>applications</span><span id="textcolor195"><span>.</span></span><span>ResNet50(</span> <span id="x1-45163r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> input_shape</span><span id="textcolor196"><span>=</span></span><span>IMG_SHAPE,</span><span> include_top</span><span id="textcolor197"><span>=</span></span><span id="textcolor198"><span>False</span></span><span>,</span><span> weights</span><span id="textcolor199"><span>=</span></span><span id="textcolor200"><span>'imagenet'</span></span> <span id="x1-45165r5"></span> </code>
<code><span> </span><span> )</span> <span id="x1-45167r6"></span> </code>
<code><span> </span><span> base_model</span><span id="textcolor201"><span>.</span></span><span>trainable</span><span> </span><span id="textcolor202"><span>=</span></span><span> </span><span id="textcolor203"><span>False</span></span> <span id="x1-45169r7"></span> </code>
<code><span> </span><span> inputs</span><span> </span><span id="textcolor204"><span>=</span></span><span> tf</span><span id="textcolor205"><span>.</span></span><span>keras</span><span id="textcolor206"><span>.</span></span><span>Input(shape</span><span id="textcolor207"><span>=</span></span><span>IMG_SHAPE)</span> <span id="x1-45171r8"></span> </code>
<code><span> </span><span> x</span><span> </span><span id="textcolor208"><span>=</span></span><span> tf</span><span id="textcolor209"><span>.</span></span><span>keras</span><span id="textcolor210"><span>.</span></span><span>applications</span><span id="textcolor211"><span>.</span></span><span>resnet50</span><span id="textcolor212"><span>.</span></span><span>divprocess_input(inputs)</span> <span id="x1-45173r9"></span> </code>
<code><span> </span><span> x</span><span> </span><span id="textcolor213"><span>=</span></span><span> base_model(x,</span><span> training</span><span id="textcolor214"><span>=</span></span><span id="textcolor215"><span>False</span></span><span>)</span> <span id="x1-45175r10"></span> </code>
<code><span> </span><span> x</span><span> </span><span id="textcolor216"><span>=</span></span><span> tf</span><span id="textcolor217"><span>.</span></span><span>keras</span><span id="textcolor218"><span>.</span></span><span>layers</span><span id="textcolor219"><span>.</span></span><span>GlobalAveragePooling2D()(x)</span> <span id="x1-45177r11"></span> </code>
<code><span> </span><span> x</span><span> </span><span id="textcolor220"><span>=</span></span><span> tf</span><span id="textcolor221"><span>.</span></span><span>keras</span><span id="textcolor222"><span>.</span></span><span>layers</span><span id="textcolor223"><span>.</span></span><span>Dropout(</span><span id="textcolor224"><span>0.2</span></span><span>)(x)</span> <span id="x1-45179r12"></span> </code>
<code><span> </span><span> outputs</span><span> </span><span id="textcolor225"><span>=</span></span><span> tf</span><span id="textcolor226"><span>.</span></span><span>keras</span><span id="textcolor227"><span>.</span></span><span>layers</span><span id="textcolor228"><span>.</span></span><span>Dense(</span><span id="textcolor229"><span>2</span></span><span>)(x)</span> <span id="x1-45181r13"></span> </code>
<code><span> </span><span> </span><span id="textcolor230"><span>return</span></span><span> tf</span><span id="textcolor231"><span>.</span></span><span>keras</span><span id="textcolor232"><span>.</span></span><span>Model(inputs,</span><span> outputs)</span></code></pre>
<p>Before we can train the model, we first need to compile it. Compiling simply means that we specify a loss function and an optimizer for the model and, optionally, add some metrics for monitoring during training<span id="dx1-45182"></span>. In the following code, we specify that the model should be trained using the binary cross-entropy loss and the Adam optimizer and that, during training, we want to monitor the model’s accuracy:</p>
<pre id="fancyvrb16" class="fancyvrb"><span id="x1-45188r1"></span> 
<code><span>model</span><span> </span><span id="textcolor233"><span>=</span></span><span> get_model()</span> <span id="x1-45190r2"></span> </code>
<code><span>model</span><span id="textcolor234"><span>.</span></span><span>compile(optimizer</span><span id="textcolor235"><span>=</span></span><span>tf</span><span id="textcolor236"><span>.</span></span><span>keras</span><span id="textcolor237"><span>.</span></span><span>optimizers</span><span id="textcolor238"><span>.</span></span><span>Adam(learning_rate</span><span id="textcolor239"><span>=</span></span><span id="textcolor240"><span>0.01</span></span><span>),</span> <span id="x1-45192r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> loss</span><span id="textcolor241"><span>=</span></span><span>tf</span><span id="textcolor242"><span>.</span></span><span>keras</span><span id="textcolor243"><span>.</span></span><span>losses</span><span id="textcolor244"><span>.</span></span><span>BinaryCrossentropy(from_logits</span><span id="textcolor245"><span>=</span></span><span id="textcolor246"><span>True</span></span><span>),</span> <span id="x1-45194r4"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> </span><span> metrics</span><span id="textcolor247"><span>=</span></span><span>[</span><span id="textcolor248"><span>'accuracy'</span></span><span>])</span></code></pre>
<p>Because of transfer learning, just fitting our model for three epochs leads to a validation accuracy of about 99 percent:</p>
<pre id="fancyvrb17" class="fancyvrb"><span id="x1-45197r1"></span> 
<code><span>model</span><span id="textcolor249"><span>.</span></span><span>fit(train_dataset,</span><span> epochs</span><span id="textcolor250"><span>=</span></span><span id="textcolor251"><span>3</span></span><span>,</span><span> validation_data</span><span id="textcolor252"><span>=</span></span><span>validation_dataset)</span></code></pre>
<p>Let’s also test our model on the test set of this dataset. We first prepare our dataset:</p>
<pre id="fancyvrb18" class="fancyvrb"><span id="x1-45209r1"></span> 
<code><span>df_test</span><span> </span><span id="textcolor253"><span>=</span></span><span> pd</span><span id="textcolor254"><span>.</span></span><span>read_csv(</span><span id="textcolor255"><span>"oxford-iiit-pet/annotations/test.txt"</span></span><span>,</span><span> sep</span><span id="textcolor256"><span>=</span></span><span id="textcolor257"><span>"</span><span> "</span></span><span>)</span> <span id="x1-45211r2"></span> </code>
<code><span>df_test</span><span id="textcolor258"><span>.</span></span><span>columns</span><span> </span><span id="textcolor259"><span>=</span></span><span> [</span><span id="textcolor260"><span>"path"</span></span><span>,</span><span> </span><span id="textcolor261"><span>"species"</span></span><span>,</span><span> </span><span id="textcolor262"><span>"breed"</span></span><span>,</span><span> </span><span id="textcolor263"><span>"ID"</span></span><span>]</span> <span id="x1-45213r3"></span> </code>
<code><span>df_test[</span><span id="textcolor264"><span>"breed"</span></span><span>]</span><span> </span><span id="textcolor265"><span>=</span></span><span> df_test</span><span id="textcolor266"><span>.</span></span><span>breed</span><span id="textcolor267"><span>.</span></span><span>apply(</span><span id="textcolor268"><span>lambda</span></span><span> x:</span><span> x</span><span> </span><span id="textcolor269"><span>-</span></span><span> </span><span id="textcolor270"><span>1</span></span><span>)</span> <span id="x1-45215r4"></span> </code>
<code><span>df_test[</span><span id="textcolor271"><span>"path"</span></span><span>]</span><span> </span><span id="textcolor272"><span>=</span></span><span> df_test[</span><span id="textcolor273"><span>"path"</span></span><span>]</span><span id="textcolor274"><span>.</span></span><span>apply(</span> <span id="x1-45217r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor275"><span>lambda</span></span><span> x:</span><span> </span><span id="textcolor276"><span>f</span></span><span id="textcolor277"><span>"/content/oxford-iiit-pet/images/</span></span><span id="textcolor278"><span>{</span></span><span>x</span><span id="textcolor279"><span>}</span></span><span id="textcolor280"><span>.jpg"</span></span> <span id="x1-45219r6"></span> </code>
<code><span>)</span> <span id="x1-45221r7"></span> </code>
<code><span id="x1-45223r8"></span></code>
<code><span>test_dataset</span><span> </span><span id="textcolor281"><span>=</span></span><span> tf</span><span id="textcolor282"><span>.</span></span><span>data</span><span id="textcolor283"><span>.</span></span><span>Dataset</span><span id="textcolor284"><span>.</span></span><span>from_tensor_slices(</span> <span id="x1-45225r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> (df_test[</span><span id="textcolor285"><span>"path"</span></span><span>],</span><span> df_test[</span><span id="textcolor286"><span>"breed"</span></span><span>])</span> <span id="x1-45227r10"></span> </code>
<code><span>)</span><span id="textcolor287"><span>.</span></span><span>map(</span><span id="textcolor288"><span>lambda</span></span><span> x,</span><span> y:</span><span> divprocess(x,</span><span> y))</span><span id="textcolor289"><span>.</span></span><span>batch(</span><span id="textcolor290"><span>256</span></span><span>)</span></code></pre>
<p>We can then feed the dataset to our trained model. We obtain a test set accuracy of about 98.3 percent:</p>
<pre id="fancyvrb19" class="fancyvrb"><span id="x1-45237r1"></span> 
<code><span>test_predictions</span><span> </span><span id="textcolor291"><span>=</span></span><span> model</span><span id="textcolor292"><span>.</span></span><span>predict(test_dataset)</span> <span id="x1-45239r2"></span> </code>
<code><span>softmax_scores</span><span> </span><span id="textcolor293"><span>=</span></span><span> tf</span><span id="textcolor294"><span>.</span></span><span>nn</span><span id="textcolor295"><span>.</span></span><span>softmax(test_predictions,</span><span> axis</span><span id="textcolor296"><span>=</span></span><span id="textcolor297"><span>1</span></span><span>)</span> <span id="x1-45241r3"></span> </code>
<code><span>df_test[</span><span id="textcolor298"><span>"predicted_label"</span></span><span>]</span><span> </span><span id="textcolor299"><span>=</span></span><span> tf</span><span id="textcolor300"><span>.</span></span><span>argmax(softmax_scores,</span><span> axis</span><span id="textcolor301"><span>=</span></span><span id="textcolor302"><span>1</span></span><span>)</span> <span id="x1-45243r4"></span> </code>
<code><span>df_test[</span><span id="textcolor303"><span>"prediction_correct"</span></span><span>]</span><span> </span><span id="textcolor304"><span>=</span></span><span> df_test</span><span id="textcolor305"><span>.</span></span><span>apply(</span> <span id="x1-45245r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor306"><span>lambda</span></span><span> x:</span><span> x</span><span id="textcolor307"><span>.</span></span><span>predicted_label</span><span> </span><span id="textcolor308"><span>==</span></span><span> x</span><span id="textcolor309"><span>.</span></span><span>breed,</span><span> axis</span><span id="textcolor310"><span>=</span></span><span id="textcolor311"><span>1</span></span> <span id="x1-45247r6"></span> </code>
<code><span>)</span> <span id="x1-45249r7"></span> </code>
<code><span>accuracy</span><span> </span><span id="textcolor312"><span>=</span></span><span> df_test</span><span id="textcolor313"><span>.</span></span><span>prediction_correct</span><span id="textcolor314"><span>.</span></span><span>value_counts(</span><span id="textcolor315"><span>True</span></span><span>)[</span><span id="textcolor316"><span>True</span></span><span>]</span> <span id="x1-45251r8"></span> </code>
<code><span id="textcolor317"><span>print</span></span><span>(accuracy)</span></code></pre>
<p>We now have a model that is pretty good at classifying cats and dogs. But what will happen if we give this model an image that is neither a cat or a dog?<span id="dx1-45252"></span> Ideally, the model should recognize that this image is not part of the data distribution and should output close to a uniform distribution. Let’s see if this actually happens in practice. We feed our model some images from the ImageNet dataset – the actual dataset that was used to pre-train our model. The ImageNet dataset is large. That is why we download a subset of the dataset: a dataset called Imagenette. This dataset contains just 10 out of the 1,000 classes of the original ImageNet dataset:</p>
<pre id="fancyvrb20" class="fancyvrb"><span id="x1-45257r1"></span> 
<code><span>curl</span><span> -X</span><span> GET</span><span> https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz</span><span> </span><span id="textcolor318"><span>\</span></span> <span id="x1-45259r2"></span> </code>
<code><span>--output</span><span> imagenette.tgz</span> <span id="x1-45261r3"></span> </code>
<code><span>tar</span><span> -xzf</span><span> imagenette.tgz</span></code></pre>
<p>We then take an image from the <code>parachute</code> class:</p>
<pre id="fancyvrb21" class="fancyvrb"><span id="x1-45270r1"></span> 
<code><span>image_path</span><span> </span><span id="textcolor319"><span>=</span></span><span> </span><span id="textcolor320"><span>"imagenette-160/val/n03888257/ILSVRC2012_val_00018229.JPEG"</span></span> <span id="x1-45272r2"></span> </code>
<code><span>image</span><span> </span><span id="textcolor321"><span>=</span></span><span> divprocess_image(image_path)</span><span id="textcolor322"><span>.</span></span><span>numpy()</span> <span id="x1-45274r3"></span> </code>
<code><span>plt</span><span id="textcolor323"><span>.</span></span><span>figure(figsize</span><span id="textcolor324"><span>=</span></span><span>(</span><span id="textcolor325"><span>5</span></span><span>,</span><span id="textcolor326"><span>5</span></span><span>))</span> <span id="x1-45276r4"></span> </code>
<code><span>plt</span><span id="textcolor327"><span>.</span></span><span>imshow(image</span><span id="textcolor328"><span>.</span></span><span>astype(</span><span id="textcolor329"><span>int</span></span><span>))</span> <span id="x1-45278r5"></span> </code>
<code><span>plt</span><span id="textcolor330"><span>.</span></span><span>axis(</span><span id="textcolor331"><span>"off"</span></span><span>)</span> <span id="x1-45280r6"></span> </code>
<code><span>plt</span><span id="textcolor332"><span>.</span></span><span>show()</span></code></pre>
<p>The image clearly does not contain a dog or a cat; it is obviously out-of-distribution:</p>
<div class="IMG---Figure">
<img src="../media/file73.png" alt="PIC"/> <span id="x1-45281r12"></span> <span id="x1-45282"></span></div>
<p class="IMG---Caption">Figure 3.12: An image of a parachute from the ImageNet dataset 
</p>
<p>We run the image through our model and print the score:</p>
<pre id="fancyvrb22" class="fancyvrb"><span id="x1-45288r1"></span> 
<code><span>logits</span><span> </span><span id="textcolor333"><span>=</span></span><span> model</span><span id="textcolor334"><span>.</span></span><span>predict(tf</span><span id="textcolor335"><span>.</span></span><span>expand_dims(image,</span><span> </span><span id="textcolor336"><span>0</span></span><span>))</span> <span id="x1-45290r2"></span> </code>
<code><span>dog_score</span><span> </span><span id="textcolor337"><span>=</span></span><span> tf</span><span id="textcolor338"><span>.</span></span><span>nn</span><span id="textcolor339"><span>.</span></span><span>softmax(logits,</span><span> axis</span><span id="textcolor340"><span>=</span></span><span id="textcolor341"><span>1</span></span><span>)[</span><span id="textcolor342"><span>0</span></span><span>][</span><span id="textcolor343"><span>1</span></span><span>]</span><span id="textcolor344"><span>.</span></span><span>numpy()</span> <span id="x1-45292r3"></span> </code>
<code><span id="textcolor345"><span>print</span></span><span>(</span><span id="textcolor346"><span>f</span></span><span id="textcolor347"><span>"Image</span><span> classified</span><span> as</span><span> a</span><span> dog</span><span> with</span><span> </span></span><span id="textcolor348"><span>{</span></span><span>dog_score</span><span id="textcolor349"><span>:</span></span><span id="textcolor350"><span>.4%</span></span><span id="textcolor351"><span>}</span></span><span id="textcolor352"><span> confidence"</span></span><span>)</span> <span id="x1-45294r4"></span> </code>
<code><span id="textcolor353"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> output:</span><span class="cmitt-10x-x-109"> Image</span><span class="cmitt-10x-x-109"> classified</span><span class="cmitt-10x-x-109"> as</span><span class="cmitt-10x-x-109"> a</span><span class="cmitt-10x-x-109"> dog</span><span class="cmitt-10x-x-109"> with</span><span class="cmitt-10x-x-109"> 99.8226%</span><span class="cmitt-10x-x-109"> confidence</span></span></code></pre>
<p>We can see that the model classifies the image of a parachute as a dog with more than 99% confidence.</p>
<p>We can test the model’s performance on the ImageNet <code>parachute</code> class more systematically as well. Let’s run all the parachute images from the train split through the model and plot a histogram of the scores of the <code>dog</code> class.</p>
<p>We first create a small function to create special dataset with all the parachute images:</p>
<pre id="fancyvrb23" class="fancyvrb"><span id="x1-45309r1"></span> 
<code><span id="textcolor354"><span>from</span></span><span> </span><span id="textcolor355"><span>pathlib</span></span><span> </span><span id="textcolor356"><span>import</span></span><span> Path</span> <span id="x1-45311r2"></span> </code>
<code><span id="x1-45313r3"></span></code>
<code><span>parachute_image_dir</span><span> </span><span id="textcolor357"><span>=</span></span><span> Path(</span><span id="textcolor358"><span>"imagenette-160/train/n03888257"</span></span><span>)</span> <span id="x1-45315r4"></span> </code>
<code><span>parachute_image_paths</span><span> </span><span id="textcolor359"><span>=</span></span><span> [</span> <span id="x1-45317r5"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor360"><span>str</span></span><span>(filepath)</span><span> </span><span id="textcolor361"><span>for</span></span><span> filepath</span><span> </span><span id="textcolor362"><span>in</span></span><span> parachute_image_dir</span><span id="textcolor363"><span>.</span></span><span>iterdir()</span> <span id="x1-45319r6"></span> </code>
<code><span>]</span> <span id="x1-45321r7"></span> </code>
<code><span>parachute_dataset</span><span> </span><span id="textcolor364"><span>=</span></span><span> (tf</span><span id="textcolor365"><span>.</span></span><span>data</span><span id="textcolor366"><span>.</span></span><span>Dataset</span><span id="textcolor367"><span>.</span></span><span>from_tensor_slices(parachute_image_paths)</span> <span id="x1-45323r8"></span> </code>
<code><span id="textcolor368"><span>.</span></span><span>map(</span><span id="textcolor369"><span>lambda</span></span><span> x:</span><span> divprocess_image(x))</span> <span id="x1-45325r9"></span> </code>
<code><span id="textcolor370"><span>.</span></span><span>batch(</span><span id="textcolor371"><span>256</span></span><span>)</span> <span id="x1-45327r10"></span> </code>
<code><span id="textcolor372"><span>.</span></span><span>divfetch(buffer_size</span><span id="textcolor373"><span>=</span></span><span>AUTOTUNE))</span></code></pre>
<p>We can then feed the dataset to our model and create a list of all the softmax scores related to the <code>dog</code> class:</p>
<pre id="fancyvrb24" class="fancyvrb"><span id="x1-45332r1"></span> 
<code><span>Predictions</span><span> </span><span id="textcolor374"><span>=</span></span><span> model</span><span id="textcolor375"><span>.</span></span><span>predict(parachute_dataset)</span> <span id="x1-45334r2"></span> </code>
<code><span>dog_scores</span><span> </span><span id="textcolor376"><span>=</span></span><span> tf</span><span id="textcolor377"><span>.</span></span><span>nn</span><span id="textcolor378"><span>.</span></span><span>softmax(predictions,</span><span> axis</span><span id="textcolor379"><span>=</span></span><span id="textcolor380"><span>1</span></span><span>)[:,</span><span> </span><span id="textcolor381"><span>1</span></span><span>]</span></code></pre>
<p>We can then plot a histogram with these scores – this shows us the distribution of softmax scores:</p>
<pre id="fancyvrb25" class="fancyvrb"><span id="x1-45342r1"></span> 
<code><span>plt</span><span id="textcolor382"><span>.</span></span><span>rcParams</span><span id="textcolor383"><span>.</span></span><span>update({</span><span id="textcolor384"><span>'font.size'</span></span><span>:</span><span> </span><span id="textcolor385"><span>22</span></span><span>})</span> <span id="x1-45344r2"></span> </code>
<code><span>plt</span><span id="textcolor386"><span>.</span></span><span>figure(figsize</span><span id="textcolor387"><span>=</span></span><span>(</span><span id="textcolor388"><span>10</span></span><span>,</span><span id="textcolor389"><span>5</span></span><span>))</span> <span id="x1-45346r3"></span> </code>
<code><span>plt</span><span id="textcolor390"><span>.</span></span><span>hist(dog_scores,</span><span> bins</span><span id="textcolor391"><span>=</span></span><span id="textcolor392"><span>10</span></span><span>)</span> <span id="x1-45348r4"></span> </code>
<code><span>plt</span><span id="textcolor393"><span>.</span></span><span>xticks(tf</span><span id="textcolor394"><span>.</span></span><span>range(</span><span id="textcolor395"><span>0</span></span><span>,</span><span> </span><span id="textcolor396"><span>1.1</span></span><span>,</span><span> </span><span id="textcolor397"><span>0.1</span></span><span>))</span> <span id="x1-45350r5"></span> </code>
<code><span>plt</span><span id="textcolor398"><span>.</span></span><span>grid()</span> <span id="x1-45352r6"></span> </code>
<code><span>plt</span><span id="textcolor399"><span>.</span></span><span>show()</span></code></pre>
<p>Ideally, we want these scores to be distributed close to 0.5 as the images in this dataset are neither dogs nor cats; the model should be very uncertain:</p>
<div class="IMG---Figure">
<img src="../media/file74.png" alt="PIC"/> <span id="x1-45353r13"></span> <span id="x1-45354"></span></div>
<p class="IMG---Caption">Figure 3.13: Distribution of softmax scores on the parachute dataset 
</p>
<p>However, we see something very different. More than 800 images are classified as a dog with at least 90% confidence. Our model clearly does not know how to handle out-of-distribution<span id="dx1-45355"></span> data. <span id="x1-45356r81"></span></p>
</section>
<section id="susceptibility-to-adversarial-manipulations" class="level3 subsectionHead" data-number="8.4.4">
<h3 class="subsectionHead" data-number="8.4.4" id="sigil_toc_id_42"><span class="titlemark">3.4.4 </span> <span id="x1-460004"></span>Susceptibility to adversarial manipulations</h3>
<p>One other vulnerability of most neural networks is that they are susceptible to adversarial attacks<span id="dx1-46001"></span>. Adversarial attacks, simply put, are ways to fool a deep learning system, most often in ways that would not fool humans. These attacks can be harmless or harmful. Here are some examples of adversarial attacks:</p>
<ul>
<li><p>A classifier can detect different types of animals. It classifies an image of a panda as a panda with 57.7% confidence. By slightly perturbing the image in a way that is invisible to humans, the image is now classified as a gibbon with 93.3% confidence.</p></li>
<li><p>A model can detect whether a movie recommendation is positive or negative. It classifies a given movie as negative. By changing a word that does not change the overall tone of the review, for example, from ”surprising” to ”astonishing,” the prediction of the model can change from a negative to a positive recommendation.</p></li>
<li><p>A stop sign detector can detect stop signs. However, by putting a relatively small sticker on top of the stop sign, the model no longer recognises the stop sign.</p></li>
</ul>
<p>These examples show that there are different kinds of adversarial attacks. A useful way to categorize adversarial attacks, is by trying to determine how much information about the model is available to the human (or machine) attacking the model. An attacker can always feed an input to the model, but what the model returns or how the attacker can inspect the model varies. With this lens, we can see the following categories:</p>
<ul>
<li><p><em>Hard-label black box</em>: The attacker only has access to the labels resulting from feeding the model an input.</p></li>
<li><p><em>Soft-label black box</em>: The attacker has access to the scores and the labels of the model.</p></li>
<li><p><em>White box</em> setting: The attacker has full access to the model. They can access the weights and can see the scores, the structure of the model, and so on.</p></li>
</ul>
<p>You can imagine that these different settings make it more or less difficult to attack a model. If people who want to fool a model can only see the label resulting from an input, they cannot be sure that a small change of the input will lead to a difference in the model’s behavior as long as the label stays the same. This becomes easier when they have access to the model’s label and scores. They can then see if a change in the input increases or decreases the confidence of the model. As a result, they can more systematically try to change our input, in ways that decrease the model’s confidence in the label. This might make it possible to find a vulnerability of the model, given that there is enough time to change the input in an iterative fashion. Now, when someone has full access to the model (white box setting), finding a vulnerability might become even easier. They can now use more information to guide the change of the image, such as the gradient of the loss with respect to the input image.</p>
<p>The amount of information available to an attacker is not the only way to distinguish between different kinds of adversarial attacks; there are many types of attacks. For example, in the context of attacks on vision models, some attacks are based on single-patch adjustments of an image (or even single pixels!) while other attacks will change an entire image. Some attacks are specific to a single model, some attacks can be applied to multiple models. We can also distinguish between attacks that digitally manipulate an image and attacks that can be applied in the real world to fool a model, or attacks that are visible by the human eye and attacks that are not. As a result of the wide variety of attacks, the research literature about this topic is still very active – there are always new ways to attack a model, and subsequently a need to find defenses for these attacks.</p>
<p>In the section about out-of-distribution data, we trained a model to determine whether a given image is a cat or a dog. We saw that the classifier worked well: it achieved a test accuracy of about 98.3 percent. Is this model robust to adversarial attacks? Let’s create an attack to find out. We will use the <strong>fast-gradient sign method</strong> (<strong>FGSM</strong>)<span id="dx1-46002"></span> to slightly perturb an image of a dog and make the model think it is actually an image of a cat. The fast-gradient sign method was introduced in 2014 by Ian Goodfellow et al. and remains one of the most famous examples of an adversarial attack. This is probably because of its simplicity; we will see that we will only need a few lines of code to create such an attack. Moreover, the results of this attack are astounding – Goodfellow himself mentioned that he could not really believe the results when he first tested this attack and he had to verify that the perturbed, adversarial<span id="dx1-46003"></span> image he fed to the model was actually different from the original input image.</p>
<p>To create an effective adversarial image, we have to make sure that the pixels in the image change – but only by so much that the change does not become apparent to the human eye. If we perturb the pixels in our image of a dog in such a way that the image now actually looks like a cat, then the model is not mistaken if it classifies that image as a cat. We make sure that we do not perturb the image too much by constraining the perturbation by the max norm – this essentially tells us that no pixel in the image can change by more than some amount <em>𝜖</em>:</p>
<div class="math-display">
<img src="../media/file75.jpg" class="math-display" alt="∥˜x − x∥∞ ≤ 𝜖 "/>
</div>
<p>Where <span class="accenttilde"><em>x</em></span> is our perturbed image and <em>x</em> our original input image.</p>
<p>Now, to create our adversarial example in the fast-gradient sign method, we use the gradient of the loss with respect to our input image to create a new image. Instead of minimizing the loss as we want to do in gradient descent, we now want to maximize the loss. Given our network weights <em>𝜃</em>, input <em>x</em>, label <em>y</em>, and <em>J</em> as a function to compute our loss, we can create an adversarial image by perturbing the image in the following way:</p>
<div class="math-display">
<img src="../media/file76.jpg" class="math-display" alt="η = 𝜖sgn(∇xJ (𝜃,x,y)) "/>
</div>
<p>In this equation, we compute the sign of the gradient of the loss with respect to the input, i.e. determine whether the gradient is positive (1), negative (-1) or 0. The sign enforces the max norm constraint, and by multiplying it by epsilon, we make sure that our perturbations are small – computing the sign just tells us that if we want to add or subtract epsilon in order to perturb the image in a way that hurts the model’s performance on the image. <em>η</em> is now the perturbation that we want to add to our image:</p>
<div class="math-display">
<img src="../media/file77.jpg" class="math-display" alt="˜x = x+ η "/>
</div>
<p>Let’s see what this looks like in Python. Given our trained network that classifies images as either dogs or cats, we can create a function that creates a perturbation that, when multiplied by epsilon and added to our image, creates an adversarial attack:</p>
<pre id="fancyvrb26" class="fancyvrb"><span id="x1-46018r1"></span> 
<code><span id="textcolor400"><span>import</span></span><span> </span><span id="textcolor401"><span>tensorflow</span></span><span> </span><span id="textcolor402"><span>as</span></span><span> </span><span id="textcolor403"><span>tf</span></span> <span id="x1-46020r2"></span> </code>
<code><span id="x1-46022r3"></span></code>
<code><span>loss_object</span><span> </span><span id="textcolor404"><span>=</span></span><span> tf</span><span id="textcolor405"><span>.</span></span><span>keras</span><span id="textcolor406"><span>.</span></span><span>losses</span><span id="textcolor407"><span>.</span></span><span>BinaryCrossentropy()</span> <span id="x1-46024r4"></span> </code>
<code><span id="x1-46026r5"></span></code>
<code><span id="textcolor408"><span>def</span></span><span> </span><span id="textcolor409"><span>get_adversarial_perturbation</span></span><span>(image,</span><span> label):</span> <span id="x1-46028r6"></span> </code>
<code><span> </span><span> image</span><span> </span><span id="textcolor410"><span>=</span></span><span> tf</span><span id="textcolor411"><span>.</span></span><span>expand_dims(image,</span><span> </span><span id="textcolor412"><span>0</span></span><span>)</span> <span id="x1-46030r7"></span> </code>
<code><span> </span><span> </span><span id="textcolor413"><span>with</span></span><span> tf</span><span id="textcolor414"><span>.</span></span><span>GradientTape()</span><span> </span><span id="textcolor415"><span>as</span></span><span> tape:</span> <span id="x1-46032r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> tape</span><span id="textcolor416"><span>.</span></span><span>watch(image)</span> <span id="x1-46034r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> prediction</span><span> </span><span id="textcolor417"><span>=</span></span><span> model(image)</span> <span id="x1-46036r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> loss</span><span> </span><span id="textcolor418"><span>=</span></span><span> loss_object(label,</span><span> prediction)</span> <span id="x1-46038r11"></span> </code>
<code><span id="x1-46040r12"></span></code>
<code><span> </span><span> gradient</span><span> </span><span id="textcolor419"><span>=</span></span><span> tape</span><span id="textcolor420"><span>.</span></span><span>gradient(loss,</span><span> image)</span> <span id="x1-46042r13"></span> </code>
<code><span> </span><span> </span><span id="textcolor421"><span>return</span></span><span> tf</span><span id="textcolor422"><span>.</span></span><span>sign(gradient)[</span><span id="textcolor423"><span>0</span></span><span>]</span></code></pre>
<p>We then create a small function that runs an input image through our model<span id="dx1-46043"></span> and returns the confidence of the model that the image contains a dog:</p>
<pre id="fancyvrb27" class="fancyvrb"><span id="x1-46050r1"></span> 
<code><span id="textcolor424"><span>def</span></span><span> </span><span id="textcolor425"><span>get_dog_score</span></span><span>(image)</span><span> </span><span id="textcolor426"><span>-</span><em>&gt;</em></span><span> </span><span id="textcolor427"><span>float</span></span><span>:</span> <span id="x1-46052r2"></span> </code>
<code><span> </span><span> scores</span><span> </span><span id="textcolor428"><span>=</span></span><span> tf</span><span id="textcolor429"><span>.</span></span><span>nn</span><span id="textcolor430"><span>.</span></span><span>softmax(</span> <span id="x1-46054r3"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span> </span><span> model</span><span id="textcolor431"><span>.</span></span><span>predict(np</span><span id="textcolor432"><span>.</span></span><span>expand_dims(image,</span><span> </span><span id="textcolor433"><span>0</span></span><span>)),</span><span> axis</span><span id="textcolor434"><span>=</span></span><span id="textcolor435"><span>1</span></span> <span id="x1-46056r4"></span> </code>
<code><span> </span><span> )</span><span id="textcolor436"><span>.</span></span><span>numpy()[</span><span id="textcolor437"><span>0</span></span><span>]</span> <span id="x1-46058r5"></span> </code>
<code><span> </span><span> </span><span id="textcolor438"><span>return</span></span><span> scores[</span><span id="textcolor439"><span>1</span></span><span>]</span></code></pre>
<p>We download an image of a cat:</p>
<pre id="fancyvrb28" class="fancyvrb"><span id="x1-46062r1"></span> 
<code><span>curl</span><span> https://images.pexels.com/photos/1317844/pexels-photo-1317844.jpeg</span><span> </span><em>&gt;</em><span> </span><span id="textcolor440"><span>\</span></span> <span id="x1-46064r2"></span> </code>
<code><span>cat.png</span></code></pre>
<p>Then, we pre-process it so we can feed it to our model. We set the label to 0, which corresponds to the <code>cat</code> label:</p>
<pre id="fancyvrb29" class="fancyvrb"><span id="x1-46069r1"></span> 
<code><span id="textcolor441"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109"> divprocess</span><span class="cmitt-10x-x-109"> function</span><span class="cmitt-10x-x-109"> defined</span><span class="cmitt-10x-x-109"> in</span><span class="cmitt-10x-x-109"> the</span><span class="cmitt-10x-x-109"> out-of-distribution</span><span class="cmitt-10x-x-109"> section</span></span> <span id="x1-46071r2"></span> </code>
<code><span>image,</span><span> label</span><span> </span><span id="textcolor442"><span>=</span></span><span> divprocess(</span><span id="textcolor443"><span>"cat.png"</span></span><span>,</span><span> </span><span id="textcolor444"><span>0</span></span><span>)</span></code></pre>
<p>We can perturb our image:</p>
<pre id="fancyvrb30" class="fancyvrb"><span id="x1-46076r1"></span> 
<code><span>epsilon</span><span> </span><span id="textcolor445"><span>=</span></span><span> </span><span id="textcolor446"><span>0.05</span></span> <span id="x1-46078r2"></span> </code>
<code><span>perturbation</span><span> </span><span id="textcolor447"><span>=</span></span><span> get_adversarial_perturbation(image,</span><span> label)</span> <span id="x1-46080r3"></span> </code>
<code><span>image_perturbed</span><span> </span><span id="textcolor448"><span>=</span></span><span> image</span><span> </span><span id="textcolor449"><span>+</span></span><span> epsilon</span><span> </span><span id="textcolor450"><span>*</span></span><span> perturbation</span></code></pre>
<p>Let’s now get the confidence of the model that the original image is a cat, and the confidence of the model that the perturbed image is a dog:</p>
<pre id="fancyvrb31" class="fancyvrb"><span id="x1-46084r1"></span> 
<code><span>cat_score_original_image</span><span> </span><span id="textcolor451"><span>=</span></span><span> </span><span id="textcolor452"><span>1</span></span><span> </span><span id="textcolor453"><span>-</span></span><span> get_dog_score(image)</span> <span id="x1-46086r2"></span> </code>
<code><span>dog_score_perturbed_image</span><span> </span><span id="textcolor454"><span>=</span></span><span> get_dog_score(image_perturbed)</span></code></pre>
<p>With this in place, we can create the following plot, showing the original image, the perturbation<span id="dx1-46087"></span> applied to the image, and the perturbed image:</p>
<pre id="fancyvrb32" class="fancyvrb"><span id="x1-46118r1"></span> 
<code><span id="textcolor455"><span>import</span></span><span> </span><span id="textcolor456"><span>matplotlib.pyplot</span></span><span> </span><span id="textcolor457"><span>as</span></span><span> </span><span id="textcolor458"><span>plt</span></span> <span id="x1-46120r2"></span> </code>
<code><span id="x1-46122r3"></span></code>
<code><span>ax</span><span> </span><span id="textcolor459"><span>=</span></span><span> plt</span><span id="textcolor460"><span>.</span></span><span>subplots(</span><span id="textcolor461"><span>1</span></span><span>,</span><span> </span><span id="textcolor462"><span>3</span></span><span>,</span><span> figsize</span><span id="textcolor463"><span>=</span></span><span>(</span><span id="textcolor464"><span>20</span></span><span>,</span><span id="textcolor465"><span>10</span></span><span>))[</span><span id="textcolor466"><span>1</span></span><span>]</span> <span id="x1-46124r4"></span> </code>
<code><span>[ax</span><span id="textcolor467"><span>.</span></span><span>set_axis_off()</span><span> </span><span id="textcolor468"><span>for</span></span><span> ax</span><span> </span><span id="textcolor469"><span>in</span></span><span> ax</span><span id="textcolor470"><span>.</span></span><span>ravel()]</span> <span id="x1-46126r5"></span> </code>
<code><span>ax[</span><span id="textcolor471"><span>0</span></span><span>]</span><span id="textcolor472"><span>.</span></span><span>imshow(image</span><span id="textcolor473"><span>.</span></span><span>numpy()</span><span id="textcolor474"><span>.</span></span><span>astype(</span><span id="textcolor475"><span>int</span></span><span>))</span> <span id="x1-46128r6"></span> </code>
<code><span>ax[</span><span id="textcolor476"><span>0</span></span><span>]</span><span id="textcolor477"><span>.</span></span><span>title</span><span id="textcolor478"><span>.</span></span><span>set_text(</span><span id="textcolor479"><span>"Original</span><span> image"</span></span><span>)</span> <span id="x1-46130r7"></span> </code>
<code><span>ax[</span><span id="textcolor480"><span>0</span></span><span>]</span><span id="textcolor481"><span>.</span></span><span>text(</span> <span id="x1-46132r8"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor482"><span>0.5</span></span><span>,</span> <span id="x1-46134r9"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor483"><span>-</span></span><span id="textcolor484"><span>.1</span></span><span>,</span> <span id="x1-46136r10"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor485"><span>f</span></span><span id="textcolor486"><span>"</span></span><span id="textcolor487"><span>\"</span></span><span id="textcolor488"><span>Cat</span></span><span id="textcolor489"><span>\"\n</span></span><span id="textcolor490"><span> </span></span><span id="textcolor491"><span>{</span></span><span>cat_score</span><span id="textcolor492"><span>:</span></span><span id="textcolor493"><span>.2%</span></span><span id="textcolor494"><span>}</span></span><span id="textcolor495"><span> confidence"</span></span><span>,</span> <span id="x1-46138r11"></span> </code>
<code><span> </span><span> </span><span> </span><span> size</span><span id="textcolor496"><span>=</span></span><span id="textcolor497"><span>12</span></span><span>,</span> <span id="x1-46140r12"></span> </code>
<code><span> </span><span> </span><span> </span><span> ha</span><span id="textcolor498"><span>=</span></span><span id="textcolor499"><span>"center"</span></span><span>,</span> <span id="x1-46142r13"></span> </code>
<code><span> </span><span> </span><span> </span><span> transform</span><span id="textcolor500"><span>=</span></span><span>ax[</span><span id="textcolor501"><span>0</span></span><span>]</span><span id="textcolor502"><span>.</span></span><span>transAxes</span> <span id="x1-46144r14"></span> </code>
<code><span>)</span> <span id="x1-46146r15"></span> </code>
<code><span>ax[</span><span id="textcolor503"><span>1</span></span><span>]</span><span id="textcolor504"><span>.</span></span><span>imshow(perturbations)</span> <span id="x1-46148r16"></span> </code>
<code><span>ax[</span><span id="textcolor505"><span>1</span></span><span>]</span><span id="textcolor506"><span>.</span></span><span>title</span><span id="textcolor507"><span>.</span></span><span>set_text(</span> <span id="x1-46150r17"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor508"><span>"Perturbation</span><span> added</span><span> to</span><span> the</span><span> image</span></span><span id="textcolor509"><span>\n</span></span><span id="textcolor510"><span>(multiplied</span><span> by</span><span> epsilon)"</span></span> <span id="x1-46152r18"></span> </code>
<code><span>)</span> <span id="x1-46154r19"></span> </code>
<code><span>ax[</span><span id="textcolor511"><span>2</span></span><span>]</span><span id="textcolor512"><span>.</span></span><span>imshow(image_perturbed</span><span id="textcolor513"><span>.</span></span><span>numpy()</span><span id="textcolor514"><span>.</span></span><span>astype(</span><span id="textcolor515"><span>int</span></span><span>))</span> <span id="x1-46156r20"></span> </code>
<code><span>ax[</span><span id="textcolor516"><span>2</span></span><span>]</span><span id="textcolor517"><span>.</span></span><span>title</span><span id="textcolor518"><span>.</span></span><span>set_text(</span><span id="textcolor519"><span>"Perturbed</span><span> image"</span></span><span>)</span> <span id="x1-46158r21"></span> </code>
<code><span>ax[</span><span id="textcolor520"><span>2</span></span><span>]</span><span id="textcolor521"><span>.</span></span><span>text(</span> <span id="x1-46160r22"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor522"><span>0.5</span></span><span>,</span> <span id="x1-46162r23"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor523"><span>-</span></span><span id="textcolor524"><span>.1</span></span><span>,</span> <span id="x1-46164r24"></span> </code>
<code><span> </span><span> </span><span> </span><span> </span><span id="textcolor525"><span>f</span></span><span id="textcolor526"><span>"</span></span><span id="textcolor527"><span>\"</span></span><span id="textcolor528"><span>Dog</span></span><span id="textcolor529"><span>\"\n</span></span><span id="textcolor530"><span> </span></span><span id="textcolor531"><span>{</span></span><span>dog_score</span><span id="textcolor532"><span>:</span></span><span id="textcolor533"><span>.2%</span></span><span id="textcolor534"><span>}</span></span><span id="textcolor535"><span> confidence"</span></span><span>,</span> <span id="x1-46166r25"></span> </code>
<code><span> </span><span> </span><span> </span><span> size</span><span id="textcolor536"><span>=</span></span><span id="textcolor537"><span>12</span></span><span>,</span> <span id="x1-46168r26"></span> </code>
<code><span> </span><span> </span><span> </span><span> ha</span><span id="textcolor538"><span>=</span></span><span id="textcolor539"><span>"center"</span></span><span>,</span> <span id="x1-46170r27"></span> </code>
<code><span> </span><span> </span><span> </span><span> transform</span><span id="textcolor540"><span>=</span></span><span>ax[</span><span id="textcolor541"><span>2</span></span><span>]</span><span id="textcolor542"><span>.</span></span><span>transAxes</span> <span id="x1-46172r28"></span> </code>
<code><span>)</span> <span id="x1-46174r29"></span> </code>
<code><span>plt</span><span id="textcolor543"><span>.</span></span><span>show()</span></code></pre>
<p><em>Figure 3.14</em> shows both the original image and the perturbed image along with the model prediction for each image.</p>
<div class="IMG---Figure">
<img src="../media/file78.png" alt="PIC"/> <span id="x1-46175r14"></span> <span id="x1-46176"></span></div>
<p class="IMG---Caption">Figure 3.14: Example of an adversarial attack 
</p>
<p>In <em>Figure</em> <a href="#x1-46175r14"><em>3.14</em></a>, we can see that our model initially classifies the image as a cat, with a confidence of 100 percent. After we applied the perturbation<span id="dx1-46177"></span> (shown in the middle) to our initial cat image (shown on the left), our image (on the right) is now classified as a dog with 98.73 percent confidence, although the image visually looks the same as the original input image. We successfully created an adversarial attack that fools our model! <span id="x1-46178r74"></span></p>
</section>
</section>
<section id="summary-2" class="level2 sectionHead" data-number="8.5">
<h2 class="sectionHead" data-number="8.5" id="sigil_toc_id_43"><span class="titlemark">3.5 </span> <span id="x1-470005"></span>Summary</h2>
<p>In this chapter, we have seen different types of common neural networks. First, we discussed the key building blocks of neural networks with a special focus on the multi-layer perceptron. Then we reviewed common neural network architectures: convolutional neural networks, recurrent neural networks, and the attention mechanism. All these components allow us to build very powerful deep learning models that can sometimes achieve super-human performance. However, in the second part of the chapter, we reviewed a few problems of neural networks. We discussed how they can be overconfident, and do not handle out-of-distribution data very well. We also saw how small, imperceptible changes to a neural network’s input can cause the model to make an incorrect prediction.</p>
<p>In the next chapter, we will combine the concepts learned in this chapter and in <a href="#x1-350003"><em>Chapter 3</em></a>, <a href="#x1-350003"><em>Fundamentals of Deep Learning</em></a>, and discuss Bayesian deep learning, which has the potential to overcome some of the challenges of standard neural networks we have seen in this chapter. <span id="x1-47001r86"></span></p>
</section>
<section id="further-reading-1" class="level2 sectionHead" data-number="8.6">
<h2 class="sectionHead" data-number="8.6" id="sigil_toc_id_44"><span class="titlemark">3.6 </span> <span id="x1-480006"></span>Further reading</h2>
<p>There are a lot of great resources to learn more about the essential building blocks of deep learning. Here are just a few popular resources that are a great start:</p>
<ul>
<li><p>Nielsen, M.A., 2015. <em>Neural networks</em> <em>and deep learning</em> (Vol. 25). San Francisco, CA, USA: Determination press., <a href="http://neuralnetworksanddeeplearning.com/" class="url"><span class="No-Break">http://neuralnetworksanddeeplearning.com/</span></a>.</p></li>
<li><p>Chollet, F., 2021. <em>Deep learning with Python</em>. Simon and Schuster.</p></li>
<li><p>Raschka, S., 2015. <em>Python Machine Learning</em>. Packt Publishing Ltd.</p></li>
<li><p>Ng, Andrew, 2022, <em>Deep Learning Specialization</em>. Coursera.</p></li>
<li><p>Johnson, Justin, 2019. EECS 498-007 / 598-005, <em>Deep Learning for</em> <em>Computer Vision</em>. University of Michigan.</p></li>
</ul>
<p>To learn more about the problems of deep learning models, you can read some of the following resources:</p>
<ul>
<li><p>Overconfidence and calibration:</p>
<ul>
<li><p>Guo, C., Pleiss, G., Sun, Y. and Weinberger, K.Q., 2017, July. <em>On calibration of modern neural networks</em>. In International conference on machine learning (pp. 1321-1330). PMLR.</p></li>
<li><p>Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J., Lakshminarayanan, B. and Snoek, J., 2019. <em>Can you</em> <em>trust your model’s uncertainty? evaluating predictive uncertainty</em> <em>under dataset shift.</em> Advances in neural information processing systems, 32.</p></li>
</ul></li>
<li><p>Out-of-distribution detection:</p>
<ul>
<li><p>Hendrycks, D. and Gimpel, K., 2016. <em>A baseline for detecting</em> <em>misclassified and out-of-distribution examples in neural networks.</em> arXiv preprint arXiv:1610.02136.</p></li>
<li><p>Liang, S., Li, Y. and Srikant, R., 2017. <em>Enhancing the reliability</em> <em>of out-of-distribution image detection in neural networks.</em> arXiv preprint arXiv:1706.02690.</p></li>
<li><p>Lee, K., Lee, K., Lee, H. and Shin, J., 2018. <em>A simple</em> <em>unified framework for detecting out-of-distribution samples and</em> <em>adversarial attacks.</em> Advances in neural information processing systems, 31.</p></li>
<li><p>Fort, S., Ren, J. and Lakshminarayanan, B., 2021. <em>Exploring</em> <em>the limits of out-of-distribution detection.</em> Advances in Neural Information Processing Systems, 34, pp.7068-7081.</p></li>
</ul></li>
<li><p>Adversarial attacks:</p>
<ul>
<li><p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. <em>Intriguing properties of</em> <em>neural networks</em>. arXiv preprint arXiv:1312.6199.</p></li>
<li><p>Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. <em>Explaining and</em> <em>harnessing adversarial examples</em>. arXiv preprint arXiv:1412.6572.</p></li>
<li><p>Nicholas Carlini, 2019. <em>Adversarial Machine Learning Reading</em> <em>List</em> <a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html" class="url"><span class="No-Break">https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html</span></a>.</p></li>
</ul></li>
</ul>
<p>You can have a look at the following resources to dive deeper into the topics and experiments covered in this chapter:</p>
<ul>
<li><p>Jasper Snoek, MIT 6.S191: <em>Uncertainty in Deep Learning</em>, January 2022.</p></li>
<li><p>TensorFlow Core Tutorial, <em>Adversarial example using FGSM</em>.</p></li>
<li><p>Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. <em>Explaining and</em> <em>harnessing adversarial examples</em>. arXiv preprint arXiv:1412.6572.</p></li>
<li><p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In <em>International Conference on</em> <em>Machine Learning</em>, pages 1321–1330. PMLR, 2017.</p></li>
<li><p>Stanford University School of Engineering, CS231N, <em>Lecture 16 —</em> <em>Adversarial Examples and Adversarial Training</em>.</p></li>
<li><p>Danilenka, Anastasiya, Maria Ganzha, Marcin Paprzycki, and Jacek Mańdziuk, 2022. <em>Using adversarial images to improve outcomes of</em> <em>federated learning for non-IID data.</em> arXiv preprint arXiv:2206.08124.</p></li>
<li><p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. <em>Intriguing properties of neural</em> <em>networks</em>. arXiv preprint arXiv:1312.6199.</p></li>
<li><p>Sharma, A., Bian, Y., Munz, P. and Narayan, A., 2022. <em>Adversarial</em> <em>Patch Attacks and Defences in Vision-Based Tasks: A Survey</em>. arXiv preprint arXiv:2206.08304.</p></li>
<li><p>Nicholas Carlini, 2019. <em>Adversarial Machine Learning Reading List</em> <a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html" class="url"><span class="No-Break">https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html</span></a>.</p></li>
<li><p>Parkhi, O.M., Vedaldi, A., Zisserman, A. and Jawahar, C.V., 2012, June. <em>Cats and dogs</em>. In 2012 IEEE conference on computer vision and pattern recognition (pp. 3498-3505). IEEE. (Dataset cat vs dog).</p></li>
<li><p>Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. <em>Imagenet: A large-scale hierarchical image database</em>. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248-255). Ieee. (ImageNet dataset).</p></li>
<li><p>Matthew D Zeiler and Rob Fergus. <em>Visualizing and understanding</em> <em>convolutional networks</em>. In European conference on computer vision, pages 818–833. Springer, 2014.</p></li>
</ul>
<p><span id="x1-48001r60"></span></p>
</section>
</section>
</body>
</html>