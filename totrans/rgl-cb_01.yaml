- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Overview of Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s embark on a journey into the world of regularization in machine learning.
    I hope you will learn a lot and find as much joy in reading this book as I did
    in writing it.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is important for any individual willing to deploy robust **machine
    learning** (**ML**) models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce some context and key concepts about regularization
    before diving deeper into it in the next chapters. At this point, you may have
    many questions about this book and about regularization in general. What is regularization?
    Why do we need regularization for production-grade ML models? How do we diagnose
    the need for regularization? What are the limits of regularization? What are the
    approaches to regularization?
  prefs: []
  type: TYPE_NORMAL
- en: All the foundational knowledge about regularization will be provided in this
    chapter in the hope of answering all these questions. Not only will this give
    you a high-level understanding of what regularization is but it will also allow
    you to fully appreciate the methods and techniques proposed in the next chapters
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing intuition about regularization on a toy dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the key concepts of underfitting, overfitting, bias, and variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will have the opportunity to generate a toy dataset, display
    it, and train basic linear regression on that data. Therefore, the following Python
    libraries will be required:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Regularization in ML is a technique used to improve the generalization performance
    of a model by adding additional constraints to the model’s parameters. This forces
    the model to use simpler representations and helps reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization can also help improve the performance of a model on unseen data
    by encouraging the model to learn more relevant, generalizable features.”
  prefs: []
  type: TYPE_NORMAL
- en: 'This definition of regularization, arguably good enough, was actually generated
    by the famous GPT-3 model when given the following prompt: *Detailed definition
    of regularization in machine learning*. Even more astonishing, this definition
    passed several plagiarism tests, meaning it’s actually fully original text. Do
    not worry if you do not yet understand all the words in this definition from GPT-3;
    it is not meant for beginners. But you will fully understand it by the end of
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3**, short for **Generative Pre-trained Transformer 3**, is a 175 billion-parameter
    model proposed by OpenAI and is available for use at [platform.openai.com/playground](https://platform.openai.com/playground).'
  prefs: []
  type: TYPE_NORMAL
- en: You can easily imagine that, to get such a result, not only has GPT-3 been trained
    on a large amount of data but it is really carefully regularized, so that it won’t
    just reproduce a learned text but will instead generate a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what regularization is about: being able to generalize and
    produce acceptable results when faced with an unknown situation.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is regularization so crucial to ML? The key to successfully deploying ML
    in production lies in the model’s ability to effectively adapt to and accommodate
    new data. Once a model is in production, it will not be fed with well-known, average
    input data. Most likely, the production model will face unseen data, exceptional
    scenarios, a drift in feature distribution, or evolving customer behavior. While
    a well-regularized ML model may not guarantee its robustness in handling various
    scenarios, a poorly regularized model is almost certain to encounter failure upon
    its initial deployment in production.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now have a look at a few examples of models that failed during deployment
    in recent years, so that we can fully understand why regularization is so important.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of models that did not pass the deployment test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last few years have been full of examples of models that failed in the first
    days of deployment. According to a Gartner report from 2020 ([https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline](https://www.gartner.com/en/newsroom/press-releases/2020-10-19-gartner-identifies-the-top-strategic-technology-trends-for-2021#:~:text=Gartner%20research%20shows%20only%2053,a%20production%2Dgrade%20AI%20pipeline)),
    more than 50% of AI prototypes will *not* make it to production deployment. Not
    all failures were only due to regularization issues, but some certainly were.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at some failed attempts at deploying models in production
    over the last few years:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon had to stop using its AI recruitment model because it was reportedly
    discriminating against women ([https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe](https://finance.yahoo.com/news/amazon-reportedly-killed-ai-recruitment-100042269.xhtml?guccounter=1&guce_referrer=aHR0cHM6Ly9hbmFseXRpY3NpbmRpYW1hZy5jb20v&guce_referrer_sig=AQAAACNWCozxgjh8_DkmyT59IZEGsn3qlmfu2pVu6IxMu5B0ExzHJVkatUuBmpO3zGcWp-0nvgWJ9yqR9eaQU-20-DvgJzJdR7xj9U8faNpVUTPo00gND-W5WWPh_wGNLNTASitfnb-MnStbjZaNN_O3EbWHDarh0_cAzXza31yeYcEe))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft’s chatbot Tay was shut down after only 16 hours of production after
    posting offensive tweets ([https://en.wikipedia.org/wiki/Tay_(chatbot)](https://en.wikipedia.org/wiki/Tay_(chatbot)))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM’s Watson was providing unsafe cancer treatment recommendations to patients
    ([https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science](https://www.theverge.com/2018/7/26/17619382/ibms-watson-cancer-ai-healthcare-science))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those are just a few examples that made the headlines from tech giants. The
    number of projects that have experienced failure yet remain undisclosed to the
    public is staggering. These failures often involve smaller companies and confidential
    initiatives. But still, there are several lessons to learn from those examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon’s case**: The input data was biased against women, as was the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft’s case**: The model was probably too sensitive to new data since
    it was feeding on new tweets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IBM’s case**: The model was perhaps trained on too much synthetic or unrealistic
    data, and not able to adapt to edge cases and unseen data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization serves as a valuable approach to enhance the success rate of
    ML models in production. Effective regularization techniques can prevent AI recruitment
    models from exhibiting gender biases, either by eliminating certain features or
    incorporating synthetic data. Additionally, proper regularization enables chatbots
    to maintain an appropriate level of sensitivity toward new tweets. It also equips
    models to handle edge cases and previously unseen data proficiently, even when
    trained on synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: As a disclaimer, there may be many other ways to overcome or prevent these kinds
    of failures that are not mutually exclusive with regularization. For example,
    having good-quality data is key. Everyone in the AI field knows the adage *garbage
    in,* *garbage out*.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps (a field that is getting more and more mature every day) and ML engineering
    best practices are also key to success for many projects. Subject matter knowledge
    can sometimes also make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the context of the project, many other parameters may impact the
    success of an ML project, but anything besides regularization is beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the need for regularization for production-level ML models,
    let’s take a step back and gain some intuition about what regularization is with
    a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition about regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization has been defined and mentioned already in this book, but let’s
    try now to develop some intuition about what it really is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider a typical, real-world use case: the real estate price per square
    meter, as a function of the surface of an apartment (or house) in the city of
    Paris. The goal, from a business perspective, is to be able to predict the price
    per square meter, given the apartment’s surface.'
  prefs: []
  type: TYPE_NORMAL
- en: We will first need some imports, as well as a helper function to plot the data
    more conveniently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plot_data()` function simply plots the provided data and adds axis labels
    and a legend if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will now allow us to generate and display our first toy
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Price per square meter as a function of the apartment surface](img/B19629_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Price per square meter as a function of the apartment surface
  prefs: []
  type: TYPE_NORMAL
- en: Even if this is a toy dataset, for the sake of pedagogy, we can assume this
    data would have been collected on the real estate market.
  prefs: []
  type: TYPE_NORMAL
- en: We can notice a downward trend in the price per square meter as the apartment
    surface increases. Indeed, in Paris, the small surfaces are much more in demand
    (perhaps because there are many students or because the price is more affordable).
    That could explain why the price per square meter is actually higher for smaller
    surfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we will omit all the typical ML workflow. Here, we just perform
    a linear regression on this data and display the result with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Price per square meter as a function of the apartment surface
    and resulting fit curve](img/B19629_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Price per square meter as a function of the apartment surface and
    resulting fit curve
  prefs: []
  type: TYPE_NORMAL
- en: Good enough! The fit seems to capture the downward trend. While it’s not so
    close to all the given data samples, the business seems happy about it for the
    moment, as the model performances are within their expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this new model, the company has now acquired a few more clients.
    From those clients, the company collected some new data from larger apartment
    sales, so our dataset now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Updated price per square meter as a function of the apartment
    surface](img/B19629_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Updated price per square meter as a function of the apartment surface
  prefs: []
  type: TYPE_NORMAL
- en: 'This actually changes everything; this is the typical failing deployment test.
    There is no global downward trend anymore: with larger apartment surfaces, the
    price per square meter actually seems to follow an upward trend. One simple business
    explanation could be the following: larger apartments may be less common and thus
    have a higher price.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without confidence, for the sake of trying, we try to reuse the exact same
    method as we did previously: linear regression. The result would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Example of underfitting: the data complexity is not fully captured
    by the model](img/B19629_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4 – Example of underfitting: the data complexity is not fully captured
    by the model'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, linear regression was not able to capture the complexity of the
    data anymore, leading to this situation. This is called **underfitting**; the
    model is not able to fully capture the complexity of the data. Indeed, with just
    the surface as a parameter, the best the model can do is a straight line, which
    is not enough for this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way for linear regression to capture more complexity is to provide more
    features. Given that our current input data is limited to the surface, a potential
    straightforward approach could involve utilizing the raised-to-the-power surface.
    For the sake of this example, let’s take a rather extreme approach and add all
    the features from `power1` to `power15`, and make them fit this dataset. This
    can be done pretty easily with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Example of overfitting: the model is capturing the noise in
    the data](img/B19629_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5 – Example of overfitting: the model is capturing the noise in the
    data'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we could expect by adding so many degrees of freedom in our model, the fit
    is now going exactly through all of the data points. Indeed, without going into
    the mathematical details, the model has more parameters than data points on which
    it trains, and is capable of going through all those points. Is it a good fit,
    though? We can imagine it does not only capture the global trend of the data but
    also the noise. This is called **overfitting**: the model is too close to the
    data, and may not be able to make correct predictions for new, unseen data. Any
    new data point would not be on the curve, and the behavior outside the training
    range is totally unpredictable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a more reasonable approach to this situation would be only taking
    the surface up to the power of 2, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Example of right fitting: the model is capturing the overall
    trend but not the noise](img/B19629_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6 – Example of right fitting: the model is capturing the overall trend
    but not the noise'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fit seems much more acceptable now. It does capture the global trend of
    the data: at first downward for small surfaces, and then upward for larger surfaces.
    Moreover, it does not try to capture the noise coming from all the data points,
    making it more robust for predicting new, unseen data. Finally, the behavior is
    quite predictable beyond the training range (for example, surface < 15![](img/Formula_01_001.png)
    or surface > 40![](img/Formula_01_002.png)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is typically the kind of desired behavior from a good model: neither underfitting
    nor overfitting. Removing some of the raised-to-the-power features allowed our
    model to generalize better; we effectively regularized our model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize this example, we explored several concepts here:'
  prefs: []
  type: TYPE_NORMAL
- en: We visualized examples of underfitting, overfitting, and well-regularized models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adding a raised-to-the-power surface to our features, we were able to go
    from underfitting to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, by removing most of the raised-to-the-power features (keeping only
    square features), we were able to go from overfitting to a well-regularized model,
    effectively adding regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, you now have a good understanding of underfitting, overfitting, and
    regularization, as well as why this is so important in ML. We can now build upon
    this by providing a more formal definition of the key concepts of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having gained some intuition regarding what constitutes a suitable fit, as well
    as understanding examples of underfitting and overfitting, let us now delve into
    a more precise definition and explore key concepts that enable us to better comprehend
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bias and variance are two key concepts when talking about regularization. We
    can define two main kinds of errors a model can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias** is how bad a model is at capturing the general behavior of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance** is how bad a model is at being robust to small input data fluctuations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those two concepts, in general, are not mutually exclusive. If we take a step
    back from ML, there is a very common figure to visualize bias and variance, assuming
    the model’s goal is to hit the center of a target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Visualization of bias and variance](img/B19629_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Visualization of bias and variance
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe those four cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High bias and low variance**: The model is hitting away from the center of
    the target, but in a very consistent manner'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low bias and high variance**: The model is, on average, hitting the center
    of the target, but is quite noisy and inconsistent in doing so'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High bias and high variance**: The model is hitting away from the center
    in a noisy way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low bias and low variance**: The best of both worlds – the model is hitting
    the center of the target consistently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting and overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw a very classic approach to bias and variance definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now, what does that mean in terms of ML? How does that relate to regularization?
    Well, before we get there, we will first revisit bias and variance in a more typical
    ML case: linear regression of real estate prices.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at how a model would behave in all those cases on our data.
  prefs: []
  type: TYPE_NORMAL
- en: High bias and low variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model is robust to data fluctuations but could not capture the high-level
    behavior of the data. Refer to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – High bias and low variance in practice for linear regression](img/B19629_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – High bias and low variance in practice for linear regression
  prefs: []
  type: TYPE_NORMAL
- en: This is *underfitting*, as we faced earlier, in *Figure 1**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: Low bias and high variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model did capture the global behavior of the data, but could not stay robust
    to input data fluctuations. Refer to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Low bias and high variance in practice for linear regression](img/B19629_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Low bias and high variance in practice for linear regression
  prefs: []
  type: TYPE_NORMAL
- en: This is *overfitting*, the case we faced previously, in *Figure 1**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: High bias and high variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model could neither capture the global behavior nor be robust enough to
    input data fluctuations. This case never happens, or high variance is hidden behind
    high bias, but it could look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – High bias and high variance in practice for linear regression:
    this most likely never actually happens on such data](img/B19629_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10 – High bias and high variance in practice for linear regression:
    this most likely never actually happens on such data'
  prefs: []
  type: TYPE_NORMAL
- en: Low bias and low variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model could both capture the global data behavior and be robust enough for
    data fluctuation. This is the end goal when training a model. This is the case
    we faced in *Figure 1**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Low bias and low variance in practice for linear regression:
    the ultimate goal](img/B19629_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11 – Low bias and low variance in practice for linear regression:
    the ultimate goal'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the goal is almost always to get both low bias and low variance,
    even if it’s not always possible. Let’s see how regularization is a means toward
    this goal.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization – from overfitting to underfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In light of all those examples, we can now get a really clear understanding
    of what regularization is.
  prefs: []
  type: TYPE_NORMAL
- en: If we look again at the definition provided by GPT-3, regularization is what
    allows us to prevent a model from overfitting by adding constraints to the model.
    Indeed, *adding regularization allows us to reduce variance* in a model, and therefore,
    to have a less overfitting model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go one step further: what if regularization is added to an already well-trained
    model (that is, low bias and low variance)? In other words, what happens if constraints
    are added to a model that works well? Intuitively, it would degrade the overall
    performance. It would not allow the model to fully grasp the data behavior, and
    thus add bias to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, here comes a fundamental drawback of regularization: **adding regularization
    increases** **model bias**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be summarized in one figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – A high variance model (bottom); the same model with more regularization
    and the right level of fitting (middle); and the same model with even more regularization,
    now underfitting (top)](img/B19629_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – A high variance model (bottom); the same model with more regularization
    and the right level of fitting (middle); and the same model with even more regularization,
    now underfitting (top)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what is called the **bias-variance trade-off**, a very important concept.
    Indeed, adding regularization is always a balance:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to have enough regularization so that the model generalizes well and
    is not sensitive to small data fluctuations and noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to not have too much regularization so that the model is free enough
    to fully capture the complexity of the data in all cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we go further in the book, more and more tools and techniques will be provided
    to diagnose our model and find the right bias-variance balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the chapters, we will see many ways to regularize a model. We think
    of regularization as just adding direct constraints to a model, but there are
    many indirect regularization methods that may help a model better generalize.
    A non-exhaustive list of the existing regularization methods could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding constraints to the model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding constraints to the model training, such as the loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding constraints from the input data by engineering it differently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding constraints from the input by generating more samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other regularization methods may be proposed, but the book will mostly focus
    on those methods for various cases, such as structured and unstructured data,
    linear models, tree-based models, deep learning models, **natural language processing**
    (**NLP**) problems, and computer vision problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'As good as a model can be with the right regularization method, most tasks
    have a hard limit on the possible performances a model can achieve: this is what
    we call **unavoidable bias**. Let’s have a look at what it is.'
  prefs: []
  type: TYPE_NORMAL
- en: Unavoidable bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In almost any task, there is unavoidable bias. For example, in *Figure 1**.13*,
    there are both Shetland Sheepdogs and Rough Collie dogs. Can you say with 100%
    accuracy which is which?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie
    dogs](img/B19629_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Random pictures of both Shetland Sheepdogs and Rough Collie dogs
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding figure, can you tell which is which? Most likely not. If
    you are a trained expert in dogs, you may have a lower error rate. But the odds
    are that given a large enough number of images, you might be wrong with some images.
    The lowest level of error an expert can achieve is what is called **human-level
    error**. In most cases, human-level error gives a very good idea of the lowest
    error a model can achieve. Anytime you are evaluating a model, it is a good idea
    to know (or to wonder) what a human could possibly do on such a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, some tasks are much easier than others:'
  prefs: []
  type: TYPE_NORMAL
- en: Humans perform well at classifying dogs and cats, as does AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humans perform well at classifying songs, as does AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humans perform quite poorly at hiring people (at least, not all recruiters will
    agree on a candidate), as does AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another possible source to compute the unavoidable bias is the Bayes error.
    Most commonly impossible to compute on complex AI tasks, the Bayes error is the
    lowest possible error rate a classifier can achieve. Most of the time, the Bayes
    error is lower than the human-level error, but much harder to estimate. This would
    be the actual theoretical limitation of any model performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayes error and the human-level error are unavoidable biases. They indicate
    the irreducible error of any model and are a key concept when evaluating whether
    a model needs more or less regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing bias and variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We usually define bias and variance using figures with a more or less accurate
    fit on some data, as we did earlier on the apartment price data. Although useful
    to explain the concepts, in real life, we mostly have highly dimensional datasets.
    By using a simple Titanic dataset, we provided a dozen features, so making this
    kind of visual inspection is simply impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we are training a dogs and cats classification model with balanced
    data. A good method is to compare the evaluation metric (whether it be accuracy,
    F-score, or whatever you deemed relevant) for both the training set and validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the concepts of evaluation metrics or training and validation sets are not
    clear, they will be explained in more detail in [*Chapter 2*](B19629_02.xhtml#_idTextAnchor034).
    Put simply, the model is trained on the training set. The metric is the value
    used to evaluate the trained model and is computed on both the training and validation
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s assume we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Hypothetical accuracy on training and validation sets](img/B19629_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Hypothetical accuracy on training and validation sets
  prefs: []
  type: TYPE_NORMAL
- en: If we think about the expected human-level error for such a task, we expect
    a much higher accuracy. Indeed, most humans can recognize a dog from a cat with
    very high confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case, the performances of training and validation sets are far
    below the human-level error rate. This is typical of a **high-bias** scenario:
    the evaluation metric is bad on both training and validation sets. In such cases,
    the model needs to be less regularized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now assume that after adding lower regularization (perhaps adding raised-to-the-power
    features as we did in the *Intuition about regularization* section), we have the
    following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Hypothetical accuracy after adding more features](img/B19629_01_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Hypothetical accuracy after adding more features
  prefs: []
  type: TYPE_NORMAL
- en: 'Those results are better; the validation set now has an accuracy of 89%. Nevertheless,
    there are two issues here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The score on the train set is way too good: it is literally perfect'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The score on the validation is still far below the human-level error rate, which
    we would expect to be at least 95%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is typical of a **high variance** scenario: results are really good (usually
    too good) on the training set, and far below on the validation set. In such cases,
    the model needs more regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding regularization, let’s assume we now have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Hypothetical accuracy after adding regularization](img/B19629_01_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Hypothetical accuracy after adding regularization
  prefs: []
  type: TYPE_NORMAL
- en: 'This seems much better: both training and validation sets have an accuracy
    that seems close to human-level performance. Perhaps with more data, a better
    model, or some other improvements, results could get a little better, but overall,
    this seems like a solid result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, diagnosing high bias and high variance is simple, and the method
    is always the same:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate your model on both the training and validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the results with each other, as well as with the human-level error rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From that point, there are mostly three cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High bias/underfitting**: Both training and validation sets exhibit poor
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High variance/overfitting**: The training set performance is far better than
    the validation set performance; validation set performance is well below the human-level
    error rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Good fit**: Both training and validation sets exhibit performance close to
    the human-level error rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, it is proposed to use this technique with training and test
    sets, instead of training and validation sets. While the reasoning holds true,
    doing such optimization on the test set directly may lead to overfitting, and
    thus overestimate the actual performance of a model. In this book, though, we
    will use the test set for simplification in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Having all the key concepts of regularization in mind, you might now start to
    understand why regularization might indeed require a whole book. While diagnosing
    a need for regularization is usually fairly easy, choosing the right regularization
    method can be really challenging. Let’s now categorize the regularization approaches
    that will be covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization – a multi-dimensional problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having the right diagnosis for a model is crucial, as it allows us to choose
    the strategy more carefully to improve the model. But from any diagnosis, many
    paths are possible to improve the model. Those paths can be separated into three
    main categories, as proposed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.17 – A proposed categorization of regularization types: data, model
    architecture, and model training](img/B19629_01_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.17 – A proposed categorization of regularization types: data, model
    architecture, and model training'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the data level, we may have the following tools for regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding more data, either synthetic or real
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, the data is of extreme importance in ML in general, and regularization
    is no exception. We will see many examples throughout the book of regularizing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the model level, the following methods may be used for regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a more or less simple architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In deep learning, many architectural designs allow regularization (for example,
    dropout)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model complexity may strongly impact regularization. An overly complicated
    architecture can easily lead to overfitting, while a too simplistic one may underfit,
    as depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Possible visualization of error as a function of model complexity,
    for both training and validation sets](img/B19629_01_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Possible visualization of error as a function of model complexity,
    for both training and validation sets
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, at the training level, some of the methods to regularize are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding penalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping is a very common way to avoid overfitting by preventing the model
    from getting too close to the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be many ways to regularize, as it is a multi-dimensional problem:
    data, model architecture, and model training are just high-level categories. Even
    though those categories are just some examples and more may exist or be defined,
    most – if not all – of the techniques this book will cover will fall into one
    of those categories.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by demonstrating, with several real-world examples,
    that regularization is the key to success in ML in a production environment. Along
    with several other methods and best practices, a robustly regularized model is
    necessary for production. In production, unseen data and edge cases will appear
    on a regular basis, thus any deployed model must have an acceptable response to
    such cases.
  prefs: []
  type: TYPE_NORMAL
- en: We then walked through some key concepts of regularization. Overfitting and
    underfitting are two common problems in ML and relate somehow to bias and variance.
    Indeed, an overfitting model has high variance, while an underfitting model has
    high bias. Thus, to perform well, a model is required to have low bias and low
    variance. We explained how, no matter how good a model can get, unavoidable bias
    limits its performance. Those key concepts allowed us to propose a method to diagnose
    bias and variance using the performance of both the training and validation sets,
    as well as human-level error estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This led us to what regularization is: regularization is adding constraints
    to a model so that it generalizes well to new data, and is not too sensitive to
    small data fluctuations. Regularization is a great tool to make an overfitting
    model a robust model. Although, due to the bias-variance trade-off, we must not
    regularize too much to avoid having an underfitting model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we categorized the different ways of regularizing that this book will
    cover. They mainly fall into three categories: the data, the model architecture,
    and the model training.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter did not include any recipes in order to build the foundational
    knowledge that will be required to fully understand the remainder of this book,
    but the next chapters will comprise recipes and will be more solution-oriented.
  prefs: []
  type: TYPE_NORMAL
