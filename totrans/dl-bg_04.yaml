- en: Preparing Data
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have successfully prepared your system to learn about deep learning,
    see [Chapter 2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup and Introduction
    to Deep Learning Frameworks*, we will proceed to give you important guidelines
    about data that you may encounter frequently when practicing deep learning. When
    it comes to learning about deep learning, having well-prepared datasets will help
    you to focus more on designing your models rather than preparing your data. However,
    everyone knows that this is not a realistic expectation and if you ask any data
    scientist or machine learning professional about this, they will tell you that
    an important aspect of modeling is knowing how to prepare your data. Knowing how
    to deal with your data and how to prepare it will save you many hours of work
    that you can spend fine-tuning your models. Any time spent preparing your data
    is time well invested indeed.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to the main concepts behind data processing
    to make it useful in deep learning. It will cover essential concepts of formatting
    outputs and inputs that are categorical or real-valued, and techniques for augmenting
    data or reducing the dimensions of data. At the end of the chapter, you should
    be able to handle the most common data manipulation techniques that can lead to
    successful choices of deep learning methodologies down the road.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter discusses the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary data and binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data and multiple classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-valued data and univariate regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Altering the distribution of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical implications of manipulating data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary data and binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus all our efforts on **preparing** data with binary
    inputs or targets. By binary, of course, we mean values that can be represented
    as either 0 or 1\. Notice the emphasis on the words *represented as*. The reason
    is that a column may contain data that is not necessarily a 0 or a 1, but could
    be interpreted as or represented by a 0 or a 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following fragment of a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *x*[1] | *x*[2] | ... | *y* |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | ... | a |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 7 | ... | a |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | ... | b |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 7 | ... | b |'
  prefs: []
  type: TYPE_TB
- en: In this short dataset example with only four rows, the column*x*[1] has values
    that are clearly binary and are either *0* or a *1*. However, *x*[2], at first
    glance, may not be perceived as binary, but if you pay close attention, the only
    values in that column are either *5* or *7*. This means that the data can be correctly
    and uniquely mapped to a set of two values. Therefore, we could map *5* to *0*,
    and *7* to *1*, or vice versa; it does not really matter.
  prefs: []
  type: TYPE_NORMAL
- en: A similar phenomenon is observed in the target output value, *y*, which also
    contains unique values that can be mapped to a set of size two. And we can do
    such mapping by assigning, say, *b* to *0*, and *a* to ***1***.
  prefs: []
  type: TYPE_NORMAL
- en: If you are going to map from strings to binary, always make sure to check what
    type of data your specific models can handle. For example, in some Support Vector
    Machine implementations, the preferred values for targets are -1 and 1\. This
    is still binary but in a different set. Always double-check before deciding what
    mapping you will use.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sub-section, we will deal specifically with binary targets using
    a dataset as a case study.
  prefs: []
  type: TYPE_NORMAL
- en: Binary targets on the Cleveland Heart Disease dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Cleveland Heart Disease* (Cleveland 1988) dataset contains patient data
    for 303 subjects. Some of the columns in the dataset have missing values; we will
    deal with this, too. The dataset contains 13 columns that include cholesterol
    and age.
  prefs: []
  type: TYPE_NORMAL
- en: The target is to detect whether a subject has heart disease or not, thus, is
    binary. The problem we will deal with is that the data is encoded with values
    from 0 to 4, where 0 indicates the absence of heart disease and the range 1 to
    4 indicates some type of heart disease.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the portion of the dataset identified as `Cleveland`, which can
    be downloaded from this link: [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The attributes of the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[1] | Age |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[2] | Sex |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[3] | Chest pain type:  1: typical angina  2: atypical angina  3: non-anginal
    pain  4: asymptomatic |'
  prefs: []
  type: TYPE_TB
- en: '| *x[4]* | Resting blood pressure (in mm Hg on admission to the hospital) |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[5] | Serum cholesterol in mg/dl |'
  prefs: []
  type: TYPE_TB
- en: '| *x[6]* | Fasting blood sugar > 120 mg/dl:  1 = true  0 = false |'
  prefs: []
  type: TYPE_TB
- en: '| *x[7]* | Resting electrocardiographic results:  0: normal  1: having ST-T
    wave abnormality  2: showing probable or definite left ventricular hypertrophy
    |'
  prefs: []
  type: TYPE_TB
- en: '| *x[8]* | Maximum heart rate achieved |'
  prefs: []
  type: TYPE_TB
- en: '| *x[9]* | Exercise-induced angina:   1 = yes'
  prefs: []
  type: TYPE_NORMAL
- en: 0 = no |
  prefs: []
  type: TYPE_NORMAL
- en: '| *x[10]* | ST depression induced by exercise relative to rest   |'
  prefs: []
  type: TYPE_TB
- en: '| *x[11]* | The slope of the peak exercise ST segment:   1: upsloping'
  prefs: []
  type: TYPE_NORMAL
- en: '2: flat'
  prefs: []
  type: TYPE_NORMAL
- en: '3: downsloping'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| *x[12]* | Number of major vessels (0-3) colored by fluoroscopy  |'
  prefs: []
  type: TYPE_TB
- en: '| *x[13]* | Thal:   3 = normal'
  prefs: []
  type: TYPE_NORMAL
- en: 6 = fixed defect
  prefs: []
  type: TYPE_NORMAL
- en: 7 = reversible defect
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| *y* | Diagnosis of heart disease (angiographic disease status):   0: < 50%
    diameter narrowing'
  prefs: []
  type: TYPE_NORMAL
- en: '1: > 50% diameter narrowing |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow the next steps in order to read the dataset into a pandas DataFrame
    and clean it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our Google Colab, we will first download the data using the `wget` command
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This, in turn, downloads the file `processed.cleveland.data` to the default
    directory for Colab. This can be verified by inspecting the Files tab on the left
    side of Colab. Please note that the preceding instruction is all one single line
    that, unfortunately, is very long.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we load the dataset using pandas to verify that the dataset is readable
    and accessible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pandas is a Python library that is very popular among data scientists and machine
    learning scientists. It makes it easy to load and save datasets, to replace missing
    values, to retrieve basic statistical properties on data, and even perform transformations.
    Pandas is a lifesaver and now most other libraries for machine learning accept
    pandas as a valid input format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands in Colab to load and display some data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `read_csv()` function loads a file that is formatted as **c****omma-separated
    values** (**CSV**). We use the argument `header=None` to tell pandas that the
    data does not have any actual headers; if omitted, pandas will use the first row
    of the data as the names for each column, but we do not want that in this case.
  prefs: []
  type: TYPE_NORMAL
- en: The loaded data is stored in a variable called `df`, which can be any name,
    but I think it is easy to remember because pandas stores the data in a DataFrame
    object. Thus, `df` seems like an appropriate, short, memorable name for the data.
    However, if we work with multiple DataFrames, then it would be more convenient
    to name all of them differently with a name that describes the data they contain.
  prefs: []
  type: TYPE_NORMAL
- en: The `head()` method that operates over a DataFrame is analog to a `unix` command
    that retrieves the first few lines of a file. On a DataFrame, the `head()` method
    returns the first five rows of data. If you wish to retrieve more, or fewer, rows
    of data, you can specify an integer as an argument to the method. Say, for example,
    that you want to retrieve the first three rows, then you would do `df.head(3)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of running the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a few things to observe and remember for future reference:'
  prefs: []
  type: TYPE_NORMAL
- en: On the left side, there is an unnamed column that has rows with consecutive
    numbers, 0, 1, ..., 4\. These are the indices that pandas assigns to each row
    in the dataset. These are unique numbers. Some datasets have unique identifiers,
    such as a filename for an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the top, there is a row that goes from 0, 1, ..., 13\. These are the column
    identifiers. These are also unique and can be set if they are given to us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the intersection of every row and column, we have values that are either
    floating-point decimals or integers. The entire dataset contains decimal numbers
    except for column 13, which is our target and contains integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because we will use this dataset as a binary classification problem, we now
    need to change the last column to contain only binary values: 0 and 1\. We will
    preserve the original meaning of 0, that is,no heart disease, and anything greater
    than or equal to 1 will be mapped to 1, indicating the diagnosis of some type
    of heart disease. We will run the following instructions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The instruction `df[13]` looks at the DataFrame and retrieves all the rows
    of the column whose index is `13`. Then, the `set()` method over all the rows
    of column 13 will create a set of all the unique elements in the column. In this
    way, we can know how many different values there are so that we can replace them.
    The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we know that 0 is no heart disease and 1 implies heart disease.
    However, 2, 3, and 4 need to be mapped to 1, because they, too, imply positive
    heart disease. We can make this change by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `replace()` function works on the DataFrame to replace specific values.
    In our case, it took three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`to_replace=[2,3,4]` denotes the list of items to search for, in order to replace
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value=1` denotes the value that will replace every matched entry .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inplace=True` indicates to pandas that we want to make the changes on the
    column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, pandas DataFrames behave like an immutable object, which, in
    this case, makes it necessary to use the `inplace=True` argument. If we did not
    use this argument, we would have to do something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '`df[13] = df[13].replace(to_replace=[2,3,4], value=1)`, which is not a problem
    for experienced pandas users. This means that you should be comfortable doing
    this either way.'
  prefs: []
  type: TYPE_NORMAL
- en: The main problem for people beginning to use pandas is that it does not *always*
    behave like an immutable object. Thus, you should keep all the pandas documentation
    close to you: [https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the preceding commands is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: First, notice that when we print the first five rows, the thirteenth column
    now exclusively has the values 0 or 1\. You can compare this to the original data
    to verify that the number in bold font actually changed. We also verified, with
    `set(df[13])`, that the set of all unique values of that column is now only `{0,
    1}`, which is the desired target.
  prefs: []
  type: TYPE_NORMAL
- en: With these changes, we could use the dataset to train a deep learning model
    and perhaps improve the existing documented performance [Detrano, R., *et al.* (1989)].
  prefs: []
  type: TYPE_NORMAL
- en: The same methodology can be applied to make any other column have binary values
    in the set we need. As an exercise, let's do another example with the famous `MNIST`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The MNIST dataset is well known in the deep learning community (Deng, L. (2012)).
    It is composed of thousands of images of handwritten digits. Figure 3.1 shows
    eight samples of the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7de1c72c-0fef-46ad-93e0-19d39bafad3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Eight samples of the MNIST dataset. The number on top of each image
    corresponds to the target class
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the samples in this dataset are messy and are very real. Every
    image has a size of 28 x 28 pixels. And there are only 10 target classes, one
    for each digit, 0, 1, 2, ..., 9\. The complication here is usually that some digits
    may look similar to others; for example, 1 and 7, or 0 and 6\. However, most deep
    learning algorithms have successfully solved the classification problem with high
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3.1*, a close inspection will reveal that the values are not exactly
    zeros and ones, that is, binary. In fact, the images are 8-bit grayscale, in the
    range [0-255]. As mentioned earlier, this is no longer a problem for most advanced
    deep learning algorithms. However, for some algorithms, such as **Restricted Boltzmann
    Machines** (**RMBs**), the input data needs to be in binary format [0,1] because
    that is how the algorithm works, traditionally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we will do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Binarize the images, so as to have binary inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binarize the targets, to make it a binary classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this example, we will arbitrarily select two numerals only, 7 and 8, as
    our target classes.
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing the images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The binarization process is a common step in image processing. It is formally
    known as image thresholding because we need a threshold to decide which values
    become zeros and ones. For a full survey about this topic, please consult (Sezgin,
    M., and Sankur, B. (2004)). This is all to say that there is a science behind
    picking the perfect threshold that will minimize the range conversion error from
    [0, 255] down to [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: However, since this is not a book about image processing, we will arbitrarily
    set a threshold of 128\. Thus, any value below 128 will become a zero, and any
    value greater than or equal to 128 will become a one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This step can be easily done by using indexing in Python. To proceed, we will
    display a small portion of the dataset to make sure the data is transformed correctly.
    We will do this by executing the following commands in the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the dataset and verify its dimensionality (shape), run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to notice is that we are using a machine learning library known
    as `scikit learn` or `sklearn` in Python. It is one of the most used libraries
    for general-purpose machine learning. The `MNIST` dataset is loaded using the
    `fetch_openml()` method, which requires an argument with the identifier of the
    dataset to be loaded, which in this case is `'mnist_784'`. The number `784` comes
    from the size of `MNIST` images, which is 28 x 28 pixels and can be interpreted
    as a vector of 784 elements rather than a matrix of 28 columns and 28 rows. By
    verifying the `shape` property, we can see that the dataset has 70,000 images
    represented as vectors of size 784, and the targets are in the same proportion.
  prefs: []
  type: TYPE_NORMAL
- en: Please note here that, as opposed to the previous section where we used a dataset
    loaded into pandas, in this example, we use the data directly as lists or arrays
    of lists. You should feel comfortable manipulating both pandas and raw datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'To actually do the binarization by verifying the data before and after, run
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The instruction `data[0].reshape(28, 28)[10:18,10:18]` is doing three things:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data[0]` returns the first image as an array of size (1, 784).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`reshape(28, 28)` resizes the (1, 784) array as a (28, 28) matrix, which is
    the actual image; this can be useful to display the actual data, for example,
    to produce *Figure 3.1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[10:18,10:18]` takes only a subset of the (28, 28) matrix at positions 10
    to 18 for both columns and rows; this more or less corresponds to the center area
    of the image and it is a good place to look at what is changing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding is for looking at the data only, but the actual changes are done
    in the next lines. The line `mnist.data[mnist.data < 128] = 0` uses Python indexing.
    The instruction `mnist.data < 128` returns a multidimensional array of Boolean
    values that `mnist.data[ ]` uses as indices on which to set the value to zero.
    The key is to do so for all values strictly less than 128\. And the next line
    does the same, but for values greater than or equal to 128.
  prefs: []
  type: TYPE_NORMAL
- en: By inspecting the output, we can confirm that the data has successfully changed
    and has been thresholded, or binarized.
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing the targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will binarize the targets by following the next two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will discard image data for other numerals and we will only keep
    7 and 8\. Then, we will map 7 to 0 and 8 to 1\. These commands will create new
    variables, `X` and `y`, that will hold only the numerals 7 and 8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Notice the use of the `OR` operator, `|`, to logically take two sets of Boolean
    indices and produce one with the `OR` operator. These indices are used to produce
    a new dataset. The shape of the new dataset contains a little over 14,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To map 7 to 0 and 8 to 1, we can run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The instruction `[0 if v=='7' else 1 for v in y]` checks every element in `y`,
    and if an element is `'7'`, then it returns a `0`, otherwise (for example, when
    it is `'8'`), it returns a `1`. As the output suggests, choosing the first 10
    elements, the data is binarized to the set {`0`, `1`}.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the target data in `y` was already binary in the sense that it only
    had two sets of unique possible numbers {`7`, `8`}. But we made it binary to the
    set {`0`, `1`} because often this is better when we use different deep learning
    algorithms that calculate very specific types of loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: With this, the dataset is ready to use with binary and general classifiers.
    But what if we actually want to have multiple classes, for example, to detect
    all 10 digits of the `MNIST` dataset and not just 2? Or what if we have features,
    columns, or inputs that are not numeric but are categorical? The next section
    will help you prepare the data in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data and multiple classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how to binarize data for different purposes, we can look into
    other types of data, such as categorical or multi-labeled data, and how to make
    them numeric. Most advanced deep learning algorithms, in fact, only accept numerical
    data. This is merely a design issue that can easily be solved later on, and it
    is not a big deal because you will learn there are easy ways to take categorical
    data and convert it to a meaningful numerical representation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical data** has information embedded as distinct categories. These
    categories can be represented as numbers or as strings. For example, a dataset
    that has a column named `country` with items such as "India", "Mexico", "France",
    and "U.S". Or, a dataset with zip codes such as 12601, 85621, and 73315\. The
    former is **non-numeric** categorical data, and the latter is **numeric** categorical
    data. Country names would need to be converted to a number to be usable at all,
    but zip codes are already numbers that are meaningless as mere numbers. Zip codes
    would be more meaningful, from a machine learning perspective, if we converted
    them to latitude and longitude coordinates; this would better capture places that
    are closer to each other than using plain numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we will address the issue of converting string categories to plain
    numbers and then we will convert those to numbers in a format called **one-hot
    encoding**.
  prefs: []
  type: TYPE_NORMAL
- en: Converting string labels to numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will take the `MNIST` dataset again and use its string labels, *0*, *1*,
    ..., *9*, and convert them to numbers. We can achieve this in many different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: We could simply map all strings to integers with one simple command, `y = list(map(int,
    mnist.target))`, and be done. The variable `y` now contains only a list of integers
    such as `[8, 7, 1, 2, ... ]`. But this will only solve the problem for this particular
    case; you need to learn something that will work for all cases. So, let's not
    do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could do some hard work by iterating over the data 10 times – `mnist.target
    = [0 if v=='0' else v for v in mnist.target]` – doing this for every numeral.
    But again, this (and other similar things) will work only for this case. Let's
    not do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could use scikit-learn's `LabelEncoder()` method, which will take any list
    of labels and map them to a number. This will work for all cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s use the `scikit` method by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sorted(list(set(mnist.target)))` command does three things:'
  prefs: []
  type: TYPE_NORMAL
- en: '`set(mnist.target)` retrieves the set of unique values in the data, for example, `{''8'',
    ''2'', ..., ''9''}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`list(set(mnist.target))` simply converts the set into a list because we need
    a list or an array for the `LabelEncoder()` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorted(list(set(mnist.target)))` is important here so that *0* maps to 0 and
    not to have *8* map to 0, and so on. It sorts the list, and the result looks like
    this - `[''0'', ''1'', ..., ''9'']`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `le.fit()` method takes a list (or an array) and produces a map (a dictionary)
    to be used forward (and backward if needed) to encode labels, or strings, into
    numbers. It stores this in a `LabelEncoder` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we could test the encoding as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `transform()` method transforms a string-based label into a number, whereas
    the `inverse_transform()` method takes a number and returns the corresponding
    string label or category.
  prefs: []
  type: TYPE_NORMAL
- en: Any attempt to map to and from an unseen category or number will cause a `LabelEncoder` object
    to produce an error. Please be diligent in providing the list of all possible
    categories to the best of your knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `LabelEncoder` object is fitted and tested, we can simply run the
    following instruction to encode the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The new encoded labels are now in `y` and ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: This method of encoding a label to an integer is also known as **Ordinal Encoding.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This methodology should work for all labels encoded as strings, for which you
    can simply map to numbers without losing context. In the case of the `MNIST` dataset,
    we can map *0* to 0 and *7* to 7 without losing context. Other examples of when
    you can do this include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Age groups**: [''18-21'', ''22-35'', ''36+''] to [0, 1, 2]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gender**: [''male'', ''female''] to [0, 1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colors**: [''red'', ''black'', ''blue'', ...] to [0, 1, 2, ...]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Studies**: [''primary'', ''secondary'', ''high school'', ''university'']
    to [0, 1, 2, 3]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, we are making one big assumption here: the labels encode no special
    meaning in themselves. As we mentioned earlier, zip codes could be simply encoded
    to smaller numbers; however, they have a geographical meaning, and doing so might
    negatively impact the performance of our deep learning algorithms. Similarly,
    in the preceding list, if studies require a special meaning that indicates that
    a *university* degree is much higher or more important than a *primary* degree,
    then perhaps we should consider different number mappings. Or perhaps we want
    our learning algorithms to *learn* such intricacies by themselves! In such cases,
    we should then use the well-known strategy of one-hot encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting categories to one-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Converting categories to one-hot encoding is better in most cases in which the
    categories or labels may have special meanings with respect to each other. In
    such cases, it has been reported to outperform ordinal encoding [Potdar, K., *et
    al.* (2017)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to represent each label as a Boolean state having independent columns.
    Take, for example, a column with the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** |'
  prefs: []
  type: TYPE_TB
- en: '| ''female'' |'
  prefs: []
  type: TYPE_TB
- en: '| ''male'' |'
  prefs: []
  type: TYPE_TB
- en: '| ''male'' |'
  prefs: []
  type: TYPE_TB
- en: '| ''female'' |'
  prefs: []
  type: TYPE_TB
- en: '| ''female'' |'
  prefs: []
  type: TYPE_TB
- en: 'This can be uniquely transformed, using one-hot encoding, into the following
    new piece of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender_Female** | **Gender_Male** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the binary bit is *hot* (is one) only if the label corresponds
    to that specific row and it is zero otherwise. Notice also that we renamed the
    columns to keep track of which label corresponds to which column; however, this
    is merely a recommended format and is not a formal rule.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways we can do this in Python. If your data is in a pandas
    DataFrame, then you can simply do `pd.get_dummies(df, prefix=['Gender'])`, assuming
    your column is in `df` and you want to use `Gender` as a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reproduce the exact results as discussed in the preceding table, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now simply do the encoding by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: A fun, and perhaps obvious, property of this encoding is that the `OR` and `XOR`
    operations along the rows of all the encoded columns will always be one, and the
    `AND` operation will yield zeros.
  prefs: []
  type: TYPE_NORMAL
- en: For cases in which the data is not a pandas DataFrame, for example, MNIST targets,
    we can use scikit-learn's `OneHotEncoder.transform()` method.
  prefs: []
  type: TYPE_NORMAL
- en: A `OneHotEncoder` object has a constructor that will automatically initialize
    everything to reasonable assumptions and determines most of its parameters using
    the `fit()` method. It determines the size of the data, the different labels that
    exist in the data, and then creates a dynamic mapping that we can use with the
    `transform()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do a one-hot encoding of the `MNIST` targets, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This code includes our classic sanity check in which we verify that label `'5'` was
    in fact converted to a row vector with 10 columns, of which number `6` is *hot*.
    It works, as expected. The new dimensionality of `y` is *n* rows and 10 columns.
  prefs: []
  type: TYPE_NORMAL
- en: This is the preferred format for the targets that use deep learning methods
    on MNIST. One-hot encoding targets are great for neural networks that will have
    exactly one neuron per class. In this case, one neuron per digit. Each neuron
    will need to learn to predict one-hot encoded behavior, that is, only one neuron
    should fire up (be "hot") while the others should be inhibited.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding process can be repeated exactly to convert any other columns into
    one-hot encoding, provided that they contain categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Categories, labels, and specific mappings to integers or bits are very helpful
    when we want to classify input data into those categories, labels, or mappings.
    But what if we want to have input data that maps to continuous data? For example,
    data to predict a person's IQ by looking at their responses; or predicting the
    price of electricity depending on the input data about weather and the seasons.
    This is known as data for **regression**, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Real-valued data and univariate regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing how to deal with categorical data is very important when using classification
    models based on deep learning; however, knowing how to prepare data for regression
    is as important. Data that contains continuous-like real values, such as temperature,
    prices, weight, speed, and others, is suitable for regression; that is, if we
    have a dataset with columns of different types of values, and one of those is
    real-valued data, we could perform regression on that column. This implies that
    we could use all the rest of the dataset to predict the values on that column.
    This is known as **univariate regression**, or regression on one variable.
  prefs: []
  type: TYPE_NORMAL
- en: Most machine learning methodologies work better if the data for regression is **normalized**.
    By that, we mean that the data will have special statistical properties that will
    make calculations more stable. This is critical for many deep learning algorithms
    that suffer from vanishing or exploding gradients (Hanin, B. (2018)). For example,
    in calculating a gradient in a neural network, an error needs to be propagated
    backward from the output layer to the input layer; but if the output layer has
    a large error and the range of values (that is their **distribution**) is also
    large, then the multiplications going backward can cause overflow on variables,
    which would ruin the training process.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these difficulties, it is desirable to normalize the distribution
    of variables that can be used for regression, or variables that are real-valued.
    The normalization process has many variants, but we will limit our discussion
    to two main methodologies, one that sets specific statistical properties of the
    data, and one that sets specific ranges on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling to a specific range of values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's go back to the heart disease dataset discussed earlier in this chapter.
    If you pay attention, many of those variables are real-valued and would be ideal
    for regression; for example, *x*[5] and *x*[10].
  prefs: []
  type: TYPE_NORMAL
- en: All variables are suitable for regression. This means that, technically, we
    can predict on any numeric data. The fact that some values are real-valued makes
    them more appealing for regression for a number of reasons. For example, the fact
    that the values in that column have a meaning that goes beyond integers and natural
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus on *x*[5 ]and *x*[10], which are the variables for measuring the
    cholesterol level and ST depression induced by exercise relative to rest, respectively.
    What if we want to change the original research question the doctors intended,
    which was to study heart disease based on different factors? What if now we want
    to use all the factors, including knowing whether patients have heart disease
    or not, to determine or predict their cholesterol level? We can do that with regression
    on *x*[5].
  prefs: []
  type: TYPE_NORMAL
- en: So, to prepare the data on *x*[5] and *x*[10], we will go ahead and scale the
    data. For verification purposes, we will retrieve descriptive statistics on the
    data before and after the scaling of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reload the dataset and display descriptive statistics, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, index, `4` and `9` correspond to *x*[5] and *x*[10], and the
    `describe()` method outputs the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The most notable properties are the mean, and maximum/minimum values contained
    in that column. These will change once we scale the data to a different range.
    If we visualize the data as a scatter plot with respective histograms, it looks
    like *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71a5370e-2596-445b-8ad1-90ec5f7e721a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Scatter plot of the two columns *x*[5] and *x*[10] and their corresponding
    histograms
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from *Figure 3.2*, the ranges are quite different, and the distribution
    of the data is different as well. The new desired range here is a minimum of 0
    and a maximum of 1\. This range is typical when we scale the data. And it can
    be achieved using scikit-learn''s `MinMaxScaler` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'What the `fit()` method does internally is to determine what the current min
    and max values are for the data. Then, the `transform()` method uses that information
    to remove the minimum and divide by the maximum to achieve the desired range.
    As can be seen, the new descriptive statistics have changed, which can be confirmed
    by looking at the range in the axes of *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ddba0fa-3bb9-478d-a6f6-b4739023945d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Scatter plot of the newly scaled columns *x*[5] and *x*[10] and
    their corresponding histograms
  prefs: []
  type: TYPE_NORMAL
- en: Notice, however, if you pay close attention, that the distribution of the data
    has not changed. That is, the histograms of the data in *Figure 3.2* and *Figure
    3.3* are still the same. And this is a very important fact because, usually, you
    do not want to change the distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing to zero mean and unit variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of preprocessing real-valued data is by making it have zero mean
    and unit variance. This process is referred to by many names, such as normalizing,
    z-scoring, centering, or standardizing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that ***x***=[*x*[5], *x*[10]], from our features above, then we
    can standardize ***x***as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55d3b551-c4fa-4283-8e6c-ad964388b135.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *µ* is a vector corresponding to the means of each column on ***x***,
    and *σ* is a vector of standard deviations of each column in ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the standardization of ***x***, if we recompute the mean and standard
    deviation, we should get a mean of zero and a standard deviation of one. In Python,
    we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that after normalization, the mean is, for numerical purposes, zero.
    And the standard deviation is one. The same thing can be done, of course, using
    the scikit-learn `StandardScaler` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This will yield the same results with negligible numerical differences. For
    practical purposes, both methods will achieve the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: Although both ways of normalizing are appropriate, in the DataFrame directly
    or using a `StandardScaler` object, you should prefer using the `StandardScaler` object
    if you are working on a production application. Once the `StandardScaler` object
    uses the `fit()` method, it can be used on new, unseen, data easily by re-invoking
    `transform()` method; however, if we do it directly on the pandas DataFrame, we
    will have to manually store the mean and standard deviation somewhere and reload
    it every time we need to standardize new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for comparison purposes, *Figure 3.4* depicts the new ranges after the
    normalization of the data. If you look at the axes closely, you will notice that
    the position of the zero values are where most of the data is, that is, where
    the mean is. Therefore, the cluster of data is centered around a mean of zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbe8723e-6944-46f1-90e9-3656d0333c96.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Scatter plot of the standardized columns *x*[5] and *x*[10] and
    their corresponding histograms
  prefs: []
  type: TYPE_NORMAL
- en: Notice, again, that in *Figure 3.4*, after applying the standardization process,
    the distribution of the data still does not change. But what if you actually want
    to change the distribution of the data? Keep reading on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Altering the distribution of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It has been demonstrated that changing the distribution of the targets, particularly
    in the case of regression, can have positive benefits in the performance of a
    learning algorithm (Andrews, D. F., et al. (1971)).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we'll discuss one particularly useful transformation known as **Quantile
    Transformation**. This methodology aims to look at the data and manipulate it
    in such a way that its histogram follows either a **normal** distribution or a
    **uniform** distribution. It achieves this by looking at estimates of quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following commands to transform the same data as in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This will effectively map the data into a new distribution, namely, a normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the term **normal distribution** refers to a Gaussian-like **probability
    density function** (**PDF**). This is a classic distribution found in any statistics
    textbook. It is usually identified by its bell-like shape when plotted.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are also using the `fit_transform()` method, which does both `fit()`
    and `transform()` at the same time, which is convenient.
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen in *Figure 3.5*, the variable related to cholesterol data, *x*[5],
    was easily transformed into a normal distribution with a bell shape. However,
    for *x*[10], the heavy presence of data in a particular region causes the distribution
    to have a bell shape, but with a long tail, which is not ideal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86423dc1-72ee-4d89-b0dc-0a3c8fbabc76.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Scatter plot of the normally transformed columns *x*[5] and *x*[10] and
    their corresponding Gaussian-like histograms
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of transforming the data for a uniform distribution is very similar.
    We simply need to make a small change in one line, on the `QuantileTransformer()`
    constructor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the data is transformed into a uniform distribution, as shown in *Figure
    3.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5052596d-f5ba-4467-b5f6-19630b3510d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Scatter plot of the uniformly transformed columns *x*[5] and *x*[10] and
    their corresponding uniform histograms
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, we can see that the data has been uniformly distributed across
    each variable. Once again, the clustering of data in a particular region has the
    effect of causing a large concentration of values in the same space, which is
    not ideal. This artifact also creates a gap in the distribution of the data that
    is usually difficult to handle, unless we use techniques to augment the data,
    which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have learned how to process the data to have specific distributions,
    it is important for you to know about data augmentation, which is usually associated
    with missing data or high-dimensional data. Traditional machine learning algorithms
    may have problems dealing with data where the number of dimensions surpasses the
    number of samples available. The problem is not particular to all deep learning
    algorithms, but some algorithms have a much more difficult time learning to model
    a problem that has more variables to figure out than samples to work on. We have
    a few options to correct that: either we reduce the dimensions or variables (see
    the following section) or we increase the samples in our dataset (this section).'
  prefs: []
  type: TYPE_NORMAL
- en: One of the tools for adding more data is known as **data augmentation **(Van
    Dyk, D. A., and Meng, X. L. (2001)).  In this section, we will use the `MNIST`
    dataset to exemplify a few techniques for data augmentation that are particular
    to images but can be conceptually extended to other types of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the basics: adding noise, rotating, and rescaling. That is, from
    one original example, we will produce three new, different images of numerals.
    We will use the image processing library known as `scikit image`.'
  prefs: []
  type: TYPE_NORMAL
- en: Rescaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by reloading the `MNIST` dataset as we have done before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can simply invoke the `rescale()` method to create a rescaled image.
    The whole purpose behind resizing an image is to rescale it back to its original
    size because this makes the image look like a small resolution image of the original.
    It loses some of its characteristics in the process, but it can actually make
    a more robust deep learning model. That is, a model robust to the scale of objects,
    or in this case, the scale of numerals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have `x` as the original image from which we will augment, we can do
    the scaling down and up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Here, the augmented image (rescaled) is in `x_`*.  *Notice that, in this case,
    the image is downscaled by a factor of two (50%) and then upscaled, also by a
    factor of two (200%). The `multichannel` argument is set to `false` since the
    images have only one single channel, meaning they are grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: When rescaling, be careful of rescaling by factors that give you exact divisions.
    For example, a 28 x 28 image that is downscaled by a factor of 0.5 goes down to
    14 x 14; this is good. But if we downscale by a factor of 0.3, it will go down
    to 8.4 x 8.4, which goes up to 9 x 9; this is not good because it can add unnecessary
    complications. Keep it simple.
  prefs: []
  type: TYPE_NORMAL
- en: Besides rescaling, we can also modify the existing data slightly so as to have
    variations of the existing data without deviating much from the original, as we'll
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similarly, we can also contaminate the original image with additive Gaussian
    noise. This creates random patterns all over the image to simulate a camera problem
    or noisy acquisition. Here, we use it to also augment our dataset and, in the
    end, to produce a deep learning model that is robust against noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we use the `random_noise()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Once again, the augmented image (noisy) is in `x_`.
  prefs: []
  type: TYPE_NORMAL
- en: Besides noise, we can also change the perspective of an image slightly so as
    to preserve the original shape at a different angle, as we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Rotating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use a plain rotation effect on the images to have even more data. The
    rotation of images is a crucial part of learning good features from images. Larger
    datasets contain, naturally, many versions of images that are slightly rotated
    or fully rotated. If we do not have such images in our dataset, we can manually
    rotate them and augment our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we use the `rotate()` method like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the number `22` specifies the angle of rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: When you are augmenting your dataset, you may want to consider having multiple
    rotations at random angles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/039c1b15-877e-421d-884e-3903881019c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – An example of the images produced with the preceding data augmentation
    techniques
  prefs: []
  type: TYPE_NORMAL
- en: The first column is the original numeral of the MNIST dataset. The second column
    shows the effect of rescaling. The third column shows the original plus additive
    Gaussian noise. The last column shows a rotation of 20 degrees (top) and -20 degrees
    (bottom).
  prefs: []
  type: TYPE_NORMAL
- en: Other augmentation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For image datasets, there are other ideas for augmenting data that include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the projection of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding compression noise (quantizing the image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other types of noise besides Gaussian, such as salt and pepper, or multiplicative
    noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The translation of the image by different distances at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But the most robust augmentation would be a combination of all of these!
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are fun because they are highly correlated in local areas. But for general
    non-image datasets, such as the heart disease dataset, we can augment data in
    other ways, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding low-variance Gaussian noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding compression noise (quantization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing new points from a calculated probability density function over the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For other special datasets, such as text-based data, we can also do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace some words with synonyms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove some words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add words that contain errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation (only if you do not care about proper language structures)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on this and many other augmentation techniques, consult
    online resources on the latest advances pertaining to your specific type of data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now dive into some techniques for dimensionality reduction that can be
    used to alleviate the problem of high-dimensional and highly correlated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As pointed out before, if we have the problem of having more dimensions (or
    variables) than samples in our data, we can either augment the data or reduce
    the dimensionality of the data. Now, we will address the basics of the latter.
  prefs: []
  type: TYPE_NORMAL
- en: We will look into reducing dimensions both in supervised and unsupervised ways
    with both small and large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised algorithms for dimensionality reduction are so called because they
    take the labels of the data into account to find better representations. Such
    methods often yield good results. Perhaps the most popular kind is called **linear
    discriminant analysis** (**LDA**), which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Linear discriminant analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scikit learn has a `LinearDiscriminantAnalysis` class that can easily perform
    dimensionality reduction on a desired number of components.
  prefs: []
  type: TYPE_NORMAL
- en: By **number of components**, the number of dimensions desired is understood.
    The name comes from **principal component analysis** (**PCA**), which is a statistical
    approach that determines the eigenvectors and eigenvalues of the centered covariance
    matrix of a dataset; then, the largest eigenvalues associated with specific eigenvectors
    are known to be the most important, *principal*, components. When we use PCA to
    reduce to a specific number of components, we say that we want to keep those components
    that are the most important in a space induced by the eigenvalues and eigenvectors
    of the covariance matrix of the data.
  prefs: []
  type: TYPE_NORMAL
- en: LDA and other dimensionality reduction techniques also have a similar philosophy
    in which they aim to find low-dimensional spaces (based on the number of components
    desired) that can better represent the data based on other properties of the data.
  prefs: []
  type: TYPE_NORMAL
- en: If we use the heart disease dataset as an example, we can perform LDA to reduce
    the entire dataset from 13 dimensions to 2 dimensions, all the while using the
    labels [0, 1, 2, 3, 4] to inform the LDA algorithm how to better separate the
    groups represented by those labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we reload the data and drop the missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we did not have to deal with missing values before on the heart
    disease dataset because pandas automatically ignores missing values. But here,
    because we are strictly converting data into numbers, missing values will be converted
    to `NaN` since we are specifying `errors='coerce'`, which forces any errors in
    the conversion to become `NaN`. Consequently, with `dropna()`, we ignore rows
    with those values from our dataset because they will cause LDA to fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we prepare the `X` and `y` variables to contain the data and targets,
    respectively, and we perform LDA as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `X_` contains the entire dataset represented in two dimensions,
    as given by `n_components=2`. The choice of two components is simply to illustrate
    graphically how the data looks. But you can change this to any number of components
    you desire.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.8* depicts how the 13-dimensional dataset looks if compressed, or
    reduced, down to two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81bc6b69-a2d6-4c3d-ab8a-038d7f2b0bbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Reducing dimensions from 13 to 2 using LDA
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the values with 0 (no heart disease) are mostly clustered toward
    the left side, while the rest of the values (that is, 1, 2, 3, and 4, which represent
    heart disease) seem to cluster toward the right side. This is a nice property
    that was not observed in *Figures* *3.2* to *3.6* when we picked two columns out
    of the 13.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, the relevant information of the 13 dimensions is still
    contained in the LDA-induced two dimensions. If the data seems to be separable
    in these low-dimensional representations, a deep learning algorithm may have a
    good chance of learning representations to classify or regress on the data with
    high performance.
  prefs: []
  type: TYPE_NORMAL
- en: While LDA can offer a very nice way to perform dimensionality reduction informed
    by the labels in the data, we might not always have labeled data, or we may not
    want to use the labels that we have. In those cases we can, and we should, explore
    other robust methodologies that require no label information, such as unsupervised
    techniques, which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised techniques are the most popular methods because they need no prior
    information about labels. We begin with a kernelized version of PCA and then we
    move on to methods that operate on larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This variant of PCA uses kernel methods to estimate distances, variances, and
    other parameters to determine the major components of the data (Schölkopf, B.,
    et al. (1997)). It may take a bit more time to produce a solution than regular
    PCA, but it is very much worth using it over traditional PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `KernelPCA` class of scikit-learn can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we use two dimensions as the new space, and we use a `''linear''` kernel.
    Other popular choices for the kernel include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''rbf''` for a radial basis function kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''poly''` for a polynomial kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personally, I like the `'rbf'` kernel in general, because it is more powerful
    and robust. But oftentimes, you spend valuable time trying to determine the best
    value for the parameter *γ*, which is how wide the bell of the radial basis function
    is. If you have the time, try `'rbf'` and experiment with the parameter `gamma`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of using kernel PCA is shown in *Figure 3.9*. The diagram again
    shows a clustering arrangement of the negative class (no heart disease, a value
    of 0) toward the bottom left of the KPCA-induced space. The positive class (heart
    disease, values ≥ 1) tends to cluster upward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88fdcb86-dfba-4e09-8355-7df6981cc28c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Reducing dimensions with kernel PCA from 13 down to 2
  prefs: []
  type: TYPE_NORMAL
- en: Compared to *Figure 3.8*, LDA produces a slightly better space where the groups
    can be separated. However, KPCA does a good job in spite of now knowing the actual
    target classes. Now, LDA and KPCA might take no time on small datasets, but what
    if we have a lot of data? We will discuss some options next.
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous examples will work well with moderate-sized datasets. However,
    when dealing with very large datasets, that is, with many dimensions or many samples,
    some algorithms may not function at their best. In the worst case, they will fail
    to produce a solution. The next two unsupervised algorithms are designed to function
    well for large datasets by using a technique called **batch training**. This technique
    is well known and has been applied in machine learning successfully (Hinton, G.
    E. (2012)).
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is to divide the dataset into small (mini) batches and partially
    make progress toward finding a global solution to the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We'll first look into a sparse-coding version of PCA available in scikit-learn
    as `MiniBatchSparsePCA`. This algorithm will determine the best transformation
    into a subspace that satisfies a sparsity constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparsity** is a property of matrices (or vectors) in which most of the elements
    are zeros. The opposite of sparsity is density. We like sparsity in deep learning
    because we do a lot of tensor (vector) multiplications, and if some of the elements
    are zeros, we do not have to perform those multiplications, thus saving time and
    optimizing for speed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps in order to use the `MNIST` dataset and reduce its dimensions,
    since it has 784 dimensions and 70,000 samples. It is large enough, but even larger
    datasets can also be used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by reloading the data and preparing it for the sparse PCA encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we perform the dimensionality reduction as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `MiniBatchSparsePCA()` constructor takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components`, which we set to 2 for visualization purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` determines how many samples the algorithm will use at a time.
    We set it to `50`, but larger numbers may cause the algorithm to slow down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_components` refers to the preprocessing of the data by *centering*
    it, that is, making it have a zero mean and a unit variance; we recommend doing
    this every time, especially if you have data that is highly correlated, such as
    images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `MNIST` dataset transformed using sparse PCA looks as depicted in *Figure
    3.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/452ad7f6-1aeb-4443-a433-7b36d4a3e64c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – MNIST dataset reduced to two dimensions using sparse PCA
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the separation between classes is not perfectly clear. There
    are some definitive clusters of digits, but it does not seem like a straightforward
    task due to the overlap between groups. This is caused in part by the fact that
    many digits may look alike. It would make sense to have the numerals 1 and 7 clustered
    together (the left side up and down), or 3 and 8 (the middle and up).
  prefs: []
  type: TYPE_NORMAL
- en: But let's also use another popular and useful algorithm called Dictionary Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dictionary Learning is the process of learning the basis of transformations,
    called **dictionaries**, by using a process that can easily scale to very large
    datasets (Mairal, J., et al. (2009)).
  prefs: []
  type: TYPE_NORMAL
- en: This was not possible with PCA-based algorithms, but this technique remains
    powerful and recently received the *Test of Time* award at one of the major conferences
    in the world, the *2019* *Intern**ational Conference in Machine Learning. *
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is available in scikit-learn through the `MiniBatchDictionaryLearning` class.
    We can use it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor `MiniBatchDictionaryLearning()` takes on similar arguments
    as `MiniBatchSparsePCA()` with the same meaning. The results of the learned space
    are shown in *Figure 3.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2c39450-0acb-48c5-9e7a-a5fcad716e5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Dimensionality reduction of MNIST data down to two dimensions
    using Dictionary Learning
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, there is a significant overlap among classes even if there are
    clearly defined clusters. This could lead to poor performance results if this
    data, the two-dimensional data, is used as input to train a classifier. This does
    not mean that algorithms are bad, necessarily. What this could mean is that, maybe,
    two dimensions are not the best choice of final dimensions. Continue reading to
    learn more about this.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the number of dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reducing dimensions is not always a necessary step. But it is highly recommended
    for data that is highly correlated, for example, images.
  prefs: []
  type: TYPE_NORMAL
- en: All the discussed dimensionality reduction techniques actually strive to remove
    redundant information in the data and preserve the important content. If we ask
    an algorithm to reduce the dimensions of our non-correlated, non-redundant dataset
    from 13 dimensions to 2, that sounds a bit risky; perhaps 8 or 9 would be a better
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: No serious-minded machine learner would try to reduce a non-correlated, non-redundant
    dataset with 784 dimensions to only 2\. Even if the data is highly correlated
    and redundant, like the `MNIST` dataset, asking to go from 784 down to 2 is a
    big stretch. It is a very risky decision that may get rid of important, discriminant,
    relevant information; perhaps 50 or 100 would be a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: There is no general way of finding which amount of dimensions is good. It is
    a process that requires experimentation. If you want to become good at this, you
    must do your due diligence and at least try two or more experiments with different
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical implications of manipulating data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ethical implications and risks when manipulating data that you
    need to know. We live in a world where most deep learning algorithms will have
    to be corrected, by re-training them, because it was found that they were biased
    or unfair. That is very unfortunate; you want to be a person who exercises responsible AI
    and produces carefully thought out models.
  prefs: []
  type: TYPE_NORMAL
- en: When manipulating data, be careful about removing outliers from the data just
    because you think they are decreasing your model's performance. Sometimes, outliers
    represent information about protected groups or minorities, and removing those
    perpetuates unfairness and introduces bias toward the majority groups. Avoid removing
    outliers unless you are absolutely sure that they are errors caused by faulty
    sensors or human error.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful of the way you transform the distribution of the data. Altering the
    distribution is fine in most cases, but if you are dealing with demographic data,
    you need to pay close attention to what you are transforming.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with demographic information such as gender, encoding female and
    male as 0 and 1 could be risky if we are considering proportions; we need to be
    careful not to promote equality (or inequality) that does not reflect the reality
    of the community that will use your models. The exception is when our current
    reality shows unlawful discrimination, exclusion, and bias. Then, our models (based
    on our data) should not reflect this reality, but the lawful reality that our
    community wants. That is, we will prepare good data to create models not to perpetuate
    societal problems, but models that will reflect the society we want to become.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed many data manipulation techniques that we will
    come back to use all the time. It is good for you to spend time doing this now
    rather than later. It will make our modeling of deep learning architectures easier.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you are now able to manipulate and produce binary
    data for classification or for feature representation. You also know how to deal
    with categorical data and labels and prepare it for classification or regression.
    When you have real-valued data, you now know how to identify statistical properties
    and how to normalize such data. If you ever have the problem of data that has
    non-normal or non-uniform distributions, now you know how to fix that. And if
    you ever encounter problems of not having enough data, you learned a few data
    augmentation techniques. Toward the end of this chapter, you learned some of the
    most popular dimensionality reduction techniques. You will learn more of these
    along the road, for example, when we talk about autoencoders, which can be used
    for dimensionality reduction as well. But sit tight, we will get there in due
    time.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we will continue our journey toward the next introductory topic about
    basic machine learning. [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data,* introduces the most elementary concepts around the theory of deep
    learning, including measuring performance on regression and classification, as
    well as the identification of overfitting. However, before we go there, please
    try to quiz yourself with the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Which variables of the heart dataset are suitable for regression?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actually, all of them. But the ideal ones are those that are real-valued.
  prefs: []
  type: TYPE_NORMAL
- en: '**Does the scaling of the data change the distribution of the data? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. The distribution remains the same. Statistical metrics such as the mean
    and variance may change, but the distribution remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the main difference between supervised and unsupervised dimensionality
    reduction methods?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supervised algorithms use the target labels, while unsupervised algorithms do
    not need that information.
  prefs: []
  type: TYPE_NORMAL
- en: '**When is it better to use batch-based dimensionality reduction?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have very large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cleveland Heart Disease Dataset (1988). Principal investigators:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert
    Detrano, M.D., Ph.D.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.J., Sandhu,
    S., Guppy, K.H., Lee, S. and Froelicher, V., (1989). International application
    of a new probability algorithm for the diagnosis of coronary artery disease. *The
    American journal of cardiology*, 64(5), 304-310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng, L. (2012). The MNIST database of handwritten digit images for machine
    learning research (best of the web). *IEEE Signal Processing Magazine*, 29(6),
    141-142.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sezgin, M., and Sankur, B. (2004). Survey over image thresholding techniques
    and quantitative performance evaluation. *Journal of Electronic imaging*, 13(1),
    146-166.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potdar, K., Pardawala, T. S., and Pai, C. D. (2017). A comparative study of
    categorical variable encoding techniques for neural network classifiers. *International
    Journal of Computer Applications*, 175(4), 7-9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin, B. (2018). Which neural net architectures give rise to exploding and
    vanishing gradients?. In*Advances in Neural Information Processing Systems* (pp.
    582-591).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrews, D. F., Gnanadesikan, R., and Warner, J. L. (1971). Transformations
    of multivariate data. *Biometrics*, 825-840.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Dyk, D. A., and Meng, X. L. (2001). The art of data augmentation. *Journal
    of Computational and Graphical Statistics*, 10(1), 1-50.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schölkopf, B., Smola, A., and Müller, K. R. (1997, October). Kernel principal
    component analysis. In *International conference on artificial neural networks*
    (pp. 583-588). Springer, Berlin, Heidelberg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton, G. E. (2012). A practical guide to training restricted Boltzmann machines.
    In *Neural networks: Tricks of the trade* (pp. 599-619). Springer, Berlin, Heidelberg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (June, 2009). Online dictionary
    learning for sparse coding. In *Proceedings of the 26th annual international conference
    on machine learning* (pp. 689-696). ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
