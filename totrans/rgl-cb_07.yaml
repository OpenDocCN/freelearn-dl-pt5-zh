- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover several tricks and techniques to regularize neural
    networks. We will reuse the L2 regularization technique, as we did in linear models,
    for example. But there are other techniques not yet presented in this book, such
    as early stopping and dropout, which will be covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing a neural network with L2 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a neural network with early stopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will train neural networks on various tasks. This will
    require us to use the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchvision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a neural network with L2 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like a linear model, whether it be a linear regression or a logistic regression,
    neural networks have weights. And so, just like a linear model, L2 penalization
    can be used on those weights to regularize the neural network. In this recipe,
    we will apply L2 penalization to a neural network on the MNIST handwritten digits
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, when training a neural network on this task in [*Chapter 6*](B19629_06.xhtml#_idTextAnchor162),
    there was a small overfitting after 20 epochs, and the results were an accuracy
    of 97% on the train set and 95% on the test set. Let’s try to reduce this overfitting
    by adding L2 regularization in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like for linear models, L2 regularization is just adding a new L2 term
    to the loss. Given the weights W=w1,w2,..., the added term to the loss would be
    ![](img/Formula_07_001.png). The consequence of this added term to the loss is
    that the weights are more constrained and must stay close to zero to keep the
    loss small. As a result, it adds bias to the model and then can help regularize
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This notation for the weights is simplified here. Actually, there are weights
    ![](img/Formula_07_002.png) for each unit `i`, each feature `j`, and each layer
    `l`. But in the end, the L2 term remains the sum of all the squared weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, only three libraries are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`matplotlib` for plots'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch` for deep learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchvision` for the MNIST dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These can be installed with `pip install matplotlib` `torch torchvision`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we reuse the exact same code as in the previous chapter when
    training a multiclass classification model on the MNIST dataset. The only difference
    will be at *step 6* – feel free to jump there if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data is the MNIST handwritten dataset: grayscale images of 28x28
    pixels. The data will thus need to be rescaled and flattened before being able
    to train a custom neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries. As in previous recipes, we import several useful
    `torch` modules and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` containing required classes for building a neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for activation functions such as ReLU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataLoader` for handling the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And we have some imports from `torchvision`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MNIST` for loading the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms` for transforming the dataset – both rescaling and flattening the
    data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the transformations. The `Compose` class is used here to compose
    three transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`transforms.ToTensor()`: Convert the input image in to `torch.Tensor` format'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Normalize()`: Normalize the image with the mean value and standard
    deviation. Will subtract the mean (i.e., `0.1307`) and then divide it by the standard
    deviation (i.e., `0.3081`) for each pixel value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Lambda(torch.flatten)`: Flatten the 2D tensor in to a 1D tensor:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Images are commonly normalized with a mean and standard deviation of 0.5\. We
    normalize with those specific values because the dataset is made with specific
    images, but 0.5 would work fine too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the train and test sets, as well as the train and test data loaders. Using
    the `MNIST` class, we both get the train and test sets using the `train=True`
    and `train=False` parameters, respectively. We apply the previously defined transformations
    directly while loading the data with the `MNIST` class too. Then we instantiate
    the data loaders with a batch size of `64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network. We define here, by default, a neural network made
    of 2 hidden layers of 24 units. The output layer has 10 units since there are
    10 classes (digits between 0 and 9). Finally, the `softmax` function is applied
    to the output layer, allowing the sum of the 10 units to be strictly equal to
    `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check the code, we instantiate the model with the right input shape of `784`
    (28x28 pixels) and check the forward propagation works properly on a given random
    tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output would be something like the following (only the sum must be
    equal to 1; other numbers may be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss function as the cross-entropy loss, available as `nn.CrossEntropyLoss()`
    in `pytorch`, and the optimizer as `Adam`. Here we set another parameter to the
    `Adam` optimizer: `weight_decay=0.001`. This parameter is the strength of the
    L2 penalization. By default, `weight_decay` is `0`, meaning there is no L2 penalization.
    A higher value means a higher regularization, just like in linear models in scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `epoch_step` helper function allowing to compute forward and
    backward propagation (for the training set only) as well as the loss and accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can finally train the neural network on 20 epochs and compute the loss and
    accuracy for each epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since we both train on the train set and evaluate on the test set, the model
    is switched to `train` mode with `model.train()` before training, whereas before
    evaluating on the test set, it is switched to `eval` mode with `model.eval()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'On the last epoch, the output should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'For visualization purposes, we can plot the loss for both the train and test
    sets as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the
    previous code](img/B19629_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the
    previous code
  prefs: []
  type: TYPE_NORMAL
- en: We can notice that the loss seems to be almost the same for both the training
    and test set, with no clear divergence. In the previous attempts without L2 penalization,
    the losses were further apart from each other, meaning we effectively regularized
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Showing related results, we can do it with accuracy too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Accuracy as a function of the epoch; output from the previous
    code](img/B19629_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Accuracy as a function of the epoch; output from the previous code
  prefs: []
  type: TYPE_NORMAL
- en: At the end, the accuracy is about 96% for both the train set and the test set,
    with no significant overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if L2 regularization is a quite common technique to regularize linear models
    such as linear regression and logistic regression, it is not usually the first
    choice with deep learning. Other methods such as early stopping or dropout are
    usually preferred.
  prefs: []
  type: TYPE_NORMAL
- en: On another note, in this recipe, we keep mentioning only the train and test
    sets. But to optimize the `weight_decay` hyperparameter properly, it is required
    to use a validation set; otherwise, the results will be biased. We have simplified
    this recipe by having only two sets to keep it concise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, in deep learning, any other hyperparameter optimization,
    such as the number of layers, number of units, activation functions, and so on
    must be optimized for the validation set too, not just for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may seem strange to adjust the L2 penalization through the optimizer of the
    model rather than directly in the loss function, and indeed it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, it would be possible to manually add an L2 penalization, but it
    would probably be suboptimal. See this PyTorch thread for more about this design
    choice, as well as an example of adding L1 penalization: [https://discuss.pytorch.org/t/simple-l2-regularization/139](https://discuss.pytorch.org/t/simple-l2-regularization/139).'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing a neural network with early stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Early stopping is a commonly employed approach in deep learning to prevent
    the overfitting of models. The concept is straightforward yet effective: if the
    model is overfitting due to prolonged training epochs, we terminate the training
    prematurely to prevent overfitting. We can utilize this technique on the breast
    cancer dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a perfect world, there is no need for regularization. What that means is
    that for both the train and validation sets, the losses are almost perfectly equal,
    for any number of epochs, as in *Figure 7**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Example with no overfitting of train and valid losses as a function
    of the number of epochs](img/B19629_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Example with no overfitting of train and valid losses as a function
    of the number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: But it’s not always that perfect. In practice, it may happen that the neural
    network is learning more and more about the data distribution of the train set
    at every epoch, at the cost of the generalization to new data. This case is depicted
    by the example in *Figure 7**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Example with overfitting of train and valid losses as a function
    of the number of epochs](img/B19629_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Example with overfitting of train and valid losses as a function
    of the number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with such a scenario, a natural solution would be to halt the training
    process once the **valid** loss of the model stops decreasing. Once the validation
    loss of the model stops decreasing, continuing to train the model for additional
    epochs may cause it to become better at memorizing the training data, rather than
    improving its ability to make accurate predictions on new, unseen data. This technique
    is **called early stopping** and allows to prevent a model from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the
    learning and consider the model fully trained; this is early stopping](img/B19629_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the learning
    and consider the model fully trained; this is early stopping
  prefs: []
  type: TYPE_NORMAL
- en: Since this recipe will be applied to the breast cancer dataset, scikit-learn
    must be installed, along with `torch` for the models and `matplotlib` for visualization.
    These libraries can be installed with `pip install sklearn` `torch matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will first train a neural network on the breast cancer dataset
    and visualize the overfitting effect amplifying with the number of epochs. Then,
    we will implement early stopping, to regularize.
  prefs: []
  type: TYPE_NORMAL
- en: Regular training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the breast cancer dataset is rather small, instead of splitting the dataset
    into train, valid, and test sets, we will consider only the train and valid sets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed libraries from `scikit-learn`, `matplotlib`, and `torch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_breast_cancer` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` to split the data into training and validation sets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` to rescale the quantitative data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy_score` to evaluate the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for display'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` itself'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` containing required classes for building a neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for activation functions such as ReLU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` and `DataLoader` for handling the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the features and labels with the `load_breast_cancer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and validation sets, specifying the random state
    for reproducibility, and convert the features and labels in to `float32` for later
    compatibility with PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `Dataset` class for handling the data. We are simply reusing the
    class implemented in the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the training and validation sets and loaders for PyTorch. Notice
    that we provide the training scaler when instantiating the validation dataset
    to make sure the scaler used with both datasets is the one fitted on the training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network architecture – 2 hidden layers of 36 units and an
    output layer with 1 unit with a sigmoid activation function since it’s a binary
    classification task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the model with the expected input shape (the number of features).
    Optionally, we can check the forward propagation works properly on a given random
    tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this code is the following (the value itself may change, but
    will be between 0 and 1 since it’s a sigmoid activation function on the last layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss function as the binary cross entropy loss since this is a binary
    classification task. Instantiate the optimizer too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement a helper function, `epoch_step`, that computes forward propagation,
    backpropagation (for the training set), loss, and accuracy for one epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now implement the `train_model` function allowing us to train a model,
    with or without patience. This function stores each epoch and then returns the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss and accuracy for the train set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy for the valid set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now train the neural network on 500 epochs reusing the previously implemented
    `train_model` function. Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'After 500 epochs, the code output will be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot the loss for both training and validation sets, as a function
    of the epoch, and visualize the overfitting effect increasing with the number
    of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few
    bumps, the training loss keeps decreasing)](img/B19629_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few bumps,
    the training loss keeps decreasing)
  prefs: []
  type: TYPE_NORMAL
- en: We indeed have a training loss that keeps decreasing overall, even reaching
    a value of zero. On the other hand, the valid loss starts decreasing to reach
    a minimum somewhere around epoch 100 and then increases slowly over the epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement early stopping to avoid this situation in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: After the first training, we could retrain the model up to 100 epochs (or any
    identified optimal validation loss), hopefully having the same results. This would
    be a waste of CPU time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could save the model at every epoch, and then pick the best one afterward.
    This solution is sometimes implemented but can be a waste of storage memory, especially
    for large models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could automatically stop the training after a given number of epochs not
    improving validation loss. The minimum number of steps without validation loss
    improvement is usually called patience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now implement the latter solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Using patience is risky too: a too-small patience may get the model stuck in
    a local minimum, while a too-large patience may miss the actual optimal epoch
    by stopping too late.'
  prefs: []
  type: TYPE_NORMAL
- en: Training with patience and early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now retrain a model using early stopping. We first instantiate a fresh
    model to avoid training an already trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a fresh model as well as a fresh optimizer. No need to test it,
    nor to instantiate the loss again if you are using the same notebook kernel. If
    you want to run this code separately, *steps 1* to *8* of the previous recipe
    must be reused:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now train this model with a patience of `30`. After 30 epochs without improving
    the `val` loss, the training will just stop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output will be something like the following (the total number of epochs
    before reaching the early stopping may vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: The training stopped after about 100 epochs (the result may vary since the results
    are not deterministic by default), with a validation accuracy of about 98%, far
    better than the 96% that we got after 500 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the train and validation losses again as a function of the number
    of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Cross-entropy loss as a function of the epoch](img/B19629_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the validation loss is already overfitting but did not have time
    to grow too much, preventing further overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As explained earlier in this recipe, for proper evaluation, it would be necessary
    to compute the accuracy (or any selected evaluation metric) on a separate test
    set. Indeed, stopping the training based on the validation set and evaluating
    the model on this same dataset is a biased approach, and may artificially improve
    the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will explore a less popular, but still sometimes useful,
    regularization method: adapting the neural network architecture. After reviewing
    why to use this method and when, we will apply it to the California housing dataset,
    a regression task.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, the best way to regularize is not to use any fancy techniques but
    only common sense. In many cases, it happens that the neural network used is just
    too large for the input task and dataset. An easy rule of thumb is to have a quick
    look at the number of parameters in the network (e.g., weights and biases) and
    compare it to the number of data points: if the ratio is above 1 (i.e., there
    are more parameters than data points), there is a risk of severe overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If transfer learning is used, this rule of thumb no longer applies since the
    network has been trained on a presumably large enough dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a step back and go back to linear models such as linear regression,
    it is well known that having too many correlated features can deteriorate the
    model’s performance. It can be the same for neural networks: having too many free
    parameters will do no good to the performances. So, depending on the task, it
    is not always required to have dozens of layers; just a few may be enough to get
    the best performances and avoid overfitting. Let’s check that in practice on the
    California dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: To do so, the libraries needed are scikit-learn, Matplotlib, and PyTorch. They
    can be installed with `pip install sklearn` `matplotlib torch`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This will be a two-step recipe: first, we will train a large model (compared
    to the dataset) on the data, to expose the effect of the network on overfitting.
    Then, we will train another, more adapted model on this same data, hopefully fixing
    the overfitting issue.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a large model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the steps to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following imports are needed first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`fetch_california_housing` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` to split the data into training and test sets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` to rescale the features'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score` to evaluate the model at the end'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` to display the loss'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` itself for some functions at the lower level of the library'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn`, which has many useful classes for building a neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` for some useful functions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` and `DataLoader` for handling the data operations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the code for these `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data using the `fetch_california_housing` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets with a ratio of 80%/20%, using the
    `train_test_split` function. Set a random state for reproducibility. For `pytorch`,
    the data is converted in to `float32` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale the data using the standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `CaliforniaDataset` class, allowing to handle the data. The only
    transformation here is the conversion from a `numpy` array to a `torch` tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the datasets for the train and test sets and the data loaders.
    We define here a batch size of `64` but this can be modified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the neural network architecture. We create a large model here on purpose
    considering the dataset – 5 hidden layers of 128 units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the model with the given input shape (the number of features).
    Optionally, we can check the network is correctly created using an input tensor
    of the expected shape (so here is the number of features):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the loss to be a mean squared error loss since this is a regression
    task, and define the optimizer to be `Adam`, with a learning rate of `0.001`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, train the neural network on 500 epochs by using the `train_model`
    function. The implementation of this function is similar to previous ones and
    can be found in the GitHub repository. Again, we purposely chose a large number
    of epochs; otherwise, the overfitting could be compensated by early stopping.
    We also store the train and test losses for each epoch, for visualization purposes
    and information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 500 epochs, the final output lines will be like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the loss for both the train and test set as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE250]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE252]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Mean squared error loss as a function of the epoch (note the
    clear divergence between the train and test losses)](img/B19629_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Mean squared error loss as a function of the epoch (note the clear
    divergence between the train and test losses)
  prefs: []
  type: TYPE_NORMAL
- en: We can notice that the train loss keeps decreasing over and over, while the
    test loss soon reaches a plateau before increasing again. This is a clear sign
    of overfitting. Let’s confirm there is overfitting by computing the R2-scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s evaluate the model on both the training and test sets with the
    R2-score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE254]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE255]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE258]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE263]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code will output values such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we are facing a clear overfitting here, with an almost perfect
    R2-score on the train set, and an R2-score of about 0.76 on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This may look like an exaggerated example, but it is fairly easy to choose an
    architecture that is way too large for the task and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with a smaller network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now train a more reasonable model and see how this impacts overfitting,
    even with the same number of epochs. The goal is not only to decrease overfitting
    but also to get better performances on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using the same kernel, there is no need to redo the first steps.
    Otherwise, *steps 1* to *6* must be redone:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the neural network. This time, we only have two hidden layers of 16
    units each, so this is much smaller than earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE265]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE267]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE268]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the network with the expected number of input features and the
    optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the neural network over 500 epochs so that we have results that we can
    compare to the previous ones. We will reuse the `train_model` function already
    used earlier in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the loss as a function of the epoch for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Mean squared error loss as a function of the epoch (note the
    train and test sets almost overlapping)](img/B19629_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Mean squared error loss as a function of the epoch (note the train
    and test sets almost overlapping)
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, this time, even with many epochs, there is no strong overfitting:
    the train and test losses remain close to each other no matter the number (except
    for a few noise bumps), even if a small amount of overfitting seems to appear
    over time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s again evaluate the model with the R2-score on the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE302]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the typical output of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: While the R2-score on the training set decreased from 0.99 to 0.81, the score
    on the test set increased from 0.76 to 0.79, effectively improving the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Even if it was a rather extreme example, the general idea remains true.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping could work well too in this case. The two techniques (early stopping
    and downsizing the network) are not mutually exclusive and can work well together.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model complexity can arguably be computed using the number of parameters.
    Even if it’s not a direct measure, it remains a good indicator.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the first neural network used in this recipe, with 10 hidden layers
    of 128 units, had 67,329 trainable parameters. On the other side, the second neural
    network, with only 2 hidden layers of 16 units, had only 433 trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of parameters in a fully connected neural network is based on the
    number of units and the number of layers: both units and layers do not have to
    be the same on the number of parameters though.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the number of trainable parameters in the torch network’s net, we
    can use the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: 'To get an idea, let’s take again three examples of neural networks with the
    same number of neurons, but with a different number of layers. Let’s assume they
    all have 10 input features and 1 unit output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network with 1 hidden layer of 100 units: 1,201 parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A neural network with 2 hidden layers of 50 units: 3,151 parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A neural network with 10 hidden layers of 10 units: 1,111 parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, there is a trade-off between the number of layers and the number of units
    per layer to get the most complex neural network for a given number of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A widely used method for regularizing is dropout. Dropout is just randomly setting
    some neurons’ activations to zero during the training phase. Let’s first review
    how this works and then apply it to a multiclass classification task, the `sklearn`
    digits dataset, which is kind of an older and smaller version of the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout is a widely adopted regularization approach in deep learning, due to
    its simplicity and effectiveness. The technique is easy to understand, yet can
    yield powerful results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle is simple – during training, we randomly ignore some units by
    setting their activations to zero, as represented in *Figure 7**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – On the left, a standard neural network with its connections,
    and, on the right, the same neural network with dropout, having, on average, 50%
    of its neurons ignored at training](img/B19629_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – On the left, a standard neural network with its connections, and,
    on the right, the same neural network with dropout, having, on average, 50% of
    its neurons ignored at training
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout adds one hyperparameter though: the dropout probability. For a 0% probability,
    there is no dropout. For a 50% probability, about 50% of the neurons will be randomly
    selected to be ignored. For a 100% probability, well, there is nothing left to
    learn. The ignored neurons are not always the same: for each new batch size, a
    new set of units is randomly selected to be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The remaining activations are consequently scaled to keep a consistent global
    input for any unit. In practice, for a dropout probability of 1/2, all the neurons
    that are not ignored are scaled by a factor of 2 (i.e., their activations are
    multiplied by 2).
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, when evaluating or inferring on new data, dropout is deactivated,
    causing all neurons to be activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what is the point of doing that? Why would randomly ignoring some neurons
    help? A formal explanation is beyond the scope of this book, but at least we can
    provide some intuition. The idea is to avoid confusing the neural network with
    too much information. As a human, having too much information can hurt more than
    it helps: sometimes, having less information allows you to make better decisions,
    preventing you from being flooded by it. This is the idea of dropout: instead
    of giving the network all the information at once, it is gently trained with less
    information by turning off a few neurons randomly for a short amount of time.
    Hopefully, this will help the network make better decisions in the end.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, this will be run on the `digits` dataset of scikit-learn, which
    is just a link to the *Optical Recognition of Handwritten Digits* dataset. A small
    subset of these images is represented in *Figure 7**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – A sample of images from the dataset and their labels: each
    image is composed of 8x8 pixels](img/B19629_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11 – A sample of images from the dataset and their labels: each image
    is composed of 8x8 pixels'
  prefs: []
  type: TYPE_NORMAL
- en: Each image is an 8x8-pixel picture of a handwritten digit. Thus, the dataset
    is made up of 10 classes, 1 for each digit.
  prefs: []
  type: TYPE_NORMAL
- en: To run the code of this recipe, the required libraries are `sklearn`, `matplotlib`,
    and `torch`. They can be installed with `pip install sklearn` `matplotlib torch`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will comprise two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will train a neural network without dropout with a rather large model,
    considering the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will train the same neural network with dropout, hopefully, to improve
    the model’s performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use the same data for both configurations, the same batch size, and
    the same number of epochs, so that we can compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Without dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the steps to regularize without dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following imports must be loaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`load_digits` from `sklearn` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` from `sklearn` to split the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch`, `torch.nn`, and `torch.nn.functional` for the neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset` and `DataLoader` from `torch` for the dataset loading in `torch`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for the visualization of the loss'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for the `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data. The dataset is made of 1,797 samples, and the images are already
    flattened to 64 values between 0 and 16 for the 8x8 pixels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets, with 80% in the training set and
    20% in the test set. The features are converted in to `float32`, while the labels
    are converted into `int64` to avoid `torch` errors later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `DigitsDataset` class for PyTorch. The only transformation to the
    features, besides converting them into `torch` tensors, is to divide the values
    by 255 to have a range of features in `[``0, 1]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the datasets for the train and test sets and the data loaders with
    a batch size of `64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE327]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network architecture – here, there are 3 hidden layers of
    128 units (by default) and a dropout probability set to 25% applied to all the
    hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE336]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE337]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE338]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE339]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE340]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE341]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE342]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE343]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE344]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE345]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE346]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE347]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE348]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE349]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE350]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE351]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE352]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE353]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE354]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE355]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, dropout is added in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate an `nn.Dropout(p=dropout)` class in the constructor, having the
    provided dropout probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the dropout layer (defined in the constructor) after the activation function
    for each hidden layer with `x =` `self.dropout(x)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a ReLU activation function, setting the dropout before or after
    the activation function won’t change the output. For other activation functions
    such as the sigmoid, though, this makes a difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the model with the right input shape of `64` (8x8 pixels) and a
    dropout of `0` since we want to check the results without dropout first. Check
    the forward propagation works properly on a given random tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE356]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE357]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE358]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE359]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE360]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE361]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE362]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this code should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE363]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss function as the cross-entropy loss and the optimizer as `Adam`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE364]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE365]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the neural network on 500 epochs using the `train_model` function available
    in the GitHub repository. For each epoch, we store and compute the loss and the
    accuracy for both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE366]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE367]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE368]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE369]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE370]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 500 epochs, you should get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE371]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the cross-entropy loss for both the training and test sets as a function
    of the epoch number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE372]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE373]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE374]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE375]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE376]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE377]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight
    divergence between train and test sets)](img/B19629_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight
    divergence between train and test sets)
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting the accuracy will show the equivalent results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE378]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE379]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE380]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE381]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE382]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE383]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Accuracy as a function of the epoch; we can again notice the
    overfitting](img/B19629_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Accuracy as a function of the epoch; we can again notice the overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The final accuracy is about 98% on the train set and only about 95% on the test
    set, showing overfitting. Let’s try now to add dropout to reduce this overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: With dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we will simply restart from *step 7*, but with dropout, and then
    compare the results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the model with an input share of `64` and a dropout probability
    of 25%. A probability of 25% means that during the training, in each of the hidden
    layers, about 32 randomly selected neurons will be ignored. Instantiate a fresh
    optimizer, still using `Adam`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE384]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE385]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE386]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the neural network again for 500 epochs, while storing the train and
    test loss and accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE387]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE388]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE389]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE390]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE391]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE392]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the train and test losses again as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE393]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE394]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE395]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE396]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE397]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE398]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced
    divergence thanks to dropout](img/B19629_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced divergence
    thanks to dropout
  prefs: []
  type: TYPE_NORMAL
- en: We face a different behavior here than seen previously. The train and test losses
    do not seem to grow apart too much with the epochs. During the initial 100 epochs,
    the test loss is marginally lower than the train loss, but afterward, the train
    loss decreases further, indicating slight overfitting of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, plot the train and test accuracy as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE399]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE400]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE401]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE402]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE403]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE404]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely
    reduced thanks to dropout)](img/B19629_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely
    reduced thanks to dropout)
  prefs: []
  type: TYPE_NORMAL
- en: We have a train accuracy of 99% against the 98% seen previously. More interestingly,
    the test accuracy climbed to 97%, from 95% previously, effectively regularizing
    and reducing the overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although dropout is not always foolproof, it has been demonstrated to be an
    effective regularization technique, particularly when training large networks
    on small datasets. More about this can be found in the publication *Improving
    neural networks by preventing co-adaptation of feature detectors*, by Hinton et
    al. This publication can be found here on `arxiv`: [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official location of the `digits` dataset: [https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PyTorch documentation about dropout: [https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
