<html><head></head><body>
		<div>
			<div id="_idContainer142" class="Content">
			</div>
		</div>
		<div id="_idContainer143" class="Content">
			<h1 id="_idParaDest-141"><a id="_idTextAnchor142"/>7. Computer Vision with Convolutional Neural Networks</h1>
		</div>
		<div id="_idContainer158" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter covers computer vision and how this is accomplished with neural networks. You will learn to build image processing applications and classify models with convolutional neural networks. You will also study the architecture of convolutional neural networks and how to utilize techniques such as max pooling and flattening, feature mapping, and feature detection. By the end of this chapter, you will be able to not only build your own image classifiers but also evaluate them effectively for your own applications.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>Introduction</h1>
			<p>In the previous chapter, we explored model evaluation in detail. We covered <strong class="source-inline">accuracy</strong> and why it may be misleading for some datasets, especially for classification tasks with highly imbalanced classes. Datasets with imbalanced classes such as the prediction of hurricanes in the Pacific Ocean or the prediction of whether someone will default on their credit card loan have positive instances that are relatively rare compared to negative instances, so accuracy scores are misleading since the null accuracy is so high.</p>
			<p>To combat class imbalance, we learned about techniques that we can use to appropriately evaluate our model, including calculating model evaluation metrics such as the sensitivity, specificity, false positive rate, and <strong class="source-inline">AUC score</strong>, and plotting the <strong class="source-inline">ROC curve</strong>. In this chapter, we will learn how to classify another type of dataset—namely, images. Image classification is extremely useful and there are many real-world applications of it, as we will discover.</p>
			<p><strong class="bold">Computer vision</strong> is one of the most important concepts in machine learning and artificial intelligence. With the wide use of smartphones for capturing, sharing, and uploading images every day, the amount of data that's generated through images is increasing exponentially. So, the need for experts who are specialized in the field of computer vision is at an all-time high. Industries such as the health care industry are on the verge of a revolution due to the progress that's been made in the field of medical imaging. </p>
			<p>This chapter will introduce you to computer vision and the various industries in which computer vision is used. You will also learn about <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>), which are the most widely used neural networks for image processing. Like neural networks, CNNs are also made up of neurons that receive inputs that are processed using weighted sums and activation functions. However, unlike <strong class="source-inline">ANNs</strong>, which use vectors as inputs, CNN uses images as its input. In this chapter, we will be studying <strong class="source-inline">CNNs</strong> in greater detail, along with the associated concepts of <strong class="bold">max pooling</strong>, <strong class="bold">flattening</strong>, <strong class="bold">feature maps</strong>, and <strong class="bold">feature selection</strong>. We will use Keras as a tool to run image processing algorithms on real-life images.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor144"/>Computer Vision</h1>
			<p>To understand computer vision, let's discuss human vision. Human vision is the ability of the human eye and brain to see and recognize objects. Computer vision is the process of giving a machine a similar, if not better, understanding of seeing and identifying objects in the real world.</p>
			<p>It is fairly simple for the human eye to precisely identify whether an animal is a tiger or a lion, but it takes a lot of training for a computer system to understand such objects distinctly. Computer vision can also be defined as building mathematical models that can mimic the function of a human eye and brain. Basically, it is about training computers to understand and process images and videos.</p>
			<p>Computer vision is an integral part of many cutting-edge areas of robotics: health care and medical (X-rays, MRI scans, CT scans, and so on), drones, self-driving cars, sports and recreation, and so on. Almost all businesses need computer vision to run successfully. </p>
			<p>Imagine a large amount of data that's generated by CCTV footage across the world, the number of pictures our smartphones capture each day, the number of videos that are shared on internet sites such as YouTube on a daily basis, and the pictures we share on popular social networking sites such as Facebook and Instagram. All of this generates huge volumes of image data. To process and analyze this data and make computers more intelligent in terms of processing, this data requires high-level experts who specialize in computer vision. Computer vision is a highly lucrative field in machine learning. The following sections will describe how computer vision is achieved with neural networks—and particularly convolutional neural networks—that perform well for computer vision tasks.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor145"/>Convolutional Neural Networks</h1>
			<p>When we talk about computer vision, we talk about CNNs in the same breath. CNN is a class of deep neural network that is mostly used in the field of computer vision and imaging. CNNs are used to identify images, cluster them by their similarity, and implement object recognition within scenes. CNN has different layers— namely, the input layer, the output layer, and multiple hidden layers. These hidden layers of a CNN consist of fully connected layers, convolutional layers, a <strong class="source-inline">ReLU layer</strong> as an <strong class="source-inline">activation function</strong>, <strong class="source-inline">normalization layers</strong>, and <strong class="source-inline">pooling layers</strong>. On a very simple level, CNNs help us identify images and label them appropriately; for example, a tiger image will be identified as a tiger:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B15777_07_01.jpg" alt="Figure 7.1: A generalized CNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1: A generalized CNN</p>
			<p>The following is an example of a CNN classifying a tiger:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B15777_07_02.jpg" alt="Figure 7.2: A CNN classifying an image of a tiger into the class “Tiger”&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2: A CNN classifying an image of a tiger into the class "Tiger"</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor146"/>The Architecture of a CNN</h1>
			<p>The main components of CNN architecture are as follows:</p>
			<ul>
				<li><strong class="source-inline">Input image</strong></li>
				<li><strong class="source-inline">Convolutional layer</strong></li>
				<li><strong class="source-inline">Pooling layer</strong></li>
				<li><strong class="source-inline">Flattening</strong></li>
			</ul>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor147"/>Input Image</h2>
			<p>An <strong class="source-inline">input image</strong> forms the first component of a CNN architecture. An image can be of any type: a human, an animal, scenery, a medical X-ray image, and so on. Each image is converted into a mathematical matrix of zeros and ones. The following figure explains how a computer views an image of the letter <strong class="bold">T</strong>. </p>
			<p>All the blocks that have a value of one represent the data, while the zeros represent blank space:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B15777_07_03.jpg" alt="Figure 7.3: Matrix for the letter ‘T’&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3: Matrix for the letter 'T'</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor148"/>Convolution Layer</h2>
			<p>The <strong class="source-inline">convolution layer</strong> is the place where image processing starts. A convolution layer consists of two parts:</p>
			<ul>
				<li><strong class="source-inline">Feature detector</strong> or <strong class="source-inline">filter</strong></li>
				<li><strong class="source-inline">Feature map</strong></li>
			</ul>
			<p><strong class="source-inline">Feature detector</strong> or a <strong class="source-inline">filter</strong>: This is a matrix or pattern that you put on an image to transform it into a feature map:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B15777_07_04.jpg" alt="Figure 7.4: Feature detector&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4: Feature detector</p>
			<p>As we can see, this feature detector is put (superimposed) on the original image and the computation is done on the corresponding elements. The computation is done by multiplying the corresponding elements, as shown in the following figure. This process is repeated for all the cells. This results in a new processed image— <strong class="source-inline">(0x0+0x0+0x1) + (0x1+1x0+0x0) + (0x0+0x1+0x1) = 0</strong>:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B15777_07_05.jpg" alt="Figure 7.5: Feature detector masked in an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5: Feature detector masked in an image</p>
			<p><strong class="source-inline">Feature Map</strong>: This is the reduced image that is produced by the convolution of an <strong class="source-inline">image</strong> and <strong class="source-inline">feature detector</strong>. We have to put the feature detector on all the possible locations of the original image and derive a smaller image from it; that derived image is the feature map of the input image:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B15777_07_06.jpg" alt="Figure 7.6: Feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6: Feature map</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Here, the <strong class="source-inline">feature detector</strong> is the filter and the <strong class="source-inline">feature map</strong> is the reduced image. Some information is lost while reducing the image.</p>
			<p>In an actual CNN, a number of feature detectors are used to produce a number of feature maps, as shown in the following figure:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B15777_07_07.jpg" alt="Figure 7.7: Multiple feature detectors and maps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7: Multiple feature detectors and maps</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor149"/>The Pooling Layer</h2>
			<p>The <strong class="source-inline">pooling layer</strong> helps us ignore the less important data in the image and reduces the image further, all while preserving its important features. Consider the following three images, which contain four cats in total:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B15777_07_08.jpg" alt="Figure 7.8: Example of cat images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8: Example of cat images</p>
			<p>To identify whether an image has a cat in it or not, the neural network analyzes the picture. It may look at ear shape, eye shape, and so on. At the same time, the image consists of lots of features that are not related to cats. The tree and leaves in the first two images are useless in the identification of the cat. The pooling mechanism helps the algorithm understand which parts of the image are relevant and which parts are irrelevant.</p>
			<p>The feature map derived from the convolution layer is passed through a pooling layer to further reduce the image, all while preserving the most relevant part of the image. The pooling layer consists of functions such as max pooling, min pooling, and average pooling. What this means is that we select a matrix size, say <strong class="source-inline">2x2</strong>, and we scan the feature map and select the maximum number from the <strong class="source-inline">2x2</strong> matrix that fits in that block. The following image gives us a clear idea of how max pooling works. Refer to the colors; the max number in each of the colored boxes from the feature map is selected in the pooled feature map:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B15777_07_09.jpg" alt="Figure 7.9: Pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9: Pooling</p>
			<p>Consider the case of the box that has number <strong class="source-inline">4</strong> in it. Let's assume that number <strong class="source-inline">4</strong> represents the ears of a cat, while the blank space around the ears is <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. So, we ignore the <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> of that block and only select <strong class="source-inline">4</strong>. The following is some example code that we would use to add a pooling layer; here, <strong class="source-inline">Maxpool2D</strong> is used for max pooling, which helps identify the most important features:</p>
			<p class="source-code">classifier.add(MaxPool2D(2,2))</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/>Flattening</h2>
			<p><strong class="source-inline">Flattening</strong> is part of a CNN where the image is made ready to use as an input to an ANN. As the name suggests, the pooled image is flattened and converted into a single column. Each row is made into a column and stacked one over another. Here, we have converted a <strong class="source-inline">3x3</strong> matrix into a <strong class="source-inline">1xn</strong> matrix, where <strong class="source-inline">n</strong>, in our case, is <strong class="source-inline">9</strong>:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B15777_07_10.jpg" alt="Figure 7.10: Flattening&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10: Flattening</p>
			<p>In real-time, we have a number of pooled feature maps, and we flatten them into a single column. This single column is used as input for an ANN. The following figure shows a number of pooled layers flattened into a single column:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B15777_07_11.jpg" alt="Figure 7.11: Pooling and flattening&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11: Pooling and flattening</p>
			<p>The following is some example code that we would use to add a flattening layer; here <strong class="source-inline">Flatten</strong> is used for flattening the CNN:</p>
			<p class="source-code">classifier.add(Flatten())</p>
			<p>Now, let's look at the overall structure of a CNN:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B15777_07_12.jpg" alt="Figure 7.12: CNN architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12: CNN architecture</p>
			<p>The following is some example code that we would use to add the first layer to a CNN:</p>
			<p class="source-code">classifier.add(Conv2D(32,3,3,input_shape=(64,64,3),activation='relu'))</p>
			<p><strong class="source-inline">32,3,3</strong> refers to the fact that there are <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">3x3</strong>. As a good practice, always start with <strong class="source-inline">32</strong>; you can add <strong class="source-inline">64</strong> or <strong class="source-inline">128</strong> later.</p>
			<p><strong class="source-inline">Input_shape</strong>: Since all the images are of different shapes and sizes, this <strong class="source-inline">input_image</strong> converts all the images into a uniform shape and size. <strong class="source-inline">(64,64)</strong> is the dimension of the converted image. It can be set to <strong class="source-inline">128</strong> or <strong class="source-inline">256</strong>, but if you are working on a CPU on a laptop, it is advisable to use <strong class="source-inline">64x64</strong>. The last argument, <strong class="source-inline">3</strong>, is used because the image is a colored image (coded in red, blue, and green, or RGB). If the image is black and white, the argument can be set to one. The activation function that's being used is ReLU.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We are using Keras with TensorFlow as the backend in this book. If the backend is Theano, then <strong class="source-inline">input_image</strong> will be coded as (<strong class="source-inline">3,64,64</strong>).</p>
			<p>The last step is to fit the data that's been created. Here is the code that we use to do so:</p>
			<p class="source-code">classifier.fit_generator(training_set,steps_per_epoch = 5000,\</p>
			<p class="source-code">                         epochs = 25,validation_data = test_set,\</p>
			<p class="source-code">                         validation_steps = 1000)</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">steps_per_epoch</strong> is the number of training images. <strong class="source-inline">validation_steps</strong> is the number of test images.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor151"/>Image Augmentation</h1>
			<p>The word <strong class="bold">augmentation</strong> means the action or process of making or becoming greater in size or amount. <strong class="bold">Image</strong> or <strong class="bold">data augmentation</strong> works in a similar manner. Image/data augmentation creates many batches of our images. Then, it applies random transformations to random images inside the batches. Data transformation can be rotating images, shifting them, flipping them, and so on. By applying this transformation, we get more diverse images inside the batches, and we also have much more data than we had originally.</p>
			<p>A cylinder can be rotated from different angles and seen differently. In the following figure, a single cylinder can be seen from five different angles. So, we have effectively created five different images from a single image:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B15777_07_13.jpg" alt="Figure 7.13: Image augmentation of a cylinder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13: Image augmentation of a cylinder</p>
			<p>The following is some example code that we would use for image augmentation; here, the <strong class="source-inline">ImageDataGenerator</strong> class is used for processing. <strong class="source-inline">shear_range</strong>, <strong class="source-inline">zoom_range</strong>, and <strong class="source-inline">horizontal_flip</strong> are all used for transforming the images:</p>
			<p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p>
			<p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255.0,\</p>
			<p class="source-code">                                   shear_range = 0.3,\</p>
			<p class="source-code">                                   zoom_range = 0.3,\</p>
			<p class="source-code">                                   horizontal_flip = False)</p>
			<p class="source-code">test_datagen = ImageDataGenerator(rescale = 1./255.0)</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor152"/>Advantages of Image Augmentation</h2>
			<p>Image augmentation is an important part of processing images:</p>
			<ul>
				<li><strong class="bold">Reduces overfitting</strong>: It helps reduce overfitting by creating multiple versions of the same image, rotated by a given amount.</li>
				<li><strong class="bold">Increases the number of images</strong>: A single image acts as multiple images. So, essentially, the dataset has fewer images, but each image can be converted into multiple images with image augmentation. Image augmentation will increase the number of images and each image will be treated differently by the algorithm.</li>
				<li><strong class="bold">Easy to predict new images</strong>: Imagine that a single image of a football is looked at from different angles and each angle is considered a distinct image. This will mean that the algorithm will be more accurate at predicting new images:</li>
			</ul>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B15777_07_14.jpg" alt="Figure 7.14: Image augmentation of an image of a football&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14: Image augmentation of an image of a football</p>
			<p>Now that we have learned about the concepts and theory behind computer vision with CNNs, let's work on some practical examples. </p>
			<p>First, we will start with an exercise in which we'll build a simple CNN. In the following exercises and activities, we will tweak our CNN using permutation and combining the following:</p>
			<ul>
				<li>Adding more CNN layers</li>
				<li>Adding more ANN layers</li>
				<li>Changing the optimizer function</li>
				<li>Changing the activation function</li>
			</ul>
			<p>Let's begin by creating our first CNN so that we can classify images of cars and flowers into their respective classes.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor153"/>Exercise 7.01: Building a CNN and Identifying Images of Cars and Flowers</h2>
			<p>For this exercise, we have images of cars and flowers, which have been divided into training and testing sets, and we have to build a CNN that identifies whether an image is a car or a flower. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the exercises and activities in this chapter will be developed in Jupyter notebooks. Please download this book's GitHub repository, along with all the prepared templates, from <a href="https://packt.live/39tID2C">https://packt.live/39tID2C</a>.</p>
			<p>Before you begin, ensure that you have downloaded the image datasets from this book's GitHub repository to your own working directory. You will need a <strong class="source-inline">training_set</strong> folder to train your model and a <strong class="source-inline">test_set</strong> folder to test your model. Each of these folders will contain a <strong class="source-inline">cars</strong> folder, containing car images, and a <strong class="source-inline">flowers</strong> folder, containing flower images.</p>
			<p>The steps for completing this exercise are as follows:</p>
			<ol>
				<li>Import the <strong class="source-inline">numpy</strong> library and the necessary Keras libraries and classes:<p class="source-code"># Import the Libraries</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Conv2D, MaxPool2D, Flatten, Dense</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p></li>
				<li>Now, set a seed and initiate the model with the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the classifier</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">classifier = Sequential()</p></li>
				<li>Add the first layer of the <strong class="source-inline">CNN</strong>, set the input shape to <strong class="source-inline">(64, 64, 3)</strong>, the dimension of each image, and set the activation function as a <strong class="source-inline">ReLU</strong>:<p class="source-code">classifier.add(Conv2D(32,3,3, input_shape=(64,64,3), \</p><p class="source-code">               activation='relu'))</p><p><strong class="source-inline">32,3,3</strong> shows that there are <strong class="source-inline">32</strong> feature detectors of <strong class="source-inline">3x3</strong> size.</p></li>
				<li>Now, add the pooling layer with the image size as <strong class="source-inline">2x2</strong>:<p class="source-code">classifier.add(MaxPool2D(2,2))</p></li>
				<li>Flatten the output of the pooling layer by adding a flattening layer to the <strong class="source-inline">CNN</strong> model:<p class="source-code">classifier.add(Flatten())</p></li>
				<li>Add the first <strong class="source-inline">Dense</strong> layer of the <strong class="source-inline">ANN</strong>. Here, <strong class="source-inline">128</strong> is the output of the number of nodes. As a good practice, <strong class="source-inline">128</strong> is good to get started. <strong class="source-inline">activation</strong> is <strong class="source-inline">relu</strong>. As a good practice, the power of two is preferred:<p class="source-code">classifier.add(Dense(128, activation='relu')) </p></li>
				<li>Add the output layer of the ANN. This is a binary classification problem, so the size is <strong class="source-inline">1</strong> and the activation is <strong class="source-inline">sigmoid</strong>: <p class="source-code">classifier.add(Dense(1, activation='sigmoid')) </p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and compute the accuracy during the training process:<p class="source-code">#Compile the network</p><p class="source-code">classifier.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">                   metrics=['accuracy'])</p></li>
				<li>Create training and test data generators. Rescale the training and test images by <strong class="source-inline">1/255</strong> so that all the values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Set these parameters for the training data generators only – <strong class="source-inline">shear_range=0.2</strong>, <strong class="source-inline">zoom_range=0.2</strong>, and <strong class="source-inline">horizontal_flip=True</strong>:<p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255,\</p><p class="source-code">                                   shear_range = 0.2,\</p><p class="source-code">                                   zoom_range = 0.2,\</p><p class="source-code">                                   horizontal_flip = True)</p><p class="source-code">test_datagen = ImageDataGenerator(rescale = 1./255)</p></li>
				<li>Create a training set from the <strong class="source-inline">training set</strong> folder. <strong class="source-inline">'../dataset/training_set'</strong> is the folder where our data has been placed. Our CNN model has an image size of <strong class="source-inline">64x64</strong>, so the same size should be passed here too. <strong class="source-inline">batch_size</strong> is the number of images in a single batch, which is <strong class="source-inline">32</strong>. <strong class="source-inline">Class_mode</strong> is set to <strong class="source-inline">binary</strong> since we are working on binary classifiers:<p class="source-code">training_set = train_datagen.flow_from_directory(\</p><p class="source-code">               '../dataset/training_set',\</p><p class="source-code">               target_size = (64, 64),\</p><p class="source-code">               batch_size = 32,\</p><p class="source-code">               class_mode = 'binary')</p></li>
				<li>Repeat <em class="italic">step 10</em> for the test set while setting the folder to the location of the test images, that is, <strong class="source-inline">'../dataset/test_set'</strong>:<p class="source-code">test_set = test_datagen.flow_from_directory(\</p><p class="source-code">           '../dataset/test_set',\</p><p class="source-code">           target_size = (64, 64),\</p><p class="source-code">           batch_size = 32,\</p><p class="source-code">           class_mode = 'binary')</p></li>
				<li>Finally, fit the data. Set the <strong class="source-inline">steps_per_epoch</strong> to <strong class="source-inline">10000</strong> and the <strong class="source-inline">validation_steps</strong> to <strong class="source-inline">2500</strong>. The following step might take some time to execute:<p class="source-code">classifier.fit_generator(training_set,steps_per_epoch = 10000,\</p><p class="source-code">                         epochs = 2,validation_data = test_set,\</p><p class="source-code">                         validation_steps = 2500,shuffle=False)</p><p>The preceding code produces the following output:</p><p class="source-code">Epoch 1/2</p><p class="source-code">10000/10000 [==============================] - 1994s 199ms/step - loss: 0.2474 - accuracy: 0.8957 - val_loss: 1.1562 - val_accuracy: 0.8400</p><p class="source-code">Epoch 2/2</p><p class="source-code">10000/10000 [==============================] - 1695s 169ms/step - loss: 0.0867 - accuracy: 0.9689 - val_loss: 1.4379 - val_accuracy: 0.8422</p><p>The accuracy on the validation set is <strong class="source-inline">84.22%</strong>.</p><p class="callout-heading">Note</p><p class="callout">To get more accurate results, try increasing the number of epochs to about <strong class="source-inline">25</strong>. This will increase the time that it takes to process the data, and the total time is dependent on the configuration of your machine.</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38njqHU">https://packt.live/38njqHU</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iqFpSN">https://packt.live/3iqFpSN</a>.</p></li>
			</ol>
			<p>That completes this exercise on processing images and identifying the contents of the images. An important thing to remember here is that this is a robust code for any binary classification problem in computer vision. This means that the code remains the same, even if the image data changes. We will test our knowledge of this by modifying some of the parameters of our model in the next activity and evaluating the model's performance.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor154"/>Activity 7.01: Amending Our Model with Multiple Layers and the Use of softmax</h2>
			<p>Since we have run a <strong class="source-inline">CNN model</strong> successfully, the next logical step is to try and improve the performance of our algorithm. There are many ways to improve its performance, and one of the most straightforward ways is by adding multiple ANN layers to the model, which we will learn about in this activity. We will also change the activation from sigmoid to softmax. By doing this, we can compare the result with that of the previous exercise. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">To build a CNN import library, set a seed and create a <strong class="source-inline">Sequential</strong> class and import <strong class="source-inline">Conv2D</strong>, <strong class="source-inline">MaxPool2D</strong>, <strong class="source-inline">Flatten</strong>, and <strong class="source-inline">Dense</strong>. <strong class="source-inline">Conv2D</strong> is used to build the convolution layer. Since our pictures are in 2D, we have used 2D here. Similarly, <strong class="source-inline">Maxpool2D</strong> is used for max pooling, <strong class="source-inline">Flatten</strong> is used for flattening the CNN, and <strong class="source-inline">Dense</strong> is used to add a fully connected CNN to an ANN.</li>
				<li>Start building a CNN architecture using the preceding libraries. After adding the first layer, add two additional layers to your CNN.</li>
				<li>Add a pooling and flattening layer to it, which will serve as the input for the ANN.</li>
				<li>Build a fully connected ANN whose inputs will be the output of the CNN. After adding the first layer of your ANN, add three additional layers. For the output layer of your ANN, use the softmax activation function. Compile the model.</li>
				<li>Perform image augmentation to process and transform the data. The <strong class="source-inline">ImageDataGenerator</strong> class is used for processing. <strong class="source-inline">shear_range</strong>, <strong class="source-inline">zoom_range</strong>, and <strong class="source-inline">horizontal_flip</strong> are all used for the transformation of images.</li>
				<li>Create the training and test set data. </li>
				<li>Lastly, fit the data that's been created.</li>
			</ol>
			<p>After implementing these steps, you should get the following expected output:</p>
			<p class="source-code">Epoch 1/2</p>
			<p class="source-code">10000/10000 [==============================] - 2452s 245ms/step - loss: 8.1783 - accuracy: 0.4667 - val_loss: 11.4999 - val_accuracy: 0.4695</p>
			<p class="source-code">Epoch 2/2</p>
			<p class="source-code">10000/10000 [==============================] - 2496s 250ms/step - loss: 8.1726 - accuracy: 0.4671 - val_loss: 10.5416 - val_accuracy: 0.4691</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 439.</p>
			<p>In this activity, we have modified our CNN model to try and improve the accuracy of our image classifier. We have added additional convolutional layers and additional ANN fully connected layers and changed the activation function in the output layer. By doing so our accuracy has decreased. In the next exercise, we will change the activation function back to a sigmoid. We will evaluate the performance by observing the accuracy evaluated on the validation dataset.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/>Exercise 7.02: Amending Our Model by Reverting to the Sigmoid Activation Function</h2>
			<p>In this exercise, we will rebuild our model but revert the activation function from softmax back to sigmoid. By doing this, we can compare the accuracy with our previous model's. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary Keras libraries and classes:<p class="source-code"># Import the Libraries </p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Conv2D, MaxPool2D, Flatten, Dense</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p></li>
				<li>Now, set the seed and initiate the model with the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the classifier</p><p class="source-code">seed = 43</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">classifier = Sequential()</p></li>
				<li>Add the first layer of the CNN, set the input shape to <strong class="source-inline">(64, 64, 3)</strong>, the dimension of each image, and set the activation function as a ReLU. Then, add <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">(3, 3)</strong>. Add two additional convolutional layers with <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">(3, 3)</strong>, also with ReLU activation functions:<p class="source-code">classifier.add(Conv2D(32,3,3,input_shape=(64,64,3),\</p><p class="source-code">                      activation='relu'))</p><p class="source-code">classifier.add(Conv2D(32, (3, 3), activation = 'relu'))</p><p class="source-code">classifier.add(Conv2D(32, (3, 3), activation = 'relu'))</p></li>
				<li>Now, add the pooling layer with the image size as <strong class="source-inline">2x2</strong>:<p class="source-code">classifier.add(MaxPool2D(2,2))</p></li>
				<li>Add one more <strong class="source-inline">Conv2D</strong> with the same parameters as in <em class="italic">step 3</em> and a pooling layer to supplement it with the same parameters that we used in <em class="italic">step 4</em>:<p class="source-code">classifier.add(Conv2D(32, (3, 3), activation = 'relu'))</p><p class="source-code">classifier.add(MaxPool2D(pool_size = (2, 2)))</p></li>
				<li>Flatten the output of the pooling layer by adding a flattening layer to the <strong class="source-inline">CNN model</strong>:<p class="source-code">classifier.add(Flatten())</p></li>
				<li>Add the first <strong class="source-inline">Dense</strong> layer of the ANN. Here, <strong class="source-inline">128</strong> is the output of the number of nodes. As a good practice, <strong class="source-inline">128</strong> is good to get started. <strong class="source-inline">activation</strong> is <strong class="source-inline">relu</strong>. As a good practice, the power of two is preferred. Add three additional layers with the same parameters:<p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p></li>
				<li>Add the output layer of the <strong class="source-inline">ANN</strong>. This is a binary classification problem, so the output is <strong class="source-inline">1</strong> and the activation is <strong class="source-inline">sigmoid</strong>:<p class="source-code">classifier.add(Dense(1,activation='sigmoid')) </p></li>
				<li>Compile the network with an Adam optimizer and compute the accuracy during the training process:<p class="source-code">classifier.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">                   metrics=['accuracy'])</p></li>
				<li>Create training and test data generators. Rescale the training and test images by <strong class="source-inline">1/255</strong> so that all the values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Set these parameters for the training data generators only – <strong class="source-inline">shear_range=0.2</strong>, <strong class="source-inline">zoom_range=0.2</strong>, and <strong class="source-inline">horizontal_flip=True</strong>:<p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255,</p><p class="source-code">                                   shear_range = 0.2,</p><p class="source-code">                                   zoom_range = 0.2,</p><p class="source-code">                                   horizontal_flip = True)</p><p class="source-code">test_datagen = ImageDataGenerator(rescale = 1./255)</p></li>
				<li>Create a training set from the <strong class="source-inline">training set</strong> folder. <strong class="source-inline">../dataset/training_set</strong> is the folder where our data is placed. Our CNN model has an image size of 64x64, so the same size should be passed here too. <strong class="source-inline">batch_size</strong> is the number of images in a single batch, which is <strong class="source-inline">32</strong>. <strong class="source-inline">class_mode</strong> is binary since we are working on binary classifiers:<p class="source-code">training_set = \</p><p class="source-code">train_datagen.flow_from_directory('../dataset/training_set',\</p><p class="source-code">                                  target_size = (64, 64),\</p><p class="source-code">                                  batch_size = 32,\</p><p class="source-code">                                  class_mode = 'binary')</p></li>
				<li>Repeat <em class="italic">step 11</em> for the test set by setting the folder to the location of the test images, that is, <strong class="source-inline">'../dataset/test_set'</strong>:<p class="source-code">test_set = \</p><p class="source-code">test_datagen.flow_from_directory('../dataset/test_set',\</p><p class="source-code">                                 target_size = (64, 64),\</p><p class="source-code">                                 batch_size = 32,\</p><p class="source-code">                                 class_mode = 'binary')</p></li>
				<li>Finally, fit the data. Set the <strong class="source-inline">steps_per_epoch</strong> to <strong class="source-inline">10000</strong> and the <strong class="source-inline">validation_steps</strong> to <strong class="source-inline">2500</strong>. The following step might take some time to execute:<p class="source-code">classifier.fit_generator(training_set,steps_per_epoch = 10000,\</p><p class="source-code">                         epochs = 2,validation_data = test_set,\</p><p class="source-code">                         validation_steps = 2500,shuffle=False)</p><p>The preceding code produces the following output:</p><p class="source-code">Epoch 1/2</p><p class="source-code">10000/10000 [==============================] - 2241s 224ms/step - loss: 0.2339 - accuracy: 0.9005 - val_loss: 0.8059 - val_accuracy: 0.8737</p><p class="source-code">Epoch 2/2</p><p class="source-code">10000/10000 [==============================] - 2394s 239ms/step - loss: 0.0810 - accuracy: 0.9699 - val_loss: 0.6783 - val_accuracy: 0.8675</p></li>
			</ol>
			<p>The accuracy of the model is <strong class="source-inline">86.75%</strong>, which is clearly greater than the accuracy of the model we built in the previous exercise. This shows the importance of activation functions. Just changing the output activation function from softmax to sigmoid increased the accuracy from <strong class="source-inline">46.91%</strong> to <strong class="source-inline">86.75%</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZD9nKM">https://packt.live/2ZD9nKM</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3dPZiiQ">https://packt.live/3dPZiiQ</a>.</p>
			<p>In the next exercise, we will experiment with a different optimizer and observe how that affects the model's performance.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In a binary classification problem (in our case, cars versus flowers), it is always better to use sigmoid as the activation function for the output.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/>Exercise 7.03: Changing the Optimizer from Adam to SGD</h2>
			<p>In this exercise, we will amend the model again by changing the optimizer to <strong class="source-inline">SGD</strong>. By doing this, we can compare the accuracy with our previous models. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary Keras libraries and classes:<p class="source-code"># Import the Libraries </p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Conv2D, MaxPool2D, Flatten, Dense</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p></li>
				<li>Now, initiate the model with the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the classifier</p><p class="source-code">seed = 42</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">classifier = Sequential()</p></li>
				<li>Add the first layer of the <strong class="source-inline">CNN</strong>, set the input shape to <strong class="source-inline">(64, 64, 3)</strong>, the dimension of each image, and set the activation function as <strong class="source-inline">ReLU</strong>. Then, add <strong class="source-inline">32</strong> feature detectors of size (<strong class="source-inline">3, 3</strong>). Add two additional convolutional layers with the same number of feature detectors with the same size:<p class="source-code">classifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),\</p><p class="source-code">               activation='relu'))</p><p class="source-code">classifier.add(Conv2D(32,(3,3),activation='relu'))</p><p class="source-code">classifier.add(Conv2D(32,(3,3),activation='relu'))</p></li>
				<li>Now, add the pooling layer with the image size as <strong class="source-inline">2x2</strong>:<p class="source-code">classifier.add(MaxPool2D(pool_size=(2, 2)))</p></li>
				<li>Add one more <strong class="source-inline">Conv2D</strong> with the same parameters as in <em class="italic">step 3</em> and a pooling layer to supplement it with the same parameters that we used in <em class="italic">step 4</em>:<p class="source-code">classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), \</p><p class="source-code">               activation = 'relu'))</p><p class="source-code">classifier.add(MaxPool2D(pool_size=(2, 2)))</p></li>
				<li>Add a <strong class="source-inline">Flatten</strong> layer to complete the CNN architecture:<p class="source-code">classifier.add(Flatten())</p></li>
				<li>Add the first <strong class="source-inline">Dense</strong> layer of the ANN of size <strong class="source-inline">128</strong>. Add three more dense layers to the network with the same parameters:<p class="source-code">classifier.add(Dense(128,activation='relu')) </p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p></li>
				<li>Add the output layer of the ANN. This is a binary classification problem, so the output is <strong class="source-inline">1</strong> and the activation is <strong class="source-inline">sigmoid</strong>:<p class="source-code">classifier.add(Dense(1,activation='sigmoid')) </p></li>
				<li>Compile the network with an <strong class="source-inline">SGD optimizer</strong> and compute the accuracy during the training process:<p class="source-code">classifier.compile(optimizer='SGD', loss='binary_crossentropy', \</p><p class="source-code">                   metrics=['accuracy'])</p></li>
				<li>Create training and test data generators. Rescale the training and test images by <strong class="source-inline">1/255</strong> so that all the values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Set these parameters for the training data generators only – <strong class="source-inline">shear_range=0.2</strong>, <strong class="source-inline">zoom_range=0.2</strong>, and <strong class="source-inline">horizontal_flip=True</strong>:<p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255,\</p><p class="source-code">                                   shear_range = 0.2,\</p><p class="source-code">                                   zoom_range = 0.2,\</p><p class="source-code">                                   horizontal_flip = True)</p><p class="source-code">test_datagen = ImageDataGenerator(rescale = 1./255)</p></li>
				<li>Create a training set from the <strong class="source-inline">training set</strong> folder. <strong class="source-inline">../dataset/training_set</strong> is the folder where our data is placed. Our CNN model has an image size of <strong class="source-inline">64x64</strong>, so the same size should be passed here too. <strong class="source-inline">batch_size</strong> is the number of images in a single batch, which is <strong class="source-inline">32</strong>. <strong class="source-inline">class_mode</strong> is binary since we are creating a binary classifier:<p class="source-code">training_set = \</p><p class="source-code">train_datagen.flow_from_directory('../dataset/training_set',\</p><p class="source-code">                                  target_size = (64, 64),\</p><p class="source-code">                                  batch_size = 32,\</p><p class="source-code">                                  class_mode = 'binary')</p></li>
				<li>Repeat <em class="italic">step 11</em> for the test set by setting the folder to the location of the test images, that is, <strong class="source-inline">'../dataset/test_set'</strong>:<p class="source-code">test_set = \</p><p class="source-code">test_datagen.flow_from_directory('../dataset/test_set',\</p><p class="source-code">                                 target_size = (64, 64),\</p><p class="source-code">                                 batch_size = 32,\</p><p class="source-code">                                 class_mode = 'binary')</p></li>
				<li>Finally, fit the data. Set the <strong class="source-inline">steps_per_epoch</strong> to <strong class="source-inline">10000</strong> and the <strong class="source-inline">validation_steps</strong> to <strong class="source-inline">2500</strong>. The following step might take some time to execute:<p class="source-code">classifier.fit_generator(training_set,steps_per_epoch = 10000,\</p><p class="source-code">                         epochs = 2,validation_data = test_set,\</p><p class="source-code">                         validation_steps = 2500,shuffle=False)</p><p>The preceding code produces the following output:</p><p class="source-code">Epoch 1/2</p><p class="source-code">10000/10000 [==============================] - 4376s 438ms/step - loss: 0.3920 - accuracy: 0.8201 - val_loss: 0.3937 - val_accuracy: 0.8531</p><p class="source-code">Epoch 2/2</p><p class="source-code">10000/10000 [==============================] - 5146s 515ms/step - loss: 0.2395 - accuracy: 0.8995 - val_loss: 0.4694 - val_accuracy: 0.8454</p><p>The accuracy is <strong class="source-inline">84.54%</strong> since we have used multiple <strong class="source-inline">ANNs</strong> and <strong class="source-inline">SGD</strong> as the optimizer.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31Hu9vm">https://packt.live/31Hu9vm</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3gqE9x8">https://packt.live/3gqE9x8</a>.</p></li>
			</ol>
			<p>So far, we have worked with a number of different permutations and combinations of our model. It seems like the best accuracy for this dataset can be obtained by doing the following:</p>
			<ul>
				<li>Adding multiple CNN layers.</li>
				<li>Adding multiple ANN layers.</li>
				<li>Having the activation as sigmoid.</li>
				<li>Having the optimizer as adam.</li>
				<li>Increasing the epoch size to about <strong class="source-inline">25</strong> (this takes a lot of computational time – make sure you have a GPU to do this). This will increase the accuracy of your predictions.</li>
			</ul>
			<p>Finally, we will go ahead and predict a new unknown image, pass it to the algorithm, and validate whether the image is classified correctly. In the next exercise, we will demonstrate how to use the model to classify new images.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>Exercise 7.04: Classifying a New Image</h2>
			<p>In this exercise, we will try to classify a new image. The image hasn't been exposed to the algorithm, so we will use this exercise to test our algorithm. You can run any of the algorithms in this chapter (although the one that gets the highest accuracy is preferred) and then use the model to classify the image.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The image that's being used in this exercise can be found in this book's GitHub repository at <a href="https://packt.live/39tID2C">https://packt.live/39tID2C</a>.</p>
			<p>Before we begin, ensure that you have downloaded <strong class="source-inline">test_image_1</strong> from this book's GitHub repository to your own working directory. This exercise follows on from the previous exercises, so ensure that you have one of the algorithms from this chapter ready to run in your workspace.</p>
			<p>The steps for completing this exercise are as follows:</p>
			<ol>
				<li value="1">Load the image. <strong class="source-inline">'test_image_1.jpg'</strong> is the path of the test image. Please change the path to where you have saved the dataset in your system. Look at the image to verify what it is:<p class="source-code">from keras.preprocessing import image</p><p class="source-code">new_image = image.load_img('../test_image_1.jpg', \</p><p class="source-code">                           target_size = (64, 64))</p><p class="source-code">new_image</p></li>
				<li>Print the class labels located in the <strong class="source-inline">class_indices</strong> attribute of the training set:<p class="source-code">training_set.class_indices</p></li>
				<li>Process the image:<p class="source-code">new_image = image.img_to_array(new_image)</p><p class="source-code">new_image = np.expand_dims(new_image, axis = 0)</p></li>
				<li>Predict the new image:<p class="source-code">result = classifier.predict(new_image)</p></li>
				<li>The <strong class="source-inline">prediction</strong> method will output the image as <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong>. To map <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong> to <strong class="source-inline">flower</strong> or <strong class="source-inline">car</strong>, use the <strong class="source-inline">class_indices</strong> method with an <strong class="source-inline">if…else</strong> statement, as follows:<p class="source-code">if result[0][0] == 1:</p><p class="source-code">    prediction = 'It is a flower'</p><p class="source-code">else:</p><p class="source-code">    prediction = 'It is a car'</p><p class="source-code">print(prediction)</p><p>The preceding code produces the following output:</p><p class="source-code">It is a car</p><p><strong class="source-inline">test_image_1</strong> is the image of a car (you can see this by viewing the image for yourself) and was correctly predicted to be a car by the model.</p></li>
			</ol>
			<p>In this exercise, we trained our model and then gave the model an image of a car. By doing this, we found out that the algorithm is classifying the image correctly. You can train the model on any type of an image by using the same process. For example, if you train the model with scans of lung infections and healthy lungs, then the model will be able to classify whether a new scan represents an infected lung or a healthy lung.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31I6B9F">https://packt.live/31I6B9F</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2BzmEMx">https://packt.live/2BzmEMx</a>.</p>
			<p>In the next activity, we will put our knowledge into practice by using a model that we trained in <em class="italic">Exercise 7.04</em>, <em class="italic">Classifying a New Image</em>.</p>
			<h2 id="_idParaDest-157">Activity 7.02:<a id="_idTextAnchor158"/> Classifying a New Image</h2>
			<p>In this activity, you will try to classify another new image, just like we did in the preceding exercise. The image is not exposed to the algorithm, so we will use this activity to test our algorithm. You can run any of the algorithms in this chapter (although the one that gets the highest accuracy is preferred) and then use the model to classify your images. The steps to implement this activity are as follows:</p>
			<ol>
				<li value="1">Run any one of the algorithms from this chapter.</li>
				<li>Load the image (<strong class="source-inline">test_image_2</strong>) from your directory.</li>
				<li>Process the image using the algorithm.</li>
				<li>Predict the subject of the new image. You can view the image yourself to check whether the prediction is correct.<p class="callout-heading">Note</p><p class="callout">The image that's being used in this activity can be found in this book's GitHub repository at <a href="https://packt.live/39tID2C">https://packt.live/39tID2C</a>.</p></li>
			</ol>
			<p>Before starting, ensure you have downloaded <strong class="source-inline">test_image_2</strong> from this book's GitHub repository to your own working directory. This activity follows on directly from the previous exercises, so please ensure that you have one of the algorithms from this chapter ready to run in your workspace.</p>
			<p>After implementing these steps, you should get the following expected output:</p>
			<p class="source-code">It is a flower</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 442.</p>
			<p>In this activity, we trained the most performant model in this chapter when given the various parameters that were modified, including the optimizer and the activation function in the output layer according to the accuracy on the validation dataset. We tested the classifier on a test image and found it to be correct.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor159"/>Summary</h1>
			<p>In this chapter, we studied why we need computer vision and how it works. We learned why computer vision is one of the hottest fields in machine learning. Then, we worked with convolutional neural networks, learned about their architecture, and looked at how we can build CNNs in real-life applications. We also tried to improve our algorithms by adding more ANN and CNN layers and by changing the activation and optimizer functions. Finally, we tried out different activation functions and loss functions.</p>
			<p>In the end, we were able to successfully classify new images of cars and flowers through the algorithm. Remember, the images of cars and flowers can be substituted with any other images, such as tigers and deer, or MRI scans of brains with and without a tumor. Any binary classification computer imaging problem can be solved with the same approach.</p>
			<p>In the next chapter, we will study an even more efficient technique for working on computer vision, which is less time-consuming and easier to implement. The following chapter will teach us how to fine-tune pre-trained models for our own applications that will help create more accurate models that can be trained in faster times. The models that will be used are called VGG-16 and ResNet50 and are popular pre-trained models that are used to classify images.</p>
		</div>
	</body></html>