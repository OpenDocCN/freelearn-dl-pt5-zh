<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Training a Single Neuron
                </header>
            
            <article>
                
<p class="mce-root">After revising the concepts around learning from data, we will now pay close attention to an algorithm that trains one of the most fundamental neural-based models: the <strong>perceptron</strong>. We will look at the steps required for the algorithm to function, and the stopping conditions. This chapter will present the perceptron model as the first model that represents a neuron, which aims to learn from data in a simple manner. The perceptron model is key to understanding basic and advanced neural models that learn from data. In this chapter, we will also cover the problems and considerations associated with non-linearly separable data. </p>
<p>Upon completion of the chapter, you should feel comfortable discussing the perceptron model, and applying its learning algorithm. You will be able to implement the algorithm over both linearly and non-linearly separable data.</p>
<p>Specifically, the following topics are covered in this chapter: </p>
<ul>
<li>The perceptron model</li>
<li>The perceptron learning algorithm</li>
<li>A perceptron over non-linearly separable data</li>
</ul>
<h1 id="uuid-80afeaef-4aa6-436b-8bf2-a322fedeac3c">The perceptron model</h1>
<p>Back in <a href="e3181710-1bb7-4069-825a-a235355bc116.xhtml">Chapter 1</a>, <em>Introduction to Machine Learning</em>, we briefly introduced the basic model of a neuron and the <strong>perceptron learning algorithm</strong> (<strong>PLA</strong>). Here, in this chapter, we will now revisit and expand the concept and show how that is coded in Python. We will begin with the basic definition.</p>
<h2 id="uuid-491b7962-497d-43fa-98a2-b1d05e5afcc3">The visual concept</h2>
<p>The perceptron is an analogy of a human-inspired information processing unit, originally conceived by F. Rosenblatt and depicted in <em>Figure 5.1</em> (Rosenblatt, F. (1958)). In the model, the input is represented with the vector <img class="fm-editor-equation" src="assets/dd2553bf-c673-4ade-b77f-c9edb436ed12.png" style="width:0.92em;height:1.00em;"/>, the activation of the neuron is given by the function <img class="fm-editor-equation" src="assets/018df1a9-86c5-40ef-afd0-12da768026e0.png" style="width:2.33em;height:1.75em;"/>, and the output is <img class="fm-editor-equation" src="assets/e9f481bc-0639-4987-8be9-75dd4d377193.png" style="width:0.58em;height:1.00em;"/>. The parameters of the neuron are <img class="fm-editor-equation" src="assets/d16c70d8-9f05-4c25-91f9-61fa60a89974.png" style="width:1.17em;height:0.92em;"/><span> </span>and <img class="fm-editor-equation" src="assets/430e33f2-b6a0-44d0-b21a-c16e32da02b8.png" style="width:0.42em;height:0.83em;"/>: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/bac392e3-087b-485d-aa22-287a161238ce.png" style="width:34.50em;height:18.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.1 – The basic model of a perceptron</div>
<p>The <em>trainable</em><span> </span>parameters of a perceptron are <img src="assets/b7dce5c4-c06a-4c49-99ac-44a00c3dde4a.png" style="width:2.92em;height:1.33em;"/>, and they are unknown. Thus, we can use input training data <img class="fm-editor-equation" src="assets/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png" style="width:6.75em;height:1.33em;"/><span> </span>to determine these parameters using the PLA. From <em>Figure 5.1</em>, <img class="fm-editor-equation" src="assets/5ebb08ed-23c4-4a97-ad97-192adaf9fa16.png" style="width:1.58em;height:1.17em;"/><sub> </sub>multiplies <img class="fm-editor-equation" src="assets/590e73b6-a479-4c5c-9285-4551930c7b91.png" style="width:1.83em;height:1.17em;"/>, then <img class="fm-editor-equation" src="assets/8b81f157-95ad-4b92-9977-3c2f3500a544.png" style="width:1.58em;height:1.17em;"/><sub> </sub>multiplies <img class="fm-editor-equation" src="assets/67beef13-3471-40ef-869e-0c5009fa2a91.png" style="width:1.83em;height:1.17em;"/>, and <img class="fm-editor-equation" src="assets/d0b9f56d-d74f-40fe-9160-7c65b95b390b.png" style="width:0.67em;height:1.33em;"/> is multiplied by 1; all these products are added and then passed into the<span> <em>sign</em> activation function, which </span>in the perceptron operates as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/25c4e4bc-29d9-4ee4-8ff0-0d8ada66ccb1.png" style="width:21.42em;height:3.08em;"/></p>
<div class="packt_infobox">The main purpose of the activation sign is to map any response of the model to a binary output: <sub><img class="fm-editor-equation" src="assets/c2b8debd-da54-4f91-82aa-f673eacf1584.png" style="width:3.50em;height:1.00em;"/></sub>.</div>
<p class="mce-root">Now let's talk about tensors in a general sense.</p>
<p class="mce-root"/>
<h2 id="uuid-c03577ac-f6d5-4781-a8a7-893c5d4b0b8a">Tensor operations</h2>
<p>In Python, the implementation of the perceptron requires a few simple tensor (vector) operations that can be performed through standard NumPy functionalities. First, we can assume that we are given data <sub><img class="fm-editor-equation" src="assets/0d85ebf4-d57f-4fd1-88ee-e155345e4c6f.png" style="width:6.83em;height:1.33em;"/></sub> in the form of a vector containing multiple vectors <img class="fm-editor-equation" src="assets/59afade2-2430-4eba-b0c8-28c69956ec56.png" style="width:0.75em;height:0.83em;"/> (a matrix), represented as <sub><img class="fm-editor-equation" src="assets/585f7ba2-8841-4a53-8431-2d1be8d31af2.png" style="width:5.25em;height:1.25em;"/></sub>, and multiple individual targets represented as a vector <sub><img class="fm-editor-equation" src="assets/b7e3a027-88db-4c9b-ad29-639468ea172f.png" style="width:5.17em;height:1.33em;"/></sub>. However, notice that for easier implementation of the perceptron it will be necessary to include <img class="fm-editor-equation" src="assets/3b0fa764-dbef-4a77-a092-47b93447c9ed.png" style="width:0.50em;height:1.00em;"/> in <img class="fm-editor-equation" src="assets/7d70cc37-fb85-4d34-99b5-37b13204746e.png" style="width:0.83em;height:0.67em;"/>, as suggested in <em>Figure 5.1</em>, so that the products and additions in <sub><img class="fm-editor-equation" src="assets/55992dcb-f61a-4702-9339-fb13e815fdec.png" style="width:17.83em;height:1.17em;"/></sub> can be simplified if we modify <img class="fm-editor-equation" src="assets/5c464c60-6998-4884-9f1d-38fbca822376.png" style="width:0.83em;height:0.92em;"/> to be <img src="assets/5fc1e66a-27b1-4b71-a1f9-e19233e0ffd6.png" style="width:10.08em;height:1.25em;"/>, and <img class="fm-editor-equation" src="assets/71f4dc57-8177-4f50-b003-612501ff1d00.png" style="width:1.08em;height:0.83em;"/> to be <sub><img class="fm-editor-equation" src="assets/c6fb2cba-f4dc-4519-8728-4d7bfb626352.png" style="width:10.08em;height:1.17em;"/></sub>. In this way, the perceptron response for an input <sub><img class="fm-editor-equation" src="assets/e8377d7f-1b36-43b3-882d-751ea830c783.png" style="width:9.58em;height:1.17em;"/></sub> could be simplified to be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e56c3f72-ce08-4071-9aac-2104e2d42362.png" style="width:17.42em;height:3.08em;"/></p>
<p>Notice that <strong><img class="fm-editor-equation" src="assets/6d483edb-3362-4650-a2ef-4ad86675a0ae.png" style="width:0.50em;height:1.00em;"/> </strong>is now implicit in <img class="fm-editor-equation" src="assets/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/>.</p>
<p>Say that we want to have training data <kbd>X</kbd>, which we need to prepare for the perceptron; we can do that with a simple linearly separable dataset that can be generated through scikit-learn's dataset method called <kbd>make_classification</kbd> as follows:</p>
<pre>from sklearn.datasets import <strong>make_classification</strong><br/><br/>X, y = <strong>make_classification</strong>(n_samples=100, n_features=2, n_classes=2,<br/>                           n_informative=2, n_redundant=0, n_repeated=0,<br/>                           n_clusters_per_class=1, class_sep=1.5, <br/>                           random_state=5)</pre>
<p>Here, we use the <kbd>make_classification</kbd> constructor to produce 100 data points (<kbd>n_samples</kbd>) for two classes (<kbd>n_classes</kbd>) and with enough separation (<kbd>class_sep</kbd>) to make data that is linearly separable. However, the dataset produced binary values in <kbd>y</kbd> in the set <sub><img class="fm-editor-equation" src="assets/5fbae48a-781e-4d49-8899-6cefba685f19.png" style="width:2.25em;height:1.08em;"/></sub>, and we need to convert it to the values in the set <sub><img class="fm-editor-equation" src="assets/db866a8e-aee2-419a-9953-74d93d142c8d.png" style="width:3.25em;height:0.92em;"/></sub>. This can be easily achieved by replacing the zero targets with the negative targets by simply doing the following:</p>
<pre>y[y==0] = -1</pre>
<p>The dataset produced looks as depicted in <em>Figure 5.2</em>: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7fe6eb0e-e3ec-42a1-a4f3-7e9aa99ab418.png" style="width:29.17em;height:20.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.2– Sample two-dimensional data for perceptron testing</div>
<p>Next, we can add the number 1 to each input vector by adding a vector of ones to <kbd><span>X</span></kbd> with length <kbd>N=100</kbd> as follows:</p>
<pre>import numpy as np<br/>X = np.append(np.ones((N,1)), X, 1)    </pre>
<p>The new data in <kbd>X</kbd> now contains a vector of ones. This will allow easier calculation of the tensor operation <sub><img class="fm-editor-equation" src="assets/f401a345-a15c-485c-8636-1fefe31ec8d7.png" style="width:2.50em;height:1.25em;"/></sub> for all <img src="assets/f58f01a4-07d4-4577-aabe-6f1b9e92ce78.png" style="width:6.25em;height:1.00em;"/>. This common tensor operation can be performed in one single step considering the matrix <img src="assets/3964917c-2a69-4948-b032-f55e473de016.png" style="width:8.92em;height:1.50em;"/> simply as <img src="assets/caa93965-f1c5-4721-8829-197f116857b1.png" style="width:8.58em;height:1.67em;"/>. We can even combine this operation and the sign activation function in one single step as follows:</p>
<pre>np.sign(<strong>w.T</strong>.dot(<strong>X</strong>[n]))</pre>
<p>This is the equivalent of the mathematical tensor operation <sub><img class="fm-editor-equation" src="assets/3ac75eb8-972d-46c9-9647-b8cda8237618.png" style="width:4.83em;height:1.25em;"/></sub>. With this in mind, let's review the PLA in more detail using the dataset introduced previously, and the operations just described.</p>
<p class="mce-root"/>
<h1 id="uuid-dc5d10ae-c185-4fae-801f-d51fa03c9b0e">The perceptron learning algorithm</h1>
<p class="mce-root">The <strong>perceptron learning algorithm</strong> (<strong>PLA</strong>) is the following:</p>
<p><strong>Input</strong>: Binary class dataset <sub><img src="assets/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png" style="width:7.25em;height:1.42em;"/></sub></p>
<ul>
<li><span>Initialize </span><img style="font-size: 1em;color: black;width:1.08em;height:0.83em;" src="assets/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png"/><span> to zeros, and iteration counter </span><img style="font-size: 1em;color: black;width:2.75em;height:1.00em;" src="assets/467437b4-17a7-42a3-af49-cd3cc8b8816e.png"/></li>
<li>While there are any incorrectly classified examples:</li>
<li style="padding-left: 30px">Pick an incorrectly classified example, call it <img src="assets/01a87dc1-7f59-4c55-9c52-a1803ae95db7.png" style="width:1.08em;height:1.00em;"/>, whose true label is <img src="assets/16eb7743-58d1-4037-ae7f-67757849756f.png" style="width:1.00em;height:1.33em;"/></li>
<li style="padding-left: 30px">Update <img src="assets/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.17em;height:0.92em;"/> as follows: <img src="assets/cfa7a1f4-17cd-4f28-b07e-f17fbac38691.png" style="width:8.00em;height:1.17em;"/></li>
<li style="padding-left: 30px">Increase iteration counter, <sub><img src="assets/99f0af99-2238-4552-9903-2580096b8378.png" style="width:3.58em;height:0.83em;"/></sub>, and repeat</li>
</ul>
<p class="mce-root"><strong>Return</strong>: <img src="assets/8ca08b66-f0e6-466a-bd50-d4d584c2cf06.png" style="width:1.25em;height:1.00em;"/></p>
<p class="mce-root">Now, let's see how this takes form in Python.</p>
<h2 id="uuid-77b681c9-e5c7-413d-b02b-7cd41f6898b1">PLA in Python</h2>
<p>Here is an implementation in Python that we will discuss part by part, while some of it has already been discussed:</p>
<pre>N = 100 # number of samples to generate<br/>random.seed(a = 7) # add this to achieve for reproducibility<br/><br/>X, y = make_classification(n_samples=N, n_features=2, n_classes=2,<br/>                           n_informative=2, n_redundant=0, n_repeated=0,<br/>                           n_clusters_per_class=1, class_sep=1.2, <br/>                           random_state=5)<br/><br/>y[y==0] = -1<br/><br/>X_train = np.append(np.ones((N,1)), X, 1) # add a column of ones<br/><br/># initialize the weights to zeros<br/>w = np.zeros(X_train.shape[1])<br/>it = 0<br/><br/># Iterate until all points are correctly classified<br/>while classification_error(w, X_train, y) != 0:<br/>  it += 1<br/>  # Pick random misclassified point<br/>  x, s = choose_miscl_point(w, X_train, y)<br/>  # Update weights<br/>  w = w + s*x<br/><br/>print("Total iterations: ", it)</pre>
<p>The first few lines have been discussed previously in the <em>Tensor operations</em> section of this chapter. The initialization of <img class="fm-editor-equation" src="assets/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/> to zeros is done with <kbd>w = np.zeros(X_train.shape[1])</kbd>. The size of this vector depends on the dimensionality of the input. Then, <kbd>it</kbd> is merely an iteration counter to keep track of the number of iterations that are performed until the PLA converges.</p>
<p>The <kbd>classification_error()</kbd> method is a helper method that takes as arguments the current vector of parameters <kbd>w</kbd>, the input data <kbd>X_train</kbd>, and corresponding target data <kbd>y</kbd>. The purpose of this method is to determine the number of misclassified points at the present state <img class="fm-editor-equation" src="assets/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/>, if there are any, and return the total count of errors. The method can be defined as follows:</p>
<pre>def classification_error(w, X, y):<br/>  err_cnt = 0<br/>  N = len(X)<br/>  for n in range(N):<br/>    s = np.sign(w.T.dot(X[n]))<br/>    if y[n] != s:<br/>      err_cnt += 1    # we could break here on large datasets<br/>  return err_cnt      # returns total number of errors</pre>
<p>This method could be simplified as follows:</p>
<pre>def classification_error(w, X, y):<br/>  <span>s = np.sign(X.dot(w))</span><br/>  return sum(s != y)</pre>
<p>However, while this is a nice optimization for small datasets, for large datasets it may not be necessary to calculate all points for error. Thus, the first (and longer) method can be used and modified according to the type of data that is expected, and if we know that we will be dealing with large datasets, we could break out of the method at the first sign of error. </p>
<p class="mce-root"/>
<p>The second helper method in our code is <kbd>choose_miscl_point()</kbd>. The main purpose of this method is to select, at random, one of the misclassified points, if there are any. It takes as arguments <span>the current vector of parameters </span><kbd>w</kbd><span>, the input data </span><kbd>X_train</kbd><span>, and corresponding target data </span><kbd>y</kbd><span>. It returns </span>a misclassified point, <kbd>x</kbd>, and what the corresponding target sign should be, <kbd>s</kbd>. The method could be implemented as follows:</p>
<pre>def choose_miscl_point(w, X, y):<br/>  mispts = []<br/>  for n in range(len(X)):<br/>    if np.sign(w.T.dot(X[n])) != y[n]:<br/>      mispts.append((X[n], y[n]))<br/>  return mispts[random.randrange(0,len(mispts))]</pre>
<p>Similarly, this could be optimized for speed by randomizing a list of indices, iterating over them, and returning the first one found, as shown here:</p>
<pre>def choose_miscl_point(w, X, y):<br/> for idx in <span>random.permutation(</span>len(X)):<br/>   if np.sign(w.T.dot(X[idx])) != y[idx]:<br/>     return X[idx], y[idx]</pre>
<p>However, the first implementation can be useful for absolute beginners or for those who would like to do some additional analysis of the misclassified points, which could be conveniently available in the list <kbd>mispts</kbd>.</p>
<p>The crucial point, no matter the implementation, is to randomize the selection of the misclassified point.</p>
<p>Finally, the update happens using the current parameters, the misclassified point, and the corresponding target on the line that executes <kbd>w = w + s*x</kbd>. </p>
<p>If you run the complete program, it should output something like this:</p>
<pre>Total iterations: 14</pre>
<p>The total number of iterations may vary depending on the type of data and the random nature of the selection of the misclassified points. For the particular dataset we are using, the decision boundary could look as shown in <em>Figure 5.3</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/61a2413b-086d-4458-bab7-0433588aa938.png" style="width:29.17em;height:20.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.3 – Decision boundary found with PLA</div>
<p>The number of iterations will also depend on the separation or gap between data points in the feature space. The larger the gap is, the easier it is to find a solution, and the converse is also true. The worst-case scenario is when the data is non-linearly separable, which we'll address next.</p>
<h1 id="uuid-1eef7b49-70a7-446b-a7a3-45e38c840556">A perceptron over non-linearly separable data</h1>
<p>As we have discussed before, a perceptron will find a solution in finite time if the data is separable. However, how many iterations it will take to find a solution depends on how close the groups are to each other in the feature space.</p>
<div class="packt_infobox"><strong>Convergence</strong> is when the learning algorithm finds a solution or reaches a steady state that is acceptable to the designer of the learning model.</div>
<p class="mce-root"/>
<p>The following paragraphs will deal with convergence on different types of data: <span>linearly </span>separable and non-linearly separable.</p>
<h2 id="uuid-a059f10b-9545-41d7-a659-0ad0c03a0b15">Convergence on linearly separable data</h2>
<p>For the particular dataset that we have been studying in this chapter, the separation between the two groups of data is a parameter that can be varied (this is usually a problem with real data). The parameter is <kbd>class_sep</kbd> and can take on a real number; for example:</p>
<pre>X, y = make_classification(..., <strong>class_sep</strong>=2.0, ...)</pre>
<p>This allows us to study how many iterations it takes, on average, for the perceptron algorithm to converge if we vary the separation parameter. The experiment can be designed as follows:</p>
<ul>
<li>We will vary the separation coefficient from large to small, recording the number of iterations it takes to converge: 2.0, 1.9, ..., 1.2, 1.1.</li>
<li>We will repeat this 1,000 times and record the average number of iterations and the corresponding standard deviation.</li>
</ul>
<p>Notice that we decided to run this experiment down to 1.1, since 1.0 already produces a non-linearly separable dataset. If we perform the experiment, we can record the results in a table and it will look like this:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Run</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2.0</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.1</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>13</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>86</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>26</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>62</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>169</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>11</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>29</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>27</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>293</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>998</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>9</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>11</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>9</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>35</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>198</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>999</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>14</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>135</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1000</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>13</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>25</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>27</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>36</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Avg.</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2.79</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3.05</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3.34</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3.67</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4.13</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4.90</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6.67</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>10.32</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>24.22</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>184.41</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Std.</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1.9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2.4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3.0</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4.7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7.8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>15.9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>75.5</strong></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This table shows that the average number of iterations taken is fairly stable when the data is nicely separated; however, as the separation gap is reduced, the number of iterations increases dramatically. To put this in a visual perspective, the same data from the table is now shown in <em>Figure 5.4</em> on a logarithmic scale:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6f74dcd4-9806-4fd8-b92b-035c1de18d94.png" style="width:26.50em;height:18.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.4 – The growth of the number of PLA iterations as the data groups are closer together</div>
<p>It is very clear that the number of iterations can grow exponentially as the separation gap is closing. <em>Figure 5.5</em> depicts the largest separation gap, 2.0, and indicates that the PLA found a solution after four iterations: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5364997e-8957-4e62-9ba5-d183b96c0dec.png" style="width:26.50em;height:18.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 5.5 – The perceptron found a solution in four iterations for a separation gap of 2.0</span></div>
<p class="mce-root"/>
<p><span>Similarly, </span><span><em>Figure 5.6</em> shows that </span><span>for the largest gap, 1.1, the PLA takes 183 iterations; a close inspection of the figure reveals that the solution for the latter case is difficult to find because the data groups are too close to each other:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/cd93c7a8-7a43-491c-b7d6-f61bcbc8b8ef.png" style="width:29.17em;height:20.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.6 – The perceptron found a solution in 183 iterations for a separation gap of 1.1</div>
<p>As noted before, data that is not linearly separable can be produced with a gap of 1.0 and the PLA will run in an infinite loop since there will always be a data point that will be incorrectly classified and the <kbd>classification_error()</kbd> <span>method </span>will never return a zero value. For those cases, we can modify the PLA to allow finding solutions on non-linearly separable data, as we'll cover in the next section.</p>
<h2 id="uuid-6e6c20fb-1ed1-410d-9f23-27adf0dfb696"><span>Convergence on non-linearly separable data</span></h2>
<p class="mce-root">The modifications to the original PLA are rather simple, but are good enough to allow finding an acceptable solution in most cases. The main two things that we need to add to the PLA are as follows:</p>
<ul>
<li>A mechanism to prevent the algorithm from running forever</li>
<li>A mechanism to store the best solution ever found</li>
</ul>
<p class="mce-root">With respect to the first point, we can simply specify a number of iterations at which the algorithm can stop. With respect to the second point, we can simply keep a solution in storage, and compare it to the one in the current iteration.</p>
<p class="mce-root"/>
<p>The relevant portion of the PLA is shown here and the new changes have been marked with bold font and will be discussed in detail:</p>
<pre>X, y = make_classification(n_samples=N, n_features=2, n_classes=2,<br/> n_informative=2, n_redundant=0, n_repeated=0,<br/> n_clusters_per_class=1, class_sep=1.0, <br/> random_state=5)<br/><br/>y[y==0] = -1<br/><br/>X_train = np.append(np.ones((N,1)), X, 1) # add a column of ones<br/><br/># initialize the weights to zeros<br/>w = np.zeros(X_train.shape[1])<br/>it = 0<br/><strong>bestW = {}<br/>bestW['err'] = N + 1 # dictionary to keep best solution</strong><br/><strong>bestW['w'] = []</strong><br/><strong>bestW['it'] = it</strong><br/><br/># Iterate until all points are correctly classified<br/>#   or maximum iterations (i.e. 1000) are reached<br/><strong>while it &lt; 1000:</strong><br/><strong>  err = classification_error(w, X_train, y)</strong><br/><strong>  if err &lt; bestW['err']:   # enter to save a new w</strong><br/><strong>    bestW['err'] = err</strong><br/><strong>    bestW['it'] = it</strong><br/><strong>    bestW['w'] = list(w)</strong><br/><strong>  if err == 0:  # exit loop if there are no errors</strong><br/><strong>    break</strong><br/>  it += 1<br/>  # Pick random misclassified point<br/>  x, s = choose_miscl_point(w, X_train, y)<br/>  # Update weights<br/>  w += s*x<br/><br/><strong>print("Best found at iteration: ", bestW['it'])</strong><br/><strong>print("Number of misclassified points: ", bestW['err'])</strong></pre>
<p>In this code, <kbd>bestW</kbd> is a dictionary for keeping track of the best results so far, and it is initialized to reasonable values. Notice first that the loop is now bounded by the number 1,000, which is the maximum number of iterations you currently allow and you can change it to anything you desire to be the maximum number of allowed iterations. It would be reasonable to reduce this number for large datasets or high-dimensional datasets where every iteration is costly. </p>
<p class="mce-root"/>
<p>The next changes are the inclusion of the conditional statement, <kbd>if err &lt; bestW['err']</kbd>, which determines whether we should store a new set of parameters. Every time the error, as determined by the total number of misclassified examples, is lower than the error of the stored parameters, then an update is made. And just for completion, we have to still check that there are no errors, which indicates that the data is linearly separable, a solution has been found, and the loop needs to terminate.</p>
<p>The last few <kbd>print</kbd> statements will simply inform the iteration and error obtained when the best solution was recorded. The output may look as follows:</p>
<pre>Best found at iteration: 95<br/>Number of misclassified points: 1</pre>
<p>This output was produced by running the updated PLA over the dataset with a separation of 1.0, which is depicted in <em>Figure 5.7</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/2e1b5cd4-6720-4c62-bd35-ec5bdf23c214.png" style="width:29.17em;height:20.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.7 – The updated PLA finds a solution with only one misclassified point after 95 iterations</div>
<p>From the figure, it can be seen that there is one sample from the positive class that is incorrectly classified. Knowing that in this example there is a total of 100 data points, we can determine that the accuracy is 99/100.</p>
<p>This type of algorithm, which stores the <em>best solution so far</em>, is usually known as a <strong>pocket algorithm</strong> (Muselli, M. 1997). And the idea of the early termination of a learning algorithm is inspired by well-known numerical optimization methods. </p>
<p class="mce-root"/>
<p>One of the general limitations is that the perceptron can only produce solutions that are based on a line in two dimensions, or a linear hyperplane in multiple dimensions. However, this limitation can be easily solved by putting several perceptrons together and in multiple layers to produce highly complex non-linear solutions for separable and non-separable problems. This will be the topic of the next chapter.</p>
<h1 id="uuid-ac714f90-15bd-4fd8-80f6-1e6fe32aeb48">Summary </h1>
<p>This chapter presented an overview of the classic perceptron model. We covered the theoretical model and its implementation in Python for both linearly and non-linearly separable datasets. At this point, you should feel confident that you know enough about the perceptron that you can implement it yourself. You should be able to recognize the perceptron model in the context of a neuron. Also, you should now be able to implement a pocket algorithm and early termination strategies in a perceptron, or any other learning algorithm in general.</p>
<p>Since the perceptron is the most essential element that paved the way for deep neural networks, after we have covered it here, the next step is to go to <a href="a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml">Chapter 6</a>, <em>Training Multiple Layers of Neurons.</em><span> </span>In that chapter, you will be exposed to the challenges of deep learning using the multi-layer perceptron algorithm, such as gradient descent techniques for error minimization, and hyperparameter optimization to achieve generalization. But before you go there, please try to quiz yourself with the following questions. </p>
<h1 id="uuid-ebd70e9d-ce6b-4079-91d8-f944eb4fcf4f">Questions and answers</h1>
<ol>
<li><strong>What is the relationship between the separability of the data and the number of iterations of the PLA?</strong></li>
</ol>
<p style="padding-left: 60px">The number of iterations can grow exponentially as the data groups get close to one another.</p>
<ol start="2">
<li><strong>Will the PLA always converge? </strong></li>
</ol>
<p style="padding-left: 60px">Not always, only for linearly separable data.</p>
<ol start="3">
<li><strong>Can the PLA converge on non-linearly separable data?</strong></li>
</ol>
<p style="padding-left: 60px">No. However, you can find an acceptable solution by modifying it with the pocket algorithm, for example.</p>
<ol start="4">
<li><strong>Why is the perceptron important?</strong></li>
</ol>
<p style="padding-left: 60px">Because it is one of the most fundamental learning strategies that has helped conceive the possibility of learning. Without the perceptron, it could have taken longer for the scientific community to realize the potential of computer-based automatic learning algorithms.</p>
<h1 id="uuid-4db8c86d-3a1c-4d65-90dc-ff4b29e5519f">References</h1>
<ul>
<li><span>Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. </span><em>Psychological review</em><span>, </span><span>65</span><span>(6), 386.</span></li>
<li>Muselli, M. (1997). On convergence properties of the pocket algorithm. <em>IEEE Transactions on Neural Networks</em>, 8(3), 623-629.</li>
</ul>


            </article>

            
        </section>
    </body></html>