<html><head></head><body>
<div id="_idContainer179">
<h1 class="chapter-number" id="_idParaDest-276"><a id="_idTextAnchor286"/><span class="koboSpan" id="kobo.1.1">19</span></h1>
<h1 id="_idParaDest-277"><a id="_idTextAnchor287"/><span class="koboSpan" id="kobo.2.1">Architecting LLM Solutions</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Large language models</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">LLMs</span></strong><span class="koboSpan" id="kobo.6.1">) have revolutionized the field of </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">natural language processing</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">NLP</span></strong><span class="koboSpan" id="kobo.10.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.13.1">AI</span></strong><span class="koboSpan" id="kobo.14.1">), offering remarkable versatility in tackling a variety of tasks. </span><span class="koboSpan" id="kobo.14.2">However, realizing</span><a id="_idIndexMarker1344"/><span class="koboSpan" id="kobo.15.1"> their full potential requires addressing</span><a id="_idIndexMarker1345"/><span class="koboSpan" id="kobo.16.1"> certain challenges and developing</span><a id="_idIndexMarker1346"/><span class="koboSpan" id="kobo.17.1"> effective LLM solutions. </span><span class="koboSpan" id="kobo.17.2">In this chapter, we’ll demystify the process of architecting LLM solutions, focusing on essential aspects such as memory, problem-solving capabilities, autonomous agents, and advanced tools for enhanced performance. </span><span class="koboSpan" id="kobo.17.3">We will be focusing on retrieval-augmented language models, which provide contextually relevant information, their practical applications, and methods to improve them further. </span><span class="koboSpan" id="kobo.17.4">Additionally, we’ll uncover the challenges, best practices, and evaluation methods to ensure the success of an </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">LLM solution.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">Building upon these foundational concepts, this chapter will equip you with the knowledge and techniques necessary to create powerful LLM solutions tailored to your specific needs. </span><span class="koboSpan" id="kobo.19.2">By mastering the art of architecting LLM solutions, you will be better prepared to tackle complex challenges, optimize performance, and unlock the true potential of these versatile models in a wide range of </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">real-world applications.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">Overview of </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">LLM solutions</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Handling knowledge for </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">LLM solutions</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">LLM solutions</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Identifying challenges with </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">LLM solutions</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Tackling challenges with </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">LLM solutions</span></span></li>
<li><span class="koboSpan" id="kobo.33.1">Leveraging LLMs to build </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">autonomous agents</span></span></li>
<li><span class="koboSpan" id="kobo.35.1">Exploring LLM solution </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">use cases</span></span></li>
</ul>
<h1 id="_idParaDest-278"><a id="_idTextAnchor288"/><span class="koboSpan" id="kobo.37.1">Overview of LLM solutions</span></h1>
<p><span class="koboSpan" id="kobo.38.1">LLMs excel in diverse tasks such as answering</span><a id="_idIndexMarker1347"/><span class="koboSpan" id="kobo.39.1"> questions, machine translation, language modeling, sentiment analysis, and text summarization. </span><span class="koboSpan" id="kobo.39.2">They generate unstructured text but can be guided to produce structured output. </span><span class="koboSpan" id="kobo.39.3">LLM solutions harness this ability and leverage custom data from knowledge bases to create targeted, valuable outcomes for organizations and individuals. </span><span class="koboSpan" id="kobo.39.4">By properly streamlining processes and enhancing output quality objectively, an LLM solution can unlock the true potential of LLM-generated content, making it more powerful and practical </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">across applications.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">The increasing accessibility of LLMs with pre-trained world knowledge has played a significant role in making these benefits more attainable for a broader audience. </span><span class="koboSpan" id="kobo.41.2">Thanks to various LLM providers and open source platforms, organizations and developers can now more easily adopt and integrate LLMs into their workflows. </span><span class="koboSpan" id="kobo.41.3">Prominent LLM providers, such as OpenAI (GPT-4 or GPT-3.5), Microsoft Azure, Google, and Amazon Bedrock, offer pre-trained models and APIs that can be seamlessly integrated into diverse applications. </span><span class="koboSpan" id="kobo.41.4">Additionally, the Hugging Face platform has made LLMs even more accessible by offering an extensive collection of open source models. </span><span class="koboSpan" id="kobo.41.5">Hugging Face provides a wide selection of pre-trained models and fine-tuning techniques while fostering an active community that continually contributes to </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">enhancing LLMs.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">As organizations and individuals harness the power of LLMs for their typical tasks and use cases, it is crucial to determine how to leverage custom knowledge effectively. </span><span class="koboSpan" id="kobo.43.2">This consideration ensures that LLMs are optimally utilized to address specific needs; this will be explored further in the </span><em class="italic"><span class="koboSpan" id="kobo.44.1">Handling knowledge for LLM solutions</span></em><span class="koboSpan" id="kobo.45.1"> section. </span><span class="koboSpan" id="kobo.45.2">By taking advantage of the increased accessibility and versatility of LLMs, organizations and individuals can unlock the full potential of these powerful models to drive innovation and </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">improve outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Despite their impressive capabilities, LLMs face some limitations when it comes to solving more complex problems that they were not made to account for. </span><span class="koboSpan" id="kobo.47.2">Some of these limitations</span><a id="_idIndexMarker1348"/><span class="koboSpan" id="kobo.48.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.50.1">Inability to access up-to-date information on </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">recent events</span></span></li>
<li><span class="koboSpan" id="kobo.52.1">Tendency to hallucinate facts or generate </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">imitative falsehoods</span></span></li>
<li><span class="koboSpan" id="kobo.54.1">Difficulties in understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">low-resource languages</span></span></li>
<li><span class="koboSpan" id="kobo.56.1">Lack of mathematical skills for </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">precise calculations</span></span></li>
<li><span class="koboSpan" id="kobo.58.1">Unawareness of the progression</span><a id="_idIndexMarker1349"/> <span class="No-Break"><span class="koboSpan" id="kobo.59.1">of time</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.60.1">To overcome these limitations and enhance LLMs’ problem-solving capabilities, advanced solutions can be developed by incorporating the </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">following components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Real-time data integration</span></strong><span class="koboSpan" id="kobo.63.1">: By connecting LLMs to real-time data sources such as APIs, databases, or web</span><a id="_idIndexMarker1350"/><span class="koboSpan" id="kobo.64.1"> services, the model can access up-to-date information and provide more </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">accurate responses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Existing tool integration</span></strong><span class="koboSpan" id="kobo.67.1">: Incorporating existing tools and APIs into the LLM architecture</span><a id="_idIndexMarker1351"/><span class="koboSpan" id="kobo.68.1"> can extend its capabilities, allowing it to perform tasks that would otherwise be impossible or challenging for a </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">standalone model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.70.1">Multiple agents with different personas and contexts</span></strong><span class="koboSpan" id="kobo.71.1">: Developing a multi-agent system</span><a id="_idIndexMarker1352"/><span class="koboSpan" id="kobo.72.1"> where each agent possesses a unique persona and context can help address the challenges of diverse problem-solving scenarios. </span><span class="koboSpan" id="kobo.72.2">These agents can collaborate, share information, and provide more comprehensive and </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">reliable solutions.</span></span></li>
</ul>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.74.1">Figure 19</span></em></span><em class="italic"><span class="koboSpan" id="kobo.75.1">.1</span></em><span class="koboSpan" id="kobo.76.1"> shows an architecture that depicts the different approaches and methods that can be applied in an LLM solution</span><a id="_idIndexMarker1353"/><span class="koboSpan" id="kobo.77.1"> and will be introduced in </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">this chapter:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<span class="koboSpan" id="kobo.79.1"><img alt="Figure 19.1 – LLM solution architecture" src="image/B18187_19_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.80.1">Figure 19.1 – LLM solution architecture</span></p>
<p><span class="koboSpan" id="kobo.81.1">In the next few sections, we will dive into the individual components listed in this LLM solution architecture and LLM solutions in general more comprehensively. </span><span class="koboSpan" id="kobo.81.2">We will start with how knowledge is handled for </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">LLM solutions.</span></span></p>
<h1 id="_idParaDest-279"><a id="_idTextAnchor289"/><span class="koboSpan" id="kobo.83.1">Handling knowledge for LLM solutions</span></h1>
<p><span class="koboSpan" id="kobo.84.1">Domain knowledge is key in creating LLM solutions</span><a id="_idIndexMarker1354"/><span class="koboSpan" id="kobo.85.1"> as it provides the background information</span><a id="_idIndexMarker1355"/><span class="koboSpan" id="kobo.86.1"> and understanding they need to solve specific problems. </span><span class="koboSpan" id="kobo.86.2">This ultimately ensures the answers or actions the solutions come up with are on point and helpful. </span><span class="koboSpan" id="kobo.86.3">Domain knowledge needs to be included as context either as part of parametric memory, non-parametric </span><a id="_idIndexMarker1356"/><span class="koboSpan" id="kobo.87.1">memory, or a combination of both. </span><span class="koboSpan" id="kobo.87.2">Parametric memory refers to the parameters that are learned in an LLM. </span><span class="koboSpan" id="kobo.87.3">Non-parametric memory refers to an external library</span><a id="_idIndexMarker1357"/><span class="koboSpan" id="kobo.88.1"> of knowledge, such as a list</span><a id="_idIndexMarker1358"/><span class="koboSpan" id="kobo.89.1"> of documents, articles, or excerpts, that</span><a id="_idIndexMarker1359"/><span class="koboSpan" id="kobo.90.1"> can be selectively</span><a id="_idIndexMarker1360"/><span class="koboSpan" id="kobo.91.1"> chosen to be injected as part of the LLM context. </span><span class="koboSpan" id="kobo.91.2">This process is also referred to as an in-context learning method, knowledge retrieval, or </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">information retrieval.</span></span></p>
<p><span class="koboSpan" id="kobo.93.1">Non-parametric external knowledge can be provided to an LLM through either of the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">following options:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.95.1">As a latent conditioner in the cross-attention mechanism</span></strong><span class="koboSpan" id="kobo.96.1">: Latent conditioning involves </span><a id="_idIndexMarker1361"/><span class="koboSpan" id="kobo.97.1">generating latent feature vectors from the external knowledge and feeding it as part of the key and value vectors in the attention mechanism that were introduced in </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.98.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.99.1">, </span><em class="italic"><span class="koboSpan" id="kobo.100.1">Understanding Neural Network Transformers</span></em><span class="koboSpan" id="kobo.101.1">, while the original input is passed in as the query vector. </span><span class="koboSpan" id="kobo.101.2">This approach typically requires some form of fine-tuning the decoder part of the network in an encoder-decoder transformer architecture. </span><span class="koboSpan" id="kobo.101.3">Ideally, the fine-tuning process will build a decoder that can generalize to the domain of the intended external latent features</span><a id="_idIndexMarker1362"/><span class="koboSpan" id="kobo.102.1"> and can attend to a variety</span><a id="_idIndexMarker1363"/><span class="koboSpan" id="kobo.103.1"> of information. </span><span class="koboSpan" id="kobo.103.2">This approach allows the inclusion of any data modality as external knowledge. </span><span class="koboSpan" id="kobo.103.3">Notably, the </span><strong class="bold"><span class="koboSpan" id="kobo.104.1">Retrieval Augmented Generation</span></strong><span class="koboSpan" id="kobo.105.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.106.1">RAG</span></strong><span class="koboSpan" id="kobo.107.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.108.1">Retrieval-Enhanced Transformer</span></strong><span class="koboSpan" id="kobo.109.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.110.1">RETRO</span></strong><span class="koboSpan" id="kobo.111.1">) methods from published research papers [1][2] use </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">this approach.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">As part of an LLM’s input prompt</span></strong><span class="koboSpan" id="kobo.114.1">: This is a straightforward process that doesn’t require any fine-tuning but can still benefit from it. </span><span class="koboSpan" id="kobo.114.2">This approach brings the lowest barrier of entry to leverage any custom domain knowledge in LLMs. </span><span class="koboSpan" id="kobo.114.3">However, this approach only supports knowledge represented in data modalities that can be effectively represented</span><a id="_idIndexMarker1364"/><span class="koboSpan" id="kobo.115.1"> as textual data, such as text, numerical, categorical, and date data. </span><span class="koboSpan" id="kobo.115.2">Notably, the </span><strong class="bold"><span class="koboSpan" id="kobo.116.1">Retrieval-Augmented Language Model Pre-Training</span></strong><span class="koboSpan" id="kobo.117.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.118.1">REALM</span></strong><span class="koboSpan" id="kobo.119.1">) method, as part of a published research paper [3], uses this approach for pre-training specifically and doesn’t use it as part of the final </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">trained</span></span><span class="No-Break"><a id="_idIndexMarker1365"/></span><span class="No-Break"><span class="koboSpan" id="kobo.121.1"> model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.122.1">Both methods require a knowledge</span><a id="_idIndexMarker1366"/><span class="koboSpan" id="kobo.123.1"> base to be established, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.124.1">Figure 19</span></em></span><em class="italic"><span class="koboSpan" id="kobo.125.1">.2</span></em><span class="koboSpan" id="kobo.126.1">, and a knowledge retrieval component that retrieves</span><a id="_idIndexMarker1367"/><span class="koboSpan" id="kobo.127.1"> information</span><a id="_idIndexMarker1368"/><span class="koboSpan" id="kobo.128.1"> from the knowledge base, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.129.1">Figure 19</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.130.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<span class="koboSpan" id="kobo.132.1"><img alt="Figure 19.2 – Establishing a knowledge base" src="image/B18187_19_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.133.1">Figure 19.2 – Establishing a knowledge base</span></p>
<p><span class="koboSpan" id="kobo.134.1">A short tabular summary</span><a id="_idIndexMarker1369"/><span class="koboSpan" id="kobo.135.1"> of the REALM, RETRO, and RAG methods is presented in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.136.1">Table 19.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.138.1">Knowledge </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.139.1">retrieval methods</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.140.1">Retriever training</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.141.1">Retrieval integration</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.142.1">RAG</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.143.1">Fine-tune with a frozen </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">base network</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.145.1">Latent conditioner </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">with cross-attention</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.147.1">REALM</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.148.1">Full </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">end-to-end training</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.150.1">Prepend to prompt specifically without </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">a template</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.152.1">RETRO</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.153.1">Fine-tune with a frozen </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">base network</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.155.1">Latent conditioner </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">with cross-attention</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.157.1">Table 19.1 – Short overview of retrieval integration with LLM methods</span></p>
<p><span class="koboSpan" id="kobo.158.1">A knowledge base requires that text data is pre-processed into appropriate logical chunks or segments. </span><span class="koboSpan" id="kobo.158.2">These segments are then transformed into embedding vectors using trained transformer models. </span><span class="koboSpan" id="kobo.158.3">Finally, a nearest neighbor index is constructed, enabling efficient retrieval of relevant information from the knowledge base. </span><span class="koboSpan" id="kobo.158.4">The nearest neighbor index can either be a simple KNN algorithm that computes raw distances between the prompt embedding vector, or an approximate KNN algorithm that approximates the distance computations. </span><span class="koboSpan" id="kobo.158.5">Both the index and the logical text chunks will then serve as the knowledge base, which can be used for retrieval. </span><span class="koboSpan" id="kobo.158.6">The method to perform retrieval can vary with different strategies, but the simplest form involves simply generating embedding from the prompt and returning the top </span><em class="italic"><span class="koboSpan" id="kobo.159.1">k</span></em><span class="koboSpan" id="kobo.160.1"> closest text chunks from the knowledge base using the index. </span><span class="koboSpan" id="kobo.160.2">These top </span><em class="italic"><span class="koboSpan" id="kobo.161.1">k</span></em><span class="koboSpan" id="kobo.162.1"> closest text chunks can then be included as part of the LLM prompt or as a </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">latent conditioner.</span></span></p>
<p><span class="koboSpan" id="kobo.164.1">For the approach of including the most relevant text chunks as part of the prompt, crafting a prompt template that can allow a specific spot to be inserted is the standard and can help organize information in a prompt properly. </span><span class="koboSpan" id="kobo.164.2">This can be as simple as using leading text such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">Context:</span></strong><span class="koboSpan" id="kobo.166.1">, following up with the retrieved relevant text chunks, and having new line separation before and after the context part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">prompt template.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">While research papers often present published methods that encompass various aspects of the retrieval process in a single method, it is helpful to consider each component of building and using the knowledge</span><a id="_idIndexMarker1370"/><span class="koboSpan" id="kobo.169.1"> base as separate, interchangeable</span><a id="_idIndexMarker1371"/><span class="koboSpan" id="kobo.170.1"> parts. </span><span class="koboSpan" id="kobo.170.2">This allows for greater flexibility in selecting the most suitable components for specific situations. </span><span class="koboSpan" id="kobo.170.3">Moreover, although there is a published method known as RAG, it is worth noting</span><a id="_idIndexMarker1372"/><span class="koboSpan" id="kobo.171.1"> that, in practice, the term RAG is commonly used to describe the general approach of integrating knowledge retrieval with LLMs, rather than referring solely to that specific method. </span><span class="koboSpan" id="kobo.171.2">Let’s briefly go through the three key method-based components</span><a id="_idIndexMarker1373"/><span class="koboSpan" id="kobo.172.1"> that can be freely modified</span><a id="_idIndexMarker1374"/><span class="koboSpan" id="kobo.173.1"> according to the use case. </span><span class="koboSpan" id="kobo.173.2">We will also choose orchestrator tools that help streamline the implementation of </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">these components.</span></span></p>
<h2 id="_idParaDest-280"><a id="_idTextAnchor290"/><span class="koboSpan" id="kobo.175.1">Exploring chunking methods</span></h2>
<p><span class="koboSpan" id="kobo.176.1">The text chunking process affects the efficiency of LLM context utilization and the quality of the resulting LLM generation. </span><span class="koboSpan" id="kobo.176.2">Choosing an appropriate chunking method depends on the </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">following factors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.178.1">The embedding model used for embedding vector generation</span></strong><span class="koboSpan" id="kobo.179.1">: Different pre-trained</span><a id="_idIndexMarker1375"/><span class="koboSpan" id="kobo.180.1"> embedding models may have different requirements or limitations when it comes to text chunking. </span><span class="koboSpan" id="kobo.180.2">Two such requirements are the supported context size and the typical text context size that was used </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">during pre-training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.182.1">The granularity of information needed for the expected prompts that will be made</span></strong><span class="koboSpan" id="kobo.183.1">: The level of detail or granularity required for the prompts can impact the choice of text chunking method. </span><span class="koboSpan" id="kobo.183.2">Depending on the specific use case, the method should be able to chunk the text into appropriate and concise segments that provide the necessary information for the LLM to generate accurate, concise, and </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">relevant responses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.185.1">The nature of the text data used to build a knowledge base</span></strong><span class="koboSpan" id="kobo.186.1">: The characteristics of the text data itself can also influence the choice of text chunking method. </span><span class="koboSpan" id="kobo.186.2">For example, if the text data consists of long paragraphs or documents, a method that breaks the text into smaller chunks or sections may be more suitable. </span><span class="koboSpan" id="kobo.186.3">On the other hand, if the text data is already organized into logical segments, a method that preserves these segments may be preferred. </span><span class="koboSpan" id="kobo.186.4">Also, if the text data is Python code, it can be suitable to chunk the text by </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">code methods.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.188.1">There are several methods</span><a id="_idIndexMarker1376"/><span class="koboSpan" id="kobo.189.1"> for chunking text, including sentence, paragraph, entity, topic, and section chunking. </span><span class="koboSpan" id="kobo.189.2">These methods help organize the text into meaningful units that can be processed by the LLM. </span><span class="koboSpan" id="kobo.189.3">One notable method that is useful and readily available is the recursive chunking method from the LangChain library. </span><span class="koboSpan" id="kobo.189.4">This method allows you to adjust the granularity of the chunks by recursively splitting the text using an ordered list of text separators, a maximum chunk size, and the percent of overlap between chunks. </span><span class="koboSpan" id="kobo.189.5">The maximum chunk size should be tailored to the context size supported by the embedding model, ensuring that the generated chunks can be effectively processed. </span><span class="koboSpan" id="kobo.189.6">Meanwhile, incorporating an overlap percentage helps minimize the risk of missing critical information that could be located at the boundaries </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">of chunks.</span></span></p>
<p><span class="koboSpan" id="kobo.191.1">Many document-specific chunking methods are created based on this recursive chunking method by specifying the appropriate ordered list of text separators. </span><span class="koboSpan" id="kobo.191.2">Specifically, as of </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">langchain==0.0.314</span></strong><span class="koboSpan" id="kobo.193.1">, recursive methods have been created for Python code with </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">PythonCodeTextSplitter</span></strong><span class="koboSpan" id="kobo.195.1">, markdown documents with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">MarkdownTextSplitter</span></strong><span class="koboSpan" id="kobo.197.1"> class, and LaTeX-formatted</span><a id="_idIndexMarker1377"/><span class="koboSpan" id="kobo.198.1"> text with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">LatexTextSplitter</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.200.1"> class.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">Next, let’s dive into embedding </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">model choices.</span></span></p>
<h2 id="_idParaDest-281"><a id="_idTextAnchor291"/><span class="koboSpan" id="kobo.203.1">Exploring embedding models</span></h2>
<p><span class="koboSpan" id="kobo.204.1">Embedding models play</span><a id="_idIndexMarker1378"/><span class="koboSpan" id="kobo.205.1"> a crucial role in generating a knowledge base for LLM solutions. </span><span class="koboSpan" id="kobo.205.2">These models are responsible for encoding the semantic information of text into vector representations, which are then used to retrieve relevant information from the </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">knowledge base.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">One benchmark that provides insights</span><a id="_idIndexMarker1379"/><span class="koboSpan" id="kobo.208.1"> into the performance of text embedding models is the </span><strong class="bold"><span class="koboSpan" id="kobo.209.1">Massive Text Embedding Benchmark</span></strong><span class="koboSpan" id="kobo.210.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">MTEB</span></strong><span class="koboSpan" id="kobo.212.1">). </span><span class="koboSpan" id="kobo.212.2">MTEB evaluates the effectiveness of various embedding models on diverse embedding tasks. </span><span class="koboSpan" id="kobo.212.3">By assessing the leaderboard results, users can identify the top-performing models for their specific use cases. </span><span class="koboSpan" id="kobo.212.4">The MTEB leaderboard is available at </span><a href="https://huggingface.co/spaces/mteb/leaderboard"><span class="koboSpan" id="kobo.213.1">https://huggingface.co/spaces/mteb/leaderboard</span></a><span class="koboSpan" id="kobo.214.1">. </span><span class="koboSpan" id="kobo.214.2">MTEB even includes the embedding models of paid providers such as the embedding model from OpenAI </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">text-embedding-ada-002</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">When selecting an embedding model for knowledge base generation, it is essential to consider factors such as model size, embedding dimensions, and sequence length. </span><span class="koboSpan" id="kobo.218.2">Traditional embedding models such as GloVe offer high speed but may lack context awareness, resulting in lower average scores. </span><span class="koboSpan" id="kobo.218.3">On the other hand, models such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">all-mpnet-base-v2</span></strong><span class="koboSpan" id="kobo.220.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">all-MiniLM-L6-v2</span></strong><span class="koboSpan" id="kobo.222.1"> strike a balance between speed and performance, providing satisfactory results. </span><span class="koboSpan" id="kobo.222.2">For maximum performance, larger models such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">bge-large-en-v1.5</span></strong><span class="koboSpan" id="kobo.224.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">ember-v1</span></strong><span class="koboSpan" id="kobo.226.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">e5-large-v2</span></strong><span class="koboSpan" id="kobo.228.1"> dominate the MTEB leaderboard, all with a 1.34 GB </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">model size.</span></span></p>
<p><span class="koboSpan" id="kobo.230.1">It’s important to note that the choice of embedding model depends on the specific task and dataset being used. </span><span class="koboSpan" id="kobo.230.2">Therefore, thoroughly exploring the various tabs of the MTEB leaderboard and considering the requirements of the knowledge base generation process can help in selecting the most suitable </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">embedding model.</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">MTEB, with its extensive collection</span><a id="_idIndexMarker1380"/><span class="koboSpan" id="kobo.233.1"> of datasets and evaluation metrics, serves as a valuable resource for researchers and practitioners in the field of NLP. </span><span class="koboSpan" id="kobo.233.2">By leveraging the insights provided by MTEB, developers can make informed decisions when choosing an embedding model for knowledge base generation in </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">LLM solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.235.1">Before we delve into exploring the knowledge base index types, it’s essential to remember that the choice of chunking method and embedding model shapes the construction of your knowledge base. </span><span class="koboSpan" id="kobo.235.2">Both components play a crucial part in how effectively the LLM can retrieve and utilize knowledge. </span><span class="koboSpan" id="kobo.235.3">Now, let’s dive deeper into the world of knowledge base index types</span><a id="_idIndexMarker1381"/><span class="koboSpan" id="kobo.236.1"> and learn how they contribute to the efficiency of </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">LLM solutions.</span></span></p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor292"/><span class="koboSpan" id="kobo.238.1">Exploring the knowledge base index types</span></h2>
<p><span class="koboSpan" id="kobo.239.1">The knowledge base index</span><a id="_idIndexMarker1382"/><span class="koboSpan" id="kobo.240.1"> is the backbone of retrieval mechanisms in LLM solutions. </span><span class="koboSpan" id="kobo.240.2">It is the component that facilitates the efficient lookup of relevant information. </span><span class="koboSpan" id="kobo.240.3">While there are several ways of implementing this index, they all aim to provide a fast and efficient way of retrieving the most relevant text chunks from the knowledge base based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">input prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">Many options are available for building a knowledge base index. </span><span class="koboSpan" id="kobo.242.2">They range from manual code implementations to using various vector database libraries, service providers, and plugins. </span><span class="koboSpan" id="kobo.242.3">Some of these options are </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">listed here:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.244.1">Manual code</span></strong><span class="koboSpan" id="kobo.245.1">: You can manually implement an in-memory vector database using libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">faiss</span></strong><span class="koboSpan" id="kobo.247.1">, a library for efficient similarity search of dense vectors, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">scipy</span></strong><span class="koboSpan" id="kobo.249.1">, a library for pairwise distance computations. </span><span class="koboSpan" id="kobo.249.2">This allows for customization but may require more effort and expertise while requiring bigger </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">RAM allocations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.251.1">Service providers</span></strong><span class="koboSpan" id="kobo.252.1">: Various cloud providers offer vector database services. </span><span class="koboSpan" id="kobo.252.2">These include Pinecone, Chroma, Vespa, and Weaviate. </span><span class="koboSpan" id="kobo.252.3">These services handle the complexities of managing a vector database, providing scalable and robust solutions that can be easily integrated into your LLM architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">and solution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.254.1">Database tools with vector computation support</span></strong><span class="koboSpan" id="kobo.255.1">: Traditional database tools such as MongoDB, Neo4j, Redis, and PostgreSQL provide vector computation support through plugins. </span><span class="koboSpan" id="kobo.255.2">This can be a good option if you’re already using these tools in your tech stack and want to leverage their capabilities for your </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">knowledge base.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.257.1">Plugins</span></strong><span class="koboSpan" id="kobo.258.1">: There are also plugins available, directly from LLM service providers, such as ChatGPT, that can help with the construction and maintenance of a </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">knowledge base.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.260.1">Choosing the right knowledge base index type depends on your specific requirements, such as the size of your knowledge base, the complexity of your retrieval needs, and the resources you have available. </span><span class="koboSpan" id="kobo.260.2">Consider factors such as scalability, ease of integration, cost, and the level of control you need over your knowledge base when making your choice. </span><span class="koboSpan" id="kobo.260.3">A recommendation is to only consider vector databases that are using the actual database technology or claim to do so when your knowledge base is big enough to matter. </span><span class="koboSpan" id="kobo.260.4">If your knowledge base</span><a id="_idIndexMarker1383"/><span class="koboSpan" id="kobo.261.1"> is small, let’s say in the six-digit range, raw distance computations take less than 1 second if you make one prompt per compute in Python! </span><span class="koboSpan" id="kobo.261.2">Next, we will briefly discover orchestrator libraries for </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">LLM solutions.</span></span></p>
<h2 id="_idParaDest-283"><a id="_idTextAnchor293"/><span class="koboSpan" id="kobo.263.1">Exploring orchestrator tools for LLM solutions</span></h2>
<p><span class="koboSpan" id="kobo.264.1">The process of architecting LLM solutions</span><a id="_idIndexMarker1384"/><span class="koboSpan" id="kobo.265.1"> can be streamlined with the use</span><a id="_idIndexMarker1385"/><span class="koboSpan" id="kobo.266.1"> of specific tools. </span><span class="koboSpan" id="kobo.266.2">Open sourced orchestrator libraries such as LangChain and LlamaIndex play a pivotal role in this context. </span><span class="koboSpan" id="kobo.266.3">Both tools simplify tasks such as setting up the knowledge base, integrating an LLM, and managing retrieval mechanisms. </span><span class="koboSpan" id="kobo.266.4">In general, an orchestrator significantly reduces the complexity and development time of </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">LLM solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">In addition to open sourced orchestrator tools, there are also paid options available that provide advanced features and support. </span><span class="koboSpan" id="kobo.268.2">Some of these DataRobot, Microsoft Azure, IBM Watson, LangSmith, OpenAI, and Google Vertex AI. </span><span class="koboSpan" id="kobo.268.3">These platforms offer a wide range of pre-built models, integrations, and tools that streamline the entire pipeline, from data ingestion to model deployment </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">and monitoring.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">As you continue to explore</span><a id="_idIndexMarker1386"/><span class="koboSpan" id="kobo.271.1"> these tools and methods, it’s crucial to establish</span><a id="_idIndexMarker1387"/><span class="koboSpan" id="kobo.272.1"> robust evaluation methods to measure the impact of these components on your solution, ensuring it meets its intended objectives. </span><span class="koboSpan" id="kobo.272.2">We’ll delve deeper into these methods in the </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">upcoming section.</span></span></p>
<h1 id="_idParaDest-284"><a id="_idTextAnchor294"/><span class="koboSpan" id="kobo.274.1">Evaluating LLM solutions</span></h1>
<p><span class="koboSpan" id="kobo.275.1">Evaluating LLM solutions is a crucial step</span><a id="_idIndexMarker1388"/><span class="koboSpan" id="kobo.276.1"> in harnessing their full potential and ensuring their effectiveness in various applications. </span><span class="koboSpan" id="kobo.276.2">By implementing a comprehensive set of evaluation approaches, organizations can better assess the performance, accuracy, and overall quality of the results from an LLM solution, while also considering the associated costs, adherence to safety standards, and potential negative impact on users. </span><span class="koboSpan" id="kobo.276.3">In other words, doing this provides you with valuable insights to help make any informed decisions. </span><span class="koboSpan" id="kobo.276.4">To achieve a comprehensive evaluation, we can view evaluation methods as part of either a quantitative measure or a qualitative measure. </span><span class="koboSpan" id="kobo.276.5">Let’s dive into evaluation methods by </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">these groups.</span></span></p>
<h2 id="_idParaDest-285"><a id="_idTextAnchor295"/><span class="koboSpan" id="kobo.278.1">Evaluating LLM solutions through quantitative metrics</span></h2>
<p><span class="koboSpan" id="kobo.279.1">Quantitative metrics can be aggregated</span><a id="_idIndexMarker1389"/><span class="koboSpan" id="kobo.280.1"> throughout a provided evaluation</span><a id="_idIndexMarker1390"/><span class="koboSpan" id="kobo.281.1"> dataset and can provide a more quick, comprehensive, and objective measure to compare multiple LLM solution setups. </span><span class="koboSpan" id="kobo.281.2">Here are some examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">quantitative metrics:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.283.1">Comprehension and fluency-based metrics</span></strong><span class="koboSpan" id="kobo.284.1">: Flesch Reading Ease, Coleman</span><a id="_idIndexMarker1391"/><span class="koboSpan" id="kobo.285.1"> Liau Index, and </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">SMOG readability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.287.1">Facts-based metrics</span></strong><span class="koboSpan" id="kobo.288.1">: Any metrics that use the facts provided by a knowledge base </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">for inference:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.290.1">Factual consistency</span></strong><span class="koboSpan" id="kobo.291.1">: This refers to comparing generated text with facts stated in the knowledge</span><a id="_idIndexMarker1392"/><span class="koboSpan" id="kobo.292.1"> base. </span><span class="koboSpan" id="kobo.292.2">It is important to note that relevant facts might not always be available</span><a id="_idIndexMarker1393"/><span class="koboSpan" id="kobo.293.1"> in the knowledge base. </span><span class="koboSpan" id="kobo.293.2">This metric is also known as the extractiveness metric. </span><span class="koboSpan" id="kobo.293.3">To measure factual consistency, you can use either semantic similarity, which focuses on differences in the meaning of the text, or lexical similarity, which emphasizes matching words in </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">the text.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.295.1">Factual relevance</span></strong><span class="koboSpan" id="kobo.296.1">: This is about how relevant the provided facts are, without considering the LLM generation. </span><span class="koboSpan" id="kobo.296.2">This is possible when you have ranked relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">document labels.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.298.1">Generated text relevance/accuracy metric</span></strong><span class="koboSpan" id="kobo.299.1">: This metric evaluates the relevance and accuracy</span><a id="_idIndexMarker1394"/><span class="koboSpan" id="kobo.300.1"> of the text generated by an LLM in comparison to an ideal ground truth. </span><span class="koboSpan" id="kobo.300.2">It can be computed using similarity metrics or self-evaluation techniques. </span><span class="koboSpan" id="kobo.300.3">Self-evaluation can be further broken down into the </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">following areas:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.302.1">With access to token probabilities</span></strong><span class="koboSpan" id="kobo.303.1">: The average of log probabilities is used to assess the quality of the generated text. </span><span class="koboSpan" id="kobo.303.2">Higher log probabilities indicate that the model is more confident in its output, suggesting greater relevance </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">and accuracy.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.305.1">Without access to token probabilities</span></strong><span class="koboSpan" id="kobo.306.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.307.1">SelfCheckGPT</span></strong><span class="koboSpan" id="kobo.308.1"> is a method that can be employed to evaluate</span><a id="_idIndexMarker1395"/><span class="koboSpan" id="kobo.309.1"> the generated text without relying on token probabilities. </span><span class="koboSpan" id="kobo.309.2">This approach leverages the LLM’s capabilities to assess the quality of its generated content, providing an alternative measure of relevance </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">and accuracy.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.311.1">Runtime metrics</span></strong><span class="koboSpan" id="kobo.312.1">: The time taken to generate text, the number of tokens processed, and </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.314.1">Cost metrics</span></strong><span class="koboSpan" id="kobo.315.1">: The number of tokens generated, API call costs, hosting costs, and </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.317.1">Guardrail violations metrics</span></strong><span class="koboSpan" id="kobo.318.1">: The percentage of outputs that violate predefined standards. </span><span class="koboSpan" id="kobo.318.2">Examples of guardrails are toxicity levels and hate </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">speech degree.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.320.1">Adversarial performance metrics</span></strong><span class="koboSpan" id="kobo.321.1">: The performance measures in handling adversarial inputs. </span><span class="koboSpan" id="kobo.321.2">These were introduced more comprehensively in </span><a href="B18187_14.xhtml#_idTextAnchor206"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.322.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.323.1">, </span><em class="italic"><span class="koboSpan" id="kobo.324.1">Analyzing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.325.1">Adversarial Performance</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.327.1">Bias and fairness metrics</span></strong><span class="koboSpan" id="kobo.328.1">: Quantitative measures for assessing biases in the generated text. </span><span class="koboSpan" id="kobo.328.2">These were introduced more comprehensively in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.329.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.330.1">, </span><em class="italic"><span class="koboSpan" id="kobo.331.1">Exploring Bias </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.332.1">and Fairness</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.334.1">Any supervised classification or regression metrics</span></strong><span class="koboSpan" id="kobo.335.1">: This can be applied to the results</span><a id="_idIndexMarker1396"/><span class="koboSpan" id="kobo.336.1"> or resulting actions from an </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">LLM solution.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.338.1">Note that quantitative methods</span><a id="_idIndexMarker1397"/><span class="koboSpan" id="kobo.339.1"> such as generated text relevance</span><a id="_idIndexMarker1398"/><span class="koboSpan" id="kobo.340.1"> and factual consistency metrics, which use similarity metrics to compare two sets of text, are not as reliable as supervised model metrics such as accuracy. </span><span class="koboSpan" id="kobo.340.2">These metrics should be taken with a grain of salt. </span><span class="koboSpan" id="kobo.340.3">Additionally, a nice bonus with quantitative metrics is that they can be used for monitoring a deployed model programmatically. </span><span class="koboSpan" id="kobo.340.4">Next, we will dive into qualitative </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">manual evaluations.</span></span></p>
<h2 id="_idParaDest-286"><a id="_idTextAnchor296"/><span class="koboSpan" id="kobo.342.1">Evaluating LLM solutions through qualitative evaluation methods</span></h2>
<p><span class="koboSpan" id="kobo.343.1">Qualitative methods, which involve human feedback</span><a id="_idIndexMarker1399"/><span class="koboSpan" id="kobo.344.1"> and manual assessments, complement quantitative measures</span><a id="_idIndexMarker1400"/><span class="koboSpan" id="kobo.345.1"> and provide a comprehensive understanding of LLM performance. </span><span class="koboSpan" id="kobo.345.2">It can also sometimes be the only way for evaluation when there are no reference ground truth datasets. </span><span class="koboSpan" id="kobo.345.3">Here are some examples of qualitative LLM solution </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">evaluation methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Human feedback scores</span></strong><span class="koboSpan" id="kobo.348.1">: These are the users’ ratings or rankings of generated responses</span><a id="_idIndexMarker1401"/><span class="koboSpan" id="kobo.349.1"> to gauge effectiveness and relevance. </span><span class="koboSpan" id="kobo.349.2">Examples include grammar and coherence </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">of text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.351.1">Generated text relevance evaluation</span></strong><span class="koboSpan" id="kobo.352.1">: This involves manually assessing the generated text’s relevance to the given context </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">or prompt.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.354.1">Prediction explanations</span></strong><span class="koboSpan" id="kobo.355.1">: These assess the reasoning behind the generated text or predictions, which can help identify potential biases or faulty logic in the </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">LLM solution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Ethical and legal compliance</span></strong><span class="koboSpan" id="kobo.358.1">: This ensures that the generated text adheres to ethical and legal guidelines through </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">manual review.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.360.1">By employing a combination of quantitative metrics and qualitative manual evaluations, organizations can gain a deeper understanding of LLM performance and identify potential areas for improvement. </span><span class="koboSpan" id="kobo.360.2">In general, try to treat LLM solutions as no different from any supervised machine learning projects and evaluate them vigorously, similarly to how you would in a supervised machine learning project. </span><span class="koboSpan" id="kobo.360.3">This holistic approach to evaluating LLM solutions</span><a id="_idIndexMarker1402"/><span class="koboSpan" id="kobo.361.1"> not only ensures consistent performance</span><a id="_idIndexMarker1403"/><span class="koboSpan" id="kobo.362.1"> and compliance but also helps in aligning these powerful models with specific needs and objectives, driving innovation and improving outcomes in </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">various applications.</span></span></p>
<h1 id="_idParaDest-287"><a id="_idTextAnchor297"/><span class="koboSpan" id="kobo.364.1">Identifying challenges with LLM solutions</span></h1>
<p><span class="koboSpan" id="kobo.365.1">Despite their impressive</span><a id="_idIndexMarker1404"/><span class="koboSpan" id="kobo.366.1"> capabilities, LLMs face challenges when solving complex real-world problems. </span><span class="koboSpan" id="kobo.366.2">In this section, we will explore some of the challenges faced by LLM solutions and discuss possible ways to tackle them. </span><span class="koboSpan" id="kobo.366.3">We will explore challenges by high-level groups, </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.368.1">Output and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.369.1">input limitations</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.371.1">LLMs just produce text</span></strong><span class="koboSpan" id="kobo.372.1">: Text output can help provide value for a lot of businesses. </span><span class="koboSpan" id="kobo.372.2">However, many</span><a id="_idIndexMarker1405"/><span class="koboSpan" id="kobo.373.1"> other use cases require predictions and recommendations in entirely </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">different formats.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.375.1">The context size of an LLM is limited</span></strong><span class="koboSpan" id="kobo.376.1">: The issue is that with a large input size, you need exponentially more compute resources to train and predict. </span><span class="koboSpan" id="kobo.376.2">So, context size usually stays in a token range of one to three thousand. </span><span class="koboSpan" id="kobo.376.3">This issue should be prevalent only for use cases that require long context, as a few thousand context sizes should be enough for most </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">use cases.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.378.1">An LLM is a text-specific model</span></strong><span class="koboSpan" id="kobo.379.1">: Other data modalities are not supported </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">by default.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.381.1">Repetitive retrieved information</span></strong><span class="koboSpan" id="kobo.382.1">: The information that’s retrieved from a knowledge base can be highly relevant but repetitive and numerous. </span><span class="koboSpan" id="kobo.382.2">As the context size of an LLM is limited, a risk arises when multiple pieces of information are placed</span><a id="_idIndexMarker1406"/><span class="koboSpan" id="kobo.383.1"> as the context is repetitive and takes up most of the context </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">limit quota.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Knowledge and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.386.1">information-related challenges</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.388.1">Inability to access up-to-date information</span></strong><span class="koboSpan" id="kobo.389.1">: LLMs may not know about recent events</span><a id="_idIndexMarker1407"/><span class="koboSpan" id="kobo.390.1"> or developments, leading to outdated or inaccurate information being provided in </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">their responses.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.392.1">Handling low-resource languages</span></strong><span class="koboSpan" id="kobo.393.1">: LLMs can struggle with understanding and processing languages with limited data </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">or resources.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.395.1">Unawareness of the progression of time</span></strong><span class="koboSpan" id="kobo.396.1">: LLMs may not understand the concept of time, leading to confusion when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">time-sensitive information.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.398.1">Information loss</span></strong><span class="koboSpan" id="kobo.399.1">: LLMs are shown to look at the beginning and end of sentences, but not so much the middle, and thus lose the most information placed in </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">the middle.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.401.1">Single index failures</span></strong><span class="koboSpan" id="kobo.402.1">: This challenge arises when an LLM lacks sufficient knowledge about a specific topic or area due to limitations in its training data. </span><span class="koboSpan" id="kobo.402.2">For instance, if you ask an LLM about a newly opened local restaurant that wasn’t covered in its training data, the LLM may provide limited or </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">irrelevant information.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.404.1">Incomplete content retrieval from documents:</span></strong><span class="koboSpan" id="kobo.405.1"> When retrieval of a chunked sentence gets the right document, but the actual content needed is below the retrieved chunk in the same document, LLMs may not provide the complete or accurate information required by the user. </span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.406.1">     Example: In a documentation search for a software's installation process, the LLM retrieves a section mentioning the installation, but the actual step-by-step instructions are located in the following section of the document. </span><span class="koboSpan" id="kobo.406.2">Consequently, the user only receives an overview without the necessary details for proper installation. </span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.407.1">The use irrelevant information in the context:</span></strong><span class="koboSpan" id="kobo.408.1"> LLMs may use irrelevant information from their context as a basis for their output, essentially mimicking or echoing opinions found in the context even if they are not applicable or appropriate for the given situation. </span><span class="koboSpan" id="kobo.408.2">This phenomenon, referred to as sycophancy, can lead to misleading or </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">unhelpful responses.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.410.1">The global knowledge base summarization task can’t be executed accurately</span></strong><span class="koboSpan" id="kobo.411.1">: A retrieval process is unaware of the type of knowledge base it requests</span><a id="_idIndexMarker1408"/><span class="koboSpan" id="kobo.412.1"> and thus can’t execute the global summarization </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">task effectively.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.414.1">Accuracy </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.415.1">and reliability</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.417.1">Hallucinations</span></strong><span class="koboSpan" id="kobo.418.1">: LLMs can generate false or misleading information</span><a id="_idIndexMarker1409"/><span class="koboSpan" id="kobo.419.1"> that may appear</span><a id="_idIndexMarker1410"/><span class="koboSpan" id="kobo.420.1"> plausible but is not based on facts. </span><span class="koboSpan" id="kobo.420.2">This phenomenon is known </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">as hallucination.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.422.1">Lack of mathematical skills</span></strong><span class="koboSpan" id="kobo.423.1">: LLMs often cannot perform precise calculations or solve complex mathematical problems. </span><span class="koboSpan" id="kobo.423.2">This issue is more widely known as it is slightly controversial, depending on how you look </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">at it.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.425.1">Imitative falsehoods</span></strong><span class="koboSpan" id="kobo.426.1">: These are false statements that LLMs generate because they mimic common misconceptions found in the training data. </span><span class="koboSpan" id="kobo.426.2">Since the model learns from the data it’s trained on, it might inadvertently reproduce widely held but incorrect beliefs. </span><span class="koboSpan" id="kobo.426.3">For example, if many people believe that a specific food causes a particular illness, an LLM might generate a similar statement, even if it’s not </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">scientifically accurate.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.428.1">Non-imitative falsehoods</span></strong><span class="koboSpan" id="kobo.429.1">: These are false statements that arise due to the model’s inability to fully achieve its training objective. </span><span class="koboSpan" id="kobo.429.2">This includes hallucinations, which are statements that seem plausible but are incorrect. </span><span class="koboSpan" id="kobo.429.3">For instance, an LLM might generate a statement about a historical event that never occurred, but the statement may sound convincing to someone who is not knowledgeable about that </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">specific event.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.431.1">Runtime performance issues</span></strong><span class="koboSpan" id="kobo.432.1">: An LLM’s runtime can be slow. </span><span class="koboSpan" id="kobo.432.2">Additionally, by adding a knowledge</span><a id="_idIndexMarker1411"/><span class="koboSpan" id="kobo.433.1"> base to it, the entire process can become slower than it </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">already is.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.435.1">Ethical implications and societal impacts</span></strong><span class="koboSpan" id="kobo.436.1">: The widespread adoption and deployment of LLMs</span><a id="_idIndexMarker1412"/><span class="koboSpan" id="kobo.437.1"> comes with several ethical implications and societal impacts. </span><span class="koboSpan" id="kobo.437.2">As these models learn from vast amounts of data, they may inadvertently inherit biases present in the training data, leading to biased outputs, perpetuating stereotypes, or promoting misinformation. </span><span class="koboSpan" id="kobo.437.3">Furthermore, LLMs can generate content that may inadvertently promote harmful behavior, hate speech, or violate privacy concerns. </span><span class="koboSpan" id="kobo.437.4">The following ethical challenges are involved in the usage of an </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">LLM solution:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.439.1">Bias and fairness</span></strong><span class="koboSpan" id="kobo.440.1">: Ensuring that the LLM does not exhibit biased behavior or discriminate against specific user groups based on their race, gender, age, or other protected attributes. </span><span class="koboSpan" id="kobo.440.2">Consider the case that a bank uses an LLM to analyze loan applications and determine </span><a id="_idTextAnchor298"/><span class="koboSpan" id="kobo.441.1">creditworthiness. </span><span class="koboSpan" id="kobo.441.2">The LLM has been trained on historical data, which may contain biases against certain ethnic groups. </span><span class="koboSpan" id="kobo.441.3">As a result, the LLM might reject loan applications from these groups at a higher rate, even when the applicants have good </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">credit scores.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.443.1">Privacy concerns</span></strong><span class="koboSpan" id="kobo.444.1">: LLMs may inadvertently generate </span><strong class="bold"><span class="koboSpan" id="kobo.445.1">personally identifiable information</span></strong><span class="koboSpan" id="kobo.446.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.447.1">PII</span></strong><span class="koboSpan" id="kobo.448.1">) or sensitive data in their outputs, which raises privacy</span><a id="_idIndexMarker1413"/><span class="koboSpan" id="kobo.449.1"> concerns and potential legal issues. </span><span class="koboSpan" id="kobo.449.2">Consider the case where a healthcare organization uses an LLM to generate personalized health recommendations for its clients. </span><span class="koboSpan" id="kobo.449.3">The LLM can inadvertently include specific patient names and medical conditions in the generated advice, which then gets shared publicly, violating </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">patient privacy.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.451.1">Misinformation and disinformation</span></strong><span class="koboSpan" id="kobo.452.1">: LLMs can potentially generate misleading or false information, which can contribute to the spread of misinformation and disinformation. </span><span class="koboSpan" id="kobo.452.2">Consider the case where an LLM is used by a news agency to automatically summarize and publish news articles. </span><span class="koboSpan" id="kobo.452.3">The model unintentionally generates a summary that misrepresents the original story, leading to the spread of misinformation about a crucial </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">business merger.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.454.1">Safety</span></strong><span class="koboSpan" id="kobo.455.1">: Ensuring that the content generated by LLMs adheres to ethical guidelines, legal regulations, and community standards while avoiding promoting harmful or offensive content. </span><span class="koboSpan" id="kobo.455.2">Consider the case where an e-commerce platform uses an LLM to generate product descriptions for sellers. </span><span class="koboSpan" id="kobo.455.3">The LLM can create a description that promotes a potentially harmful product, such as a recalled item or an item that violates safety regulations, exposing the platform to legal and </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">ethical issues.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.457.1">Transparency and explainability</span></strong><span class="koboSpan" id="kobo.458.1">: Ensuring that the decisions made by LLMs are transparent, understandable, and justifiable to users and stakeholders. </span><span class="koboSpan" id="kobo.458.2">Consider the case where an insurance company uses an LLM to assess risk and determine premiums for customers. </span><span class="koboSpan" id="kobo.458.3">A customer receives a significantly higher premium and requests an explanation for the increase. </span><span class="koboSpan" id="kobo.458.4">The LLM’s decision-making</span><a id="_idIndexMarker1414"/><span class="koboSpan" id="kobo.459.1"> process is, by itself, opaque and difficult to understand, making it challenging for the company to provide a clear and </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">justifiable explanation.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.461.1">Now that we have identified these challenges, let’s move on to the next section, where we will explore potential solutions and strategies to overcome </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">these limitations.</span></span></p>
<h1 id="_idParaDest-288"><a id="_idTextAnchor299"/><span class="koboSpan" id="kobo.463.1">Tackling challenges with LLM solutions</span></h1>
<p><span class="koboSpan" id="kobo.464.1">Tackling the pesky challenges</span><a id="_idIndexMarker1415"/><span class="koboSpan" id="kobo.465.1"> that LLMs face is key to unlocking their full potential and making them our trusty tools or sidekicks in solving real-world problems. </span><span class="koboSpan" id="kobo.465.2">Only by tackling these challenges can an LLM solution be formed objectively and effectively. </span><span class="koboSpan" id="kobo.465.3">In this section, we’ll dive into various complementary strategies that can help us tackle these challenges and boost the performance of LLMs by its high-level issue type. </span><span class="koboSpan" id="kobo.465.4">We will start with output and </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">input limitations.</span></span></p>
<h2 id="_idParaDest-289"><a id="_idTextAnchor300"/><span class="koboSpan" id="kobo.467.1">Tackling the output and input limitation challenge</span></h2>
<p><span class="koboSpan" id="kobo.468.1">Navigating the output and input limitation</span><a id="_idIndexMarker1416"/><span class="koboSpan" id="kobo.469.1"> challenges is vital for unlocking the full potential of LLMs, allowing them to efficiently process diverse data types, formats, and context sizes while delivering accurate and reliable results. </span><span class="koboSpan" id="kobo.469.2">The solutions are </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.471.1">Customized pre-processing</span></strong><span class="koboSpan" id="kobo.472.1">: Design tailored pre-processing techniques to transform non-text data into a format that can be efficiently processed by LLMs. </span><span class="koboSpan" id="kobo.472.2">For example, design a structure that places structured tabular data as the </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">LLM prompt.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.474.1">Use context limit expansion neural network components</span></strong><span class="koboSpan" id="kobo.475.1">: Implement advanced neural network components such as LongLORA, which requires you to fine-tune an existing model, to expand the context window size, allowing LLMs to process larger amounts of information. </span><span class="koboSpan" id="kobo.475.2">However, it is essential to note that this option might not be available for external LLM providers and might only be feasible if you are considering hosting your own </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">LLM model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.477.1">LLM context optimization</span></strong><span class="koboSpan" id="kobo.478.1">: Any wasted space or repetitive content limits the depth and breadth of the answers we can extract and generate. </span><span class="koboSpan" id="kobo.478.2">There are three possible </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">methods here:</span></span><ul><li><span class="koboSpan" id="kobo.480.1">Select only the most relevant and unique information to be included in the LLM’s context window. </span><span class="koboSpan" id="kobo.480.2">The maximal marginal relevance algorithm can be used to find a set of both relevant and unique sets of information from the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">distance scores.</span></span></li><li><span class="koboSpan" id="kobo.482.1">Consider compressing and summarizing the information provided, which can also be done by an LLM, and then use the summarized information as context in the main </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">LLM prompt.</span></span></li><li><span class="koboSpan" id="kobo.484.1">Apply knowledge retrieval on demand instead of by default. </span><span class="koboSpan" id="kobo.484.2">This on-demand behavior can be enforced by treating the RAG as a tool and either teaching the LLM to use it via fine-tuning or </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">in-context learning.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.486.1">Next, we will tackle the challenges with knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">and information.</span></span></p>
<h2 id="_idParaDest-290"><a id="_idTextAnchor301"/><span class="koboSpan" id="kobo.488.1">Tackling the knowledge- and information-related challenge</span></h2>
<p><span class="koboSpan" id="kobo.489.1">Addressing the output and input limitation</span><a id="_idIndexMarker1417"/><span class="koboSpan" id="kobo.490.1"> challenges is crucial for enhancing the versatility and effectiveness of LLMs in solving a wider range of real-world problems across various data modalities and context sizes. </span><span class="koboSpan" id="kobo.490.2">The solutions are </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.492.1">Real-time data integration</span></strong><span class="koboSpan" id="kobo.493.1">: Connecting LLMs to real-time data sources such as APIs, databases, or web services can help them access up-to-date information and provide more accurate responses. </span><span class="koboSpan" id="kobo.493.2">Incorporating relevant information from knowledge bases, using the RAG approach, is part of this solution. </span><span class="koboSpan" id="kobo.493.3">RAG can also help reduce hallucinations compared to if a model is fine-tuned with custom data if a rigorous strict prompt is made to instruct the LLM to not deviate from the context provided in </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">the prompt.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Tool integration</span></strong><span class="koboSpan" id="kobo.496.1">: Enhancing LLM architecture by integrating existing tools, APIs, and specialized algorithms can significantly extend their capabilities, allowing them to tackle tasks that are challenging or impossible for standalone models. </span><span class="koboSpan" id="kobo.496.2">Tools can be used to retrieve extra input context needed for the generation process. </span><span class="koboSpan" id="kobo.496.3">Alternatively, they can be used to accomplish specific tasks that the generated text tells them to do. </span><span class="koboSpan" id="kobo.496.4">Examples include leveraging external search engines, domain-specific APIs, and computational libraries to provide accurate responses, solve complex mathematical problems, or address queries related to real-time data. </span><span class="koboSpan" id="kobo.496.5">For LLMs such as GPT-3.5, which have API access, this can be achieved</span><a id="_idIndexMarker1418"/><span class="koboSpan" id="kobo.497.1"> through effective</span><a id="_idIndexMarker1419"/><span class="koboSpan" id="kobo.498.1"> few-shot prompting, while advanced models such as </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">Toolformer</span></strong><span class="koboSpan" id="kobo.500.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.501.1">WebGPT</span></strong><span class="koboSpan" id="kobo.502.1"> by OpenAI showcase the potential of integrating external tools seamlessly into the LLM’s parametric memory and framework. </span><span class="koboSpan" id="kobo.502.2">WebGPT can browse the internet by detecting the Bing search engine identifier it generates and subsequently execute the search before continuing the generation it’s appended. </span><span class="koboSpan" id="kobo.502.3">Toolformer, on the other hand, is an LLM that can autonomously select and utilize APIs, integrating tools such as calculators, Q&amp;A systems, search engines, translators, and calendars for improved generation. </span><span class="koboSpan" id="kobo.502.4">This is a key functionality of transforming an LLM into an agent that can accomplish </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">real-world tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.504.1">Reordering the relevant context position in the LLM context</span></strong><span class="koboSpan" id="kobo.505.1">: This solution involves reordering the input text to distribute important information more evenly throughout the context. </span><span class="koboSpan" id="kobo.505.2">By following a specific pattern, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">[1,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.507.1">3,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.508.1">5,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.509.1">7,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.510.1">9,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.511.1">10,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.512.1">8,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.513.1">6,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.514.1">4,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.515.1">2]</span></strong><span class="koboSpan" id="kobo.516.1">, the LLM is encouraged to pay equal attention to all parts of the text, reducing the likelihood of missing valuable information placed in </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">the middle.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.518.1">Utilizing the surrounding information from the same document in LLM context:</span></strong><span class="koboSpan" id="kobo.519.1"> This solution enhances the LLM's understanding by incorporating additional information from the source document. </span><span class="koboSpan" id="kobo.519.2">Expanding the scope of retrieval to include surrounding text or metadata helps the LLM generate more accurate and comprehensive responses, ensuring it considers the broader context. </span><span class="koboSpan" id="kobo.519.3">This approach improves the LLM's ability to address complex questions and provide well-informed responses, which effectively solves the documentation search use case issue. </span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.520.1">Filtering out irrelevant context using the LLM:</span></strong><span class="koboSpan" id="kobo.521.1"> Before proceeding with the generation task, the LLM is employed to identify and remove any irrelevant context. </span><span class="koboSpan" id="kobo.521.2">This refined context is then used for generating responses. </span><span class="koboSpan" id="kobo.521.3">This seemingly simple and logical method has demonstrated its effectiveness in most cases as introduced in the paper </span><a href="https://arxiv.org/abs/2311.11829v1"><span class="koboSpan" id="kobo.522.1">https://arxiv.org/abs/2311.11829v1</span></a><span class="koboSpan" id="kobo.523.1">. </span><span class="koboSpan" id="kobo.523.2">Moreover, the black box nature of this technique allows for easy implementation, contributing to more intuitive and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">LLM-generated content.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.525.1">Regularly building an up-to-date knowledge base</span></strong><span class="koboSpan" id="kobo.526.1">: To address the issue of single index failures, it is essential to maintain and update the LLM’s knowledge base regularly. </span><span class="koboSpan" id="kobo.526.2">This ensures that the LLM stays current with recent developments and can provide accurate information across a wide range of subjects, ultimately enhancing its reliability and effectiveness in solving </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">real-world problems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.528.1">Treat RAG as a tool an LLM can use dynamically based on its generation</span></strong><span class="koboSpan" id="kobo.529.1">: This will help solve the problem of not being able to perform summarization at the global level </span><a id="_idIndexMarker1420"/><span class="koboSpan" id="kobo.530.1">of a knowledge base. </span><span class="koboSpan" id="kobo.530.2">Similar to how Deadpool is aware of being a comic book character, we need the retrieval process to be aware of the type of knowledge base it is retrieving from, along with a special handler for summarization tasks. </span><span class="koboSpan" id="kobo.530.3">A bonus here is to allow the LLM to configure how many rows of relevant text to return to the scope of summarization that can be expanded and shrunk </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">as required.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.532.1">Multi-index retrieval</span></strong><span class="koboSpan" id="kobo.533.1">: To address the issue of single-index failures, a multi-index retrieval approach can be employed. </span><span class="koboSpan" id="kobo.533.2">This solution involves decomposing – or in other words, chunking – the user’s query into multiple components and retrieving information from various sources or knowledge indexes. </span><span class="koboSpan" id="kobo.533.3">This multi-faceted search strategy helps gather more diverse and comprehensive information, reducing the likelihood of overlooking relevant details due to a single index’s limitations. </span><span class="koboSpan" id="kobo.533.4">Consider a user asking about a rare bird species. </span><span class="koboSpan" id="kobo.533.5">Using a single index might yield limited information. </span><span class="koboSpan" id="kobo.533.6">With a multi-index retrieval approach, the LLM would do </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">the following:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.535.1">Decompose the query into components (for example, habitat, diet, </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">and appearance).</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.537.1">Retrieve data from various sources (for example, ornithology databases, nature websites, and </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">social media).</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.539.1">Aggregate and synthesize the data to generate a </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">comprehensive response.</span></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.541.1">Set up directed acyclic graph (DAG) workflows</span></strong><span class="koboSpan" id="kobo.542.1">: Setting up DAG workflows involves organizing a series of tasks or processes in a structured, non-circular sequence to efficiently process multiple sources of information and extend an LLM’s functionality. </span><span class="koboSpan" id="kobo.542.2">In the context of LLMs, a DAG workflow can be manually designed to connect various tools, APIs, and algorithms while addressing the challenges related to real-time data integration, tool integration, and multi-index retrieval. </span><span class="koboSpan" id="kobo.542.3">Let’s consider a use case where a user wants to plan a trip and needs information on various aspects of the destination, such as weather, attractions, and local cuisine. </span><span class="koboSpan" id="kobo.542.4">An LLM could use the DAG workflow to address this complex query efficiently. </span><span class="koboSpan" id="kobo.542.5">Here’s an example</span><a id="_idIndexMarker1421"/><span class="koboSpan" id="kobo.543.1"> of a DAG workflow </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">for LLMs:</span></span><ol><li class="upper-roman" value="1"><span class="koboSpan" id="kobo.545.1">Decompose the user’s query into sub-queries or components, specifically, weather forecast, top attractions, and </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">cuisines topics.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.547.1">For each subquery, identify relevant tools, APIs, or data sources. </span><span class="koboSpan" id="kobo.547.2">For weather forecast, we will retrieve data from a weather API. </span><span class="koboSpan" id="kobo.547.3">For top attractions, we will extract information from travel website knowledge base. </span><span class="koboSpan" id="kobo.547.4">For local cuisine, we will gather data from restaurant review </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">website APIs.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.549.1">Apply summarization</span><a id="_idIndexMarker1422"/><span class="koboSpan" id="kobo.550.1"> to each fact separately before using it as part of the LLM’s </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">input context.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.552.1">Execute the LLM generation process with the user query and the </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">summarized facts.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.554.1">Publish the results on a website through </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">an API.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.556.1">This DAG is depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.557.1">Figure 19</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.558.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">:</span></span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer178">
<span class="koboSpan" id="kobo.560.1"><img alt="Figure 19.3 – An example LLM DAG workflow" src="image/B18187_19_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.561.1">Figure 19.3 – An example LLM DAG workflow</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.562.1">By setting up a manual DAG workflow, an LLM can efficiently process information from multiple sources, leverage external tools and APIs, and provide accurate and reliable responses to a wide range of </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">real-world problems.</span></span></p>
<p><span class="koboSpan" id="kobo.564.1">This strategy helps the LLM provide a more accurate and detailed response, even when information is scarce or not readily available</span><a id="_idIndexMarker1423"/><span class="koboSpan" id="kobo.565.1"> in a single index. </span><span class="koboSpan" id="kobo.565.2">The type of problem</span><a id="_idIndexMarker1424"/><span class="koboSpan" id="kobo.566.1"> this solves is more commonly known as multi-hop </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">question answering.</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">Next, we will tackle the challenges of accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">and reliability.</span></span></p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor302"/><span class="koboSpan" id="kobo.570.1">Tackling the challenges of accuracy and reliability</span></h2>
<p><span class="koboSpan" id="kobo.571.1">Ensuring the accuracy and reliability</span><a id="_idIndexMarker1425"/><span class="koboSpan" id="kobo.572.1"> of LLMs is crucial for building trust in their abilities and making them effective problem-solving tools. </span><span class="koboSpan" id="kobo.572.2">The solutions that can help solve accuracy and reliability-related challenges are </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.574.1">Treat the LLM solution as any modeling experiment</span></strong><span class="koboSpan" id="kobo.575.1">: Pair the LLM with a knowledge base, evaluate its performance using relevant metrics, and gather insights to fine-tune its capabilities iteratively according to the deep learning life cycle. </span><span class="koboSpan" id="kobo.575.2">This will help you choose a model that at least produces fewer hallucinations and can help you understand its effectiveness for your </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">use case.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.577.1">Fine-tune retriever embedding models</span></strong><span class="koboSpan" id="kobo.578.1">: This is instead of just depending on pre-trained embedding models or embedding model providers. </span><span class="koboSpan" id="kobo.578.2">This can improve the retrieval accuracy, thereby boosting the quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">LLM-generated responses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.580.1">Prompt engineering</span></strong><span class="koboSpan" id="kobo.581.1">: Prompt engineering is the process of crafting effective and targeted prompts to guide a language model’s response, thereby improving its accuracy, relevance, and overall performance. </span><span class="koboSpan" id="kobo.581.2">Consider implementing the </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">following techniques:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.583.1">Chain-of-thought (CoT)</span></strong><span class="koboSpan" id="kobo.584.1">: The method encourages LLMs to generate step-by-step reasoning</span><a id="_idIndexMarker1426"/><span class="koboSpan" id="kobo.585.1"> traces, leading to more accurate</span><a id="_idIndexMarker1427"/><span class="koboSpan" id="kobo.586.1"> and structured responses for tasks involving arithmetic, commonsense reasoning, and other problem-solving scenarios. </span><span class="koboSpan" id="kobo.586.2">By guiding the LLM through a series of reasoning steps, CoT helps reduce issues such as fact hallucination while enhancing the overall quality and coherence of the </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">generated content.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.588.1">ReAct</span></strong><span class="koboSpan" id="kobo.589.1">: This method is a framework</span><a id="_idIndexMarker1428"/><span class="koboSpan" id="kobo.590.1"> that interleaves reasoning traces and task-specific actions, enabling LLMs to generate more reliable and factual responses. </span><span class="koboSpan" id="kobo.590.2">By incorporating dynamic reasoning and interaction with external sources, ReAct effectively addresses issues such as fact hallucination and error propagation, resulting in improved human interpretability and trustworthiness </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">of LLMs.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.592.1">Prompt tuning</span></strong><span class="koboSpan" id="kobo.593.1">: Prompt tuning is a technique for refining</span><a id="_idIndexMarker1429"/><span class="koboSpan" id="kobo.594.1"> LLM behavior by optimizing the input prompts using gradient-based methods, which allows for better control over the model’s responses, and leads to improved accuracy and relevance in various problem-solving tasks. </span><span class="koboSpan" id="kobo.594.2">By fine-tuning prompts, users can effectively guide the LLM to generate more desirable and context-specific outputs. </span><span class="koboSpan" id="kobo.594.3">This only applies to LLMs you can host</span><a id="_idIndexMarker1430"/> <span class="No-Break"><span class="koboSpan" id="kobo.595.1">yourself, however.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.596.1">Relying on well-engineered prompts</span></strong><span class="koboSpan" id="kobo.597.1">: Leverage published well-engineered prompts instead of crafting one of your own. </span><span class="koboSpan" id="kobo.597.2">This is a technique used by Langchain and Auto-GPT. </span><span class="koboSpan" id="kobo.597.3">AutoGPT is an open source Python application based on GPT-4. </span><span class="koboSpan" id="kobo.597.4">It automates the execution of tasks without requiring multiple</span><a id="_idIndexMarker1431"/><span class="koboSpan" id="kobo.598.1"> prompts, using AI agents to access the web and perform actions with minimal guidance. </span><span class="koboSpan" id="kobo.598.2">Unlike ChatGPT, AutoGPT can execute larger tasks such as creating websites and developing marketing strategies without needing step-by-step instructions. </span><span class="koboSpan" id="kobo.598.3">It has various applications, such as generating content, designing logos, and </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">developing chatbots.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.600.1">Rejection sampling (best-of-n) reference</span></strong><span class="koboSpan" id="kobo.601.1">: Use rejection sampling techniques to improve the quality of generated responses by selecting the best response from multiple attempts. </span><span class="koboSpan" id="kobo.601.2">The best response can be evaluated through a </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">chosen metric.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.603.1">Re-ranking relevance distance scores from knowledge retrieval</span></strong><span class="koboSpan" id="kobo.604.1">: Knowledge retrieval is in the domain of recommendation systems. </span><span class="koboSpan" id="kobo.604.2">A common technique that’s used is to implement proper regression-based recommendation models to re-rank relevance distance scores. </span><span class="koboSpan" id="kobo.604.3">This can help provide more accurate and potentially more personalized relevant information with more contextual data. </span><span class="koboSpan" id="kobo.604.4">This technique is used by most real-world large-scale recommendation-based products, such </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">as YouTube.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">Iterative retrieval and generation</span></strong><span class="koboSpan" id="kobo.607.1">: Use techniques such as self-ask, Active RAG, and ITER-RETGEN to generate temporary responses, evaluate their quality, and iteratively refine them using retrieved knowledge. </span><span class="koboSpan" id="kobo.607.2">This approach can reduce hallucinations and improve the quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">LLM-generated content.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">Multi-agent systems</span></strong><span class="koboSpan" id="kobo.610.1">: Develop a multi-agent system composed of agents with unique personas and contexts to address diverse problem-solving scenarios. </span><span class="koboSpan" id="kobo.610.2">These agents can collaborate, share information, and provide more comprehensive and reliable solutions. </span><span class="koboSpan" id="kobo.610.3">An example of this is </span><strong class="bold"><span class="koboSpan" id="kobo.611.1">AutoAgents</span></strong><span class="koboSpan" id="kobo.612.1">. </span><span class="koboSpan" id="kobo.612.2">AutoAgents is an innovative</span><a id="_idIndexMarker1432"/><span class="koboSpan" id="kobo.613.1"> framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. </span><span class="koboSpan" id="kobo.613.2">The framework consists of two stages: drafting and execution. </span><span class="koboSpan" id="kobo.613.3">In the drafting stage, an agent team and execution plan are generated based on the input task, while the execution stage refines the plan through inter-agent collaboration and feedback to produce the outcome. </span><span class="koboSpan" id="kobo.613.4">AutoAgents can dynamically synthesize and coordinate multiple expert agents to form customized AI teams for diverse tasks. </span><span class="koboSpan" id="kobo.613.5">Experiments on open-ended question-answering and trivia creative writing tasks demonstrate the effectiveness of AutoAgents compared to existing methods. </span><span class="koboSpan" id="kobo.613.6">AutoAgents offers new perspectives for tackling complex tasks by assigning different roles to different tasks and promoting </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">team</span></span><span class="No-Break"><a id="_idIndexMarker1433"/></span><span class="No-Break"><span class="koboSpan" id="kobo.615.1"> cooperation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.616.1">Next, we will dive into solutions for the runtime </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">performance challenge</span></span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">.</span></span></p>
<h2 id="_idParaDest-292"><a id="_idTextAnchor303"/><span class="koboSpan" id="kobo.619.1">Tackling the runtime performance challenge</span></h2>
<p><span class="koboSpan" id="kobo.620.1">The runtime performance challenge</span><a id="_idIndexMarker1434"/><span class="koboSpan" id="kobo.621.1"> is a critical issue that can significantly impact the efficiency and effectiveness of language models. </span><span class="koboSpan" id="kobo.621.2">As LLMs continue to grow in complexity and scale, optimizing their runtime performance becomes more crucial than ever. </span><span class="koboSpan" id="kobo.621.3">The solutions to solve this issue are </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.623.1">Caching outputs</span></strong><span class="koboSpan" id="kobo.624.1">: Temporarily store results to avoid recomputing information, enabling faster response times and improved performance. </span><span class="koboSpan" id="kobo.624.2">This approach is particularly useful when dealing with repetitive or </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">similar queries.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.626.1">GPUs and GPU inference accelerators</span></strong><span class="koboSpan" id="kobo.627.1">: This is only applicable for LLMs you host yourself. </span><span class="koboSpan" id="kobo.627.2">LLMs need to run with these components to run in a reasonable time. </span><span class="koboSpan" id="kobo.627.3">These were introduced in more detail in </span><a href="B18187_15.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.628.1">Chapter 15</span></em></span></a><span class="koboSpan" id="kobo.629.1">, </span><em class="italic"><span class="koboSpan" id="kobo.630.1">Deploying Deep Learning Models </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.631.1">to Production</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Use approximate KNN indexes</span></strong><span class="koboSpan" id="kobo.634.1">: Approximate KNN indexes take a much longer time than basic KNN indexes to be set up. </span><span class="koboSpan" id="kobo.634.2">However, after setting them up, the inference time can be 1,000 times faster while maintaining a reasonable retrieval accuracy. </span><span class="koboSpan" id="kobo.634.3">Go to </span><a href="https://ann-benchmarks.com/"><span class="koboSpan" id="kobo.635.1">https://ann-benchmarks.com/</span></a><span class="koboSpan" id="kobo.636.1"> to understand the implications of different approximate KNN </span><a id="_idIndexMarker1435"/><span class="koboSpan" id="kobo.637.1">algorithms. </span><span class="koboSpan" id="kobo.637.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">scann</span></strong><span class="koboSpan" id="kobo.639.1"> algorithm and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">faiss</span></strong> <em class="italic"><span class="koboSpan" id="kobo.641.1">IVFPQFS</span></em><span class="koboSpan" id="kobo.642.1"> algorithm provide a good balance between index build time, index size, retrieval recall, and retrieval runtime. </span><span class="koboSpan" id="kobo.642.2">However, an approximate KNN algorithm is only required with large knowledge bases since the retrieval speed of a small knowledge base is already fast, which is less than 1 second. </span><span class="koboSpan" id="kobo.642.3">Typically, suitable data</span><a id="_idIndexMarker1436"/><span class="koboSpan" id="kobo.643.1"> dimensions lie in the range of three-digit vector column sizes, and seven-digit vector </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">row sizes.</span></span></li>
</ul>
<h2 id="_idParaDest-293"><a id="_idTextAnchor304"/><span class="koboSpan" id="kobo.645.1">Tackling the challenge of ethical implications and societal impacts</span></h2>
<p><span class="koboSpan" id="kobo.646.1">Addressing the ethical implications</span><a id="_idIndexMarker1437"/><span class="koboSpan" id="kobo.647.1"> and societal impacts of LLM solutions is crucial for ensuring their responsible and sustainable deployment across various applications. </span><span class="koboSpan" id="kobo.647.2">By considering the ethical and societal consequences of LLM-generated content, developers can create models that respect user values, adhere to legal guidelines, and contribute positively </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">to society.</span></span></p>
<p><span class="koboSpan" id="kobo.649.1">The strategies to tackle these challenges are </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.651.1">Bias and fairness mitigation</span></strong><span class="koboSpan" id="kobo.652.1">: Consider the following strategy in the context of the methods that were introduced in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.653.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.654.1">, </span><em class="italic"><span class="koboSpan" id="kobo.655.1">Exploring Bias </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.656.1">and Fairness</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.658.1">Data collection and preparation</span></strong><span class="koboSpan" id="kobo.659.1">: Ensure a diverse and representative dataset for fine-tuning LLM models. </span><span class="koboSpan" id="kobo.659.2">Balance sensitive attributes in the data, and eliminate or control potential biases that may arise from these attributes. </span><span class="koboSpan" id="kobo.659.3">Additionally, you can instruct the LLM to specifically not perpetuate bias in natural language as part of the input context. </span><span class="koboSpan" id="kobo.659.4">Better yet, empower users to define their preferences, values, and ethical guidelines, enabling LLMs to generate content that aligns with individual user needs </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">and values.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.661.1">Bias mitigation during fine-tuning</span></strong><span class="koboSpan" id="kobo.662.1">: Implement techniques such as counterfactual data augmentation, adversarial training, or re-sampling during the fine-tuning process to reduce the impact of biased features and </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">improve fairness.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.664.1">Post-processing</span></strong><span class="koboSpan" id="kobo.665.1">: Modify LLM-generated content using techniques such as equalized odds post-processing to ensure fairness in the outputs. </span><span class="koboSpan" id="kobo.665.2">This can be applied when using LLM providers such as OpenAI GPT-4 or fine-tuned open </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">source models.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.667.1">Monitoring and evaluation</span></strong><span class="koboSpan" id="kobo.668.1">: Continuously monitor LLM-generated content for potential biases using bias and fairness metrics, and adjust the model as needed to ensure compliance with ethical guidelines and </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">fairness requirements.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.670.1">Privacy-preserving techniques</span></strong><span class="koboSpan" id="kobo.671.1">: Adopt privacy-preserving approaches, such as differential privacy, federated</span><a id="_idIndexMarker1438"/><span class="koboSpan" id="kobo.672.1"> learning, and homomorphic encryption, to protect sensitive information in the training data and generated content. </span><span class="koboSpan" id="kobo.672.2">Implement policies and guidelines to prevent the inadvertent disclosure of </span><strong class="bold"><span class="koboSpan" id="kobo.673.1">Personal Identifiable Information</span></strong><span class="koboSpan" id="kobo.674.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.675.1">PII</span></strong><span class="koboSpan" id="kobo.676.1">) in </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">LLM-generated content.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.678.1">Fact-checking and credibility assessment</span></strong><span class="koboSpan" id="kobo.679.1">: Incorporate fact-checking and credibility assessment mechanisms into LLM solutions to reduce the risk of generating misleading or false information. </span><span class="koboSpan" id="kobo.679.2">This can involve integrating LLMs with external knowledge sources, such as knowledge databases, to verify the accuracy of generated content. </span><span class="koboSpan" id="kobo.679.3">You can also instruct the LLM to be humble and not return a statement if no facts can be used </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">for verification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.681.1">Content moderation and guardrails</span></strong><span class="koboSpan" id="kobo.682.1">: Implement content moderation techniques, such as keyword filtering, machine learning-based classifiers, and human-in-the-loop review processes, to prevent the generation of harmful or offensive content. </span><span class="koboSpan" id="kobo.682.2">Establish guardrails, such as toxicity thresholds or ethical guidelines, to ensure that LLM-generated content adheres to community standards and </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">legal regulations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.684.1">Transparency and explainability</span></strong><span class="koboSpan" id="kobo.685.1">: Develop methods for enhancing the transparency and explainability of LLM-generated content, such as providing reasoning traces, saliency maps, or counterfactual explanations. </span><span class="koboSpan" id="kobo.685.2">The concepts that were introduced in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.686.1">Chapter 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.687.1">, Explaining Neural Network Predictions</span></em><span class="koboSpan" id="kobo.688.1">, and </span><a href="B18187_12.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.689.1">Chapter 12</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.690.1">, Interpreting Neural Networks</span></em><span class="koboSpan" id="kobo.691.1">, can be applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">an LLM.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.693.1">By implementing these strategies, developers can create LLM solutions that not only respect user values and adhere to legal guidelines but also contribute positively to society. </span><span class="koboSpan" id="kobo.693.2">Addressing the ethical implications and societal impacts of LLMs is an essential step toward building trust in the technology and ensuring its responsible and sustainable deployment across </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">various applications.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">With a comprehensive understanding</span><a id="_idIndexMarker1439"/><span class="koboSpan" id="kobo.696.1"> of the challenges associated with LLMs mentioned, as well as their potential solutions, we can now turn our attention to addressing the overarching challenge of LLM solution adoption across organizations </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">and industries.</span></span></p>
<h2 id="_idParaDest-294"><a id="_idTextAnchor305"/><span class="koboSpan" id="kobo.698.1">Tackling the overarching challenge of LLM solution adoption</span></h2>
<p><span class="koboSpan" id="kobo.699.1">One overarching challenge</span><a id="_idIndexMarker1440"/><span class="koboSpan" id="kobo.700.1"> in realizing the full potential of LLM solutions lies in their adoption across organizations and industries. </span><span class="koboSpan" id="kobo.700.2">Similar to the adoption of any machine learning or deep learning solution, the key factor driving the adoption of LLMs is confidence. </span><span class="koboSpan" id="kobo.700.3">Confidence in the technology’s capabilities, its effectiveness in addressing specific use cases, and its ability to deliver tangible results are essential for </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">widespread adoption.</span></span></p>
<p><span class="koboSpan" id="kobo.702.1">To overcome this challenge, it is crucial to systematically educate organizations about the power of LLMs, their diverse applications, and how they can be tailored to meet specific needs. </span><span class="koboSpan" id="kobo.702.2">This includes demonstrating the benefits of LLMs through real-world success stories, providing practical guidance on implementing LLM solutions, and offering support for organizations navigating the complexities of integrating LLMs into </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">their workflows.</span></span></p>
<p><span class="koboSpan" id="kobo.704.1">Building confidence in LLM solutions involves thoroughly evaluating their performance, addressing the challenges discussed earlier in this chapter, and ensuring that the solutions meet their intended objectives. </span><span class="koboSpan" id="kobo.704.2">By implementing a comprehensive set of evaluation approaches, including quantitative metrics and qualitative manual evaluations, organizations can better assess the performance, accuracy, and overall quality of the results from an LLM solution. </span><span class="koboSpan" id="kobo.704.3">These evaluations should be conducted iteratively, allowing for ongoing refinement and improvement of the </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">LLM solution.</span></span></p>
<p><span class="koboSpan" id="kobo.706.1">Additionally, addressing the challenges identified in this chapter, such as output and input limitations, knowledge and information-related challenges, accuracy and reliability issues, and runtime performance challenges, is essential for building confidence in LLM solutions. </span><span class="koboSpan" id="kobo.706.2">By leveraging the strategies and techniques discussed in this chapter, organizations can optimize the performance of LLMs and ensure their effectiveness in </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">various applications.</span></span></p>
<p><span class="koboSpan" id="kobo.708.1">Another essential aspect of building confidence in LLM solutions is effective communication and collaboration with stakeholders. </span><span class="koboSpan" id="kobo.708.2">This includes sharing evaluation results, discussing the benefits and potential limitations of LLMs, and addressing any concerns that stakeholders may have regarding the adoption of </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">LLM solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.710.1">In conclusion, adopting LLM solutions successfully requires a combination of rigorous evaluation, addressing challenges, and effective communication with stakeholders. </span><span class="koboSpan" id="kobo.710.2">By treating the adoption of LLM solutions with the same level of care as any machine learning or deep learning solution, organizations can build confidence in the capabilities and performance of LLMs, unlocking their full potential in a wide range of real-world applications. </span><span class="koboSpan" id="kobo.710.3">And with that, we have explored</span><a id="_idIndexMarker1441"/><span class="koboSpan" id="kobo.711.1"> the challenges that plague LLMs and their solutions </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">in detail.</span></span></p>
<p><span class="koboSpan" id="kobo.713.1">In the following section, we will dive deeper into leveraging LLMs to build autonomous agents, which can significantly expand and improve our </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">problem-solving skills.</span></span></p>
<h1 id="_idParaDest-295"><a id="_idTextAnchor306"/><span class="koboSpan" id="kobo.715.1">Leveraging LLM to build autonomous agents</span></h1>
<p><span class="koboSpan" id="kobo.716.1">One promising area in which LLMs</span><a id="_idIndexMarker1442"/><span class="koboSpan" id="kobo.717.1"> can be harnessed is the development</span><a id="_idIndexMarker1443"/><span class="koboSpan" id="kobo.718.1"> of autonomous agents that can efficiently solve complex problems and interact with their environment. </span><span class="koboSpan" id="kobo.718.2">This section will focus on leveraging LLMs to build such agents and discuss the key aspects that contribute to </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">their effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.720.1">Autonomous agents are AI-driven entities that can perform tasks, make decisions, and interact with their environment independently. </span><span class="koboSpan" id="kobo.720.2">By incorporating LLMs into these agents, developers can create versatile and adaptive systems that can tackle a wide range of challenges. </span><span class="koboSpan" id="kobo.720.3">Here are some essential components</span><a id="_idIndexMarker1444"/><span class="koboSpan" id="kobo.721.1"> of LLM-powered </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">autonomous agents:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.723.1">Planning and decision making</span></strong><span class="koboSpan" id="kobo.724.1">: LLMs can be utilized to generate plans and strategies that guide the agent’s actions, taking into account the context </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">and goals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.726.1">Observing and learning from the environment</span></strong><span class="koboSpan" id="kobo.727.1">: LLMs can be trained to observe and interpret the environment, learning from past experiences and adjusting their </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">behavior accordingly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.729.1">Collaborative problem-solving</span></strong><span class="koboSpan" id="kobo.730.1">: Multi-agent systems can be developed, where each agent has a unique persona and context. </span><span class="koboSpan" id="kobo.730.2">These agents can collaborate, share information, and provide more comprehensive and </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">reliable solutions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.732.1">Self-refinement</span></strong><span class="koboSpan" id="kobo.733.1">: Autonomous agents can use LLMs to analyze their performance, identify areas for improvement, and refine their strategies and behaviors </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">over time.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.735.1">Agents encompass parts and pieces of the solutions to the challenges identified in the </span><em class="italic"><span class="koboSpan" id="kobo.736.1">Identifying challenges with LLM solutions</span></em><span class="koboSpan" id="kobo.737.1"> section of this chapter. </span><span class="koboSpan" id="kobo.737.2">Additionally, the solutions that were introduced here can be combined to expand the scope of problems the overall architected LLM solution can cover. </span><span class="koboSpan" id="kobo.737.3">Examples of published agent methods that were also introduced earlier in this chapter are WebGPT, Toolformer, Auto-GPT, and AutoAgents. </span><span class="koboSpan" id="kobo.737.4">Autonomous agents that leverage LLMs are the key to making powerful LLM solutions. </span><span class="koboSpan" id="kobo.737.5">By combining the strengths of LLMs with the adaptability and decision-making capabilities of agents, developers can create groundbreaking systems that can revolutionize various domains </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">and industries.</span></span></p>
<p><span class="koboSpan" id="kobo.739.1">With a comprehensive understanding</span><a id="_idIndexMarker1445"/><span class="koboSpan" id="kobo.740.1"> of LLM solutions and their potential</span><a id="_idIndexMarker1446"/><span class="koboSpan" id="kobo.741.1"> applications, let’s explore some specific use cases where they can be </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">employed effectively.</span></span></p>
<h1 id="_idParaDest-296"><a id="_idTextAnchor307"/><span class="koboSpan" id="kobo.743.1">Exploring LLM solution use cases</span></h1>
<p><span class="koboSpan" id="kobo.744.1">In this section, we will explore some fascinating</span><a id="_idIndexMarker1447"/><span class="koboSpan" id="kobo.745.1"> real-world applications where LLM solutions can truly shine. </span><span class="koboSpan" id="kobo.745.2">This will give you a sense of how revolutionary LLM solutions are. </span><span class="koboSpan" id="kobo.745.3">The use cases are </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.747.1">Travel itinerary planner</span></strong><span class="koboSpan" id="kobo.748.1">: LLMs can be employed to develop advanced travel itinerary planners that generate personalized trip plans based on user preferences and constraints. </span><span class="koboSpan" id="kobo.748.2">By integrating LLMs with travel APIs, such as flight, hotel, and attraction databases, as well as real-time data sources such as weather and traffic information, these planners can provide context-aware recommendations tailored to individual traveler needs. </span><span class="koboSpan" id="kobo.748.3">Notably, companies such as Booking.com and Expedia integrated this into their products, and Agoda announced that they will be working </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">on it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.750.1">Intelligent tutoring systems</span></strong><span class="koboSpan" id="kobo.751.1">: LLMs can be used to develop intelligent tutoring systems that offer personalized learning experiences for students. </span><span class="koboSpan" id="kobo.751.2">By integrating LLMs with educational content, assessment tools, and learner data, these systems can generate targeted learning materials, provide real-time feedback, and adapt to individual learning needs. </span><span class="koboSpan" id="kobo.751.3">This enables a more efficient and engaging learning experience for students. </span><span class="koboSpan" id="kobo.751.4">Notably, Duolingo is a company that implemented such a solution in their gamified language </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">learning product.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.753.1">Automated email responses</span></strong><span class="koboSpan" id="kobo.754.1">: LLM solutions can be employed to develop automated email response systems that handle various types of inquiries, such as customer support, sales inquiries, or general information requests. </span><span class="koboSpan" id="kobo.754.2">By integrating LLMs with email APIs, CRM systems, and relevant knowledge bases, email responses can be personalized, accurate, and contextually relevant. </span><span class="koboSpan" id="kobo.754.3">This helps businesses streamline their customer communication and provide efficient support. </span><span class="koboSpan" id="kobo.754.4">Notably, Nanonets AI is a company that implemented this as part of </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">their product.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.756.1">Code generation</span></strong><span class="koboSpan" id="kobo.757.1">: LLMs can be used to generate code snippets, algorithms, or entire software programs</span><a id="_idIndexMarker1448"/><span class="koboSpan" id="kobo.758.1"> based on user input or specific requirements. </span><span class="koboSpan" id="kobo.758.2">Solutions such as GitHub Copilot leverage LLMs to assist developers in writing code, suggesting relevant code snippets, and completing sections of code based on context. </span><span class="koboSpan" id="kobo.758.3">By integrating LLMs with code repositories, programming language APIs, and domain-specific knowledge bases, code generation can be tailored to specific programming languages, frameworks, and use cases, improving </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">developer productivity.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.760.1">Customer support chatbots</span></strong><span class="koboSpan" id="kobo.761.1">: LLM solutions can be employed to develop advanced, context-aware chatbots that can handle customer inquiries and support requests more effectively. </span><span class="koboSpan" id="kobo.761.2">By integrating LLMs with </span><strong class="bold"><span class="koboSpan" id="kobo.762.1">customer relationship management</span></strong><span class="koboSpan" id="kobo.763.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.764.1">CRM</span></strong><span class="koboSpan" id="kobo.765.1">) systems and knowledge bases, chatbots can provide personalized and accurate responses to customer queries. </span><span class="koboSpan" id="kobo.765.2">This helps businesses improve their customer</span><a id="_idIndexMarker1449"/><span class="koboSpan" id="kobo.766.1"> support services, reduce response times, and increase customer satisfaction. </span><span class="koboSpan" id="kobo.766.2">Companies such as forethought.ai, Ada, and EBI.AI provide such a solution in </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">their product.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.768.1">Medical diagnosis and treatment suggestions</span></strong><span class="koboSpan" id="kobo.769.1">: LLMs can be employed to develop advanced diagnostic tools</span><a id="_idIndexMarker1450"/><span class="koboSpan" id="kobo.770.1"> that analyze patient symptoms, medical history, and relevant medical literature to suggest potential diagnoses</span><a id="_idIndexMarker1451"/><span class="koboSpan" id="kobo.771.1"> and treatment options. </span><span class="koboSpan" id="kobo.771.2">By integrating LLMs with </span><strong class="bold"><span class="koboSpan" id="kobo.772.1">electronic health record</span></strong><span class="koboSpan" id="kobo.773.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.774.1">EHR</span></strong><span class="koboSpan" id="kobo.775.1">) systems, medical databases, and domain-specific knowledge bases, these tools can help healthcare professionals make more informed decisions and improve patient outcomes. </span><span class="koboSpan" id="kobo.775.2">Notably, Harman, a Samsung company, implemented and offered such a solution as part of their </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">offered services.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.777.1">Personal finance management</span></strong><span class="koboSpan" id="kobo.778.1">: LLMs can be used to develop intelligent personal finance management applications that provide tailored financial advice, budgeting suggestions, and investment recommendations based on user-specific financial goals and risk tolerance. </span><span class="koboSpan" id="kobo.778.2">By integrating LLMs with banking APIs, stock market data, and financial knowledge bases, these applications can offer context-aware financial planning and guidance to users. </span><span class="koboSpan" id="kobo.778.3">Although not exactly a service or product, Bloomberg has developed BloombergGPT, a 50-billion parameter large language model designed specifically for finance, which showcases the potential of LLMs in the </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">financial domain.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.780.1">Creative content generation</span></strong><span class="koboSpan" id="kobo.781.1">: LLMs can be employed to generate creative content, such as stories, poetry, or music, based on user inputs, preferences, or inspirations. </span><span class="koboSpan" id="kobo.781.2">By integrating LLMs with databases of literary works, music libraries, and knowledge bases on creative techniques and styles, these applications can produce unique and engaging content that caters to individual artistic tastes and needs. </span><span class="koboSpan" id="kobo.781.3">Notably, Jasper built a platform to account for this </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">use case.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.783.1">Legal document analysis and drafting</span></strong><span class="koboSpan" id="kobo.784.1">: LLMs can be used to develop advanced legal document analysis and drafting tools that assist legal professionals in reviewing contracts, identifying potential issues, and generating legal documents based on specific requirements. </span><span class="koboSpan" id="kobo.784.2">By integrating LLMs with legal databases, contract templates, and domain-specific knowledge bases, these tools can help streamline legal work and improve efficiency in the legal industry. </span><span class="koboSpan" id="kobo.784.3">Notably, netdocuments implemented this use case with </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">their product.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.786.1">Smart home automation</span></strong><span class="koboSpan" id="kobo.787.1">: LLMs can be employed to develop intelligent home automation systems that understand natural language commands and adapt to user preferences and routines. </span><span class="koboSpan" id="kobo.787.2">By integrating LLMs with smart home devices, APIs, and user behavior data, these systems can provide a more intuitive and personalized home automation experience, enabling users to control their home environment</span><a id="_idIndexMarker1452"/><span class="koboSpan" id="kobo.788.1"> with ease and convenience. </span><span class="koboSpan" id="kobo.788.2">Amazon Alexa is a prime example of this </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">use case.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.790.1">In each of these use cases, integrating LLMs with the relevant tools, APIs, and data sources ensures that the generated content, recommendations, and responses are accurate, contextually relevant, and tailored to specific needs, enhancing user experiences and providing valuable support in </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">various domains.</span></span></p>
<h1 id="_idParaDest-297"><a id="_idTextAnchor308"/><span class="koboSpan" id="kobo.792.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.793.1">In this chapter, we explored LLMs and their potential to address real-world problems and create value across various applications. </span><span class="koboSpan" id="kobo.793.2">We discussed the key aspects of architecting LLM solutions, such as handling knowledge, interacting with real-time data and tools, evaluating LLM solutions, identifying and addressing challenges, and leveraging LLMs to build autonomous agents. </span><span class="koboSpan" id="kobo.793.3">We also emphasized the importance of retrieval-augmented language models for providing contextually relevant information and examined various techniques and libraries to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">LLM solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.795.1">We also discussed the limitations of LLMs, such as output and input limitations, knowledge and information-related challenges, accuracy and reliability issues, runtime performance challenges, ethical implications and societal impacts, and the overarching challenge of LLM solution adoption. </span><span class="koboSpan" id="kobo.795.2">To tackle these limitations, we presented various complementary strategies, such as real-time data integration, tool integration, prompt engineering, rejection sampling, multi-agent systems, runtime optimization techniques, bias and fairness mitigation, content moderation, and enabling LLM transparency and explainability. </span><span class="koboSpan" id="kobo.795.3">Lastly, we discussed leveraging LLMs to build autonomous agents, which can significantly expand and improve problem-solving abilities in </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">diverse applications.</span></span></p>
<p><span class="koboSpan" id="kobo.797.1">By understanding the intricacies of LLM solutions and applying the strategies and techniques discussed in this chapter, organizations and individuals can harness the full potential of LLMs to drive innovation and improve outcomes across </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">various applications.</span></span></p>
<p><span class="koboSpan" id="kobo.799.1">By reading </span><em class="italic"><span class="koboSpan" id="kobo.800.1">The Deep Learning Architect Handbook</span></em><span class="koboSpan" id="kobo.801.1">, you have gained invaluable insights into the various stages of the deep learning life cycle, exploring critical aspects from planning and data preparation to model deployment and governance. </span><span class="koboSpan" id="kobo.801.2">By reaching the end of this enlightening journey, you are now armed with the knowledge and skills to design, develop, and deploy effective deep learning solutions. </span><span class="koboSpan" id="kobo.801.3">To build upon this strong foundation, consider taking the following </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">next steps:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.803.1">Apply your newfound knowledge to real-world projects, either in your professional field or through open source contributions, to gain practical experience and deepen </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">your understanding</span></span></li>
<li><span class="koboSpan" id="kobo.805.1">Stay up-to-date with the latest research, trends, and breakthroughs in deep learning by attending conferences, following influential researchers, and reading </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">research papers</span></span></li>
<li><span class="koboSpan" id="kobo.807.1">Explore specialized areas within deep learning that interest you, such as reinforcement learning, generative adversarial networks, or few-shot learning, to further expand </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">your expertise</span></span></li>
<li><span class="koboSpan" id="kobo.809.1">Collaborate with fellow deep learning enthusiasts and professionals, joining communities, discussion forums, and social media groups to exchange ideas, share experiences, and learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">each other</span></span></li>
<li><span class="koboSpan" id="kobo.811.1">Consider pursuing advanced courses, certifications, or even a degree in deep learning or a related field to enhance your education </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">and qualifications</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.813.1">Embrace the challenges and triumphs that lie ahead, for with the mastery of building deep learning models with intricate deep learning architectures, a keen understanding of bias and fairness, and the ability to monitor and maintain model performance, you are well-prepared to unleash the full potential of deep learning and drive innovation across a vast array of applications. </span><span class="koboSpan" id="kobo.813.2">Here’s to your continued success and growth in the world of </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">deep learning!</span></span></p>
<h1 id="_idParaDest-298"><a id="_idTextAnchor309"/><span class="koboSpan" id="kobo.815.1">Further reading</span></h1>
<ol>
<li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.816.1">RAG</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">: </span></span><a href="https://doi.org/10.48550/arXiv.2005.11401"><span class="No-Break"><span class="koboSpan" id="kobo.818.1">https://doi.org/10.48550/arXiv.2005.11401</span></span></a></li>
<li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.819.1">RETRO</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">: </span></span><a href="https://arxiv.org/pdf/2112.04426.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.821.1">https://arxiv.org/pdf/2112.04426.pdf</span></span></a></li>
<li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.822.1">REALM</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">: </span></span><a href="https://doi.org/10.48550/arXiv.2002.08909"><span class="No-Break"><span class="koboSpan" id="kobo.824.1">https://doi.org/10.48550/arXiv.2002.08909</span></span></a></li>
</ol>
</div>
</body></html>