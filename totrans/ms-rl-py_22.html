<html><head></head><body>
		<div id="_idContainer1789">
			<h1 id="_idParaDest-347"><em class="italic"><a id="_idTextAnchor388"/>Chapter 18</em>: Challenges and Future Directions in Reinforcement Learning</h1>
			<p>In this last chapter, we will summarize the end of our journey in this book: you have done a lot, so think of this as a celebration as well as a bird's-eye view of your achievement. On the other hand, when you take your learnings to use <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) in real-world problems, you will likely encounter multiple challenges. After all, deep RL is still a fast-moving field making a lot of progress in solving these challenges. We have already mentioned most of them in the book and proposed solution approaches. We will briefly recap them and talk about additional future directions for RL. Finally, we will go over some resources and strategies for you to become an RL expert, which you are very well positioned for by coming this far.</p>
			<p>So, here is what we will cover in this chapter:</p>
			<ul>
				<li>What you have achieved with this book</li>
				<li>Challenges and future directions</li>
				<li>Suggestions for aspiring RL experts</li>
				<li>Final words</li>
			</ul>
			<h1 id="_idParaDest-348"><a id="_idTextAnchor389"/>What you have achieved with this book</h1>
			<p>First of all, congratulations! You have come a long way beyond the fundamentals, acquiring the skills and the mindset to apply RL in the real world. Here is what we have done together so far:</p>
			<ul>
				<li>We spent a fair amount of time on bandit problems, which have a tremendous number of applications, not just in industry but also in academia as an auxiliary tool to solve other problems.</li>
				<li>We went deeper into the theory than a typical applied book would to strengthen your foundations of RL.</li>
				<li>We covered many of the algorithms and architectures behind the most successful applications of RL.</li>
				<li>We discussed advanced training strategies to get the most out of the advanced RL algorithms. </li>
				<li>We did hands-on work with realistic case studies.</li>
				<li>Throughout this journey, we both implemented our versions of some of the algorithms, as well as utilized libraries, such as Ray and RLlib, which power many teams and platforms at the top tech companies for their RL applications.</li>
			</ul>
			<p>You absolutely deserve to take a moment to celebrate your success!</p>
			<p>Now, once you are back, it is time to talk about what is ahead of you. RL is at the beginning of its rise. This means multiple things: </p>
			<ul>
				<li>First, it is an opportunity. You are now ahead of the game by making this investment and coming this far. </li>
				<li>Second, since this is cutting edge, there are many challenges to be solved before RL becomes a mature, easy-to-use technology. </li>
			</ul>
			<p>In the next section, we will discuss what those challenges are. That way, you will recognize them when you see them, know that you are not alone, and can set your expectations accordingly in terms of what is needed (data, time, compute resource, and so on) to solve your problem. But you shouldn't worry! RL is a very active and accelerating area of research, so our arsenal to tackle those challenges is getting stronger by the day. See the number of papers submitted to the NeurIPS conference on RL over the years, compiled and presented by Katja Hofmann, a prominent RL researcher, during the conference in 2019:</p>
			<div>
				<div id="_idContainer1787" class="IMG---Figure">
					<img src="image/B14160_18_01.jpg" alt="Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled and presented by Katja Hofmann (source: Hofmann, 2019)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled and presented by Katja Hofmann (source: Hofmann, 2019)</p>
			<p>Therefore, while we talk about the challenges, we also talk about the related research directions, so you will know where to look for the answers.</p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor390"/>Challenges and future directions</h1>
			<p>You may be<a id="_idIndexMarker1454"/> wondering why we are back to talking about <a id="_idIndexMarker1455"/>RL challenges after finishing an advanced-level book on this topic. Indeed, throughout the book, we presented many approaches to mitigate them. On the other hand, we cannot claim these challenges are solved. So, it is important to call them out and discuss future directions for each to provide you with a compass to navigate them. Let's start our disccusion with one of the most important challenges: sample efficiency.</p>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor391"/>Sample efficiency</h2>
			<p>As you are now well <a id="_idIndexMarker1456"/>aware, it takes a lot of data to train an RL model. OpenAI Five, which became a world-class player in the strategy game Dota 2, took 128,000 CPUs and GPUs to train, over many months, collecting a total of 900 years' worth of game experience <strong class="bold">per day</strong> (OpenAI, 2018). RL algorithms are benchmarked on their performances after they are trained on over 10 billion Atari frames (Kapturowski, 2019). This is certainly a lot of compute and resources just to play games. So, sample efficiency is one of the biggest challenges that face real-world RL <a id="_idIndexMarker1457"/>applications.</p>
			<p>Let's discuss the overall directions to mitigate this problem.</p>
			<h3>Sample-efficient algorithms</h3>
			<p>An obvious<a id="_idIndexMarker1458"/> direction is to try to create algorithms that are more sample efficient. Indeed, there is a big push in the research community to this end. We will increasingly compare the algorithms not just based on their best possible performance but also on how quickly and efficiently they reach those performance levels.</p>
			<p>To this end, we will likely talk more and more about the following algorithm classes:</p>
			<ul>
				<li><strong class="bold">Off-policy methods</strong>, which don't require data to be collected under the most recent policy, giving them an edge over policy methods in terms of sample efficiency.</li>
				<li><strong class="bold">Model-based methods</strong>, which can be orders of magnitude more efficient than their model-free counterparts by leveraging the information they possess on environment dynamics.</li>
				<li><strong class="bold">Models with informed priors</strong>, which limit the hypothesis space of the models to a plausible set. Examples of this class use neural ordinary differential equations and Lagrangian neural networks in RL models (Du, 2020; Shen, 2020).</li>
			</ul>
			<h3>Specialized hardware and software architectures for distributed training</h3>
			<p>We can expect<a id="_idIndexMarker1459"/> the <a id="_idIndexMarker1460"/>progress on the algorithmic frontier to be gradual and slow. For those of us who are excited and impatient enthusiasts, a quicker solution is to dump more compute resources into RL projects, get the most out of the existing ones. and train bigger and bigger models. So, it is only reasonable to expect what happened in the <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) space to happen to RL too: NLP models went from being 8billion-parameter models to 17billion, then to 175billion in size with OpenAI's GPT-3 in less than a year, thanks to the optimizations in training architectures and, of course, the supercomputers dedicated to the task:</p>
			<div>
				<div id="_idContainer1788" class="IMG---Figure">
					<img src="image/B14160_18_02.jpg" alt="Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is the number of parameters. Image modified from Rosset, 2020&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is the number of parameters. Image modified from Rosset, 2020</p>
			<p>In addition, innovations<a id="_idIndexMarker1461"/> in RL <a id="_idIndexMarker1462"/>training architectures, such as in Ape-X and SEED RL (<em class="italic">Espeholt, 2019</em>), help existing RL algorithms to run more efficiently, a direction we can expect to see more progress in.</p>
			<h3>Machine teaching</h3>
			<p>Machine teaching <a id="_idIndexMarker1463"/>approaches, such as curriculum learning, reward shaping, and demonstration learning, aim to infuse context and expertise into RL training. They often lead to significant sample efficiency during training and in some cases are needed to make learning even more feasible. Machine <a id="_idIndexMarker1464"/>teaching approaches will become  increasingly popular in the near future to increase the sample efficiency in RL.</p>
			<h3>Multi-task/meta/transfer learning</h3>
			<p>Since training <a id="_idIndexMarker1465"/>an RL model from scratch <a id="_idIndexMarker1466"/>could be very expensive, it only makes<a id="_idIndexMarker1467"/> sense to reuse the models that are trained on other relevant tasks. Today, when we want to develop an application that involves image classification, for example, it is rare that we train a model from scratch. Instead, we use one of the pre-trained models and fine-tune it for our application.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models in an open-standard format for popular tasks such as image classification and machine translation, which I highly recommend you take a look at: <a href="https://github.com/onnx/models">https://github.com/onnx/models</a>.</p>
			<p>We can expect to see similar repositories for RL tasks as well. On a relevant note, approaches such as multi-task learning, which is about training models for more than one task, and meta-learning, which is about training models that can be efficiently transferred to new tasks, will gain momentum and broader adaptation among RL researchers and practitioners.</p>
			<h3>RL-as-a-service</h3>
			<p>If you need to translate<a id="_idIndexMarker1468"/> texts in your app programmatically, one approach is, as we mentioned, to use a pre-trained model. But what if you don't want to invest in maintaining these models? The answer is often to buy it as a service from companies such as Microsoft, Google, and Amazon. These companies have access to huge amounts of data and compute resources, and they constantly upgrade their models using cutting-edge machine learning approaches. Doing the same could be a daunting, even infeasible, or simply impractical undertaking for other businesses without the same resources and focus. We can expect to see RL-as-a-service trend in the industry.</p>
			<p>Sample efficiency is a tough nut to crack, but there are developments in multiple frontiers to achieve<a id="_idIndexMarker1469"/> it, as we have summarized. Next, let's talk about another major challenge: the need for good simulation models.</p>
			<h2 id="_idParaDest-351"><a id="_idTextAnchor392"/>Need for high-fidelity and fast simulation models</h2>
			<p>One of the biggest<a id="_idIndexMarker1470"/> barriers in the way of the broad adaptation of RL in industry is the absence of simulations of processes that companies are interested in optimizing, either <a id="_idIndexMarker1471"/>altogether or at sufficient fidelity. Creating these simulation models often requires a lot of investment, and in some complex tasks, the fidelity of a simulation model won't be high enough for a typical RL approach. To overcome these challenges, we can expect to see more developments in the following areas.</p>
			<h3>Offline RL to learn directly from data</h3>
			<p>Although most <a id="_idIndexMarker1472"/>processes in industry don't have simulation models, it is much more common to have logs describing what<a id="_idIndexMarker1473"/> happened in it. Offline RL approaches, which aim to learn policies directly from data, will become of higher and higher importance as RL finds its way into real-world applications.</p>
			<h3>Approaches to deal with generalization, partial observability, and non-stationarity</h3>
			<p>Even in <a id="_idIndexMarker1474"/>cases <a id="_idIndexMarker1475"/>where a simulation model of a <a id="_idIndexMarker1476"/><a id="_idIndexMarker1477"/>process exists, it is <a id="_idIndexMarker1478"/>quite rare for such a model to be of enough fidelity<a id="_idIndexMarker1479"/> to naively train an RL model that would work in the real-world without additional considerations. This sim2real gap can be thought of as a form of partial observability, which is often handled through memory in the RL model architecture. Combined with generalization techniques, such as domain randomization and regularization, we are already seeing very successful applications where policies trained in a simulation are transferred to the real world. Dealing with non-stationarity is also closely related to the generalization capabilities of RL algorithms.</p>
			<h3>Online learning and fine-tuning on the edge</h3>
			<p>One of the important capabilities that will enable the successful use of RL approaches will be to be able <a id="_idIndexMarker1480"/>to continue training after the deployment of<a id="_idIndexMarker1481"/> a model on the edge. With that, we will be able to fine-tune the models that are trained on a simulation with actual data. In addition, this capability will help RL policies to adapt to the changing conditions in the environment.</p>
			<p>To summarize, we will witness a shift from RL being a tool to use in video games to an alternative to the traditional control and decision-making methods, which will be facilitated by approaches that will remove the dependency on high-fidelity simulations. </p>
			<h2 id="_idParaDest-352"><a id="_idTextAnchor393"/>High-dimensional action spaces</h2>
			<p>CartPole, the iconic<a id="_idIndexMarker1482"/> testbed for RL, has only a few elements in its action space, like in most RL environments used in RL research. Real-life problems, however, can be quite complex in terms of the number of available actions to the agent, which also often depends on the state the agent is in. Approaches such as action embeddings, action masking, and action elimination will come in handy to tackle this challenge in such realistic scenarios.</p>
			<h2 id="_idParaDest-353"><a id="_idTextAnchor394"/>Reward function fidelity</h2>
			<p>Crafting the right <a id="_idIndexMarker1483"/>reward function that leads the agent to the desired behavior is a notoriously difficult undertaking in RL. Inverse RL approaches, which learn the reward from demonstrations, and curiosity-driven learning, which relies on intrinsic rewards, are promising methods to reduce the dependency on hand-engineering the reward function.</p>
			<p>The challenge with reward function engineering is exacerbated when there are multiple and qualitative objectives. There is growing literature on multi-objective RL approaches that either deal with each subproblem individually or produce policies for a given mixture of objectives. </p>
			<p>Another challenge with respect to reward signals in RL environments is the delay and sparsity of rewards. For example, a marketing strategy controlled by an RL agent might observe rewards days, weeks, or even months after the action is taken. Approaches that deal with causality and credit assignment in RL are critical to be able to leverage RL in these environments.</p>
			<p>These are all important branches to keep an eye on since real-world RL problems rarely have well-defined, dense, and scalar objectives.</p>
			<h2 id="_idParaDest-354"><a id="_idTextAnchor395"/>Safety, behavior guarantees, and explainability</h2>
			<p>When training <a id="_idIndexMarker1484"/>RL agents in a computer <a id="_idIndexMarker1485"/>simulation, it is okay, in fact, needed, to <a id="_idIndexMarker1486"/>try random and crazy actions to figure out better policies. For an RL model competing against world-class players in a board game, the worst scenario that can happen is to lose the game, perhaps embarrassingly. The risks are of a different category when an RL agent is in charge of a chemical process or a self-driving car. These are safety-critical systems where the room for error is little to none. In fact, this is one of the biggest disadvantages of RL methods compared to traditional control theory approaches that often come with theoretical guarantees and a solid understanding of the expected behavior. Research on constrained RL and safe exploration is, therefore, crucial to be able to use RL in such systems.</p>
			<p>Even when the system is not safety-critical, such as in inventory replenishment problems, a related challenge is the explainability of the actions suggested by the RL agent in charge. Experts<a id="_idIndexMarker1487"/> who <a id="_idIndexMarker1488"/>oversee the decisions in these processes often demand explanations, especially <a id="_idIndexMarker1489"/>when the suggested actions are counterintuitive. People tend to trade accuracy for explanation, which puts black-box approaches at a disadvantage. Deep learning has come a long way in terms of explainability, and RL will surely benefit from it. On the other hand, this will be an ongoing challenge for machine learning at large.</p>
			<h2 id="_idParaDest-355"><a id="_idTextAnchor396"/>Reproducibility and sensitivity to hyperparameter choices</h2>
			<p>It is one thing to<a id="_idIndexMarker1490"/> train an RL model with the <a id="_idIndexMarker1491"/>close oversight and guidance of many experts for a specific task and after many iterations, yet it is another thing to deploy multiple RL models in production for various environments, which are to be re-trained periodically and hands-off-the-wheel as new data come in. The consistency and resiliency of RL algorithms in terms of producing successful policies under a variety of conditions will be an increasingly important factor when benchmarking them for the research community, as well as for the practitioners who will get to deal with these models and their maintenance in real life.</p>
			<h2 id="_idParaDest-356"><a id="_idTextAnchor397"/>Robustness and adversarial agents</h2>
			<p>Deep learning is<a id="_idIndexMarker1492"/> known to be brittle about its representations. This<a id="_idIndexMarker1493"/> lack of robustness allows adversarial agents to manipulate systems that rely on deep learning. This is a major concern and a very active area of research in the machine learning community. RL will surely piggyback on the developments in the broader machine learning research to address robustness issues in this field.</p>
			<p>These challenges in RL are important to be aware of, especially for practitioners who want to use these tools to solve real-world problems, and this recap will hopefully help with that. We <a id="_idIndexMarker1494"/>covered many of the solution approaches in the book and mentioned the overall<a id="_idIndexMarker1495"/> directions in this section as well, so you know where to look for solutions. All of these are active areas of research, so whenever you encounter these challenges, it is a good idea to take a fresh look at the literature.</p>
			<p>Before we wrap up, I would like to offer my two cents to aspiring RL experts.</p>
			<h1 id="_idParaDest-357"><a id="_idTextAnchor398"/>Suggestions for aspiring RL experts</h1>
			<p>This book is<a id="_idIndexMarker1496"/> designed for an audience who already know the fundamentals of RL. Now that you have finished this book too, you are well-positioned to be an expert in RL. Having said that, RL is a big topic, and this book is really meant to be a compass and kickstarter for you. At this point, if you decide to go deeper to become an RL expert, I have some suggestions.</p>
			<h2 id="_idParaDest-358"><a id="_idTextAnchor399"/>Go deeper into the theory</h2>
			<p>In machine learning, models often fail to produce the expected level of results, at least at the beginning. One big factor that will help you pass these obstacles is to have a good foundation for the math behind the algorithms you are using to solve your problems. This will help you better understand the limitations and assumptions of those algorithms and identify whether they conflict with the realities of the problem at hand. To this end, here is some advice:</p>
			<ul>
				<li>It is never a bad idea to deepen your understanding of probability and statistics. Don't forget that all these algorithms are essentially statistical models.</li>
				<li>A solid understanding of the basic ideas in RL, such as Q-learning and the Bellman equation, is critical to have a good foundation to build modern RL on. This book serves this purpose to some extent. However, I highly recommend you read, multiple times, Rich Sutton and Andrew Barto's book <em class="italic">Reinforcement Learning: An Introduction</em>, which is essentially the Bible of traditional RL.</li>
				<li>Professor Sergey Levine's UC Berkeley course on deep RL, which this book benefited greatly from, is an excellent resource to go deeper into RL. This course is available at <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">http://rail.eecs.berkeley.edu/deeprlcourse/</a>.</li>
				<li>Another great<a id="_idIndexMarker1497"/> resource, specific to multi-tasking and meta-learning, is Professor Chelsea Finn's Stanford course at <a href="https://cs330.stanford.edu/">https://cs330.stanford.edu/</a>.</li>
				<li>The Deep RL Bootcamp taught by the experts in the field is another excellent resource: <a href="https://sites.google.com/view/deep-rl-bootcamp/home">https://sites.google.com/view/deep-rl-bootcamp/home</a>.</li>
			</ul>
			<p>As you go through these resources, and refer back to them from time to time, you will notice your understanding of the topic becomes much deeper.</p>
			<h2 id="_idParaDest-359"><a id="_idTextAnchor400"/>Follow good practitioners and research labs</h2>
			<p>There are excellent research labs focusing on RL, who also publish their findings in detailed blog posts that contain a lot of theoretical and practical insights. Here are some examples:</p>
			<ul>
				<li>OpenAI blog: <a href="https://openai.com/blog/">https://openai.com/blog/</a></li>
				<li>DeepMind blog: <a href="https://deepmind.com/blog">https://deepmind.com/blog</a></li>
				<li><strong class="bold">Berkeley AI Research</strong> (<strong class="bold">BAIR</strong>) blog: <a href="https://bair.berkeley.edu/blog">https://bair.berkeley.edu/blog</a></li>
				<li>Microsoft <a id="_idIndexMarker1498"/>Research RL group: <a href="https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/">https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/</a></li>
				<li>Google AI blog: <a href="https://ai.googleblog.com/">https://ai.googleblog.com/</a></li>
			</ul>
			<p>Even if you don't read every single post, it is a good idea to monitor them regularly to stay synced with the trends of RL research.</p>
			<h2 id="_idParaDest-360"><a id="_idTextAnchor401"/>Learn from papers and their good explanations</h2>
			<p>A single year in AI research is like a year in dog years: a lot happens in it. So, the best way to stay up to date is really to follow the research in the area. This will also expose you to the theory<a id="_idIndexMarker1499"/> and rigorous explanations of the methods. Now, there are two challenges that come with this:</p>
			<ul>
				<li>There are a ton of papers published every year, which makes it impossible to read them all.</li>
				<li>It could be daunting to read equations and proofs.</li>
			</ul>
			<p>To address the former, I suggest you focus on papers accepted to conferences such as NeurIPS, ICLR, ICML, and AAAI. This will still amount to a lot, so you will still have to develop your own thresholds about what to read.</p>
			<p>To address the latter, you can check whether there are good blog posts explaining the papers you would like to understand better. Some high-quality blogs (not specific to RL) to follow are the following:</p>
			<ul>
				<li>Lilian Weng's blog: <a href="https://lilianweng.github.io/lil-log/">https://lilianweng.github.io/lil-log/</a></li>
				<li>Distill: <a href="https://distill.pub/">https://distill.pub/</a></li>
				<li>The Gradient: <a href="https://thegradient.pub/">https://thegradient.pub/</a></li>
				<li>Adrian Colyer's The Morning Paper: <a href="https://blog.acolyer.org/">https://blog.acolyer.org/</a></li>
				<li>Jay Alammar's blog: <a href="http://jalammar.github.io/">http://jalammar.github.io/</a></li>
				<li>Christopher Olah's blog (who is also in the Distill team): <a href="https://colah.github.io/">https://colah.github.io/</a></li>
				<li>Jian Zhang's blog: <a href="mailto:https://medium.com/@jianzhang_23841">https://medium.com/@jianzhang_23841</a></li>
			</ul>
			<p>I have personally learned a lot from these blogs, and still continue to learn from them.</p>
			<h2 id="_idParaDest-361"><a id="_idTextAnchor402"/>Stay up to date with trends in other fields of deep learning</h2>
			<p>Most major developments in deep learning, such as the Transformer architecture, take only months to find their way into RL. Therefore, staying up to date with major trends in the broader machine learning and deep learning research will help you predict what is upcoming for RL. The blogs we listed in the previous section are a good way to follow these trends.</p>
			<h2 id="_idParaDest-362"><a id="_idTextAnchor403"/>Read open source repos</h2>
			<p>At this point, there are just too many algorithms in RL to explain line by line in a book. So, at some point, you need to develop that literacy and directly read good implementations of <a id="_idIndexMarker1500"/>these algorithms. Here are my suggestions:</p>
			<ul>
				<li>The OpenAI Spinning Up website, <a href="https://spinningup.openai.com/">https://spinningup.openai.com/</a>, and repo, <a href="https://github.com/openai/spinningup">https://github.com/openai/spinningup</a></li>
				<li>OpenAI Baselines: <a href="https://github.com/openai/baselines">https://github.com/openai/baselines</a></li>
				<li>Stable Baselines: <a href="https://github.com/hill-a/stable-baselines">https://github.com/hill-a/stable-baselines</a></li>
				<li>DeepMind OpenSpiel: <a href="https://github.com/deepmind/open_spiel">https://github.com/deepmind/open_spiel</a></li>
				<li>Ray and RLlib: <a href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a></li>
				<li>The <em class="italic">Hands-On</em> <em class="italic">Deep Reinforcement Learning</em> book repo by Maxim Lapan: <a href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On">https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On</a></li>
			</ul>
			<p>In addition to these, many papers now come with their own repos, as we used in some chapters in this book. There is a very nice website, <a href="https://paperswithcode.com/">https://paperswithcode.com/</a>, which you can use to identify such papers.</p>
			<h2 id="_idParaDest-363"><a id="_idTextAnchor404"/>Practice!</h2>
			<p>Regardless of how much you read, you will truly learn only by practicing. So, try to get your hands dirty wherever possible. This could be through reproducing RL papers and algorithms, or even better, doing your own RL projects. The benefit you will get by going into the guts of an implementation cannot be replaced by anything else.</p>
			<p>I hope this set of resources is helpful to you. To be clear, this is a lot to consume. It will take time to go<a id="_idIndexMarker1501"/> over these, so set your targets realistically. Moreover, you will have to be selective about what to read and follow, a habit that you will develop over time.</p>
			<h1 id="_idParaDest-364"><a id="_idTextAnchor405"/>Final words</h1>
			<p>Well, it is time to wrap up. I would like to thank you for investing your time and effort into reading this book. I hope it has been beneficial for you. As a last word, I would like to emphasize that getting good at something takes a long time and there is no limit to how good you can become. Nobody is an expert at everything, even for subdisciplines such as RL or computer vision.  you need to run. Consistency and continuity of your efforts will make the difference, no matter what your goal is. I wish you the best on this journey.</p>
			<h1 id="_idParaDest-365"><a id="_idTextAnchor406"/>References</h1>
			<ul>
				<li>Hofmann, K. (2019). <em class="italic">Reinforcement Learning: Past, Present, and Future Perspectives</em>. Conference on Neural Information Processing Systems, Vancouver, Canada. URL: <a href="https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives">https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives</a></li>
				<li>OpenAI (2018). <em class="italic">OpenAI Five</em>. OpenAI Blog. URL: <a href="https://openai.com/blog/openai-five/">https://openai.com/blog/openai-five/</a></li>
				<li>Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J., &amp; Munos R. (2019). <em class="italic">Recurrent Experience Replay in Distributed Reinforcement Learning</em>. In International Conference on Learning Representations</li>
				<li>Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., &amp; Michalski, M. (2019). <em class="italic">SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</em>. arXiv.org, <a href="http://arxiv.org/abs/1910.06591">http://arxiv.org/abs/1910.06591</a></li>
				<li>Du, J., Futoma, J., &amp; Doshi-Velez, F. (2020). <em class="italic">Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs</em>. arXiv.org, <a href="https://arxiv.org/abs/2006.16210">https://arxiv.org/abs/2006.16210</a></li>
				<li>Shen, P. (2020). <em class="italic">Neural ODE for Differentiable Reinforcement Learning and Optimal Control: Cartpole Problem Revisited</em>. The startup. URL: <a href="https://bit.ly/2RROQi3">https://bit.ly/2RROQi3</a> </li>
				<li>Rosset, C. (2020). <em class="italic">Turing-NLG: A 17-billion-parameter language model by Microsoft</em>. Microsoft Research blog. URL: <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/</a> </li>
				<li>Dulac-Arnold, G., et al. (2020). <em class="italic">An Empirical Investigation of the Challenges of Real-World Reinforcement Learning</em>. arXiv:2003.11881 [Cs]. arXiv.org, <a href="http://arxiv.org/abs/2003.11881">http://arxiv.org/abs/2003.11881</a></li>
				<li>Dulac-Arnold, G., et al. (2019). <em class="italic">Challenges of Real-World Reinforcement Learning</em>. arXiv:1904.12901 [Cs, Stat]. arXiv.org, <a href="http://arxiv.org/abs/1904.12901">http://arxiv.org/abs/1904.12901</a></li>
				<li>Irpan, A. (2018). <em class="italic">Deep Reinforcement Learning Doesn't Work Yet</em>. <a href="http://www.alexirpan.com/2018/02/14/rl-hard.html">http://www.alexirpan.com/2018/02/14/rl-hard.html</a></li>
				<li>Levine, S. (2019). Deep Reinforcement Learning – CS285 Fa19 11/18/19, YouTube, <a href="https://youtu.be/tzieElmtAjs?t=3336">https://youtu.be/tzieElmtAjs?t=3336</a>. Accessed 26 September 2020.</li>
				<li>Hoffmann, K. et al. (2020). <em class="italic">Challenges &amp; Opportunities in Lifelong Reinforcement Learning</em>. ICML 2020. URL: <a href="https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest">https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest</a></li>
			</ul>
		</div>
	</body></html>