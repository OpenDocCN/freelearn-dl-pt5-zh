- en: 6\. LSTMs, GRUs, and Advanced RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study and implement advanced models and variations
    of the plain **Recurrent Neural Network** (**RNN**) that overcome some of RNNs'
    practical drawbacks and are among the best performing deep learning models at
    the moment. We will start by understanding the drawbacks of plain RNNs and see
    how the novel idea of **Long Short-Term Memory** overcomes them. We will then
    see and implement a **Gated Recurrent Unit** based model. We will also work with
    bidirectional and stacked RNNs and explore attention-based models. By the end
    of this chapter, you will have built and assessed the performance of these models
    on a sentiment classification task, observing for yourself the trade-offs in choosing
    the different models.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say you''re working with product reviews for a mobile phone and your
    task is to classify the sentiment in the reviews as being positive or negative.
    You encounter a review that says: *"The phone does not have a great camera, or
    an amazingly vivid display, or an excellent battery life, or great connectivity,
    or other great features that make it the best."* Now, when you read this, you
    can easily identify that the sentiment in the review is negative, despite the
    presence of many positive phrases such as *"excellent battery life"* and *"makes
    it the best"*. You understand that the presence of the term *"not"* right toward
    the beginning of the text negates everything else that comes after.'
  prefs: []
  type: TYPE_NORMAL
- en: Will the models we've created so far be able to identify the sentiment in such
    a case? Probably not, because if your models don't realize that the term *"not"*
    toward the beginning of the sentences is important and needs to be connected strongly
    to the output several terms later, they won't be able to identify the sentiment
    correctly. This, unfortunately, is a major drawback of plain RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at a couple of deep learning approaches for
    dealing with sequences, that is, one-dimensional convolutions and RNNs. We saw
    that RNNs are extremely powerful models that provide us with a great amount of
    flexibility to handle different sequence tasks. The plain RNNs that we saw have
    been subject to plenty of research. Now, we will look at some approaches that
    have been built on top of RNNs to create new, powerful models that overcome the
    drawbacks of RNNs. We will look at LSTM, GRUs, stacked and bidirectional LSTMs,
    and attention-based models. We will apply these models to a sentiment classification
    task, thereby bringing together the concepts discussed in *Chapter 4, Deep Learning
    for Text – Embeddings*, and *Chapter 5, Deep Learning for Sequences*, as well.
  prefs: []
  type: TYPE_NORMAL
- en: Long-Range Dependence/Influence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sample mobile phone review we saw in the previous section was an example
    of a long-range dependence/influence – where a term/value in a sequence has an
    influence on the assessment of a lot of the subsequent terms/values. Consider
    the following example, where you need to fill in the blank with a missing country
    name: *"After a top German university granted her admission for her Masters in
    Dentistry, Hina was extremely excited to start this new phase of her career with
    international exposure and couldn''t wait till the end of the month to book her
    flight to ____."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct answer, of course, is **Germany**, arriving at which would require
    you to understand the importance of the term "German", which appears at the beginning
    of the sentence, on the outcome at the end of the sentence. This is another example
    of long-range dependence. The following figure shows the long-range dependence
    of the answer, "*Germany*", on the term "*German*" appearing early in the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Long-range dependence'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: Long-range dependence'
  prefs: []
  type: TYPE_NORMAL
- en: To get the best outcome, we need to be able to handle long-range dependencies.
    In the context of deep learning models and RNNs, this would mean that learning
    (or the backpropagation of errors) needs to happen smoothly and effectively over
    many time steps. This is easier said than done, primarily because of the vanishing
    gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Vanishing Gradient Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest challenges while training standard feedforward deep neural
    networks is the vanishing gradient problem (as discussed in *Chapter 2, Neural
    Networks*). As the model gets more and more layers, backpropagating the errors
    all the way back to the initial layers becomes increasingly difficult. Layers
    close to the output will be "learning"/updated at a good pace, but by the time
    the error propagates to the initial layers, its value will have diminished greatly
    and have little or no effect on the parameters for the initial layers.
  prefs: []
  type: TYPE_NORMAL
- en: With RNNs, this problem is further compounded, as the parameters need to be
    updated not only along the depth but also for the time steps. If we have one hundred
    time steps in the inputs (which isn't uncommon, especially when working with text),
    the network needs to propagate the error (calculated at the 100th time step) all
    the way back to the first time step. For plain RNNs, this task can be a bit too
    much to handle. This is where RNN variants can come in useful.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Another practical issue with training deep networks is the exploding gradient
    problem, where the gradient values get very high – too high to be represented
    by the system. This issue has a rather simple workaround called "**Gradient Clipping**",
    which means capping the values of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence Models for Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 5, Deep Learning for Sequences*, we learned that RNNs perform extremely
    well on sequence-modeling tasks and provide high performance on text-related tasks.
    In this chapter, we will use plain RNNs and variants of RNNs on a sentiment classification
    task: processing the input sequence and predicting whether the sentiment is positive
    or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the IMDb reviews dataset for this task. The dataset contains 50,000
    movie reviews, along with their sentiment – 25,000 highly polar movie reviews
    for training and 25,000 for testing. A few reasons for using this dataset are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is very conveniently available to load Keras (tokenized version) with a single
    command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is commonly used for testing new approaches/models. This should
    help you compare your results with other approaches easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longer sequences in the data (IMDb reviews can get very long) help us assess
    the differences between the variants of RNNs better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started by building our first model using plain RNNs and then benchmark
    the future model performances against that of the plain RNN. Let's start with
    data preprocessing and formatting the model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you work on all the exercises and example codes in this chapter
    in the same Jupyter Notebook. Note that the code in this section will load the
    dataset. To ensure all exercises and example codes that follow work, please ensure
    that you do not skip this section. You can access the complete code for the exercises
    at [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, you need to start a new Jupyter Notebook and import the `imdb` module
    from the Keras datasets. Note that unless mentioned otherwise, the code and exercises
    for the rest of this chapter should continue in the same Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With the module imported, importing the dataset (tokenized and separated into
    train and test sets) is as easy as running `imdb.load_data`. The only parameter
    we need to provide is the vocabulary size we wish to use. Recall that the vocabulary
    size is the total number of unique terms we wish to consider for the modeling
    process. When we specify a vocabulary size, *V*, we work with the top *V* terms
    in the data. Here, we will specify a vocabulary size of 8,000 for our models (an
    arbitrary choice; you can modify this as desired) and load the data using the
    `load_data` method, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s inspect the `X_train` variable to see what we are working with. Let''s
    print the type of it and the type of constituting elements, and also have a look
    at one of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `X_train` variable is a `numpy` array – each element of the array is a list
    representing the text for a single review. The terms in the text are present as
    numerical tokens instead of raw tokens. This is a very convenient format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to define an upper limit on the length of the sequences that
    we''ll work with and limit all sequences to the defined maximum length. We''ll
    use `200` – an arbitrary choice, in this case – to quickly get started with the
    model-building process`200` steps so that the networks don''t get too heavy, and
    because `200` time steps are sufficient to demonstrate the different RNN approaches.
    Let''s define the `maxlen` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to get all our sequences to the same length using the `pad_sequences`
    utility from Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*****Ideally, we would analyze the lengths of the sequences and identify one
    that covers most of the reviews. We''ll perform these steps in the activity at
    the end of the chapter, in which we''ll use ideas from not only the current chapter,
    but also from *Chapter 4, Deep Learning for Text – Embeddings*, and *Chapter 5,
    Deep Learning for Sequences*, bringing this all together in a single activity.'
  prefs: []
  type: TYPE_NORMAL
- en: Staging and Preprocessing Our Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `pad_sequences` utility from the `sequences` module in Keras helps us in
    getting all the sequences to a specified length. If the input sequence is shorter
    than the specified length, the utility pads the sequence with a reserved token
    (indicating a blank/missing). If the input sequence is longer than the specified
    length, the utility truncates the sequence to limit it. In the following example,
    we will apply the `pad_sequences` utility to our test and train datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand the result of the steps, let''s see the output for a particular
    instance in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The processed instance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Result of pad_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: Result of pad_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are plenty of `0`s at the beginning of the result. As
    you may have inferred, this is the padding that's done by the `pad_sequence` utility
    because the input sequence was shorter than `200`. Padding at the beginning of
    the sequence is the default behavior of the utility. For a sequence that is less
    than the specified limit, the truncation, by default, is done from the left –
    that is, the last `200` terms would be retained. All instances in the output now
    have `200` terms. The dataset is now ready for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The default behavior of the utility is to pad the beginning of the sequence
    and truncate from the left. These can be important hyperparameters. If you believe
    that the first few terms are the most important for the prediction, you may want
    to truncate the last terms by specifying the "`truncating`" parameter as "`post`".
    Similarly, to have padding toward the end of the sequence, you can set "`padding`"
    to "`post`".
  prefs: []
  type: TYPE_NORMAL
- en: The Embedding Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 4, Deep Learning for Text – Embeddings*, we discussed that we can't
    feed text directly into a neural network, and therefore need good representations.
    We discussed that embeddings (low-dimensional, dense vectors) are a great way
    of representing text. To pass the embeddings into the neural network's layers,
    we need to employ the embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functionality of the embedding layer is two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: For any input term, perform a lookup and return its word embedding/vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, learn these word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The part about looking up is straightforward – the word embeddings are stored
    as a matrix of the `V × D` dimensionality, where `V` is the vocabulary size (the
    number of unique terms considered) and `D` is the length/dimensionality of each
    vector. The following figure illustrates the embedding layer. The input term,
    "`life`", is passed to the embedding layer, which performs a lookup and returns
    the corresponding vector of length `D`. This vector, which is the representation
    for the term `life`, is fed to the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'What do we mean by learning these embeddings while training the predictive
    model? Aren''t word embeddings learned by using an algorithm such as `word2vec`,
    which tries to predict the center word based on context terms (remember the CBOW
    architecture we discussed in *Chapter 4, Deep Learning for Text – Embeddings*)?
    Well, yes and no:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Embedding layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Embedding layer'
  prefs: []
  type: TYPE_NORMAL
- en: The `word2vec` approach had the objective of learning a representation that
    captures the meaning of the term. Therefore, predicting the target word based
    on context was a perfect formulation for the objective. In our case, the objective
    is different – we wish to learn representations that help us best predict the
    sentiment in the text. It makes sense, then, to learn the representation that
    works explicitly toward our objective.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer is always the first layer in the model. You can follow it
    up with any architecture of your choice (RNNs, in our case). We randomly initialize
    the vectors, essentially the weights in the embedding layer. While the model trains,
    the weights are updated in a way that predicts the outcome in a better way. The
    weights learned, and therefore the word vectors, are then tuned to the task. This
    is a very useful step – why use generic representations when you can tune them
    to your task?
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding layer in Keras has two main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_dim` : The number of unique terms in the vocabulary, that is, the vocabulary
    size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_dim` : The dimension of the embedding/the length of the word vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `input_dim` parameter needs to be set to the vocabulary size being employed.
    The `output_dim` parameter specifies the length of the embedding vector for each
    term.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the embedding layer in Keras also allows you to use your own specified
    weight matrix in the embedding layer. This means you can use pre-trained embeddings
    (such as `GloVe`, or even embeddings you trained in a different model) in the
    embedding layer. The `GloVe` model has been trained on billions of tokens and
    it could be useful to leverage this powerful general representation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you use pre-trained embeddings, you also have the option to make them trainable
    in your model – essentially, use `GloVe` embeddings as a starting point and fine-tune
    them for your task. This is a great example of transfer learning for text.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Plain RNN Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the next exercise, we will build our first model for the sentiment classification
    task using plain RNNs. The model architecture we''ll use is depicted in the following
    figure, which demonstrates how the model would process an input sentence "`Life
    is good`", with the term "`Life`" coming in at time step `T=0` and "`good`" appearing
    at time step `T=2`. The model will process the inputs one by one, using the embedding
    layer to look up the word embeddings that will be passed to the hidden layers.
    The classification will be done when the final term, "`good`", is processed at
    time step `T=2`. We''ll use Keras to build and train our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Architecture using an embedding layer and RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4: Architecture using an embedding layer and RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.01: Building and Training an RNN Model for Sentiment Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will build and train an RNN model for sentiment classification.
    Initially, we will define the architecture for the recurrent and prediction layers,
    and we will assess the model''s performance on the test data. We will add the
    embedding layer and some dropout and complete the model definition by adding the
    RNN layer, dropout, and a dense layer to finish. Then, we''ll check the accuracy
    of the predictions on the test data to assess how well the model generalizes.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by setting the seed for `numpy` and `tensorflow` random number
    generation, to get, to the best extent possible, reproducible results. We''ll
    import `numpy` and `tensorflow` and set the seed using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Even though we have set the seeds for `numpy` and `tensorflow` to achieve reproducible
    results, there are a lot more causes for variation, owing to which you may get
    a result that's different from ours. This applies to all the models we'll use
    from now on. While the values you see may be different, the output you see should
    largely agree with ours. If the model's performance is very different, you may
    want to tweak the number of epochs – the reason for this being that the weights
    in neural networks are initialized randomly, so the starting points for you and
    us could be slightly different, and we may reach a similar position when training
    a different number of epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s continue by importing all the necessary packages and layers and
    initializing a sequential model named `model_rnn` using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to specify the embedding layer. The `input_dim` parameter needs
    to be set to the `vocab_size` variable. For the `output_dim` parameter, we''ll
    choose `32`. Recall from *Chapter 4, Deep Learning for Text – Embeddings*, that
    this is a hyperparameter and you may want to experiment with this to get better
    results. Let''s specify the embedding layer and use dropout (to minimize overfitting)
    using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the dropout employed here is `SpatialDropout1D` – this version performs
    the same function as regular dropout layer, but instead of dropping individual
    elements, it drops entire one-dimensional feature maps (vectors, in our case).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add a `SimpleRNN` layer with `32` neurons to the model (chosen arbitrarily;
    another hyperparameter to tune):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, add a dropout layer with `40%` dropout (again, an arbitrary choice):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a dense layer with a `sigmoid` activation function to complete the model
    architecture. This is the output layer that makes the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model and view the model summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5: Summary of the plain RNN model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.5: Summary of the plain RNN model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that there are `258,113` parameters, most of which are present in
    the embedding layer. The reason for this is that the word embeddings are being
    learned during the training – so we're learning the embedding matrix, which is
    of dimensionality `vocab_size(8000) × output_dim(32)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's proceed and train the model (with the hyperparameters that we've observed
    to provide the best result with this data and architecture).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model on the train data with a batch size of `128` for `10` epochs
    (both of these are hyperparameters that you can tune). Use a validation split
    of `0.2` – monitoring this will give us a sense of the model performance on unseen
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The training output for the last five epochs will be as follows. Depending
    on your system configuration, this step could take more or less time than it did
    here for us:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6: Training the plain RNN model – the final five epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.6: Training the plain RNN model – the final five epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the training output, we can see that the validation accuracy goes up to
    about 86%. Let's make predictions on the test set and check the performance of
    the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the test data using the `predict_classes` method of the
    model and use the `accuracy_score` method from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy of the test is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the model does a decent job. We used a simple architecture with
    `32` neurons and used a vocabulary size of just `8000`. Tweaking these and other
    hyperparameters may get you better results and you are encouraged to do so.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we have seen how to build an RNN-based model for text. We
    saw how an embedding layer can be used to derive word vectors for the task at
    hand. These word vectors are the representations for each incoming term, which
    are passed to the RNN layer. We have seen that even a simple architecture can
    give us good results. Now, let's discuss how this model can be used to make predictions
    on new, unseen reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions on Unseen Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've trained your model on some data and assessed its performance
    on the test data, the next thing is to learn how to use this model to predict
    the sentiment for new data. That is the purpose of the model, after all – being
    able to predict the sentiment for data previously unseen by the model. Essentially,
    for any new review in the form of raw text, we should be able to classify its
    sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: The key step for this would be to create a process/pipeline that converts the
    raw text into a format the predictive model understands. This would mean that
    the new text would need to undergo exactly the same preprocessing steps that were
    performed on the text data that was used to train the model. The function for
    preprocessing needs to return formatted text for any input raw text. The complexity
    of this function depends on the steps performed on the train data. If tokenization
    was the only preprocessing step performed, then the function only needs to perform
    tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model (`model_rnn`) was trained on IMDb reviews that were tokenized, had
    their case lowered, had punctuation removed, had a defined vocabulary size, and
    were converted into a sequence of indices. Our function/pipeline for preparing
    data for the RNN model needs to perform the same steps. Let''s work toward creating
    our own function. To begin, let''s create a new variable called "`inp_review`"
    containing the text "*An excellent movie*" using the following code. This is the
    variable containing the raw review text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The sentiment in the text is positive. If the model is working well enough,
    it should predict the sentiment as positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must tokenize this text into its constituent terms, normalize its
    case, and remove punctuation. To do so, we need to import the `text_to_word_sequence`
    utility from Keras using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To check if it works as we expect, we can apply this to the `inp_review` variable,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenized sentence will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that it works just as expected – the case has been normalized, the
    sentences have been tokenized, and punctuation has been removed from the input
    text. The next step would be to use a defined vocabulary for the data. This would
    require using the same vocabulary that was used by TensorFlow when we loaded the
    data. The vocabulary and the term-to-index mapping can be loaded using the `get_word_index`
    method from the `imdb` module (that we employed to load the code). The following
    code can be used to load the vocabulary into a dictionary named `word_map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This dictionary contains the mapping for about 88.6 K terms that were available
    in the raw reviews data. We loaded the data with a vocabulary size of `8000`,
    thereby using the first `8000` indices from the mapping. Let''s create our mapping
    with limited vocabulary so that we can use the same terms/indices that the training
    data used. We''ll limit the mapping to `8000` terms by sorting the `word_map`
    variable on the index and picking the first `8000` terms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The vocab map will be a dictionary containing the term for index mapping for
    the `8000` terms in the vocabulary. Using this mapping, we''ll convert the tokenized
    sentence into a sequence of term indices by performing a lookup for each term
    and returning the corresponding index. Using the following code, we''ll define
    a function that accepts raw text, applies the `text_to_word_sequence` utility
    to it, performs a lookup from `vocab_map`, and returns the corresponding sequence
    of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can apply this function to the `inp_review` variable, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the sequence of term indices corresponding to the raw text. Note that
    the data is now in the same format as the IMDb data we loaded. This sequence of
    indices can be fed to the RNN model (using the `predict_classes` method) to classify
    the sentiment, as shown in the following code. If the model is working well enough,
    it should predict the sentiment as positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output prediction is `1` (positive), just as we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the function to another raw text review and supply it to the model
    for prediction. Let''s update the `inp_review` variable so that it contains the
    text "`Don''t watch this movie – poor acting, poor script, bad direction.`" The
    sentiment in the review is negative. We expect the model to classify it as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply our preprocessing function to the `inp_review` variable and make
    a prediction using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction is `0`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The predicted sentiment is negative, just as we would expect the model to behave.
  prefs: []
  type: TYPE_NORMAL
- en: We applied this pipeline in the form of a function on a single review, but you
    can very easily apply this to a whole collection of reviews to make predictions
    using the model. You are now ready to classify the sentiment of any new review
    using the RNN model we trained.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline we built here is specifically for this dataset and model. This
    is not a generic processing function that you can utilize for predictions from
    any model. The vocabulary used, the cleanup that was done, the patterns the model
    learned – these were all specific to this task and dataset. For any other model,
    you need to create your pipeline accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The higher-level approach can be employed to make processing pipelines for other
    models too. Depending on the data, the preprocessing steps, and setting up where
    the model will be deployed, the pipeline can vary. All these factors also affect
    the steps you may want to include in the model building process. Therefore, we
    encourage to you to start thinking about these aspects right away when you begin
    the whole modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to make predictions on unseen data using the trained RNN model, thereby
    giving us an understanding of the end-to-end process. In the next section, we'll
    begin working with variants of RNNs. The implementation-related ideas we've discussed
    so far are applicable to all the subsequent models.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs, GRUs, and Other Variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind plain RNNs is very powerful and the architecture has shown tremendous
    promise. Due to this, researchers have experimented with the architecture of RNNs
    to find ways to overcome the one major drawback (the vanishing gradient problem)
    and exploit the power of RNNs. This led to the development of LSTMs and GRUs,
    which have now practically replaced RNNs. Indeed, these days, when we refer to
    RNNs, we usually refer to LSTMs, GRUs, or their variants.
  prefs: []
  type: TYPE_NORMAL
- en: This is because these variants are designed specifically to handle the vanishing
    gradient problem and learn long-range dependencies. Both approaches have outperformed
    plain RNNs significantly in most tasks around sequence modeling, and the difference
    is especially higher for long sequences. The paper titled *Learning Phrase Representations
    using RNN Encoder-Decoder for Statistical Machine Translation* (available at [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078))
    performs an empirical analysis of the performance of plain RNNs, LSTMs, and GRUs.
    How have these approaches overcome the drawbacks of plain RNNS? We'll understand
    this in the next section, where we'll discuss LSTMs in detail.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's think about this for a moment. Knowing the architecture of the plain RNN,
    how can we tweak it, or what can be done differently to capture long-range influences?
    We can't add more layers; that would be counterproductive for sure, as every added
    layer would compound the problem. One idea (available at [https://pubmed.ncbi.nlm.nih.gov/9377276](https://pubmed.ncbi.nlm.nih.gov/9377276)),
    proposed in 1997 by Sepp Hochreiter and Jurgen Schmidhuber, is to use an explicit
    value (state) that does not pass through activations. If we had a cell (corresponding
    to a neuron for plain RNNs) value flowing freely and not through activations,
    this value could potentially help us model long-range dependence. This is the
    first key difference in an LSTM – an explicit cell state.
  prefs: []
  type: TYPE_NORMAL
- en: The cell state can be thought of as a way to identify and store information
    over multiple time steps. Essentially, we are identifying some value as the long-term
    memory of the network that helps us predict the output better and taking care
    to retain this value as long as required.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we regulate the flow of this cell state? How do we decide when to
    update the value and by how much? For this, Hochreiter and Schmidhuber proposed
    the use of *gating mechanisms* as a way to regulate how and when to update the
    value of the cell state. This is the other key difference in an LSTM. The freely
    flowing cell state, together with the regulatory mechanisms, allow the LSTM to
    perform extremely well on longer sequences and provide it with all its predictive
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A detailed treatment of the inner workings of the LSTM and the associated math
    is beyond the scope of this book. For those interested in reading further, [https://packt.live/3gL42Ib](https://packt.live/3gL42Ib)
    is a good reference that provides a good visual understanding of LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the intuition behind the working of the LSTM. The following
    figure shows the internals of the LSTM cell. Apart from the usual outputs, that
    is, the hidden state, the LSTM cell also outputs a cell "state". The hidden state
    holds the short-term memory, while the cell state holds the long-term memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: The LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.7: The LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: This view of the internals can be intimidating, which is why we'll look at a
    more abstracted view, as can be seen in *Figure 6.8*. The first thing to notice
    is that the only operations that take place on the cell state are two linear operations
    – a multiplication and an addition. The cell state does not pass through any activation
    function. This is why we said that the cell state flows freely. This free-flow
    setup is also called a "Constant Error Carousel" – a moniker you *don't* need
    to remember.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `FORGET` block is multiplied by the cell state. Because the
    output of this block is between `0` and `1` (modeled by the sigmoid activation),
    a multiplication of this with the cell state will regulate how much of the previous
    cell state is to be forgotten. If the `FORGET` block outputs `0`, the previous
    cell state is completely forgotten; while for output `1`, the cell state is completely
    retained. Note that the inputs to the `FORGET` gate are the output from the hidden
    layer from the previous time step (`h`t-1) and the new input at the present time
    step, `x`t (for a layer deep in the network, this could be the output from the
    previous hidden layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: Abstracted view of the LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.8: Abstracted view of the LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see that after the cell state is multiplied
    by the `FORGET` block's result, the next decision is how much to update the cell
    state by. This comes from the `UPDATE` block's output, which is added (note the
    plus sign) to the processed cell state. This way, the processed cell state is
    updated. That's all the operations that are performed on the previous cell state,
    `(C`t-1`)`, to give us the new cell state, `(C`t`)`, as an output. This is how
    the long-term memory of the cell is regulated. The cell also needs to update the
    hidden state. This operation takes place in the `OUTPUT` block and is pretty much
    the same as the update in a plain RNN. The only difference is that the explicit
    cell state is multiplied by the output from the sigmoid to form the final hidden
    state, `h`t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the individual blocks/gates, let''s see them marked
    on the following detailed figure. This should clarify how these gating mechanisms
    come together to regulate the flow of information in an LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: The LSTM cell explained'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9: The LSTM cell explained'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this example more concrete, let''s take a look at the following figure
    and understand how the cell state is updated. We can assume the previous cell
    state, `(C`t-1`)`, was `5`. How much of this value should be propagated is decided
    by the output of the `FORGET` gate. The output value of the `FORGET` gate is multiplied
    by the previous cell state, `C`t-1\. In this case, the output of the forget block
    is `0.5`, resulting in `2.5` as the processed cell state being passed. This value
    (`2.5`) then encounters the addition from the `UPDATE` gate. Since the `UPDATE`
    gate output value of `-0.8`, the result of the addition is `1.7`. This is the
    final, updated cell state, `C`t, that is passed to the next time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10: LSTM cell state update example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.10: LSTM cell state update example'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters in an LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs are built on plain RNNs. If you simplified the LSTM and removed all the
    gates, retaining only the tanh function for the hidden state update, you would
    have a plain RNN. The number of activations that the information – the new input
    data at time `t` and the previous hidden state at time `t-1` (`x`t and `h`t-1)
    – passes through in an LSTM is four times the number that it passes through in
    a plain RNN. The activations are applied once in the forget gate, twice in the
    update gate, and once in the output gate. The number of weights/parameters in
    an LSTM is, therefore, four times the number of parameters in a plain RNN.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 5*, *Deep Learning For Sequences,* in the section titled *Parameters
    in an RNN*, we calculated the number of parameters in a plain RNN and saw that
    we already have a quite a few parameters to work with (`n`2 `+ nk + nm`, where
    `n` is the number of neurons in the hidden layer, `m` is the number of inputs,
    and `k` is the dimension of the output layer). With LSTMs, we saw that the number
    is four times this. Needless to say, we have a lot of parameters in an LSTM, and
    that isn't necessarily a good thing, especially when working with smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.02: LSTM-Based Sentiment Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will build a simple LSTM-based model to predict sentiment
    on our data. We will continue with the same setup we used previously (that is,
    the number of cells, embedding dimensions, dropout, and so on). Thus, you must
    continue this exercise in the same Jupyter Notebook. Follow these steps to complete
    this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the LSTM layer from Keras `layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add a 40% spatial dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an LSTM layer with `32` cells:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the dropout (`40%` dropout) and dense layers, compile the model, and print
    the model summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11: Summary of the LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.11: Summary of the LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see from the model summary that the number of parameters in the LSTM
    layer is `8320`. A quick check can confirm that this is exactly four times the
    number of parameters in the plain RNN layer we saw in *Exercise 6.01,* *Building
    and Training an RNN Model for Sentiment Classification*, which is in line with
    our expectations. Next, let's fit the model on the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit on the training data for `5` epochs (this gives us the best result for
    the model) with a batch size of `128`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the training process is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12: LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.12: LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that training the LSTM took much longer than it does with plain RNNs.
    Again, considering the architecture of the LSTM and the sheer number of parameters,
    this was expected. Also, note that the validation accuracy is significantly higher
    than that of the plain RNN. Let's check the performance on the test data in terms
    of the accuracy score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the test set and print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy is printed out as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy we got (87%) is a significant improvement from the accuracy we
    got using plain RNNs (85.1%). It looks like the extra parameters and the extra
    predictive power from the cell state came in handy for our task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we saw how we can employ LSTMs for sentiment classification
    of text. The training time was significantly higher, and the number of parameters
    is higher too. But in the end, even this simple architecture (without any hyperparameter
    tuning) gave better results than the plain RNN. You are encouraged to tune the
    hyperparameters further to get the most out of the powerful LSTM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM versus Plain RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw that LSTMs are built on top of plain RNNs, with the primary goal of
    addressing the vanishing gradient problem to enable modeling long-range dependencies.
    Looking at the following figure tells us that a plain RNN passes only the hidden
    state (the short-term memory), whereas an LSTM passes the hidden state as well
    as the explicit cell state (the long-term memory), giving it more power. So, when
    the term "`good`" is being processed in the LSTM, the recurrent layer also passes
    the cell states holding the long-term memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13: Plain RNNs (left) and LSTMs (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.13: Plain RNNs (left) and LSTMs (right)'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, does this mean that you always need an LSTM? The answer to this
    question, as with most questions in data science and especially deep learning,
    is, "it depends". To understand these considerations, we need to understand the
    benefits and drawbacks of LSTMs compared to plain RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Benefits of LSTMs:**'
  prefs: []
  type: TYPE_NORMAL
- en: More powerful, as it uses more parameters and an explicit cell state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models long-range dependencies better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drawbacks of LSTMs:**'
  prefs: []
  type: TYPE_NORMAL
- en: Many more parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takes more time to train
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More prone to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have long sequences to work with, LSTM would be a good choice. If you
    have a small dataset and the sequences you are dealing with are short (<10), then
    you're probably okay to use a plain RNN, owing to there being a lower number of
    parameters (although you could also try LSTMs, making sure to use regularization
    to avoid overfitting). A larger dataset with long sequences would probably extract
    the most out of powerful models such as LSTMs. Note that training LSTMs is computationally
    expensive and time-consuming, so if you have an extremely large dataset, training
    LSTMs may not be the most practical approach. Of course, all these statements
    should serve merely as guidance – the best approach would be what works best for
    your data and your task.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrence Units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw that LSTMs have a lot of parameters and seem
    much more complex than the regular RNN. You may be wondering, are all these apparent
    complications really necessary? Can the LSTM be simplified a little without it
    losing significant predictive power? Researchers wondered the same for a while,
    and in 2014, Kyunghyun Cho and their team proposed the GRU as an alternative to
    LSTMs in their paper ([https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078))
    on machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: GRUs are simplified forms of LSTMs and aim at reducing the number of parameters
    while retaining the power of the LSTM. In tasks around speech modeling and language
    modeling, GRUs provide the same performance as LSTMs, but with fewer parameters
    and faster training times.
  prefs: []
  type: TYPE_NORMAL
- en: 'One major simplification done in a GRU is the omission of the explicit cell
    state. This sounds counterintuitive considering that the freely flowing cell state
    was what gave the LSTM its power, right? What really gave LSTMs all that power
    was the freely flowing nature of the cell state and not the cell state itself?
    Indeed, if the cell state were also subject to activations, LSTMs probably wouldn''t
    have had the success they did:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14: Gated Recurrent Unit'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.14: Gated Recurrent Unit'
  prefs: []
  type: TYPE_NORMAL
- en: So, the freely flowing values is the key differentiating idea. GRUs retain this
    idea, by allowing the hidden state to flow freely. Let's look at the preceding
    figure to understand what this means. GRUs allow the hidden state to pass through
    freely. Another way to look at this is that GRUs effectively bring the idea of
    the cell state (as in LSTMs) to the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: We still need to regulate the flow of the hidden state, though, so we still
    have gates. GRUs combine the forget gate and update gate into a single update
    gate. To understand the motivation behind this, consider this – if we forget a
    cell state, and don't update it, what are we really doing? Maybe there is merit
    in having a single update operation. This is the second major difference in the
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of these two changes, GRUs have the data pass through three activations
    instead of four, as in LSTMs, reducing the number of parameters. While GRUs still
    have three times the number of parameters of a plain RNN, these have 75% of the
    parameters of LSTMs, and that is a welcome change. We still have information flowing
    freely through the network and this should allow us to model long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how a GRU-based model performs on our task of sentiment classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.03: GRU-Based Sentiment Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will build a simple GRU-based model to predict sentiments
    in our data. We will continue with the same setup that we used previously (that
    is, the number of cells, embedding dimensions, dropout, and so on). Using GRUs
    instead of LSTMs in the model is as simple as replacing "`LSTM`" with "`GRU`"
    when adding the layer. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `GRU` layer from Keras `layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add 40% spatial dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a GRU layer with 32 cells. Set the `reset_after` parameter to `False` (this
    is a minor TensorFlow 2 implementation detail in order to maintain consistency
    with the implementation of plain RNNs and LSTMs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the dropout (40%) and dense layers, compile the model, and print the model
    summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.15: Summary of the GRU model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.15: Summary of the GRU model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the summary of the GRU model, we can see that the number of parameters
    in the GRU layer is `6240`. You can check that this is exactly three times the
    number of parameters in the plain RNN layer we saw in *Exercise 6.01,* *Building
    and Training an RNN Model for Sentiment Classification*, and `0.75` times the
    parameters of the LSTM layer we saw in *Exercise 6.02,* *LSTM-Based Sentiment
    Classification Model* – again, this is in line with our expectations. Next, let's
    fit the model on the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit on the training data for four epochs (which gives us the best result):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the training process is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.16: GRU training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.16: GRU training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that training the GRUs also took much longer than plain RNNs but was
    faster than LSTMs. The validation accuracy is better than the plain RNN and seems
    close to that of the LSTM. Let's see how the model fares on the test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the test set and print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy is printed out as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the accuracy of the GRU model (87.15%) is very similar to that
    of the LSTM (87%) and is higher than the plain RNN. This is an important point
    – GRUs are simplifications of LSTMs that aim to provide similar accuracy with
    fewer parameters. Our exercise here shows this is true.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we saw how we can employ GRUs for the sentiment classification
    of text. The training time was slightly lower than the LSTM model and the number
    of parameters is lower. Even this simple architecture (without any hyperparameter
    tuning) gave better results than the plain RNN model and gave results similar
    to the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM versus GRU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, which one should you choose? The LSTM has more parameters and an explicit
    cell state designed to store long-term memory. The GRU has fewer parameters, which
    means faster training, and also has a free-flowing cell state to allow it to model
    long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: An empirical evaluation (available at [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555))
    by Junyoung Chung, Yoshua Bengio, and their team in 2014 on music-modeling and
    speech-modeling tasks showed that both LSTMs and GRUs are markedly superior to
    plain RNNs. They also found that GRUs are on par with LSTMs in terms of performance.
    They remarked that tuning hyperparameters such as layer size is probably more
    important than choosing between LSTM and GRU.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, Gail Weiss, Yoav Goldberg, and their team demonstrated and concluded
    that LSTMs outperform GRUs in tasks that require unbounded counting, that is,
    those that need to handle sequences of an arbitrary length. The Google Brain team,
    in 2018, also showed that the performance of LSTMs is superior to GRUs when it
    comes to machine translation. This leads us to think that the extra power that
    LSTMs bring may be very useful in certain applications.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RNN models we've just looked at – LSTMs, GRUs – are powerful indeed and
    provide extremely good results when it comes to sequence-processing tasks. Now,
    let's discuss how to make them even more powerful, and the methods that yield
    the amazing successes in deep learning that you have been hearing about.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with the idea of bidirectional RNNs. The idea applies to all variants
    of RNNs, including, but not limited to, LSTMs and GRUs. Bidirectional RNNs process
    the sequence in both directions, allowing the network to have both backward and
    forward information about the sequence, providing it with a much richer context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: Bidirectional LSTM'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.17: Bidirectional LSTM'
  prefs: []
  type: TYPE_NORMAL
- en: The bidirectional model essentially employs two RNNs in parallel – one as the
    "**forward layer**" and the other as the "**backward layer**". As shown in the
    preceding figure, the forward layer processes the sequence in the order of its
    elements. For the sentence, "*Life is good*", the forward layer will process the
    term "*Life*" first, followed by "*is*", followed by "*good*" – no different from
    the usual RNN layer. The backward layer reverses this order – it processes "good"
    first, followed by "is", followed by "Life". At each step, the states of the forward
    and the backward layers are concatenated to form the output.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of tasks benefit the most from this architecture? Looking at both
    sides of the context helps resolve any ambiguity about the term at hand. When
    we read a statement such as "*The stars*", we're not sure as to what "*stars*"
    we're reading about – is it the stars in the sky or movie stars? But when we also
    see the terms coming later in the sequence and read "*The stars at the movie premiere*",
    we're confident that this sentence is about movie stars. The tasks that can benefit
    the most from such a setup are machine translation, parts-of-speech tagging, named
    entity recognition, and word prediction tasks, to list a few. Bidirectional RNNs
    show performance gains for general text classification tasks as well. Let's apply
    a bidirectional LSTM-based model to our sentiment classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.04: Bidirectional LSTM-Based Sentiment Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will use bidirectional LSTMs to predict sentiment on our
    data. We''ll be using the bidirectional wrapper from Keras to create bidirectional
    layers on LSTMs (you could create a bidirectional GRU model by simply replacing
    `LSTM` with `GRU` in the wrapper). Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Bidirectional` layer from Keras `layers`. This layer is essentially
    a wrapper you can use around other RNNs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add a 40% spatial dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a `Bidirectional` wrapper to an LSTM layer with `32` cells:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the dropout (40%) and dense layers, compile the model, and print the model
    summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.18: Summary of the bidirectional LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.18: Summary of the bidirectional LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note the parameters of the model shown in the preceding screenshot. Not surprisingly,
    the bidirectional LSTM layer has `16640` parameters – twice the number of parameters
    that the LSTM layer (`8320` parameters) had in *Exercise 6.02, LSTM-Based Sentiment
    Classification Model*. This is eight times the parameters of the plain RNN. Next,
    let's fit the model on the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training data for four epochs with a batch size of `128`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from training is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19: Bidirectional LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.19: Bidirectional LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that, as we expect, training bidirectional LSTMs takes much longer than
    regular LSTMs, and several times longer than plain RNNs. The validation accuracy
    seems to be closer to the LSTM's accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the test set and print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy we received here (87.7%) is a slight improvement over the LSTM
    model's accuracy, which was 87%. Again, you can tune the hyperparameters even
    further to extract the most out of this powerful architecture. Note that we had
    twice the number of parameters compared to the LSTM model, and eight times the
    parameters of the plain RNN. Working with a large dataset may make the performance
    differences bigger.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's look at another approach we can follow to extract more power from
    RNNs. In all the models we've looked at in this chapter, we've used a single layer
    for the RNN layer (plain RNN, LSTM, or GRU). Going deeper, that is, adding more
    layers, has typically helped us for feedforward networks so that we can learn
    more complex patterns/features in the deeper layers. There is merit in trying
    this idea for recurrent networks. Indeed, stacked RNNs do seem to give us more
    predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a simple two-layer stacked LSTM model. Stacking
    RNNs simply means feeding the output of one RNN layer to another RNN layer. The
    RNN layers can output sequences (that is, output at each time step) and these
    can be fed, like any input sequence, into the subsequent RNN layer. In terms of
    implementation through code, stacking RNNs is as simple as returning sequences
    from one layer, and providing this as input to the next RNN layer, that is, the
    immediate next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20: Two-layer stacked RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.20: Two-layer stacked RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the stacked RNN (LSTM) in action by using it on our sentiment classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.05: Stacked LSTM-Based Sentiment Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will "go deeper" into the RNN architecture by stacking
    two LSTM layers to predict sentiment in our data. We will continue with the same
    setup that we used in the previous exercises (the number of cells, embedding dimensions,
    dropout, and so on) for the other layers. Follow these steps to complete this
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add 40% spatial dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an LSTM layer with `32` cells. Make sure to specify `return_sequences`
    as `True` in the LSTM layer. This will return the output of the LSTM at each time
    step, which can then be passed to the next LSTM layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add another LSTM layer with `32` cells. This time, you don''t need to return
    sequences. You can either specify the `return_sequences` option as `False` or
    skip it altogether (the default value is `False`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the dropout (50% dropout; this is higher since we''re building a more complex
    model) and dense layers, compile the model, and print the model summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.21: Summary of the stacked LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.21: Summary of the stacked LSTM model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the stacked LSTM model has the same number of parameters as the bidirectional
    model. Let's fit the model on the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model on the training data for four epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from training is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22: Stacked LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_06_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.22: Stacked LSTM training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training stacked LSTMs took less time than training bidirectional LSTMs. The
    validation accuracy seems to be close to that of the bidirectional LSTM model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the test set and print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy is printed out as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy of `87.6%` is an improvement over the LSTM model (`87%`) and is
    practically the same as that of the bidirectional model (`87.7%`). This is a somewhat
    significant improvement over the performance of the regular LSTM model, considering
    that we're working with a rather small dataset. The larger your dataset is, the
    more you can benefit from these sophisticated architectures. Try tuning the hyperparameters
    in order to get the most out of this powerful architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing All the Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve looked at different variants of RNNs – from plain RNNs
    to LSTMs to GRUs. We also looked at the bidirectional approach and the stacking
    approach to using RNNs. Now is a good time to take a holistic look at things and
    make a comparison between the models. Let''s look at the following table, which
    compares the five models in terms of parameters, training time, and performance
    (that is, the level of accuracy on our dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23: Comparing the five models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.23: Comparing the five models'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier in the chapter, while working through the practical elements,
    you may have obtained values different from the ones shown above; however, the
    test accuracies you obtain should largely agree with ours. If the model's performance
    is very different, you may want to tweak the number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Plain RNNs are the lowest on parameters and have the lowest training times but
    have the lowest accuracy of all the models. This is in line with our expectations
    – we are dealing with sequences that are 200 characters in length, and we know
    not to expect much from plain RNNs, and that gated RNNs (LSTMs, GRUs) are more
    suitable. Indeed, LSTMs and GRUs do perform significantly better than plain RNNs.
    But the accuracy comes at the cost of significantly higher training times, and
    several times the parameters, making these models more prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The approaches of stacking and using bidirectional processing seem to provide
    an incremental benefit in terms of predictive power, but this is at the cost of
    significantly higher training times and several times the parameters. The stacked
    and bidirectional approaches gave us the highest accuracy, even on this small
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: While the performance results are specific to our dataset, the gradation in
    performance we see here is fairly common. The stacked and bidirectional models
    are present in many of the solutions today that provide state-of-the-art results
    in various tasks. With a larger dataset and when working with much longer sequences,
    we would expect the differences in model performances to be larger.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attention models were first introduced in late 2015 by Dzmitry Bahdanau, KyungHyun
    Cho, and Yoshua Bengio in their influential and seminal paper ([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473))
    that demonstrated the state-of-the-art results of English-to-French translation.
    Since then, this idea has been used for many sequence-processing tasks with great
    success, and attention models are becoming increasingly popular. While a detailed
    explanation and mathematical treatment is beyond the scope of this book, let's
    understand the intuition behind the idea that is considered by many big names
    in the field of deep learning as a significant development in our approach to
    sequence modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition behind attention can be best understood using an example from
    the task it was developed for – translation. When a novice human translates a
    long sentence between languages, they don''t translate the entire sentence in
    one go. They break the original sentence down into smaller, manageable chunks,
    thereby generating a translation for each chunk sequentially. For each chunk,
    there would be a part that is the most important for the translation task, that
    is, where you need to pay the most attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24: Idea of attention simplified'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.24: Idea of attention simplified'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows a simple example where we're translating the sentence,
    "`Azra is moving to Berlin`", into French. The French translation is, "`Azra déménage
    à Berlin`". To get the first term in the French translation, "`Azra`", we need
    to pay attention primarily to the first term in the original sentence (underscored
    by a light gray line) and maybe a bit to the second (underscored by a dark gray
    line) – these terms get higher importance (weight). The remaining parts of the
    sentence aren't relevant. Similarly, to generate the term "`déménage`" in the
    output, we need to pay attention to the terms "`is`" and "`moving`". The importance
    of each term toward the output term is expressed as weights. This is known as
    "**alignment**".
  prefs: []
  type: TYPE_NORMAL
- en: These alignments can be seen in the following figure, which was sourced from
    the original paper ([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)).
    It beautifully demonstrates what the model identified as most important for each
    term in the output. A lighter color in a cell in the grid means a higher weight
    for the corresponding input term in the column. We can see that for the output
    term "`marin`", the model correctly identifies "`marine`" as the most important
    input term to pay attention to. Similarly, it has identified "`environment`" as
    the most important term for "`environnement`", "`known`" for "`connu`", and so
    on. Pretty neat, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25: The alignment learned by the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_06_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.25: The alignment learned by the model'
  prefs: []
  type: TYPE_NORMAL
- en: While attention models were originally designed for translation tasks, the models
    have been employed on a variety of other tasks with good success. That being said,
    note that the attention models have a very high number of parameters. The models
    are typically employed on bidirectional LSTM layers and add additional weights
    for the importance values. A high number of parameters makes the model more prone
    to overfitting, which means they will need much larger datasets to utilize their
    power.
  prefs: []
  type: TYPE_NORMAL
- en: More Variants of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen quite a few variations of RNNs in this chapter – covering all the
    prominent ones and the major upcoming (in terms of popularity) variations. Sequence
    modeling and its associated architectures are a hot area of research, and we see
    plenty of developments coming in every year. Many variants aim to make lighter
    models with fewer parameters that aren't as hardware hungry as current RNNs. **Clockwork
    RNNs** (**CWRNNs**) are a recent development and show great success. There are
    also **Hierarchal Attention Networks**, built on the idea of attention, but ultimately
    also propose that you shouldn't use RNNs as building blocks. There's a lot going
    on in this exciting area, so keep your eyes and ears open for the next big idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Sentiment Analysis of Amazon Product Reviews'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've looked at the variants of RNNs and used them to predict sentiment
    on movie reviews from the IMDb dataset. In this activity, we will build a sentiment
    classification model on Amazon product reviews. The data contains reviews for
    several categories of products. The original dataset, available at [https://snap.stanford.edu/data/web-Amazon.html](https://snap.stanford.edu/data/web-Amazon.html),
    is huge; therefore, we have sampled 50,000 reviews for this activity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The sampled dataset, which has been split into train and test sets, can be found
    at [https://packt.live/3iNTUjN](https://packt.live/3iNTUjN).
  prefs: []
  type: TYPE_NORMAL
- en: This activity will bring together the concepts and methods we discussed in this
    chapter and those discussed in *Chapter 4, Deep Learning for Text – Embeddings*,
    and *Chapter 5, Deep Learning for Sequences*. You will begin by performing a detailed
    text cleanup and conduct preprocessing to get it ready for the deep learning model.
    You will also use embeddings to represent text. For the prediction part, you will
    employ stacked LSTMs (two layers) and two dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience (and awareness), you will also utilize the `Tokenizer` API
    from TensorFlow (Keras) to convert the cleaned-up text into the corresponding
    sequences. The `Tokenizer` combines the function of the tokenizer from `NLTK`
    with the `vectorizer` (`CountVectorizer`/ `TfIdfVectorizer`) by tokenizing the
    text first and then learning a vocabulary from a dataset. Let''s see it in action
    by creating some toy data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Tokenizer` can be imported, instantiated, and fit on the toy data using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the vocabulary has been trained on the toy data (index learned for each
    term), we can convert the input text into a corresponding sequence of indices
    for the terms. Let''s convert the toy data into the corresponding sequences of
    indices using the `texts_to_sequences` method of the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now, the data format is the same as that of the IMDb dataset we've used throughout
    this chapter, and it can be processed in a similar fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, you are now ready to get started. The following are the high-level
    steps you will need to follow to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in the data files for the train and test sets (`Amazon_reviews_train.csv`
    and `Amazon_reviews_test.csv`). Examine the shapes of the datasets and print out
    the top five records from the train data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For convenience when it comes to processing, separate the raw text and the
    labels for the train and test set. Print the first two reviews from the train
    text. You should have the following four variables: `train_raw` comprising the
    raw text for the train data, `train_labels` with labels for the train data, `test_raw`
    containing raw text for the test data, and `test_labels` with labels for the test
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Normalize the case and tokenize the test and train texts using NLTK''s `word_tokenize`
    (after importing it, of course – hint: use list comprehension for cleaner code).
    Print the first review from the train data to check if the tokenization worked.
    Download `punkt` from NLTK if you haven''t used the tokenizer before.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import `stopwords` (built in to NLTK) and punctuation from the string module.
    Define a function (`drop_stop`) to remove these tokens from any input tokenized
    sentence. Download `stopwords` from NLTK if you haven't used it before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the defined function (`drop_stop`), remove the redundant stop words from
    the train and the test texts. Print the first review of the processed train texts
    to check if the function worked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `Porter Stemmer` from NLTK, stem the tokens for the train and test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the strings for each of the train and text reviews. This will help us
    work with the utilities in Keras to create and pad the sequences. Create the `train_texts`
    and `test_texts` variables. Print the first review from the processed train data
    to confirm it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the Keras preprocessing utilities for text (`keras.preprocessing.text`),
    import the `Tokenizer` module. Define a vocabulary size of `10000` and instantiate
    the tokenizer with this vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the tokenizer on the train texts. This works just like `CountVectorizer`
    did in *Chapter 4, Deep Learning for Text – Embeddings*, and trains the vocabulary.
    After fitting, use the `texts_to_sequences` method of the tokenizer on the train
    and test sets to create the sequences for them. Print the sequence for the first
    review in the train data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to find the optimal length of the sequences to process in the model.
    Get the length of the reviews from the train set into a list and plot the histogram
    of the lengths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is now in the same format as the IMDb data we used in the chapter.
    Using a sequence length of `100` (define the `maxlen = 100` variable), use the
    `pad_sequences` method from the `sequence` module in Keras' preprocessing utilities
    (`keras.preprocessing.sequence`) to limit the sequences to `100` for both the
    train and test data. Check the shape of the result for the train data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To build the model, import all the necessary layers from Keras (`embedding`,
    `spatialdropout`, `LSTM`, `dropout`, and `dense`) and import the `Sequential`
    model. Initialize the `Sequential` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an embedding layer with `32` as the vector size (`output_dim`). Add a spatial
    dropout of `40%`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a stacked LSTM model with `2` layers with `64` cells each. Add a dropout
    layer with `40%` dropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a dense layer with `32` neurons with `relu` activation, then a `50%` dropout
    layer, followed by another dense layer of `32` neurons with `relu` activation,
    and follow this up with another dropout layer with `50%` dropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a final dense layer with a single neuron with `sigmoid` activation and compile
    the model. Print the model summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model on the training data with a `20%` validation split and a batch
    size of `128`. Train for `5` epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a prediction on the test set using the `predict_classes` method of the
    model. Using the `accuracy_score` method from `scikit-learn`, calculate the accuracy
    on the test set. Also, print out the confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the preceding parameters, you should get about `86%` accuracy. With some
    hyperparameter tuning, you should be able to get a significantly higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 416.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by understanding the reasons for plain RNNs not
    being practical for very large sequences – the main culprit being the vanishing
    gradient problem, which makes modeling long-range dependencies impractical. We
    saw the LSTM as an update that performs extremely well for long sequences, but
    it is rather complicated and has a large number of parameters. GRU is an excellent
    alternative that is a simplification over LSTM and works well on smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we started looking at ways to extract more power from these RNNs by using
    bidirectional RNNs and stacked layers of RNNs. We also discussed attention mechanisms,
    a significant new approach that provides state-of-the-art results in translation
    but can also be employed on other sequence-processing tasks. All of these are
    extremely powerful models that have changed the way several tasks are performed
    and form the basis for models that produce state-of-the-art results. With active
    research in the area, we expect things to only get better as more novel variants
    and architectures are released.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discussed a variety of powerful modeling approaches, in the next
    chapter, we will be ready to discuss a very interesting topic in the deep learning
    domain that enables AI to be creative – **Generative Adversarial Networks**.
  prefs: []
  type: TYPE_NORMAL
