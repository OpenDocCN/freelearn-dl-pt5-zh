<html><head></head><body>
  <div id="_idContainer301">
    <h1 class="chapterNumber">13</h1>
    <h1 id="_idParaDest-257" class="chapterTitle">Visualizing Networks with TensorFlow 2.x and TensorBoard</h1>
    <p class="normal">In this chapter, we are going to take a peek inside a machine's "mind" while it's "thinking" through the layers of a deep learning neural network. The number of lines of code required to build a sequential classifier for a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) has been drastically reduced with TensorFlow 2. Running the classifier only takes a click. However, to understand the program when something goes wrong is a more difficult task, and visualizing the outputs of the layers can be very productive.</p>
    <p class="normal">Visualizing the output of the layers of a CNN can provide an in-depth knowledge of each individual step comprising the whole process.</p>
    <p class="normal">In this chapter, as in several of the preceding chapters, we will define the layers of a CNN. This time, we will add more layers and extract the output of each layer to create output images. We will build this process from scratch in a bottom-to-top approach in TensorFlow 2 in Python.</p>
    <p class="normal">Once the outputs have been defined, we will display the output of the convolutional, pooling, dropout, flattening, and dense layers.</p>
    <p class="normal">Viewing the output of the layers provides an intuitive sense of what the layers are doing. Being able to visualize the global graph of the model makes the architecture of the CNN visible. </p>
    <p class="normal">We will use TensorBoard to explore the conceptual model, the epochs versus the accuracy, and the detail of the operations of mathematical functions. These graphs and measurements will be built using a top-to-bottom approach using Google Colaboratory.</p>
    <p class="normal">Google Colaboratory provides a free server with ready-to-use libraries and modules. We will use a Google Colaboratory notebook to explore TensorBoard functions. The chapter is divided into three main sections. The first two sections describe how to build a sequential classifier with TensorFlow 2.2 and display the outputs of the layers with TensorFlow 2.2. The third section describes how to display the graph information and accuracy measurement with the TensorFlow 2 version of TensorBoard.</p>
    <p class="normal">The topics covered in this chapter will provide visual insights into CNNs:</p>
    <ul>
      <li class="list">Building a CNN layer by layer</li>
      <li class="list">Displaying the dataset</li>
      <li class="list">Displaying the output of the layers of the CNN</li>
      <li class="list">Using Google Colaboratory</li>
      <li class="list">Visualizing the architecture of a neural network with TensorBoard</li>
      <li class="list">Visualizing accuracy measurements with TensorBoard</li>
    </ul>
    <p class="normal">Let's start off this chapter by discussing how we can explore the output of the layers within a CNN.</p>
    <h1 id="_idParaDest-258" class="title">Exploring the output of the layers of a CNN in two steps with TensorFlow</h1>
    <p class="normal">Many <a id="_idIndexMarker619"/>corporate contracts in the field of business <a id="_idIndexMarker620"/>intelligence require an explanation process for any algorithm that makes automatic and critical decisions. It is often mandatory for the editor of algorithms, artificial intelligence or not, to provide an explanation. We need to be prepared for that.</p>
    <p class="normal">Also, maintenance becomes critical once artificial intelligence runs in production. Developers often move from one department to another, from one company to another. The person that has to maintain the program needs to understand it in detail.</p>
    <p class="normal">Exploring and visualizing a CNN is a good way to get our hands dirty, open the hood of our roadster and see how the engine works!</p>
    <ul>
      <li class="list">First, we will first build the CNN layer by layer. We will be building the sequential classifier with TensorFlow 2 from the bottom to the top.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">We will not be using a Keras model directly; we'll use TensorFlow's integrated Keras module, which brings the number of lines of header down to only two lines:</p>
        <pre class="programlisting"><code class="hljs elm"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
</code></pre>
      </li>
      <li class="list">Then<a id="_idIndexMarker621"/> we will explore the visual<a id="_idIndexMarker622"/> output of the layers to gain insights into the way it "thinks."</li>
    </ul>
    <p class="normal">With that, let's get on with building!</p>
    <h2 id="_idParaDest-259" class="title">Building the layers of a CNN</h2>
    <p class="normal">A CNN was<a id="_idIndexMarker623"/> described in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>. In the example to follow, the CNN will contain more layers<a id="_idIndexMarker624"/> to visualize the way a neural network extracts features step by step.</p>
    <p class="normal">We will be using a dataset that uses a single image to explore the layers of a CNN. This image is repeated several times for the training dataset and the test dataset is enough to build and run the model to visualize the layers of the neural network.</p>
    <p class="normal">The dataset contains an image of a flower repeated several times—an iris.</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.1: The image of the image we are exploring in this chapter</p>
    <p class="normal">The goal is not to have many variations of the image, but to simply see how a CNN represents an iris layer by layer. The dataset contains a repeated image. However, you can change these images and use a dataset of your own then display the images as follows using the same code:</p>
    <pre class="programlisting"><code class="hljs angelscript">cv_img=[]
images = []
<span class="hljs-keyword">for</span> img_path <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-string">'dataset/training_set/img/*.png'</span>):
    images.append(mpimg.imread(img_path))
    
plt.figure(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>)) #<span class="hljs-number">20</span>,<span class="hljs-number">10</span>
columns = <span class="hljs-number">5</span>
<span class="hljs-keyword">for</span> i, image <span class="hljs-keyword">in</span> enumerate(images):
    plt.subplot(len(images) / columns + <span class="hljs-number">1</span>, columns, i + <span class="hljs-number">1</span>)
    plt.imshow(image)
</code></pre>
    <p class="normal">The result will be a <a id="_idIndexMarker625"/>figure with <a id="_idIndexMarker626"/>lines of images from the dataset:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.2: Displaying the dataset</p>
    <p class="normal">We will first import the neural network modules:</p>
    <pre class="programlisting"><code class="hljs elm"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
</code></pre>
    <p class="normal">Building the CNN only takes a few lines. This makes it deceptively simple because it appears to be a black box. In our example, the structure described in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, in its enhanced version here, only requires a few minutes to create from lines 30 to 68:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#initializing the Tensorflow 2 classifier</span>
classifier = models.Sequential()
<span class="hljs-comment">#adding the convolution layers to the layers</span>
classifier.<span class="hljs-builtin-name">add</span>(layers.Conv2D(32, (3, 3), <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, input_shape = (28, 28, 3), activation = <span class="hljs-string">'relu'</span>))
classifier.<span class="hljs-builtin-name">add</span>(layers.Conv2D(32, (3, 3), <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>))
<span class="hljs-built_in">..</span>.
<span class="hljs-comment">#adding dense-dropout-dense layers</span>
classifier.<span class="hljs-builtin-name">add</span>(layers.Dense(units = 512, activation = <span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">We will go back to these layers in the next section when we explore their output. The main point to focus on here remains the simplicity of the code. Building a CNN as a black box in a few minutes might work. However, understanding each layer when a problem comes up requires a <a id="_idIndexMarker627"/>deeper understanding of the representations of the<a id="_idIndexMarker628"/> layers.</p>
    <p class="normal">Before exploring those layers, the program prints the structure of the classifier (CNN):</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-selector-id">#Printing</span> the model <span class="hljs-selector-tag">summary</span>
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"Model Summary"</span>,classifier.summary()</span></span>)
</code></pre>
    <p class="normal">The model contains a fair number of layers to explore:</p>
    <pre class="programlisting"><code class="hljs markdown">Model: "sequential"
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
<span class="hljs-section">Layer (type)                 Output Shape              Param #   
=================================================================</span>
conv2d (Conv2D)              (None, 28, 28, 32)        896       
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_1 (Conv2D)            (None, 26, 26, 32)        9248      
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dropout (Dropout)            (None, 13, 13, 32)        0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
max<span class="hljs-emphasis">_pooling2d_</span>1 (MaxPooling2 (None, 5, 5, 64)          0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dropout_1 (Dropout)          (None, 5, 5, 64)          0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_4 (Conv2D)            (None, 5, 5, 64)          36928     
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
max<span class="hljs-emphasis">_pooling2d_</span>2 (MaxPooling2 (None, 1, 1, 64)          0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dropout_2 (Dropout)          (None, 1, 1, 64)          0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
flatten (Flatten)            (None, 64)                0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dense (Dense)                (None, 512)               33280     
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dropout_3 (Dropout)          (None, 512)               0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
<span class="hljs-section">dense_1 (Dense)              (None, 3)                 1539      
===============================================================</span>
</code></pre>
    <p class="normal">Keep an eye on this summary. It will prove useful when choosing the number of layers you wish to explore to visualize the outputs.</p>
    <p class="normal">The<a id="_idIndexMarker629"/> model is then compiled:</p>
    <pre class="programlisting"><code class="hljs awk"><span class="hljs-comment"># Compiling the convolutional neural network (CNN)</span>
classifier.compile(optimizer = <span class="hljs-string">'rmsprop'</span>,
    loss = <span class="hljs-string">'categorical_crossentropy'</span>,metrics = [<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Then<a id="_idIndexMarker630"/> training and test datasets are processed (rescaled) and defined:</p>
    <pre class="programlisting"><code class="hljs nix"><span class="hljs-attr">train_datagen</span> = ImageDataGenerator(<span class="hljs-attr">rescale</span> = <span class="hljs-number">1</span>./<span class="hljs-number">255</span>)
<span class="hljs-attr">test_datagen</span> = ImageDataGenerator(<span class="hljs-attr">rescale</span> = <span class="hljs-number">1</span>./<span class="hljs-number">255</span>)
<span class="hljs-attr">training_set</span> = train_datagen.flow_from_directory(
                                  'dataset/training_set',
                                  <span class="hljs-attr">target_size</span> = (<span class="hljs-number">28</span>, <span class="hljs-number">28</span>),
                                  <span class="hljs-attr">batch_size</span> = <span class="hljs-number">16</span>,
                                  <span class="hljs-attr">class_mode</span> =
                                  'categorical')
<span class="hljs-attr">test_set</span> = test_datagen.flow_from_directory('dataset/test_set',
                                           <span class="hljs-attr">target_size</span> = (<span class="hljs-number">28</span>, <span class="hljs-number">28</span>),
                                           <span class="hljs-attr">batch_size</span> = <span class="hljs-number">16</span>,
                                           <span class="hljs-attr">class_mode</span> =
                                           'categorical')
</code></pre>
    <p class="normal">If we stop here, the CNN will work. But will we have really understood the model? I don't think so. Of course, it only takes a simple click to get the CNN to run after installing a ready-to-use dataset. This black box approach can work, but exploring the visual output of a layer provides a better representation of the network. Let's take a look at that next.</p>
    <h2 id="_idParaDest-260" class="title">Processing the visual output of the layers of a CNN</h2>
    <p class="normal">The idea is to <a id="_idIndexMarker631"/>focus on an image and actually <em class="italics">see</em> the "mental," visual, representation a CNN calculates, layer by layer.</p>
    <p class="normal">To process the layers, the program first selects an image for the activation model to work on:</p>
    <pre class="programlisting"><code class="hljs reasonml">#Selecting an image for the activation model
img_path = 'dataset/test_set/img/img1.png'
img1 = image.load<span class="hljs-constructor">_img('<span class="hljs-params">dataset</span><span class="hljs-operator">/</span><span class="hljs-params">test_set</span><span class="hljs-operator">/</span><span class="hljs-params">img</span><span class="hljs-operator">/</span><span class="hljs-params">img1</span>.<span class="hljs-params">png</span>', <span class="hljs-params">target_size</span>=(28, 28)</span>)
img = image.img<span class="hljs-constructor">_to_array(<span class="hljs-params">img1</span>)</span>
img = np.expand<span class="hljs-constructor">_dims(<span class="hljs-params">img</span>, <span class="hljs-params">axis</span>=0)</span>
img /= <span class="hljs-number">255.</span>
plt.imshow(img<span class="hljs-literal">[<span class="hljs-number">0</span>]</span>)
plt.show<span class="hljs-literal">()</span>
print(<span class="hljs-string">"img tensor shape"</span>,img.shape)
</code></pre>
    <p class="normal">Then the visualization process runs in a few steps, which will take us inside the CNN:</p>
    <ul>
      <li class="list"><strong class="bold">Selecting the number of layers to visualize using the </strong><code class="Code-In-Text--PACKT-">e</code><strong class="bold"> variable</strong>: Going back to the model summary displayed previously, you can choose the layer you want to stop at. In this example, we're stopping at <code class="Code-In-Text--PACKT-">e=12</code>. You can choose to start with <code class="Code-In-Text--PACKT-">e=4</code> to visualize the first convolutional and pooling layers:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment">#Selecting the number of layers to display</span>
<span class="hljs-attr">e</span>=<span class="hljs-number">12</span> <span class="hljs-comment">#last layer displayed</span>
<span class="hljs-attr">layer_outputs</span> = [layer.output for layer in classifier.layers[<span class="hljs-number">0</span>:e]]
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">If <code class="Code-In-Text--PACKT-">e=3</code>, the program will stop at <code class="Code-In-Text--PACKT-">max_pooling2d</code>:</p>
        <pre class="programlisting"><code class="hljs gml">Displaying <span class="hljs-symbol">layer</span>: conv2d
Displaying <span class="hljs-symbol">layer</span>: conv2d_1
Displaying <span class="hljs-symbol">layer</span>: max_pooling2d
</code></pre>
      </li>
      <li class="list"><strong class="bold">Selecting the top n layers that will be explored</strong>: The program refers to <code class="Code-In-Text--PACKT-">layer_outputs</code> to extract the information it needs to visualize the target layers:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># Extracting the information of the top n layers</span>
activation_model = models.Model(<span class="hljs-attribute">inputs</span>=classifier.input,
                                <span class="hljs-attribute">outputs</span>=layer_outputs)
</code></pre>
      </li>
      <li class="list"><strong class="bold">Applying the activation model to extract the requested layers</strong>: Activating the model forces the classifier to get to work and run through the layers. That way, we can peek inside its "thought" process and see how it represents inputs:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment"># Activating the model</span>
<span class="hljs-attr">activations</span> = activation_model.predict(img)
</code></pre>
      </li>
      <li class="list"><strong class="bold">Retrieving the layer names to display, along with the visual representation of the layer</strong>: Layer <a id="_idIndexMarker632"/>names help us understand what we are looking at. Use the model summary we printed earlier as a map to see where you are when the layer name is displayed, along with the representation of the output of that same layer:
        <pre class="programlisting"><code class="hljs gml">#<span class="hljs-symbol">layer</span> names
layer_names = []
<span class="hljs-keyword">for</span> <span class="hljs-symbol">layer</span> in classifier.layers[:<span class="hljs-number">12</span>]:
    layer_names.append(<span class="hljs-symbol">layer</span>.name)
</code></pre>
      </li>
      <li class="list"><strong class="bold">Processing layer outputs and organizing them into grids</strong>: To avoid having to watch a sequential display of the variations of the representations in a given layer, we are going to organize them in one grid image:
        <pre class="programlisting"><code class="hljs maxima"># Processing the layer outputs
<span class="hljs-keyword">for</span> layer_name, layer_activation <span class="hljs-keyword">in</span> zip(layer_names,
        activations):
    #getting the layer_names <span class="hljs-keyword">and</span> their activations
    n_features = layer_activation.shape[-<span class="hljs-number">1</span>] #<span class="hljs-built_in">features</span> <span class="hljs-keyword">in</span> the layer
    size = layer_activation.shape[<span class="hljs-number">1</span>] #shape of the <span class="hljs-built_in">feature</span> <span class="hljs-built_in">map</span>
    n_cols = n_features // images_per_row #number of images per <span class="hljs-built_in">row</span>
    display_grid = <span class="hljs-built_in">np</span>.zeros((size * n_cols,
                             images_per_row * size)) #size of the <span class="hljs-built_in">display</span> <span class="hljs-built_in">grid</span>
    <span class="hljs-keyword">for</span> <span class="hljs-built_in">col</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_cols): #organizing the <span class="hljs-built_in">columns</span>
        <span class="hljs-keyword">for</span> <span class="hljs-built_in">row</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(images_per_row): #...<span class="hljs-keyword">and</span> rows to <span class="hljs-built_in">display</span>
            <span class="hljs-built_in">image</span> = layer_activation[<span class="hljs-number">0</span>,:, :,
                <span class="hljs-built_in">col</span> * images_per_row + <span class="hljs-built_in">row</span>] #retrieving the <span class="hljs-built_in">image</span>...
            <span class="hljs-built_in">image</span> -= <span class="hljs-built_in">image</span>.<span class="hljs-built_in">mean</span>() #...<span class="hljs-keyword">and</span> processing it <span class="hljs-keyword">in</span> the...
            <span class="hljs-keyword">if</span>(<span class="hljs-built_in">image</span>.<span class="hljs-built_in">std</span>()&gt;<span class="hljs-number">0</span>): # ...following lines to <span class="hljs-built_in">display</span> it
                <span class="hljs-built_in">image</span> /= <span class="hljs-built_in">image</span>.<span class="hljs-built_in">std</span>()
                <span class="hljs-built_in">image</span> *= <span class="hljs-number">64</span>
                <span class="hljs-built_in">image</span> += <span class="hljs-number">128</span>
                <span class="hljs-built_in">image</span> = <span class="hljs-built_in">np</span>.clip(<span class="hljs-built_in">image</span>, <span class="hljs-number">0</span>,
                    <span class="hljs-number">255</span>).astype('uint8')
                display_grid[<span class="hljs-built_in">col</span> * size : (<span class="hljs-built_in">col</span> + <span class="hljs-number">1</span>) * size,
                    <span class="hljs-built_in">row</span> * size : (<span class="hljs-built_in">row</span> + <span class="hljs-number">1</span>) * size] = <span class="hljs-built_in">image</span>
</code></pre>
      </li>
      <li class="list"><strong class="bold">Displaying the processed layer outputs</strong>: Now that the work is done, we just have to display the layer names along with the corresponding grids:
        <pre class="programlisting"><code class="hljs processing">    #displaying the layer names and processed grids
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Displaying layer:"</span>,layer_name)
    <span class="hljs-built_in">scale</span> = <span class="hljs-number">1.</span> / <span class="hljs-built_in">size</span>
    plt.figure(figsize=(<span class="hljs-built_in">scale</span> * display_grid.<span class="hljs-built_in">shape</span>[<span class="hljs-number">1</span>],
                        <span class="hljs-built_in">scale</span> * display_grid.<span class="hljs-built_in">shape</span>[<span class="hljs-number">0</span>]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect=<span class="hljs-string">'auto'</span>, cmap=<span class="hljs-string">'viridis'</span>)
    plt.savefig(<span class="hljs-string">"dataset/output/"</span>+layer_name)
    plt.show()
</code></pre>
      </li>
    </ul>
    <p class="normal">Note that the<a id="_idIndexMarker633"/> figures are saved by <code class="Code-In-Text--PACKT-">plt.savefig</code> in an output directory for later use.</p>
    <p class="normal">You will obtain a list of figures with the name of the layer for the layers you chose to visualize. For example, you can view the images of the first seven layers. You can look at them in the following figures or by running the program. In any case, the best way to analyze the layers is to look closely at the first layer and then at the last layer. You will see that the CNN is carefully extracting an abstract representation of the image and displaying higher dimensions. The reason the differences between the layers are difficult to perceive with the human eye comes from two factors:</p>
    <ul>
      <li class="list">The number of elements to analyze is extremely difficult for us to observe. Usually our brain does this without us having to think about it!</li>
      <li class="list">There are several convolutional layers versus one layer that would rush through the process of obtaining an abstract representation of the image. It's going layer by layer, just like a human brain processes an image step-by-step.</li>
    </ul>
    <p class="normal">Look at the following first layer then the last one, then go back and observe the differences between the layers.</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_03.png" alt="Une image contenant équipement électronique, capture d’écran  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.3: Convolutional layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_04.png" alt="Une image contenant équipement électronique, afficher  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.4: Convolutional layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_05.png" alt="Une image contenant équipement électronique, afficher  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.5: Pooling layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_06.png" alt="Une image contenant équipement électronique, afficher  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.6: Dropout layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_07.png" alt="Une image contenant tableau de points, afficher, équipement électronique  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.7: Convolutional layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_08.png" alt="Une image contenant tableau de points, texte  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.8: Convolutional layer</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_09.png" alt="Une image contenant tableau de points, texte  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 13.9: Pooling layer</p>
    <p class="normal">Visualizing the<a id="_idIndexMarker634"/> output of the layers of a CNN provides a fantastic way to understand and analyze a neural network. Let's go a step further and analyze the layers.</p>
    <h3 id="_idParaDest-261" class="title">Analyzing the visual output of the layers of a CNN</h3>
    <p class="normal">An input image is chaos<a id="_idIndexMarker635"/> until some form of intelligence makes sense of it. Any form of intelligence will detect patterns and structures. Machine intelligence works on the same grounds by increasing the level of abstraction through dimensionality reduction. The process of going from chaos to an organized representation is at the heart of the fantastic invention of present-day neural networks.</p>
    <p class="normal">When running <code class="Code-In-Text--PACKT-">cnn_layers.py</code>, the layer outputs will be displayed. Let's explore some of the layers. You can explore some or all of them by simply changing the value of the <code class="Code-In-Text--PACKT-">e = &lt;number-of-layers&gt;</code> variable on line 107.</p>
    <h4 class="title">Convolutional layer activation functions</h4>
    <p class="normal">One of the key<a id="_idIndexMarker636"/> options of a convolutional layer is the activation function. <code class="Code-In-Text--PACKT-">relu</code> is used in the following <code class="Code-In-Text--PACKT-">cnn_layers.py</code>:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#adding more convolution layers to the layers</span>
classifier.<span class="hljs-builtin-name">add</span>(layers.Conv2D(64, (3, 3), <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, activation = <span class="hljs-string">'relu'</span>))
classifier.<span class="hljs-builtin-name">add</span>(layers.Conv2D(64, (3, 3), <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">For more on ReLU, please go the explanation in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">relu</code> produces the following output for the <code class="Code-In-Text--PACKT-">Conv2d</code> layer:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.10: conv2d output 1</p>
    <p class="normal">Now go to line 33 and replace <code class="Code-In-Text--PACKT-">relu</code> with <code class="Code-In-Text--PACKT-">softmax</code> as follows:</p>
    <pre class="programlisting"><code class="hljs angelscript">classifier.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>,
    input_shape = (<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>), activation = <span class="hljs-string">'softmax'</span>))
</code></pre>
    <p class="normal">The output is quite different, as we can see:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.11: conv2d output 2</p>
    <p class="normal">It takes some time for the human eye to adjust to the change. Look at each version. Try to memorize it for a few seconds by closing your eyes and then looking at the other one. Our brain does this implicitly which is why it takes an effort to do this explicitly.</p>
    <p class="normal">Which one should <a id="_idIndexMarker637"/>you use? Welcome to deep learning! There is no certain answer to that question. It is a trial-and-error process. An activation function might fit one model and not another. Even if the accuracy of the network is acceptable during the training process, you might have to change the activation function over time when new data produces bad results.</p>
    <p class="normal">For more on softmax, please go back to the explanation in <em class="italics">Chapter 2</em>, <em class="italics">Building a Reward Matrix – Designing Your Datasets</em>.</p>
    <p class="normal">Let's try the logistic sigmoid activation function, <code class="Code-In-Text--PACKT-">sigmoid</code>, also described in <em class="italics">Chapter 2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.12: conv2d output 3</p>
    <p class="normal">Notice the differences again. Observe the last image on row 1 for each activation function. The differences are fascinating because they provide a variety of possible representations.</p>
    <p class="normal">Try other activation <a id="_idIndexMarker638"/>functions to get the feel of the way an artificial neural network transforms what it perceives into a higher level of abstraction through a reduction of the dimensions that it has to process.</p>
    <h4 class="title">Convolutional layers higher-level representations through the layers</h4>
    <p class="normal">Notice the incredible level<a id="_idIndexMarker639"/> of abstraction the sequential classifier reaches through the following outputs going from <code class="Code-In-Text--PACKT-">conv2d</code> to <code class="Code-In-Text--PACKT-">conv2d_5</code>, which is, in fact, the sixth (0 to 5) convolutional layer of <code class="Code-In-Text--PACKT-">cnn_layers.py</code>.</p>
    <p class="normal">The network starts at a relatively figurative representation to reach a highly abstract level at <code class="Code-In-Text--PACKT-">conv2d_5</code>. We are literally inside the machine's "mind," watching it think and learn!</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.13: Initial conv2d output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.14: conv2d_1 output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.15: conv2d_2 output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.16: conv2d_3 output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.17: conv2d_4 output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.18: conv2d_5 output</p>
    <p class="normal">This abstraction<a id="_idIndexMarker640"/> process owes a lot to the other layers, such as the pooling layer.</p>
    <h4 class="title">Pooling layers to obtain higher-level representations</h4>
    <p class="normal">The pooling layer is<a id="_idIndexMarker641"/> going to reduce the number of dimensions of its input and choose the most representative features it finds:</p>
    <pre class="programlisting"><code class="hljs dockerfile"><span class="hljs-comment">#adding a max pooling layer to the layers</span>
classifier.<span class="hljs-keyword">add</span><span class="bash">(layers.MaxPooling2D(pool_size=(2, 2)))</span>
</code></pre>
    <p class="normal">Let's explore the evolution of the first two pooling layers in this example:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.19: max_pooling2d output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.20: max_pooling2d_1 output</p>
    <p class="normal">Once again, we can see the<a id="_idIndexMarker642"/> powerful level of abstraction a CNN can attain.</p>
    <p class="normal">For more information on the pooling layer, please read the explanations in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>.</p>
    <h4 class="title">Dropout layer higher-level representations through the layers</h4>
    <p class="normal">The dropout layers<a id="_idIndexMarker643"/> provide a way to abandon many features in order to reach a simplified higher level of representation:</p>
    <pre class="programlisting"><code class="hljs css"><span class="hljs-selector-tag">classifier</span><span class="hljs-selector-class">.add</span>(<span class="hljs-selector-tag">layers</span><span class="hljs-selector-class">.Dropout</span>(0<span class="hljs-selector-class">.5</span>)) # <span class="hljs-selector-tag">antes</span> <span class="hljs-selector-tag">era</span> 0<span class="hljs-selector-class">.25</span>
</code></pre>
    <p class="normal">It is not always necessary to add a dropout layer because it depends on the productivity and architecture of the model you are exploring. For this example, the first two dropout layers are quite instructive. Dropouts are also a way to avoid overfitting. The model learns how to extract key <a id="_idIndexMarker644"/>features to obtain an abstract representation, not a literal one.</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.21: dropout output</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.22: dropout_1 output</p>
    <p class="normal">Observe again how the dropout layers accelerate the abstraction process. We can see the CNN's "mind" working.</p>
    <h4 class="title">Recommendation</h4>
    <p class="normal">I recommend you try different activation functions and various options for the layers of this example. Then run the program and get a feel of what is going inside a CNN's "machine thinking" process. Even if it is a purely mathematical architecture, it provides a good idea of how a CNN, while not human at all, has its own "machine thinking" approach.</p>
    <p class="normal">Now that we have explored a CNN from bottom to top, let's see how to observe the accuracy of a CNN from top to bottom using TensorBoard.</p>
    <h1 id="_idParaDest-262" class="title">Analyzing the accuracy of a CNN using TensorBoard</h1>
    <p class="normal">In this <a id="_idIndexMarker645"/>section, we will first get started with a free Google Colaboratory server and then explore some of the TensorBoard ANN measurement functions.</p>
    <h2 id="_idParaDest-263" class="title">Getting started with Google Colaboratory</h2>
    <p class="normal">You can get access to your<a id="_idIndexMarker646"/> free instance of Google Colaboratory server in just a few steps:</p>
    <ol>
      <li class="list">Make sure you have a Google account and log into it.</li>
      <li class="list">Click on the following link, which<a id="_idIndexMarker647"/> takes leads you to Google Colaboratory: <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true"><span class="url">https://colab.research.google.com/notebooks/welcome.ipynb#recent=true</span></a>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">You will be taken to the following page:</p>
        <figure class="mediaobject"><img src="../Images/B15438_13_23.png" alt=""/></figure>
        <p class="packt_figref">Figure 13.23: Colaboratory initial landing page</p>
      </li>
      <li class="list">Click on the <strong class="screen-text">UPLOAD</strong> option in the top right:<figure class="mediaobject"><img src="../Images/B15438_13_24.png" alt=""/></figure>
        <p class="packt_figref">Figure 13.24: Upload option</p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">You can choose or drag and drop a file, then upload it.</p>
      </li>
      <li class="list">Upload <code class="Code-In-Text--PACKT-">TF_2_graphs.ipynb</code>.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">You can download the<a id="_idIndexMarker648"/> program from this link and then upload it: <a href="https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/blob/master/CH1"><span class="url">https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/blob/master/CH13/TF_2_graphs.ipynb</span></a></p>
      </li>
      <li class="list">Once the program is open, you will see the following page:<figure class="mediaobject"><img src="../Images/B15438_13_25.png" alt=""/></figure>
        <p class="packt_figref">Figure 13.25: A Colaboratory notebook</p>
      </li>
      <li class="list">Go to <strong class="screen-text">File</strong> in the menu and save the notebook to your Google Drive:</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B15438_13_26.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.26: The File menu</p>
    <p class="normal">Once the file is saved, you are ready to go!</p>
    <p class="normal">You have many <a id="_idIndexMarker649"/>runtime options, such as making a choice between using a CPU or a GPU, display options (background, for example), and many more ways to use Google Colaboratory. I recommend reading the document to find the many options available.</p>
    <p class="normal">You are on your free Colaboratory server, and you're ready to explore your notebook.</p>
    <h2 id="_idParaDest-264" class="title">Defining and training the model</h2>
    <p class="normal">We will run the <a id="_idIndexMarker650"/>notebook and then analyze the results. This should provide an <a id="_idIndexMarker651"/>introduction to both Google Colaboratory and some TensorBoard functions.</p>
    <p class="normal">First, run the program by clicking on the <strong class="screen-text">Run all</strong> option in the <strong class="screen-text">Runtime</strong> menu:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_27.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.27: Runtime options</p>
    <p class="normal">The program will then go through its cells and provide information on the training process. To obtain this information, we will explore some of TensorBoard's key functions.</p>
    <p class="normal">We will first install TensorFlow and then run TensorBoard.</p>
    <ul>
      <li class="list"><strong class="bold">Installing TensorFlow and getting TensorBoard running</strong>: As you saw in the previous section, you<a id="_idIndexMarker652"/> do not need to run the program cell by cell unless a cell contains an error. In that case, click on the run button in the cell:</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_13_28.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.28: Running a cell</p>
    <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The cell will execute the code. In this case, it will install TensorFlow 2.x:</p>
    <pre class="programlisting"><code class="hljs cmake"><span class="hljs-comment"># Ensure TensorFlow 2.0 is installed.</span>
!pip <span class="hljs-keyword">install</span> -q tf-nightly-<span class="hljs-number">2.0</span>-preview
<span class="hljs-comment"># Load the TensorBoard notebook extension.</span>
%load_ext tensorboard
</code></pre>
    <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Once the <a id="_idIndexMarker653"/>installation is finished and TensorBoard is loaded, the program runs through the headers to import the necessary modules.</p>
    <div class="Info-Box-Indent-1--PACKT-">
      <p class="Information-Box--PACKT-">Be careful with TensorBoard versions! You might have a previous or different version installed that you are using for another project. Before running this program, check any application that is using TensorBoard in your environment. Check your configuration carefully before doing anything. If there is a risk, use another environment or just read the notebook without running it.</p>
    </div>
    <ul>
      <li class="list"><strong class="bold">The program now defines a simplified model you are now familiar with</strong>: The following model has been simplified. The goal here is to show how TensorBoard works. You can, of course, add more layers once you have explored the notebook:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># Define the model.</span>
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(32, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'softmax'</span>)
])
</code></pre>
      </li>
      <li class="list"><strong class="bold">The model is then compiled with an optimizer and accuracy metrics</strong>: The model now needs to be compiled and run to provide measurement output:
        <pre class="programlisting"><code class="hljs routeros">model.compile(
    <span class="hljs-attribute">optimizer</span>=<span class="hljs-string">'adam'</span>,
    <span class="hljs-attribute">loss</span>=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
    metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">For more on the Adam optimizer and cross-entropy, see <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>.</p>
      </li>
    </ul>
    <p class="normal">The model is now trained and ready for metric callbacks for TensorBoard.</p>
    <p class="normal">While the program was running the training, it was saving a log of the main functionalities to display. The graph of the model can be displayed in TensorBoard with one line along with many other functions.</p>
    <pre class="programlisting"><code class="hljs haml"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> --logdir logs
</code></pre>
    <p class="normal">The following graph of the model contains many details:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_29.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.29: TensorFlow graph</p>
    <p class="normal">If you want to have a simplified view, a conceptual view of the model is also displayed:</p>
    <figure class="mediaobject"><img src="../Images/B15438_13_30.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.30: A partial view of the TensorFlow graph</p>
    <p class="normal">We have explored the architecture of a neural network model using TensorBoard graphs. Let's see how to visualize the measurements of the training process of our model.</p>
    <h2 id="_idParaDest-265" class="title">Introducing some of the measurements</h2>
    <p class="normal">While training, the program saved key information in its log directory that can now be displayed.</p>
    <ul>
      <li class="list"><strong class="bold">Epoch accuracy</strong>: If the <a id="_idIndexMarker654"/>accuracy increases with the epochs, the classifier is progressing, and it is learning correctly. If it decreases, we are in trouble! We will have to go back and check the dataset, the activation functions, and how the layers were designed.</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_13_31.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.31: Accuracy of the model</p>
    <ul>
      <li class="list"><strong class="bold">Basic flow, including an activation function</strong>: TensorBoard<a id="_idIndexMarker655"/> has drill-down functionality. You can drill down into the actual operations TensorFlow 2.x is calculating:</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_13_32.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.32: The activation function</p>
    <ul>
      <li class="list"><strong class="bold">Exploring the details of an activation function</strong>: Once you have seen the flow of an operation, you can even peek inside it to see how it is built:</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_13_33.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.33: The activation function</p>
    <p class="normal">These TensorBoard graphs and <a id="_idIndexMarker656"/>measurements help you dig into the mechanics of your model. They provide insights that will add to the ones you acquired when exploring the outputs of layers in the first section of this chapter.</p>
    <p class="normal">In this section, we have explored the architecture functions of TensorBoard through the many graphs available as well as tools to measure the training performance of our model.</p>
    <h1 id="_idParaDest-266" class="title">Summary</h1>
    <p class="normal">In this chapter, we explored deep learning from the inside. We saw that building a CNN is now easy with TensorFlow 2.x, but peeking inside the way it "thinks" gives critical insight.</p>
    <p class="normal">We first built a CNN with many layers. The level of abstraction of a CNN increases through each layer. Reducing the number of dimensions per layer makes patterns appear. A neural network can be described as a process that goes from chaos to meaning.</p>
    <p class="normal">After building the CNN, we wrote a program that can read the "mental" images of the layers. The output of each layer shows how the network is creating patterns and structures. Since we humans often think using mental images, the output images of the CNN help us understand how a machine learns.</p>
    <p class="normal">Finally, we used a Google Colaboratory server to visualize the measurements of the CNN's learning process with TensorBoard running on top of TensorFlow 2.x. Measuring the accuracy of the training process of a CNN is critical. Visualizing these measurements makes it easier to see what is going wrong. TensorBoard provides a graph of a model to help us go from source code to a mental representation of an ANN.</p>
    <p class="normal">To sum this chapter up in a nutshell, we can say that artificial intelligence, through mathematics, transforms the chaos surrounding us into intelligible structures and patterns.</p>
    <p class="normal">In the next chapter, we are going to go further and learn how to visualize another aspect of a neural network: weights. We are going to use the weights of a restricted Boltzmann machine (RBM) to create a visual representation in TensorBoard using principal component analysis.</p>
    <h1 id="_idParaDest-267" class="title">Questions</h1>
    <ol>
      <li class="list">A CNN always has the same number of layers. (Yes | No)</li>
      <li class="list">ReLU is the best activation function. (Yes | No)</li>
      <li class="list">It is not necessary to compile a sequential classifier. (Yes | No)</li>
      <li class="list">The output of a layer is best viewed without running a prediction. (Yes | No)</li>
      <li class="list">The names of the layers mean nothing when viewing their outputs. (Yes | No)</li>
      <li class="list">TensorFlow 2.x does not include Keras. (Yes | No)</li>
      <li class="list">Google Colaboratory is just a repository, like GitHub. (Yes | No)</li>
      <li class="list">Google Colaboratory cannot run notebooks. (Yes | No)</li>
      <li class="list">It is possible to run TensorBoard in Google Colaboratory notebooks. (Yes | No)</li>
      <li class="list">Accuracy is displayed in TensorBoard. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-268" class="title">Further reading</h1>
    <ul>
      <li class="list">For more information on activation functions, visit <a href="https://keras.io/activations/"><span class="url">https://keras.io/activations/</span></a>.</li>
      <li class="list">Click on this link for more information on Google Colaboratory: <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true"><span class="url">https://colab.research.google.com/notebooks/welcome.ipynb#recent=true</span></a>.</li>
    </ul>
  </div>
</body></html>