["```py\n    ARG OPEN_MPI_PATH=/opt/amazon/openmpi/\n    ENV NCCL_VERSION=2.7.8\n    ENV EFA_VERSION=1.11.2\n    ENV BRANCH_OFI=1.1.1\n    ```", "```py\n    RUN cd $HOME \\\n      && curl -O https://s3-us-west-2.amazonaws.com/aws-efa-installer/aws-efa-installer-${EFA_VERSION}.tar.gz \\\n      && tar -xf aws-efa-installer-${EFA_VERSION}.tar.gz \\\n      && cd aws-efa-installer \\\n      && ./efa_installer.sh -y --skip-kmod -g \\\n    ENV PATH=\"$OPEN_MPI_PATH/bin:$PATH\"\n    ENV LD_LIBRARY_PATH=\"$OPEN_MPI_PATH/lib/:$LD_LIBRARY_PATH\"\n    ```", "```py\n    RUN cd $HOME \\\n      && git clone https://github.com/NVIDIA/nccl.git -b v${NCCL_VERSION}-1 \\\n      && cd nccl \\\n      && make -j64 src.build BUILDDIR=/usr/local\n    ```", "```py\n    RUN apt-get update && apt-get install -y autoconf\n    RUN cd $HOME \\\n      && git clone https://github.com/aws/aws-ofi-nccl.git -b v${BRANCH_OFI} \\\n      && cd aws-ofi-nccl \\\n      && ./autogen.sh \\\n      && ./configure --with-libfabric=/opt/amazon/efa \\\n           --with-mpi=/opt/amazon/openmpi \\\n           --with-cuda=/usr/local/cuda \\\n           --with-nccl=/usr/local --prefix=/usr/local \\\n      && make && make install\n    ```", "```py\n    RUN cd $HOME \\\n      && git clone https://github.com/NVIDIA/nccl-tests \\\n      && cd nccl-tests \\\n      && make MPI=1 MPI_HOME=/opt/amazon/openmpi CUDA_HOME=/usr/local/cuda NCCL_HOME=/usr/local\n    ```", "```py\n    from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n    ```", "```py\n    hyperparameters = {\n        \"epochs\": 5,\n        \"train_batch_size\": 24,\n        \"model_name\": \"bert-base-cased\",\n    }\n    # Scale the learning rate by batch size, as original LR was using batch size of 32\n    hyperparameters[\"learning_rate\"] = float(\"5e-5\") / 32 * hyperparameters[\"train_batch_size\"]\n    ```", "```py\n    sm_training_compiler_estimator = HuggingFace(\n        entry_point=\"train.py\",\n        instance_type=\"ml.p3.2xlarge\",\n        instance_count=1,\n        role=role,\n        py_version=\"py38\",\n        transformers_version=\"4.11.0\",\n        pytorch_version=\"1.9.0\",\n        compiler_config=TrainingCompilerConfig(),\n        hyperparameters=hyperparameters,\n        disable_profiler=True,\n        debugger_hook_config=False,\n    )\n    ```", "```py\nFound configuration for Training Compiler\nConfiguring SM Training Compiler...\n```"]