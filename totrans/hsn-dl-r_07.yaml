- en: Multilayer Perceptron for Signal Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will show you how to build a multilayer perceptron neural network
    for signal detection. We will first discuss the architecture of multilayer perceptron
    neural networks. Then we will cover how to prepare the data, how to decide on
    hidden layers and neurons, and how to train and evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: The section on preparing the data will be important going forward as these deep
    learning models require data to be in particular formats in order to pass the
    data to the models. The hidden layer is the part of the neural network that separates
    it from other machine learning algorithms, and in this chapter, we will show you
    how to search for the optimal number of nodes in a hidden layer. In addition,
    over the course of this chapter, you will become much more familiar with the MXNet
    syntax, including the model training and evaluation steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multilayer perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing and processing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding on the hidden layers and neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files of this chapter at the corresponding GitHub link
    at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A multilayer perceptron is an instance of a feedforward neural network that
    only uses fully connected layers consisting of perceptrons. A perceptron is a
    node that takes input values and multiplies them by weights, and then passes this
    aggregate value to an activation function that returns a value that indicates
    how much this set of inputs and weights matches the pattern we are trying to find.
  prefs: []
  type: TYPE_NORMAL
- en: The multilayer perceptron can be thought of as the most basic neural network
    implementation. As we mentioned, all layers are fully connected, which means that
    there are no convolution or pooling layers. It is also a feedforward model, which
    means that information from backpropagation is not looped back at every step,
    as it is in a recurrent neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity can be an asset in terms of the ease of the interpretability of the network
    and its initial setup; however, the main drawback is that with so many fully connected
    layers, the count of weights will grow to such a level that the model will take
    a long time to train for large datasets. It also has a vanishing gradient problem,
    which means that the model will reach a point where the value that is passed back
    to correct the model is so small that it no longer significantly impacts the results.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we will use the Adult dataset. We will walk through the steps
    to get this dataset in the proper form so that we can train a multilayer perceptron
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first load the libraries that we need. We will use the `mxnet` package
    to train the MLP model, the `tidyverse` family of packages for our data cleaning
    and manipulation, and `caret` to evaluate our model. We load the libraries using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will not produce any output to the console; however, you will see
    a checkmark next to these libraries in the Packages pane, indicating that the
    packages are now ready to use. Your Packages pane should look like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/989aa574-bd75-4019-a461-f2c865aeba62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will load our training and test data. In addition, we will add a column
    called `dataset` that we will populate with the `train` and `test` values to label
    our data. We will do this so that we can combine our data and perform some manipulation
    on the full data to save ourselves from repeating steps, and then be able to split
    the data again afterward. We then load the data and add the label with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will place two data objects in your Environment pane. This
    will be the train and test data that we will use for this modeling exercise. Your
    Environment pane will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5528f5aa-b276-40ec-b87f-0c5476e831c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this step, we will combine our data with a row bind and then remove any
    rows with NA values. Removing rows with missing values is not always the most
    appropriate course of action; at times, other tactics should be used to handle
    missing values. These tactics include **imputation** and **replacement**. In this
    case, we would just like to remove these rows for ease of use, since we are just
    using this data for example purposes. We combine the data and remove rows with
    missing values using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will add one more data object to our Environment pane. Your
    Environment pane will now look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/427aa6d5-af4b-4b3e-a1be-f66395687c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the data object `all` contains rows from `test` and `train`.
    We can now modify both `train` and `test` at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we take any factor value and trim the whitespace. We do this
    because there are multiple values that should mean the same thing but are showing
    up as distinct values because of whitespace, such as `Male` and ` Male`. We will
    demonstrate that whitespace is causing an issue when accurately defining categories
    and then correct the issue with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the first line of the preceding code, we will print the distinct
    factor levels to the console. Your console will look similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e7c5839-7c1a-4d51-bcf8-a17cbb8456cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, after running the second line and removing the whitespace, we will
    see that this has been corrected, and the output will look different. If you run
    the `unique()` function again, the output in your console will look like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ea849c2-c230-40ee-a6f6-edd8bec4b1ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Correcting this issue will help the algorithm to use the proper number of categories
    when creating the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we filter just our training data. We will extract the target variable
    to a vector and convert the values to numeric. Afterward, we can remove the target
    variable from the dataset, as well as the dataset variable. We extract the train
    data, create the target variable vector, and remove the unneeded columns using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run this code, we will see that the `train` data has been updated
    in our `Environment` pane and the `train_target` vector has been added. Your `Environment`
    pane will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cceeb682-871e-4583-b29e-2363fc2e411f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the train data has two fewer variables now that we have removed
    the target and the dataset. The `target` column is now extracted to its own vector
    and we have the dependent variable and independent variables in separate data
    objects so that they are in the proper format and ready to be passed to the modeling
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to separate the data column-wise so that one subset contains
    only columns containing numeric values while the other contains only columns containing
    string values. As stated earlier, all values will need to be numeric, so we will
    be using one-hot encoding on the string values, which is also known as creating
    dummy variables. This will create a column for every possible field name–value
    pair and populate this column with either a `1` or `0`, representing whether the
    value is present for the given field name per row. We split our data column-wise
    in the way described here using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run the preceding code, we will see two new data objects—one with
    the 6 numeric columns among the 14 total and the other with the remaining 8 columns,
    which contain string values. Your Environment pane will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b77bc154-3b9b-4e8e-80ea-bbd5dfa878f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that our data is in this format, we can one-hot encode the columns that
    contain only strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we will actually create our dummy variables. We use the `dummyVars()`
    function from the caret package. This takes two steps. In the first, we define
    the columns that we would like converted to dummy variables. Since we would like
    all columns to be converted, we just include a dot after the tilde. Next, we use
    the `predict()` function to actually create the new variables using the formula.
    We create our new dummy variable columns using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will have two new data objects in our
    Environment pane. One is the `ohe` object, which is a list with all the details
    that we need to convert our string columns to dummy variables, and the other is
    the `train_ohe` object, which contains the dummy variables. Your Environment pane
    will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24d01370-cffd-45fa-8dd4-8d9aefdf3c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that creating dummy variables results in a dataset with many more
    columns than our original data. As stated, we take every column name and value
    pair and create a new column, which results in the growth of columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the columns containing string values have been converted to columns with
    numeric values, then we can column-bind both subsets back together again. We combine
    the data that was already numeric and the data that was converted to a numeric
    format using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `train` object in our Environment pane has changed. Your
    Environment pane will now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02f29f52-6162-4468-bb11-5b3183ef0f51.png)'
  prefs: []
  type: TYPE_IMG
- en: The train data now contains all numeric columns and is in the proper format
    to be used with a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, for the columns that originally held numeric values, we will rescale
    the values so that all values are in a range between `0` and `1`, and as such
    are on the same scale as our one-hot encoded columns. We get all of our data on
    the same scale using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we run the preceding code, let''s first see what the train data columns
    look like in our Environment pane. Yours will look like the following screenshot
    before running the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd3cc60c-0bfc-4778-aaa0-a9c37cbd6a09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Your Environment pane will look like the following screenshot after running
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c752c279-20c6-473c-8f59-667ce1ce0966.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that all the values that were on different scales in the first image
    are now all rescaled so that all values are between `0` and `1`. Having all values
    on the same scale will help to make training the model more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now repeat the same steps for the test dataset. We prepare our test
    data for modeling using the same steps as the training data by running the following
    lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of running this code, you will see test data objects that have
    been modified in the same way as the training data objects. Your Environment pane
    will now look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db7590e8-e1ac-4168-b993-6534fce3a247.png)'
  prefs: []
  type: TYPE_IMG
- en: All of our data is now in the proper format and ready to be used for training
    our neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one last cleanup step. If we look at our column count, we can see
    that the `train` dataframe has one more column than the test dataframe. We can
    use the `setdiff` function to see which column exists in the `train` and not in
    the `test` set. Once that has been identified, we can just remove that column
    from the `train` set. We need our data to have the same number of columns for
    modeling. We find the column that doesn''t exist in both datasets and remove it
    using the following two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the first line of code, we will print the value of the output to
    our console. Your console output will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b94150a-edb4-4a3c-bc8e-a2de1c69d51d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now know that the `train` has the column `native.countryHoland.Netherlands` while
    `test` does not. We use the second line of code to remove this column from the
    `train`. After we run the second line of code, we will notice a difference in
    our Environment pane. Your Environment pane will now look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8d30171-6ba2-47ab-9ec8-0872b958a569.png)'
  prefs: []
  type: TYPE_IMG
- en: When we look at `train` and `test` now, we can see that both data objects have
    the same number of columns, which is required for using the two data objects to
    train and test our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last data preparation step is to take our target variable vectors and subtract `1`
    from all values. When we first cast these to numeric format, we took their factor
    level values so that we got vectors coded with values of either `1` or `2`; however,
    we want these to be coded as either `0` or `1` so that they are at the same scale
    as our independent variables. We get our target variables on the same scale as
    our independent variables by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the following code, you will notice one last change to your Environment
    pane. Your Environment pane will now look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59e1b6ed-2c81-4007-b77f-a5df543d743c.png)'
  prefs: []
  type: TYPE_IMG
- en: All of our data is now numeric and on the same scale, so the data is completely
    prepared for modeling at this point.
  prefs: []
  type: TYPE_NORMAL
- en: We started with data in the state that it was originally stored in and took
    some steps to get the data into the proper format so that we could use it to train
    a neural network. Neural networks, especially deep-learning implementations, offer
    the convenience of not needing to perform feature engineering like you might have
    to do with other machine-learning techniques. That being the case, some data preparation
    is still often required as neural networks require all data to be stored as numeric
    values, and your model will perform better if all numeric value data is on the
    same scale. The data manipulation and transformation steps that we just performed
    are representative of the type of data preparation work that will need to be completed
    for other data that you will use for training neural networks. With our data in
    the proper format, we can now devise a system to search for the optimal number
    of nodes for the hidden layer in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding on the hidden layers and neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multilayer perceptrons provide only a few choices during the model design process:
    the activation function used in the hidden layers, the number of hidden layers,
    and the number of nodes or artificial neurons in each layer. The topic of selecting
    the optimal number of layers and nodes will be covered in this section. We can
    begin with a single layer and use a set of heuristics to guide our starting point
    for selecting the number of nodes to include in this hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: When beginning this process, a good starting point is 66% of the length of the
    input or the number of independent variable columns. This value, in general, will
    fall within a range between the size of the output to two times the size of the
    input; however, 66% of the length of the input is a good starting point within
    this range.
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean that this starting point will always be the optimal number
    of nodes to use. To discover the optimal number, we can write a function that
    will train our model using a different number of nodes right around our starting
    point in order to see trends and attempt to find the optimal value. In this case,
    we will train with a larger learning rate using only a few rounds for fast run-time.
    If you are working with a large dataset, then it may be necessary to use a subset
    of the data when using this strategy so that long run times per iteration do not
    create an issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now walk through the creation of a function to test the performance
    of the model given a number of different nodes in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let''s look at the number of independent variable columns and then
    get 66% of this value to arrive at our starting point. We decide on the starting
    point for the number of nodes to include in our hidden layer by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print the following output to your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7531e213-13f8-491c-9efc-96204c5e25a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The precise value is `67.98`, but we will round this up to `70` as our starting
    value. Keep in mind that you can use any value you like, as this is just a heuristic—working
    with round numbers is convenient, and you can always drill down to the exact optimal
    number of neurons at a later time—however, the performance difference when making
    small changes will be minimal and may not be present when generalizing with this
    model later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s choose two values that are larger than this starting point as
    well as two that are smaller and store these in a vector. These will be the options
    that we will pass through as arguments to our function. In this case, we started
    with `70`, so we will also include `50`, `60`, `80`, and `90`. We create the vector
    of possible nodes for our hidden layer by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will see that this data object is now
    in our Environment pane. Your Environment pane will now look like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3babbfa2-371d-43bf-9fa7-e06d5470bcef.png)'
  prefs: []
  type: TYPE_IMG
- en: We will use the values from this vector later to loop through these choices
    and see which performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set our seed at this point to ensure reproducibility. This is always
    important, and should always be done when working with any type of model that
    introduces quasirandom numbers. In our case, it is important for this demonstration
    to show that the function we create produces the same result as running the code
    alone. We set the seed specifically for use with our MXNet model by running the
    following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After running this code, no output is printed to the console and there are no
    noticeable changes in RStudio; however, using this method, we ensure consistent
    model results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before writing our function, we will first define and run our model and look
    at the syntax and options for training a multilayer perceptron using the `mxnet`
    package. We define and run our multilayer perceptron model using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will see model details printed to our
    console for every run. The output to your console will look like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc49ba07-9139-4554-9c98-dbb9f22d7795.png)'
  prefs: []
  type: TYPE_IMG
- en: The output to the console lists all the accuracy values using a holdout set
    from the `train` data. We will cover all the options for modeling with MXNet after
    this code to cover each argument in more detail. Simply put, we are using values
    to make the model run quickly at this point, while we are preparing to test for
    the optimal number of nodes to include.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to training this model, we would also like a data object to hold
    performance results so that we can compare the performance after trying the different
    hidden layer sizes. Here, we can make a prediction using our model and then select
    the class with the highest likelihood. Lastly, we calculate the accuracy by summing
    up the cases where the prediction is correct over the length of the test target
    variables. We can also see how we can now store these two values in a table. We
    do this here to demonstrate the entire inside of our function, which will hold
    all the different node size choices along with the accuracy of using that given
    number of nodes. We make predictions and calculate the accuracy using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will have four new data objects in our
    Environment pane. Your Environment pane will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee70f05b-0c90-4498-85cf-f6b13c457734.png)'
  prefs: []
  type: TYPE_IMG
- en: The `preds` object holds the results from making predictions with our model.
    MXNet stores these prediction results in a matrix with the probabilities for each
    class. If we transpose this matrix or rotate it 90 degrees and select the maximum
    value for every column, then we will get the highest row number that corresponds
    with the most likely class; however, this will be using values `1` and `2` for
    the rows, so we subtract 1 from all values to get our prediction values on the
    same scale as our true test classes, which are `0` or `1`. For the accuracy value,
    we sum all the cases where the predicted values and true values are the same over
    the total number of true cases. Finally, we can put the node count and accuracy
    score in a table. This will be useful for comparing results when we try different
    node counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have everything coded for an individual case, we can create our
    function by replacing the value assigned to the argument that we would like to
    test with a variable. We then move that variable to be the argument for our new
    function. We can see that everything in the code is the exact same as it was previously,
    except that the `70` that we had as a value for the `hidden_node` argument, and
    that we had later as a value to add under the `nodes` column in the new table
    we will create, are now replaced with `x`. The `x` is then moved outside and added
    as an argument for our new function. In this way, we can now pass any value to
    our new `mlp_loop()` function and have it replace the two instances of `x` in
    our code. We write our custom function to try different values for the `hidden_node`
    argument using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run the preceding code, we will see the change in our Environment
    pane. Your Environment pane will now look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b33b27f-1380-4193-a2eb-ce826e38f194.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that we now have a custom function defined and stored in our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can first test our function with the value from the previous run. We
    test our function by supplying the value `70` to the function that we just made
    using the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run the following code, we will get a printout on our console with
    the accuracy value and the number of nodes that we included in the hidden layer,
    as well as the results of testing for equality. Your console will look like the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7700478a-3334-4134-922a-cfe0332d5964.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the results of the code that we just ran, we can see that our function
    produces the same results that it would produce by just passing the values to
    the modeling code directly. This makes sense, as we are just swapping in `70`
    for all places where `x` is in the model function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that it is confirmed that our new function is working (as we get the same
    result when passing the value `70` through the function that we get by just having
    it in the code), we can now pass our entire vector of values through the function.
    In order to do so, we will use the `map()` function from the `purrr` package,
    which makes iterating very simple and straightforward. In this case, we will use
    the `map_df()` function in order to get a dataframe, after looping all values
    through the function call. We loop through our function, passing in all values
    from the vector we created earlier by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code, we will see a printout to the console just
    like in *step 4* for all five model runs. After this, we will get the `results`
    dataframe, which now has all the accuracy scores for all the node count attempts.
    In your console, you may notice some rounding, which prevents an immediate determination
    of which model performed best. Let''s click on the results from our Environment
    pane instead and view the data that way. After clicking on `results`, you should
    see a table similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d23427e0-cf3d-4a59-ad58-20f362e8ac02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While we were able to create a loop to pass different node count values to
    the `mlp()` function to determine the optimal count, we will see in the next step
    that using a similar loop technique for finding the optimal layers is not as straightforward.
    To add layers, we must abandon the convenience of the `mlp()` function and create
    our multilayer perceptron one layer at a time. We can create an MLP one layer
    at a time using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will have a number of new data objects
    in our Environment pane and an accuracy score printed to our console. Your console
    will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a223a863-7dc1-4156-94e1-19c920739a33.png)'
  prefs: []
  type: TYPE_IMG
- en: From this output, we can see that using the two highest-scoring values from
    our test of node counts in two separate layers did not improve our score. Adding
    more layers will not always lead to a better performing model, though you can
    continue to experiment with different layers using the preceding code as a guide
    to try to see if you can improve the score. Let's review what the preceding code
    is doing and how it differs from the model we created using the `mlp()` function.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we initiate our model by creating a symbolic variable. We then
    create two fully connected layers with 90 and 50 nodes respectively. We then define
    an output layer using the softmax activation function. We then use the `FeedForward()`
    function to define the other options that we used previously. In doing this, we
    can see that most of the arguments can be passed to `FeedForward` while the `hidden_node`
    argument moves to the `FullyConnected()` function for as many layers as you want
    and the `out_node` and `out_activation` arguments move to an output function,
    which in this case is `SoftmaxOutput`.
  prefs: []
  type: TYPE_NORMAL
- en: Using our prepared data, we looked at how to test for the optimal number of
    nodes for a hidden layer. We also looked at how we need to change our code to
    add additional layers. With MLPs, there are fewer options than other neural network
    implementations, so we have focused on making changes to the hidden layers to
    try to optimize our model using the main strength of neural network models. In
    the next step, we will take everything we have learned while tuning parameters
    to run a model that will maximize performance while giving a more in-depth explanation
    of our model options using MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After parameter tuning, we can now run the model for maximum performance. In
    order to do so, we will make a few important changes to the model options. Ahead
    of making the changes, let''s have a more in-depth review of the model options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_node`: These are the number of nodes in the hidden layer. We used a
    looping function to find the optimal number of nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_node`: These are the number of nodes in the output layer and must be set
    equal to the number of target classes. In this case, that number is `2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_activation`: This is the activation function to use for the output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num.round`: This is the number of iterations we take to train our model. In
    the parameter tuning stage, we set this number low so that we could quickly loop
    through a number of options; to get maximum accuracy, we would allow the model
    to run for more rounds while at the same time dropping the learning rate, which
    we will cover soon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`array.batch.size`: This sets the batch size, which is the number of rows that
    are trained at the same time during each round. The higher this is set, the more
    memory will be required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning.rate`: This is the constant value applied to the gradient from the
    loss function that is used to adjust weights. For parameter tuning, we set this
    to a large number to move quickly along the cost surface in a small number of
    rounds. To achieve the best performance, we will set this to a lower number to
    make more subtle adjustments while learning new weights so we don''t constantly
    overadjust the values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`momentum`: This uses the decaying values from previous gradients to avoid
    sudden shifts in movement along the cost surface. As a heuristic, a good starting
    value for `momentum` is between `0.5` and `0.8`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval.metric`: This is the metric that you will use to evaluate performance.
    In our case, we are using `accuracy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, that we have covered the options included in our model using the `mlp()`
    function, we will make adjustments to improve accuracy. In order to improve accuracy,
    we will increase the number of rounds while simultaneously dropping the learning
    rate. We will keep the other values constant and use the node count that led to
    the best performance from our loop earlier. You can set the model for better performance
    using what we learned when parameter tuning using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, you will see a printout in your console with the accuracy
    score after running the model with the adjustments to the parameters. Your console
    output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d45fe33-d287-4bb3-a3ae-0a911f7975de.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our accuracy has improved from our adjustments. Before, when
    we were testing parameters, the best accuracy we could achieve was 84.28%, and
    we can see that we now have an accuracy score of 85.01%.
  prefs: []
  type: TYPE_NORMAL
- en: After preparing our data so that it is in the proper format to model with MXNet,
    and then parameter tuning to find the best values for our model, we then made
    adjustments to further improve performance using what we learned earlier. All
    of these steps together describe a complete cycle of manipulating and transforming
    data, optimizing parameters, and then running our final model. We saw how to use
    MXNet, which offers a convenience function for simple MLPs and also offers the
    functionality to build MLPs with additional hidden layers using the activation,
    output, and feedforward functions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multilayer perceptrons are the simplest form of neural networks. They are feedforward
    without the feedback loops of recurrent neural networks, and all hidden layers
    are dense, fully connected layers, unlike convolutional neural networks, which
    feature convolutional layers and pooling layers. Given their simplicity, there
    are fewer options to adjust; however, in this chapter, we focused on adjusting
    the nodes in the hidden layer and looked at adding additional layers, as this
    aspect is the main element that separates neural network models, and as such,
    all deep learning methods from other machine learning algorithms. Using all the
    code in this chapter, you have learned how to process data so that it was ready
    to model, how to select the optimal number of nodes and layers, and how to train
    and evaluate a model using the `mxnet` library for R.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to code deep autoencoders. This model
    is a form of unsupervised learning that is used to automatically categorize our
    input data. We will use this clustering process to code a recommender system using
    collaborative filtering.
  prefs: []
  type: TYPE_NORMAL
