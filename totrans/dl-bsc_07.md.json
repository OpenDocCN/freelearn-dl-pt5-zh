["```py\nclass SGD:\n    def __init__ (self, lr=0.01):\n        self.lr = lr\n    def update(self, params, grads): \n        for key in params.keys():\n            params[key] -= self.lr * grads[key]\n```", "```py\nnetwork = TwoLayerNet(...)\noptimizer = SGD()\nfor i in range(10000):\n    ...\n    x_batch, t_batch = get_mini_batch(...)  # Mini-batch\n    grads = network.gradient(x_batch, t_batch)\n    params = network.params\n    optimizer.update(params, grads)\n    ...\n```", "```py\nclass Momentum:\n    def __ init __ (self,  lr=0.01,  momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum \n        self.v = None\n    def update(self, params, grads): \n        if self.v is None:\n        self.v = {}\n        for key, val in params.items(): \n            self.v[key] = np.zeros_like(val)\n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n            params[key] += self.v[key]\n```", "```py\nclass AdaGrad:\n    def __init__ (self, lr=0.01): \n        self.lr = lr\n        self.h = None\n    def update(self, params, grads): \n        if self.h is None:\n        self.h = {}\n        for key, val in params.items(): \n            self.h[key] = np.zeros_like(val)\nfor key in params.keys():\n    self.h[key] += grads[key] * grads[key]\n    params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nx = np.random.randn(1000, 100) # 1000 data\nnode_num = 100 # Number of nodes (neurons) in each hidden layer\nhidden_layer_size = 5 # Five hidden layers exist\nactivations = {}\t# The results of activations are stored here\nfor i in range(hidden_layer_size):\n    if i != 0:\n        x = activations[i-1]\n    w = np.random.randn(node_num, node_num) * 1\n    z = np.dot(x, w)\n    a = sigmoid(z) # Sigmoid function!\n    activations[i] = a\n```", "```py\n# Draw histograms\nfor i, a in activations.items( ): \n    plt.subplot(1, len(activations), i+1)\n    plt.title(str(i+1) + \"-layer\")\n    plt.hist(a.flatten(), 30, range=(0,1))\nplt.show()\n```", "```py\n# w = np.random.randn(node_num, node_num) * 1\nw = np.random.randn(node_num, node_num) * 0.01\n```", "```py\nnode_num = 100 # Number of nodes in the previous layer\nw = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n```", "```py\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n# Reduce learning data to reproduce overfitting\nx_train = x_train[:300] \nt_train = t_train[:300]\n```", "```py\nnetwork = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\noptimizer = SGD(lr=0.01) # Use SGD with the learning rate of 0.01 to update the parameters\nmax_epochs = 201\ntrain_size = x_train.shape[0]\nbatch_size = 100\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\niter_per_epoch = max(train_size / batch_size, 1\nepoch_cnt = 0\nfor i in range(1000000000):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    grads = network.gradient(x_batch, t_batch)\n    optimizer.update(network.params, grads)\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        epoch_cnt += 1\n        if epoch_cnt >= max_epochs:\n            break\n```", "```py\nclass Dropout:\n    def __init__ (self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n    def forward(self, x, train_flg=True): \n        if  train_flg:\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        else:\n            return x * (1.0 - self.dropout_ratio)\n    def backward(self, dout):\n        return dout * self.mask\n```", "```py\n(x_train, t_train), (x_test, t_test) = load_mnist()\n# Shuffle training data\nx_train, t_train = shuffle_dataset(x_train,  t_train)\n# Separate validation data\nvalidation_rate = 0.20\nvalidation_num = int(x_train.shape[0] * validation_rate)\nx_val = x_train[:validation_num]\nt_val = t_train[:validation_num]\nx_train = x_train[validation_num:]\nt_train = t_train[validation_num:]\n```", "```py\nweight_decay = 10 ** np.random.uniform(-8, -4)\nlr = 10 ** np.random.uniform(-6, -2)\n```", "```py\nBest-1 (val acc:0.83) | lr:0.0092, weight decay:3.86e-07\nBest-2 (val acc:0.78) | lr:0.00956, weight decay:6.04e-07\nBest-3 (val acc:0.77) | lr:0.00571, weight decay:1.27e-06\nBest-4 (val acc:0.74) | lr:0.00626, weight decay:1.43e-05\nBest-5 (val acc:0.73) | lr:0.0052, weight decay:8.97e-06\n```"]