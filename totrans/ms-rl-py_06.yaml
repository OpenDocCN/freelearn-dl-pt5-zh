- en: '*Chapter 5*: Solving the Reinforcement Learning Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we provided the mathematical foundations for modeling
    a reinforcement learning (RL) problem. In this chapter, we'll lay the foundations
    for solving it. Many of the following chapters will focus on some specific solution
    approaches that will build on this foundation. To this end, we'll first cover
    the **dynamic programming** (**DP**) approach, with which we'll introduce some
    key ideas and concepts. DP methods provide optimal solutions to **Markov decision
    processes** (**MDPs**) yet require the complete knowledge and a compact representation
    of the state transition and reward dynamics of the environment. This could be
    severely limiting and impractical in a realistic scenario, where the agent is
    either directly trained in the environment itself or in a simulation of it. The
    **Monte Carlo** and **temporal difference** (**TD**) approaches, which we'll cover
    later, unlike DP, use sampled transitions from the environment and relax the aforementioned
    limitations. Finally, we'll also talk in detail about what makes a simulation
    model suitable for RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, here are the sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your agent with Monte Carlo methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the importance of simulation in reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP is a branch of mathematical optimization that proposes optimal solution methods
    to MDPs. Although most real-world problems are too complex to optimally solve
    via DP methods, the ideas behind these algorithms are central to many RL approaches.
    So, it is important to have a solid understanding of them. Throughout this chapter,
    we'll go from these optimal methods to more practical approaches by systematically
    introducing approximations.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start this section by describing an example that will serve as a use case
    for the algorithms that we will introduce later in the chapter. Then, we will
    cover how to do prediction and control using DP.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Example use case – Inventory replenishment of a food truck
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our use case involves a food truck business that needs to decide how many burger
    patties to buy every weekday to replenish its inventory. Inventory planning is
    an important class of problems in retail and manufacturing that a lot of companies
    need to deal with all the time. Of course, for pedagogical reasons, our example
    is much simpler than what you would see in practice. However, it should still
    give you an idea about this problem class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s dive into the example:'
  prefs: []
  type: TYPE_NORMAL
- en: Our food truck operates downtown during the weekdays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every weekday morning, the owner needs to decide on how many burger patties
    to buy with the following options: ![](img/Formula_05_001.png). The cost of a
    single patty is ![](img/Formula_05_002.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The food truck can store the patties up to a capacity of ![](img/Formula_05_003.png)
    during the weekdays. However, since the truck does not operate over the weekend,
    and any inventory unsold by Friday evening spoils, if during a weekday, the number
    of patties purchased and the existing inventory exceeds the capacity, the excess
    inventory also spoils.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burger demand for any weekday is a random variable ![](img/Formula_05_004.png)
    with the following probability mass function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The net revenue per burger (after the cost of the ingredients other than the
    patty) is ![](img/Formula_05_006.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sales in a day is the minimum of the demand and the available inventory, since
    the truck cannot sell more burgers than the number of patties available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what we have is a multi-step inventory planning problem and our goal is
    to maximize the total expected profit (![](img/Formula_05_007.png)) in a week.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: The one-step inventory planning problem is often called "the newsvendor problem."
    It is about balancing the cost of overage and underage given a demand distribution.
    For many common demand distributions, this problem can be solved analytically.
    Of course, many real-world inventory problems are multi-step, similar to what
    we will solve in this chapter. You can read more about the newsvendor problem
    at [https://en.wikipedia.org/wiki/Newsvendor_model](https://en.wikipedia.org/wiki/Newsvendor_model).
    We will solve a more sophisticated version of this problem in [*Chapter 15*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329),
    *Supply Chain Management*.
  prefs: []
  type: TYPE_NORMAL
- en: So far so good. Next, let's create this environment in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the food truck environment in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What we are about to create is a simulation environment for the food truck example
    as per the dynamics we described above. In doing so, we will use the popular framework
    designed for exactly the same purpose, which is OpenAI's Gym library. Chances
    are you have probably come across it before. But if not, that is perfectly fine
    since it does not play a critical role in this example. We will cover what you
    need to know as we go through it.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI''s Gym is the standard library for defining RL environments and developing
    and comparing solution methods. It is also compatible with various RL solution
    libraries, such as RLlib. If you are not already familiar with the Gym environment,
    take a look at its concise documentation here: [https://gym.openai.com/docs/](https://gym.openai.com/docs/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go into the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the libraries we will need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a Python class, which is initialized with the environment parameters
    we described in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The state is a tuple of the day of the week (or the weekend) and the starting
    inventory level for the day. Again, the action is the number of patties to purchase
    before the sales start. This purchased inventory becomes available immediately.
    Note that this is a fully observable environment, so the state space and the observation
    space are the same. Possible inventory levels are 0, 100, 200, and 300 at the
    beginning of a given day (because of how we defined the action set, possible demand
    scenarios, and the capacity); except we start Monday with no inventory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s define a method that calculates the next state and the reward
    along with the relevant quantities, given the current state, the action, and the
    demand. Note that this method does not change anything in the object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define a method that returns all possible transitions and rewards for
    a given state-action pair using the `get_next_state_reward` method, together with
    the corresponding probabilities. Notice that different demand scenarios will lead
    to the same next state and reward if the demand exceeds the inventory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's all we need for now. Later, we will add other methods to this class to
    be able to simulate the environment. Now, we'll dive into DP with the policy evaluation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Policy evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In MDPs (and RL), our goal is to obtain (near) optimal policies. But how do
    we even evaluate a given policy? After all, if we cannot evaluate it, we cannot
    compare it against another policy and decide which one is better. So, we start
    discussing the DP approaches with **policy evaluation** (also called the **prediction
    problem**). There are multiple ways to evaluate a given policy. In fact, in [*Chapter
    4*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080), *Making of the Markov Decision
    Process*, when we defined the state-value function, we discussed how to calculate
    it analytically and iteratively. Well, that is policy evaluation! In this section,
    we will go with the iterative version, which we'll turn to next.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative policy evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's first discuss the iterative policy evaluation algorithm and refresh your
    mind on what we covered in the previous chapter. Then, we will evaluate the policy
    that the owner of the food truck already has (the base policy).
  prefs: []
  type: TYPE_NORMAL
- en: Iterative policy iteration algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall that the value of a state is defined as follows for a given policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_05_009.png) is the expected discounted cumulative reward starting
    in state ![](img/Formula_05_010.png) and following policy ![](img/Formula_05_011.png).
    In our food truck example, the value of the state ![](img/Formula_05_012.png)
    is the expected reward (profit) of a week that starts with zero inventory on Monday.
    The policy that maximizes ![](img/Formula_05_013.png) would be the optimal policy!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the Bellman equation tells us the state values must be consistent with
    each other. It means that the expected one-step reward together with the discounted
    value of the next state should be equal to the value of the current state. More
    formally, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_014.jpg)![](img/Formula_05_015.jpg)![](img/Formula_05_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know all the transition probabilities for this simple problem, we
    can analytically calculate this expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ![](img/Formula_05_018.png) term at the beginning is because the policy
    may suggest taking actions probabilistically given the state. Since the transition
    probabilities depend on the action, we need to account for each possible action
    the policy may lead us to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all we need to do to obtain an iterative algorithm is to convert the Bellman
    equation into an update rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*A single round of updates,* ![](img/Formula_05_020.png)*, involves updating
    all the state values*. The algorithm stops until the changes in state values are
    sufficiently small in successive iterations. We won''t go into the proof, but
    this update rule can be shown to converge to ![](img/Formula_05_021.png) as ![](img/Formula_05_022.png)
    This algorithm is called **iterative policy evaluation** with **expected update**
    since we take into account all possible one-step transitions.'
  prefs: []
  type: TYPE_NORMAL
- en: One last note before we implement this method is that rather than carrying two
    copies of the state values for ![](img/Formula_05_023.png) and ![](img/Formula_05_024.png),
    and replacing ![](img/Formula_05_025.png) with ![](img/Formula_05_026.png) after
    a full round of updates, we'll just make in-place updates. This tends to converge
    faster since we make the latest estimate for the value of a state immediately
    available to be used for the other state updates.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's evaluate a base policy for the inventory replenishment problem.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative evaluation of a base inventory replenishment policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine that the owner of the food truck has the following policy: At the beginning
    of a weekday, the owner replenishes the inventory up to 200 or 300 patties, with
    equal probability. For example, if the inventory at the beginning of the day is
    100, they are equally likely to purchase 100 or 200 patties. Let''s evaluate this
    policy and see how much profit we should expect in the course of a week:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define a function that returns a `policy` dictionary, in which the
    keys correspond to the states. The value that corresponds to a state is another
    dictionary that has actions as the keys and the probability of selecting that
    action in that state as the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the policy evaluation. We define a function that will calculate the expected
    update for a given state and the corresponding policy for that state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In other words, this function calculates ![](img/Formula_05_027.png) for a given
    ![](img/Formula_05_028.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The policy evaluation function executes the expected updates for all states
    until the state values converge (or it reaches a maximum number of iterations):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s elaborate on how this function works:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The `policy_evaluation` function receives an environment object, which will
    be an instance of the `FoodTruck` class in our example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The function evaluates the specified policy, which is in the form of a dictionary
    that maps states to action probabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) All the state values are initialized to 0 unless an initialization is passed
    into the function. The state values for the terminal states (states corresponding
    to the weekend in this example) are not updated since we don't expect any reward
    from that point on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) We define an epsilon value to use as a threshold for convergence. If the
    maximum change between updates among all the state values is less than this threshold
    in a given round, the evaluation is terminated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Since this is an episodic task with a finite number of steps, we set the
    discount factor `gamma` to 1 by default.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) The function returns the state values, which we will need later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we evaluate the base policy the owner has using this function. First,
    create a `foodtruck` object from the class we defined above:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the base policy for the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the base policy and get the corresponding state values – specifically
    for the initial state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The state-value of `("Mon", 0)`, which is the initial state, is 2515 under this
    policy. Not a bad profit for a week!
  prefs: []
  type: TYPE_NORMAL
- en: Great job so far! Now you are able to evaluate a given policy and calculate
    the state values corresponding to that policy. Before going into improving the
    policy, though, let's do one more thing. Let's verify that simulating the environment
    under this policy leads to a similar reward.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the policy evaluation against a simulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to be able to simulate the environment, we need to add a few more
    methods to the `FoodTruck` class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `reset` method, which simply initializes/resets the object to Monday
    morning with zero inventory. We will call this method before we start an episode,
    every time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a method to check if a given state is terminal or not. Remember
    that episodes terminate at the end of the week in this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, define the `step` method that simulates the environment for a one-time
    step given the current state and the action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The method returns the new state, one-step reward, whether the episode is complete,
    and any additional information we would like to return. This is the standard Gym
    convention. It also updates the state stored within the class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that our `FoodTruck` class is ready for the simulation. Next, let''s create
    a function that chooses an action from a – possibly probabilistic – policy given
    a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a function (outside of the class) to simulate a given policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `simulate_policy` function simply performs the following actions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Receives a policy dictionary that returns the actions and the corresponding
    probabilities the policy suggests for a given state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) It simulates the policy for a specified number of episodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Within an episode, it starts at the initial state and probabilistically selects
    the actions suggested by the policy at each step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The selected action is passed to the environment, which transitions into
    the next state as per the dynamics of the environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's simulate the environment with the base policy!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Great! This closely matches what we calculated analytically! Now it is time
    to use this iterative policy evaluation method for something more useful: finding
    optimal policies!'
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a way of evaluating a given policy, we can use it to compare
    two policies and iteratively improve them. In this section, we first discuss how
    policies are compared. Then, we introduce the policy improvement theorem and finally
    put everything together in the policy improvement algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Policy comparison and improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that we have two policies, ![](img/Formula_05_029.png) and ![](img/Formula_05_030.png),
    that we would like to compare. We say ![](img/Formula_05_030.png) is as good as
    ![](img/Formula_05_032.png) if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_033.png).'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the state values under a policy ![](img/Formula_05_034.png)
    are greater than or equal to the state values under another policy ![](img/Formula_05_035.png)
    for all possible states, then it means ![](img/Formula_05_036.png) is as good
    as ![](img/Formula_05_037.png). If this relation is a strict inequality for any
    state ![](img/Formula_05_028.png), then ![](img/Formula_05_039.png) is a better
    policy than ![](img/Formula_05_040.png). This should be intuitive since a state-value
    represents the expected cumulative reward from that point on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the question is how we go from ![](img/Formula_05_037.png) to a better
    policy ![](img/Formula_05_030.png). For that, we need to recall the action-value
    function we defined in [*Chapter 4*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080),
    *Making of the Markov Decision Process*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember that the definition of the action-value function is a bit nuanced.
    It is the expected cumulative future reward when:'
  prefs: []
  type: TYPE_NORMAL
- en: Action ![](img/Formula_05_044.png) is taken at the current state ![](img/Formula_05_045.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the policy ![](img/Formula_05_046.png) is followed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nuance is that policy ![](img/Formula_05_047.png) may normally suggest another
    action when in state ![](img/Formula_05_048.png). The q-value represents a one-time
    deviation from policy ![](img/Formula_05_035.png) that happens in the current
    step.
  prefs: []
  type: TYPE_NORMAL
- en: How does this help with improving the policy though? The **policy improvement
    theorem** suggests that *if it is better to select* ![](img/Formula_05_050.png)
    *initially when in state* ![](img/Formula_05_051.png) *and then follow* ![](img/Formula_05_052.png)
    *rather than following* ![](img/Formula_05_053.png) *all along, selecting* ![](img/Formula_05_050.png)
    *every time when in state* ![](img/Formula_05_055.png) *is a better policy than*
    ![](img/Formula_05_053.png). In other words, if ![](img/Formula_05_057.png), then
    we can improve ![](img/Formula_05_037.png) by taking action ![](img/Formula_05_059.png)
    when in state ![](img/Formula_05_060.png) and following ![](img/Formula_05_061.png)
    for the rest of the states. We don't include it here but the proof of this theorem
    is actually quite intuitive, and it is available *Sutton & Barto, 2018*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generalize this argument. Say some policy ![](img/Formula_05_030.png)
    is at least as good as another policy ![](img/Formula_05_061.png) if, for all
    ![](img/Formula_05_064.png), the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_065.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then, all we need to do to improve a policy is to choose actions that maximize
    the respective q-values for each state. Namely,
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One last note before we close this discussion: Although we described the policy
    improvement method for deterministic policies, only a single action is suggested
    by the policies for a given state ![](img/Formula_05_067.png), the method holds
    for stochastic policies as well.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good! Now, let's turn this policy improvement into an algorithm that
    will allow us to find optimal policies!
  prefs: []
  type: TYPE_NORMAL
- en: The policy iteration algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The policy iteration algorithm simply includes starting with an arbitrary policy,
    followed by a policy evaluation step, and then with a policy improvement step.
    This procedure, when repeated, eventually leads to an optimal policy. This process
    is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Generalized policy iteration
  prefs: []
  type: TYPE_NORMAL
- en: Actually, iterating between some forms of policy evaluation and policy improvement
    steps is a general recipe for solving RL problems. That is why this idea is named
    **generalized policy iteration (GPI)** *Sutton & Barto, 2018*. It is just that
    the policy iteration method we describe in this section involves specific forms
    of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement a policy iteration for the food truck environment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a policy iteration for the inventory replenishment problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already coded the policy evaluation and expected update steps. What
    we need additionally for the policy iteration algorithm is the policy improvement
    step, and then we can obtain an optimal policy! This is exciting, so, let''s dive
    right in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by implementing the policy improvement as we described above:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function searches for the action that gives the maximum q-value for a given
    state using the value functions obtained under the current policy. For the terminal
    states, the q-value is always equal to 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we put everything together in a policy iteration algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This algorithm starts with a random policy, and in each iteration, implements
    the policy evaluation and improvement steps. It stops when the policy becomes
    stable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: And the big moment! Let's find out the optimal policy for our food truck and
    see what the expected weekly profit is!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have just figured out a policy that gives an expected weekly profit of $2,880\.
    This is a significant improvement over the base policy! Well, thanks for supporting
    your local business!
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the output that there were two policy improvement steps over
    the random policy. The third policy improvement step did not lead to any changes
    in the policy and the algorithm terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the optimal policy looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Optimal policy for the food truck example
  prefs: []
  type: TYPE_NORMAL
- en: 'What the policy iteration algorithm comes up with is quite intuitive. Let''s
    analyze this policy for a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: On Monday and Tuesday, it is guaranteed that 400 burgers will be sold in the
    remainder of the week. Since the patties can be safely stored during the weekdays,
    it makes sense to fill the inventory up to capacity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the beginning of Wednesday, there is a chance that the total number of sales
    will be 300 till the end of the week and 100 patties will be spoiled. However,
    this is a rather small likelihood and the expected profit is still positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Thursday and Friday, it makes more sense to be more conservative and de-risk
    costly spoilage in case the demand is less than the inventory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations! You have successfully solved an MDP using policy iteration!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The optimal policy, we have found, heavily depends on the cost of patties, the
    net revenue per unit, as well as the demand distribution. You can gain more intuition
    on the structure of the optimal policy and how it changes by modifying the problem
    parameters and solving it again.
  prefs: []
  type: TYPE_NORMAL
- en: You have come a long way! We built an exact solution method starting with the
    fundamentals of MDP and DP. Next, we will look into another algorithm that is
    often more efficient than policy iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Policy iteration requires us to fully evaluate the policies until the state
    values converge before we perform an improvement step. In more sophisticated problems,
    it could be quite costly to wait for a complete evaluation. Even in our example,
    a single policy evaluation step took 5-6 sweeps over all states until it converged.
    It turns out that we can get away with terminating the policy evaluation before
    it converges without losing the convergence guarantee of the policy iteration.
    In fact, we can even combine policy iteration and policy improvement into a single
    step by turning the Bellman optimality equation we introduced in the previous
    chapter into an update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_068.jpg)![](img/Formula_05_069.jpg)![](img/Formula_05_070.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We simply perform the update for all states again and again until the state
    values converge. This algorithm is called **value iteration**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Notice the difference between a policy evaluation update and a value iteration
    update. The former selects the actions from a given policy, hence the ![](img/Formula_05_071.png)
    term in front of the expected update. The latter, on the other hand, does not
    follow a policy but actively searches for the best actions through the ![](img/Formula_05_072.png)
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: This is all we need to implement the value iteration algorithm. So, let's dive
    right into the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the value iteration for the inventory replenishment problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement value iteration, we'll use the `policy_improvement` function we
    defined earlier. However, after improving the policy for each state, we'll also
    update the state-value estimate of the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can go ahead and implement the value iteration using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the value iteration function as defined above with in-place
    replacement of the state values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we execute the value iteration and observe the value of the initial state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Value iteration gives the optimal policy, but with less computational effort
    compared to the policy iteration algorithm! It took only a total of 6 sweeps over
    the state space with the value iteration, while the policy iteration arrived at
    the same optimal policy after 20 sweeps (17 for policy evaluation and 3 for policy
    improvement).
  prefs: []
  type: TYPE_NORMAL
- en: Now, remember our discussion around generalized policy improvement. You can,
    in fact, combine the policy improvement step with a truncated policy evaluation
    step, which, in some sophisticated examples where the state values change significantly
    after the policy improvement, converge faster than both policy iteration and value
    iteration algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Great work! We have covered how to solve the prediction problem for MDPs using
    DP and then two algorithms to find optimal policies. And they worked great for
    our simple example. On the other hand, DP methods suffer from two important drawbacks
    in practice. Let's discuss what those are next, and why we need the other approaches
    that we will introduce later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of dynamic programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DP methods are great to learn to get a solid grasp of how MDPs can be solved.
    They are also much more efficient compared to direct search algorithms or linear
    programming methods. On the other hand, in practice, these algorithms are still
    either intractable or impossible to use. Let's elaborate on why.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both the policy iteration and the value iteration algorithms iterate over the
    entire state space, multiple times, until they arrive at an optimal policy. We
    also store the policy, the state values, and the action values for each state
    in a tabular form. Any realistic problem, on the other hand, would have a gigantic
    number of possible states, explained by a phenomenon called the **curse of dimensionality**.
    This refers to the fact that the possible number of values of a variable (states)
    grows exponentially as we add more dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our food truck example. In addition to keeping track of patties, let's
    assume we also keep track of burger buns, tomatoes, and onions. Also assume that
    the capacity for each of these items is 400, and we have a precise count of the
    inventory. The possible number of states in this case would be ![](img/Formula_05_073.png),
    that is, greater than ![](img/Formula_05_074.png) This is a ridiculous number
    of states to keep track of for such a simple problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'One mitigation for the curse of dimensionality is **asynchronous dynamic programming**:'
  prefs: []
  type: TYPE_NORMAL
- en: This approach suggests not sweeping over the entire state space in each iteration
    of policy improvement but focusing on the states that are more likely to be encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many problems, not all parts of the state space are of equal importance.
    Therefore, it is wasteful to wait for a complete sweep of the state space before
    there is an update to the policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With an asynchronous algorithm, we can simulate the environment in parallel
    to the policy improvement, observe which states are visited, and update the policy
    and the value functions for those states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, we can pass the updated policy to the agent so the simulation
    would continue with the new policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that the agent sufficiently explores the state space, the algorithm would
    converge to an optimal solution eventually.
  prefs: []
  type: TYPE_NORMAL
- en: A more important tool that we use to address this problem, on the other hand,
    is **function approximators**, such as deep neural networks. Think about it! What
    is the benefit in storing a separate policy/state-value/action-value for the inventory
    levels 135, 136, 137? Not much, really. Function approximators represent what
    we would like to learn in a much more compact manner (although approximately)
    compared to a tabular representation. In fact, in many cases, deep neural networks
    are only a meaningful choice for function approximation due to their representation
    power. That is why, starting from the next chapter, we will exclusively focus
    on deep RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The need for a complete model of the environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the methods we have used so far, we have relied on the transition probabilities
    of the environment in our policy evaluation, policy iteration, and value iteration
    algorithms to obtain optimal policies. This is a luxury that we usually don't
    have in practice. It is either these probabilities are very difficult to calculate
    for each possible transition (which is often impossible to even enumerate), or
    we simply don't know them. You know what is much easier to obtain? A sample trajectory
    of transitions, either from the environment itself or from its **simulation**.
    In fact, simulation is a particularly important component in RL, as we will discuss
    separately towards the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Then the question becomes how we use sample trajectories to learn near-optimal
    policies. Well, this is exactly what we'll cover next in the rest of this chapter
    with Monte Carlo and TD methods. The concepts you will learn are at the center
    of many of the advanced RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Training your agent with Monte Carlo methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say you would like to learn the chance of flipping heads with a particular,
    possibly biased, coin:'
  prefs: []
  type: TYPE_NORMAL
- en: One way of calculating this is through a careful analysis of the physical properties
    of the coin. Although this could give you the precise probability distribution
    of the outcomes, it is far from being a practical approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you can just flip the coin many times and look at the distribution
    in your sample. Your estimate could be a bit off if you don't have a large sample,
    but it will do the job for most practical purposes. The math you need to deal
    with using the latter method will be incomparably simpler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just like in the coin example, we can estimate the state values and action
    values in an MDP from random samples. **Monte Carlo (MC)** estimation is a general
    concept that refers to making estimations through repeated random sampling. In
    the context of RL, it refers to *a collection of methods that estimates state
    values and action values using sample trajectories of complete episodes*. Using
    random samples is incredibly convenient, and in fact essential, for any realistic
    RL problem, because the environment dynamics (state transition and reward probability
    distributions) are often either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Too complex to deal with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not known in the first place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo methods, therefore, are powerful methods that allow an RL agent
    to learn optimal policies only from the experience it collects through interacting
    with its environment, without knowing how the environment works.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll first look into estimating the state values and the action
    values for a given policy with MC methods. Then, we'll cover how to make improvements
    to obtain optimal policies.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the DP methods, we need to be able to evaluate a given policy ![](img/Formula_05_075.png)
    to be able to improve it. In this section, we'll cover how to evaluate a policy
    by estimating the corresponding state and action values. In doing so, we'll briefly
    revisit the grid world example from the previous chapter and then go into the
    food truck inventory replenishment problem.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the state-value function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Remember that the value of a state ![](img/Formula_05_010.png) under policy
    ![](img/Formula_05_061.png), ![](img/Formula_05_078.png) is defined as the expected
    cumulative reward when started in state ![](img/Formula_05_079.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_080.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MC prediction suggests simply observing (many) sample **trajectories**, sequences
    of state-action-reward tuples, starting in ![](img/Formula_05_081.png), to estimate
    this expectation. This is similar to flipping a coin to estimate its distribution
    from a sample.
  prefs: []
  type: TYPE_NORMAL
- en: It is best to explain Monte Carlo methods with an example. In particular, it
    will be quite intuitive to see how it works in the grid world example, so let's
    revisit that next.
  prefs: []
  type: TYPE_NORMAL
- en: Using sample trajectories for state value estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall that the robot in the grid world receives +1 reward for every move as
    long as it does not crash. When it does crash into a wall, the episode ends. Assume
    that this robot can be controlled only to a certain extent. When instructed to
    go in a particular direction, there is a 70% chance of it following the command.
    There is a 10% chance of the robot going in each of the other three directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a deterministic policy ![](img/Formula_05_061.png) illustrated in
    * Figure 5.3 (a)*. If the robot starts in state (1,2), two example trajectories
    it can follow, ![](img/Formula_05_083.png) and ![](img/Formula_05_084.png), and
    the corresponding probabilities of making each of the transitions are shown in
    * Figure 5.3 (b)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – a) A deterministic policy π, b) Two sample trajectories under π
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the robot follows a random trajectory but the policy itself is deterministic,
    which means the action taken (the command sent to the robot) in a given state
    is always the same. The randomness comes from the environment due to probabilistic
    state transitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the trajectory ![](img/Formula_05_085.png), the probability of observing
    it and the corresponding discounted return are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_086.jpg)![](img/Formula_05_087.jpg)![](img/Formula_05_088.jpg)![](img/Formula_05_089.jpg)![](img/Formula_05_090.jpg)![](img/Formula_05_091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And for ![](img/Formula_05_092.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_093.jpg)![](img/Formula_05_094.jpg)![](img/Formula_05_095.jpg)![](img/Formula_05_096.jpg)![](img/Formula_05_097.jpg)![](img/Formula_05_098.jpg)![](img/Formula_05_099.jpg)![](img/Formula_05_100.jpg)![](img/Formula_05_101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For these two example trajectories, we were able to calculate the corresponding
    probabilities and the returns. To calculate the state-value of ![](img/Formula_05_102.png),
    though, we need to evaluate the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_103.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'That means we need to identify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Every single possible trajectory ![](img/Formula_05_104.png) that can originate
    from ![](img/Formula_05_105.png) under policy ![](img/Formula_05_106.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of observing ![](img/Formula_05_107.png) under ![](img/Formula_05_108.png),
    ![](img/Formula_05_109.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The corresponding discounted return, ![](img/Formula_05_110.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, that is an impossible task. Even in this simple problem, there is an infinite
    number of possible trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly where Monte Carlo prediction comes in. It simply tells us to
    estimate the value of state ![](img/Formula_05_111.png) by averaging the sample
    returns as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_112.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That's it! Sample trajectories and returns are all you need to estimate the
    value of the state.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Note that ![](img/Formula_05_021.png) denotes the true value of the state, whereas
    ![](img/Formula_05_114.png) denotes an estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you may be asking the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How come it''s enough to have two sample returns to estimate a quantity that
    is the outcome of an infinite number of trajectories?* It is not. The more sample
    trajectories you have, the more accurate your estimate is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How do we know we have enough sample trajectories?* That is hard to quantify.
    But more complex environments, especially when there is a significant degree of
    randomness, would require more samples for an accurate estimation. It is a good
    idea, though, to check if the estimate is converging as you add more trajectory
    samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_05_115.png) *and* ![](img/Formula_05_116.png) *have very different
    likelihoods of occurring. Is it appropriate to assign them equal weights in the
    estimation?* This is indeed problematic when we have only two trajectories in
    the sample. However, as we sample more trajectories, we can expect to observe
    the trajectories in the sample occurring proportionally to their true probabilities
    of occurrence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Can we use the same trajectory in estimating the values of the other states
    it visits?* Yes! Indeed, that is what we will do in Monte Carlo prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's elaborate on how we can use the same trajectory in estimating the values
    of different states next.
  prefs: []
  type: TYPE_NORMAL
- en: First-visit versus every-visit Monte Carlo prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you recall what the Markov property is, it simply tells us the future depends
    on the current state, not the past. Therefore, we can think of, for example, ![](img/Formula_05_117.png)
    as three separate trajectories that originate from states ![](img/Formula_05_118.png),
    and ![](img/Formula_05_119.png). Let''s call the latter two trajectories ![](img/Formula_05_120.png)
    and ![](img/Formula_05_121.png). So, we can obtain a value estimate for all the
    states visited by the trajectories in our sample set. For instance, the value
    estimate for state ![](img/Formula_05_122.png) would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_123.jpg)![](img/Formula_05_124.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since there is no other trajectory visiting state ![](img/Formula_05_125.png),
    we used a single return to estimate the state-value. Notice the discounts are
    applied to the rewards according to their time distance to the initial time-step.
    That is why the exponents of the ![](img/Formula_05_126.png) discount reduced
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following set of trajectories in *Figure 5.4* and assume we again
    want to estimate ![](img/Formula_05_127.png). None of the sample trajectories
    actually originate from state ![](img/Formula_05_128.png) but that is totally
    fine. We can use trajectories ![](img/Formula_05_129.png), ![](img/Formula_05_130.png),
    and ![](img/Formula_05_131.png) for the estimation. But then there is an interesting
    case here: ![](img/Formula_05_132.png) visits state ![](img/Formula_05_133.png)
    twice. Should we use the return only from its first visit or from each of its
    visits?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Monte Carlo estimation of ![](img/Formula_05_134.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Both these approaches are valid. The former method is called **first-visit
    MC method** and the latter is called **every-visit MC method**. They compare to
    each other as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Both converge to true ![](img/Formula_05_135.png) as the number of visits approaches
    infinity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first-visit MC method gives an unbiased estimate of the state-value whereas
    the every-visit MC method is biased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **mean squared error** (**MSE**) of the first-visit MC method is higher
    with fewer samples but lower than every-visit MSE with more samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The every-visit MC method is more natural to use with function approximations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If it sounds complicated, it is actually not! Just remember these things:'
  prefs: []
  type: TYPE_NORMAL
- en: We are trying to estimate a parameter, such as the state-value of ![](img/Formula_05_136.png),
    ![](img/Formula_05_137.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By observing the random variable ![](img/Formula_05_138.png) , the discounted
    value returns starting from ![](img/Formula_05_139.png), many times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is both valid to consider trajectories from their first visit of ![](img/Formula_05_140.png)
    on, or from, their every visit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it is time to implement Monte Carlo prediction next!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing first-visit Monte Carlo estimation of the state values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have used the grid world example to get a visual intuition, and now let's
    go back to our food truck example for the implementation. Here, we will implement
    the first-visit MC method, but the implementation can be easily modified to every-visit
    MC. This can be done by removing the condition that calculates the return only
    if the state does not appear in the trajectory up to that point. For the food
    truck example, since a trajectory will never visit the same state – because the
    day of the week, which is part of the state, changes after every transition –
    both methods are identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow these steps for the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining a function that takes a trajectory and calculates the
    returns from the first visit for each state that appears in the trajectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function does the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Takes a dictionary `returns` as input, whose keys are states and values are
    lists of returns calculated in some other trajectories.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Takes a list `trajectory` as input, which is a list of state-action-reward
    tuples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Appends the calculated return for each state to `returns`. If that state
    has never been visited by another trajectory before, it initializes the list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Traverses the trajectory backward to conveniently calculate the discounted
    returns. It applies the discount factor in each step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Checks if a state is visited earlier in the trajectory after each calculation.
    It saves the calculated return to the `returns` dictionary only for the first
    visit of a state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we implement a function that simulates the environment for a single episode
    with a given policy and returns the trajectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, implement the first-visit return Monte Carlo function, which simulates
    the environment for a specified number of episodes/trajectories with a given policy.
    We keep track of the trajectories and average the returns for each state calculated
    by the `first_visit_return` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We make sure that we create an environment instance (or we can simply use the
    one from the previous sections). Also obtain the base policy, which fills the
    patty inventory up to 200 or 300 with equal chance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s use the first-visit MC method to estimate the state values from
    1,000 trajectories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result, `v_est`, will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, remember that we can use DP''s policy evaluation method to calculate the
    true state values for comparison:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The true state values will look very similar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can obtain the estimations with a different number of trajectories, such
    as 10, 100, and 1000\. Let''s do that and compare how the state value estimations
    get closer to the true values, as shown in *Figure 5.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B14160_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – First-visit Monte Carlo estimates versus true state values
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the results a bit more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: a) As we collected more trajectories, the estimates got closer to the true state
    values. You can increase the number of trajectories to even higher numbers to
    obtain even better estimates.
  prefs: []
  type: TYPE_NORMAL
- en: b) After we collected 10 trajectories, no values were estimated for ("Tue",
    200). This is because this state was never visited within those 10 trajectories.
    This highlights the importance of collecting enough trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: c) No values are estimated for states that started the day with 300 units of
    inventory. This is because, under the base policy, these states are impossible
    to visit. But then, we have no idea about the value of those states. On the other
    hand, they might be valuable states that we want our policy to lead us to. This
    is an **exploration problem** we need to address.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a way of estimating state values without knowing the environment
    dynamics, and by only using the agent''s experience in the environment. Great
    job so far! However, an important problem remains. With the state value estimates
    alone, we cannot really improve the policy on hand. To see why this is the case,
    recall how we improved the policy with the DP methods, such as value iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_141.jpg)![](img/Formula_05_142.jpg)![](img/Formula_05_143.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We used the state value estimates *together with the transition probabilities*
    to obtain action (q) values. We then chose, for each state, the action that maximized
    the q-value of the state. Right now, since we don't assume knowledge of the environment,
    we can't go from the state values to action values.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leaves us with one option: We need to estimate the action values directly.
    Fortunately, it will be similar to how we estimated the state values. Let''s look
    into using Monte Carlo methods to estimate the action values next.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the action-value function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Action values, ![](img/Formula_05_144.png), represent the expected discounted
    cumulative return when starting in state ![](img/Formula_05_045.png), taking action
    ![](img/Formula_05_146.png), and following the policy ![](img/Formula_05_075.png).
    Consider the following trajectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_148.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The discounted return observed can be used in estimating ![](img/Formula_05_149.png),
    ![](img/Formula_05_150.png), and so on. We can then use them to determine the
    best action for a given state ![](img/Formula_05_010.png), as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_152.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the challenge: What if we don''t have the action-value estimates for
    all possible ![](img/Formula_05_059.png) that can be taken in state ![](img/Formula_05_154.png)?
    Consider the grid world example. If the policy is always to go right when in state
    ![](img/Formula_05_155.png), we will never have a trajectory that starts with
    state-action pairs ![](img/Formula_05_156.png), or ![](img/Formula_05_157.png).
    So, even if one of those actions gives a higher action value than ![](img/Formula_05_158.png),
    we will never discover it. The situation was similar with the base policy in the
    food truck example. In general, this is the case when we use a deterministic policy,
    or a stochastic policy that does not take all of the actions with some positive
    probability.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what we have here is essentially an *exploration* problem, which is a fundamental
    challenge in RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible solutions to this:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting the trajectories with a random action selected in a random initial
    state, and then following a policy ![](img/Formula_05_061.png) as usual. This
    is called **exploring starts**. This ensures choosing all state-action pairs at
    least once, so we can estimate the action-values. The drawback of this approach
    is that we need to keep starting episodes with random initializations. If we want
    to learn the action-values from ongoing interaction with the environment, without
    frequent restarts, this method will not be too helpful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other and more common solution to the exploration problem is to maintain
    a policy that selects all actions in any state with positive probability. More
    formally, we need a policy that satisfies ![](img/Formula_05_160.png), for all
    ![](img/Formula_05_161.png) and for all ![](img/Formula_05_162.png); where ![](img/Formula_05_163.png)
    is the set of all possible states and ![](img/Formula_05_164.png) is the set of
    all possible actions available in state ![](img/Formula_05_165.png). Such a policy
    ![](img/Formula_05_166.png) is called a **soft policy**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will use soft policy action-value estimation, which
    will in turn be used for policy improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monte Carlo control refers to a collection of methods that find optimal/near-optimal
    policies using the *samples* of discounted return. In other words, this is learning
    optimal policies from experience. And, since we depend on experience to discover
    optimal policies, we have to explore, as we explained above. Next, we'll implement
    ![](img/Formula_05_167.png)**-greedy** policies that enable us to explore during
    training, which is a particular form of soft policy. After that, we'll cover two
    different flavors of Monte Carlo control, namely on-policy and off-policy methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ![](img/Formula_05_168.png)-greedy policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very similar to what we did in bandit problems, an ![](img/Formula_05_1681.png)-greedy
    policy picks an action at random with ε probability; and with ![](img/Formula_05_169.png)
    probability, it selects the action that maximizes the action-value function. This
    way, we continue to explore all state-action pairs while selecting the best actions
    we identify with high likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now implement a function that converts a deterministic action to an
    ![](img/Formula_05_170.png)-greedy one, which we will need later. The function
    assigns ![](img/Formula_05_171.png) probability to the best action, and ![](img/Formula_05_172.png)
    probability to all other actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Exploration is necessary to find optimal policies during training. On the other
    hand, we would not want to take exploratory actions after the training/during
    inference but take the best ones. Therefore, the two policies differ. To make
    the distinction, the former is called the **behavior policy** and the latter is
    called the **target policy**. We can make the state and the action values aligned
    with the former or the latter, leading to two different classes of methods: **on-policy**
    and **off-policy**. Let''s compare the two approaches in detail next.'
  prefs: []
  type: TYPE_NORMAL
- en: On-policy versus off-policy methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that the state and the action values are associated with a particular
    policy, hence the notation ![](img/Formula_05_173.png) and ![](img/Formula_05_174.png).
    On-policy methods estimate the state and the action values for the behavior policy
    used during training, such as the one that generates the training data/the experience.
    Off-policy methods estimate the state and the action values for a policy that
    is other than the behavior policy, such as the target policy. We ideally want
    to decouple exploration from value estimation. Let's look into why this is the
    case in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of on-policy methods on value function estimates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exploratory policies are usually not optimal as they take random actions once
    in a while for the sake of exploration. Since on-policy methods estimate the state
    and action values for the behavior policy, that sub-optimality is reflected in
    the value estimates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following modified grid world example to see how involving the
    effects of the exploratory actions in value estimation could be potentially harmful:
    The robot needs to choose between going left or right in state ![](img/Formula_05_175.png)
    of a ![](img/Formula_05_176.png) grid world, and between up and down in states
    1 and 3\. The robot follows the actions perfectly, so there is no randomness in
    the environment. This is illustrated in *Figure 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Modified grid world
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot has an ![](img/Formula_05_177.png)-greedy policy, which suggests
    taking the best action with 0.99 chance, and an exploratory action with 0.01 chance.
    The best policy in state 3 is to go up with a high likelihood. In state 1, the
    choice does not really matter. The state value estimates obtained in an on-policy
    manner will then be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_178.jpg)![](img/Formula_05_179.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A policy obtained in an on-policy fashion for state 2 will suggest going left,
    towards state 1\. On the other hand, in this deterministic environment, the robot
    could perfectly avoid the big penalty when there is no exploration involved. An
    on-policy method would fail to identify this since the exploration influences
    the value estimates and yields a sub-optimal policy as a result. On the other
    hand, in some cases, we may want the agent to take the impact of exploration into
    account if, for example, the samples are collected using a physical robot and
    some states are very costly to visit.
  prefs: []
  type: TYPE_NORMAL
- en: Sample efficiency comparison between on-policy and off-policy methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned above, off-policy methods estimate the state and the action values
    for a policy that is different than the behavior policy. On-policy methods, on
    the other hand, estimate these values only for the behavior policy. When we cover
    deep RL on-policy methods in later chapters, we will see that this will require
    the on-policy methods to discard the past experience once the behavior policy
    is updated. Off-policy deep RL methods, however, can reuse the past experience
    again and again. This is a significant advantage in sample efficiency, especially
    if it is costly to obtain the experience.
  prefs: []
  type: TYPE_NORMAL
- en: Another area where off-policy methods' ability to use the experience generated
    by a policy other than the behavior policy comes in handy is when we want to warm-start
    the training based on the data generated by a non-RL controller, such as classical
    PID controllers or a human operator. This is especially useful when the environment
    is hard/expensive to simulate or collect experience from.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of on-policy methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The natural question is then why do we even talk about on-policy methods instead
    of just ignoring them? There are several advantages of on-policy methods:'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned above, if the cost of sampling from the environment is high (the
    actual environment rather than a simulation), we may want to have a policy that
    reflects the impact of exploration to avoid catastrophic outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-policy methods, when combined with function approximators, could have issues
    with converging to a good policy. We will discuss this in the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-policy methods are easier to work with when the action space is continuous,
    again, as we will discuss later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, it is finally time to implement first on-policy and then off-policy
    Monte Carlo methods!
  prefs: []
  type: TYPE_NORMAL
- en: On-policy Monte Carlo control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GPI framework we described earlier, which suggests going back and forth
    between some forms of policy evaluation and policy improvement, is also what we
    use with the Monte Carlo methods to obtain optimal policies. In each cycle, we
    collect a trajectory from a complete episode, then estimate the action-values
    and update the policy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement an on-policy MC control algorithm and use it to optimize the
    food truck inventory replenishment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a function that generates a random policy where all actions
    are equally likely to be taken, to be used to initialize the policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now, we build the on-policy first-visit MC control algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is very similar to the first-visit MC prediction method, with the following
    key differences:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Instead of estimating the state values, ![](img/Formula_05_180.png), we estimate
    the action values, ![](img/Formula_05_181.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) To explore all state-action pairs, we use ε-greedy policies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) This is a first-visit approach, but instead of checking if the state appeared
    earlier in the trajectory, we check if the state-action pair appeared before in
    the trajectory before we update the ![](img/Formula_05_182.png) estimate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Whenever we update the ![](img/Formula_05_183.png) estimate of a state-action
    pair, we also update the policy to assign the highest probability to the action
    that maximizes the ![](img/Formula_05_184.png), which is ![](img/Formula_05_185.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the algorithm to optimize the food truck policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the `policy` dictionary. You will see that it is the optimal one that
    we found earlier with DP methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it! This algorithm does not use any knowledge of the environment, unlike
    the DP methods. It learns from its interaction with the (simulation of the) environment!
    And, when we run the algorithm long enough, it converges to the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Great job! Next, let's proceed to off-policy MC control.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy Monte Carlo control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In an off-policy approach, we have a set of samples (trajectories) collected
    under some behavior policy ![](img/Formula_05_186.png), yet we would like to use
    that experience to estimate the state and action values under a target policy
    ![](img/Formula_05_046.png). This requires us to use a trick called **importance
    sampling**. Let's look into what it is next, and then describe off-policy Monte
    Carlo control.
  prefs: []
  type: TYPE_NORMAL
- en: Importance sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s start describing importance sampling in a simple game setting, in which
    you roll a six-sided die. Depending on what comes up on the die, you receive a
    random reward. This could be something like this: If you get a 1, you get a reward
    from a normal distribution with mean 10 and variance 25\. There are similar hidden
    reward distributions for all outcomes, which are unknown to you. Let''s denote
    the reward you receive when the face ![](img/Formula_05_188.png) comes up with
    the random variable ![](img/Formula_05_189.png).'
  prefs: []
  type: TYPE_NORMAL
- en: You want to estimate the expected reward after a single roll, which is
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/Formula_05_191.png) is the probability of observing side ![](img/Formula_05_192.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, assume that you have two dice to choose from, A and B, with different
    probability distributions. You first pick A, and roll the die ![](img/Formula_05_193.png)
    times. Your observations are as in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 5.1 – Observed rewards after n rolls with die A
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, ![](img/Formula_05_194.png) denotes the reward observed after the ![](img/Formula_05_195.png)
    roll when the observed side is ![](img/Formula_05_196.png). The estimated expected
    reward with die A,  ![](img/Formula_05_197.png), is simply given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_198.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, the question is whether we can use this data to estimate the expected reward
    with die B, such as ![](img/Formula_05_199.png), without any new observations?
    The answer is yes if we know ![](img/Formula_05_200.png) and ![](img/Formula_05_201.png).
    Here is how.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the current estimation, each observation has a weight of 1\. Importance
    sampling suggests scaling these weights according to ![](img/Formula_05_202.png).
    Now, consider the observation ![](img/Formula_05_203.png). If with die B, we are
    three times as likely to observe ![](img/Formula_05_204.png) compared to die A,
    then we increase the weight of this observation in the sum to 3\. More formally,
    we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_205.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the ratio ![](img/Formula_05_206.png) is called the importance sampling
    ratio. By the way, the above preceding estimation, ![](img/Formula_05_207.png),
    is called **ordinary importance sampling**. We can also normalize the new estimate
    with respect to the new weights and obtain **weighted importance sampling** as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_208.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, after this detour, let's go back to using this to get off-policy predictions.
    In the context of Monte Carlo prediction, observing a particular side of the die
    corresponds to observing a particular trajectory, and the die reward corresponds
    to the total return.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are thinking *"well, we don''t really know the probability of observing
    a particular trajectory, that''s why we are using Monte Carlo methods, isn''t
    it?"* You are right, but we don''t need to. Here is why. Starting in some ![](img/Formula_05_209.png),
    the probability of observing a trajectory ![](img/Formula_05_210.png) under a
    behavior policy ![](img/Formula_05_211.png) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_212.jpg)![](img/Formula_05_213.jpg)![](img/Formula_05_214.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression is the same for the target policy, except ![](img/Formula_05_035.png)
    replaces ![](img/Formula_05_216.png). Now, when we calculate the importance sampling
    ratio, the transition probabilities cancel out and we end up with the following
    (*Sutton & Barto, 2018*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_217.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With that, we go from estimating the expectation under the behavior policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_218.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'to estimating the expectation under the target policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_219.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s close this section with a few notes on importance sampling before we
    implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to be able to use the samples obtained under behavior policy ![](img/Formula_05_211.png)
    to estimate the state and action values under ![](img/Formula_05_221.png), it
    is required to have ![](img/Formula_05_222.png) if ![](img/Formula_05_223.png).
    Since we would not want to impose what ![](img/Formula_05_224.png) could be taken
    under the target policy, ![](img/Formula_05_225.png) is usually chosen as a soft
    policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the weighted importance sampling, if the denominator is zero, then the estimate
    is considered zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The formulas above ignored the discount in the return, which is a bit more complicated
    to deal with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The observed trajectories can be chopped with respect to the first-visit or
    every-visit rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ordinary importance sampling is unbiased but can have very high variance.
    The weighted importance sampling, on the other hand, is biased but usually has
    a much lower variance, hence it's preferred in practice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was a quite detour, and we hope you are still with us! If you are, let's
    go back to coding!
  prefs: []
  type: TYPE_NORMAL
- en: Application to the inventory replenishment problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our application of the off-policy Monte Carlo method, we use the weighted
    importance sampling. The behavior policy is chosen as an ![](img/Formula_05_226.png)-greedy
    policy, yet the target is a greedy policy maximizing the action value for each
    state. Also, we use an incremental method to update the state and action value
    estimates as follows (Sutton & Barto, 2018):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_227.jpg)![](img/Formula_05_228.jpg)![](img/Formula_05_229.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the implementation can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define the function for the incremental implementation of the
    off-policy Monte Carlo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the off-policy MC to optimize the food truck inventory replenishment
    policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, display the obtained policy. You will see that it is the optimal one!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Yay! We are done with the Monte Carlo methods for now. Congratulations, this
    was not the easiest part! You deserve a break. Next, we will dive into yet another
    very important topic: temporal-difference learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Temporal-difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first class of methods to solve MDP we covered in this chapter was DP:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to completely know the environment dynamics to be able to find the
    optimal solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows us to progress toward the solution with one-step updates of the value
    functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then covered the MC methods:'
  prefs: []
  type: TYPE_NORMAL
- en: They only require the ability to sample from the environment, therefore they
    learn from experience, as opposed to knowing the environment dynamics – a huge
    advantage over DP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But they need to wait for a complete episode trajectory to update a policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal-difference (TD)** methods are, in some sense, the best of both worlds:
    They learn from experience, and they can update the policy after each step by
    **bootstrapping**. This comparison of TD to DP and MC is illustrated in *Table
    5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 5.2 – Comparison of DP, MC, and TD learning methods
  prefs: []
  type: TYPE_NORMAL
- en: As a result, TD methods are central in RL and you will encounter them again
    and again in various forms. In this section, you will learn how to implement TD
    methods in tabular form. Modern RL algorithms, which we will cover in the following
    chapters, implement TD methods with function approximations such as neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: One-step TD learning – TD(0)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TD methods can update a policy after a single state transition or multiple ones.
    The former version is called one-step TD learning or TD(0) and it is easier to
    implement compared to ![](img/Formula_05_231.png)-step TD learning. We'll start
    covering one-step TD learning with the prediction problem. We'll then introduce
    an on-policy method, SARSA, and then an off-policy algorithm, the famous Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: TD prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Remember how we can write the state-value function of a policy ![](img/Formula_05_046.png)
    in terms of one-step reward and the value of the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_233.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the agent takes an action under policy ![](img/Formula_05_061.png) while
    in state ![](img/Formula_05_010.png), it observes three random variables realized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_236.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/Formula_05_237.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/Formula_05_238.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'We already know the probability of observing ![](img/Formula_05_059.png) as
    part of the policy, but the latter two come from the environment. The observed
    quantity ![](img/Formula_05_240.png) gives us a new estimate for ![](img/Formula_05_241.png)
    based on a single sample. Of course, we don''t want to completely discard the
    existing estimate and replace it with the new one, because the transitions are
    usually random, and we could have also observed completely different ![](img/Formula_05_242.png)
    and ![](img/Formula_05_243.png) values even with the same action ![](img/Formula_05_244.png).
    *The idea in TD learning is that we use this observation to update the existing
    estimate *![](img/Formula_05_245.png) *by moving it in the direction of this new
    estimate.* And the step-size ![](img/Formula_05_246.png) controls how aggressively
    we move towards the new estimate. More formally, we use the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_247.jpg)![](img/Formula_05_248.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The term in the square brackets is called the **TD error**. As is obvious from
    the name, it estimates how much the current state-value estimate of ![](img/Formula_05_249.png),
    ![](img/Formula_05_250.png) is off from the truth based on the latest observation.
    ![](img/Formula_05_251.png) would completely ignore the new signal while ![](img/Formula_05_252.png)
    would completely ignore the existing estimate. Again, since a single observation
    is often noisy, and the new estimate itself uses another erroneous estimate (![](img/Formula_05_253.png));
    the new estimate cannot be solely relied on. Therefore, the ![](img/Formula_05_254.png)
    value is chosen between ![](img/Formula_05_255.png) and 1, and often closer to
    ![](img/Formula_05_256.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, it is trivial to implement a TD prediction method to evaluate a
    given policy ![](img/Formula_05_046.png) and estimate the state values. Follow
    along to use TD prediction to evaluate the base policy in the food truck example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we implement the TD prediction as described above as in the following
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function simply simulates a given environment using a specified policy
    over a specified number of iterations. After each observation, it does a one-step
    TD update using the given ![](img/Formula_05_258.png) step size and the discount
    factor ![](img/Formula_05_259.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we get the base policy as defined by the `base_policy` function we introduced
    before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let''s estimate the state values using TD prediction for ![](img/Formula_05_260.png)
    and ![](img/Formula_05_261.png) over 100,000 steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After rounding, the state-value estimates will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you go back to the DP methods section and check what the true state values
    are under the base policy, which we obtained using the policy evaluation algorithm,
    you will see that the TD estimates are very much in line with them.
  prefs: []
  type: TYPE_NORMAL
- en: So, great! We have successfully evaluated a given policy using TD prediction
    and things are working as expected. On the other hand, just like with the MC methods,
    we know that we have to estimate the action values to be able to improve the policy
    and find an optimal one in the absence of the environment dynamics. Next, we'll
    look into two different methods, SARSA and Q-learning, which do exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: On-policy control with SARSA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With slight additions and modifications to the TD(0), we can turn it into an
    optimal control algorithm. Namely, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that we always have a soft policy, such as ![](img/Formula_05_262.png)-greedy,
    to try all actions for a given state over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the action values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the policy based on the action-value estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And we will do all of these at each step and using the observations ![](img/Formula_05_263.png),
    hence the name **SARSA**. In particular, the action values are updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_264.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's dive into the implementation!
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the function `sarsa`, which will take as the arguments the environment,
    the discount factor ![](img/Formula_05_265.png), the exploration parameter ![](img/Formula_05_266.png),
    and the learning step size ![](img/Formula_05_267.png). Also, implement the usual
    initializations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we implement the algorithm loop, in which we simulate the environment
    for a single step, observe ![](img/Formula_05_268.png) and ![](img/Formula_05_269.png),
    choose the next action ![](img/Formula_05_270.png) based on ![](img/Formula_05_271.png)
    and the ![](img/Formula_05_272.png)-greedy policy, and update the action-value
    estimates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let''s execute the algorithm with a selection of hyperparameters, such
    as ![](img/Formula_05_273.png), and over 1 million iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `policy` we obtain is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the exploratory actions would be ignored when implementing this policy
    after training, and we would simply always pick the action with the highest probability
    for each state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The action values, for example, for the state Monday – 0 beginning inventory
    (accessed via `Q[(''Mon'', 0)]`), are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And that''s it! We have successfully implemented the TD(0) algorithm for our
    example. Notice that, however, the policy we obtain is a near-optimal one, not
    the optimal one we obtained with the DP methods. There are also inconsistencies
    in the policy, such as having a policy of buying 300 patties both when in states
    (Tuesday, 0) and (Tuesday, 100). There are several culprits for not getting the
    optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: SARSA converges to an optimal solution in the limit, such as when ![](img/Formula_05_274.png).
    In practice, we run the algorithm for a limited number of steps. Try increasing
    ![](img/Formula_05_275.png) and you will see that the policy (usually) will get
    better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate ![](img/Formula_05_254.png) is a hyperparameter that needs
    to be tuned. The speed of the convergence depends on this selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an on-policy algorithm. Therefore, the action values reflect the exploration
    due to the ![](img/Formula_05_277.png)-greedy policy, which is not what we really
    want in this example. Because, after training, there will be no exploration while
    following the policy in practice (since we need the exploration just to discover
    the best actions for each state). The policy we would use in practice is not the
    same as the policy we estimated the action values for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we turn to Q-learning, which is an off-policy TD method.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy control with Q-learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned above, we would like to isolate the action-value estimates from
    the exploration effect, which means having an off-policy method. Q-learning is
    such an approach, which makes it very powerful, and as a result, a very popular
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are how the action values are updated in Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_278.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice that instead of ![](img/Formula_05_279.png), we have the term ![](img/Formula_05_280.png).
    The difference may look small, but it is key. *It means that the action the agent
    uses to update the action-value,* ![](img/Formula_05_281.png)*, is not necessarily
    the one it will take in the next step when in* ![](img/Formula_05_282.png)*,*
    ![](img/Formula_05_283.png)*. Instead,* ![](img/Formula_05_284.png) *is an action
    that maximizes* ![](img/Formula_05_285.png)*, just like what we would use if not
    in training.* As a result, no exploratory actions are involved in action-value
    estimates and they are aligned with the policy that would be followed after training.
  prefs: []
  type: TYPE_NORMAL
- en: It means the action the agent takes in the next step, such as ![](img/Formula_05_286.png),
    is not used in the update. Instead, we use the maximum action value for the state
    ![](img/Formula_05_287.png), such as ![](img/Formula_05_288.png), in the update.
    Such an action ![](img/Formula_05_281.png) is what we would use after training
    with those action values, hence no exploratory actions are involved in action-value
    estimations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of Q-learning is only slightly different than that of SARSA.
    Let''s go ahead and see Q-learning in action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the `q_learning` function with the usual initializations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we implement the main loop, where the action the agent takes in ![](img/Formula_05_290.png)
    comes from the ![](img/Formula_05_291.png)-greedy policy. During the update, the
    maximum of ![](img/Formula_05_292.png) is used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We return the policy stripped of the exploratory actions after the main loop
    is finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we execute the algorithm with a selection of the hyperparameters,
    such as the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Observe the returned `policy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will see that this hyperparameter set gives you the optimal policy (or something
    close to it depending on how randomization plays out in your case).
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our discussion on Q-learning. Next, let's discuss how these approaches
    can be extended to ![](img/Formula_05_193.png)-step learning.
  prefs: []
  type: TYPE_NORMAL
- en: n-step TD learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the Monte Carlo methods, we collected complete episodes before making a
    policy update. With TD(0), on the other extreme, we updated the value estimates
    and the policy after a single transition in the environment. One could possibly
    find a sweet spot by following a path in between by updating the policy after
    ![](img/Formula_05_294.png)-steps of transitions. For ![](img/Formula_05_295.png),
    the two-step return looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_296.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the general form is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This form can be used in the TD update to reduce the weight of the estimates
    used in bootstrapping, which could be especially inaccurate at the beginning of
    the training. We don't include the implementation here as it gets a bit messy
    but still wanted to bring this alternative to your attention for you to have it
    in your toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have completed the TD methods! Before finishing the chapter, let's
    take a closer look at the importance of simulations in RL.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of simulation in reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve mentioned multiple times so far, and especially in the first chapter
    when we talked about RL success stories, RL''s hunger for data is orders of magnitude
    greater than regular deep learning. That is why it takes many months to train
    some complex RL agents, over millions and billions of iterations. Since it is
    often impractical to collect such data in a physical environment, we heavily rely
    on simulation models in training RL agents. This brings some challenges along
    with it:'
  prefs: []
  type: TYPE_NORMAL
- en: Many businesses don't have a simulation model for their business processes.
    This makes it challenging to put RL technology to use in the business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a simulation model exists, it is often too simplistic to capture the real-world
    dynamics. As a result, RL models could easily overfit the simulation environment
    and may fail in deployment. It takes significant time and resources to calibrate
    and validate a simulation model to make it reflect reality sufficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, deploying an RL agent that is trained in simulation in the real
    world is not easy, because, well, they are two different worlds. This is against
    the core principle in machine learning that says the training and the test should
    follow the same distribution. This is known as the **simulation-to-real (sim2real)**
    gap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased fidelity in simulation comes with slowness and compute resource consumption,
    which is a real disadvantage for fast experimentation and RL model development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many simulation models are not generic enough to cover scenarios that have not
    been encountered in the past but are likely to be encountered in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of commercial simulation software could be hard to integrate (due to the
    lack of a proper API) with the languages RL packages are naturally available in,
    such as Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even when integration is possible, the simulation software may not be flexible
    enough to work with the algorithms. For example, it may not reveal the state of
    the environment, reset it when needed, define terminal states, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many simulation vendors allow a limited number of sessions per license, whereas
    RL model development is the fastest – you can run thousands of simulation environments
    in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we will cover some techniques to overcome some of these challenges,
    such as domain randomization for the sim2real gap and offline RL for environments
    without simulation. However, the key message of this section is that you usually
    should invest in your simulation model to get the best out of RL. In particular,
    your simulation model should be fast, accurate, and scalable to many sessions.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude this chapter. Great work! This marks a milestone in our
    journey with this book. We have come a long way and built a solid foundation of
    RL solution approaches! Next, let's summarize what we have learned and see what
    is coming up in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered three important approaches to solving MDPs: DP,
    Monte Carlo methods, and temporal-difference learning. We have seen that while
    DP provides exact solutions to MDPs, it relies on the knowledge of the environment.
    Monte Carlo and TD learning methods, on the other hand, explore the environment
    and learn from experience. TD learning, in particular, can learn from even a single
    step transition in the environment. Along the way, we also discussed on-policy
    methods, which estimate the value functions for a behavior policy, and off-policy
    methods for a target policy. Finally, we also discussed the importance of the
    simulator in RL experiments and what to pay attention to when working with one.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll take our journey to the next stage and dive into deep RL, which
    will enable us to solve some real-world problems using RL. Particularly, in the
    next chapter, we'll cover deep Q-learning in detail.
  prefs: []
  type: TYPE_NORMAL
- en: See you there!
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Change these values to see how the optimal policy changes for the modified problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a policy evaluation step after the policy improvement step in the value
    iteration algorithm. You can set the number of iterations you want to perform
    the evaluation for before you go back to policy improvement. Use the `policy_evaluation`
    function with a `max_iter` value of your choice. Also, be careful about how you
    track changes in the state values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction.*
    A Bradford Book. URL: [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barto, A. (2006). *Reinforcement learning*. University of Massachusetts – Amherst
    CMPSCI 687\. URL: [https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goldstick, J. (2009). *Importance sampling. Statistics 406: Introduction to
    Statistical Computing at the University of Michigan*: [http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf](http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
