["```py\n@mlflow_mixin\ndef train_dl_model():\n    mlflow.pytorch.autolog()\n    trainer = flash.Trainer(\n        max_epochs=num_epochs,\n        callbacks=[TuneReportCallback(\n            metrics, on='validation_end')])\n    trainer.finetune()\n```", "```py\n    pip install ray[tune]==1.9.2\n    ```", "```py\n    pip install -r requirements.txt\n    ```", "```py\n    export MLFLOW_TRACKING_URI=http://localhost\n    export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000\n    export AWS_ACCESS_KEY_ID=\"minio\"\n    export AWS_SECRET_ACCESS_KEY=\"minio123\"\n    ```", "```py\n    mlflow run . -P pipeline_steps='download_data' --experiment-name dl_model_chapter06\n    ```", "```py\n    datamodule = TextClassificationData.from_csv(\n        input_fields=\"review\",\n        target_fields=\"sentiment\",\n        train_file=f\"{data_path}/imdb/train.csv\",\n        val_file=f\"{data_path}/imdb/valid.csv\",\n        test_file=f\"{data_path}/imdb/test.csv\")\n    classifier_model = TextClassifier(\n        backbone= \"prajjwal1/bert-tiny\",\n        num_classes=datamodule.num_classes, \n        metrics=torchmetrics.F1(datamodule.num_classes))\n    trainer = flash.Trainer(max_epochs=3)\n    trainer.finetune(classifier_model, \n        datamodule=datamodule, strategy=\"freeze\") \n    ```", "```py\n    @mlflow_mixin\n    def finetuning_dl_model(config, data_dir=None,\n                            num_epochs=3, num_gpus=0):\n    ```", "```py\n    mlflow.pytorch.autolog()\n    ```", "```py\nmlflow.log_param('batch_size',config['batch_size'])\n```", "```py\n    datamodule = TextClassificationData.from_csv(\n        input_fields=\"review\",\n        target_fields=\"sentiment\",\n        train_file=f\"{data_dir}/imdb/train.csv\",\n        val_file=f\"{data_dir}/imdb/valid.csv\",\n        test_file=f\"{data_dir}/imdb/test.csv\",\n        batch_size=config['batch_size'])\n    ```", "```py\n    classifier_model = TextClassifier(\n        backbone=config['foundation_model'],\n        learning_rate=config['lr'],\n        optimizer=config['optimizer_type'],\n        num_classes=datamodule.num_classes,\n        metrics=torchmetrics.F1(datamodule.num_classes))\n    ```", "```py\nmetrics = {\"loss\":\"val_cross_entropy\", \"f1\":\"val_f1\"}\n```", "```py\ntrainer = flash.Trainer(max_epochs=num_epochs,\n    gpus=num_gpus,\n    progress_bar_refresh_rate=0,\n    callbacks=[TuneReportCallback(metrics, \n        on='validation_end')])\n```", "```py\n    trainer.finetune(classifier_model,\n        datamodule=datamodule,\n        strategy=config['finetuning_strategies'])\n    ```", "```py\n    trainable = tune.with_parameters(finetuning_dl_model, data_dir, num_epochs, num_gpus)\n    ```", "```py\n    mlflow.set_tracking_uri(tracking_uri)\n    mlflow.set_experiment(experiment_name)\n    ```", "```py\n    config = {\n            \"lr\": tune.loguniform(1e-4, 1e-1),\n            \"batch_size\": tune.choice([32, 64, 128]),\n            \"foundation_model\": \"prajjwal1/bert-tiny\",\n            \"finetuning_strategies\": \"freeze\",\n            \"optimizer_type\": \"Adam\",\n            \"mlflow\": {\n                \"experiment_name\": experiment_name,\n                \"tracking_uri\": mlflow.get_tracking_uri()\n            },\n        }\n    ```", "```py\n    trainable = tune.with_parameters(\n        finetuning_dl_model,\n        data_dir=data_dir,\n        num_epochs=num_epochs,\n        num_gpus=gpus_per_trial)\n    ```", "```py\n    analysis = tune.run(\n        trainable,\n        resources_per_trial={\n            \"cpu\": 1,\n            \"gpu\": gpus_per_trial\n        },\n        metric=\"f1\",\n        mode=\"max\",\n        config=config,\n        num_samples=num_samples,\n        name=\"hpo_tuning_dl_model\")\n    ```", "```py\n    logger.info(\"Best hyperparameters found were: %s\", analysis.best_config)\n    ```", "```py\npython pipeline/hpo_finetuning_model.py\n```", "```py\nBest hyperparameters found were: {'lr': 0.025639008922511797, 'batch_size': 64, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}\n```", "```py\n    pip install optuna==2.10.0\n    ```", "```py\n    from ray.tune.suggest import ConcurrencyLimiter\n    from ray.tune.schedulers import AsyncHyperBandScheduler\n    from ray.tune.suggest.optuna import OptunaSearch\n    ```", "```py\n    searcher = OptunaSearch()\n    searcher = ConcurrencyLimiter(searcher, max_concurrent=4)\n    scheduler = AsyncHyperBandScheduler()\n    ```", "```py\n    analysis = tune.run(\n        trainable,\n        resources_per_trial={\n            \"cpu\": 1,\n            \"gpu\": gpus_per_trial\n        },\n        metric=\"f1\",\n        mode=\"max\",\n        config=config,\n        num_samples=num_samples,\n        search_alg=searcher,\n        scheduler=scheduler,\n        name=\"hpo_tuning_dl_model\")\n    ```", "```py\npython pipeline/hpo_finetuning_model_optuna.py\n```", "```py\n[I 2022-02-06 21:01:27,609] A new study created in memory with name: optuna\n```", "```py\n2022-02-06 21:11:59,695    INFO tune.py:626 -- Total run time: 632.10 seconds (631.91 seconds for the tuning loop).\n2022-02-06 21:11:59,728 Best hyperparameters found were: {'lr': 0.0009599443695046438, 'batch_size': 128, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}\n```"]