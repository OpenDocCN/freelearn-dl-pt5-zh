<html><head></head><body>
<div id="_idContainer195">
<h1 class="c apter-number" id="_idParaDest-147"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-148"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.2.1">Optimizing Models with Transfer Learning and Fine-Tuning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">As models grow in size (the depth and number of processing modules per layer), training them grows exponentially as more time is spent per epoch, and typically, more epochs are required to reach </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">optimum performance.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">For this reason, </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">MXNet</span></strong><span class="koboSpan" id="kobo.7.1"> provides</span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.8.1"> state-of-the-art</span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.9.1"> pre-trained models </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.10.1">via </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">GluonCV</span></strong><span class="koboSpan" id="kobo.12.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.13.1">GluonNLP</span></strong><span class="koboSpan" id="kobo.14.1"> libraries. </span><span class="koboSpan" id="kobo.14.2">As we have seen in previous chapters, these models can help us solve a variety of problems when our final dataset is similar to the one the selected model has been </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">pre-trained on.</span></span></p>
<p><span class="koboSpan" id="kobo.16.1">However, sometimes this is not good enough, and our final dataset might have some nuances that the pre-trained model is not picking up. </span><span class="koboSpan" id="kobo.16.2">In these cases, it would be ideal to combine the stored knowledge of the pre-trained model with our final dataset. </span><span class="koboSpan" id="kobo.16.3">This is called transfer learning, where the knowledge of our pre-trained model is transferred to a new task (</span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">final dataset).</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">In this chapter, we will learn how to use GluonCV and GluonNLP, which are MXNet Gluon libraries that</span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.19.1"> are </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.20.1">specific to </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">Computer Vision</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.23.1">CV</span></strong><span class="koboSpan" id="kobo.24.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.25.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.26.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.27.1">NLP</span></strong><span class="koboSpan" id="kobo.28.1">), respectively. </span><span class="koboSpan" id="kobo.28.2">We will also learn how to retrieve pre-trained models from their model zoos, and how to optimize our own networks by transferring the learnings from these </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">pre-trained models.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">Specifically, we will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">our recipes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.32.1">Understanding transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">and fine-tuning</span></span></li>
<li><span class="koboSpan" id="kobo.34.1">Improving performance for </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">classifying images</span></span></li>
<li><span class="koboSpan" id="kobo.36.1">Improving performance for </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">segmenting images</span></span></li>
<li><span class="koboSpan" id="kobo.38.1">Improving performance for translating English </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">to German</span></span></li>
</ul>
<h1 id="_idParaDest-149"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.40.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.41.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.42.1">Preface</span></em><span class="koboSpan" id="kobo.43.1">, the following technical </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">requirements apply:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.45.1">Ensure that you have completed the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.46.1">Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.47.1">, from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.48.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.49.1">, </span><em class="italic"><span class="koboSpan" id="kobo.50.1">Up and Running </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.51.1">with MXNet</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.53.1">Ensure that you have completed </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.54.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.55.1">, </span><em class="italic"><span class="koboSpan" id="kobo.56.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.57.1">, and </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.58.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.59.1">, </span><em class="italic"><span class="koboSpan" id="kobo.60.1">Understanding Text with Natural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.61.1">Language Processing</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.63.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07"><span class="No-Break"><span class="koboSpan" id="kobo.65.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.66.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">Furthermore, you can access each recipe directly from Google Colab; for example, the first recipe of this chapter can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">here: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.69.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.70.1">.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.71.1">Understanding transfer learning and fine-tuning</span></h1>
<p><span class="koboSpan" id="kobo.72.1">In the previous</span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.73.1"> chapters, we </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.74.1">saw how we could leverage MXNet, GluonCV, and GluonNLP to retrieve pre-trained models in certain datasets (such as ImageNet, MS COCO, and IWSLT2015) and use them for our specific tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">and datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">In this recipe, we will introduce a methodology called </span><strong class="bold"><span class="koboSpan" id="kobo.77.1">transfer learning</span></strong><span class="koboSpan" id="kobo.78.1">, which will allow us to combine the information from pre-trained models (on general knowledge datasets) and the information from the new domain (the dataset from the task we want to solve). </span><span class="koboSpan" id="kobo.78.2">There are two main significant advantages to this approach. </span><span class="koboSpan" id="kobo.78.3">On the one hand, pre-training datasets are typically large-scale (ImageNet-22k has 14 million images), and using a pre-trained model saves us that training time. </span><span class="koboSpan" id="kobo.78.4">On the other hand, we use our specific dataset not only for evaluation but also for training the model, improving its performance in the desired scenario. </span><span class="koboSpan" id="kobo.78.5">As we will discover, there is not always an easy way to achieve this, as it requires the capability to obtain a sizable dataset, or even one right way, as it might not yield the expected results. </span><span class="koboSpan" id="kobo.78.6">We will also explore the optional next step after transfer learning, called fine-tuning, where we will try to use our specific dataset to modify the model parameters even further. </span><span class="koboSpan" id="kobo.78.7">We will put both techniques to </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">the test.</span></span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.80.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.81.1">As for previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">at all.</span></span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.83.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.84.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.86.1">Introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">transfer learning</span></span></li>
<li><span class="koboSpan" id="kobo.88.1">Describing the advantages of transfer learning and when to </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">use it</span></span></li>
<li><span class="koboSpan" id="kobo.90.1">Understanding </span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.91.1">the</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.92.1"> fundamentals of </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">representation learning</span></span></li>
<li><span class="koboSpan" id="kobo.94.1">Focusing on </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">practical applications</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.96.1">Let’s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.98.1">Introducing transfer learning</span></h3>
<p><span class="koboSpan" id="kobo.99.1">In the previous </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.100.1">chapters, we learned how to train deep learning neural networks from scratch, exploring problems in CV and NLP. </span><span class="koboSpan" id="kobo.100.2">As introduced in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.101.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.102.1">, </span><em class="italic"><span class="koboSpan" id="kobo.103.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.104.1">, deep learning neural networks try to imitate the biological networks in our brains. </span><span class="koboSpan" id="kobo.104.2">One interesting point of view is that when we (and our brains) learn new tasks, we leverage previous knowledge we have acquired in a very strong way. </span><span class="koboSpan" id="kobo.104.3">For example, a very good tennis player will become a relatively good player at squash with a few hours of play. </span><span class="koboSpan" id="kobo.104.4">Transfer learning is a field of study that contains different techniques to achieve similar results as in </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">this example.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<span class="koboSpan" id="kobo.106.1"><img alt="Figure 7.1 – Comparison between traditional ﻿machine ﻿learning (ML) and transfer learning" src="image/B16591_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.107.1">Figure 7.1 – Comparison between traditional machine learning (ML) and transfer learning</span></p>
<p><span class="koboSpan" id="kobo.108.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.109.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.110.1">.1</span></em><span class="koboSpan" id="kobo.111.1">, we can see a comparison between both paradigms where, in transfer learning, the approach to solving Task 2 leverages the knowledge acquired while solving Task 1. </span><span class="koboSpan" id="kobo.111.2">This implies, however, that to solve a single desired task (Task 2), we are training the model twice (for Task 1 and for Task 2 later). </span><span class="koboSpan" id="kobo.111.3">In practice, as we will see in the next steps, we will work </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.112.1">with pre-trained models from MXNet’s GluonCV and GluonNLP model zoos, and therefore, we will only have to train the model once, for </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">Task 2.</span></span></p>
<h3><span class="koboSpan" id="kobo.114.1">Describing the advantages of transfer learning and when to use it</span></h3>
<p><span class="koboSpan" id="kobo.115.1">There are several</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.116.1"> reasons why using transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">offers advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">Faster</span></strong><span class="koboSpan" id="kobo.119.1">: As we leverage pre-trained models from model zoos, the training will converge much faster than training from scratch, requiring much fewer epochs and </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">less time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.121.1">More general</span></strong><span class="koboSpan" id="kobo.122.1">: Typically, pre-trained models have been trained in large-scale datasets (such as ImageNet); therefore, the parameters (weights) learned are generalistic and can then be reused for a large number of tasks. </span><span class="koboSpan" id="kobo.122.2">It is an objective that outputs from the feature extraction part of the pre-trained model (also known </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.123.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.124.1">representations</span></strong><span class="koboSpan" id="kobo.125.1">), learned by training using large-scale datasets that are general and domain-invariant (can </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">be reused).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.127.1">Requires less data</span></strong><span class="koboSpan" id="kobo.128.1">: To adapt a pre-trained model for a given new task, the amount of data required is much less than for training that specific model architecture from scratch. </span><span class="koboSpan" id="kobo.128.2">This is because representations can be reused (as mentioned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">previous point).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.130.1">More environmentally friendly</span></strong><span class="koboSpan" id="kobo.131.1">: As the training time, datasets, and compute requirements for transfer learning are much lower than training from scratch, less pollution is required to train </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">a model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.133.1">Performance improvements</span></strong><span class="koboSpan" id="kobo.134.1">: It has been proven (for example, in </span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf"><span class="koboSpan" id="kobo.135.1">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf</span></a><span class="koboSpan" id="kobo.136.1">) that using transfer learning with small-scale datasets yields strong performance improvements, and on large-scale datasets, the </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.137.1">same performance point is achieved much faster than training </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">from scratch.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.139.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.140.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.141.1">.2</span></em><span class="koboSpan" id="kobo.142.1">, different methods to compute representations are analyzed, and although specialized networks can reach better performance, this is only possible if large-scale datasets, high-end compute resources, and longer training times </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">are given.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<span class="koboSpan" id="kobo.144.1"><img alt="Figure 7.2 – Comparing different approaches for representations" src="image/B16591_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.145.1">Figure 7.2 – Comparing different approaches for representations</span></p>
<p><span class="koboSpan" id="kobo.146.1">In a more general setting, there are different ways to achieve transfer learning, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<span class="koboSpan" id="kobo.148.1"><img alt="Figure 7.3 – Different types of transfer learning" src="image/B16591_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.149.1">Figure 7.3 – Different types of transfer learning</span></p>
<p><span class="koboSpan" id="kobo.150.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.152.1">.3</span></em><span class="koboSpan" id="kobo.153.1">, we can</span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.154.1"> see the different types of transfer learning, depending on the similarity of the source and target domain and the availability of source and target data. </span><span class="koboSpan" id="kobo.154.2">In this chapter, we will explore the usual setting of having a pre-trained model in a similar domain to our intended task (equal source and target domain), and the tasks will be slightly different, with some amount of labeled data in the target</span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.155.1"> domain (</span><strong class="bold"><span class="koboSpan" id="kobo.156.1">inductive </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.157.1">transfer learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">Andrew Ng, chief scientist of Baidu and co-founder of Google Brain, said the following in a tutorial in NIPS 2016 called </span><em class="italic"><span class="koboSpan" id="kobo.160.1">Nuts and Bolts of Building AI Applications Using Deep Learning</span></em><span class="koboSpan" id="kobo.161.1">: “</span><em class="italic"><span class="koboSpan" id="kobo.162.1">In the next few years, we’ll see a lot of concrete value driven through transfer learning</span></em><span class="koboSpan" id="kobo.163.1">,” and he</span><a id="_idIndexMarker769"/> <span class="No-Break"><span class="koboSpan" id="kobo.164.1">was right.</span></span></p>
<h3><span class="koboSpan" id="kobo.165.1">Understanding the fundamentals of representation learning</span></h3>
<p><span class="koboSpan" id="kobo.166.1">In this section, we will answer</span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.167.1"> the question, from a more theoretical point of view, about how to use transfer learning and why it works. </span><span class="koboSpan" id="kobo.167.2">In </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.168.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.169.1">, </span><em class="italic"><span class="koboSpan" id="kobo.170.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.171.1">, and </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.172.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.173.1">, </span><em class="italic"><span class="koboSpan" id="kobo.174.1">Understanding Text with Natural Language Processing</span></em><span class="koboSpan" id="kobo.175.1">, we introduced the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">representations</span></strong><span class="koboSpan" id="kobo.177.1"> for features in images using GluonCV and for words/sentences in text </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">using GluonNLP.</span></span></p>
<p><span class="koboSpan" id="kobo.179.1">We can revisit, in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.181.1">.4</span></em><span class="koboSpan" id="kobo.182.1">, the usual architecture of a </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">CNN architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<span class="koboSpan" id="kobo.184.1"><img alt="Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)" src="image/B16591_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.185.1">Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)</span></p>
<p><span class="koboSpan" id="kobo.186.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.187.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.188.1">.5</span></em><span class="koboSpan" id="kobo.189.1">, we can revisit the usual </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">Transformer architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<span class="koboSpan" id="kobo.191.1"><img alt="Figure 7.5 – Refresh of the Transformer architecture (encoder on the left, and decoder on the right)" src="image/B16591_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.192.1">Figure 7.5 – Refresh of the Transformer architecture (encoder on the left, and decoder on the right)</span></p>
<p><span class="koboSpan" id="kobo.193.1">The underlying idea</span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.194.1"> is common in both fields; for example, the feature extractor part of CNNs and the encoder in Transformers are representations, and the training of these network sections is called </span><strong class="bold"><span class="koboSpan" id="kobo.195.1">representation learning</span></strong><span class="koboSpan" id="kobo.196.1">, an active field of study due to the capability of being able to train these networks in both supervised and </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">unsupervised settings.</span></span></p>
<p><span class="koboSpan" id="kobo.198.1">The main idea behind transfer learning is to transfer the representations learned in a task to a different task; therefore, we will typically follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">next steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.200.1">Retrieve a pre-trained model from MXNet’s Model Zoo (GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">or GluonNLP).</span></span></li>
<li><span class="koboSpan" id="kobo.202.1">Remove the last layers (typically, a classifier). </span><span class="koboSpan" id="kobo.202.2">Keep the parameters in the rest of the layers frozen (not updatable </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">during training).</span></span></li>
<li><span class="koboSpan" id="kobo.204.1">Add new layers (a new classifier) corresponding to the </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">new task</span></span></li>
<li><span class="koboSpan" id="kobo.206.1">Train the updated model (only the new layers, not frozen, will be updated during training) with the </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">target data.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.208.1">If we have enough labeled </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.209.1">data for the task that we want to solve (target task), another step (that can be done after the previous step or substituting it) is</span><a id="_idIndexMarker773"/> <span class="No-Break"><span class="koboSpan" id="kobo.210.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.211.1">fine-tuning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">Fine-tuning takes into account that the representations originally learned might not fit perfectly with the target task and, therefore, could also improve with updating. </span><span class="koboSpan" id="kobo.213.2">In this scenario, the steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.215.1">Unfreeze the weights of the </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">representation network.</span></span></li>
<li><span class="koboSpan" id="kobo.217.1">Retrain the network with target data, typically with a smaller learning rate as the representations should be close (</span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">same domain).</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.219.1">Both processes (transfer learning and fine-tuning) are summarized visually in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.220.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.221.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<span class="koboSpan" id="kobo.223.1"><img alt="Figure 7.6 – Transfer learning and fine-tuning" src="image/B16591_07_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.224.1">Figure 7.6 – Transfer learning and fine-tuning</span></p>
<p><span class="koboSpan" id="kobo.225.1">Both processes can be</span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.226.1"> applied sequentially, with</span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.227.1"> adequate </span><strong class="bold"><span class="koboSpan" id="kobo.228.1">hyperparameters</span></strong><span class="koboSpan" id="kobo.229.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">each one.</span></span></p>
<h3><span class="koboSpan" id="kobo.231.1">Focusing on practical applications</span></h3>
<p><span class="koboSpan" id="kobo.232.1">In this section, we will use </span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.233.1">what we have learned so far about representation learning and we will apply it to a practical example: detecting cats </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">and dogs.</span></span></p>
<p><span class="koboSpan" id="kobo.235.1">To do this, we will retrieve a model from</span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.236.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.237.1">GluonCV Model Zoo</span></strong><span class="koboSpan" id="kobo.238.1">; we will remove the classifier (last layers) and keep the feature extraction stage. </span><span class="koboSpan" id="kobo.238.2">We will then analyze how the representations of the cats and dogs have been learned. </span><span class="koboSpan" id="kobo.238.3">To load the model, we can use this </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">code snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.240.1">
alexnet = gcv.model_zoo.get_model("resnet152_v2", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.241.1">In the previous code snippet, for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">pretrained</span></strong><span class="koboSpan" id="kobo.243.1"> parameter, we have assigned the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">True</span></strong><span class="koboSpan" id="kobo.245.1">, indicating that we want the pretrained weights to be retrieved (and not only the architecture of </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">the model).</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">When trained correctly, CNNs learn hierarchical representations of the features of the images in the training dataset, with each progressive layer learning more and more complex patterns. </span><span class="koboSpan" id="kobo.247.2">Therefore, when an image is processed (when processing on successive layers), the network can compute more complex patterns associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">the network.</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">Now, we can use a new MXNet library, MXBoard (see the recipe for installation instructions), with this model to evaluate the different steps that a dog image goes through and see some examples of how a pre-trained model computes </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">its representations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<span class="koboSpan" id="kobo.251.1"><img alt="Figure 7.7 – Cat and dog representations – convolutional filters" src="image/B16591_07_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.252.1">Figure 7.7 – Cat and dog representations – convolutional filters</span></p>
<p><span class="koboSpan" id="kobo.253.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.254.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.255.1">.7</span></em><span class="koboSpan" id="kobo.256.1">, we can</span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.257.1"> see the convolutional filters corresponding to the first convolutional layer of a ResNet152 pre-trained network (on ImageNet). </span><span class="koboSpan" id="kobo.257.2">Please note how these filters focus on simple patterns such as specific shapes (vertical and horizontal lines, circles, and so on) and specific colors (</span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">red blobs).</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">Let’s analyze the results with a </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">specific image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<span class="koboSpan" id="kobo.261.1"><img alt="Figure 7.8 – Example image of a dog" src="image/B16591_07_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.262.1">Figure 7.8 – Example image of a dog</span></p>
<p><span class="koboSpan" id="kobo.263.1">We select an image </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.264.1">from our </span><em class="italic"><span class="koboSpan" id="kobo.265.1">Dogs vs. </span><span class="koboSpan" id="kobo.265.2">Cats</span></em><span class="koboSpan" id="kobo.266.1"> dataset, such as the dog depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.267.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.268.1">.8</span></em><span class="koboSpan" id="kobo.269.1">. </span><span class="koboSpan" id="kobo.269.2">When passing this image through our network, we will find results similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<span class="koboSpan" id="kobo.271.1"><img alt="Figure 7.9 – Output from convolutional filters" src="image/B16591_07_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.272.1">Figure 7.9 – Output from convolutional filters</span></p>
<p><span class="koboSpan" id="kobo.273.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.275.1">.9</span></em><span class="koboSpan" id="kobo.276.1">, we can </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.277.1">see the output of the filters in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.278.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.279.1">.7</span></em><span class="koboSpan" id="kobo.280.1"> for our dog example. </span><span class="koboSpan" id="kobo.280.2">Note how different outputs highlight simple shapes such as the eyes or the legs (larger values, closer </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">to white).</span></span></p>
<p><span class="koboSpan" id="kobo.282.1">Finally, as the image traverses the network, its features are more and more compressed, yielding (for ResNet152) a final vector of 2,048 elements. </span><span class="koboSpan" id="kobo.282.2">This vector can be computed easily with networks retrieved using MXNet’s </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">Model Zoo:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.284.1">
resnet152.features(summary_image.as_in_context(ctx))</span></pre> <p><span class="koboSpan" id="kobo.285.1">This code excerpt provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.287.1">
[[2.5350871e-04 2.8519407e-01 1.6196619e-03 ... </span><span class="koboSpan" id="kobo.287.2">7.2884483e-05
  2.9618644e-07 7.8995163e-03]]
 &lt;NDArray 1x2048 @gpu(0)&gt;</span></pre> <p><span class="koboSpan" id="kobo.288.1">As we can see, we have a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">2048</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.290.1"> element.</span></span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.291.1">How it works…</span></h2>
<p><span class="koboSpan" id="kobo.292.1">In this recipe, we</span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.293.1"> introduced the concepts of transfer learning and</span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.294.1"> fine-tuning. </span><span class="koboSpan" id="kobo.294.2">We explained when it made sense to use these two different techniques and </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">their advantages.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">We also explored when these techniques can be useful and their connections to representation learning, explaining how representations play a significant role in the knowledge being transferred when using these techniques. </span><span class="koboSpan" id="kobo.296.2">We used a new library, </span><strong class="bold"><span class="koboSpan" id="kobo.297.1">MXBoard</span></strong><span class="koboSpan" id="kobo.298.1">, to </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.299.1">produce visualizations for </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">the representations.</span></span></p>
<p><span class="koboSpan" id="kobo.301.1">Moreover, we intuitively and practically showed how to apply these techniques to CV and NLP tasks and computed a representation for a </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">specific example.</span></span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.303.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.304.1">Transfer learning, including fine-tuning, is an active field of study. </span><span class="koboSpan" id="kobo.304.2">In this recipe, we have only covered the most useful scenario for deep learning, inductive transfer learning. </span><span class="koboSpan" id="kobo.304.3">For a more comprehensive but still easy-to-read introduction, I recommend reading </span><em class="italic"><span class="koboSpan" id="kobo.305.1">Transfer learning: a friendly introduction</span></em><span class="koboSpan" id="kobo.306.1">, which can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">at: </span></span><a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w"><span class="No-Break"><span class="koboSpan" id="kobo.308.1">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.309.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.310.1">Moreover, the concept of transferring knowledge from one system to another is not new, and there are references to concepts such as </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">learning to learn</span></strong><span class="koboSpan" id="kobo.312.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">knowledge transfer</span></strong><span class="koboSpan" id="kobo.314.1"> as early as 1995, when a </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">Neural Information Processing Systems</span></strong><span class="koboSpan" id="kobo.316.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.317.1">NeurIPS</span></strong><span class="koboSpan" id="kobo.318.1">) workshop on the topic was presented. </span><span class="koboSpan" id="kobo.318.2">A summary of the workshop can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">here: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">http://socrates.acadiau.ca/courses/comp/dsilver/nips95_ltl/nips95.workshop.pdf</span></span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">Furthermore, as introduced 21 years later in the same venue, Andrew Ng was able to correctly foresee the importance of transfer learning. </span><span class="koboSpan" id="kobo.322.2">His 2016 NeurIPS tutorial can be found here (jump to 1h 37m for the transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">quote): </span></span><a href="https://www.youtube.com/watch?v=F1ka6a13S9I"><span class="No-Break"><span class="koboSpan" id="kobo.324.1">https://www.youtube.com/watch?v=F1ka6a13S9I</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.325.1">.</span></span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.326.1">Improving performance for classifying images</span></h1>
<p><span class="koboSpan" id="kobo.327.1">After introducing transfer </span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.328.1">learning and fine-tuning in the previous recipe, in this one, we will apply it to </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">image classification</span></strong><span class="koboSpan" id="kobo.330.1">, a </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">CV task.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">In the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.333.1">Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.334.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.335.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.336.1">, </span><em class="italic"><span class="koboSpan" id="kobo.337.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.338.1">, we saw how we could use GluonCV to retrieve pre-trained models and use them directly for an image classification task. </span><span class="koboSpan" id="kobo.338.2">In the first instance, we looked at training them from scratch, effectively only leveraging past knowledge by using the architecture of the pre-trained model, without leveraging any past knowledge contained in the pre-trained weights, which were re-initialized, deleting any past information. </span><span class="koboSpan" id="kobo.338.3">Afterward, the pre-trained models were used directly for the task, effectively also leveraging the weights/parameters of </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">In this recipe, we will combine the weights/parameters of the model with the target dataset, applying the techniques introduced in this chapter, transfer learning and fine-tuning. </span><span class="koboSpan" id="kobo.340.2">The dataset used for the pre-training</span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.341.1"> was </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">ImageNet-1k</span></strong><span class="koboSpan" id="kobo.343.1"> (source task) and we will run several experiments to train and evaluate our models in a new (target) task, using </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.344.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">Dogs vs </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">Cats</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.347.1"> dataset.</span></span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.348.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.349.1">As for previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">Furthermore, we will be working with text datasets; therefore, we will revisit some concepts already seen in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.352.1">Classifying images with MXNet: GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.353.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.354.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.355.1">, </span><em class="italic"><span class="koboSpan" id="kobo.356.1">Analyzing Images with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.357.1">Computer Vision</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">.</span></span></p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.359.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.360.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.362.1">Revisiting the </span><em class="italic"><span class="koboSpan" id="kobo.363.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.364.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.365.1">Dogs vs. </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.366.1">Cats</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.367.1"> datasets</span></span></li>
<li><span class="koboSpan" id="kobo.368.1">Training</span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.369.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">ResNet</span></strong><span class="koboSpan" id="kobo.371.1"> model from scratch with </span><em class="italic"><span class="koboSpan" id="kobo.372.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.373.1">vs Cats</span></em></span></li>
<li><span class="koboSpan" id="kobo.374.1">Using a pre-trained ResNet model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.375.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.376.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.377.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.378.1">vs Cats</span></em></span></li>
<li><span class="koboSpan" id="kobo.379.1">Fine-tuning our pre-trained ResNet model on </span><em class="italic"><span class="koboSpan" id="kobo.380.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.381.1">vs Cats</span></em></span></li>
</ol>
<p><span class="koboSpan" id="kobo.382.1">Let’s look at these</span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.383.1"> steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">detail next.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.385.1">Revisiting the ImageNet-1k and Dogs vs Cats datasets</span></h2>
<p><em class="italic"><span class="koboSpan" id="kobo.386.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.387.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.388.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.389.1"> are both </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.390.1">image classification datasets; however, they are quite</span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.391.1"> different. </span><em class="italic"><span class="koboSpan" id="kobo.392.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.393.1"> is a large-scale</span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.394.1"> dataset</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.395.1"> containing ~1.2 million images labeled into 1,000 classes and has been used extensively in research and academia for benchmarking. </span><em class="italic"><span class="koboSpan" id="kobo.396.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.397.1"> is a small-scale dataset containing 1,400 images depicting either a dog or a cat, and its fame is mostly due to a Kaggle competition launched </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">in 2013.</span></span></p>
<p><span class="koboSpan" id="kobo.399.1">MXNet GluonCV does not provide methods to directly download any of the datasets. </span><span class="koboSpan" id="kobo.399.2">However, we do not need the </span><em class="italic"><span class="koboSpan" id="kobo.400.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.401.1"> dataset (its size is ~133 GB), only the pre-trained parameters for our chosen model. </span><span class="koboSpan" id="kobo.401.2">The pre-trained models can be downloaded directly from the MXNet GluonCV Model Zoo, we have seen examples in previous chapters and we will use them again in </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">this one.</span></span></p>
<p><span class="koboSpan" id="kobo.403.1">Here are some examples </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">from </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.405.1">ImageNet-1k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<span class="koboSpan" id="kobo.407.1"><img alt="Figure 7.10 – ImageNet-1k examples" src="image/B16591_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.408.1">Figure 7.10 – ImageNet-1k examples</span></p>
<p><span class="koboSpan" id="kobo.409.1">The source </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.410.1">of </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.411.1">the</span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.412.1"> preceding</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.413.1"> figure </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">is </span></span><a href="https://cs.stanford.edu/people/karpathy/cnnembed/"><span class="No-Break"><span class="koboSpan" id="kobo.415.1">https://cs.stanford.edu/people/karpathy/cnnembed/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.416.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.417.1">For </span><em class="italic"><span class="koboSpan" id="kobo.418.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.419.1">, all the information on how to retrieve the dataset can be found in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.420.1">Classifying images with MXNet: GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.421.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.422.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.423.1">, </span><em class="italic"><span class="koboSpan" id="kobo.424.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.425.1">. </span><span class="koboSpan" id="kobo.425.2">Taking that recipe’s code as a reference, we can display </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">some examples:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<span class="koboSpan" id="kobo.427.1"><img alt="Figure 7.11 – Dogs vs Cats dataset" src="image/B16591_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.428.1">Figure 7.11 – Dogs vs Cats dataset</span></p>
<p><span class="koboSpan" id="kobo.429.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.430.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.431.1">.10</span></em><span class="koboSpan" id="kobo.432.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.434.1">.11</span></em><span class="koboSpan" id="kobo.435.1">, we can</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.436.1"> see </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.437.1">how</span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.438.1"> some images from </span><em class="italic"><span class="koboSpan" id="kobo.439.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.440.1"> resemble </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.441.1">some of the images from </span><em class="italic"><span class="koboSpan" id="kobo.442.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.443.1">vs Cats</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.445.1">Training a ResNet model from scratch with Dogs vs Cats</span></h3>
<p><span class="koboSpan" id="kobo.446.1">As</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.447.1"> described</span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.448.1"> in the </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.449.1">second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.450.1">Classifying images with MXNet: GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.451.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.452.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.453.1">, </span><em class="italic"><span class="koboSpan" id="kobo.454.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.455.1">, we will be </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.456.1">using </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">softmax cross-entropy</span></strong><span class="koboSpan" id="kobo.458.1"> as the loss function and </span><strong class="bold"><span class="koboSpan" id="kobo.459.1">accuracy</span></strong><span class="koboSpan" id="kobo.460.1"> and the </span><strong class="bold"><span class="koboSpan" id="kobo.461.1">confusion matrix</span></strong><span class="koboSpan" id="kobo.462.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">evaluation </span></span><span class="No-Break"><a id="_idIndexMarker805"/></span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.465.1">We have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">ResNet model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<span class="koboSpan" id="kobo.467.1"><img alt="Figure 7.12 – ResNet training evolution (training loss and validation loss, and validation accuracy) – training from scratch" src="image/B16591_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.468.1">Figure 7.12 – ResNet training evolution (training loss and validation loss, and validation accuracy) – training from scratch</span></p>
<p><span class="koboSpan" id="kobo.469.1">Furthermore, for</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.470.1"> the </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.471.1">best</span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.472.1"> iteration, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">accuracy</span></strong><span class="koboSpan" id="kobo.474.1"> value obtained in the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.476.1">
('accuracy', 0.75)</span></pre> <p><span class="koboSpan" id="kobo.477.1">The confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<span class="koboSpan" id="kobo.479.1"><img alt="Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained from scratch" src="image/B16591_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.480.1">Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained from scratch</span></p>
<p><span class="koboSpan" id="kobo.481.1">Both the accuracy </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.482.1">value </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.483.1">obtained (75%) and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.484.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.485.1">.13</span></em><span class="koboSpan" id="kobo.486.1"> show quite average</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.487.1"> performance after training for several epochs (100, in this example). </span><span class="koboSpan" id="kobo.487.2">You are encouraged to run your own experiments trying out </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">different hyperparameters.</span></span></p>
<p><span class="koboSpan" id="kobo.489.1">Qualitatively, we can also check how well our model is performing with an example image. </span><span class="koboSpan" id="kobo.489.2">In our case, we chose </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<span class="koboSpan" id="kobo.491.1"><img alt="Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat" src="image/B16591_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.492.1">Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat</span></p>
<p><span class="koboSpan" id="kobo.493.1">We can check the</span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.494.1"> output of </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.495.1">our</span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.496.1"> model by running this image through it with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">code snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.498.1">
# Qualitative Evaluation
# Qualitative Evaluation
# Expected Output
print("Expected Output:", example_label)
# Model Output
example_output = resnet50_ft(example_image_preprocessed)
 class_output = np.argmax(example_output, axis=1).asnumpy()[0]
 print("Class Output:", class_output)
assert class_output == 0 # Cat 0</span></pre> <p><span class="koboSpan" id="kobo.499.1">These code statements will give us the </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">following output:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.501.1">
Expected Output: 0
Class Output: 0</span></pre> <p><span class="koboSpan" id="kobo.502.1">As can be seen </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.503.1">from</span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.504.1"> the</span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.505.1"> results, the image has been correctly classified as </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">a cat.</span></span></p>
<h3><span class="koboSpan" id="kobo.507.1">Using a pre-trained ResNet model to optimize performance via transfer learning from ImageNet-1k to Dogs vs Cats</span></h3>
<p><span class="koboSpan" id="kobo.508.1">In the</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.509.1"> previous </span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.510.1">recipe, we</span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.511.1"> trained a new model from scratch using our dataset. </span><span class="koboSpan" id="kobo.511.2">However, this has two </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">important drawbacks:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.513.1">A large amount of data is required for training </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">from scratch.</span></span></li>
<li><span class="koboSpan" id="kobo.515.1">The training process can take a long time due to the large size of the dataset and the number of epochs needed for the model to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">the task.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.517.1">Therefore, in this recipe, we will follow a different approach: we will use pre-trained models from MXNet GluonCV to solve the task. </span><span class="koboSpan" id="kobo.517.2">These models have been trained in </span><em class="italic"><span class="koboSpan" id="kobo.518.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.519.1">, a dataset that contains the classes we are interested in (cats and dogs); therefore, we can use those learned representations and easily transfer them to </span><em class="italic"><span class="koboSpan" id="kobo.520.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.521.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">same domain).</span></span></p>
<p><span class="koboSpan" id="kobo.523.1">For a ResNet model, use </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.525.1">
# ResNet50 from Model Zoo (This downloads v1d)
 resnet50 = gcv.model_zoo.get_model("resnet50_v1d", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.526.1">As we can see in the previous code snippet, following the discussion in this chapter’s first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.527.1">Understanding transfer learning and fine-tuning</span></em><span class="koboSpan" id="kobo.528.1">, for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">pretrained</span></strong><span class="koboSpan" id="kobo.530.1"> parameter, we have assigned the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">True</span></strong><span class="koboSpan" id="kobo.532.1">, indicating that we want the pre-trained weights to be retrieved (and not only the architecture of </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">the model).</span></span></p>
<p><span class="koboSpan" id="kobo.534.1">In order to adequately evaluate the improvements that transfer learning brings, we are going to evaluate our pre-trained model directly (the source task is </span><em class="italic"><span class="koboSpan" id="kobo.535.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.536.1">) before applying transfer learning to </span><em class="italic"><span class="koboSpan" id="kobo.537.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.538.1"> and after applying it. </span><span class="koboSpan" id="kobo.538.2">Therefore, using our </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.539.1">pre-trained </span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.540.1">model </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.541.1">as is, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.543.1">
('accuracy', 0.925)</span></pre> <p><span class="koboSpan" id="kobo.544.1">The confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<span class="koboSpan" id="kobo.546.1"><img alt="Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model" src="image/B16591_07_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.547.1">Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model</span></p>
<p><span class="koboSpan" id="kobo.548.1">As we can see, our pre-trained Transformer model is already showing good performance values as it is the same domain; however, simply using a pre-trained model does not yield better performance than training from scratch. </span><span class="koboSpan" id="kobo.548.2">The great advantage of using pre-trained models is </span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.549.1">the </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.550.1">time savings, as loading one just takes a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">of code.</span></span></p>
<p><span class="koboSpan" id="kobo.552.1">We can also check how well our model is performing qualitatively with the same image example. </span><span class="koboSpan" id="kobo.552.2">Note how the code slightly differs from the previous qualitative image excerpt, as now we need to convert ImageNet classes (the output of our ResNet50 pre-trained model) to our classes (</span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">0</span></strong><span class="koboSpan" id="kobo.554.1"> for cats and </span><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">1</span></strong><span class="koboSpan" id="kobo.556.1"> for dogs). </span><span class="koboSpan" id="kobo.556.2">The new code is given </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.558.1">
# Qualitative Evaluation
# Expected Output
print("Expected Output:", example_label)
# Model Output
example_output = resnet50(example_image_preprocessed)
class_output = model.CLASSES_DICT[np.argmax(example_output, axis=1).asnumpy()[0]]
print("Class Output:", class_output)
assert class_output == 0 # Cat</span></pre> <p><span class="koboSpan" id="kobo.559.1">These code </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.560.1">statements</span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.561.1"> will give us the </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">following </span></span><span class="No-Break"><a id="_idIndexMarker828"/></span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">output:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.564.1">
Expected Output: 0
Class Output: 0</span></pre> <p><span class="koboSpan" id="kobo.565.1">As can be seen from the result, the image has been correctly classified as </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">a cat.</span></span></p>
<p><span class="koboSpan" id="kobo.567.1">Now that we have a baseline for comparison, let’s apply transfer learning to our task. </span><span class="koboSpan" id="kobo.567.2">From the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.568.1">Understanding transfer learning and fine-tuning</span></em><span class="koboSpan" id="kobo.569.1">, the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV or GluonNLP), which we have </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">already done.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">The second step was to remove the last layers (typically, a classifier), keeping the parameters in the rest of the layers frozen (not updatable during training), so let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">do it!</span></span></p>
<p><span class="koboSpan" id="kobo.573.1">We can replace the classifier with the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.575.1">
# Replace the classifier (with gradients activated)
 resnet50_tl.fc = mx.gluon.nn.Dense(2)
 resnet50_tl.fc.initialize(ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.576.1">We can freeze the ResNet feature extraction layers with the </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.578.1">
for param in resnet50_tl.collect_params().values():
param.grad_req = 'null'</span></pre> <p><span class="koboSpan" id="kobo.579.1">We can replace the classifier with the </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.581.1">
# Replace the classifier (with gradients activated)
 resnet50_tl.fc = mx.gluon.nn.Dense(2)
 resnet50_tl.fc.initialize(ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.582.1">Now, we can apply </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.583.1">the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.584.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.585.1">, and we</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.586.1"> have the following evolution in the </span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.587.1">training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">ResNet model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<span class="koboSpan" id="kobo.589.1"><img alt="Figure 7.16 – ResNet training evolution (training loss and validation loss) – transfer learning" src="image/B16591_07_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.590.1">Figure 7.16 – ResNet training evolution (training loss and validation loss) – transfer learning</span></p>
<p><span class="koboSpan" id="kobo.591.1">Furthermore, for </span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.592.1">the best iteration, the accuracy obtained in </span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.593.1">the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.595.1">
('accuracy', 0.985)</span></pre> <p><span class="koboSpan" id="kobo.596.1">The confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<span class="koboSpan" id="kobo.598.1"><img alt="Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer learning" src="image/B16591_07_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.599.1">Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer learning</span></p>
<p><span class="koboSpan" id="kobo.600.1">Compared with our</span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.601.1"> previous experiment of training from scratch, this </span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.602.1">experiment yields much higher performance, and it took us literally minutes to get this model to start working well for us in our intended task, whereas the training required for the previous experiment took hours and required several tries to tune the hyperparameters, which can then turn into several days of effort </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">in total.</span></span></p>
<p><span class="koboSpan" id="kobo.604.1">We can also check how well our model is performing qualitatively with the same image example and</span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.605.1"> code. </span><span class="koboSpan" id="kobo.605.2">The</span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.606.1"> output</span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.607.1"> is given </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.609.1">
Expected Output: 0
Class Output: 0</span></pre> <p><span class="koboSpan" id="kobo.610.1">As can be seen from the result, the image has been correctly classified as </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">a cat.</span></span></p>
<h3><span class="koboSpan" id="kobo.612.1">Fine-tuning our pre-trained ResNet model on Dogs vs Cats</span></h3>
<p><span class="koboSpan" id="kobo.613.1">In the</span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.614.1"> previous</span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.615.1"> recipe, we </span><em class="italic"><span class="koboSpan" id="kobo.616.1">froze</span></em><span class="koboSpan" id="kobo.617.1"> the </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.618.1">parameters in the encoder layers. </span><span class="koboSpan" id="kobo.618.2">However, as the dataset we are currently working with (</span><em class="italic"><span class="koboSpan" id="kobo.619.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.620.1">) has enough data samples, we can </span><em class="italic"><span class="koboSpan" id="kobo.621.1">unfreeze</span></em><span class="koboSpan" id="kobo.622.1"> those parameters and train the model, effectively allowing the new training process to update the representations (with transfer learning, we were working directly with the representations learned for </span><em class="italic"><span class="koboSpan" id="kobo.623.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.624.1">). </span><span class="koboSpan" id="kobo.624.2">This process is </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">called fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.626.1">There are two variants </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">of fine-tuning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.628.1">Applying transfer learning by freezing the layers and unfreezing them afterward (fine-tuning after </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">transfer learning)</span></span></li>
<li><span class="koboSpan" id="kobo.630.1">Directly applying fine-tuning without the preliminary step of freezing the layers (</span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">fine-tuning directly)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.632.1">Let’s compute both experiments and draw conclusions by comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.634.1">For the first experiment, we can take the network obtained in the previous recipe, unfreeze the layers, and restart the training. </span><span class="koboSpan" id="kobo.634.2">In MXNet, to unfreeze the encoder parameters, we can run the </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.636.1">
# Un-freeze weights
for param in resnet50_ft.collect_params().values():
    if param.name in updated_params:
        param.grad_req = 'write'</span></pre> <p><span class="koboSpan" id="kobo.637.1">Now, we can apply the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.638.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.639.1">, and we have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">ResNet model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<span class="koboSpan" id="kobo.641.1"><img alt="Figure 7.18 – ResNet training evolution (training loss and validation loss) – fine-tuning after transfer learning" src="image/B16591_07_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.642.1">Figure 7.18 – ResNet training evolution (training loss and validation loss) – fine-tuning after transfer learning</span></p>
<p><span class="koboSpan" id="kobo.643.1">Furthermore, for</span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.644.1"> the </span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.645.1">best </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.646.1">iteration, the accuracy obtained in the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.648.1">
('accuracy', 0.90255)</span></pre> <p><span class="koboSpan" id="kobo.649.1">The confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<span class="koboSpan" id="kobo.651.1"><img alt="Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning after transfer learning" src="image/B16591_07_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.652.1">Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning after transfer learning</span></p>
<p><span class="koboSpan" id="kobo.653.1">Compared </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.654.1">with our</span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.655.1"> previous </span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.656.1">experiment of transfer learning, this experiment yields worse performance. </span><span class="koboSpan" id="kobo.656.2">This is due to a combination of the size of the dataset and the hyperparameters chosen. </span><span class="koboSpan" id="kobo.656.3">You are encouraged to try your </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">own experiments.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">We can also check how well our model is performing qualitatively with the same image example and code. </span><span class="koboSpan" id="kobo.658.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.660.1">
 Expected Output: 0
Class Output: 0</span></pre> <p><span class="koboSpan" id="kobo.661.1">As can be seen from the results, the image has been correctly classified as </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">a cat.</span></span></p>
<p><span class="koboSpan" id="kobo.663.1">Let’s continue now with the second fine-tuning experiment where, instead of applying transfer learning, we apply fine-tuning directly to the whole model (no </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">frozen layers).</span></span></p>
<p><span class="koboSpan" id="kobo.665.1">We need to again retrieve the pre-trained ResNet model for </span><em class="italic"><span class="koboSpan" id="kobo.666.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.667.1">, with the following code snippet for </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">MXNet GluonCV:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.669.1">
# ResNet50 from Model Zoo (This downloads v1d)
 resnet50 = gcv.model_zoo.get_model("resnet50_v1d", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.670.1">And now, without freezing, we can apply the training process, which will update all layers of our ResNet </span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.671.1">model, giving</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.672.1"> the following </span><a id="_idIndexMarker850"/><span class="No-Break"><span class="koboSpan" id="kobo.673.1">loss curves:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<span class="koboSpan" id="kobo.674.1"><img alt="Figure 7.20 – ResNet training evolution (training loss and validation loss) – fine-tuning without freezing" src="image/B16591_07_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.675.1">Figure 7.20 – ResNet training evolution (training loss and validation loss) – fine-tuning without freezing</span></p>
<p><span class="koboSpan" id="kobo.676.1">Furthermore, for the best iteration, the accuracy obtained in the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.678.1">
('accuracy', 0.98)</span></pre> <p><span class="koboSpan" id="kobo.679.1">The value is similar to the previous experiment with transfer learning. </span><span class="koboSpan" id="kobo.679.2">For the confusion matrix, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<span class="koboSpan" id="kobo.681.1"><img alt="Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning without freezing" src="image/B16591_07_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.682.1">Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning without freezing</span></p>
<p><span class="koboSpan" id="kobo.683.1">As </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.684.1">mentioned, compared</span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.685.1"> with </span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.686.1">our previous fine-tuning experiment, we can see how this experiment yields higher performance. </span><span class="koboSpan" id="kobo.686.2">Empirically, this has been proven to be a repeatable result and has been indicated to be because initially freezing the encoder allows for the decoder to learn (using the encoder representations) the new task at hand. </span><span class="koboSpan" id="kobo.686.3">From an information flow point of view, in this step, there is a knowledge transfer from the feature extraction stage to the classifier. </span><span class="koboSpan" id="kobo.686.4">In the secondary step when the feature extraction stage is unfrozen, the learned parameters from the classifier perform auxiliary transfer learning – this time, from the classifier to the feature </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">extraction stage.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">We can also check how well our model is performing qualitatively with the same image example and </span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.689.1">code. </span><span class="koboSpan" id="kobo.689.2">The </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.690.1">output</span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.691.1"> is given </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.693.1">
Expected Output: 0
Class Output: 0</span></pre> <p><span class="koboSpan" id="kobo.694.1">As can be seen from the results, the image has been correctly classified as </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">a cat.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.696.1">How it works…</span></h2>
<p><span class="koboSpan" id="kobo.697.1">In this recipe, we </span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.698.1">applied the techniques of transfer learning and fine-tuning, introduced at the beginning of the chapter, to the task of image classification, which was also presented previously, in the second recipe, </span><em class="italic"><span class="koboSpan" id="kobo.699.1">Classifying images with MXNet: GluonCV Model Zoo, AlexNet, and ResNet</span></em><span class="koboSpan" id="kobo.700.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.701.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.702.1">, </span><em class="italic"><span class="koboSpan" id="kobo.703.1">Analyzing Images with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.704.1">Computer Vision</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.706.1">We revisited two known datasets, </span><em class="italic"><span class="koboSpan" id="kobo.707.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.708.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.709.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.710.1">, which we intended to combine using knowledge transfer based on the former dataset and refining that knowledge with the latter. </span><span class="koboSpan" id="kobo.710.2">Moreover, this was achieved by leveraging the tools that MXNet </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">GluonCV provided:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.712.1">A pre-trained ResNet model </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">for </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.714.1">ImageNet-1k</span></em></span></li>
<li><span class="koboSpan" id="kobo.715.1">Tools for easy-to-use access to </span><em class="italic"><span class="koboSpan" id="kobo.716.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.717.1">vs Cats</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.718.1">Furthermore, we continued using the loss functions and metrics introduced for image classification, softmax cross-entropy, accuracy, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">confusion matrix.</span></span></p>
<p><span class="koboSpan" id="kobo.720.1">Having all these tools readily available within MXNet and GluonCV allowed us to run the following experiments with just a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">of code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.722.1">Training a model from scratch in </span><em class="italic"><span class="koboSpan" id="kobo.723.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.724.1">vs Cats</span></em></span></li>
<li><span class="koboSpan" id="kobo.725.1">Using a pre-trained model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.726.1">ImageNet-1k</span></em><span class="koboSpan" id="kobo.727.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.728.1">Dogs </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.729.1">vs Cats</span></em></span></li>
<li><span class="koboSpan" id="kobo.730.1">Fine-tuning our pre-trained model on </span><em class="italic"><span class="koboSpan" id="kobo.731.1">Dogs vs Cats</span></em><span class="koboSpan" id="kobo.732.1"> (with and without </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">freezing layers)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.734.1">After running the different experiments, we obtained an effective tie between transfer learning and fine-tuning directly (accuracies of 0.985 and 0.98, respectively). </span><span class="koboSpan" id="kobo.734.2">The actual results obtained when running these experiments might differ based on model architecture, datasets, and hyperparameters chosen, so you are encouraged to try out different techniques </span><a id="_idIndexMarker858"/><span class="No-Break"><span class="koboSpan" id="kobo.735.1">and variations.</span></span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.736.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.737.1">Transfer learning, including fine-tuning, is an active field of research. </span><span class="koboSpan" id="kobo.737.2">A recent paper published in 2022 explores the latest advances in image classification. </span><span class="koboSpan" id="kobo.737.3">The paper is titled </span><em class="italic"><span class="koboSpan" id="kobo.738.1">Deep Transfer Learning for Image Classification: A survey</span></em><span class="koboSpan" id="kobo.739.1">, and can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">here: </span></span><a href="https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey"><span class="No-Break"><span class="koboSpan" id="kobo.741.1">https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.742.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.743.1">For a more general approach to CV use cases, a recent paper was published, </span><em class="italic"><span class="koboSpan" id="kobo.744.1">Transfer Learning Methods as a New Approach in Computer Vision Tasks with Small Datasets</span></em><span class="koboSpan" id="kobo.745.1">, where the problem of small datasets is evaluated, and these techniques are applied to solve medical imaging tasks. </span><span class="koboSpan" id="kobo.745.2">It can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">here: </span></span><a href="https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets"><span class="No-Break"><span class="koboSpan" id="kobo.747.1">https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.748.1">.</span></span></p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.749.1">Improving performance for segmenting images</span></h1>
<p><span class="koboSpan" id="kobo.750.1">In this recipe, we</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.751.1"> will apply transfer learning and fine-tuning</span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.752.1"> to </span><strong class="bold"><span class="koboSpan" id="kobo.753.1">semantic segmentation</span></strong><span class="koboSpan" id="kobo.754.1">, a </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">CV task.</span></span></p>
<p><span class="koboSpan" id="kobo.756.1">In the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.757.1">Segmenting objects in images with MXNet: PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.758.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.759.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.760.1">, </span><em class="italic"><span class="koboSpan" id="kobo.761.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.762.1">, we saw how we could use GluonCV to retrieve pre-trained models and use them directly for a semantic segmentation task, effectively leveraging past knowledge by using the architecture and the weights/parameters of the </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">pre-trained model.</span></span></p>
<p><span class="koboSpan" id="kobo.764.1">In this recipe, we will continue leveraging the weights/parameters of the model, obtained for a task consisting of classifying images among a set of 21 classes using semantic segmentation models. </span><span class="koboSpan" id="kobo.764.2">The dataset used for the pre-training was </span><em class="italic"><span class="koboSpan" id="kobo.765.1">MS COCO</span></em><span class="koboSpan" id="kobo.766.1"> (source task) and we will run several experiments to evaluate our models in a new (target) task, using </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.767.1">the </span><em class="italic"><span class="koboSpan" id="kobo.768.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.769.1"> dataset. </span><span class="koboSpan" id="kobo.769.2">In these experiments, we will also include knowledge from the target dataset to improve our semantic </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">classification performance.</span></span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.771.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.772.1">As for previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.774.1">Furthermore, we will be working with text datasets; therefore, we will revisit some concepts already seen in the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.775.1">Segmenting objects in images with MXNet: PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.776.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.777.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.778.1">, </span><em class="italic"><span class="koboSpan" id="kobo.779.1">Analyzing Images with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.780.1">Computer Vision</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">.</span></span></p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.782.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.783.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.785.1">Revisiting the </span><em class="italic"><span class="koboSpan" id="kobo.786.1">MS COCO</span></em><span class="koboSpan" id="kobo.787.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.788.1">Penn-Fudan </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.789.1">Pedestrian</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.790.1"> datasets</span></span></li>
<li><span class="koboSpan" id="kobo.791.1">Training a </span><strong class="bold"><span class="koboSpan" id="kobo.792.1">DeepLab-v3</span></strong><span class="koboSpan" id="kobo.793.1"> model</span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.794.1"> from scratch with </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.795.1">Penn-Fudan Pedestrian</span></em></span></li>
<li><span class="koboSpan" id="kobo.796.1">Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.797.1">MS COCO</span></em><span class="koboSpan" id="kobo.798.1"> to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.799.1">Penn-Fudan Pedestrian</span></em></span></li>
<li><span class="koboSpan" id="kobo.800.1">Fine-tuning </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.801.1">our pre-trained DeepLab-v3 model on </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.802.1">Penn-Fudan Pedestrian</span></em></span></li>
</ol>
<p><span class="koboSpan" id="kobo.803.1">Let’s look at these steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">detail next.</span></span></p>
<h3><span class="koboSpan" id="kobo.805.1">Revisiting the MS COCO and Penn-Fudan Pedestrian datasets</span></h3>
<p><em class="italic"><span class="koboSpan" id="kobo.806.1">MS COCO</span></em><span class="koboSpan" id="kobo.807.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.808.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.809.1"> are</span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.810.1"> both object</span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.811.1"> detection</span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.812.1"> and semantic segmentation</span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.813.1"> datasets; however, they are quite different. </span><em class="italic"><span class="koboSpan" id="kobo.814.1">MS COCO</span></em><span class="koboSpan" id="kobo.815.1"> is a large-scale dataset containing ~150k images labeled into 80 classes (21 main ones) and has been used extensively in research and academia for benchmarking. </span><em class="italic"><span class="koboSpan" id="kobo.816.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.817.1"> is a small-scale dataset containing 170 images of 423 pedestrians. </span><span class="koboSpan" id="kobo.817.2">For this recipe, we will focus on the semantic </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">segmentation task.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">MXNet GluonCV does not provide methods to directly download any of the datasets. </span><span class="koboSpan" id="kobo.819.2">However, we do not need the </span><em class="italic"><span class="koboSpan" id="kobo.820.1">MS COCO</span></em><span class="koboSpan" id="kobo.821.1"> dataset (its size is ~19 GB), only the pre-trained parameters for our </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">chosen model.</span></span></p>
<p><span class="koboSpan" id="kobo.823.1">Here are some examples from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.824.1">MS COCO</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<span class="koboSpan" id="kobo.826.1"><img alt="Figure 7.22 – MS COCO example" src="image/B16591_07_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.827.1">Figure 7.22 – MS COCO example</span></p>
<p><span class="koboSpan" id="kobo.828.1">For </span><em class="italic"><span class="koboSpan" id="kobo.829.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.830.1">, all the information on how to retrieve the dataset can be found in the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.831.1">Segmenting objects in images with MXNet: PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.832.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.833.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.834.1">, </span><em class="italic"><span class="koboSpan" id="kobo.835.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.836.1">. </span><span class="koboSpan" id="kobo.836.2">Taking that recipe’s code as a reference, we can display </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">some examples:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<span class="koboSpan" id="kobo.838.1"><img alt="Figure 7.23 – Penn-Fudan Pedestrian dataset examples" src="image/B16591_07_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.839.1">Figure 7.23 – Penn-Fudan Pedestrian dataset examples</span></p>
<p><span class="koboSpan" id="kobo.840.1">From </span><em class="italic"><span class="koboSpan" id="kobo.841.1">Figures 7.22</span></em><span class="koboSpan" id="kobo.842.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.843.1">7.23</span></em><span class="koboSpan" id="kobo.844.1">, we </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.845.1">can</span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.846.1"> see how some images from </span><em class="italic"><span class="koboSpan" id="kobo.847.1">MS COCO</span></em><span class="koboSpan" id="kobo.848.1"> resemble </span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.849.1">some</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.850.1"> of the images from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.851.1">Penn-Fudan Pedestrian</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.853.1">Training a DeepLab-v3 model from scratch with Penn-Fudan Pedestrian</span></h3>
<p><span class="koboSpan" id="kobo.854.1">As </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.855.1">described</span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.856.1"> in</span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.857.1"> the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.858.1">Segmenting objects in images with MXNet: PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.859.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.860.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.861.1">, </span><em class="italic"><span class="koboSpan" id="kobo.862.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.863.1">, we will be using softmax cross-entropy as the loss function and pixel accuracy and </span><strong class="bold"><span class="koboSpan" id="kobo.864.1">Mean Intersection over Union</span></strong><span class="koboSpan" id="kobo.865.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.866.1">mIoU</span></strong><span class="koboSpan" id="kobo.867.1">) as </span><a id="_idIndexMarker875"/><span class="No-Break"><span class="koboSpan" id="kobo.868.1">evaluation metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.869.1">By following the code in our recipe, we have the following evolution while training from scratch our </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.870.1">DeepLab-v3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.871.1"> model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<span class="koboSpan" id="kobo.872.1"><img alt="Figure 7.24 – DeepLab-v3 training evolution (training loss and validation loss) – training from scratch" src="image/B16591_07_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.873.1">Figure 7.24 – DeepLab-v3 training evolution (training loss and validation loss) – training from scratch</span></p>
<p><span class="koboSpan" id="kobo.874.1">Furthermore, for </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.875.1">the </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.876.1">best </span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.877.1">iteration, the pixel accuracy and mIoU values obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.879.1">
PixAcc:  0.8454046875
mIoU  :  0.6548404063890942</span></pre> <p><span class="koboSpan" id="kobo.880.1">Even after training for 40 epochs, the evaluation values obtained do not show strong performance  (an mIoU value of </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">only 0.65).</span></span></p>
<p><span class="koboSpan" id="kobo.882.1">Qualitatively, we can also check how well our model is performing with an example image. </span><span class="koboSpan" id="kobo.882.2">In our case, we chose </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<span class="koboSpan" id="kobo.884.1"><img alt="Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results" src="image/B16591_07_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.885.1">Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results</span></p>
<p><span class="koboSpan" id="kobo.886.1">We can check </span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.887.1">the</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.888.1"> output</span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.889.1"> of our model by running this image through it with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">code snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.891.1">
# Compute and plot prediction
transformed_image = gcv.data.transforms.presets.segmentation.test_transform(test_image, ctx)
 output = deeplab_ts(transformed_image)
 filtered_output = mx.nd.argmax(output[0], 1)
 masked_output = gcv.utils.viz.plot_mask(test_image, filtered_output)
 axes = fig.add_subplot(1, 2, 2)
 axes.set_title("Prediction", fontsize=16, y=-0.3)
 axes.axis('off')
 axes.imshow(masked_output);</span></pre> <p><span class="koboSpan" id="kobo.892.1">The preceding code snippet shows the ground truth segmentations and the prediction from </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">our model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<span class="koboSpan" id="kobo.894.1"><img alt="Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch" src="image/B16591_07_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.895.1">Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch</span></p>
<p><span class="koboSpan" id="kobo.896.1">As can be seen from the results, the pedestrians have only started to be correctly segmented. </span><span class="koboSpan" id="kobo.896.2">To improve the results, we will need to train for more epochs and/or adjust the</span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.897.1"> hyperparameters. </span><span class="koboSpan" id="kobo.897.2">However, a </span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.898.1">better, faster, and </span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.899.1">simpler approach would be to use transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">and fine-tuning.</span></span></p>
<h3><span class="koboSpan" id="kobo.901.1">Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning from MS COCO to Penn-Fudan Pedestrian</span></h3>
<p><span class="koboSpan" id="kobo.902.1">In the previous</span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.903.1"> recipe, we</span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.904.1"> trained a new model from scratch using our dataset. </span><span class="koboSpan" id="kobo.904.2">However, this has three </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">important drawbacks:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.906.1">A large amount of data is required for training </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">from scratch.</span></span></li>
<li><span class="koboSpan" id="kobo.908.1">The training process can take a very long time due to the large size of the dataset and the number of epochs needed for the model to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.909.1">the task.</span></span></li>
<li><span class="koboSpan" id="kobo.910.1">The compute resources required might be expensive or difficult </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">to procure.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.912.1">Therefore, in this recipe, we will follow a different approach. </span><span class="koboSpan" id="kobo.912.2">We will use pre-trained models from the MXNet GluonCV Model Zoo to solve the task. </span><span class="koboSpan" id="kobo.912.3">These models have been trained in </span><em class="italic"><span class="koboSpan" id="kobo.913.1">MS COCO</span></em><span class="koboSpan" id="kobo.914.1">, a dataset that contains the classes we are interested in (</span><strong class="source-inline"><span class="koboSpan" id="kobo.915.1">person</span></strong><span class="koboSpan" id="kobo.916.1"> in this case); therefore, we can use those learned representations and easily transfer them to </span><em class="italic"><span class="koboSpan" id="kobo.917.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.918.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">same domain).</span></span></p>
<p><span class="koboSpan" id="kobo.920.1">For a DeepLab-v3 model, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.922.1">
# DeepLab-v3 from Model Zoo
deeplab_pt
gcv.model_zoo.get_model('deeplab_resnet101_coco'
pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.923.1">As we can see in </span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.924.1">the </span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.925.1">preceding code snippet, following the discussion in this chapter’s first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.926.1">Understanding Transfer-Learning and Fine-Tuning</span></em><span class="koboSpan" id="kobo.927.1">, for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.928.1">pretrained</span></strong><span class="koboSpan" id="kobo.929.1"> parameter, we have assigned the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.930.1">True</span></strong><span class="koboSpan" id="kobo.931.1">, indicating that we want the pretrained weights to be retrieved (and not only the architecture of </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">the model).</span></span></p>
<p><span class="koboSpan" id="kobo.933.1">In order to evaluate adequately the improvements that transfer learning brings, we are going to directly evaluate our pre-trained model in our target task (the task source is </span><em class="italic"><span class="koboSpan" id="kobo.934.1">MS COCO</span></em><span class="koboSpan" id="kobo.935.1">) before applying transfer learning to </span><em class="italic"><span class="koboSpan" id="kobo.936.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.937.1"> and after applying it. </span><span class="koboSpan" id="kobo.937.2">Therefore, using our pre-trained model as is, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.939.1">
PixAcc:  0.9640322916666667
mIoU  :  0.476540873665686</span></pre> <p><span class="koboSpan" id="kobo.940.1">As we can see, our pre-trained Transformer model is already showing good performance values as it is in the same domain. </span><span class="koboSpan" id="kobo.940.2">Moreover, the great advantage of using pre-trained models is the time savings, as loading a pre-trained model just takes a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">of code.</span></span></p>
<p><span class="koboSpan" id="kobo.942.1">We can also check how well our model is performing qualitatively with the same image example and code. </span><span class="koboSpan" id="kobo.942.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<span class="koboSpan" id="kobo.944.1"><img alt="Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model" src="image/B16591_07_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.945.1">Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model</span></p>
<p><span class="koboSpan" id="kobo.946.1">As can be seen from the results, the pedestrians have been correctly segmented. </span><span class="koboSpan" id="kobo.946.2">Please note a side advantage of using pre-trained models in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.947.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.948.1">.27</span></em><span class="koboSpan" id="kobo.949.1">: in the ground truth image, the people in the background were not segmented, but the pre-trained model correctly picked them up (which might explain the low </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">mIoU values).</span></span></p>
<p><span class="koboSpan" id="kobo.951.1">Now that we have a baseline for comparison, let’s apply transfer learning to our task. </span><span class="koboSpan" id="kobo.951.2">In the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.952.1">Understanding transfer learning and fine-tuning</span></em><span class="koboSpan" id="kobo.953.1">, the first step was to retrieve a </span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.954.1">pre-trained</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.955.1"> model from the MXNet Model Zoo (GluonCV or GluonNLP), which we have </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">already done.</span></span></p>
<p><span class="koboSpan" id="kobo.957.1">The second step is to remove the last layers (typically, a classifier), keeping the parameters in the rest of the layers frozen (not updatable during training), so let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">do it!</span></span></p>
<p><span class="koboSpan" id="kobo.959.1">We can freeze the </span><em class="italic"><span class="koboSpan" id="kobo.960.1">DeepLab-v3</span></em><span class="koboSpan" id="kobo.961.1"> feature extraction layers with the </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.963.1">
for param in deeplab_tl.collect_params().values():
param.grad_req = 'null'</span></pre> <p><span class="koboSpan" id="kobo.964.1">Furthermore, we will also need to replace the segmentation task head. </span><span class="koboSpan" id="kobo.964.2">Previously, it supported 21 classes from </span><em class="italic"><span class="koboSpan" id="kobo.965.1">MS COCO</span></em><span class="koboSpan" id="kobo.966.1">. </span><span class="koboSpan" id="kobo.966.2">For our experiments, two classes are enough, </span><strong class="source-inline"><span class="koboSpan" id="kobo.967.1">background</span></strong><span class="koboSpan" id="kobo.968.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.969.1">person</span></strong><span class="koboSpan" id="kobo.970.1">. </span><span class="koboSpan" id="kobo.970.2">This is done with the </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.972.1">
# Replace the last layers
deeplab_tl.head = gcv.model_zoo.deeplabv3._DeepLabHead(2)
deeplab_tl.head.initialize(ctx=ctx)
deeplab_tl.head.collect_params().setattr('lr_mult', 10)</span></pre> <p><span class="koboSpan" id="kobo.973.1">Now, we can apply the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.974.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.975.1">, and we have the </span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.976.1">following evolution</span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.977.1"> in the training using the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.978.1">DeepLab-v3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.979.1"> model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<span class="koboSpan" id="kobo.980.1"><img alt="Figure 7.28 – DeepLab-v3 training evolution (training loss and validation loss) – transfer learning" src="image/B16591_07_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.981.1">Figure 7.28 – DeepLab-v3 training evolution (training loss and validation loss) – transfer learning</span></p>
<p><span class="koboSpan" id="kobo.982.1">Furthermore, for the best iteration, the evaluation metrics obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.984.1">
PixAcc:  0.9503427083333333
mIoU  :  0.8799470898171042</span></pre> <p><span class="koboSpan" id="kobo.985.1">Compared with our previous experiments of training from scratch and pre-training, this experiment yields slightly better performance, and it took us literally minutes to get this model to start working for us in our intended task, whereas the training required for the training from scratch experiment took hours and required several tries to tune the hyperparameters, which turned into several days of effort </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">in total.</span></span></p>
<p><span class="koboSpan" id="kobo.987.1">We can also check how well our model is performing qualitatively with the same image example and code. </span><span class="koboSpan" id="kobo.987.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<span class="koboSpan" id="kobo.989.1"><img alt="Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained model with transfer learning" src="image/B16591_07_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.990.1">Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained model with transfer learning</span></p>
<p><span class="koboSpan" id="kobo.991.1">As </span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.992.1">can </span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.993.1">be seen from the results, the pedestrians have been </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">correctly segmented.</span></span></p>
<h3><span class="koboSpan" id="kobo.995.1">Fine-tuning our pre-trained DeepLab-v3 model on Penn-Fudan Pedestrian</span></h3>
<p><span class="koboSpan" id="kobo.996.1">In the</span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.997.1"> previous</span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.998.1"> recipe, we </span><em class="italic"><span class="koboSpan" id="kobo.999.1">froze</span></em><span class="koboSpan" id="kobo.1000.1"> the</span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.1001.1"> parameters in the encoder layers. </span><span class="koboSpan" id="kobo.1001.2">However, with the dataset we are currently working with (</span><em class="italic"><span class="koboSpan" id="kobo.1002.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1003.1">), we can </span><em class="italic"><span class="koboSpan" id="kobo.1004.1">unfreeze</span></em><span class="koboSpan" id="kobo.1005.1"> those parameters and train the model, effectively allowing the new training process to update the representations (with transfer learning, we were working directly with the representations learned for </span><em class="italic"><span class="koboSpan" id="kobo.1006.1">MS COCO</span></em><span class="koboSpan" id="kobo.1007.1">). </span><span class="koboSpan" id="kobo.1007.2">As introduced in this chapter, this process is </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">called fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.1009.1">There are two variants </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">of fine-tuning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1011.1">Apply transfer learning by freezing the layers and unfreezing </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">them afterward.</span></span></li>
<li><span class="koboSpan" id="kobo.1013.1">Directly apply fine-tuning without the preliminary step of freezing </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">the layers.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1015.1">Let’s compute both experiments and draw conclusions by comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.1017.1">For the first experiment, we can take the network obtained in the previous recipe, unfreeze the layers, and restart the training. </span><span class="koboSpan" id="kobo.1017.2">In MXNet, to unfreeze the encoder parameters, we can run the </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">following snippet:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1019.1">
for param in deeplab_ft.collect_params().values():
param.grad_req = 'write'</span></pre> <p><span class="koboSpan" id="kobo.1020.1">Now, we can apply</span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.1021.1"> the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.1022.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1023.1">, and we have the following evolution in the training using the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1024.1">DeepLab-v3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1025.1"> model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<span class="koboSpan" id="kobo.1026.1"><img alt="Figure 7.30 – DeepLab-v3 training evolution (training loss and validation loss) – fine-tuning after transfer learning" src="image/B16591_07_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1027.1">Figure 7.30 – DeepLab-v3 training evolution (training loss and validation loss) – fine-tuning after transfer learning</span></p>
<p><span class="koboSpan" id="kobo.1028.1">Furthermore, for the best iteration, the evaluation metrics obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1030.1">
PixAcc:  0.9637550347222222
mIoU  :  0.9091450223893902</span></pre> <p><span class="koboSpan" id="kobo.1031.1">Compared with our previous experiment in transfer learning, this experiment yields ~3% better performance in mIoU, a very good increase taking into account the low training </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">time invested.</span></span></p>
<p><span class="koboSpan" id="kobo.1033.1">We can</span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.1034.1"> also</span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.1035.1"> check</span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.1036.1"> how well our model is performing qualitatively with the same image example and code. </span><span class="koboSpan" id="kobo.1036.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<span class="koboSpan" id="kobo.1038.1"><img alt="Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained model with fine-tuning after transfer learning" src="image/B16591_07_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1039.1">Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained model with fine-tuning after transfer learning</span></p>
<p><span class="koboSpan" id="kobo.1040.1">As can be seen from</span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.1041.1"> the</span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.1042.1"> results, the pedestrians have been </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">correctly segmented.</span></span></p>
<p><span class="koboSpan" id="kobo.1044.1">Let’s continue now with the second fine-tuning experiment, in which we do not apply transfer learning (no frozen layers) and, instead, apply fine-tuning directly to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">whole model.</span></span></p>
<p><span class="koboSpan" id="kobo.1046.1">We need to retrieve the pre-trained </span><em class="italic"><span class="koboSpan" id="kobo.1047.1">DeepLab-v3</span></em><span class="koboSpan" id="kobo.1048.1"> model for </span><em class="italic"><span class="koboSpan" id="kobo.1049.1">MS COCO</span></em><span class="koboSpan" id="kobo.1050.1">, with the following code snippet for </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">MXNet GluonCV:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1052.1">
# DeepLab-v3 from Model Zoo
 deeplab_ft_direct = gcv.model_zoo.get_model("deeplab_resnet101_coco", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1053.1">And now, without</span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.1054.1"> freezing, we can apply the training process, which will update all layers of our </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1055.1">DeepLab-v3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1"> model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<span class="koboSpan" id="kobo.1057.1"><img alt="Figure 7.32 – DeepLab-v3 training evolution (training loss and validation loss) – fine-tuning without freezing" src="image/B16591_07_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1058.1">Figure 7.32 – DeepLab-v3 training evolution (training loss and validation loss) – fine-tuning without freezing</span></p>
<p><span class="koboSpan" id="kobo.1059.1">Furthermore, for </span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.1060.1">the</span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.1061.1"> best iteration, the evaluation metrics obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1063.1">
PixAcc:  0.9639182291666667
mIoU  :  0.9095065032946663</span></pre> <p><span class="koboSpan" id="kobo.1064.1">Compared with our previous fine-tuning experiment, we can see how these experiments yield very similar performance. </span><span class="koboSpan" id="kobo.1064.2">Empirically, it has been proven that this fine-tuning experiment can also yield slightly lower results because initially freezing the encoder allows for the decoder to learn (using the encoder representations) the new task at hand. </span><span class="koboSpan" id="kobo.1064.3">From a point of view, in this step, there is a knowledge transfer from the encoder to the decoder. </span><span class="koboSpan" id="kobo.1064.4">In a secondary step, when the encoder is unfrozen, the learned parameters from the decoder perform auxiliary transfer learning, this time from the</span><em class="italic"> </em><span class="koboSpan" id="kobo.1065.1">decoder to </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">the encoder.</span></span></p>
<p><span class="koboSpan" id="kobo.1067.1">We can also check how well our model is performing qualitatively with the same image example </span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.1068.1">and</span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.1069.1"> code. </span><span class="koboSpan" id="kobo.1069.2">The </span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.1070.1">output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<span class="koboSpan" id="kobo.1072.1"><img alt="Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained model with fine-tuning without freezing" src="image/B16591_07_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1073.1">Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained model with fine-tuning without freezing</span></p>
<p><span class="koboSpan" id="kobo.1074.1">As can be seen from the results, the pedestrians have been correctly segmented, although, as mentioned, if we look at the person on the right, the arm closer to the person on the</span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.1075.1"> left could be segmented better. </span><span class="koboSpan" id="kobo.1075.2">As discussed, sometimes this version of fine-tuning yields slightly lower results than </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">other approaches.</span></span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.1077.1">How it works…</span></h2>
<p><span class="koboSpan" id="kobo.1078.1">In this recipe, we applied the</span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.1079.1"> techniques of transfer learning and fine-tuning, introduced at the beginning of the chapter, to the task of image classification, which was also presented previously, in the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1080.1">Segmenting objects in images with MXNet: PSPNet and DeepLab-v3</span></em><span class="koboSpan" id="kobo.1081.1">, in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1082.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.1083.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1084.1">Analyzing Images with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1085.1">Computer Vision</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1087.1">We revisited two known datasets, </span><em class="italic"><span class="koboSpan" id="kobo.1088.1">MS COCO</span></em><span class="koboSpan" id="kobo.1089.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1090.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1091.1">, which we intended to combine using knowledge transfer based on the former dataset and refining that knowledge with the latter. </span><span class="koboSpan" id="kobo.1091.2">Moreover, MXNet GluonCV provided </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1093.1">A pre-trained </span><em class="italic"><span class="koboSpan" id="kobo.1094.1">DeepLab-v3</span></em><span class="koboSpan" id="kobo.1095.1"> model for </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1096.1">MS COCO</span></em></span></li>
<li><span class="koboSpan" id="kobo.1097.1">Tools for easy-to-use access to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1098.1">Penn-Fudan Pedestrian</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1099.1">Furthermore, we continued using the loss functions and metrics introduced for semantic segmentation, softmax cross-entropy, pixel accuracy, </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">and mIoU.</span></span></p>
<p><span class="koboSpan" id="kobo.1101.1">Having all these tools readily available within MXNet and GluonCV allowed us to run the following experiments with just a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">of code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1103.1">Training a model from scratch with </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1104.1">Penn-Fudan Pedestrian</span></em></span></li>
<li><span class="koboSpan" id="kobo.1105.1">Using a pre-trained model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.1106.1">MS COCO</span></em><span class="koboSpan" id="kobo.1107.1"> to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1108.1">Penn-Fudan Pedestrian</span></em></span></li>
<li><span class="koboSpan" id="kobo.1109.1">Fine-tuning our pre-trained model on </span><em class="italic"><span class="koboSpan" id="kobo.1110.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1111.1"> (with and without </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">freezing layers)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1113.1">After running the different experiments and taking into account the qualitative results and the quantitative results, transfer learning (with a pixel accuracy of 0.95 and mIoU of 0.88) has been the best experiment for our task. </span><span class="koboSpan" id="kobo.1113.2">The actual results obtained when running these experiments might differ based on model architecture, datasets, and hyperparameters </span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.1114.1">chosen, so you are encouraged to try out different techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">and variations.</span></span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.1116.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1117.1">Transfer learning, including fine-tuning, is an active field of research. </span><span class="koboSpan" id="kobo.1117.2">A recent paper published in 2022 explores the latest advances in image classification. </span><span class="koboSpan" id="kobo.1117.3">The paper is titled </span><em class="italic"><span class="koboSpan" id="kobo.1118.1">Deep Transfer Learning for Image Classification: A survey</span></em><span class="koboSpan" id="kobo.1119.1">, and can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">here: </span></span><a href="https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey"><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1122.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1123.1">An interesting paper that combines transfer learning and semantic segmentation is </span><em class="italic"><span class="koboSpan" id="kobo.1124.1">Semantic Segmentation with Transfer Learning for Off-Road Autonomous Driving</span></em><span class="koboSpan" id="kobo.1125.1">, in which a change of domain is also studied by the usage of synthetic data. </span><span class="koboSpan" id="kobo.1125.2">It can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">here: </span></span><a href="https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving"><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1129.1">A more general overview is given in this paper: </span><em class="italic"><span class="koboSpan" id="kobo.1130.1">Learning Transferable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network</span></em><span class="koboSpan" id="kobo.1131.1">, accepted for </span><strong class="bold"><span class="koboSpan" id="kobo.1132.1">Computer Vision and Pattern Recognition</span></strong><span class="koboSpan" id="kobo.1133.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1134.1">CVPR</span></strong><span class="koboSpan" id="kobo.1135.1">) symposium in 2016. </span><span class="koboSpan" id="kobo.1135.2">It can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">here: </span></span><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">.</span></span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.1139.1">Improving performance for translating English to German</span></h1>
<p><span class="koboSpan" id="kobo.1140.1">In the previous </span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.1141.1">recipes, we have seen how we can leverage pre-trained models and new datasets for transfer learning and fine-tuning applied to CV tasks. </span><span class="koboSpan" id="kobo.1141.2">In this recipe, we will follow a similar approach, but with an NLP task, translating from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.1143.1">In the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1144.1">Translating text from Vietnamese to English</span></em><span class="koboSpan" id="kobo.1145.1">, in </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1146.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1147.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1148.1">Understanding Text with Natural Language Processing</span></em><span class="koboSpan" id="kobo.1149.1">, we saw how we could use GluonNLP to retrieve pre-trained models and use them directly for a translation task, training them from scratch, effectively only leveraging past knowledge by using the architecture of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">pre-trained model.</span></span></p>
<p><span class="koboSpan" id="kobo.1151.1">In this recipe, we will also leverage the weights/parameters of the model, obtained for a task consisting of translating text from English to German</span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.1152.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.1153.1">machine translation</span></strong><span class="koboSpan" id="kobo.1154.1"> models. </span><span class="koboSpan" id="kobo.1154.2">The dataset that we will use for pre-training will be </span><em class="italic"><span class="koboSpan" id="kobo.1155.1">WMT2014</span></em><span class="koboSpan" id="kobo.1156.1"> (task source), and we will run several experiments to evaluate our models in a new (target) task, using the dataset </span><em class="italic"><span class="koboSpan" id="kobo.1157.1">WMT2016</span></em><span class="koboSpan" id="kobo.1158.1"> dataset (with a ~20% increased vocabulary of words and sentences for </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">German-English pairs).</span></span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.1160.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.1161.1">As for previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">at all.</span></span></p>
<p><span class="koboSpan" id="kobo.1163.1">Furthermore, we will be working with text datasets; therefore, we will revisit some concepts already seen in the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1164.1">Understanding text datasets – load, manage, and visualize Enron Emails dataset</span></em><span class="koboSpan" id="kobo.1165.1"> from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1166.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1167.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1168.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1169.1">and DataLoader</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">.</span></span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.1171.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.1172.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1174.1">Introducing the </span><em class="italic"><span class="koboSpan" id="kobo.1175.1">WMT2014</span></em><span class="koboSpan" id="kobo.1176.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1177.1">WMT2016</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1"> datasets</span></span></li>
<li><span class="koboSpan" id="kobo.1179.1">Training a Transformer model from scratch </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1181.1">WMT2016</span></em></span></li>
<li><span class="koboSpan" id="kobo.1182.1">Using a pre-trained Transformer model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.1183.1">WMT2014</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1184.1">to </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1185.1">WMT2016</span></em></span></li>
<li><span class="koboSpan" id="kobo.1186.1">Fine-tuning</span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.1187.1"> our pre-trained Transformer model </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">on </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1189.1">WMT2016</span></em></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1190.1">Let’s look at these steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">detail next.</span></span></p>
<h3><span class="koboSpan" id="kobo.1192.1">Introducing the WMT2014 and WMT2016 datasets</span></h3>
<p><em class="italic"><span class="koboSpan" id="kobo.1193.1">WMT2014</span></em><span class="koboSpan" id="kobo.1194.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1195.1">WMT2016</span></em><span class="koboSpan" id="kobo.1196.1"> are </span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.1197.1">multi-modal (multi-language) translation</span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.1198.1"> datasets, including</span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.1199.1"> Chinese, English, and German corpus. </span><em class="italic"><span class="koboSpan" id="kobo.1200.1">WMT2014</span></em><span class="koboSpan" id="kobo.1201.1"> was </span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.1202.1">first introduced in 2014 in </span><em class="italic"><span class="koboSpan" id="kobo.1203.1">Proceedings of the Ninth Workshop on Statistical Machine Translation</span></em><span class="koboSpan" id="kobo.1204.1">, as part of the evaluation campaign of the translation models. </span><span class="koboSpan" id="kobo.1204.2">This workshop was upgraded to its own conference in 2016, and </span><em class="italic"><span class="koboSpan" id="kobo.1205.1">WMT2016</span></em><span class="koboSpan" id="kobo.1206.1"> was introduced as part of the evaluation campaign of translation models in </span><em class="italic"><span class="koboSpan" id="kobo.1207.1">Proceedings of the First Conference on Machine Translation</span></em><span class="koboSpan" id="kobo.1208.1">. </span><span class="koboSpan" id="kobo.1208.2">Both datasets are very similar, retrieving information from news sources, and the largest difference is the corpus (the size of the vocabulary used for both). </span><span class="koboSpan" id="kobo.1208.3">WMT2014 is about ~140k distinct words, whereas WMT2016 is slightly larger with ~150k words, and specifically for German-English pairs, an increase of ~20% of words </span><span class="No-Break"><span class="koboSpan" id="kobo.1209.1">and sentences.</span></span></p>
<p><span class="koboSpan" id="kobo.1210.1">MXNet GluonNLP provides ready-to-use versions of these datasets. </span><span class="koboSpan" id="kobo.1210.2">For our case, we will work with </span><em class="italic"><span class="koboSpan" id="kobo.1211.1">WMT2016</span></em><span class="koboSpan" id="kobo.1212.1">, which only contains </span><em class="italic"><span class="koboSpan" id="kobo.1213.1">train</span></em><span class="koboSpan" id="kobo.1214.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1215.1">test</span></em><span class="koboSpan" id="kobo.1216.1"> splits. </span><span class="koboSpan" id="kobo.1216.2">We will further split the test set to obtain </span><em class="italic"><span class="koboSpan" id="kobo.1217.1">validation</span></em><span class="koboSpan" id="kobo.1218.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1219.1">test</span></em><span class="koboSpan" id="kobo.1220.1"> splits. </span><span class="koboSpan" id="kobo.1220.2">Here is the code to load </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1222.1">
# WMT2016 Dataset (Train, Validation and Test)
# Dataset Parameters
src_lang, tgt_lang = "en", "de"
src_max_len, tgt_max_len = 50, 50
 wmt2016_train_data = nlp.data.WMT2016BPE(
    'train',
 src_lang=src_lang,
    tgt_lang=tgt_lang)
wmt2016_val_data = nlp.data.WMT2016BPE(
    'newstest2016',
    src_lang=src_lang,
    tgt_lang=tgt_lang)
wmt2016_test_data = nlp.data.WMT2016BPE(
    'newstest2016',
    src_lang=src_lang,
    tgt_lang=tgt_lang)</span></pre> <p><span class="koboSpan" id="kobo.1223.1">Here is the code to generate </span><em class="italic"><span class="koboSpan" id="kobo.1224.1">validation</span></em><span class="koboSpan" id="kobo.1225.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1226.1">test</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1"> splits:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1228.1">
 # Split Val / Test sets
val_length = 1500
test_length = len(wmt2016_test_text) - val_length
wmt2016_val_data._data[0] = wmt2016_val_data._data[0][:val_length]
 wmt2016_val_data._data[1] = wmt2016_val_data._data[1][:val_length]
 wmt2016_val_data._length = val_length
wmt2016_val_text._data[0] = wmt2016_val_text._data[0][:val_length]
 wmt2016_val_text._data[1] = wmt2016_val_text._data[1][:val_length]
 wmt2016_val_text._length = val_length
wmt2016_test_data._data[0] = wmt2016_test_data._data[0][-test_length:]
 wmt2016_test_data._data[1] = wmt2016_test_data._data[1][-test_length:]
 wmt2016_test_data._length = test_length</span></pre> <p><span class="koboSpan" id="kobo.1229.1">After splitting, our </span><em class="italic"><span class="koboSpan" id="kobo.1230.1">WMT2016</span></em><span class="koboSpan" id="kobo.1231.1"> datasets provide the </span><span class="No-Break"><span class="koboSpan" id="kobo.1232.1">following data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1233.1">
 Length of train set: 4500966
Length of val set  : 1500
Length of test set : 1499</span></pre> <p><span class="koboSpan" id="kobo.1234.1">From the </span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.1235.1">large </span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.1236.1">number </span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.1237.1">of </span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.1238.1">instances on each of the datasets, we can confirm that these are suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.1239.1">our experiments.</span></span></p>
<h3><span class="koboSpan" id="kobo.1240.1">Training a Transformer model from scratch in WMT2016</span></h3>
<p><span class="koboSpan" id="kobo.1241.1">As described</span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.1242.1"> in the</span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.1243.1"> fourth </span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.1244.1">recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1245.1">Translating text from Vietnamese to English</span></em><span class="koboSpan" id="kobo.1246.1">, in </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1247.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1248.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1249.1">Understanding Text with Natural Language Processing</span></em><span class="koboSpan" id="kobo.1250.1">, we will be using </span><strong class="bold"><span class="koboSpan" id="kobo.1251.1">Perplexity</span></strong><span class="koboSpan" id="kobo.1252.1"> for our </span><em class="italic"><span class="koboSpan" id="kobo.1253.1">per-batch computations</span></em><span class="koboSpan" id="kobo.1254.1"> in training, and </span><strong class="bold"><span class="koboSpan" id="kobo.1255.1">BLEU</span></strong><span class="koboSpan" id="kobo.1256.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.1257.1">per-epoch computations</span></em><span class="koboSpan" id="kobo.1258.1">, which will show us the evolution of our training process, as part of the typically used training and validation losses. </span><span class="koboSpan" id="kobo.1258.2">We will also use them for quantitative evaluation, and for qualitative evaluation, we will choose a sentence (feel free to use any other sentence you can come </span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">up with).</span></span></p>
<p><span class="koboSpan" id="kobo.1260.1">We have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1261.1">Transformer model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<span class="koboSpan" id="kobo.1262.1"><img alt="Figure 7.34 – Transformer training evolution (training loss) – training from scratch" src="image/B16591_07_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1263.1">Figure 7.34 – Transformer training evolution (training loss) – training from scratch</span></p>
<p><span class="koboSpan" id="kobo.1264.1">Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1266.1">
 WMT16 test loss: 3.01; test bleu score: 14.50</span></pre> <p><span class="koboSpan" id="kobo.1267.1">Current </span><strong class="bold"><span class="koboSpan" id="kobo.1268.1">State-of-the-Art</span></strong><span class="koboSpan" id="kobo.1269.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1270.1">SOTA</span></strong><span class="koboSpan" id="kobo.1271.1">) models </span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.1272.1">can yield above 30 points in the BLEU score; we reach about halfway with 10 epochs, reaching SOTA performance in ~</span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">30 epochs.</span></span></p>
<p><span class="koboSpan" id="kobo.1274.1">Qualitatively, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.1274.2">In our case, we chose: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1275.1">"I learn new things every day"</span></strong><span class="koboSpan" id="kobo.1276.1">, and this can be verified</span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.1277.1"> with</span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.1278.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">following </span></span><span class="No-Break"><a id="_idIndexMarker930"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1281.1">
print("Qualitative Evaluation:  Translating from English to German")
# From Google Translate
expected_tgt_seq = " Ich lerne jeden Tag neue Dinge."
 </span><span class="koboSpan" id="kobo.1281.2">print("Expected translation:")
 print(expected_tgt_seq)
src_seq = "I learn new things every day."
 </span><span class="koboSpan" id="kobo.1281.3">print("In English:")
 print(src_seq)
translation_out = nmt.utils.translate(
     transformer_ts_translator,
    src_seq,
     wmt_src_vocab,
    wmt_tgt_vocab,
    ctx)
print("The German translation is:")
 print(" ".join(translation_out[0]))</span></pre> <p><span class="koboSpan" id="kobo.1282.1">These code statements will give us the </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1284.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne jeden Tag neue Dinge.
 </span><span class="koboSpan" id="kobo.1284.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1284.3">The German translation is:
 Ich halte es für so , dass es hier so ist.</span></pre> <p><span class="koboSpan" id="kobo.1285.1">The German sentence means </span><em class="italic"><span class="koboSpan" id="kobo.1286.1">I think that’s the case here</span></em><span class="koboSpan" id="kobo.1287.1">; therefore, as can be seen from this result, the </span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.1288.1">text </span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.1289.1">has </span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.1290.1">not been correctly translated from English to German, and we would need to invest more time in training to achieve the </span><span class="No-Break"><span class="koboSpan" id="kobo.1291.1">right results.</span></span></p>
<h3><span class="koboSpan" id="kobo.1292.1">Using a pre-trained Transformer model to optimize performance via transfer learning from WMT2014 to WMT2016</span></h3>
<p><span class="koboSpan" id="kobo.1293.1">In the </span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.1294.1">previous recipe, we trained a new model from scratch using our dataset. </span><span class="koboSpan" id="kobo.1294.2">However, this has two </span><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">important drawbacks:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1296.1">A large amount of data is required for training </span><span class="No-Break"><span class="koboSpan" id="kobo.1297.1">from scratch.</span></span></li>
<li><span class="koboSpan" id="kobo.1298.1">The training process can take a very long time due to the large size of the dataset and the number of epochs needed for the model to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">the task.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1300.1">Therefore, in this recipe, we will follow a different approach. </span><span class="koboSpan" id="kobo.1300.2">We will use pre-trained models from MXNet GluonNLP to solve the task. </span><span class="koboSpan" id="kobo.1300.3">These models have been trained on </span><em class="italic"><span class="koboSpan" id="kobo.1301.1">WMT2014</span></em><span class="koboSpan" id="kobo.1302.1"> a very similar dataset, so the representations learned for this task can be easily transferred to </span><em class="italic"><span class="koboSpan" id="kobo.1303.1">WMT2016</span></em><span class="koboSpan" id="kobo.1304.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">same domain).</span></span></p>
<p><span class="koboSpan" id="kobo.1306.1">For a Transformer model, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1308.1">
 wmt_model_name = 'transformer_en_de_512'
wmt_transformer_model_pt, wmt_src_vocab, wmt_tgt_vocab = nlp.model.get_model(
    wmt_model_name,
    dataset_name='WMT2014',
    pretrained=True,
    ctx=ctx)
print('Source Vocab:', len(wmt_src_vocab), ', Target Vocab:', len(wmt_tgt_vocab))</span></pre> <p><span class="koboSpan" id="kobo.1309.1">The output shows us the size of the vocabulary of the </span><em class="italic"><span class="koboSpan" id="kobo.1310.1">WMT2014</span></em><span class="koboSpan" id="kobo.1311.1"> dataset (the pre-trained English to German </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">translation task):</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1313.1">
Source Vocab: 36794 , Target Vocab: 36794</span></pre> <p><span class="koboSpan" id="kobo.1314.1">This is a subset of the whole corpus available for </span><em class="italic"><span class="koboSpan" id="kobo.1315.1">WMT2014</span></em><span class="koboSpan" id="kobo.1316.1">. </span><span class="koboSpan" id="kobo.1316.2">As we can also see in the preceding code snippet, following the discussion in this chapter’s first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1317.1">Understanding transfer learning and fine-tuning</span></em><span class="koboSpan" id="kobo.1318.1">, for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1319.1">pretrained</span></strong><span class="koboSpan" id="kobo.1320.1"> parameter, we have assigned the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1321.1">True</span></strong><span class="koboSpan" id="kobo.1322.1">, indicating that we want the pretrained weights to be retrieved (and not only the architecture of </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">the model).</span></span></p>
<p><span class="koboSpan" id="kobo.1324.1">In order to evaluate adequately the improvements that transfer learning brings, we are going to directly evaluate our pre-trained model (task source is </span><em class="italic"><span class="koboSpan" id="kobo.1325.1">WMT2014</span></em><span class="koboSpan" id="kobo.1326.1">) before applying transfer learning to </span><em class="italic"><span class="koboSpan" id="kobo.1327.1">WMT2016</span></em><span class="koboSpan" id="kobo.1328.1"> and after applying it. </span><span class="koboSpan" id="kobo.1328.2">Therefore, using our pre-trained model as is, we obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1330.1">
 WMT16 test loss: 1.59; test bleu score: 29.76</span></pre> <p><span class="koboSpan" id="kobo.1331.1">As we can see, our pre-trained Transformer model is already showing very good performance values </span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.1332.1">as it is the same domain; however, simply using a pre-trained model does not yield SOTA performance, which can be achieved if training from scratch. </span><span class="koboSpan" id="kobo.1332.2">The great advantage of using pre-trained models is the time and compute savings as loading a pre-trained model just takes a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.1333.1">of code.</span></span></p>
<p><span class="koboSpan" id="kobo.1334.1">We can also check how well our model is performing qualitatively with the same sentence example and code. </span><span class="koboSpan" id="kobo.1334.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1336.1">
 Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne jeden Tag neue Dinge.
 </span><span class="koboSpan" id="kobo.1336.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1336.3">The German translation is:
 Ich lerne neue Dinge, die in jedem Fall auftreten.</span></pre> <p><span class="koboSpan" id="kobo.1337.1">The German sentence means </span><em class="italic"><span class="koboSpan" id="kobo.1338.1">I learn new things that arise in every case</span></em><span class="koboSpan" id="kobo.1339.1">; therefore, as can be seen from the results, the text has not yet been correctly translated from English to German, but this time, was much closer than our </span><span class="No-Break"><span class="koboSpan" id="kobo.1340.1">previous experiment.</span></span></p>
<p><span class="koboSpan" id="kobo.1341.1">Now that we have a baseline for comparison, let’s apply transfer learning to our task. </span><span class="koboSpan" id="kobo.1341.2">In the first recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1342.1">Understanding transfer learning and fine-tuning</span></em><span class="koboSpan" id="kobo.1343.1">, the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV or GluonNLP), which we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">already done.</span></span></p>
<p><span class="koboSpan" id="kobo.1345.1">The second step is to remove the last layers (typically, a classifier), keeping the parameters in the rest of the layers frozen (not updatable during training), so let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">do it!</span></span></p>
<p><span class="koboSpan" id="kobo.1347.1">We can freeze all parameters except the classifier with the following snippet, keeping the parameters </span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.1348.1">frozen (we will unfreeze them in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">later experiment):</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1350.1">
 updated_params = []
for param
wmt_transformer_model_tl.collect_params().values():
    if param.grad_req == "write":
        param.grad_req = "null"
        updated_params += [param.name]</span></pre> <p><span class="koboSpan" id="kobo.1351.1">Now, we can apply the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.1352.1">WMT2016</span></em><span class="koboSpan" id="kobo.1353.1">, and we have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1354.1">Transformer model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<span class="koboSpan" id="kobo.1355.1"><img alt="Figure 7.35 – Transformer training evolution (training loss) – transfer learning" src="image/B16591_07_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1356.1">Figure 7.35 – Transformer training evolution (training loss) – transfer learning</span></p>
<p><span class="koboSpan" id="kobo.1357.1">Furthermore, for </span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.1358.1">the best iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1360.1">
 WMT16 test loss: 1.20; test bleu score: 27.78</span></pre> <p><span class="koboSpan" id="kobo.1361.1">Compared with our previous experiments, this experiment yields slightly lower numerical performance; however, it took us literally minutes to get this model to start working for us in our intended task, whereas training from scratch in our previous experiment took hours and required several tries to tune the hyperparameters, becoming several days of effort </span><span class="No-Break"><span class="koboSpan" id="kobo.1362.1">in total.</span></span></p>
<p><span class="koboSpan" id="kobo.1363.1">We can also check how well our model is performing qualitatively with the same sentence example and code. </span><span class="koboSpan" id="kobo.1363.2">The output is given </span><span class="No-Break"><span class="koboSpan" id="kobo.1364.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1365.1">
 Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne jeden Tag neue Dinge.
 </span><span class="koboSpan" id="kobo.1365.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1365.3">The German translation is:
 Ich erlerne jedes Mal neue Dinge</span></pre> <p><span class="koboSpan" id="kobo.1366.1"> The German sentence means </span><em class="italic"><span class="koboSpan" id="kobo.1367.1">I learn new things every time</span></em><span class="koboSpan" id="kobo.1368.1">; therefore, as can be seen from the results, the text has been almost correctly translated from English to German, improving from our</span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.1369.1"> previous experiment (pre-trained model), although the (better) quantitative results were </span><span class="No-Break"><span class="koboSpan" id="kobo.1370.1">suggesting otherwise.</span></span></p>
<h3><span class="koboSpan" id="kobo.1371.1">Fine-tuning our pre-trained Transformer model on WMT2016</span></h3>
<p><span class="koboSpan" id="kobo.1372.1">In the previous</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.1373.1"> recipe, we </span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.1374.1">froze </span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.1375.1">all the parameters except the classifier. </span><span class="koboSpan" id="kobo.1375.2">However, as the dataset we are currently working with (</span><em class="italic"><span class="koboSpan" id="kobo.1376.1">WMT2016</span></em><span class="koboSpan" id="kobo.1377.1">) has enough data samples, we can unfreeze those parameters and train the model, effectively allowing the new training process to update the representations (with transfer learning, we were working directly with the representations learned for </span><em class="italic"><span class="koboSpan" id="kobo.1378.1">WMT2014</span></em><span class="koboSpan" id="kobo.1379.1">). </span><span class="koboSpan" id="kobo.1379.2">This process, as we know, is </span><span class="No-Break"><span class="koboSpan" id="kobo.1380.1">called fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.1381.1">There are two variants </span><span class="No-Break"><span class="koboSpan" id="kobo.1382.1">of fine-tuning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1383.1">Apply transfer learning by freezing the layers and unfreezing </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">them afterward.</span></span></li>
<li><span class="koboSpan" id="kobo.1385.1">Directly apply fine-tuning without the preliminary step of freezing </span><span class="No-Break"><span class="koboSpan" id="kobo.1386.1">the layers.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1387.1">Let’s compute both experiments and draw conclusions by comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.1388.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.1389.1">For the first experiment, we can take the network obtained in the previous recipe, unfreeze the layers, and restart the training. </span><span class="koboSpan" id="kobo.1389.2">In MXNet, to unfreeze the encoder parameters, we can run the </span><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">following snippet:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1391.1">
 for param in wmt_transformer_model_ft.collect_params().values():
    if param.name in updated_params:
        param.grad_req = 'write'</span></pre> <p><span class="koboSpan" id="kobo.1392.1">Now, we can apply the usual training process with </span><em class="italic"><span class="koboSpan" id="kobo.1393.1">WMT2016</span></em><span class="koboSpan" id="kobo.1394.1">, and we have the following evolution in the training using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1395.1">Transformer model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<span class="koboSpan" id="kobo.1396.1"><img alt="Figure 7.36 – Transformer training evolution (training loss) – fine-tuning after transfer learning" src="image/B16591_07_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1397.1">Figure 7.36 – Transformer training evolution (training loss) – fine-tuning after transfer learning</span></p>
<p><span class="koboSpan" id="kobo.1398.1">Furthermore, for</span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.1399.1"> the </span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.1400.1">best</span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.1401.1"> iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1402.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1403.1">
 WMT16 test loss: 1.23; test bleu score: 26.05</span></pre> <p><span class="koboSpan" id="kobo.1404.1">Compared with our previous experiment in transfer learning, this experiment yields a slightly worse </span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">quantitative performance.</span></span></p>
<p><span class="koboSpan" id="kobo.1406.1">Qualitatively, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.1406.2">In our case, we chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.1407.1">"I learn new things every day"</span></strong><span class="koboSpan" id="kobo.1408.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.1409.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1410.1">
 Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne jeden Tag neue Dinge.
 </span><span class="koboSpan" id="kobo.1410.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1410.3">The German translation is:
 Ich lerne jedes Mal Neues.</span></pre> <p><span class="koboSpan" id="kobo.1411.1">The German sentence means </span><em class="italic"><span class="koboSpan" id="kobo.1412.1">I learn something new every time</span></em><span class="koboSpan" id="kobo.1413.1">; therefore, as can be seen from the results, the text has been almost correctly translated from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.1415.1">Let’s continue now with the second fine-tuning experiment, where we do not apply transfer learning (no frozen layers), and instead apply fine-tuning directly to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1416.1">whole model.</span></span></p>
<p><span class="koboSpan" id="kobo.1417.1">We need</span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.1418.1"> to</span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.1419.1"> retrieve</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.1420.1"> again the pre-trained Transformer model for </span><em class="italic"><span class="koboSpan" id="kobo.1421.1">WMT2014</span></em><span class="koboSpan" id="kobo.1422.1">, with the following code snippet for </span><span class="No-Break"><span class="koboSpan" id="kobo.1423.1">MXNet GluonNLP:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1424.1">
 wmt_model_name = 'transformer_en_de_512'
wmt_transformer_model_ft_direct, _, _ = nlp.model.get_model(
    wmt_model_name,
    dataset_name='WMT2014',
    pretrained=True,
    ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1425.1">And now, without freezing, we can apply the training process, which will update all layers of our </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">Transformer model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<span class="koboSpan" id="kobo.1427.1"><img alt="Figure 7.37 – Transformer training evolution (training loss) – fine-tuning without freezing" src="image/B16591_07_37.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1428.1">Figure 7.37 – Transformer training evolution (training loss) – fine-tuning without freezing</span></p>
<p><span class="koboSpan" id="kobo.1429.1">Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied by 100) obtained in the test set are </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1431.1">
 WMT16 test loss: 1.22; test bleu score: 26.75</span></pre> <p><span class="koboSpan" id="kobo.1432.1">Compared with </span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.1433.1">our </span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.1434.1">previous </span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.1435.1">fine-tuning experiment, we can see how this experiment yields a slightly higher performance. </span><span class="koboSpan" id="kobo.1435.2">Empirically, however, the opposite results were expected (for this experiment to yield a slightly lower performance). </span><span class="koboSpan" id="kobo.1435.3">This has been proven to be a repeatable result because initially freezing the encoder allows for the decoder to learn (using the encoder representations) the new task at hand. </span><span class="koboSpan" id="kobo.1435.4">From a point of view, in this step, there is a knowledge transfer from the encoder to the decoder. </span><span class="koboSpan" id="kobo.1435.5">In a secondary step, when the encoder is unfrozen, the learned parameters from the decoder perform auxiliary transfer learning – this time, from the decoder to </span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">the encoder.</span></span></p>
<p><span class="koboSpan" id="kobo.1437.1">Qualitatively, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.1437.2">In our case, we chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.1438.1">"I learn new things every day"</span></strong><span class="koboSpan" id="kobo.1439.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.1440.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1441.1">
 Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne jeden Tag neue Dinge.
 </span><span class="koboSpan" id="kobo.1441.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1441.3">The German translation is:
 Ich lerne jedes Mal neue Dinge</span></pre> <p><span class="koboSpan" id="kobo.1442.1">The German sentence</span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.1443.1"> means </span><em class="italic"><span class="koboSpan" id="kobo.1444.1">I learn new things every time</span></em><span class="koboSpan" id="kobo.1445.1">; therefore, as can be seen from </span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.1446.1">the</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.1447.1"> results, the text has been almost correctly translated from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">to German.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.1449.1">How it works…</span></h2>
<p><span class="koboSpan" id="kobo.1450.1">In this recipe, we </span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.1451.1">applied the techniques of transfer learning and fine-tuning, introduced at the beginning of the chapter, to the task of machine translation, which was also presented previously, in the fourth recipe, </span><em class="italic"><span class="koboSpan" id="kobo.1452.1">Translating text from Vietnamese to English</span></em><span class="koboSpan" id="kobo.1453.1">, in </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1454.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.1455.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1456.1">Understanding Text with Natural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1457.1">Language Processing</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1459.1">We explored two new datasets, </span><em class="italic"><span class="koboSpan" id="kobo.1460.1">WMT2014</span></em><span class="koboSpan" id="kobo.1461.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1462.1">WMT2016</span></em><span class="koboSpan" id="kobo.1463.1">, which, among other language pairs, support translations between German and English. </span><span class="koboSpan" id="kobo.1463.2">Moreover, MXNet GluonNLP provided </span><span class="No-Break"><span class="koboSpan" id="kobo.1464.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1465.1">A pre-trained Transformer model </span><span class="No-Break"><span class="koboSpan" id="kobo.1466.1">for </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1467.1">WMT2014</span></em></span></li>
<li><span class="koboSpan" id="kobo.1468.1">A data loader ready to be used </span><span class="No-Break"><span class="koboSpan" id="kobo.1469.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1470.1">WMT2016</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1471.1">Furthermore, we continued using the metrics introduced for machine translation, perplexity, </span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">and BLEU.</span></span></p>
<p><span class="koboSpan" id="kobo.1473.1">Having all these tools readily available within MXNet and GluonNLP allowed us to run the following experiments with just a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">of code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1475.1">Training a model from scratch </span><span class="No-Break"><span class="koboSpan" id="kobo.1476.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1477.1">WMT2016</span></em></span></li>
<li><span class="koboSpan" id="kobo.1478.1">Using a pre-trained model to optimize performance via transfer learning from </span><em class="italic"><span class="koboSpan" id="kobo.1479.1">WMT2014</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1480.1">to </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1481.1">WMT2016</span></em></span></li>
<li><span class="koboSpan" id="kobo.1482.1">Fine-tuning our pre-trained model on </span><em class="italic"><span class="koboSpan" id="kobo.1483.1">WMT2016</span></em><span class="koboSpan" id="kobo.1484.1"> (with and without </span><span class="No-Break"><span class="koboSpan" id="kobo.1485.1">freezing layers)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1486.1">We compared the</span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.1487.1"> results and derived the best approach for this particular task, which was applying transfer learning and </span><span class="No-Break"><span class="koboSpan" id="kobo.1488.1">fine-tuning afterward.</span></span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.1489.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.1490.1">In this recipe, we introduced two new datasets, </span><em class="italic"><span class="koboSpan" id="kobo.1491.1">WMT2014</span></em><span class="koboSpan" id="kobo.1492.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1493.1">WMT2016</span></em><span class="koboSpan" id="kobo.1494.1">. </span><span class="koboSpan" id="kobo.1494.2">These datasets were introduced as </span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.1495.1">challenges in the </span><strong class="bold"><span class="koboSpan" id="kobo.1496.1">Workshop on Statistical Machine Translation</span></strong><span class="koboSpan" id="kobo.1497.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1498.1">WMT</span></strong><span class="koboSpan" id="kobo.1499.1">) conference. </span><span class="koboSpan" id="kobo.1499.2">The</span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.1500.1"> results for 2014 and 2016 are </span><span class="No-Break"><span class="koboSpan" id="kobo.1501.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1502.1">Findings of the 2014 Workshop on Statistical Machine </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1503.1">Translation:</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1"> https://aclanthology.org/W14-3302.pdf</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1505.1">Findings of the 2016 Conference on Machine Translation (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1506.1">WMT16):</span></strong></span><span class="No-Break"> </span><a href="https://aclanthology.org/W16-2301.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1507.1">https://aclanthology.org/W16-2301.pdf</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1508.1">Transfer learning, including fine-tuning, for machine translation is an active area of research. </span><span class="koboSpan" id="kobo.1508.2">A paper published in 2020 explores its applications, titled </span><em class="italic"><span class="koboSpan" id="kobo.1509.1">In Neural Machine Translation, What Does Transfer Learning Transfer?</span></em><span class="koboSpan" id="kobo.1510.1"> and can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1511.1">here: </span></span><a href="https://aclanthology.org/2020.acl-main.688.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1512.1">https://aclanthology.org/2020.acl-main.688.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1513.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1514.1">For a more general approach to NLP use cases, a recent paper was published, </span><em class="italic"><span class="koboSpan" id="kobo.1515.1">A Survey on Transfer Learning in Natural Language Processing</span></em><span class="koboSpan" id="kobo.1516.1">, and can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1517.1">here: </span></span><a href="https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing"><span class="No-Break"><span class="koboSpan" id="kobo.1518.1">https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1519.1">.</span></span></p>
</div>
</body></html>