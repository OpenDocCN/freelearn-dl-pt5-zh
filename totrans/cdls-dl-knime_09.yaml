- en: '*Chapter 7:* Implementing NLP Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent
    Neural Networks for Demand Prediction*, we introduced **Recurrent Neural Networks**
    (**RNNs**) as a family of neural networks that are especially powerful to analyze
    sequential data. As a case study, we trained a **Long Short-Term Memory** (**LSTM**)-based
    RNN to predict the next value in the time series of consumed electrical energy.
    However, RNNs are not just suitable for strictly numeric time series, as they
    have also been applied successfully to other types of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Another field where RNNs are state of the art is **Natural Language Processing**
    (**NLP**). Indeed, RNNs have been applied successfully to text classification,
    language models, and neural machine translation. In all of these tasks, the time
    series is a sequence of words or characters, rather than numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will run a short review of some classic NLP case studies
    and their RNN-based solutions: a sentiment analysis application, a solution for
    free text generation, and a similar solution for the generation of name candidates
    for new products.'
  prefs: []
  type: TYPE_NORMAL
- en: We will start with an overview of text encoding techniques to prepare the sequence
    of words/characters to feed our neural network. The first case study, then, classifies
    text based on its sentiment. The last two case studies generate new text as sequences
    of new words, and new words as sequences of new characters, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Text Encoding Techniques for Neural Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the Tone of your Customers' Voice – Sentiment Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Free Text with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Product Names with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Text Encoding Techniques for Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned that feedforward networks
    – and all other neural networks as well – are trained on numbers and don't understand
    nominal values. In this chapter, we want to feed words and characters into neural
    networks. Therefore, we need to introduce some techniques to encode sequences
    of words or characters – that is, sequences of nominal values – into sequences
    of numbers or numerical vectors. In addition, in NLP applications with RNNs, it
    is mandatory that the order of words or characters in the sequence is retained
    throughout the text encoding procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at some **text encoding** techniques before we dive into the
    NLP case studies.
  prefs: []
  type: TYPE_NORMAL
- en: Index Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned about **index encoding**
    for nominal values. The idea was to represent each nominal class with an integer
    value, also called an index.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this same idea for text encoding. Here, instead of encoding each
    class with a different index, we encode each word or each character with a different
    index. First, a dictionary must be created to map all words/characters in the
    text collection to an index; afterward, through this mapping, each word/character
    is transformed into its corresponding index and, therefore, each sequence of words/characters
    into the sequence of corresponding indexes. In the end, each text is represented
    as a sequence of indexes, where each index encodes a word or a character. The
    following figure gives you an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – An example of text encoding via indexes at the word level](img/B16391_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – An example of text encoding via indexes at the word level
  prefs: []
  type: TYPE_NORMAL
- en: Notice that index 1, for the word *the*, and index 13, for the word *brown*,
    are repeated twice in the sequence, as the words appear twice in the example sentence,
    *the quick brown fox jumped over the brown dog*.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, in the *Finding the Tone of Your Customers' Voice – Sentiment
    Analysis* section, we'll use index encoding on words to represent text.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Free Text Generation with RNNs* section, on the other hand, we'll use
    one-hot vectors as text encoding on characters. Let's explore what one-hot vector
    encoding is.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Vector Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sequence of indexes has the disadvantage that it introduces an artificial
    distance between words/characters. For example, if *apple* is encoded as 1, *shoe*
    as 2, and *pear* as 3, *apple* and *pear* are further away from each other (distance
    = 2) than *shoe* and *pear* (distance = 1), which semantically might not make
    sense. In this way, as words don't have an ordered structure, we would introduce
    an artificial distance/similarity between words that might not exist in reality.
    We also encountered this problem in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*, and we solved it by introducing
    the concept of one-hot vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of `1` to encode a specific word/character, or otherwise to `0`. This
    means that each word/character is represented as a one-hot vector and therefore,
    each text is a sequence of one-hot vectors. The following figure shows an example
    of one-hot vector encoding for the sentence *the quick brown fox jumped over the
    brown dog*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice, in *Figure 7.2*, that the one-hot vectors for the words *the* and *brown*
    repeat twice in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – An example of text encoding via one-hot vectors at the word
    level](img/B16391_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – An example of text encoding via one-hot vectors at the word level
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the **Keras Learner** node can convert index-based encodings into
    one-hot vectors. Thus, to train a neural network on one-hot-vectors, it is sufficient
    to feed it with an index-based encoding of the text document.
  prefs: []
  type: TYPE_NORMAL
- en: A commonly used text encoding – similar to one-hot vectors but that doesn't
    retain the word order – are `1`) or absence (`0`) of the words. One vector represents
    one text document and contains multiple 1s. Notice that this encoding does not
    retain the word order because all of the text is encoded within the same vector
    structure regardless of the word order.
  prefs: []
  type: TYPE_NORMAL
- en: Working with words, the dimension of one-hot vectors is equal to the dictionary
    size – that is, to the number of words available in the document corpus. If the
    document corpus is large, the dictionary size quickly becomes the number of words
    in the whole language. Therefore, one-hot vector encoding on a word level can
    lead to very large and sparse representations.
  prefs: []
  type: TYPE_NORMAL
- en: Working with characters, the dictionary size is the size of the character set,
    which, even including punctuation and special signs, is much smaller than in the
    previous case. Thus, one-hot vector encoding fits well for character encoding
    but might lead to dimensionality explosion on word encoding.
  prefs: []
  type: TYPE_NORMAL
- en: To encode a document at the word level, a much more appropriate method is embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings for Word Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of word embeddings is to map words into a geometric space. This is
    done by associating a numeric vector to every word in a dictionary in a way that
    words with similar meanings have similar vectors and the distance between any
    two vectors captures part of the semantic relationship between the two associated
    words. The geometric space formed by these vectors is called the *embedding space*.
    For word encoding, the embedding space has a lower dimension (only a few tens
    or hundreds) than the vector space for one-hot vector encodings (in the order
    of many thousands).
  prefs: []
  type: TYPE_NORMAL
- en: To learn the projection of each word into the continuous vector space, a dedicated
    neural network layer is used, which is called the embedding layer. This layer
    learns to associate a vector representation with each word. The best-known word
    embedding techniques are **Word2vec** and **GloVe**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways that words embeddings can be used (J. Brownlee, *How to
    Use Word Embedding Layers for Deep Learning with Keras*, Machine Learning Mastery
    Blog, 2017, [https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)):'
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a ready-to-go layer previously trained on some external text corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a new embedding layer as part of your neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If trained jointly with a neural network, the input to an embedding layer is
    an index-based encoded sequence. The number of output units in the embedding layer
    defines the dimension of the embedding space. The weights of the embedding layer,
    which are used to calculate the embedding representation of each index, and therefore
    of each word, are learned during the training of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with different text encoding techniques, let's move
    on to our first NLP use case.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Tone of Your Customers' Voice – Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common use case for NLP is **sentiment analysis**. Here, the goal is to identify
    the underlying emotion in some text, whether positive or negative, and all the
    nuances in between. Sentiment analysis is implemented in many fields, such as
    to analyze incoming messages, emails, reviews, recorded conversations, and other
    similar texts.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, sentiment analysis belongs to a bigger group of NLP applications
    known as text classification. In the case of sentiment analysis, the goal is to
    predict the sentiment class.
  prefs: []
  type: TYPE_NORMAL
- en: Another common example of text classification is language detection. Here, the
    goal is to recognize the text language. In both cases, if we use an RNN for the
    task, we need to adopt a *many-to-one architecture*. A many-to-one neural architecture
    accepts a sequence of inputs at different times, ![](img/Formula_B16391_07_001.png),
    and uses the final state of the output unit to predict the one single class –
    that is, sentiment or language.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows an example of a many-to-one architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3  –  An example of a many-to-one neural architecture: a sequence
    of many inputs at different times and only the final status of the output](img/B16391_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3 – An example of a many-to-one neural architecture: a sequence of
    many inputs at different times and only the final status of the output'
  prefs: []
  type: TYPE_NORMAL
- en: In our first use case in this chapter, we want to analyze the sentiment of movie
    reviews. The goal is to train an RNN at a word level, with an embedding layer
    and an LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will use the IMDb dataset, which contains two columns:
    the text of the movie reviews and the sentiment. The sentiment is encoded as `1`
    for positive reviews and as `0` for negative reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.4* shows you a small subset with some positive and some negative
    movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews](img/B16391_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with reading and encoding the texts of the movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing Movie Reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The embedding layer expects index-based encoded input sequences. That is, each
    review must be encoded as a sequence of indexes, where each index (an integer
    value) represents a word in the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of words available in the IMDb document corpus is very high, we
    decided to reduce them during the text preprocessing phase, by removing stop words
    and reducing all words to their stems. In addition, only the ![](img/Formula_B16391_07_002.png)
    most frequent terms in the training set are encoded with a dedicated index, while
    all others receive just the default index.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, RNNs can handle sequences of variable length. In practice, though,
    the sequence length for all input samples in one training batch must be the same.
    As the number of words per review might differ, we define a fixed sequence length
    and we zero-pad too-short sequences – that is, we add 0s to complete the sequence
    – and we truncate too-long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: All these preprocessing steps are applied to the training set and the test set,
    with one difference. In the preprocessing of the training set, the dictionary
    with the ![](img/Formula_B16391_06_021.png) most frequent terms is created. This
    dictionary is then only applied during the preprocessing of the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we perform the following preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Read and partition the dataset into training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize, clean, and stem the movie reviews in the training set and the test
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dictionary of all the terms. The ![](img/Formula_B16391_03_029.png)
    most frequent terms in the training set are represented by dedicated indexes and
    all other terms by a default index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map the words in the training and test set to the corresponding dictionary indexes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Truncate too-long word sequences in the training set and test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zero-pad too-short sequences in the training set and test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The workflow in *Figure 7.5* performs all these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study](img/B16391_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study
  prefs: []
  type: TYPE_NORMAL
- en: The first metanode, **Read and partition data**, reads the table with the movie
    reviews and sentiment information and partitions the dataset into a training set
    and a test set. The **Preprocessing training set** metanode performs the different
    preprocessing steps on the training set and creates and applies the dictionary,
    which is available at the second output port. The last metanode, **Preprocess
    test set**, applies the created dictionary to the test set and performs the different
    preprocessing steps on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how all these steps are implemented in KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Partitioning the Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first part, reading and partitioning the dataset, is performed by the **Read
    and partition data** metanode.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.6* shows you the workflow snippet inside the metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Workflow snippet inside the Read and partition metanode](img/B16391_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Workflow snippet inside the Read and partition metanode
  prefs: []
  type: TYPE_NORMAL
- en: The **Table Reader** node reads the table with the sentiment information as
    an integer value and the movie reviews as a string value. Next, the sentiment
    information is transformed into a string with the **Number To String** node. This
    step is necessary to allow stratified sampling in the **Partitioning** node. In
    the last step, the data type of the column sentiment is transformed back into
    an integer using the **String To Number** node so that it can be used as the target
    column during training by the Keras Learner node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a training set and a test set, let's continue with the preprocessing
    of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Training Set and Dictionary Creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preprocessing of the training set and the creation of the dictionary is
    performed in the **Preprocess training set** metanode.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.7* shows you the inside of the metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Workflow snippet inside the Preprocess training set metanode](img/B16391_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Workflow snippet inside the Preprocess training set metanode
  prefs: []
  type: TYPE_NORMAL
- en: For the preprocessing of the movie reviews, the **KNIME Text Processing** extension
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME Text Processing extension includes nodes to read and write documents
    from and to a variety of text formats; to transform words; to clean up sentences
    of spurious characters and meaningless words; to transform a text into a numeric
    table; to calculate all required text statistics; and finally, to explore topics
    and sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNIME Text Processing extension relies on a new data type: **Document object**.
    Raw text becomes a document when additional metadata, such as title, author(s),
    source, and class, are added to it. Text in a document is tokenized following
    one of the many available language-specific tokenization algorithms. **Document
    tokenization** produces a hierarchical structure of the text items: sections,
    paragraphs, sentences, and words. Words are often referred to as tokens or terms.'
  prefs: []
  type: TYPE_NORMAL
- en: To make use of the preprocessing nodes of the KNIME Text Processing extension,
    we need to transform the movie reviews into documents, via the **Strings To Document**
    node. This node collects values from different columns and turns them into a document
    object, after tokenizing the main text.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.8* shows you the configuration window of the **Strings To Document**
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Configuration window of the Strings To Document node](img/B16391_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Configuration window of the Strings To Document node
  prefs: []
  type: TYPE_NORMAL
- en: 'The node gives you the opportunity to define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The document text via the **Full text** option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The document title, as a **Column**, **Row ID**, or **Empty string** value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The document source, document category, document authors, and document publication
    date as a fixed string or a column value. If column values are used, remember
    to enable the corresponding flag. Often, the **Document category** field is used
    to store the task class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The document type, as **Transaction**, **Proceeding**, **Book**, or just **UNKNOWN**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of the output document column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of parallel processes to execute the word tokenizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word tokenizer algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the document objects are cleaned through a sequence of text preprocessing
    nodes, contained in the **Text Preprocessing** component of the workflow in *Figure
    7.7*. The inside of the **Text Preprocessing** component is shown in *Figure 7.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
    ](img/B16391_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
  prefs: []
  type: TYPE_NORMAL
- en: The workflow snippet starts with the **Punctuation Erasure** node, to strip
    all punctuation from the input documents.
  prefs: []
  type: TYPE_NORMAL
- en: The **Number Filter** node filters out all numbers, expressed as digits, including
    decimal separators (**,** or **.**) and possible leading signs (**+** or **-**).
  prefs: []
  type: TYPE_NORMAL
- en: The **N Chars Filter** node filters out all terms with less than ![](img/Formula_B16391_07_005.png)
    – in our case, ![](img/Formula_B16391_07_006.png) – characters, as specified in
    the configuration window of the node.
  prefs: []
  type: TYPE_NORMAL
- en: Filler words, such as *so*, *thus*, and so on, are called **stop words**. They
    carry little information and can be removed with the **Stop Word Filter** node.
    This node filters out all terms that are contained in the selected stop word list.
    A custom stop word list can be passed to the node via the second input port, or
    a default built-in stop word list can be adopted. A number of built-in stop word
    lists are available for various languages.
  prefs: []
  type: TYPE_NORMAL
- en: The **Case Converter** node converts all terms into upper or lowercase. In this
    case study, they are converted into lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the **Snowball Stemmer** node reduces words to their stem, removing
    the grammar inflection, using the Snowball stemming library ([http://snowball.tartarus.org/](http://snowball.tartarus.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The goal of stemming is to reduce inflectional forms and derivationally related
    forms to a common base form. For example, *look*, *looking*, *looks*, and *looked*
    are all replaced by their stem, *look*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have cleaned up the text of the movie reviews of the training set,
    we can create the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Dictionary Based on the Training Set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dictionary must assign two indexes to each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Index**: A progressive integer index to each of the ![](img/Formula_B16391_07_007.png)
    most frequent terms in the training set and the same default index to all other
    terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Counter**: A progressive eight-digit index to each of the words. This eight-digit
    index is just a temporary index that will help us deal with truncation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.10* shows you a subset of the dictionary we want to create:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index](img/B16391_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index
  prefs: []
  type: TYPE_NORMAL
- en: 'Both indexes are created in the **Create Dictionary** component and *Figure
    7.11* shows you the workflow snippet inside the component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Workflow snippet contained in the Create Dictionary component](img/B16391_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Workflow snippet contained in the Create Dictionary component
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Create Dictionary** component has a configuration window, which you can
    see in *Figure 7.12*. The input option in the configuration window is inherited
    from the **Integer Configuration** node and requests the dictionary size as the
    number of the ![](img/Formula_B16391_03_173.png) most frequent words in the document
    collection. The default is ![](img/Formula_B16391_07_009.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Configuration window of the Create Dictionary component](img/B16391_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Configuration window of the Create Dictionary component
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow inside the component first creates a global set of unique terms
    over all the documents by using the **Unique Term Extractor** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Configuration window of the Unique Term Extractor node](img/B16391_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Configuration window of the Unique Term Extractor node
  prefs: []
  type: TYPE_NORMAL
- en: This node allows us to create an index column and a frequency column, as shown
    in the preceding screenshot. The index column contains a progressive integer number
    starting from `1`, where `1` is assigned to the most frequent term.
  prefs: []
  type: TYPE_NORMAL
- en: 'The node optionally provides the possibility to filter the top *k* most frequent
    terms. For that, three frequency measures are available: the **term frequency**,
    the **document frequency**, and the **inverse document frequency**. For now, we
    want to select all terms and we will work on the dictionary size later.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency** (**TF**): The number of occurrences of a term in all documents'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document frequency** (**DF**): The number of documents in which a term occurs'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverse document frequency** (**IDF**): Logarithm of the number of documents
    divided by DF'
  prefs: []
  type: TYPE_NORMAL
- en: The eight-digit index is created via the `1` as the step size. This minimum
    value guarantees the eight-digit format.
  prefs: []
  type: TYPE_NORMAL
- en: The **Index** and **Counter** columns are then converted from integers into
    strings with the **Number To String** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the reduction of the dictionary size. The top ![](img/Formula_B16391_03_173.png)
    most frequent terms keep the progressive index assigned by the **Unique Term Extractor**
    node, while all other terms get a default index of ![](img/Formula_B16391_07_011.png).
    Remember that ![](img/Formula_B16391_03_252.png) can be changed via the component''s
    configuration window. For this example, ![](img/Formula_B16391_07_013.png) was
    set to 20,000\. In the lower part of the component sub-workflow, the **Row Splitter**
    node splits the input data table into two sub-tables: the top ![](img/Formula_B16391_07_007.png)
    rows (top output port) and the rest of the rows (lower output port).'
  prefs: []
  type: TYPE_NORMAL
- en: The **Constant Value Column** node then replaces all index values with the default
    index value ![](img/Formula_B16391_07_015.png) in the lower sub-table. Lastly,
    the two sub-tables are concatenated back together.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the dictionary is ready, we can continue with the truncation of the
    movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Truncating Too-Long Documents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have stated that we will work with fixed-size documents – that is, with
    a maximum number of words for each document. If a document has more words than
    allowed, it will be truncated. If it has fewer words than allowed, it will be
    zero-padded. Let''s see how the **truncation** procedure works – that is, how
    we remove the last words from a too-long document. This all happens in the **Truncation**
    component. *Figure 7.14* shows you the workflow snippet inside the component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Workflow snippet inside the Truncation component](img/B16391_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Workflow snippet inside the Truncation component
  prefs: []
  type: TYPE_NORMAL
- en: First, we set the maximum number, ![](img/Formula_B16391_07_016.png), of terms
    allowed in a document. Again, this is a parameter that can be changed through
    the component's configuration window, shaped via the **Integer Configuration**
    node. We set the maximum number of terms in a document – that is, the maximum
    document size – as ![](img/Formula_B16391_07_017.png) terms. If a document is
    too long, we should just keep the first ![](img/Formula_B16391_07_018.png) terms
    and throw away the rest.
  prefs: []
  type: TYPE_NORMAL
- en: It is not easy to count the number of words in a text. Since words have variable
    lengths, we should detect the spaces separating the words within a loop and then
    count the words. Loops, however, often slow down execution. So, an alternative
    trick is to use the eight-digit representation of the words inside the text.
  prefs: []
  type: TYPE_NORMAL
- en: Within the text, each word is substituted by its eight-digit code via the **Dictionary
    Replacer** node. The **Dictionary Replacer** node matches terms in the input documents
    at the top input port with dictionary terms at the lower input port and then replaces
    them with the corresponding value in the dictionary table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Dictionary Replacer** node has two input ports:'
  prefs: []
  type: TYPE_NORMAL
- en: The upper input port for the documents containing the terms to be replaced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lower input port with the dictionary table for the matching and replacement
    operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dictionary table must consist of at least two string columns. One string
    column contains the terms to replace (keys) and the other string column contains
    the replacement strings (values). In the configuration window, we can set both
    columns from the data table at the lower input port.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, we have text with terms of fixed length (`8 digits + 1 <space>`)
    and not words of variable length. So, limiting a text to ![](img/Formula_B16391_07_019.png)
    words is the same as limiting a text to ![](img/Formula_B16391_07_020.png) characters,
    if ![](img/Formula_B16391_07_021.png), to 720 characters. This operation is much
    easier to carry out without loops or complex node structures, but just with a
    **String Manipulation** node. However, the **String Manipulation** node works
    on string objects and not on documents. To use it, we need to move temporarily
    back to text as strings.
  prefs: []
  type: TYPE_NORMAL
- en: The text is extracted from the document as a simple string with the **Document
    Data Extractor** node. This node extracts information, such as, for example, the
    text and title, from a document cell.
  prefs: []
  type: TYPE_NORMAL
- en: The **Math Formula (Variable)** node takes the flow variable for the maximum
    document size and calculates the maximum number of characters allowed in a document.
  prefs: []
  type: TYPE_NORMAL
- en: The `0`) until the maximum number of characters allowed, using the `substr()`
    function. This effectively keeps only the top ![](img/Formula_B16391_03_031.png)
    terms and removes all others.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the text is transformed back into a document, called **Truncated Document**,
    and all superfluous columns are removed in the **Column Filter** node.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the eight-digit indexes have exhausted their task and can be
    substituted with the progressive integer index for the encoding. This is done
    in the **Dictionary Replacer** node, once again.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have truncated too-long documents to the maximum number of terms
    allowed. Next, we need to zero-pad too-short documents.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Padding Too-Short Documents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When sequences are too short with respect to a set number of values, **zero-padding**
    is often applied. Zero-padding means that 0s are added to the sequence until the
    set number of values is reached. In our case, if a document has fewer words than
    the set number, we fill the remaining empty spaces with 0s. This happens in the
    **Zero Pad** component.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.15* shows you the workflow snippet inside the Zero Pad component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Workflow snippet inside the Zero Pad component](img/B16391_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Workflow snippet inside the Zero Pad component
  prefs: []
  type: TYPE_NORMAL
- en: Zero-padding is again performed at the string level, and not at the document
    level. After the text has been extracted as a string from the input document using
    the `<space>` and creates one new column for each index.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that all truncated text now has a maximum length of ![](img/Formula_B16391_03_255.png)
    indexes from the previous step. So, from those texts, the number of newly generated
    columns is surely ![](img/Formula_B16391_03_255.png). For all other texts with
    shorter-term sequences, the **Cell Splitter** node will fill the empty columns
    with missing values. It is enough to turn these missing values into 0s and the
    zero-padding procedure is complete. This replacement of missing values with 0s
    is performed by the **Missing Value** node.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, all superfluous columns are removed within the **Column Filter** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that all term sequences – that is, all text – have the same length, collection
    cells are created with the **Create Collection Cell** node to feed the Keras Learner
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to perform the same preprocessing on the test and apply the created
    dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Test Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The preprocessing of the test set is performed in the **Preprocess test set**
    metanode. This metanode has two input ports: the upper port for the dictionary
    created in the **Preprocess training set** metanode and the lower port for the
    test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.16* shows you the workflow snippet inside the Preprocess test set
    metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Workflow snippet inside the Preprocess test set metanode](img/B16391_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Workflow snippet inside the Preprocess test set metanode
  prefs: []
  type: TYPE_NORMAL
- en: The lower part of the workflow is similar to the workflow snippet inside the
    **Preprocess training set** metanode, only the part including the creation of
    the dictionary is different. Here, the dictionary for the test set is based on
    the dictionary from the training set. All terms available in the training set
    dictionary receive the corresponding index encoding; all remaining terms receive
    the default index.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, first a list of all terms in the test set is created using the **Unique
    Term Extractor** node. Next, this list is joined with the list of terms in the
    training set dictionary using a right outer join. A right outer join allows us
    to keep all the rows from the lower input port – that is, all terms in the test
    set – and to add the indexes from the training dictionary, if available. For all
    terms that are not in the training dictionary, the joiner node creates missing
    values in the index columns. These missing values are then replaced with the default
    index value using the **Missing Value** node.
  prefs: []
  type: TYPE_NORMAL
- en: All other steps, such as truncation and zero-padding, are performed in the same
    way as in the preprocessing of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: We have finished the preprocessing phase and we can now continue with the definition
    of the network architecture and its training.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Training the Network Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will define and train the network architecture for this
    sentiment classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We want to use an LSTM-based RNN, where we train the embedding as well. The
    embedding is trained by an embedding layer. Therefore, we create a neural network
    with four layers:'
  prefs: []
  type: TYPE_NORMAL
- en: An **input layer** to define the input size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **embedding layer** to produce an embedding representation of the term space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **LSTM layer** to exploit the sequential property of the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **dense layer** with one unit with the sigmoid activation function, as we
    have a binary classification problem at hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding layer expects a sequence of index-based encoded terms as input.
    Therefore, the input layer must accept sequences of ![](img/Formula_B16391_03_031.png)
    integer indexes (in our case, ![](img/Formula_B16391_07_026.png)). This means
    `Shape = 80` and `data type = Int 32` in the configuration window of the **Keras
    Input Layer** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the **Keras Embedding Layer** node must learn to embed the integer indexes
    into an appropriate high-dimensional vector space. *Figure 7.17* shows its configuration
    window. The input tensor is directly recovered from the output of the previous
    input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Configuration window of the Keras Embedding Layer node](img/B16391_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Configuration window of the Keras Embedding Layer node
  prefs: []
  type: TYPE_NORMAL
- en: There are two important configuration settings for the `128`. The output tensor
    of the `[sequence length` ![](img/Formula_B16391_03_255.png)`, embedding dimension]`.
    In our case, this is `[80, 128]`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `128` units, which means `Units = 128`, `Activation = Tanh`, `Recurrent
    activation = Hard sigmoid`, `Dropout = 0.2`, `Recurrent dropout = 0.2`, and return
    sequences, return state, go backward, and unroll all `unchecked`.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, a **Keras Dense Layer** node with one unit with the sigmoid activation
    function is used to predict the final binary sentiment classification.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our preprocessed data and the neural architecture, we can start
    training the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Recurrent Network with Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network is trained, as usual, with the **Keras Network Learner** node.
  prefs: []
  type: TYPE_NORMAL
- en: In the first tab, **Input Data**, the **From Collection of Number (integer)**
    conversion is selected, as our input is a collection cell of integer values (the
    indexes), encoding our movie reviews. Next, the collection cell is selected as
    input.
  prefs: []
  type: TYPE_NORMAL
- en: In the second tab, **Target Data**, the **From Number (integer)** conversion
    type and the column with the sentiment class are selected. In the lower part,
    the binary cross-entropy is selected as the loss function since it is a binary
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: In the third tab, `Epochs = 30`, `Training batch size = 100`, shuffle training
    data before each epoch is activated, and `Optimizer = Adam` (with the default
    settings).
  prefs: []
  type: TYPE_NORMAL
- en: Now that the network is trained, we can apply it to the test set and evaluate
    how good its performance is at predicting the sentiment behind a review text.
  prefs: []
  type: TYPE_NORMAL
- en: Executing and Evaluating the Network on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To execute the network on the test set, the **Keras Network Executor** node
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window, we again select **From Collection of Number (integer)**
    as the conversion type and the collection cell as input.
  prefs: []
  type: TYPE_NORMAL
- en: As output, we are interested in the output of the last dense layer, as this
    gives us the probability for sentiment being equal to `1` (positive). Therefore,
    we click on the **add output** button, select the sigmoid layer, and make sure
    that the **To Number (double)** conversion is used.
  prefs: []
  type: TYPE_NORMAL
- en: The `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the **Rule Engine** node translates this probability into a class prediction
    with the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `$dense_1/Sigmoid:0_0$` is the name of the output column from the network.
  prefs: []
  type: TYPE_NORMAL
- en: The expression transforms all values above `0.5` into 1s, and into 0s otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the different instruction lines in a **Rule Engine** node are
    executed sequentially. Execution stops when the antecedent in one line is verified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, the **Scorer** node evaluates the performance of the model and the
    **Keras Network Writer** node saves the trained network for deployment. *Figure
    7.18* shows the network performance, in the view of the **Scorer** node, achieving
    a respectable 83% of correct sentiment classification on the movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification](img/B16391_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have finished our first NLP case study. *Figure 7.19* displays
    the complete workflow used to implement the example. You can download the workflow
    from the KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Complete workflow to prepare the text and build, train, and
    evaluate the neural network for sentiment analysis](img/B16391_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Complete workflow to prepare the text and build, train, and evaluate
    the neural network for sentiment analysis
  prefs: []
  type: TYPE_NORMAL
- en: For now, we offer no deployment workflow. In [*Chapter 10*](B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367),
    *Deploying a Deep Learning Network*, we will come back to this trained network
    to build a deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on to the next NLP application: free text generation with RNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating Free Text with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have seen how RNNs can be used for text classification, we can
    move on to the next case study. Here, we want to train an RNN to generate new
    free text in a certain style, be it Shakespearean English, a rap song, or mimicking
    a Brothers Grimm fairy tale. We will focus on the last application: training a
    network to generate free text in the style of Brothers Grimm fairy tales. However,
    the network and the process can be easily adjusted to produce a new rap song or
    a text in old Shakespearean English.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we train an RNN to generate new text?
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you need a text corpus to train the network to generate new text.
    Any text corpus is good. However, keep in mind that the text you use for training
    will define the style of the text automatically generated. If you train the network
    on Shakespearean theater, you will get new text in old Shakespearean English;
    if you train the network on rap songs, you will get urban-style text, maybe even
    with rhyme; if you train the network on fairy tales, you will get text in the
    fairy tale style.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for a network to generate new fairy tales, it must be trained on existing
    fairy tales. We downloaded the Brothers Grimm corpus from the Gutenberg project,
    from [https://www.gutenberg.org/ebooks/2591](https://www.gutenberg.org/ebooks/2591).
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Words or Characters?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second decision to make is whether to train the network at the word or character
    level. Both options have their pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Training a network at the word level sounds more logical since languages are
    structured by words and not by characters. Input sequences (sequences of words)
    are short but the dictionary size (all words in the domain) is large. On the other
    hand, training the network at a character level relies on much smaller and more
    manageable dictionaries, but might lead to very long input sequences. According
    to Wikipedia, the English language, for example, has around 170,000 different
    words and only 26 different letters. Even if we distinguish between uppercase
    and lowercase, and we add numbers, punctuation signs, and special characters,
    we have a dictionary with less than 100 characters.
  prefs: []
  type: TYPE_NORMAL
- en: We want to train a network to generate text in the Brothers Grimm style. In
    order to do that, we train the network with a few Brothers Grimm tales, which
    already implies a very large number of words in the dictionary. So, to avoid the
    problem of a huge dictionary and the consequent possibly unmanageable network
    size, we opt to train our fairy tale generator at the character level.
  prefs: []
  type: TYPE_NORMAL
- en: Training at the character level means that the network must learn to predict
    the next character after the past ![](img/Formula_B16391_03_029.png) characters
    have passed through the input. The training set, then, must consist of many samples
    of sequences of ![](img/Formula_B16391_05_005.png) characters together with the
    next character to predict (the target value).
  prefs: []
  type: TYPE_NORMAL
- en: During deployment, a start sequence of ![](img/Formula_B16391_05_005.png) characters
    must trigger the network to generate the new text. Indeed, this first sequence
    predicts the next character; then in the next step, the ![](img/Formula_B16391_07_031.png)
    most recent initial characters and the predicted character will make the new input
    sequence to predict the next character, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to clean, transform, and encode the
    text data from the Grimms' fairy tales to feed the network.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We populate the training set using the sliding window approach – that is, with
    partially overlapping sequences. To make this clearer, let's include the sentence
    `Once upon a time` in the training set using a window length of ![](img/Formula_B16391_07_032.png)
    and a sliding step of `1`. The five characters `Once<space>` should predict `u`;
    then we slide the window one step to the right, and `nce<space>u` should predict
    `p`. Again, we slide the window one character to the right and `ce<space>up` should
    predict `o`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left of *Figure 7.20*, you can see the created input sequences and the
    target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Example of overlapping sequences used for training](img/B16391_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Example of overlapping sequences used for training
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to encode the character sequences. In order to avoid introducing
    an artificial distance among characters, we opted for one-hot vector encoding.
    We will perform the one-hot encoding in two steps. First, we perform an index-based
    encoding; then we convert it into one-hot encoding in the **Keras Network Learner**
    node via the **From Collection of Number (integer)** conversion option to **One-Hot
    Tensor**. The resulting overlapping index-encoded sequences for the training set
    are shown on the right of *Figure 7.20*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow snippet in the next figure reads and transforms the fairy tales
    into overlapping index-based encoded character sequences and their associated
    target character. Both the input sequence and target character are stored in a
    collection-type column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Preprocessing workflow snippet reading and transforming text
    from'
  prefs: []
  type: TYPE_NORMAL
- en: Brothers Grimm fairy tales](img/B16391_07_021.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Preprocessing workflow snippet reading and transforming text from
    Brothers Grimm fairy tales
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Reads all the fairy tales from the corpus and extracts five fairy tales for
    training and `Snow white and Rose red` as the seed for deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshapes the text, placing one character per row in a single column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and applies the index-based dictionary, consisting, in this case, of
    the character set, including punctuation and special signs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the **Lag Column** node, creates the overlapping sequences and then re-sorts
    them from the oldest to the newest character in the sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encapsulates the input sequence and target character into collection-type columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's have a look at these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Extracting Fairy Tales
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The workflow snippet, in the **Read and Extract Fairy Tales** metanode, first
    reads the fairy tales using a **File Reader** node. The table has one column,
    where the content of each row corresponds to one line of a fairy tale.
  prefs: []
  type: TYPE_NORMAL
- en: Then, a **Row Filter** node removes the unnecessary meta-information at the
    top and the bottom of the file, such as the author, title, table of contents,
    and license agreement. We will not use any of this meta-information during training
    or deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The `Snow white and Rose red` and at the top output port, all the other fairy
    tales. We'll save `Snow white and Rose red` for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, a **Row Filter** node is used to extract the first five fairy tales, which
    are used for training.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is the reshaping of the text into a sequence of characters with
    one single column.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping the Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can create the overlapping sequences of characters to feed the network,
    we need to transform all the fairy tales text into a long sequence (column) of
    single characters: one character in each row. This step is called **reshaping**
    and it is implemented in the **Reshape Text** metanode. *Figure 7.22* shows its
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Workflow snippet inside the Reshape Text metanode](img/B16391_07_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Workflow snippet inside the Reshape Text metanode
  prefs: []
  type: TYPE_NORMAL
- en: 'It starts with two `<space>` character, by using the `regexReplace()` function.
    `regexReplace()` takes advantage of regular expressions, such as `"[^\\s]"` to
    match any character in the input string and `"$0 "` for the matched character
    plus `<space>`. The final syntax for the `regexReplace()` function, used within
    the `$Col0$`, is then the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, the `<space>` character, producing many columns with one character per
    cell.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the last character in the paragraph (the newline) has not received
    the `<space>` character afterward. To solve this problem, a constant column with
    a `<space>` character is added using the **Constant Value Column** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Unpivoting** node reshapes the data table from many columns into one
    column only with a sequence of single characters. Let''s spend a bit of time on
    the **Unpivoting** node and its unsuspected tricks for reshaping data tables.
    The **Unpivoting** node performs a disaggregation of the input data table. *Figure
    7.23* shows you an example. It distinguishes between value columns and retaining
    columns. The selected value columns are then rotated to become rows and attached
    to the corresponding values in the retaining columns. Since the rotation of the
    value columns might result in more than one row, a duplication of the rows with
    the retaining column values might be necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns](img/B16391_07_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns
  prefs: []
  type: TYPE_NORMAL
- en: For the reshaping of the text, we set all columns as value columns and none
    as retaining columns. The result is the representation of the fairy tale as a
    long sequence of characters within one column.
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, some cleaning up: all rows with missing values are removed with the
    **Row Filter** node.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Applying the Dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now need to create the dictionary and the index-based mapping for the index-based
    encoding. Since we work at the character level, the dictionary here is nothing
    more than the character set – that is, the list of unique characters in the fairy
    tales corpus. To get this list, we remove all duplicate characters from the reshaped
    text using the **Remove Duplicate Filter** node.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The **Remove Duplicate Filter** node is a powerful node when it comes to detecting
    and handling duplicate records in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we assign an index to each row – that is, to each unique character – with
    the `0` for `1` for **Scale Unit**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the dictionary ready, we apply it with the **Cell Replacer**
    node, already introduced in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Resorting the Overlapping Sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create the overlapping sequences of characters, we use the `100`, `1`, and
    incomplete rows at the beginning and end of the output table are skipped.
  prefs: []
  type: TYPE_NORMAL
- en: According to the way the `col-100`) is in the farthest column to the right;
    the current character to predict (`col`) is in the farthest column to the left.
    Basically, the time of the sequence is sorted backward with respect to what the
    network is expecting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows you an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Resulting output of the Lag Column node, where the time is
    sorted in ascending order from right to left](img/B16391_07_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Resulting output of the Lag Column node, where the time is sorted
    in ascending order from right to left
  prefs: []
  type: TYPE_NORMAL
- en: We need to reorder the columns to follow an ascending order from left to right,
    in order to have the oldest character on the left and the most recent character
    on the right. This re-sorting is performed by the **Resort Columns** metanode.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.25* shows you the inside of the metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Workflow snippet contained in the Resort Columns metanode](img/B16391_07_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Workflow snippet contained in the Resort Columns metanode
  prefs: []
  type: TYPE_NORMAL
- en: Here, the **Reference Column Resorter** node changes the order of the data columns
    in the table at the top input port according to the order established in the data
    table at the lower input port. The reference data table at the lower input port
    must contain a string-type column with the column headers from the first input
    table in a particular order. The columns in the first data table are then sorted
    according to the row order of the column names in the second data table.
  prefs: []
  type: TYPE_NORMAL
- en: To create the table with sorted column headers, we extract the column headers
    with the **Extract Column Header** node. The **Extract Column Header** node separates
    the column headers from the table content and outputs the column headers at the
    top output port and the content at the lower output port.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the row of column headers is transposed into a column with the **Transpose**
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we assign an increasing integer number to each column header via the
    **Counter Generation** node and we sort them by counter value in descending order
    using the **Sorter** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the column headers from the first table sorted correctly in
    time, we can input it at the lower port of the **Reference Column Resorter** node.
    The result is a data table where each row is a sequence of ![](img/Formula_B16391_07_034.png)
    characters, time is sorted from left to right, and subsequent rows contain overlapping
    character sequences. At this point, we can create the collection cells for the
    input and target data of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Even though the target data consists of only one single value, we still need
    to transform it into a collection cell so that the index can be transformed into
    a one-hot vector by the **Keras Network Learner** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to the next step: defining and training the network architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Training the Network Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now design and train an appropriate neural network architecture to deal
    with time series, character encoding, and overfitting, and to predict the next
    character in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this case study, we decided to use a neural network with four layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Keras input layer**, to define the input shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras LSTM layer**, to deal with time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dropout layer**, to prevent overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dense layer**, to output the probability of the next character
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As usual, we define the input shape of the neural network using a `?, 65`.
  prefs: []
  type: TYPE_NORMAL
- en: As we don't need the intermediate hidden states, we leave most of the settings
    as default in the `512`.
  prefs: []
  type: TYPE_NORMAL
- en: Free text generation can be seen as a multi-class classification application,
    where the characters are the classes. Therefore, the **Keras Dense Layer** node
    at the output of the network is set to have 65 units (one for each character in
    the character set) with the softmax activation function, to score the probability
    of each character to be the next character.
  prefs: []
  type: TYPE_NORMAL
- en: Let's proceed with training this network on the encoded overlapping sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, to train the network, we use the by-now-familiar **Keras Network Learner**
    node.
  prefs: []
  type: TYPE_NORMAL
- en: In the first configuration tab, **Input Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** to handle encoding conversion and the
    collection column with the character sequence as input.
  prefs: []
  type: TYPE_NORMAL
- en: In the second configuration tab, **Target Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** again on the collection column containing
    the target value. As this is a multi-class classification problem, we set the
    loss function to **Categorical Cross Entropy**.
  prefs: []
  type: TYPE_NORMAL
- en: In the third configuration tab, `50 epochs`, training batch size `256`, shuffling
    option `on`, and optimizer as `Adam` with default settings for the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: The network is finally saved in **Keras format** with the **Keras Network Writer**
    node. In addition, the network is converted into a TensorFlow network with the
    **Keras to TensorFlow Network Converter** node and saved with the **TensorFlow
    Network Writer** node. The TensorFlow network is used in deployment to avoid a
    time-consuming Python startup, required by the Keras network.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.26* shows the full workflow implementing all the described steps
    to train a neural network to generate fairy tales. This workflow and the used
    dataset are available on KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Workflow to train a neural network to generate fairy tales](img/B16391_07_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Workflow to train a neural network to generate fairy tales
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have trained and saved the network, let's move on to deployment
    to generate a new fairy tale's text.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To trigger the generation of new text during deployment, we start with an input
    sequence of the same length as each of the training sequences (![](img/Formula_B16391_07_036.png)).
    We feed the network with that sequence to predict the next character; then, we
    delete the oldest character in the sequence, add the predicted one, and apply
    the network again to our new input sequence, and so on. This is exactly the same
    procedure that we used in the case study for demand prediction. So, we will implement
    it here again with a recursive loop (*Figure 7.27*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Deployment workflow to generate new free text](img/B16391_07_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – Deployment workflow to generate new free text
  prefs: []
  type: TYPE_NORMAL
- en: 'The trigger sequence was taken from the **Snow white and Rose red** fairy tale.
    The text for the trigger sequence was preprocessed, sequenced, and encoded as
    in the workflow used to train the network. This is done in the **Read and Pre-Process**
    metanode, shown in *Figure 7.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence](img/B16391_07_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence
  prefs: []
  type: TYPE_NORMAL
- en: The workflow reads the **Snow white and Rose red** fairy tale as well as the
    dictionary from the files created in the training workflow. Then, the same preprocessing
    steps as in the training workflow are applied.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we read the trained TensorFlow network and apply it to the trigger
    sequence with the **TensorFlow Network Executor** node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the network is the probability of each character to be the next.
    We can pick the predicted character following two possible strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: The character with the highest probability is assigned to be the next character,
    known as the greedy strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next character is picked randomly according to the probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented both strategies in the **Extract Index** metanode in two
    different deployment workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.29* shows the content of the **Extract Index** metanode when implementing
    the first strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Workflow snippet to extract the character with the highest
    probability](img/B16391_07_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – Workflow snippet to extract the character with the highest probability
  prefs: []
  type: TYPE_NORMAL
- en: This metanode takes as input the output probabilities from the executed network
    and extracts the character with the highest probability. The key node here is
    the **Many to One** node, which extracts the cell with the highest score (probability)
    from the network output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.30* shows the content of the **Extract Index** metanode when implementing
    the second strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution](img/B16391_07_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution
  prefs: []
  type: TYPE_NORMAL
- en: This workflow snippet expects as input the probability distribution for the
    characters and picks one according to it. The key node here is the **Random Label
    Assigner (Data)** node, which assigns a value based on the input probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Random Label Assigner (Data)** node assigns one index to each data row
    at the lower input port based on the probability distribution at the upper input
    port. The data table at the upper input port must have two columns: one column
    with the class values – in our case, the index-encoded characters in string format
    – and one column with the corresponding probabilities. Therefore, the first part
    of the workflow snippet in *Figure 7.30* prepares the data table for the top input
    port of the **Random Label Assigner (Data)** node, from the network output, using
    the **Transpose** node, the **Counter Generation** node, and the **Number To String**
    node, while the **Table Creator** node creates a new table with only one row using
    the **Table Creator** node. This means the **Random Label Assigner (Data)** node
    then picks one index, based on the probability distribution defined by the table
    at the first input port.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the recursive loop and its implementation are explained in detail
    in [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent Neural
    Networks for Demand Prediction*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the deployment workflow, implementing both options, from the
    KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  prefs: []
  type: TYPE_NORMAL
- en: The New Fairy Tale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At last, I am sure you want to see the kind of free text that the network was
    able to produce. The following is an example of free generated text, using the
    first strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The trigger sequence of 100 characters (not italics) comes from the first sentence
    of the fairy tale, *Snow white and Rose red*. The remaining text has been automatically
    generated by the network.
  prefs: []
  type: TYPE_NORMAL
- en: '*SNOW-WHITE AND ROSE-RED There was once a poor widow who lived in a lonely
    cottage*. In front of the cas, and a hunbred of wine behind the door of the; and
    he said the ansmer: ''What want yeurnKyow yours went for bridd, like is good any,
    or cries, and we will say I only gave the witeved to the brood of the country
    to go away with it.'' But when the father said, ''The cat soon crick.'' The youth,
    the old …'
  prefs: []
  type: TYPE_NORMAL
- en: The network has successfully learned the structure of the English language.
    Although the text is not perfect, you can see sensible character combinations,
    full words, some correct usage of quotation marks, and other similarly interesting
    features that the network has assimilated from the training text.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Product Names with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This last NLP case study is similar to the previous one. There, we wanted the
    network to create new free text based on a start sequence; here, we want the network
    to create new free words based on a start token. There, we wanted the network
    to create new sequences of words; here, we want the network to create new sequences
    of characters. Indeed, the goal of this product name generation case study is
    to create new names – that is, new words. While there'll be some differences,
    the approaches will be similar.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the details of this new approach.
  prefs: []
  type: TYPE_NORMAL
- en: The Problem of Product Name Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, we don't associate artificial intelligence with creativity, as it
    is usually used to predict the outcome based on previously seen examples. The
    challenge for this case study is to use artificial intelligence to create something
    new, which is thought to be in the domain of creative minds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a classic creative marketing example: product naming. Before a
    new product can be launched to the market, it actually needs a name. To find the
    name, the most creative minds of the company come together to generate a number
    of proposals for product names, taking different requirements into account. For
    example, the product name should sound familiar to the customers and yet be new
    and fresh too. Of all those candidates, ultimately only one will survive and be
    adopted as the name for the new product. Not an easy task!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take one of the most creative industries: fashion. A company specializing
    in outdoor wear has a new line of clothes ready for the market. The task is to
    generate a sufficiently large number of name candidates for the new line of clothing.
    Names of mountains were proposed, as many other outdoor fashion labels have. Names
    of mountains evoke the feeling of nature and sound familiar to potential customers.
    However, new names must also be copyright free and original enough to stand out
    in the market.'
  prefs: []
  type: TYPE_NORMAL
- en: Why not use fictitious mountain names then? Since they are fictitious, they
    are copyright free and differ from competitor names; however, since they are similar
    to existing mountain names, they also sound familiar enough to potential customers.
    Could an artificial intelligence model help generate new fictitious mountain names
    that still sound realistic enough and are evocative of adventure? What kind of
    network architecture could we use for such a task?
  prefs: []
  type: TYPE_NORMAL
- en: As we want to be able to form new words that are somehow reminiscent of mountain
    names, the network must be trained on the names of already-existing mountains.
  prefs: []
  type: TYPE_NORMAL
- en: To form the training set, we use a list of 33,012 names of US mountains, as
    extracted from Wikipedia through a Wikidata query.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.31* shows you a subset of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Subset of US mountain names in the training set](img/B16391_07_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Subset of US mountain names in the training set
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have some training data, we can think about the network architecture.
    This time, we want to train a **many-to-many** LSTM-based RNN (see *Figure 7.32*).
    This means that during training, we have a sequence as input and a sequence as
    output. During deployment, the RNN, based on some initialized hidden states and
    the start token, must predict the first character of the new name candidate; then
    at the next step, based on the predicted character and on the updated hidden states,
    it must predict the next character – and so on until an end token is predicted
    and the process of generating the new candidate name is concluded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN
    architecture for the product name generation case study](img/B16391_07_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN architecture
    for the product name generation case study
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the LSTM unit for this task, we need two sequences: an input sequence,
    made of a start token plus the mountain name, and a target sequence, made of the
    mountain name plus an end token. Notice that, at each training iteration, we feed
    the correct character into the network from the training set and not its prediction.
    This is called the **teacher forcing** training approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus first on preprocessing and encoding input and target sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and Encoding Mountain Names
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the preprocessing is to create and encode input and target sequences,
    including the start and end tokens. As in the previous case study, we want to
    use one-hot encoding. Therefore, we create an index-based encoding, and we use
    the `1` as the start token index and `0` as the end token index.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last case study, you learned that during training, the lengths of the
    sequences in one batch have to be the same. Therefore, we take the number of characters
    of the longest mountain name (58) plus 1 as the sequence length. Since this is
    the length of the longest mountain name, there is no need for truncation, but
    all shorter sequences will be zero-padded by adding multiple end tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation](img/B16391_07_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow snippet in the preceding figure creates the input and target sequences
    by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the mountain names and removing duplicates by using the **Table Reader**
    node and the **Duplicate Row Filter** node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replacing each `<space>` with a tilde character and afterward, each character
    with the character itself and `<space>`, using two **String Manipulation** nodes
    (this step is described in detail in the preprocessing of the previous case study,
    *Free text generation with RNNs*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating and applying a dictionary (we will have a close look at this step in
    the next sub-section)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Character splitting based on `<space>` and replacing all missing values with
    end tokens, to zero pad too-short sequences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating input and target sequences as collection type cells
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of the steps are similar to the preprocessing steps in the case study of
    free text generation with RNNs. We will only take a closer look at *step 3* and
    *step 5*. Let's start with *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Applying a Dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating and applying the dictionary is implemented in the **Create and apply
    dictionary** metanode. *Figure 7.34* shows its contents. The input to this metanode
    is mountain names with spaced characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode](img/B16391_07_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode
  prefs: []
  type: TYPE_NORMAL
- en: In this metanode, we again use nodes from the KNIME Text Processing extension.
    The `2`, as we want to use indexes `0` and `1` for the end and start tokens. To
    use it as a dictionary in the next step, the created numerical indexes are transformed
    into strings by the **Number To String** node. Finally, the dictionary is applied
    (the **Dictionary Replacer** node), to transform characters into indexes in the
    original mountain names, and the text is extracted from the document (the **Document
    Data Extractor** node).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME Text Processing extension and some of their nodes, such as **Strings
    To Document**, **Unique Term Extractor**, **Dictionary Replacer**, and **Document
    Data Extractor**, were introduced more in detail in the first case study of this
    chapter, *Finding the Tone of Your Customers' Voice – Sentiment Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the separate, lower branch of the workflow snippet, we finalize the dictionary
    for the deployment by adding one more row for the end token, using the `0` as
    the default value for the integer cells and an empty string for the string cells.
    This adds one new row to our dictionary table, with `0` in the index column and
    empty strings in the character columns. We need this additional row in the deployment
    workflow to remove the end token(s):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Configuration window of the Add Empty Rows node](img/B16391_07_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Configuration window of the Add Empty Rows node
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the last step of the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Input and Target Sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the **Missing Value** node in the workflow in *Figure 7.35*, we have the
    zero-padded, encoded sequences. What is missing, though, is the start token at
    the beginning of the input sequence and the end token at the end of the target
    sequence, to make sure that the input and target sequence have the same length.
  prefs: []
  type: TYPE_NORMAL
- en: The additional values are added with `1` is used for the start token in the
    input sequence and the value `0` for the end token in the target sequence. In
    the case of the input sequence, the new column with the start token must be at
    the beginning. This is taken care of by the **Column Resorter** node. Now, the
    sequences can be aggregated and transformed into collection cells, using the **Create
    Collection Column** node.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now design and train the appropriate network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Training the Network Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of designing and training the network is similar to the process
    used in the previous NLP case studies.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case, we want to use a network with five layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Keras input layer** to define the input shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras LSTM layer** for the sequence analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dropout layer** for regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dense layers** with linear activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras softmax layer** to transform the output into a probability distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of unique characters in the training set – that is, the character
    set size – is `95`. Since we allow sequences of variable length, the shape of
    the input layer is `?, 95`. The `?` stands for a variable sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the `256` units for this layer and we have left all other settings
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we want to add even more randomization to the character
    pick at the output layer, to increment the network creativity. This is done by
    introducing the ![](img/Formula_B16391_07_037.png) **temperature** parameter in
    the softmax function of the trained output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, the softmax function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B16391_07_038.png) with ![](img/Formula_B16391_07_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we now introduce the additional ![](img/Formula_B16391_07_040.png) **temperature**
    parameter, the formula for the activation function changes to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B16391_07_041.png) with ![](img/Formula_B16391_07_042.png)'
  prefs: []
  type: TYPE_IMG
- en: This means we divide the linear part by ![](img/Formula_B16391_07_043.png) before
    applying the softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to insert the temperature parameter after training, we split the
    output layer into two layers: one **Keras Dense Layer** node with a linear activation
    function for the linear part and one **Keras Softmax Layer** node to apply the
    activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Temperature is a parameter that can be added after training to control the confidence
    of the network output. ![](img/Formula_B16391_07_044.png) makes the network more
    confident but also more conservative. This often leads to generating the same
    results at every run.![](img/Formula_B16391_07_045.png) implements softer probability
    distributions over the different outputs. This leads to more diversity but, at
    the same time, also to more mistakes, such as in this case, character combinations
    that are impossible in English.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Postprocessing the Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network is trained using the **Keras Network Learner** node. For the input
    data and the target data, the **From Collection of Number (integer)** conversion
    to **One-Hot Tensor** is selected. The different characters are again like different
    classes in a multi-class classification problem; therefore, the **Categorical
    Cross Entropy** loss function is adopted for training.
  prefs: []
  type: TYPE_NORMAL
- en: In the third tab, `30` epochs, with a batch size of `128` data rows, shuffling
    the data before each epoch, and using `Adam` as the optimizer algorithm with the
    default settings. So far, this is all the same as in the previous NLP case studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the network, the temperature, ![](img/Formula_B16391_07_040.png),
    is added by using the **DL Python Editor** node with the following lines of Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the hidden states of the previous LSTM unit are always used as
    input in the next LSTM unit. Therefore, three inputs are defined in the code:
    two for the two hidden states and one for the last predicted character encoded
    as a one-hot vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the network is transformed into a TensorFlow network object and saved
    for deployment. The final training workflow is shown in *Figure 7.36*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Training workflow for the product name generation case study](img/B16391_07_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Training workflow for the product name generation case study
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow is available on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with the deployment workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deployment workflow again uses the recursive loop approach, similar to the
    deployment workflow of the NLP and the demand prediction case studies. This time,
    though, there is one big difference.
  prefs: []
  type: TYPE_NORMAL
- en: In the last two case studies, the hidden state vectors were re-initialized at
    each iteration, as we always had ![](img/Formula_B16391_03_031.png) previous characters
    or ![](img/Formula_B16391_03_255.png) previous values as input. In this case study,
    we pass back, from the loop end node to the loop start node, not only the predicted
    index but also the two hidden state tensors from the LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.37*, you can see the deployment workflow, which is also available
    on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.
    Let''s look at the setting differences in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Deployment workflow to create multiple possible product names](img/B16391_07_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.37 – Deployment workflow to create multiple possible product names
  prefs: []
  type: TYPE_NORMAL
- en: The first component, `1`. The other two columns contain the initial hidden states
    – that is, collection cells with 256 zeros in both columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **TensorFlow Network Executor** node executes the network one first time,
    producing as output the probability distribution over the indexes. In the configuration
    window of **TensorFlow Network Executor**, we have selected as input the columns
    with the first hidden state, the second hidden state, and the input collection.
    In addition, we set three output columns: one output column for the probability
    distribution, one output column for the first hidden state, and one output column
    for the second hidden state. We then pick the next index-encoded character according
    to the output probability distribution using the **Random Label Assigner (Data)**
    node in the **First Char** metanode. All these output values, predicted indexes,
    and hidden states make their way to the loop start node to predict the second
    index-encoded character.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we start the recursive loop to generate one character after the next.
    At each iteration, we apply the network to the last predicted index and hidden
    states. We then pick the next character, again with the **Random Number Assigner
    (Data)** node, and we feed the last predicted value and the new hidden states
    into the lower input port of the **Recursive Loop End** node so that they can
    reach back to the loop start node.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Extract Mountain Names** component, we finally apply the dictionary
    – created in the training workflow – and we remove all the mountain names that
    appeared already in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.38*, you can see some of the generated mountain names. Indeed,
    they are new, copyright-free, evocative of mountains, and nature-feeling, and
    can be generated automatically in a number ![](img/Formula_B16391_03_036.png)
    as high as desired:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38 – Mountain names generated by the deployment workflow](img/B16391_07_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.38 – Mountain names generated by the deployment workflow
  prefs: []
  type: TYPE_NORMAL
- en: One of them will eventually be chosen as the new product name.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the end of this relatively long chapter. Here, we have described
    three NLP case studies, each one solved by training an LSTM-based RNN applied
    to a time series prediction kind of problem.
  prefs: []
  type: TYPE_NORMAL
- en: The first case study analyzed movie review texts to extract the sentiment hidden
    in it. We dealt there with a simplified problem, considering a binary classification
    (positive versus negative) rather than considering too many nuances of possible
    user sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: The second case study was language modeling. Training an RNN on a given text
    corpus with a given style produced a network capable of generating free text in
    that given style. Depending on the text corpus on which the network is trained,
    it can produce fairy tales, Shakespearean dialogue, or even rap songs. We showed
    an example that generates text in fairy tale style. The same workflows can be
    easily extended with more success to generate rap songs (R. Silipo, *AI generated
    rap songs*, CustomerThink, 2019, [https://customerthink.com/ai-generated-rap-songs/](https://customerthink.com/ai-generated-rap-songs/))
    or Shakespearean dialogue (R. Silipo, *Can AI write like Shakespeare?*, Towards
    data Science, 2019, [https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee](https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee)).
  prefs: []
  type: TYPE_NORMAL
- en: The last case study involved the generation of candidates for new product names
    that must be innovative and copyright-free, stands out in the market, and be evocative
    of nature. So, we trained an RNN to generate fictitious mountain names to be used
    as name candidates for a new outdoor clothing line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will describe one more NLP example: neural machine
    translation.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a word embedding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) An encoding functionality that can be trained within the neural network
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A text cleaning procedure
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) A training algorithm for an RNN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) A postprocessing technique to choose the most likely character
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which statement regarding sentiment analysis is true?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Sentiment analysis can only be solved with RNNs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Sentiment analysis is the same as emotion detection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Sentiment analysis identifies the underlying sentiment in a text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Sentiment analysis is an image processing task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What does a many-to-many architecture mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) An architecture with an input sequence and an output sequence
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) An architecture with an input sequence and a vector as output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) An architecture with many hidden units and many outputs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) An architecture with one input feature and an output sequence
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why do I need a trigger sequence for free text generation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) To calculate the probabilities
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) To compare the prediction with the target
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) To initialize the hidden states before predicting the next character
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) To save the network in TensorFlow format
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
