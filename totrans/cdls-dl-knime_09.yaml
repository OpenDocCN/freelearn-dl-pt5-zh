- en: '*Chapter 7:* Implementing NLP Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章：* 实现NLP应用'
- en: In [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent
    Neural Networks for Demand Prediction*, we introduced **Recurrent Neural Networks**
    (**RNNs**) as a family of neural networks that are especially powerful to analyze
    sequential data. As a case study, we trained a **Long Short-Term Memory** (**LSTM**)-based
    RNN to predict the next value in the time series of consumed electrical energy.
    However, RNNs are not just suitable for strictly numeric time series, as they
    have also been applied successfully to other types of time series.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181)《需求预测的循环神经网络》中，我们介绍了**循环神经网络**（**RNN**）作为一种特别擅长分析顺序数据的神经网络家族。作为案例研究，我们训练了一个基于**长短期记忆**（**LSTM**）的RNN来预测消耗的电能时间序列中的下一个值。然而，RNN不仅仅适用于严格的数字时间序列，它们也成功地应用于其他类型的时间序列。
- en: Another field where RNNs are state of the art is **Natural Language Processing**
    (**NLP**). Indeed, RNNs have been applied successfully to text classification,
    language models, and neural machine translation. In all of these tasks, the time
    series is a sequence of words or characters, rather than numbers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的另一个先进应用领域是**自然语言处理**（**NLP**）。实际上，RNN已成功应用于文本分类、语言模型和神经机器翻译等任务。在所有这些任务中，时间序列是由单词或字符组成的序列，而不是数字。
- en: 'In this chapter, we will run a short review of some classic NLP case studies
    and their RNN-based solutions: a sentiment analysis application, a solution for
    free text generation, and a similar solution for the generation of name candidates
    for new products.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将简要回顾一些经典的NLP案例研究及其基于RNN的解决方案：情感分析应用、自由文本生成解决方案，以及为新产品生成名称候选的类似解决方案。
- en: We will start with an overview of text encoding techniques to prepare the sequence
    of words/characters to feed our neural network. The first case study, then, classifies
    text based on its sentiment. The last two case studies generate new text as sequences
    of new words, and new words as sequences of new characters, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述文本编码技术，以准备将单词/字符序列输入我们的神经网络。第一个案例研究将文本根据其情感进行分类。接下来的两个案例研究分别生成新的文本序列和新的单词序列，新的单词则由新字符序列构成。
- en: 'In this chapter we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Exploring Text Encoding Techniques for Neural Networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络的文本编码技术
- en: Finding the Tone of your Customers' Voice – Sentiment Analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到客户声音的语气——情感分析
- en: Generating Free Text with RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN生成自由文本
- en: Generating Product Names with RNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN生成产品名称
- en: Exploring Text Encoding Techniques for Neural Networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索神经网络的文本编码技术
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned that feedforward networks
    – and all other neural networks as well – are trained on numbers and don't understand
    nominal values. In this chapter, we want to feed words and characters into neural
    networks. Therefore, we need to introduce some techniques to encode sequences
    of words or characters – that is, sequences of nominal values – into sequences
    of numbers or numerical vectors. In addition, in NLP applications with RNNs, it
    is mandatory that the order of words or characters in the sequence is retained
    throughout the text encoding procedure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建与训练前馈神经网络》中，你学习了前馈网络——以及所有其他神经网络——是通过数字进行训练的，并不理解名义值。在本章中，我们希望将单词和字符输入神经网络。因此，我们需要引入一些技术，将单词或字符序列——也就是名义值的序列——编码为数字序列或数值向量。此外，在使用RNN的NLP应用中，必须确保在整个文本编码过程中保持单词或字符序列的顺序。
- en: Let's have a look at some **text encoding** techniques before we dive into the
    NLP case studies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入NLP案例研究之前，让我们先看一下几种**文本编码**技术。
- en: Index Encoding
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引编码
- en: In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Neural Network*, you learned about **index encoding**
    for nominal values. The idea was to represent each nominal class with an integer
    value, also called an index.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建与训练前馈神经网络》中，你学习了针对名义值的**索引编码**。其思想是用整数值表示每个名义类，这个整数值也叫做索引。
- en: 'We can use this same idea for text encoding. Here, instead of encoding each
    class with a different index, we encode each word or each character with a different
    index. First, a dictionary must be created to map all words/characters in the
    text collection to an index; afterward, through this mapping, each word/character
    is transformed into its corresponding index and, therefore, each sequence of words/characters
    into the sequence of corresponding indexes. In the end, each text is represented
    as a sequence of indexes, where each index encodes a word or a character. The
    following figure gives you an example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的思路进行文本编码。在这里，我们不是用不同的索引来编码每个类别，而是用不同的索引来编码每个单词或每个字符。首先，必须创建一个字典，将文本集合中的所有单词/字符映射到一个索引；然后，通过这个映射，每个单词/字符被转换成其对应的索引，从而将每个单词/字符序列转换成相应索引的序列。最终，每个文本被表示为一个索引序列，其中每个索引编码了一个单词或一个字符。下面的图给出了一个例子：
- en: '![Figure 7.1 – An example of text encoding via indexes at the word level](img/B16391_07_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 基于索引的单词级文本编码示例](img/B16391_07_001.jpg)'
- en: Figure 7.1 – An example of text encoding via indexes at the word level
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 基于索引的单词级文本编码示例
- en: Notice that index 1, for the word *the*, and index 13, for the word *brown*,
    are repeated twice in the sequence, as the words appear twice in the example sentence,
    *the quick brown fox jumped over the brown dog*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在序列中，单词*the*的索引1和单词*brown*的索引13在序列中被重复了两次，因为这两个单词在示例句子*the quick brown fox
    jumped over the brown dog*中各出现了两次。
- en: Later in this chapter, in the *Finding the Tone of Your Customers' Voice – Sentiment
    Analysis* section, we'll use index encoding on words to represent text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，在*找到客户声音的语气 – 情感分析*一节中，我们将使用基于索引的编码来表示文本中的单词。
- en: In the *Free Text Generation with RNNs* section, on the other hand, we'll use
    one-hot vectors as text encoding on characters. Let's explore what one-hot vector
    encoding is.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在*使用RNN进行自由文本生成*一节中，我们将使用独热向量作为字符级文本编码。让我们来探索什么是独热向量编码。
- en: One-Hot Vector Encoding
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热向量编码
- en: The sequence of indexes has the disadvantage that it introduces an artificial
    distance between words/characters. For example, if *apple* is encoded as 1, *shoe*
    as 2, and *pear* as 3, *apple* and *pear* are further away from each other (distance
    = 2) than *shoe* and *pear* (distance = 1), which semantically might not make
    sense. In this way, as words don't have an ordered structure, we would introduce
    an artificial distance/similarity between words that might not exist in reality.
    We also encountered this problem in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*, and we solved it by introducing
    the concept of one-hot vectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 索引序列的缺点在于，它在单词/字符之间引入了人为的距离。例如，如果*apple*被编码为1，*shoe*为2，*pear*为3，那么*apple*和*pear*之间的距离是2，而*shoe*和*pear*之间的距离是1，这在语义上可能并不合理。通过这种方式，由于单词没有顺序结构，我们会人为地在单词之间引入一些可能在实际中并不存在的距离/相似性。我们在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建和训练前馈神经网络》中也遇到了这个问题，并通过引入独热向量的概念来解决它。
- en: The idea of `1` to encode a specific word/character, or otherwise to `0`. This
    means that each word/character is represented as a one-hot vector and therefore,
    each text is a sequence of one-hot vectors. The following figure shows an example
    of one-hot vector encoding for the sentence *the quick brown fox jumped over the
    brown dog*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`1`来编码特定的单词/字符，否则使用`0`。这意味着每个单词/字符都被表示为一个独热向量，因此，每个文本是一个独热向量的序列。下图展示了句子*the
    quick brown fox jumped over the brown dog*的独热向量编码示例。
- en: 'Notice, in *Figure 7.2*, that the one-hot vectors for the words *the* and *brown*
    repeat twice in the sequence:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图 7.2*中，单词*the*和*brown*的独热向量在序列中被重复了两次：
- en: '![Figure 7.2 – An example of text encoding via one-hot vectors at the word
    level](img/B16391_07_002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 基于独热向量的单词级文本编码示例](img/B16391_07_002.jpg)'
- en: Figure 7.2 – An example of text encoding via one-hot vectors at the word level
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 基于独热向量的单词级文本编码示例
- en: Tip
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Remember that the **Keras Learner** node can convert index-based encodings into
    one-hot vectors. Thus, to train a neural network on one-hot-vectors, it is sufficient
    to feed it with an index-based encoding of the text document.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，**Keras Learner**节点可以将基于索引的编码转换为独热向量。因此，要训练一个神经网络来处理独热向量，只需将文本文档的基于索引的编码输入即可。
- en: A commonly used text encoding – similar to one-hot vectors but that doesn't
    retain the word order – are `1`) or absence (`0`) of the words. One vector represents
    one text document and contains multiple 1s. Notice that this encoding does not
    retain the word order because all of the text is encoded within the same vector
    structure regardless of the word order.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的文本编码——类似于独热向量，但不保留单词顺序——是`1）`或缺失（`0`）的单词。一个向量表示一个文本文档，包含多个1。请注意，这种编码不保留单词顺序，因为所有文本都被编码在同一个向量结构中，而不管单词的顺序如何。
- en: Working with words, the dimension of one-hot vectors is equal to the dictionary
    size – that is, to the number of words available in the document corpus. If the
    document corpus is large, the dictionary size quickly becomes the number of words
    in the whole language. Therefore, one-hot vector encoding on a word level can
    lead to very large and sparse representations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理单词时，独热编码向量的维度等于词典大小——也就是说，等于文档语料库中可用单词的数量。如果文档语料库很大，词典大小会迅速变成整个语言中的单词数量。因此，在单词级别上的独热编码可能会导致非常大且稀疏的表示。
- en: Working with characters, the dictionary size is the size of the character set,
    which, even including punctuation and special signs, is much smaller than in the
    previous case. Thus, one-hot vector encoding fits well for character encoding
    but might lead to dimensionality explosion on word encoding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理字符时，词典大小是字符集的大小，即使包括标点符号和特殊符号，这也比前一种情况小得多。因此，独热向量编码适合字符编码，但在单词编码时可能导致维度爆炸。
- en: To encode a document at the word level, a much more appropriate method is embeddings.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要在单词级别对文档进行编码，一种更合适的方法是使用嵌入技术。
- en: Embeddings for Word Encoding
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词编码的嵌入技术
- en: The goal of word embeddings is to map words into a geometric space. This is
    done by associating a numeric vector to every word in a dictionary in a way that
    words with similar meanings have similar vectors and the distance between any
    two vectors captures part of the semantic relationship between the two associated
    words. The geometric space formed by these vectors is called the *embedding space*.
    For word encoding, the embedding space has a lower dimension (only a few tens
    or hundreds) than the vector space for one-hot vector encodings (in the order
    of many thousands).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的目标是将单词映射到几何空间中。通过为词典中的每个单词关联一个数值向量来实现这一点，使得具有相似含义的单词具有相似的向量，并且任何两个向量之间的距离捕捉了这两个相关单词之间的某些语义关系。由这些向量构成的几何空间被称为*嵌入空间*。对于单词编码，嵌入空间的维度比独热编码的向量空间要低（通常只有几十或几百），而独热编码的向量空间维度则通常达到几千。
- en: To learn the projection of each word into the continuous vector space, a dedicated
    neural network layer is used, which is called the embedding layer. This layer
    learns to associate a vector representation with each word. The best-known word
    embedding techniques are **Word2vec** and **GloVe**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习每个单词在连续向量空间中的投影，使用了一个专门的神经网络层，这个层被称为嵌入层。这个层学习将每个单词与一个向量表示关联起来。最著名的词嵌入技术是**Word2vec**和**GloVe**。
- en: 'There are two ways that words embeddings can be used (J. Brownlee, *How to
    Use Word Embedding Layers for Deep Learning with Keras*, Machine Learning Mastery
    Blog, 2017, [https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以有两种使用方式（J. Brownlee，*如何在Keras中使用词嵌入层进行深度学习*，机器学习精通博客，2017年，[https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)）：
- en: Adopting a ready-to-go layer previously trained on some external text corpus
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用一个已经在某些外部文本语料库上训练好的现成层
- en: Training a new embedding layer as part of your neural network
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新的嵌入层作为神经网络的一部分进行训练
- en: If trained jointly with a neural network, the input to an embedding layer is
    an index-based encoded sequence. The number of output units in the embedding layer
    defines the dimension of the embedding space. The weights of the embedding layer,
    which are used to calculate the embedding representation of each index, and therefore
    of each word, are learned during the training of the network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与神经网络联合训练，嵌入层的输入是一个基于索引的编码序列。嵌入层中的输出单元数量定义了嵌入空间的维度。嵌入层的权重用于计算每个索引的嵌入表示，因此也是每个单词的嵌入表示，这些权重在网络训练过程中学习得到。
- en: Now that we are familiar with different text encoding techniques, let's move
    on to our first NLP use case.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了不同的文本编码技术，接下来让我们进入第一个自然语言处理应用案例。
- en: Finding the Tone of Your Customers' Voice – Sentiment Analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找客户声音的语气 – 情感分析
- en: A common use case for NLP is **sentiment analysis**. Here, the goal is to identify
    the underlying emotion in some text, whether positive or negative, and all the
    nuances in between. Sentiment analysis is implemented in many fields, such as
    to analyze incoming messages, emails, reviews, recorded conversations, and other
    similar texts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的一个常见用例是 **情感分析**。在这里，目标是识别文本中的潜在情感，无论是正面、负面，还是介于两者之间的各种细微差别。情感分析已被应用于许多领域，如分析来电信息、电子邮件、评论、录音对话及其他类似文本。
- en: Generally, sentiment analysis belongs to a bigger group of NLP applications
    known as text classification. In the case of sentiment analysis, the goal is to
    predict the sentiment class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，情感分析属于更大范围的自然语言处理（NLP）应用组，被称为文本分类。在情感分析的情况下，目标是预测情感类别。
- en: Another common example of text classification is language detection. Here, the
    goal is to recognize the text language. In both cases, if we use an RNN for the
    task, we need to adopt a *many-to-one architecture*. A many-to-one neural architecture
    accepts a sequence of inputs at different times, ![](img/Formula_B16391_07_001.png),
    and uses the final state of the output unit to predict the one single class –
    that is, sentiment or language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的文本分类示例是语言检测。在这种情况下，目标是识别文本的语言。在这两种情况下，如果我们使用 RNN 来处理任务，我们需要采用 *多对一架构*。多对一神经网络架构接受不同时间的输入序列，![](img/Formula_B16391_07_001.png)，并利用输出单元的最终状态来预测单一类别——即情感或语言。
- en: '*Figure 7.3* shows an example of a many-to-one architecture:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3* 显示了一个多对一架构的示例：'
- en: '![Figure 7.3  –  An example of a many-to-one neural architecture: a sequence
    of many inputs at different times and only the final status of the output](img/B16391_07_003.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 一个多对一神经网络架构的示例：一系列在不同时间的输入，仅使用输出的最终状态](img/B16391_07_003.jpg)'
- en: 'Figure 7.3 – An example of a many-to-one neural architecture: a sequence of
    many inputs at different times and only the final status of the output'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 一个多对一神经网络架构的示例：一系列在不同时间的输入，仅使用输出的最终状态
- en: In our first use case in this chapter, we want to analyze the sentiment of movie
    reviews. The goal is to train an RNN at a word level, with an embedding layer
    and an LSTM layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个用例中，我们想要分析电影评论的情感。目标是训练一个基于词级别的 RNN，并结合嵌入层和 LSTM 层。
- en: 'For this example, we will use the IMDb dataset, which contains two columns:
    the text of the movie reviews and the sentiment. The sentiment is encoded as `1`
    for positive reviews and as `0` for negative reviews.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 IMDb 数据集，该数据集包含两列：电影评论的文本和情感标签。情感标签用 `1` 表示正面评论，用 `0` 表示负面评论。
- en: '*Figure 7.4* shows you a small subset with some positive and some negative
    movie reviews:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.4* 显示了一个小子集，其中包含一些正面和一些负面的电影评论：'
- en: '![Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews](img/B16391_07_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – IMDb 数据集的摘录，显示了标记为正面和负面的评论](img/B16391_07_004.jpg)'
- en: Figure 7.4 – Extract of the IMDb dataset, showing positive- and negative-labeled
    reviews
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – IMDb 数据集的摘录，显示了标记为正面和负面的评论
- en: Let's start with reading and encoding the texts of the movie reviews.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取和编码电影评论文本开始。
- en: Preprocessing Movie Reviews
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电影评论的预处理
- en: The embedding layer expects index-based encoded input sequences. That is, each
    review must be encoded as a sequence of indexes, where each index (an integer
    value) represents a word in the dictionary.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层期望基于索引的编码输入序列。也就是说，每条评论必须编码为一个索引序列，其中每个索引（整数值）代表字典中的一个词。
- en: As the number of words available in the IMDb document corpus is very high, we
    decided to reduce them during the text preprocessing phase, by removing stop words
    and reducing all words to their stems. In addition, only the ![](img/Formula_B16391_07_002.png)
    most frequent terms in the training set are encoded with a dedicated index, while
    all others receive just the default index.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 IMDb 文档语料库中的可用词汇量非常大，我们决定在文本预处理阶段对其进行简化，方法是去除停用词并将所有词语还原为词干。此外，仅将训练集中的最频繁术语用专用索引进行编码，而其他所有术语则使用默认索引。
- en: In theory, RNNs can handle sequences of variable length. In practice, though,
    the sequence length for all input samples in one training batch must be the same.
    As the number of words per review might differ, we define a fixed sequence length
    and we zero-pad too-short sequences – that is, we add 0s to complete the sequence
    – and we truncate too-long sequences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNN 可以处理可变长度的序列。然而，在实践中，一个训练批次中的所有输入样本的序列长度必须相同。由于每个评论中的词汇数量可能不同，我们定义了固定的序列长度，并对过短的序列进行零填充
    —— 即，我们添加 0 来补充序列 —— 并截断过长的序列。
- en: All these preprocessing steps are applied to the training set and the test set,
    with one difference. In the preprocessing of the training set, the dictionary
    with the ![](img/Formula_B16391_06_021.png) most frequent terms is created. This
    dictionary is then only applied during the preprocessing of the test set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预处理步骤都应用于训练集和测试集，唯一的区别在于：在训练集的预处理中，会创建一个包含最频繁词汇的字典。这个字典仅在测试集的预处理过程中使用。
- en: 'In summary, we perform the following preprocessing steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们执行以下预处理步骤：
- en: Read and partition the dataset into training and test sets.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取并将数据集划分为训练集和测试集。
- en: Tokenize, clean, and stem the movie reviews in the training set and the test
    set.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集中的电影评论进行分词、清理和词干提取。
- en: Create a dictionary of all the terms. The ![](img/Formula_B16391_03_029.png)
    most frequent terms in the training set are represented by dedicated indexes and
    all other terms by a default index.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所有术语的字典。训练集中最频繁的术语由专用索引表示，所有其他术语由默认索引表示。
- en: Map the words in the training and test set to the corresponding dictionary indexes.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集和测试集中的词汇映射到相应的字典索引。
- en: Truncate too-long word sequences in the training set and test set.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截断训练集和测试集中过长的词序列。
- en: Zero-pad too-short sequences in the training set and test set.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和测试集中的过短序列进行零填充。
- en: 'The workflow in *Figure 7.5* performs all these steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.5* 中的工作流执行了所有这些步骤：'
- en: '![Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study](img/B16391_07_005.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 情感分析案例研究的预处理工作流片段](img/B16391_07_005.jpg)'
- en: Figure 7.5 – Preprocessing workflow snippet for the sentiment analysis case
    study
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 情感分析案例研究的预处理工作流片段
- en: The first metanode, **Read and partition data**, reads the table with the movie
    reviews and sentiment information and partitions the dataset into a training set
    and a test set. The **Preprocessing training set** metanode performs the different
    preprocessing steps on the training set and creates and applies the dictionary,
    which is available at the second output port. The last metanode, **Preprocess
    test set**, applies the created dictionary to the test set and performs the different
    preprocessing steps on the test set.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元节点**读取并划分数据**读取包含电影评论和情感信息的表格，并将数据集划分为训练集和测试集。**预处理训练集**元节点对训练集执行不同的预处理步骤，创建并应用字典，该字典会通过第二个输出端口提供。最后一个元节点**预处理测试集**将创建的字典应用于测试集，并对测试集执行不同的预处理步骤。
- en: Let's see how all these steps are implemented in KNIME Analytics Platform.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些步骤是如何在 KNIME Analytics Platform 中实现的。
- en: Reading and Partitioning the Dataset
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取和划分数据集
- en: The first part, reading and partitioning the dataset, is performed by the **Read
    and partition data** metanode.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，读取和划分数据集，由**读取并划分数据**元节点执行。
- en: '*Figure 7.6* shows you the workflow snippet inside the metanode:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.6* 显示了元节点内部的工作流片段：'
- en: '![Figure 7.6 – Workflow snippet inside the Read and partition metanode](img/B16391_07_006.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 读取并划分元节点内部的工作流片段](img/B16391_07_006.jpg)'
- en: Figure 7.6 – Workflow snippet inside the Read and partition metanode
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 读取并划分元节点内部的工作流片段
- en: The **Table Reader** node reads the table with the sentiment information as
    an integer value and the movie reviews as a string value. Next, the sentiment
    information is transformed into a string with the **Number To String** node. This
    step is necessary to allow stratified sampling in the **Partitioning** node. In
    the last step, the data type of the column sentiment is transformed back into
    an integer using the **String To Number** node so that it can be used as the target
    column during training by the Keras Learner node.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格读取器**节点读取包含情感信息的表格作为整数值，以及作为字符串值的电影评论。接下来，使用**数字转字符串**节点将情感信息转换为字符串。这一步骤是必要的，以便在**分区**节点中进行分层抽样。在最后一步，使用**字符串转数字**节点将情感列的数据类型转换回整数，以便在训练过程中作为目标列由
    Keras Learner 节点使用。'
- en: Now that we have a training set and a test set, let's continue with the preprocessing
    of the training set.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练集和测试集，让我们继续进行训练集的预处理。
- en: Preprocessing the Training Set and Dictionary Creation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练集预处理和字典创建
- en: The preprocessing of the training set and the creation of the dictionary is
    performed in the **Preprocess training set** metanode.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集的预处理和字典的创建在**预处理训练集**元节点中进行。
- en: '*Figure 7.7* shows you the inside of the metanode:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.7* 显示了元节点内部的内容：'
- en: '![Figure 7.7 – Workflow snippet inside the Preprocess training set metanode](img/B16391_07_007.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 预处理训练集元节点中的工作流片段](img/B16391_07_007.jpg)'
- en: Figure 7.7 – Workflow snippet inside the Preprocess training set metanode
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 预处理训练集元节点中的工作流片段
- en: For the preprocessing of the movie reviews, the **KNIME Text Processing** extension
    is used.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电影评论的预处理，使用了**KNIME 文本处理**扩展。
- en: Tip
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The KNIME Text Processing extension includes nodes to read and write documents
    from and to a variety of text formats; to transform words; to clean up sentences
    of spurious characters and meaningless words; to transform a text into a numeric
    table; to calculate all required text statistics; and finally, to explore topics
    and sentiment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 文本处理扩展包含读取和写入多种文本格式文档的节点；转换单词；清理句子中的无关字符和无意义单词；将文本转换为数字表格；计算所有所需的文本统计信息；最后，探索主题和情感。
- en: 'The KNIME Text Processing extension relies on a new data type: **Document object**.
    Raw text becomes a document when additional metadata, such as title, author(s),
    source, and class, are added to it. Text in a document is tokenized following
    one of the many available language-specific tokenization algorithms. **Document
    tokenization** produces a hierarchical structure of the text items: sections,
    paragraphs, sentences, and words. Words are often referred to as tokens or terms.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME 文本处理扩展依赖于一种新的数据类型：**文档对象**。原始文本通过添加附加的元数据，如标题、作者、来源和类别，变成了文档。文档中的文本按照多种可用的语言特定分词算法进行分词。**文档分词**生成了文本项的层次结构：章节、段落、句子和单词。单词通常被称为令牌或术语。
- en: To make use of the preprocessing nodes of the KNIME Text Processing extension,
    we need to transform the movie reviews into documents, via the **Strings To Document**
    node. This node collects values from different columns and turns them into a document
    object, after tokenizing the main text.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 KNIME 文本处理扩展的预处理节点，我们需要通过**Strings To Document**节点将电影评论转换为文档。该节点从不同的列收集值，并在对主文本进行分词后将其转换为文档对象。
- en: '*Figure 7.8* shows you the configuration window of the **Strings To Document**
    node:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.8* 显示了 **Strings To Document** 节点的配置窗口：'
- en: '![Figure 7.8 – Configuration window of the Strings To Document node](img/B16391_07_008.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – Strings To Document 节点的配置窗口](img/B16391_07_008.jpg)'
- en: Figure 7.8 – Configuration window of the Strings To Document node
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – Strings To Document 节点的配置窗口
- en: 'The node gives you the opportunity to define the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点为您提供了定义以下内容的机会：
- en: The document text via the **Full text** option.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**全文**选项获取文档文本。
- en: The document title, as a **Column**, **Row ID**, or **Empty string** value.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档标题，作为**列**、**行 ID**或**空字符串**值。
- en: The document source, document category, document authors, and document publication
    date as a fixed string or a column value. If column values are used, remember
    to enable the corresponding flag. Often, the **Document category** field is used
    to store the task class.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为固定字符串或列值的文档来源、文档类别、文档作者和文档发布日期。如果使用列值，请记得启用相应的标志。通常，**文档类别**字段用于存储任务类别。
- en: The document type, as **Transaction**, **Proceeding**, **Book**, or just **UNKNOWN**.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档类型，如**事务**、**会议记录**、**书籍**，或只是**未知**。
- en: The name of the output document column.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出文档列的名称。
- en: The maximum number of parallel processes to execute the word tokenizer.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行词法分析器的最大并行进程数。
- en: The word tokenizer algorithm.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词法分析器算法。
- en: 'Next, the document objects are cleaned through a sequence of text preprocessing
    nodes, contained in the **Text Preprocessing** component of the workflow in *Figure
    7.7*. The inside of the **Text Preprocessing** component is shown in *Figure 7.9*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，文档对象通过一系列文本预处理节点进行清理，这些节点包含在工作流的**文本预处理**组件中，见*图 7.7*。**文本预处理**组件的内部结构如*图
    7.9*所示：
- en: '![Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
    ](img/B16391_07_009.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 显示预处理组件内部的工作流片段](img/B16391_07_009.jpg)'
- en: Figure 7.9 – Workflow snippet showing the inside of the Preprocessing component
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 显示预处理组件内部的工作流片段
- en: The workflow snippet starts with the **Punctuation Erasure** node, to strip
    all punctuation from the input documents.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流片段从**标点符号删除**节点开始，用于去除输入文档中的所有标点符号。
- en: The **Number Filter** node filters out all numbers, expressed as digits, including
    decimal separators (**,** or **.**) and possible leading signs (**+** or **-**).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**数字过滤器**节点过滤掉所有数字，包含数字形式的数字、十进制分隔符（**,** 或 **.**）和可能的前导符号（**+** 或 **-**）。'
- en: The **N Chars Filter** node filters out all terms with less than ![](img/Formula_B16391_07_005.png)
    – in our case, ![](img/Formula_B16391_07_006.png) – characters, as specified in
    the configuration window of the node.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**字符数过滤器**节点过滤掉所有少于 ![](img/Formula_B16391_07_005.png) – 在我们的例子中，少于 ![](img/Formula_B16391_07_006.png)
    – 字符的术语，正如在节点的配置窗口中指定的那样。'
- en: Filler words, such as *so*, *thus*, and so on, are called **stop words**. They
    carry little information and can be removed with the **Stop Word Filter** node.
    This node filters out all terms that are contained in the selected stop word list.
    A custom stop word list can be passed to the node via the second input port, or
    a default built-in stop word list can be adopted. A number of built-in stop word
    lists are available for various languages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 填充词，如*so*、*thus* 等，被称为**停用词**。它们承载的信息较少，可以通过**停用词过滤器**节点去除。该节点过滤掉所有在选定停用词列表中的术语。可以通过第二输入端口传递自定义停用词列表，或采用默认的内置停用词列表。为不同语言提供了多个内置停用词列表。
- en: The **Case Converter** node converts all terms into upper or lowercase. In this
    case study, they are converted into lowercase.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**大小写转换器**节点将所有术语转换为大写或小写。在这个案例中，它们被转换为小写。'
- en: Lastly, the **Snowball Stemmer** node reduces words to their stem, removing
    the grammar inflection, using the Snowball stemming library ([http://snowball.tartarus.org/](http://snowball.tartarus.org/)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**Snowball 词干提取器**节点将单词简化为词干，去除语法屈折，使用 Snowball 词干提取库 ([http://snowball.tartarus.org/](http://snowball.tartarus.org/))。
- en: Important note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The goal of stemming is to reduce inflectional forms and derivationally related
    forms to a common base form. For example, *look*, *looking*, *looks*, and *looked*
    are all replaced by their stem, *look*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取的目标是将屈折形式和派生相关形式归约为共同的基础形式。例如，*look*、*looking*、*looks* 和 *looked* 都被替换为它们的词干
    *look*。
- en: Now that we have cleaned up the text of the movie reviews of the training set,
    we can create the dictionary.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理了训练集中的电影评论文本，可以创建词典了。
- en: Creating the Dictionary Based on the Training Set
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于训练集创建词典
- en: 'The dictionary must assign two indexes to each word:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 词典必须为每个单词分配两个索引：
- en: '**Index**: A progressive integer index to each of the ![](img/Formula_B16391_07_007.png)
    most frequent terms in the training set and the same default index to all other
    terms.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：为训练集中最常见的术语分配递增的整数索引，并为所有其他术语分配相同的默认索引。  '
- en: '**Counter**: A progressive eight-digit index to each of the words. This eight-digit
    index is just a temporary index that will help us deal with truncation.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数器**：为每个单词分配一个递增的八位数索引。这个八位数索引只是一个临时索引，帮助我们处理截断问题。'
- en: '*Figure 7.10* shows you a subset of the dictionary we want to create:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.10* 显示了我们想要创建的词典的子集：'
- en: '![Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index](img/B16391_07_010.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 词典的小子集，每个单词通过递增的整数索引和另一个递增的八位数整数索引来表示](img/B16391_07_010.jpg)'
- en: Figure 7.10 – A small subset of the dictionary, where each word is represented
    by a progressive integer index and another progressive eight-digit integer index
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 字典的小子集，其中每个单词由递增的整数索引和另一个递增的八位数字索引表示
- en: 'Both indexes are created in the **Create Dictionary** component and *Figure
    7.11* shows you the workflow snippet inside the component:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 两个索引都在**创建字典**组件中创建，*图 7.11*向您展示了组件内部的工作流片段：
- en: '![Figure 7.11 – Workflow snippet contained in the Create Dictionary component](img/B16391_07_011.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 创建字典组件中的工作流片段](img/B16391_07_011.jpg)'
- en: Figure 7.11 – Workflow snippet contained in the Create Dictionary component
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 创建字典组件中的工作流片段
- en: 'The **Create Dictionary** component has a configuration window, which you can
    see in *Figure 7.12*. The input option in the configuration window is inherited
    from the **Integer Configuration** node and requests the dictionary size as the
    number of the ![](img/Formula_B16391_03_173.png) most frequent words in the document
    collection. The default is ![](img/Formula_B16391_07_009.png):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建字典**组件有一个配置窗口，您可以在*图 7.12*中看到。配置窗口中的输入选项继承自**整数配置**节点，并要求字典大小为文档集合中出现最频繁的！[](img/Formula_B16391_03_173.png)个单词。默认值为！[](img/Formula_B16391_07_009.png)：'
- en: '![Figure 7.12 – Configuration window of the Create Dictionary component](img/B16391_07_012.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 创建字典组件的配置窗口](img/B16391_07_012.jpg)'
- en: Figure 7.12 – Configuration window of the Create Dictionary component
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 创建字典组件的配置窗口
- en: 'The workflow inside the component first creates a global set of unique terms
    over all the documents by using the **Unique Term Extractor** node:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 组件内的工作流首先通过使用**独特术语提取器**节点在所有文档中创建一个独特术语的全局集合：
- en: '![Figure 7.13 – Configuration window of the Unique Term Extractor node](img/B16391_07_013.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 独特术语提取器节点的配置窗口](img/B16391_07_013.jpg)'
- en: Figure 7.13 – Configuration window of the Unique Term Extractor node
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 独特术语提取器节点的配置窗口
- en: This node allows us to create an index column and a frequency column, as shown
    in the preceding screenshot. The index column contains a progressive integer number
    starting from `1`, where `1` is assigned to the most frequent term.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点允许我们创建索引列和频率列，如上面的截图所示。索引列包含从`1`开始的递增整数，其中`1`分配给最频繁的术语。
- en: 'The node optionally provides the possibility to filter the top *k* most frequent
    terms. For that, three frequency measures are available: the **term frequency**,
    the **document frequency**, and the **inverse document frequency**. For now, we
    want to select all terms and we will work on the dictionary size later.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点可选择性地提供过滤最*常见的 k 个术语*的功能。为此，有三种频率度量可用：**词频**、**文档频率**和**逆文档频率**。现在，我们希望选择所有术语，字典大小稍后再处理。
- en: Important note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Term frequency** (**TF**): The number of occurrences of a term in all documents'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频**（**TF**）：术语在所有文档中出现的次数'
- en: '**Document frequency** (**DF**): The number of documents in which a term occurs'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**文档频率**（**DF**）：术语出现的文档数量'
- en: '**Inverse document frequency** (**IDF**): Logarithm of the number of documents
    divided by DF'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）：文档总数与文档频率（DF）之比的对数'
- en: The eight-digit index is created via the `1` as the step size. This minimum
    value guarantees the eight-digit format.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 八位数字索引是通过`1`作为步长创建的。这个最小值确保了八位数字格式。
- en: The **Index** and **Counter** columns are then converted from integers into
    strings with the **Number To String** node.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**索引**和**计数器**列将通过**数字转字符串**节点从整数转换为字符串。
- en: 'Next comes the reduction of the dictionary size. The top ![](img/Formula_B16391_03_173.png)
    most frequent terms keep the progressive index assigned by the **Unique Term Extractor**
    node, while all other terms get a default index of ![](img/Formula_B16391_07_011.png).
    Remember that ![](img/Formula_B16391_03_252.png) can be changed via the component''s
    configuration window. For this example, ![](img/Formula_B16391_07_013.png) was
    set to 20,000\. In the lower part of the component sub-workflow, the **Row Splitter**
    node splits the input data table into two sub-tables: the top ![](img/Formula_B16391_07_007.png)
    rows (top output port) and the rest of the rows (lower output port).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是字典大小的缩减。最上方的 ![](img/Formula_B16391_03_173.png) 最常见术语保留由**唯一术语提取器**节点分配的逐步索引，而所有其他术语都获得一个默认索引
    ![](img/Formula_B16391_07_011.png)。请记住，![](img/Formula_B16391_03_252.png) 可以通过组件的配置窗口进行更改。在这个例子中，![](img/Formula_B16391_07_013.png)
    设置为 20,000。在组件子工作流的下部分，**行分割器**节点将输入数据表拆分为两个子表：最上方的 ![](img/Formula_B16391_07_007.png)
    行（顶部输出端口）和其余行（底部输出端口）。
- en: The **Constant Value Column** node then replaces all index values with the default
    index value ![](img/Formula_B16391_07_015.png) in the lower sub-table. Lastly,
    the two sub-tables are concatenated back together.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**常量值列**节点将下方子表中的所有索引值替换为默认索引值 ![](img/Formula_B16391_07_015.png)。最后，两个子表被重新连接在一起。
- en: Now that the dictionary is ready, we can continue with the truncation of the
    movie reviews.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在字典已经准备好，我们可以继续进行电影评论的截断。
- en: Truncating Too-Long Documents
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 截断过长的文档
- en: 'We have stated that we will work with fixed-size documents – that is, with
    a maximum number of words for each document. If a document has more words than
    allowed, it will be truncated. If it has fewer words than allowed, it will be
    zero-padded. Let''s see how the **truncation** procedure works – that is, how
    we remove the last words from a too-long document. This all happens in the **Truncation**
    component. *Figure 7.14* shows you the workflow snippet inside the component:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说明我们将处理固定大小的文档——即每个文档的最大单词数。如果文档的单词数超过允许的数量，它将被截断。如果文档的单词数少于允许的数量，它将被零填充。现在让我们看看**截断**过程是如何工作的——即如何从过长的文档中删除最后的单词。这一切都发生在**截断**组件中。*图
    7.14*展示了组件内部的工作流片段：
- en: '![Figure 7.14 – Workflow snippet inside the Truncation component](img/B16391_07_014.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 截断组件中的工作流片段](img/B16391_07_014.jpg)'
- en: Figure 7.14 – Workflow snippet inside the Truncation component
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 截断组件中的工作流片段
- en: First, we set the maximum number, ![](img/Formula_B16391_07_016.png), of terms
    allowed in a document. Again, this is a parameter that can be changed through
    the component's configuration window, shaped via the **Integer Configuration**
    node. We set the maximum number of terms in a document – that is, the maximum
    document size – as ![](img/Formula_B16391_07_017.png) terms. If a document is
    too long, we should just keep the first ![](img/Formula_B16391_07_018.png) terms
    and throw away the rest.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置文档中允许的最大单词数 ![](img/Formula_B16391_07_016.png)。同样，这个参数可以通过组件的配置窗口进行更改，形状由**整数配置**节点定义。我们设置文档中的最大单词数——即最大文档大小——为
    ![](img/Formula_B16391_07_017.png) 个单词。如果文档太长，我们应该只保留前 ![](img/Formula_B16391_07_018.png)
    个单词，并丢弃其余部分。
- en: It is not easy to count the number of words in a text. Since words have variable
    lengths, we should detect the spaces separating the words within a loop and then
    count the words. Loops, however, often slow down execution. So, an alternative
    trick is to use the eight-digit representation of the words inside the text.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 计算文本中的单词数并不容易。由于单词的长度可变，我们应该在循环中检测分隔单词的空格，然后计算单词数。然而，循环往往会减慢执行速度。因此，一个替代的技巧是使用文本中单词的八位数字表示。
- en: Within the text, each word is substituted by its eight-digit code via the **Dictionary
    Replacer** node. The **Dictionary Replacer** node matches terms in the input documents
    at the top input port with dictionary terms at the lower input port and then replaces
    them with the corresponding value in the dictionary table.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中，每个单词通过**字典替换器**节点用其八位数字代码进行替换。**字典替换器**节点将输入文档顶部输入端口中的术语与下方输入端口中的字典术语进行匹配，然后用字典表中对应的值进行替换。
- en: 'The **Dictionary Replacer** node has two input ports:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**字典替换器**节点有两个输入端口：'
- en: The upper input port for the documents containing the terms to be replaced
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上输入端口用于包含要替换术语的文档
- en: The lower input port with the dictionary table for the matching and replacement
    operation
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下输入端口与用于匹配和替换操作的字典表
- en: Important note
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The dictionary table must consist of at least two string columns. One string
    column contains the terms to replace (keys) and the other string column contains
    the replacement strings (values). In the configuration window, we can set both
    columns from the data table at the lower input port.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典表必须至少包含两列字符串。一列字符串列包含要替换的术语（键），另一列字符串列包含替换的字符串（值）。在配置窗口中，我们可以从数据表的下部输入端口设置这两列。
- en: At this point, we have text with terms of fixed length (`8 digits + 1 <space>`)
    and not words of variable length. So, limiting a text to ![](img/Formula_B16391_07_019.png)
    words is the same as limiting a text to ![](img/Formula_B16391_07_020.png) characters,
    if ![](img/Formula_B16391_07_021.png), to 720 characters. This operation is much
    easier to carry out without loops or complex node structures, but just with a
    **String Manipulation** node. However, the **String Manipulation** node works
    on string objects and not on documents. To use it, we need to move temporarily
    back to text as strings.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们得到的是具有固定长度（`8 位数字 + 1 <space>`）的文本，而不是可变长度的单词。因此，将文本限制为 ![](img/Formula_B16391_07_019.png)
    个单词就等于将文本限制为 ![](img/Formula_B16391_07_020.png) 个字符，如果 ![](img/Formula_B16391_07_021.png)，则为
    720 个字符。这个操作可以更容易地进行，无需使用循环或复杂的节点结构，只需使用 **字符串操作** 节点即可。然而，**字符串操作** 节点只对字符串对象进行操作，而不是对文档进行操作。为了使用它，我们需要暂时将文本返回为字符串。
- en: The text is extracted from the document as a simple string with the **Document
    Data Extractor** node. This node extracts information, such as, for example, the
    text and title, from a document cell.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 文本通过 **文档数据提取器** 节点从文档中提取为简单的字符串。该节点提取文档单元格中的信息，例如文本和标题。
- en: The **Math Formula (Variable)** node takes the flow variable for the maximum
    document size and calculates the maximum number of characters allowed in a document.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学公式（变量）** 节点获取流变量，以确定文档的最大大小，并计算文档中允许的最大字符数。'
- en: The `0`) until the maximum number of characters allowed, using the `substr()`
    function. This effectively keeps only the top ![](img/Formula_B16391_03_031.png)
    terms and removes all others.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `substr()` 函数，直到达到允许的最大字符数，`0`)。这实际上仅保留前 ![](img/Formula_B16391_03_031.png)
    个单词并移除其他所有单词。
- en: Lastly, the text is transformed back into a document, called **Truncated Document**,
    and all superfluous columns are removed in the **Column Filter** node.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，文本被转换回一个称为 **截断文档** 的文档，所有多余的列将在 **列过滤器** 节点中被移除。
- en: At this point, the eight-digit indexes have exhausted their task and can be
    substituted with the progressive integer index for the encoding. This is done
    in the **Dictionary Replacer** node, once again.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，八位数字索引已完成其任务，可以被逐渐递增的整数索引替代，用于编码。这在 **字典替换器** 节点中再次完成。
- en: With that, we have truncated too-long documents to the maximum number of terms
    allowed. Next, we need to zero-pad too-short documents.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就将过长的文档截断至允许的最大单词数。接下来，我们需要对过短的文档进行零填充。
- en: Zero-Padding Too-Short Documents
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零填充过短的文档
- en: When sequences are too short with respect to a set number of values, **zero-padding**
    is often applied. Zero-padding means that 0s are added to the sequence until the
    set number of values is reached. In our case, if a document has fewer words than
    the set number, we fill the remaining empty spaces with 0s. This happens in the
    **Zero Pad** component.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当序列相对于设置的值数量过短时，通常会应用 **零填充**。零填充意味着向序列中添加 0，直到达到设定的值数量。在我们的例子中，如果文档的单词数少于设定数量，我们将用
    0 填充剩余的空位。这发生在 **Zero Pad** 组件中。
- en: '*Figure 7.15* shows you the workflow snippet inside the Zero Pad component:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.15* 展示了 Zero Pad 组件中的工作流片段：'
- en: '![Figure 7.15 – Workflow snippet inside the Zero Pad component](img/B16391_07_015.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – Zero Pad 组件中的工作流片段](img/B16391_07_015.jpg)'
- en: Figure 7.15 – Workflow snippet inside the Zero Pad component
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – Zero Pad 组件中的工作流片段
- en: Zero-padding is again performed at the string level, and not at the document
    level. After the text has been extracted as a string from the input document using
    the `<space>` and creates one new column for each index.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Zero-padding 再次是在字符串级别进行，而不是在文档级别进行。文本通过 `<space>` 从输入文档中提取为字符串后，并为每个索引创建一个新的列。
- en: Remember that all truncated text now has a maximum length of ![](img/Formula_B16391_03_255.png)
    indexes from the previous step. So, from those texts, the number of newly generated
    columns is surely ![](img/Formula_B16391_03_255.png). For all other texts with
    shorter-term sequences, the **Cell Splitter** node will fill the empty columns
    with missing values. It is enough to turn these missing values into 0s and the
    zero-padding procedure is complete. This replacement of missing values with 0s
    is performed by the **Missing Value** node.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，所有被截断的文本现在的最大长度是来自前一步骤的 ![](img/Formula_B16391_03_255.png) 索引。因此，从这些文本中，新生成的列数肯定是
    ![](img/Formula_B16391_03_255.png)。对于所有其他文本（具有较短术语序列的文本），**单元拆分器**节点将用缺失值填充空列。只需要将这些缺失值替换为
    0，零填充过程就完成了。这种用 0 替换缺失值的操作是由**缺失值**节点完成的。
- en: Lastly, all superfluous columns are removed within the **Column Filter** node.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有多余的列在**列过滤器**节点中被移除。
- en: Now that all term sequences – that is, all text – have the same length, collection
    cells are created with the **Create Collection Cell** node to feed the Keras Learner
    node.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有的术语序列——即所有文本——都有相同的长度，使用**创建集合单元**节点来创建集合单元，以便将其输入到 Keras 学习节点中。
- en: Next, we need to perform the same preprocessing on the test and apply the created
    dictionary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对测试集进行相同的预处理，并应用已创建的字典。
- en: Preprocessing the Test Set
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试集预处理
- en: 'The preprocessing of the test set is performed in the **Preprocess test set**
    metanode. This metanode has two input ports: the upper port for the dictionary
    created in the **Preprocess training set** metanode and the lower port for the
    test set.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的预处理是在**预处理测试集**元节点中执行的。此元节点有两个输入端口：上端口用于从**预处理训练集**元节点创建的字典，下端口用于测试集。
- en: '*Figure 7.16* shows you the workflow snippet inside the Preprocess test set
    metanode:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.16* 展示了预处理测试集元节点内的工作流片段：'
- en: '![Figure 7.16 – Workflow snippet inside the Preprocess test set metanode](img/B16391_07_016.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 预处理测试集元节点内的工作流片段](img/B16391_07_016.jpg)'
- en: Figure 7.16 – Workflow snippet inside the Preprocess test set metanode
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 预处理测试集元节点内的工作流片段
- en: The lower part of the workflow is similar to the workflow snippet inside the
    **Preprocess training set** metanode, only the part including the creation of
    the dictionary is different. Here, the dictionary for the test set is based on
    the dictionary from the training set. All terms available in the training set
    dictionary receive the corresponding index encoding; all remaining terms receive
    the default index.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流的下半部分与**预处理训练集**元节点内的工作流片段类似，只不过包括字典创建的部分不同。在这里，测试集的字典是基于训练集的字典创建的。所有在训练集字典中可用的术语都将获得相应的索引编码；所有其余的术语将获得默认索引。
- en: Therefore, first a list of all terms in the test set is created using the **Unique
    Term Extractor** node. Next, this list is joined with the list of terms in the
    training set dictionary using a right outer join. A right outer join allows us
    to keep all the rows from the lower input port – that is, all terms in the test
    set – and to add the indexes from the training dictionary, if available. For all
    terms that are not in the training dictionary, the joiner node creates missing
    values in the index columns. These missing values are then replaced with the default
    index value using the **Missing Value** node.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先使用**唯一术语提取器**节点创建测试集中的所有术语的列表。接下来，使用右外连接将该列表与训练集字典中的术语列表连接。右外连接允许我们保留来自下输入端口的所有行——即测试集中的所有术语——并添加训练字典中的索引（如果有的话）。对于所有不在训练字典中的术语，连接节点会在索引列中创建缺失值。这些缺失值随后会通过**缺失值**节点被替换为默认的索引值。
- en: All other steps, such as truncation and zero-padding, are performed in the same
    way as in the preprocessing of the training set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他步骤，如截断和零填充，都与训练集的预处理方式相同。
- en: We have finished the preprocessing phase and we can now continue with the definition
    of the network architecture and its training.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了预处理阶段，现在可以继续定义网络架构并进行训练。
- en: Defining and Training the Network Architecture
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义和训练网络架构
- en: In this section, we will define and train the network architecture for this
    sentiment classification task.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义并训练用于情感分类任务的网络架构。
- en: Network Architecture
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络架构
- en: 'We want to use an LSTM-based RNN, where we train the embedding as well. The
    embedding is trained by an embedding layer. Therefore, we create a neural network
    with four layers:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用基于 LSTM 的 RNN，其中我们也训练嵌入。嵌入由嵌入层进行训练。因此，我们创建一个具有四层的神经网络：
- en: An **input layer** to define the input size
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **输入层** 用于定义输入大小
- en: An **embedding layer** to produce an embedding representation of the term space
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **嵌入层** 用于生成术语空间的嵌入表示
- en: An **LSTM layer** to exploit the sequential property of the text
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **LSTM 层** 用于利用文本的序列特性
- en: A **dense layer** with one unit with the sigmoid activation function, as we
    have a binary classification problem at hand
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **全连接层**，具有一个单元，并使用 sigmoid 激活函数，因为我们面临的是一个二分类问题
- en: The embedding layer expects a sequence of index-based encoded terms as input.
    Therefore, the input layer must accept sequences of ![](img/Formula_B16391_03_031.png)
    integer indexes (in our case, ![](img/Formula_B16391_07_026.png)). This means
    `Shape = 80` and `data type = Int 32` in the configuration window of the **Keras
    Input Layer** node.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层期望以基于索引的编码术语序列作为输入。因此，输入层必须接受 ![](img/Formula_B16391_03_031.png) 整数索引序列（在我们的例子中，![](img/Formula_B16391_07_026.png)）。这意味着在
    **Keras 输入层** 节点的配置窗口中，`Shape = 80` 和 `data type = Int 32`。
- en: 'Next, the **Keras Embedding Layer** node must learn to embed the integer indexes
    into an appropriate high-dimensional vector space. *Figure 7.17* shows its configuration
    window. The input tensor is directly recovered from the output of the previous
    input layer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**Keras 嵌入层**节点必须学习将整数索引嵌入到合适的高维向量空间中。*图 7.17* 显示了它的配置窗口。输入张量直接从前一个输入层的输出中恢复：
- en: '![Figure 7.17 – Configuration window of the Keras Embedding Layer node](img/B16391_07_017.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – Keras 嵌入层节点的配置窗口](img/B16391_07_017.jpg)'
- en: Figure 7.17 – Configuration window of the Keras Embedding Layer node
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – Keras 嵌入层节点的配置窗口
- en: There are two important configuration settings for the `128`. The output tensor
    of the `[sequence length` ![](img/Formula_B16391_03_255.png)`, embedding dimension]`.
    In our case, this is `[80, 128]`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `128`，有两个重要的配置设置。输出张量为 `[序列长度` ![](img/Formula_B16391_03_255.png)`, 嵌入维度]`。在我们的例子中，这就是
    `[80, 128]`。
- en: Next, the `128` units, which means `Units = 128`, `Activation = Tanh`, `Recurrent
    activation = Hard sigmoid`, `Dropout = 0.2`, `Recurrent dropout = 0.2`, and return
    sequences, return state, go backward, and unroll all `unchecked`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`128` 单元，表示 `Units = 128`，`Activation = Tanh`，`Recurrent activation = Hard
    sigmoid`，`Dropout = 0.2`，`Recurrent dropout = 0.2`，并且返回序列，返回状态，反向传播，和展开所有 `unchecked`。
- en: Lastly, a **Keras Dense Layer** node with one unit with the sigmoid activation
    function is used to predict the final binary sentiment classification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用一个 **Keras 全连接层** 节点，具有一个单元和 sigmoid 激活函数，用于预测最终的二元情感分类。
- en: Now that we have our preprocessed data and the neural architecture, we can start
    training the network.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了预处理的数据和神经网络架构，可以开始训练网络。
- en: Training the Recurrent Network with Embeddings
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用嵌入训练递归网络
- en: The network is trained, as usual, with the **Keras Network Learner** node.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 网络按常规方式训练，通过 **Keras 网络学习器** 节点。
- en: In the first tab, **Input Data**, the **From Collection of Number (integer)**
    conversion is selected, as our input is a collection cell of integer values (the
    indexes), encoding our movie reviews. Next, the collection cell is selected as
    input.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个标签页，**输入数据**，选择了 **从数字集合（整数）** 转换，因为我们的输入是一个整数值集合单元（索引），它对我们的电影评论进行编码。接下来，选择集合单元作为输入。
- en: In the second tab, **Target Data**, the **From Number (integer)** conversion
    type and the column with the sentiment class are selected. In the lower part,
    the binary cross-entropy is selected as the loss function since it is a binary
    classification task.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个标签页，**目标数据**，选择了 **从数字（整数）** 转换类型和带有情感类别的列。在下方选择了二元交叉熵作为损失函数，因为这是一个二分类任务。
- en: In the third tab, `Epochs = 30`, `Training batch size = 100`, shuffle training
    data before each epoch is activated, and `Optimizer = Adam` (with the default
    settings).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个标签页中，`Epochs = 30`，`训练批次大小 = 100`，在每个 epoch 之前对训练数据进行洗牌，`优化器 = Adam`（使用默认设置）。
- en: Now that the network is trained, we can apply it to the test set and evaluate
    how good its performance is at predicting the sentiment behind a review text.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在网络已经训练完成，我们可以将其应用到测试集并评估其在预测评论文本情感方面的表现。
- en: Executing and Evaluating the Network on the Test Set
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上执行并评估网络
- en: To execute the network on the test set, the **Keras Network Executor** node
    is used.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window, we again select **From Collection of Number (integer)**
    as the conversion type and the collection cell as input.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: As output, we are interested in the output of the last dense layer, as this
    gives us the probability for sentiment being equal to `1` (positive). Therefore,
    we click on the **add output** button, select the sigmoid layer, and make sure
    that the **To Number (double)** conversion is used.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The `1`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the **Rule Engine** node translates this probability into a class prediction
    with the following expression:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `$dense_1/Sigmoid:0_0$` is the name of the output column from the network.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The expression transforms all values above `0.5` into 1s, and into 0s otherwise.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the different instruction lines in a **Rule Engine** node are
    executed sequentially. Execution stops when the antecedent in one line is verified.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, the **Scorer** node evaluates the performance of the model and the
    **Keras Network Writer** node saves the trained network for deployment. *Figure
    7.18* shows the network performance, in the view of the **Scorer** node, achieving
    a respectable 83% of correct sentiment classification on the movie reviews:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification](img/B16391_07_018.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Performance of the LSTM and embedding-based network on sentiment
    classification
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have finished our first NLP case study. *Figure 7.19* displays
    the complete workflow used to implement the example. You can download the workflow
    from the KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Complete workflow to prepare the text and build, train, and
    evaluate the neural network for sentiment analysis](img/B16391_07_019.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Complete workflow to prepare the text and build, train, and evaluate
    the neural network for sentiment analysis
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: For now, we offer no deployment workflow. In [*Chapter 10*](B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367),
    *Deploying a Deep Learning Network*, we will come back to this trained network
    to build a deployment workflow.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on to the next NLP application: free text generation with RNNs.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Generating Free Text with RNNs
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have seen how RNNs can be used for text classification, we can
    move on to the next case study. Here, we want to train an RNN to generate new
    free text in a certain style, be it Shakespearean English, a rap song, or mimicking
    a Brothers Grimm fairy tale. We will focus on the last application: training a
    network to generate free text in the style of Brothers Grimm fairy tales. However,
    the network and the process can be easily adjusted to produce a new rap song or
    a text in old Shakespearean English.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we train an RNN to generate new text?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you need a text corpus to train the network to generate new text.
    Any text corpus is good. However, keep in mind that the text you use for training
    will define the style of the text automatically generated. If you train the network
    on Shakespearean theater, you will get new text in old Shakespearean English;
    if you train the network on rap songs, you will get urban-style text, maybe even
    with rhyme; if you train the network on fairy tales, you will get text in the
    fairy tale style.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for a network to generate new fairy tales, it must be trained on existing
    fairy tales. We downloaded the Brothers Grimm corpus from the Gutenberg project,
    from [https://www.gutenberg.org/ebooks/2591](https://www.gutenberg.org/ebooks/2591).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Words or Characters?
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second decision to make is whether to train the network at the word or character
    level. Both options have their pros and cons.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Training a network at the word level sounds more logical since languages are
    structured by words and not by characters. Input sequences (sequences of words)
    are short but the dictionary size (all words in the domain) is large. On the other
    hand, training the network at a character level relies on much smaller and more
    manageable dictionaries, but might lead to very long input sequences. According
    to Wikipedia, the English language, for example, has around 170,000 different
    words and only 26 different letters. Even if we distinguish between uppercase
    and lowercase, and we add numbers, punctuation signs, and special characters,
    we have a dictionary with less than 100 characters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We want to train a network to generate text in the Brothers Grimm style. In
    order to do that, we train the network with a few Brothers Grimm tales, which
    already implies a very large number of words in the dictionary. So, to avoid the
    problem of a huge dictionary and the consequent possibly unmanageable network
    size, we opt to train our fairy tale generator at the character level.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Training at the character level means that the network must learn to predict
    the next character after the past ![](img/Formula_B16391_03_029.png) characters
    have passed through the input. The training set, then, must consist of many samples
    of sequences of ![](img/Formula_B16391_05_005.png) characters together with the
    next character to predict (the target value).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: During deployment, a start sequence of ![](img/Formula_B16391_05_005.png) characters
    must trigger the network to generate the new text. Indeed, this first sequence
    predicts the next character; then in the next step, the ![](img/Formula_B16391_07_031.png)
    most recent initial characters and the predicted character will make the new input
    sequence to predict the next character, and so on.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to clean, transform, and encode the
    text data from the Grimms' fairy tales to feed the network.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and Encoding
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We populate the training set using the sliding window approach – that is, with
    partially overlapping sequences. To make this clearer, let's include the sentence
    `Once upon a time` in the training set using a window length of ![](img/Formula_B16391_07_032.png)
    and a sliding step of `1`. The five characters `Once<space>` should predict `u`;
    then we slide the window one step to the right, and `nce<space>u` should predict
    `p`. Again, we slide the window one character to the right and `ce<space>up` should
    predict `o`, and so on.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left of *Figure 7.20*, you can see the created input sequences and the
    target values:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Example of overlapping sequences used for training](img/B16391_07_020.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Example of overlapping sequences used for training
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to encode the character sequences. In order to avoid introducing
    an artificial distance among characters, we opted for one-hot vector encoding.
    We will perform the one-hot encoding in two steps. First, we perform an index-based
    encoding; then we convert it into one-hot encoding in the **Keras Network Learner**
    node via the **From Collection of Number (integer)** conversion option to **One-Hot
    Tensor**. The resulting overlapping index-encoded sequences for the training set
    are shown on the right of *Figure 7.20*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow snippet in the next figure reads and transforms the fairy tales
    into overlapping index-based encoded character sequences and their associated
    target character. Both the input sequence and target character are stored in a
    collection-type column:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Preprocessing workflow snippet reading and transforming text
    from'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Brothers Grimm fairy tales](img/B16391_07_021.jpg)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Preprocessing workflow snippet reading and transforming text from
    Brothers Grimm fairy tales
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow performs the following steps:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Reads all the fairy tales from the corpus and extracts five fairy tales for
    training and `Snow white and Rose red` as the seed for deployment
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshapes the text, placing one character per row in a single column
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and applies the index-based dictionary, consisting, in this case, of
    the character set, including punctuation and special signs
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the **Lag Column** node, creates the overlapping sequences and then re-sorts
    them from the oldest to the newest character in the sequence
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encapsulates the input sequence and target character into collection-type columns
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's have a look at these steps in detail.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Extracting Fairy Tales
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The workflow snippet, in the **Read and Extract Fairy Tales** metanode, first
    reads the fairy tales using a **File Reader** node. The table has one column,
    where the content of each row corresponds to one line of a fairy tale.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Then, a **Row Filter** node removes the unnecessary meta-information at the
    top and the bottom of the file, such as the author, title, table of contents,
    and license agreement. We will not use any of this meta-information during training
    or deployment.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The `Snow white and Rose red` and at the top output port, all the other fairy
    tales. We'll save `Snow white and Rose red` for deployment.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Next, a **Row Filter** node is used to extract the first five fairy tales, which
    are used for training.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The next step is the reshaping of the text into a sequence of characters with
    one single column.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping the Text
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can create the overlapping sequences of characters to feed the network,
    we need to transform all the fairy tales text into a long sequence (column) of
    single characters: one character in each row. This step is called **reshaping**
    and it is implemented in the **Reshape Text** metanode. *Figure 7.22* shows its
    contents:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Workflow snippet inside the Reshape Text metanode](img/B16391_07_022.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Workflow snippet inside the Reshape Text metanode
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'It starts with two `<space>` character, by using the `regexReplace()` function.
    `regexReplace()` takes advantage of regular expressions, such as `"[^\\s]"` to
    match any character in the input string and `"$0 "` for the matched character
    plus `<space>`. The final syntax for the `regexReplace()` function, used within
    the `$Col0$`, is then the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, the `<space>` character, producing many columns with one character per
    cell.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the last character in the paragraph (the newline) has not received
    the `<space>` character afterward. To solve this problem, a constant column with
    a `<space>` character is added using the **Constant Value Column** node.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Unpivoting** node reshapes the data table from many columns into one
    column only with a sequence of single characters. Let''s spend a bit of time on
    the **Unpivoting** node and its unsuspected tricks for reshaping data tables.
    The **Unpivoting** node performs a disaggregation of the input data table. *Figure
    7.23* shows you an example. It distinguishes between value columns and retaining
    columns. The selected value columns are then rotated to become rows and attached
    to the corresponding values in the retaining columns. Since the rotation of the
    value columns might result in more than one row, a duplication of the rows with
    the retaining column values might be necessary:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns](img/B16391_07_023.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Example for the unpivoting operation, where Product 1 and Product
    2 are the selected Value columns, and ID and City are the selected retaining columns
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: For the reshaping of the text, we set all columns as value columns and none
    as retaining columns. The result is the representation of the fairy tale as a
    long sequence of characters within one column.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, some cleaning up: all rows with missing values are removed with the
    **Row Filter** node.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Applying the Dictionary
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now need to create the dictionary and the index-based mapping for the index-based
    encoding. Since we work at the character level, the dictionary here is nothing
    more than the character set – that is, the list of unique characters in the fairy
    tales corpus. To get this list, we remove all duplicate characters from the reshaped
    text using the **Remove Duplicate Filter** node.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The **Remove Duplicate Filter** node is a powerful node when it comes to detecting
    and handling duplicate records in the dataset.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Next, we assign an index to each row – that is, to each unique character – with
    the `0` for `1` for **Scale Unit**.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the dictionary ready, we apply it with the **Cell Replacer**
    node, already introduced in [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Building and Training a Feedforward Neural Network*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Resorting the Overlapping Sequences
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create the overlapping sequences of characters, we use the `100`, `1`, and
    incomplete rows at the beginning and end of the output table are skipped.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: According to the way the `col-100`) is in the farthest column to the right;
    the current character to predict (`col`) is in the farthest column to the left.
    Basically, the time of the sequence is sorted backward with respect to what the
    network is expecting.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows you an example:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Resulting output of the Lag Column node, where the time is
    sorted in ascending order from right to left](img/B16391_07_024.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Resulting output of the Lag Column node, where the time is sorted
    in ascending order from right to left
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: We need to reorder the columns to follow an ascending order from left to right,
    in order to have the oldest character on the left and the most recent character
    on the right. This re-sorting is performed by the **Resort Columns** metanode.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.25* shows you the inside of the metanode:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Workflow snippet contained in the Resort Columns metanode](img/B16391_07_025.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Workflow snippet contained in the Resort Columns metanode
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Here, the **Reference Column Resorter** node changes the order of the data columns
    in the table at the top input port according to the order established in the data
    table at the lower input port. The reference data table at the lower input port
    must contain a string-type column with the column headers from the first input
    table in a particular order. The columns in the first data table are then sorted
    according to the row order of the column names in the second data table.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: To create the table with sorted column headers, we extract the column headers
    with the **Extract Column Header** node. The **Extract Column Header** node separates
    the column headers from the table content and outputs the column headers at the
    top output port and the content at the lower output port.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Then, the row of column headers is transposed into a column with the **Transpose**
    node.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we assign an increasing integer number to each column header via the
    **Counter Generation** node and we sort them by counter value in descending order
    using the **Sorter** node.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the column headers from the first table sorted correctly in
    time, we can input it at the lower port of the **Reference Column Resorter** node.
    The result is a data table where each row is a sequence of ![](img/Formula_B16391_07_034.png)
    characters, time is sorted from left to right, and subsequent rows contain overlapping
    character sequences. At this point, we can create the collection cells for the
    input and target data of the network.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Even though the target data consists of only one single value, we still need
    to transform it into a collection cell so that the index can be transformed into
    a one-hot vector by the **Keras Network Learner** node.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to the next step: defining and training the network architecture.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Training the Network Architecture
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now design and train an appropriate neural network architecture to deal
    with time series, character encoding, and overfitting, and to predict the next
    character in the sequence.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Network Architecture
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this case study, we decided to use a neural network with four layers:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: A **Keras input layer**, to define the input shape
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras LSTM layer**, to deal with time series
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dropout layer**, to prevent overfitting
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dense layer**, to output the probability of the next character
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As usual, we define the input shape of the neural network using a `?, 65`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: As we don't need the intermediate hidden states, we leave most of the settings
    as default in the `512`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Free text generation can be seen as a multi-class classification application,
    where the characters are the classes. Therefore, the **Keras Dense Layer** node
    at the output of the network is set to have 65 units (one for each character in
    the character set) with the softmax activation function, to score the probability
    of each character to be the next character.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Let's proceed with training this network on the encoded overlapping sequences.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Training the Network
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, to train the network, we use the by-now-familiar **Keras Network Learner**
    node.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: In the first configuration tab, **Input Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** to handle encoding conversion and the
    collection column with the character sequence as input.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: In the second configuration tab, **Target Data**, we select **From Collection
    of Number (integer) to One-Hot-Tensor** again on the collection column containing
    the target value. As this is a multi-class classification problem, we set the
    loss function to **Categorical Cross Entropy**.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: In the third configuration tab, `50 epochs`, training batch size `256`, shuffling
    option `on`, and optimizer as `Adam` with default settings for the learning rate.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The network is finally saved in **Keras format** with the **Keras Network Writer**
    node. In addition, the network is converted into a TensorFlow network with the
    **Keras to TensorFlow Network Converter** node and saved with the **TensorFlow
    Network Writer** node. The TensorFlow network is used in deployment to avoid a
    time-consuming Python startup, required by the Keras network.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.26* shows the full workflow implementing all the described steps
    to train a neural network to generate fairy tales. This workflow and the used
    dataset are available on KNIME Hub at https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Workflow to train a neural network to generate fairy tales](img/B16391_07_026.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Workflow to train a neural network to generate fairy tales
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have trained and saved the network, let's move on to deployment
    to generate a new fairy tale's text.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To trigger the generation of new text during deployment, we start with an input
    sequence of the same length as each of the training sequences (![](img/Formula_B16391_07_036.png)).
    We feed the network with that sequence to predict the next character; then, we
    delete the oldest character in the sequence, add the predicted one, and apply
    the network again to our new input sequence, and so on. This is exactly the same
    procedure that we used in the case study for demand prediction. So, we will implement
    it here again with a recursive loop (*Figure 7.27*):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Deployment workflow to generate new free text](img/B16391_07_027.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – Deployment workflow to generate new free text
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'The trigger sequence was taken from the **Snow white and Rose red** fairy tale.
    The text for the trigger sequence was preprocessed, sequenced, and encoded as
    in the workflow used to train the network. This is done in the **Read and Pre-Process**
    metanode, shown in *Figure 7.28*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence](img/B16391_07_028.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – Workflow content in the Read and Pre-Process metanode to read
    and preprocess the trigger sequence
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The workflow reads the **Snow white and Rose red** fairy tale as well as the
    dictionary from the files created in the training workflow. Then, the same preprocessing
    steps as in the training workflow are applied.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: After that, we read the trained TensorFlow network and apply it to the trigger
    sequence with the **TensorFlow Network Executor** node.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the network is the probability of each character to be the next.
    We can pick the predicted character following two possible strategies:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The character with the highest probability is assigned to be the next character,
    known as the greedy strategy.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next character is picked randomly according to the probability distribution.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented both strategies in the **Extract Index** metanode in two
    different deployment workflows.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.29* shows the content of the **Extract Index** metanode when implementing
    the first strategy:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Workflow snippet to extract the character with the highest
    probability](img/B16391_07_029.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – Workflow snippet to extract the character with the highest probability
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: This metanode takes as input the output probabilities from the executed network
    and extracts the character with the highest probability. The key node here is
    the **Many to One** node, which extracts the cell with the highest score (probability)
    from the network output.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.30* shows the content of the **Extract Index** metanode when implementing
    the second strategy:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution](img/B16391_07_030.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – Workflow snippet to pick the next character based on a probability
    distribution
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: This workflow snippet expects as input the probability distribution for the
    characters and picks one according to it. The key node here is the **Random Label
    Assigner (Data)** node, which assigns a value based on the input probability distribution.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Random Label Assigner (Data)** node assigns one index to each data row
    at the lower input port based on the probability distribution at the upper input
    port. The data table at the upper input port must have two columns: one column
    with the class values – in our case, the index-encoded characters in string format
    – and one column with the corresponding probabilities. Therefore, the first part
    of the workflow snippet in *Figure 7.30* prepares the data table for the top input
    port of the **Random Label Assigner (Data)** node, from the network output, using
    the **Transpose** node, the **Counter Generation** node, and the **Number To String**
    node, while the **Table Creator** node creates a new table with only one row using
    the **Table Creator** node. This means the **Random Label Assigner (Data)** node
    then picks one index, based on the probability distribution defined by the table
    at the first input port.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the recursive loop and its implementation are explained in detail
    in [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent Neural
    Networks for Demand Prediction*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the deployment workflow, implementing both options, from the
    KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: The New Fairy Tale
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At last, I am sure you want to see the kind of free text that the network was
    able to produce. The following is an example of free generated text, using the
    first strategy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: The trigger sequence of 100 characters (not italics) comes from the first sentence
    of the fairy tale, *Snow white and Rose red*. The remaining text has been automatically
    generated by the network.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '*SNOW-WHITE AND ROSE-RED There was once a poor widow who lived in a lonely
    cottage*. In front of the cas, and a hunbred of wine behind the door of the; and
    he said the ansmer: ''What want yeurnKyow yours went for bridd, like is good any,
    or cries, and we will say I only gave the witeved to the brood of the country
    to go away with it.'' But when the father said, ''The cat soon crick.'' The youth,
    the old …'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The network has successfully learned the structure of the English language.
    Although the text is not perfect, you can see sensible character combinations,
    full words, some correct usage of quotation marks, and other similarly interesting
    features that the network has assimilated from the training text.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Generating Product Names with RNNs
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This last NLP case study is similar to the previous one. There, we wanted the
    network to create new free text based on a start sequence; here, we want the network
    to create new free words based on a start token. There, we wanted the network
    to create new sequences of words; here, we want the network to create new sequences
    of characters. Indeed, the goal of this product name generation case study is
    to create new names – that is, new words. While there'll be some differences,
    the approaches will be similar.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the details of this new approach.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The Problem of Product Name Generation
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, we don't associate artificial intelligence with creativity, as it
    is usually used to predict the outcome based on previously seen examples. The
    challenge for this case study is to use artificial intelligence to create something
    new, which is thought to be in the domain of creative minds.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a classic creative marketing example: product naming. Before a
    new product can be launched to the market, it actually needs a name. To find the
    name, the most creative minds of the company come together to generate a number
    of proposals for product names, taking different requirements into account. For
    example, the product name should sound familiar to the customers and yet be new
    and fresh too. Of all those candidates, ultimately only one will survive and be
    adopted as the name for the new product. Not an easy task!'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take one of the most creative industries: fashion. A company specializing
    in outdoor wear has a new line of clothes ready for the market. The task is to
    generate a sufficiently large number of name candidates for the new line of clothing.
    Names of mountains were proposed, as many other outdoor fashion labels have. Names
    of mountains evoke the feeling of nature and sound familiar to potential customers.
    However, new names must also be copyright free and original enough to stand out
    in the market.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Why not use fictitious mountain names then? Since they are fictitious, they
    are copyright free and differ from competitor names; however, since they are similar
    to existing mountain names, they also sound familiar enough to potential customers.
    Could an artificial intelligence model help generate new fictitious mountain names
    that still sound realistic enough and are evocative of adventure? What kind of
    network architecture could we use for such a task?
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: As we want to be able to form new words that are somehow reminiscent of mountain
    names, the network must be trained on the names of already-existing mountains.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: To form the training set, we use a list of 33,012 names of US mountains, as
    extracted from Wikipedia through a Wikidata query.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.31* shows you a subset of the training data:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Subset of US mountain names in the training set](img/B16391_07_031.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Subset of US mountain names in the training set
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have some training data, we can think about the network architecture.
    This time, we want to train a **many-to-many** LSTM-based RNN (see *Figure 7.32*).
    This means that during training, we have a sequence as input and a sequence as
    output. During deployment, the RNN, based on some initialized hidden states and
    the start token, must predict the first character of the new name candidate; then
    at the next step, based on the predicted character and on the updated hidden states,
    it must predict the next character – and so on until an end token is predicted
    and the process of generating the new candidate name is concluded:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN
    architecture for the product name generation case study](img/B16391_07_032.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – Simplified, unrolled visualization of the many-to-many RNN architecture
    for the product name generation case study
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the LSTM unit for this task, we need two sequences: an input sequence,
    made of a start token plus the mountain name, and a target sequence, made of the
    mountain name plus an end token. Notice that, at each training iteration, we feed
    the correct character into the network from the training set and not its prediction.
    This is called the **teacher forcing** training approach.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus first on preprocessing and encoding input and target sequences.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and Encoding Mountain Names
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the preprocessing is to create and encode input and target sequences,
    including the start and end tokens. As in the previous case study, we want to
    use one-hot encoding. Therefore, we create an index-based encoding, and we use
    the `1` as the start token index and `0` as the end token index.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last case study, you learned that during training, the lengths of the
    sequences in one batch have to be the same. Therefore, we take the number of characters
    of the longest mountain name (58) plus 1 as the sequence length. Since this is
    the length of the longest mountain name, there is no need for truncation, but
    all shorter sequences will be zero-padded by adding multiple end tokens:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation](img/B16391_07_033.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 – Workflow to read, encode, and create the input and target sequences
    for mountain name generation
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow snippet in the preceding figure creates the input and target sequences
    by doing the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Reading the mountain names and removing duplicates by using the **Table Reader**
    node and the **Duplicate Row Filter** node
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replacing each `<space>` with a tilde character and afterward, each character
    with the character itself and `<space>`, using two **String Manipulation** nodes
    (this step is described in detail in the preprocessing of the previous case study,
    *Free text generation with RNNs*)
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating and applying a dictionary (we will have a close look at this step in
    the next sub-section)
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Character splitting based on `<space>` and replacing all missing values with
    end tokens, to zero pad too-short sequences
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating input and target sequences as collection type cells
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of the steps are similar to the preprocessing steps in the case study of
    free text generation with RNNs. We will only take a closer look at *step 3* and
    *step 5*. Let's start with *step 3*.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Applying a Dictionary
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating and applying the dictionary is implemented in the **Create and apply
    dictionary** metanode. *Figure 7.34* shows its contents. The input to this metanode
    is mountain names with spaced characters:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode](img/B16391_07_034.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Workflow snippet inside the Create and apply dictionary metanode
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: In this metanode, we again use nodes from the KNIME Text Processing extension.
    The `2`, as we want to use indexes `0` and `1` for the end and start tokens. To
    use it as a dictionary in the next step, the created numerical indexes are transformed
    into strings by the **Number To String** node. Finally, the dictionary is applied
    (the **Dictionary Replacer** node), to transform characters into indexes in the
    original mountain names, and the text is extracted from the document (the **Document
    Data Extractor** node).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME Text Processing extension and some of their nodes, such as **Strings
    To Document**, **Unique Term Extractor**, **Dictionary Replacer**, and **Document
    Data Extractor**, were introduced more in detail in the first case study of this
    chapter, *Finding the Tone of Your Customers' Voice – Sentiment Analysis*.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'In the separate, lower branch of the workflow snippet, we finalize the dictionary
    for the deployment by adding one more row for the end token, using the `0` as
    the default value for the integer cells and an empty string for the string cells.
    This adds one new row to our dictionary table, with `0` in the index column and
    empty strings in the character columns. We need this additional row in the deployment
    workflow to remove the end token(s):'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Configuration window of the Add Empty Rows node](img/B16391_07_035.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Configuration window of the Add Empty Rows node
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the last step of the preprocessing.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Input and Target Sequences
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the **Missing Value** node in the workflow in *Figure 7.35*, we have the
    zero-padded, encoded sequences. What is missing, though, is the start token at
    the beginning of the input sequence and the end token at the end of the target
    sequence, to make sure that the input and target sequence have the same length.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: The additional values are added with `1` is used for the start token in the
    input sequence and the value `0` for the end token in the target sequence. In
    the case of the input sequence, the new column with the start token must be at
    the beginning. This is taken care of by the **Column Resorter** node. Now, the
    sequences can be aggregated and transformed into collection cells, using the **Create
    Collection Column** node.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Let's now design and train the appropriate network architecture.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Defining and Training the Network Architecture
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of designing and training the network is similar to the process
    used in the previous NLP case studies.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Network
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case, we want to use a network with five layers:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: A **Keras input layer** to define the input shape
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras LSTM layer** for the sequence analysis
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dropout layer** for regularization
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras dense layers** with linear activation
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Keras softmax layer** to transform the output into a probability distribution
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of unique characters in the training set – that is, the character
    set size – is `95`. Since we allow sequences of variable length, the shape of
    the input layer is `?, 95`. The `?` stands for a variable sequence length.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the `256` units for this layer and we have left all other settings
    unchanged.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we want to add even more randomization to the character
    pick at the output layer, to increment the network creativity. This is done by
    introducing the ![](img/Formula_B16391_07_037.png) **temperature** parameter in
    the softmax function of the trained output layer.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, the softmax function is defined as follows:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B16391_07_038.png) with ![](img/Formula_B16391_07_039.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'If we now introduce the additional ![](img/Formula_B16391_07_040.png) **temperature**
    parameter, the formula for the activation function changes to the following:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B16391_07_041.png) with ![](img/Formula_B16391_07_042.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
- en: This means we divide the linear part by ![](img/Formula_B16391_07_043.png) before
    applying the softmax function.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to insert the temperature parameter after training, we split the
    output layer into two layers: one **Keras Dense Layer** node with a linear activation
    function for the linear part and one **Keras Softmax Layer** node to apply the
    activation function.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Temperature is a parameter that can be added after training to control the confidence
    of the network output. ![](img/Formula_B16391_07_044.png) makes the network more
    confident but also more conservative. This often leads to generating the same
    results at every run.![](img/Formula_B16391_07_045.png) implements softer probability
    distributions over the different outputs. This leads to more diversity but, at
    the same time, also to more mistakes, such as in this case, character combinations
    that are impossible in English.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Training and Postprocessing the Network
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network is trained using the **Keras Network Learner** node. For the input
    data and the target data, the **From Collection of Number (integer)** conversion
    to **One-Hot Tensor** is selected. The different characters are again like different
    classes in a multi-class classification problem; therefore, the **Categorical
    Cross Entropy** loss function is adopted for training.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: In the third tab, `30` epochs, with a batch size of `128` data rows, shuffling
    the data before each epoch, and using `Adam` as the optimizer algorithm with the
    default settings. So far, this is all the same as in the previous NLP case studies.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the network, the temperature, ![](img/Formula_B16391_07_040.png),
    is added by using the **DL Python Editor** node with the following lines of Python
    code:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Remember that the hidden states of the previous LSTM unit are always used as
    input in the next LSTM unit. Therefore, three inputs are defined in the code:
    two for the two hidden states and one for the last predicted character encoded
    as a one-hot vector.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the network is transformed into a TensorFlow network object and saved
    for deployment. The final training workflow is shown in *Figure 7.36*:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Training workflow for the product name generation case study](img/B16391_07_036.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Training workflow for the product name generation case study
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow is available on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with the deployment workflow.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deployment Workflow
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deployment workflow again uses the recursive loop approach, similar to the
    deployment workflow of the NLP and the demand prediction case studies. This time,
    though, there is one big difference.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: In the last two case studies, the hidden state vectors were re-initialized at
    each iteration, as we always had ![](img/Formula_B16391_03_031.png) previous characters
    or ![](img/Formula_B16391_03_255.png) previous values as input. In this case study,
    we pass back, from the loop end node to the loop start node, not only the predicted
    index but also the two hidden state tensors from the LSTM layer.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.37*, you can see the deployment workflow, which is also available
    on the KNIME Hub: https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%207/.
    Let''s look at the setting differences in detail:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Deployment workflow to create multiple possible product names](img/B16391_07_037.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: Figure 7.37 – Deployment workflow to create multiple possible product names
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: The first component, `1`. The other two columns contain the initial hidden states
    – that is, collection cells with 256 zeros in both columns.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'The **TensorFlow Network Executor** node executes the network one first time,
    producing as output the probability distribution over the indexes. In the configuration
    window of **TensorFlow Network Executor**, we have selected as input the columns
    with the first hidden state, the second hidden state, and the input collection.
    In addition, we set three output columns: one output column for the probability
    distribution, one output column for the first hidden state, and one output column
    for the second hidden state. We then pick the next index-encoded character according
    to the output probability distribution using the **Random Label Assigner (Data)**
    node in the **First Char** metanode. All these output values, predicted indexes,
    and hidden states make their way to the loop start node to predict the second
    index-encoded character.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Then, we start the recursive loop to generate one character after the next.
    At each iteration, we apply the network to the last predicted index and hidden
    states. We then pick the next character, again with the **Random Number Assigner
    (Data)** node, and we feed the last predicted value and the new hidden states
    into the lower input port of the **Recursive Loop End** node so that they can
    reach back to the loop start node.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: In the **Extract Mountain Names** component, we finally apply the dictionary
    – created in the training workflow – and we remove all the mountain names that
    appeared already in the training set.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.38*, you can see some of the generated mountain names. Indeed,
    they are new, copyright-free, evocative of mountains, and nature-feeling, and
    can be generated automatically in a number ![](img/Formula_B16391_03_036.png)
    as high as desired:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38 – Mountain names generated by the deployment workflow](img/B16391_07_038.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: Figure 7.38 – Mountain names generated by the deployment workflow
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: One of them will eventually be chosen as the new product name.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the end of this relatively long chapter. Here, we have described
    three NLP case studies, each one solved by training an LSTM-based RNN applied
    to a time series prediction kind of problem.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: The first case study analyzed movie review texts to extract the sentiment hidden
    in it. We dealt there with a simplified problem, considering a binary classification
    (positive versus negative) rather than considering too many nuances of possible
    user sentiment.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: The second case study was language modeling. Training an RNN on a given text
    corpus with a given style produced a network capable of generating free text in
    that given style. Depending on the text corpus on which the network is trained,
    it can produce fairy tales, Shakespearean dialogue, or even rap songs. We showed
    an example that generates text in fairy tale style. The same workflows can be
    easily extended with more success to generate rap songs (R. Silipo, *AI generated
    rap songs*, CustomerThink, 2019, [https://customerthink.com/ai-generated-rap-songs/](https://customerthink.com/ai-generated-rap-songs/))
    or Shakespearean dialogue (R. Silipo, *Can AI write like Shakespeare?*, Towards
    data Science, 2019, [https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee](https://towardsdatascience.com/can-ai-write-like-shakespeare-de710befbfee)).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: The last case study involved the generation of candidates for new product names
    that must be innovative and copyright-free, stands out in the market, and be evocative
    of nature. So, we trained an RNN to generate fictitious mountain names to be used
    as name candidates for a new outdoor clothing line.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will describe one more NLP example: neural machine
    translation.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a word embedding?
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) An encoding functionality that can be trained within the neural network
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A text cleaning procedure
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) A training algorithm for an RNN
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) A postprocessing technique to choose the most likely character
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which statement regarding sentiment analysis is true?
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Sentiment analysis can only be solved with RNNs.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Sentiment analysis is the same as emotion detection.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Sentiment analysis identifies the underlying sentiment in a text.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Sentiment analysis is an image processing task.
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What does a many-to-many architecture mean?
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) An architecture with an input sequence and an output sequence
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) An architecture with an input sequence and a vector as output
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) An architecture with many hidden units and many outputs
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) An architecture with one input feature and an output sequence
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why do I need a trigger sequence for free text generation?
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) To calculate the probabilities
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) To compare the prediction with the target
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) To initialize the hidden states before predicting the next character
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) To save the network in TensorFlow format
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
