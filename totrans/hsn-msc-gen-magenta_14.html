<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assessments</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1: Introduction to Magenta and Generative Art</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root">Randomness.</li>
<li class="mce-root">Markov chain.</li>
<li class="mce-root">Algorave.</li>
<li class="mce-root"><strong>Long short-term memory</strong> (<strong>LSTM</strong>).</li>
<li class="mce-root">Autonomous systems generate music without operator input; assisting music systems will complement an artist while working.</li>
<li class="mce-root">Symbolic: sheet music, MIDI, MusicXML, AbcNotation. Sub-symbolic: raw audio (waveform), spectrogram.</li>
<li class="mce-root">"Note On" and "Note Off" timing, pitch between 1 and 127 kHz, velocity, and channel.</li>
<li class="mce-root">At a sample rate of 96 kHz, the Nyquist frequency is 96 kHz/2 = 48 kHz and the frequency range is 0 to 48 kHz. This is worse for listening to audio since 28 kHz of audio is lost on the ear (remember anything over 20 khz cannot be heard), and that sampling rate is not properly supported by much audio equipment. It is useful in recording and audio editing though.</li>
<li class="mce-root">A single musical note, A4, is played for 1 second loudly.</li>
<li class="mce-root">Drums, voice (melody), harmony (polyphony), and interpolation and manipulation.</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 2: Generating Drum Sequences with the Drums RNN</h1>
                </header>
            
            <article>
                
<ol start="1">
<li>
<p>Given a current sequence, predict the score for the next note, then do a prediction for each step you want to generate.</p>
</li>
<li>(1) RNNs operate on sequences of vectors, for the input and output, which is good for sequential data such as a music score, and (2) keep an internal state composed of the previous output steps, which is good for doing a prediction based on past inputs, not only the current input.</li>
<li>(1) First, the hidden layer will get <em>h(t + 1)</em><span>, which is the output of the previous hidden layer, and (2) it will also receive</span> <em>x(t + 2)</em>, <span>which is the input of the current step.</span></li>
<li>The number of bars <span>generated </span>will be 2 bars, or 32 steps, since we have 16 steps per bar. At 80 QPM, each step takes 0.1875 seconds, because you take the number of seconds in a minute, divide by the QPM, and divide by the number of steps per quarter: 60 / 80 / 4 = 0.1875. Finally, you have 32 steps at 0.1875 seconds each, so the total time is 32 * 0.1875 = 6 seconds.</li>
<li>Increasing the branch factor reduces the randomness, since you have more branches to choose from when selecting the best branch, but increasing the temperature will increase the randomness. Doing both at the same time will cancel out each other, we just don't know in what proportions.</li>
<li>At each step, the algorithm will generate four branches and keep two. At the last iteration, the beam search will search for the best branch in the graph by checking the remaining two nodes at each level multiplied by the number of steps of the generated graph (also the height of the tree), which is three. So we go through 2 * 3 nodes = 6 nodes.</li>
<li><kbd>NoteSequence</kbd>.</li>
<li>The MIDI notes maps to the following classes: 36 maps to 0 (kick drum), 40 maps to 1 (snare drum), 42 maps to 2 (closed hi-hat). The resulting index is calculated with 2<sup>0</sup> <span>+ 2</span><sup>1</sup> <span>+ 2</span><sup>2</sup> <span>= 7, so the resulting vector will be</span> <em>v = [0, 0, 0, 0, 0, 0, 1, 0, ... ]</em><span>.</span></li>
<li>The bit representation of index 131 is <kbd>10000011</kbd> <span>(in Python, you can use</span> <kbd>"{0:b}".format(131)</kbd> <span>to get that). This is represented as 2</span><sup>0</sup> <span>+ 2</span><sup>1</sup> <span>+ 2</span><sup>7</sup><span>, which gives us the following classes: 0 (drum kit), 1 (snare drum) and 7 (crash cymbal). We then arbitrarily take the first element of each class:</span> <em>{36, 38, 49}</em><span>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3: Generating Polyphonic Melodies</h1>
                </header>
            
            <article>
                
<ol start="1">
<li>
<p>Vanishing gradients (values get multiplied by small values in each RNN step) and exploding gradients are common RNN problems that occur when training during the backpropagation step. LSTM provides a dedicated cell state that is modified by forget, input, and output gates to alleviate those problems.</p>
</li>
<li>
<p><strong>Gated</strong> <strong>recurrent</strong> <strong>units</strong> (<strong>GRUs</strong>) are simpler but less expressive memory cells, where the forget and input gates are combined into a single <strong>update gate</strong>.</p>
</li>
<li>For a 3/4 time signature, you need 3 steps per quarter note, times 4 steps per quarter note, which equals 12 steps per bar. For a binary step counter to count to 12, you need 5 bits (like for 4/4 time) that will only count to 12. For 3 lookbacks, you'll need to look at the past 3 bars, with each bar being 12 steps, so you have <em>[36, 24, 12]</em><span>.</span></li>
<li>The resulting vector is the sum of the previous step vectors, each applied with the attention mask, so we have 0.1 applied to <em>[1, 0, 0, 0]</em><span>, plus 0.5 applied to</span> <em>[0, 1, 0, x]</em> <span>giving</span> <em>[0.10, 0.50, 0.00, 0.25]</em><span>. The value of x is 0.5, because 0.5 times 0.5 equals 0.25.</span></li>
<li>A C major chord of one quarter note.</li>
<li>In Polyphony RNN, there are no note end events. If no <kbd>CONTINUED_NOTE</kbd> is used for a pitch in a step, it stops the note. In Perfomance RNN, a <kbd>NOTE_END 56</kbd> event would be used.</li>
<li>(1) Expressive timing using <kbd>TIME_SHIFT</kbd> events, present in all Performance RNN models, for example in the <kbd>performance</kbd> configuration, and (2) dynamic play using <kbd>VELOCITY</kbd> events, present in the <kbd>performance_with_dynamics</kbd> configuration.</li>
<li>It will change the number of iterations during the RNN steps call. A bigger number of notes per seconds will ask for more RNN steps during the generation.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4: Latent Space Interpolation with MusicVAE</h1>
                </header>
            
            <article>
                
<ol start="1">
<li>
<p>The main use is dimensionality reduction, to force the network to learn important features, making it possible to reconstruct the original input. The downside of AE is that the latent space represented by the hidden layer is not continuous, making it hard to sample since the decoder won't be able to make sense of some of the points.</p>
</li>
<li>The reconstruction loss penalizes the network when it creates outputs that are different from the input.</li>
<li>In VAE, the latent space is continuous and smooth, making it possible to sample any point of the space and interpolate between two points. It is achieved by having the latent variables follow a probability distribution of P(z), often a Gaussian distribution.</li>
<li>The KL divergence measures how much two probability distributions diverge from each other. When combined with the reconstruction loss, it centers the clusters around 0 and makes them more or less close to one another.</li>
<li>We sample the normal distribution using <kbd>np.random.randn(4, 512)</kbd>.</li>
<li>Calculate the direction between two points in the latent space.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5: Audio Generation with NSynth and GANSynth</h1>
                </header>
            
            <article>
                
<ol>
<li>You have to handle 16,000 samples per second (at least) and keep track of the general structure at a bigger time scale.</li>
<li>NSynth is a WaveNet-style autoencoder that learns its own temporal embedding, making it possible to capture long term structure, and providing access to a useful hidden space.</li>
<li>The colors in the rainbowgram are the 16 dimensions of the temporal embedding.</li>
<li>Check the <kbd>timestretch</kbd> <span>method in the <kbd>audio_utils.py</kbd> file in the chapter's code.</span></li>
</ol>
<p> </p>
<ol start="5">
<li>GANSynth uses upsampling convolutions, making the training and generation processing in parallel possible for the entire audio sample.</li>
<li>
<p>You need to sample the random normal distribution using <kbd>np.random.normal(size=[10, 256])</kbd>, where 10 is the number of sampled instruments, and 256 is the size of the latent vector (given by the <kbd>latent_vector_size</kbd> configuration).</p>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6: Data Preparation for Training</h1>
                </header>
            
            <article>
                
<ol>
<li>MIDI is not a text format, so it is harder to use and modify, but it is extremely common. MusicXML is rather rare and cumbersome but has the advantage of being in text format. ABCNotation is also rather rare, but has the advantage of being in text format and closer to sheet music.</li>
<li>Use the code from <kbd>chapter_06_example_08.py</kbd>, and change the <kbd>program=43</kbd> <span>in the extraction.</span></li>
<li>There are 1,116 rock songs in LMD and 3,138 songs for jazz, blues, and country. Refer to <kbd>chapter_06_example_02.py</kbd> and <kbd>chapter_06_example_03.py</kbd> to see how to make statistics with genre information.</li>
<li>Use the <kbd>RepeatSequence</kbd> <span>class in <kbd>melody_rnn_pipeline_example.py</kbd>.</span></li>
<li>Use the code from <kbd>chapter_06_example_09.py</kbd>. Yes, we can train a quantized model with it since the data preparation pipeline quantizes the input.</li>
<li>For small datasets, data augmentation plays an essential role in creating more data, because sometimes you just don't have more. For bigger datasets, it also plays a role by creating more relevant data and variations on existing data, which is good for the network training phase.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7: Training Magenta Models</h1>
                </header>
            
            <article>
                
<ol>
<li>See <kbd>chapter_07_example_03.py</kbd>.</li>
<li>
<p>A network that underfits is a network that hasn't reached its optimum, meaning it won't predict well with the evaluation data, because it fits poorly the training data (for now). It can be fixed by letting it train long enough, by adding more network capacity, and more data.</p>
</li>
</ol>
<p> </p>
<ol start="3">
<li>A network that overfits is a network that has learned to predict the input but cannot generalize to values outside of its training set. It can be fixed by adding more data, by reducing the network capacity, or by using regularization techniques such as dropout.</li>
<li>Early stopping.</li>
<li>Read <em>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</em>, which explains that a larger batch size leads to sharp minimizers, which in turn leads to poorer generalization. Therefore it is worse in terms of efficiency, but might be better in terms of training time, since more data is processed at the same time.</li>
<li>A bigger network is a network that will be more precise in its prediction, so maximizing that should be important. The network size should also grow with the size (and quality) of the data. For example, a network that's too big for its data will likely overfit.</li>
<li>It helps with the exploding gradients problem because the weights will be multiplied by smaller values, limiting the possibility of having big gradients. Another way of doing this is by reducing the learning rate.</li>
<li>It can be used to launch training on more powerful machines and to launch multiple training sessions at the same time. Unfortunately, using cloud providers have a cost, meaning the more training time and power we use, the more costly our training will get.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8: Magenta in the Browser with Magenta.js</h1>
                </header>
            
            <article>
                
<ol start="1">
<li>We can train models using TensorFlow.js, but we cannot train models using Magenta.js. We need to train the models in Magenta using Python and import the resulting models in Magenta.js.</li>
<li>The Web Audio API enables audio synthesis in the browser using audio nodes for generation, transformation, and routing. The easiest way to use it is to use an audio framework such as Tone.js.</li>
<li>The method is <kbd>randomSample</kbd> <span>and the argument is the pitch of the generated note. As an example, using 60 will result in a single note at MIDI pitch 60, or C4 in letter notation. This is also useful as a reference for pitching the note up or down using Tone.js.</span></li>
</ol>
<p> </p>
<ol start="4">
<li>The method is <kbd>sample</kbd> <span>and the number of instruments depends on the model that is being used. In our example, we've used the <kbd>trio</kbd> model, which generates three instruments. Using a <kbd>melody</kbd> model will generate only one lead instrument.</span></li>
<li>Since JavaScript is single threaded, long synchronous computations that are launched in the UI thread will block its execution. Using a web worker makes it possible to execute code in another thread.</li>
<li>Using the Web MIDI API in the browser, which is not well supported at the moment, or using Magenta.js in a Node.js process on the server side, making it easier to send MIDI to other processes.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 9: Making Magenta Interact with Music Applications</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>A DAW will have more functions geared towards music production such as recording, audio, MIDI editing, effects and mastering, and song composition. A software synthesizer like FluidSynth will have less functionalities, but have the advantage of being lightweight and easy to use.</p>
</li>
<li>Most music software won't open MIDI ports by themselves, so to send sequences back and forth between them we have to manually open ports.</li>
<li>See the code in <kbd>chapter_09_example_05.py</kbd> in this chapter's code.</li>
<li>Because syncing two pieces of software that have desynced requires restarting them. A MIDI clock enables syncing once per beat.</li>
<li>
<p>Because Magenta Studio integrates with existing music production tools such as DAWs and doesn't require any technical knowledge, it makes AI-generated music available to a greater audience, which is ultimately the goal of Magenta.</p>
</li>
<li>Magenta Studio plugins and Magenta Studio standalone are both based on Magenta.js, packaged using Electron. Magenta Studio Plugins uses the Max MSP integration in Ableton Live to execute inside it.</li>
</ol>


            </article>

            
        </section>
    </body></html>