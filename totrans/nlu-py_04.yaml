- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting Libraries and Tools for Natural Language Understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will get you set up to process natural language. We will begin
    by discussing how to install Python, and then we will discuss general software
    development tools such as JupyterLab and GitHub. We will also review major Python
    **natural language processing** (**NLP**) libraries, including the **Natural Language
    Toolkit** (**NLTK**), **spaCy**, and **TensorFlow/Keras**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural language understanding** (**NLU**) technology has benefited from
    a wide assortment of very capable, freely available tools. While these tools are
    very powerful, there is no one library that can do all of the NLP tasks needed
    for all applications, so it is important to understand what the strengths of the
    different libraries are and how to combine them.'
  prefs: []
  type: TYPE_NORMAL
- en: Making the best use of these tools will greatly accelerate any NLU development
    project. These tools include the Python language itself, development tools such
    as JupyterLab, and a number of specific natural language libraries that can perform
    many NLU tasks. It is equally important to know that because these tools are widely
    used by many developers, active online communities such as Stack Overflow ([https://stackoverflow.com/](https://stackoverflow.com/))
    have developed. These are great resources for getting answers to specific technical
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing software—JupyterLab and GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at an example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there are many online resources for using tools such as Python, JupyterLab,
    and GitHub, we will only briefly outline their usage here in order to be able
    to spend more time on NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we will illustrate the installation of the libraries in the
    base system. However, you may wish to install the libraries in a virtual environment,
    especially if you are working on several different Python projects. The following
    link may be helpful for installing a virtual environment: [https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).'
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the examples in this chapter, you will need the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` or `conda` (preferably `pip`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JupyterLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next sections will go over the process of installing these packages, which
    should be installed in the order in which they are listed here.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in setting up your development environment is to install Python.
    If you have already installed Python on your system, you can skip to the next
    section, but do make sure that your Python installation includes Python 3, which
    is required by most NLP libraries. You can check your Python version by entering
    the following command in a command-line window, and the version will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you have both Python 2 and Python 3 installed, you may have to
    run the `python3 –version` command to check the Python 3 version. If you don’t
    have Python 3, you’ll need to install it. Some NLP libraries require not just
    Python 3 but Python 3.7 or greater, so if your version of Python is older than
    3.7, you’ll need to update it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python runs on almost any operating system that you choose to use, including
    Windows, macOS, and Linux. Python can be downloaded for your operating system
    from [http://www.python.org](http://www.python.org). Download the executable installer
    for your operating system and run the installer. When Python is installed, you
    can check the installation by running the preceding command on your command line
    or terminal. You will see the version you’ve just installed, as shown in the following
    command-line output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This installs Python, but you will also need to install add-on libraries for
    NLP. Installing libraries is done with the auxiliary programs `pip` and `conda`.
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` and `conda` are two cross-platform tools that can be used for installing
    Python libraries. We will be using them to install several important natural language
    and `pip` in this book, but you can also use `conda` if it’s your preferred Python
    management tool. `pip` is included by default with Python versions 3.4 and newer,
    and since you’ll need 3.7 for the NLP libraries, `pip` should be available in
    your Python environment. You can check the version with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section, we will discuss the development environment we will be
    using: JupyterLab.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing software – JupyterLab and GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The development environment can make all the difference in the efficiency of
    the development process. In this section, we will discuss two popular development
    resources: JupyterLab and GitHub. If you are familiar with other Python **interactive
    development environments** (**IDEs**), then you can go ahead and use the tools
    that you’re familiar with. However, the examples discussed in this book will be
    shown in a JupyterLab environment.'
  prefs: []
  type: TYPE_NORMAL
- en: JupyterLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JupyterLab is a cross-platform coding environment that makes it easy to experiment
    with different tools and techniques without requiring a lot of setup time. It
    operates in a browser environment but doesn’t require a cloud server—a local server
    is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing JupyterLab is done with the following `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once JupyterLab is installed, you can run it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This command should be run in a command line in the directory where you would
    like to keep your code. The command will launch a local server, and the Jupyter
    environment will appear in a browser window, as shown in *Figure 4**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – JupyterLab user interface on startup](img/B19005_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – JupyterLab user interface on startup
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment shown in *Figure 4**.1* includes three types of content, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook**—Contains your coding projects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Console**—Gives you access to command-line or terminal functions from directly
    within the Jupyter notebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start` command was run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you click on the **Python 3** icon under **Notebook**, you’ll get a new
    notebook showing a coding cell, and you’ll be ready to start coding in Python.
    We’ll return to the JupyterLab environment and start coding in Python in the *Looking
    at an example* section later in this chapter and again in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of you are probably already familiar with GitHub, a popular open source
    code repository system ([https://github.com](https://github.com)). GitHub provides
    very extensive capabilities for storing and sharing code, developing code branches,
    and documenting code. The core features of GitHub are currently free.
  prefs: []
  type: TYPE_NORMAL
- en: The code examples used in this book can be found at [https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python](https://github.com/PacktPublishing/Natural-Language-Understanding-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to learn about several important libraries, including NLTK,
    spaCy, and Keras, which we will be using extensively in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review several of the major Python libraries that are
    used in NLP; specifically, NLTK, spaCy, and Keras. These are very useful libraries,
    and they can perform most basic NLP tasks. However, as you gain experience with
    NLP, you will also find additional NLP libraries that may be appropriate for specific
    tasks as well, and you are encouraged to explore those.
  prefs: []
  type: TYPE_NORMAL
- en: Using NLTK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLTK ([https://www.nltk.org/](https://www.nltk.org/)) is a very popular open
    source Python library that greatly reduces the effort involved in developing natural
    language applications by providing support for many frequently performed tasks.
    NLTK also includes many corpora (sets of ready-to-use natural language texts)
    that can be used for exploring NLP problems and testing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go over what NLTK can do, and then discuss the NLTK
    installation process.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059), many distinct
    tasks can be performed in an NLU pipeline as the processing moves from raw words
    to a final determination of the meaning of a document. NLTK can perform many of
    these tasks. Most of these functions don’t provide results that are directly useful
    in themselves, but they can be very helpful as part of a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the basic tasks that are needed in nearly all natural language projects
    can easily be done with NLTK. For example, texts to be processed need to be broken
    down into words before processing. We can do this with NLTK’s `word_tokenize`
    function, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be an array of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the word `we''d` is separated into two components, `we` and `''d`,
    because it is a contraction that actually represents two words: *we* and *would*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NLTK also provides some functions for basic statistics such as counting word
    frequencies in a text. For example, continuing from the text we just looked at,
    *we’d like to book a flight from Boston to London*, we can use the NLTK `FreqDist()`
    function to count how often each word occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we imported the `FreqDist()` function from NLTK’s `probability`
    package and used it to count the frequencies of each word in the text. The result
    is a Python dict where the keys are the words and the values are how often the
    words occur. The word `to` occurs twice, and each of the other words occurs once.
    For such a short text, the frequency distribution is not particularly insightful,
    but it can be very helpful when you’re looking at larger amounts of data. We will
    see the frequency distribution for a large corpus in the *Looking at an example*
    section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLTK can also do `nltk.pos_tag(tokenized_text)` function is used for POS tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, NLTK provides functions for parsing texts. Recall that parsing was
    discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059). As discussed in
    [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016), NLTK also provides functions
    for creating and applying **regular** **expressions** (**regexes**).
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the most useful capabilities of NLTK. The full set of NLTK
    capabilities is too large to list here, but we will be reviewing some of these
    other capabilities in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134) and [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159).
  prefs: []
  type: TYPE_NORMAL
- en: Installing NLTK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NLTK requires Python 3.7 or greater. The installation process for Windows is
    to run the following command in a command window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For a Mac or Unix environment, run the following command in a terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will go over another popular NLU library, spaCy, and
    explain what it can do. As with NLTK, we will be using spaCy extensively in later
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Using spaCy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: spaCy is another very popular package that can do many of the same NLP tasks
    as NLTK. Both toolkits are very capable. spaCy is generally faster, and so is
    more suitable for deployed applications. Both toolkits support many languages,
    but not all NLU tasks are supported for all languages, so in making a choice between
    NLTK and spaCy, it is important to consider the specific language requirements
    for that application.
  prefs: []
  type: TYPE_NORMAL
- en: As with NLTK, spaCy can perform many basic text-processing functions. Let's
    check it out!
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to set up tokenization in spaCy is very similar to the code for NLTK,
    with a slightly different function name. The result is an array of words, where
    each element is one token. Note that the `nlp` object is initialized with an `en_core_web_sm`
    model that tells it to use the statistics from a particular set of web-based data,
    `en_core_web_sm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate statistics such as the frequency of the words that occur
    in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between spaCy and NLTK is that NLTK uses the `FreqDist`
    function and spaCy uses the `Counter` function. The result, a Python dict with
    the words as keys and the frequencies as values, is the same for both libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with NLTK, we can perform POS tagging with spaCy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following POS assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, NLTK and spaCy use different labels for the different parts of
    speech. This is not necessarily a problem, because there is no *correct* or *standard*
    set of parts of speech, even for one language. However, it’s important for the
    parts of speech to be consistent within an application, so developers should be
    aware of this difference and be sure not to confuse the NLTK and spaCy parts of
    speech.
  prefs: []
  type: TYPE_NORMAL
- en: Another very useful capability that spaCy has is **named entity recognition**
    (**NER**). NER is the task of identifying references to specific persons, organizations,
    locations, or other entities that occur in a text. NER can be either an end in
    itself or it can be part of another task. For example, a company might be interested
    in finding when their products are mentioned on Facebook, so NER for their products
    would be all that they need. On the other hand, a company might be interested
    in finding out if their products are mentioned in a positive or negative way,
    so in that case, they would want to perform both NER and **sentiment** **analysis**
    (**SA**).
  prefs: []
  type: TYPE_NORMAL
- en: 'NER can be performed in most NLP libraries; however, it is particularly easy
    to do in spaCy. Given a document, we just have to request rendering of the document
    using the `ent` style, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The rendered result shows that the `boston` and `new york` named entities are
    assigned a **geopolitical entity** (**GPE**) label, as shown in *Figure 4**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”](img/B19005_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – NER for “we’d like to book a flight from Boston to New York”
  prefs: []
  type: TYPE_NORMAL
- en: 'Parsing or the analysis of syntactic relationships among the words in a sentence
    can be done very easily with almost the same code, just by changing the value
    of the `style` parameter to `dep` from `ent`. We’ll see an example of a syntactic
    parse later on in *Figure 4**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing spaCy is done with the following `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The next library we will look at is the Keras ML library.
  prefs: []
  type: TYPE_NORMAL
- en: Using Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras ([https://keras.io/](https://keras.io/)) is another popular Python NLP
    library. Keras is much more focused on ML than NLTK or spaCy and will be the go-to
    library for NLP **deep learning** (**DL**) applications in this book. It’s built
    on top of another package called TensorFlow ([https://www.tensorflow.org/](https://www.tensorflow.org/)),
    which was developed by Google. Because Keras is built on TensorFlow, TensorFlow
    functions can be used in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Since Keras focuses on ML, it has limited capabilities for preprocessing text.
    For example, unlike NLTK or spaCy, it does not support POS tagging or parsing
    directly. If these capabilities are needed, then it’s best to preprocess the text
    with NLTK or spaCy. Keras does support tokenization and removal of extraneous
    tokens such as punctuation and HTML markup.
  prefs: []
  type: TYPE_NORMAL
- en: Keras is especially strong for text-processing applications using **neural networks**
    (**NN**). This will be discussed in much more detail in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
    Although Keras includes few high-level functions for performing NLP functions
    such as POS tagging or parsing in one step, it does include capabilities for training
    POS taggers from a dataset and then deploying the tagger in an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Keras is included in TensorFlow, Keras is automatically installed when
    TensorFlow is installed. It is not necessary to install Keras as an additional
    step. Thus, the following command is sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Learning about other NLP libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are quite a few other Python libraries that include NLP capabilities and
    that can be useful in some cases. These include PyTorch ([https://pytorch.org/](https://pytorch.org/))
    for processing based on **deep neural networks** (**DNN**), scikit-learn ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)),
    which includes general ML functions, and Gensim ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)),
    for topic modeling, among others. However, I would recommend working with the
    basic packages that we’ve covered here for a few projects at first until you get
    more familiar with NLP. If you later have a requirement for additional functionality,
    a different language, or faster processing speed than what the basic packages
    provide, you can explore some of these other packages at that time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next topic, we will discuss how to choose among NLP libraries. It’s good
    to keep in mind that choosing libraries isn’t an all-or-none process—libraries
    can easily be mixed and matched if one library has strengths that another one
    doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing among NLP libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The libraries discussed in the preceding sections are all very useful and powerful.
    In some cases, they have overlapping capabilities. This raises the question of
    selecting which libraries to use in a particular application. Although all of
    the libraries can be combined in the same application, it reduces the complexity
    of applications if fewer libraries are used.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK is very strong in corpus statistics and rule-based linguistic preprocessing.
    For example, some useful corpus statistics include counting words, counting parts
    of speech, counting pairs of words (bigrams), and tabulating words in context
    (concordances). spaCy is fast, and its displaCy visualization library is very
    helpful in gaining insight into processing results. Keras is very strong in DL.
  prefs: []
  type: TYPE_NORMAL
- en: During the lifetime of a project, it is often useful to start with tools that
    help you quickly get a good overall picture of the data, such as NLTK and spaCy.
    This initial analysis will be very helpful for selecting the tools that are needed
    for full-scale processing and deployment. Since training DL models using tools
    such as Keras can be very time-consuming, doing some preliminary investigation
    with more traditional approaches will help narrow down the possibilities that
    need to be investigated in order to select a DL approach.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about other packages useful for NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to packages that directly support NLP, there are also a number
    of other useful general-purpose open source Python packages that provide tools
    for generally managing data, including natural language data. These include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**: NumPy ([https://numpy.org/](https://numpy.org/)) is a powerful package
    that includes many functions for the numerical calculations that we’ll be working
    with in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),[*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193), and [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas**: pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    provides general tools for data analysis and manipulation, including natural language
    data, especially data in the form of tables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**: scikit-learn is a powerful package for ML, including text
    processing ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also several visualization packages that will be very helpful for
    graphical representations of data and for processing results. Visualization is
    important in NLP development because it can often give you a much more comprehensible
    representation of results than a numerical table. For example, visualization can
    help you see trends, pinpoint errors, and compare experimental conditions. We’ll
    be using visualization tools throughout the book, but especially in [*Chapter
    6*](B19005_06.xhtml#_idTextAnchor134). Visualization tools include generic tools
    for representing different kinds of numerical results, whether they have to do
    with NLP or not, as well as tools specifically designed to represent natural language
    information such as parses and NER results. Visualization tools include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Matplotlib**: Matplotlib ([https://matplotlib.org/](https://matplotlib.org/))
    is a popular Python visualization library that’s especially good at creating plots
    of data, including NLP data. If you’re trying to compare the results of processing
    with several different techniques, plotting the results can often provide insights
    very quickly about how well the different techniques are working, which can be
    helpful for evaluation. We will be returning to the topic of evaluation in [*Chapter
    13*](B19005_13.xhtml#_idTextAnchor226)*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seaborn**: Seaborn ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    is based on Matplotlib. It enables developers to produce attractive graphs representing
    statistical information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**displaCy**: displaCy is part of the spaCy tools, and is especially good at
    representing natural language results such as POS tags, parses, and named entities,
    which we discussed in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WordCloud**: WordCloud ([https://amueller.github.io/word_cloud/](https://amueller.github.io/word_cloud/))
    is a specialized library for visualizing word frequencies in a corpus, which can
    be useful when word frequencies are of interest. We’ll see an example of a word
    cloud in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to this point, we’ve reviewed the technical requirements for our software
    development environment as well as the NLP libraries that we’ll be working with.
    In the next section, we’ll put everything together with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate some of these concepts, we’ll work through an example using JupyterLab
    where we explore an SA task for movie reviews. We’ll look at how we can apply
    the NLTK and spaCy packages to get some ideas about what the data is like, which
    will help us plan further processing.
  prefs: []
  type: TYPE_NORMAL
- en: The corpus (or dataset) that we’ll be looking at is a popular set of 2,000 movie
    reviews, classified as to whether the writer expressed a positive or negative
    sentiment about the movie ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: '*Bo Pang* and *Lillian Lee*, *Seeing stars: Exploiting class relationships
    for sentiment categorization with respect to rating scales*, *Proceedings of the*
    *ACL*, *2005*.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a good example of the task of SA, which was introduced in [*Chapter
    1*](B19005_01.xhtml#_idTextAnchor016).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up JupyterLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll be working with JupyterLab, so let’s start it up. As we saw earlier,
    you can start JupyterLab by simply typing the following command into a command
    (Windows) or terminal (Mac) window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will start a local web server and open a JupyterLab window in a web browser.
    In the JupyterLab window, open a new notebook by selecting **File** | **New**
    | **Notebook**, and an untitled notebook will appear (you can rename it at any
    time by selecting **File** | **Rename Notebook**).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the libraries that we’ll be using in this example,
    as shown next. We’ll be using the NLTK and spaCy NLP libraries, as well as some
    general-purpose libraries for numerical operations and visualization. We’ll see
    how these are used as we go through the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Enter the preceding code into a JupyterLab cell and run the cell. Running this
    cell (**Run** | **Run Selected Cells**) will import the libraries and give you
    a new code cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the movie review data by typing `nltk.download()` into the new code
    cell. This will open a new **NLTK Downloader** window, as shown in *Figure 4**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.3 – NLTK \uFEFFDownloader window](img/B19005_04_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – NLTK Downloader window
  prefs: []
  type: TYPE_NORMAL
- en: On the `movie_reviews`. Click the **Download** button to download the corpus.
    You can select **File** | **Change Download Directory** if you want to change
    where the data is downloaded. Click **File** | **Exit** from the downloader window
    to exit from the **Download** window and return to the JupyterLab interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a look at the directory where you downloaded the data, you will
    see two directories: `neg` and `pos`. The directories contain negative and positive
    reviews, respectively. This represents the annotation of the reviews; that is,
    a human annotator’s opinion of whether the review was positive or negative. This
    directory structure is a common approach for representing text classification
    annotations, and you’ll see it in many datasets. The `README` file in the `movie_reviews`
    folder explains some details on how the annotation was done.'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at some of the movie reviews in the corpus, you’ll see that the
    correct annotation for a text is not always obvious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code block shows importing the reviews and printing one sentence from
    the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Since `movie_reviews` is an NLTK corpus, a number of corpora methods are available,
    including listing the sentences as an array of individual sentences. We can also
    select individual sentences from the corpus by number, as shown in the previous
    code block, where we selected and printed sentence number nine in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the sentences have been tokenized, or separated into individual
    words (including punctuation marks). This is an important preparatory step in
    nearly all NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Processing one sentence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s do some actual NLP processing for this sample sentence. We’ll use
    the spaCy library to perform POS tagging and rule-based parsing and then visualize
    the results with the displaCy library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to create an `nlp` object based on web data, `en_core_web_sm`,
    which is a basic small English model. There are larger models available, but they
    take longer to load, so we will stick with the small model here for brevity. Then,
    we use the `nlp` object to identify the parts of speech and parse this sentence,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `displacy.render` command, we have requested a dependency parse (`styles=''dep''`).
    This is a type of analysis we’ll go into in more detail in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
    For now, it’s enough to say that it’s one common approach to showing how the words
    in a sentence are related to each other. The resulting dependency parse is shown
    in *Figure 4**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Dependency parse for “they get in an accident”](img/B19005_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Dependency parse for “they get in an accident”
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve loaded the corpus and looked at a few examples of the kinds of
    sentences it contains, we will look at some of the overall properties of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at corpus properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While corpora have many properties, some of the most interesting and insightful
    properties are word frequencies and POS frequencies, which we will be reviewing
    in the next two sections. Unfortunately, there isn’t space to explore additional
    corpus properties in detail, but looking at word and POS frequencies should get
    you started.
  prefs: []
  type: TYPE_NORMAL
- en: Word frequencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will look at some properties of the full corpus. For example,
    we can look at the most frequent words using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As seen in the preceding code, we start by collecting the words in the `movie_review`
    corpus by using the `words()` method of the corpus object. We then count the words
    with NLTK’s `FreqDist()` function. At the same time, we are lowercasing the words
    and ignoring non-alpha words such as numbers and punctuation. Then, for clarity
    in the visualization, we’ll restrict the words we’ll look at to the most frequent
    25 words. You may be interested in trying different values of `top_words` in the
    code block to see how the graph looks with more and fewer words.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we call `plt.show()`, the distribution of word frequencies is displayed.
    This can be seen in *Figure 4**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Visualizing the most frequent words in the movie review corpus](img/B19005_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Visualizing the most frequent words in the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 4**.5* shows, the most frequent word, not surprisingly, is **the**,
    which is about twice as frequent as the second most common word, **a**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative visualization of word frequencies that can also be helpful is
    a `all_fdist`, and displaying it with Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting word cloud is shown in *Figure 4**.6*. We can see that very frequent
    words such as **the** and **a** appear in very large fonts in comparison to other
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – A word cloud for the top 25 words in the movie review corpus](img/B19005_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – A word cloud for the top 25 words in the movie review corpus
  prefs: []
  type: TYPE_NORMAL
- en: Notice that nearly all of the frequent words are words generally used in most
    English texts. The exception is **film**, which is to be expected in a corpus
    of movie reviews. Since most of these frequent words occur in the majority of
    texts, their occurrence won’t enable us to distinguish different categories of
    texts. If we’re dealing with a classification problem such as SA, we should consider
    removing these common words from texts before we try to train an SA classifier
    on this corpus. These kinds of words are called **stopwords**, and their removal
    is a common preprocessing step. We will discuss stopword removal in detail in
    [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107)*.*
  prefs: []
  type: TYPE_NORMAL
- en: POS frequencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also look at the most frequent parts of speech with the following code.
    To reduce the complexity of the graph, we’ll restrict the display to the 18 most
    common parts of speech. After we tag the words, we loop through the sentences,
    counting up the occurrences of each tag. Then, the list of tags is sorted with
    the most frequent tags first.
  prefs: []
  type: TYPE_NORMAL
- en: The parts of speech used in the NLTK POS tagging code are the widely used Penn
    Treebank parts of speech, documented at [https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html](https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html).
    This tagset includes 36 tags overall. Previous work on NLP has found that the
    traditional English parts of speech (noun, verb, adjective, adverb, conjunction,
    interjection, pronoun, and preposition) are not fine-grained enough for computational
    purposes, so additional parts of speech are normally added. For example, different
    forms of verbs, such as *walk*, *walks*, *walked*, and *walking*, are usually
    assigned different parts of speech. For example, *walk* is assigned the *VB*—or
    *Verb base* form—POS, but *walks* is assigned the *VBZ*—or *Verb, third - person
    singular present*—POS. Traditionally, these would all be called *verbs*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll extract the sentences from the corpus, and then tag each word
    with its part of speech. It’s important to perform POS tagging on an entire sentence
    rather than just individual words, because many words have multiple parts of speech,
    and the POS assigned to a word depends on the other words in its sentence. For
    example, *book* at the beginning of *book a flight* can be recognized and tagged
    as a verb, while in *I read the book*, *book* can be tagged as a noun. In the
    following code, we display the frequencies of the parts of speech in this corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the movie review corpus, we can see that by far the most common tag is *NN*
    or *common noun*, followed by *IN* or *preposition or coordinating conjunction*,
    and *DT* or *determiner*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result, again using the Matplotlib and Seaborn libraries, is shown graphically
    in *Figure 4**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Visualizing the most frequent parts of speech in the movie review
    corpus](img/B19005_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Visualizing the most frequent parts of speech in the movie review
    corpus
  prefs: []
  type: TYPE_NORMAL
- en: We can look at other text properties such as the distribution of the lengths
    of the texts, and we can compare the properties of the positive and negative reviews
    to see if we can find some properties that distinguish the two categories. Do
    positive and negative reviews have different average lengths or different distributions
    of parts of speech? If we notice some differences, then we can make use of them
    in classifying new reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the major development tools and Python libraries
    that are used in NLP application development. We discussed the JupyterLab development
    environment and the GitHub software repository system. The major libraries that
    we covered were NLTK, spaCy, and Keras. Although this is by no means an exhaustive
    list of NLP libraries, it’s sufficient to get a start on almost any NLP project.
  prefs: []
  type: TYPE_NORMAL
- en: We covered installation and basic usage for the major libraries, and we provided
    some suggested tips on selecting libraries. We summarized some useful auxiliary
    packages, and we concluded with a simple example of how the libraries can be used
    to do some NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The topics discussed in this chapter have given you a basic understanding of
    the most useful Python packages for NLP, which you will be using for the rest
    of the book. In addition, the discussion in this chapter has given you a start
    on understanding the principles for selecting tools for future projects. We have
    achieved our goal of getting you set up with tools for processing natural language,
    along with an illustration of some simple text processing using NLTK and spaCy
    and visualization with Matplotlib and Seaborn.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to identify and prepare data for processing
    with NLP techniques. We will discuss data from databases, the web, and other documents,
    as well as privacy and ethics considerations. For readers who don’t have access
    to their own data or who wish to compare their results to those of other researchers,
    this chapter will also discuss generally available corpora. It will then go on
    to discuss preprocessing steps such as tokenization, stemming, stopword removal,
    and lemmatization.
  prefs: []
  type: TYPE_NORMAL
