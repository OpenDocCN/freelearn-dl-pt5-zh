- en: Deep Autoencoders
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: This chapter introduces the concept of deep belief networks and the significance
    of this type of deep unsupervised learning. It explains such concepts by introducing
    deep autoencoders along with two regularization techniques that can help create
    robust models. These regularization techniques, batch normalization and dropout,
    have been known to facilitate the learning of deep models and have been widely
    adopted. We will demonstrate the power of a deep autoencoder on MNIST and on a
    much harder dataset known as CIFAR-10, which contains color images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了深度信念网络的概念及这种深度无监督学习方式的重要性。通过引入深度自编码器以及两种有助于创建稳健模型的正则化技术，来解释这些概念。这些正则化技术——批量归一化和丢弃法，已被证明能促进深度模型的学习，并广泛应用。我们将在MNIST数据集和一个更具挑战性的彩色图像数据集CIFAR-10上展示深度自编码器的强大能力。
- en: By the end of this chapter, you will appreciate the benefits of making deep
    belief networks by observing the ease of modeling and quality of the output that
    they provide. You will be able to implement your own deep autoencoder and prove
    to yourself that deeper models are better than shallow models for most tasks.
    You will become familiar with batch normalization and dropout strategies for optimizing
    models and maximizing performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够体会到构建深度信念网络的好处，观察到它们在建模和输出质量方面的优势。你将能够实现自己的深度自编码器，并证明对于大多数任务，深层模型比浅层模型更优。你将熟悉批量归一化和丢弃法策略，以优化模型并最大化性能。
- en: 'This chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结构如下：
- en: Introducing deep belief networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍深度信念网络
- en: Making deep autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度自编码器
- en: Exploring latent spaces with deep autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度自编码器探索潜在空间
- en: Introducing deep belief networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度信念网络
- en: In machine learning, there is a field that is often discussed when talking about
    **deep learning** (**DL**), called **deep belief networks** (**DBNs**) (Sutskever,
    I., and Hinton, G. E. (2008)). Generally speaking, this term is used also for
    a type of machine learning model based on graphs, such as the well-known **Restricted
    Boltzmann Machine**. However, DBNs are usually regarded as part of the DL family,
    with deep autoencoders as one of the most notable members of that family.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，有一个领域在讨论**深度学习**（**DL**）时常常被提及，叫做**深度信念网络**（**DBNs**）（Sutskever, I. 和
    Hinton, G. E. (2008)）。一般而言，这个术语也用于指基于图的机器学习模型，例如著名的**限制玻尔兹曼机**。然而，DBNs通常被视为深度学习家族的一部分，其中深度自编码器是该家族中最为突出的成员之一。
- en: Deep autoencoders are considered DBNs in the sense that there are latent variables
    that are only visible to single layers in the forward direction. These layers
    are usually many in number compared to autoencoders with a single pair of layers.
    One of the main tenets of DL and DBNs in general is that during the learning process,
    there is different knowledge represented across different sets of layers. This
    knowledge representation is learned by *feature learning* without a bias toward
    a specific class or label. Furthermore, it has been demonstrated that such knowledge
    appears to be hierarchical. Consider images, for example; usually, layers closer
    to the input layer learn features that are of low order (that is, edges), while
    deeper layers learn higher-order features, that is, well-defined shapes, patterns,
    or objects (Sainath, T. N., et.al. (2012)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器被视为DBNs的一种形式，因为在前向传播过程中，有些潜在变量仅对单层可见。这些层的数量通常比单层自编码器要多。深度学习（DL）和DBNs的一项核心原则是，在学习过程中，不同层次之间代表着不同的知识。这些知识表示是通过*特征学习*获得的，并且没有偏向特定的类别或标签。此外，研究表明，这种知识表现出层次结构。例如，考虑图像；通常，靠近输入层的层学习的是低阶特征（如边缘），而更深的层学习的是高阶特征，即明确的形状、模式或物体（Sainath,
    T. N. 等人（2012））。
- en: In DBNs, as in most DL models, the interpretability of the feature space can
    be difficult. Usually, looking at the weights of the first layer can offer information
    about the features learned and or the looks of the feature maps; however, due
    to high non-linearities in the deeper layers, interpretability of the feature
    maps has been a problem and careful considerations need to be made (Wu, K., et.al. (2016)).
    Nonetheless, despite this, DBNs are showing excellent results in feature learning.
    In the next few sections, we will cover deeper versions of autoencoders on highly
    complex datasets. We will be introducing a couple of new types of layers into
    the mix to demonstrate how deep a model can be.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度信念网络（DBN）中，与大多数深度学习模型一样，特征空间的可解释性可能很困难。通常，查看第一层的权重可以提供有关学习的特征和/或特征图外观的信息；然而，由于更深层中的高非线性，特征图的可解释性一直是一个问题，需要谨慎考虑（Wu,
    K., 等人（2016））。尽管如此，DBN在特征学习方面仍然表现出色。在接下来的几节中，我们将介绍在高度复杂数据集上的更深层版本的自编码器。我们将引入几种新型的层，以展示模型可以有多深。
- en: Making deep autoencoders
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度自编码器
- en: An autoencoder can be called *deep* so long as it has more than one pair of
    layers (an encoding one and a decoding one). Stacking layers on top of each other
    in an autoencoder is a good strategy to improve its power for feature learning
    in finding unique latent spaces that can be highly discriminatory in classification
    or regression applications. However, in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    we covered how to stack layers onto an autoencoder, and we will do that again,
    but this time we will use a couple of new types of layers that are beyond the
    dense layers we have been using. These are the **batch normalization** and **dropout **layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 只要自编码器有不止一对层（一个编码层和一个解码层），它就可以被称为*深度*自编码器。在自编码器中堆叠层是提高其特征学习能力的好策略，能够找到在分类或回归应用中具有高度判别性的独特潜在空间。然而，在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml)《自编码器》中，我们已经介绍了如何在自编码器上堆叠层，我们将再次这样做，但这次我们将使用一些新的层类型，这些层超出了我们之前使用的全连接层。它们是**批量归一化**和**丢弃**层。
- en: There are no neurons in these layers; however, they act as mechanisms that have
    very specific purposes during the learning process that can lead to more successful
    outcomes by means of preventing overfitting or reducing numerical instabilities.
    Let's talk about each of these and then we will continue to experiment with both
    of these on a couple of important datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层中没有神经元；然而，它们作为具有非常具体目的的机制，在学习过程中发挥作用，通过防止过拟合或减少数值不稳定性，能够带来更成功的结果。让我们讨论一下这些层，然后我们将在几个重要的数据集上继续实验这两种层。
- en: Batch normalization
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: 'Batch normalization has been an integral part of DL since it was introduced
    in 2015 (Ioffe, S., and Szegedy, C. (2015)). It has been a major game-changer
    because it has a couple of nice properties:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化自2015年被引入深度学习（Ioffe, S., 和 Szegedy, C. (2015)）以来，已经成为深度学习的一个重要组成部分。它是一项重大变革，因为它具有几个优点：
- en: It can prevent the problem known as **vanishing gradient** or **exploding gradient**,
    which is very common in recurrent networks (Hochreiter, S. (1998)).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以防止被称为**消失梯度**或**爆炸梯度**的问题，这在递归网络中非常常见（Hochreiter, S. (1998)）。
- en: It can lead to faster training by acting as a regularizer to the learning model
    (Van Laarhoven, T. (2017)).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过充当学习模型的正则化器，从而加快训练过程（Van Laarhoven, T. (2017)）。
- en: 'A summary of these properties and the block image we will use to denote batch
    normalization are shown in *Figure 8.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性的总结以及我们将用于表示批量归一化的块图像显示在*图8.1*中：
- en: '![](img/37f526d5-f4cd-42bf-b81a-c1f884b173bb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37f526d5-f4cd-42bf-b81a-c1f884b173bb.png)'
- en: Figure 8.1 – Batch normalization layer main properties
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 批量归一化层的主要属性
- en: The authors of *batch norm*, as it is often called by data scientists, introduced
    this simple mechanism to accelerate training or model convergence by providing
    stability to the calculation of gradients and how they affect the update of the
    weights across different layers of neurons. This is because they can prevent gradient
    vanishing or explosion, which is a natural consequence of gradient-based optimization
    operating on DL models. That is, the deeper the model is, the way the gradient
    affects the layers and individual units in deeper layers can have the effect of
    large updates or very small updates that can lead to variable overflow or numerical-zero
    values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated at the top of *Figure 8.2*, batch normalization has the ability
    to regulate the boundaries of the input data by normalizing the data that goes
    in so that the output follows a normal distribution. The bottom of the figure
    illustrates where batch normalization is applied, that is, within the neuron right
    before sending the output to the next layer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f3dd44-a3c2-4505-8287-996983c125dc.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Batch normalization on a simple autoencoder
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider having a (mini-)batch of data, ![](img/4e0de339-6188-4267-a7d6-f8310f64141f.png),
    of size ![](img/f575e97f-0ad0-4d83-b281-0f35baff870a.png), which allows us to
    define the following equations. First, the mean of the batch, at layer ![](img/e5cd3a66-efd6-4e6f-9fce-a8fdd940b0bd.png),
    can be calculated as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30413e7c-0c75-4ebe-9c23-f8005cae11eb.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding standard deviation is calculated as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdb6c81f-6cb2-4c00-9901-f0389a4bc6c1.png).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can normalize every unit ![](img/76f8290e-3700-4064-9e45-9d2ee62a754e.png) in
    layer ![](img/cb840e84-cf93-45a2-a7b2-29bb2eae0783.png) as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0fa2623-dd01-4b71-88d1-5d93f35557dc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/1227909d-af4a-4fac-b9ec-397992525707.png) is a constant introduced
    merely for numerical stability, but can be altered as needed. Finally, the normalized
    neural output of every unit ![](img/90c9d851-81ed-4cda-8c3e-7f34a7552b23.png) in
    layer ![](img/474e8170-88cb-4656-9737-d964b318a040.png), ![](img/db17d453-c78e-44b6-9c54-265c14cb0674.png),
    can be calculated before it goes into the activation function as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3443d3f0-ccf0-422c-81a3-f8af404ca4c8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/4cb094b3-7c28-451f-bce4-1cd64bdd67e1.png) and ![](img/1ac6ba44-eb6b-43d4-aac8-e42df3047cea.png) are
    parameters that need to be learned for each neural unit. After this, any choice
    of activation function at unit ![](img/899c2cb8-2ba1-4548-9d22-37a526fb0f50.png) in
    layer ![](img/6e5c045c-0614-444b-acd3-7110b61ada78.png) will receive the normalized
    input, [![](img/c8199dba-4ba0-4160-99f1-f26f0912be69.png)], and produce an output
    that is optimally normalized to minimize the loss function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'One easy way to look at the benefits is to imagine the normalization process:
    although it occurs at each unit, the learning process itself determines the best
    normalization that is required to maximize the performance of the model (loss
    minimization). Therefore, it has the capability to nullify the effects of the
    normalization if it is not necessary for some feature or latent spaces, or it
    can also use the normalization effects. The important point to remember is that,
    when batch normalization is used, the learning algorithm will learn to use normalization
    optimally.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看收益的一种简单方法是想象归一化过程：虽然它发生在每个单元上，但学习过程本身决定了所需的最佳归一化方式，以最大化模型的性能（最小化损失）。因此，它能够在某些特征或潜在空间中，如果归一化不必要，就消除归一化的效果，或者它也可以利用归一化的效果。需要记住的一个重要点是，当使用批量归一化时，学习算法会学会如何最优化地使用归一化。
- en: 'We can use `tensorflow.keras.layers.BatchNormalization` to create a batch normalization
    layer as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tensorflow.keras.layers.BatchNormalization`来创建一个批量归一化层，如下所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is obviously done using the functional paradigm. Consider the following
    example of a dataset corresponding to movie reviews, called *IMDb* (Maas, A. L.,
    et al. (2011)), which we will explain in more detail in [Chapter 13](a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml),
    *Recurrent Neural Networks*. In this example, we are simply trying to prove the
    effects of adding a batch normalization layer as opposed to not having one. Take
    a close look at the following code fragment:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是使用函数式编程范式完成的。考虑以下示例，它是一个关于电影评论的数据集，名为*IMDb*（Maas, A. L., 等人，2011），我们将在[第13章](a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml)《循环神经网络》中详细解释。在这个例子中，我们只是尝试证明添加批量归一化层与不添加的效果。仔细看看下面的代码片段：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And we proceed with building the model:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们继续构建模型：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this code fragment, batch normalization is placed right before the activation
    layer. This will, therefore, normalize the input to the activation function, which
    in this case is a `sigmoid.` Similarly, we can build the same model without a
    batch normalization layer as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，批量归一化层被放置在激活层之前。因此，它将归一化输入到激活函数，在本例中是`sigmoid`。类似地，我们也可以构建一个不带批量归一化层的相同模型，如下所示：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we train both models and plot their performance as they minimize the loss
    function, we will notice quickly that having batch normalization pays off, as
    shown in *Figure 8.3*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练这两个模型，并绘制它们在最小化损失函数时的表现，我们会很快注意到，使用批量归一化会带来显著的效果，如*图 8.3*所示：
- en: '![](img/12072ed9-ec0b-4784-a47d-b726124dc480.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12072ed9-ec0b-4784-a47d-b726124dc480.png)'
- en: Figure 8.3 – Comparison of learning progress with and without batch normalization
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 带有和不带批量归一化的学习进度比较
- en: The figure indicates that having batch normalization has the effect of reducing
    the loss function both in training and in validation sets of data. These results
    are consistent with many other experiments that you can try on your own! However,
    as we said before, it is not necessarily a guarantee that this will happen all
    the time. This is a relatively modern technique that has proven to function properly
    so far, but this does not mean that it works for everything that we know of.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图表表明，使用批量归一化的效果是在训练集和验证集的损失函数都减少。这些结果与许多你可以自己尝试的实验是一致的！然而，正如我们之前所说的，这并不一定意味着每次都会发生这种情况。这是一种相对现代的技术，迄今为止已证明它能正常工作，但这并不意味着它对我们已知的所有情况都有效。
- en: We highly recommend that in all your models, you first try to solve the problem
    with a model that has no batch normalization, and then once you feel comfortable
    with the performance you have, come back and use batch normalization to see if
    you can get a slight boost in **performance** and **training speed**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议在所有模型中，首先尝试使用没有批量归一化的模型来解决问题，然后在你对现有性能感到满意时，再回来使用批量归一化，看看是否能略微提升**性能**和**训练速度**。
- en: Let's say that you tried batch normalization and you were rewarded with a boost
    in performance, speed, or both, but you have now discovered that your model was
    overfitting all this time. Fear not! There is another interesting and novel technique,
    known as **dropout**. This can offer a model an alternative to reduce overfitting,
    as we will discuss in the following section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你尝试了批量归一化，并且得到了性能、速度或两者的提升，但现在你发现模型一直在过拟合。别担心！还有一种有趣且新颖的技术，叫做**随机失活**。正如我们在接下来的部分所讨论的，它可以为模型提供一种减少过拟合的替代方法。
- en: Dropout
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机失活
- en: 'Dropout is a technique published in 2014 that became popular shortly after
    that year (Srivastava, N., Hinton, G., et.al. (2014)). It came as an alternative
    to combat overfitting, which is one of its major properties, and can be summarized
    as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: It can reduce the chances of overfitting.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can lead to better generalization.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can reduce the effect of dominating neurons.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can promote neuron diversity.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can promote better neuron teamwork.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The block image we will use for dropout, along with its main properties, is
    shown in *Figure 8.4*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20c83967-c1c8-4cbc-a56d-0702da606ca5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Dropout layer properties
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'A dropout strategy works because it enables the network to search for an alternative
    hypothesis to solve the problem by disconnecting a particular number of neurons
    that represent certain hypotheses (or models) within a network itself. One easy
    way to look at this strategy is by thinking about the following: Imagine that
    you have a number of experts that are tasked with passing judgment on whether
    an image contains a cat or a chair. There might be a large number of experts that
    moderately believe that there is a chair in the image, but it only takes one expert
    to be particularly loud and fully convinced that there is a cat to persuade the
    decision-maker into listening to this particularly loud expert and ignoring the
    rest. In this analogy, experts are neurons.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'There might be some neurons that are particularly convinced (sometimes incorrectly,
    due to overfitting on irrelevant features) of a certain fact about the information,
    and their output values are particularly high compared to the rest of the neurons
    in that layer, so much so that the deeper layers learn to listen more to that
    particular layer, thus perpetuating overfitting on deeper layers. **Dropout**
    is the mechanism that will select a number of neurons in a layer and completely
    disconnect them from the layer so that no input flows into those neurons nor is
    there output coming out of those neurons, as shown in *Figure 8.5*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b2a57b2-b790-49fc-9060-b8f35c0a5ced.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Dropout mechanism over the first hidden layer. Dropout here disconnects
    one neuron from the layer
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the first hidden layer has a dropout rate of one
    third. This means that, completely at random, one third of the neurons will be
    disconnected. *Figure 8.5* shows an example of when the second neuron in the first
    hidden layer is disconnected: no input from the input layer goes in, and no output
    comes out of it. The model is completely oblivious to its existence; for all practical
    purposes, this is a different neural network!'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the neurons that are disconnected are only disconnected for one training
    step: their weights are unchanged for one training step, while all other weights
    are updated. This has a few interesting implications:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Due to the random selection of neurons, those *troublemakers* that tend to dominate
    (overfit) on particular features are bound to be selected out at some point, and
    the rest of the neurons will learn to process feature spaces without those *troublemakers*.
    This leads to the prevention and reduction of overfitting, while promoting collaboration
    among diverse neurons that are experts in different things.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于神经元的随机选择，那些倾向于主导（过拟合）某些特征的*麻烦制造者*最终会被选中，而其余的神经元将学会在没有这些*麻烦制造者*的情况下处理特征空间。这有助于防止和减少过拟合，同时促进不同神经元之间的协作，它们在不同领域具有专业知识。
- en: Due to the constant ignorance/disconnection of neurons, the network has the
    potential of being fundamentally different – it is almost as if we are training
    multiple neural networks in every single step without actually having to make
    many different models. It all happens because of dropout.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于神经元的持续忽略/断开连接，网络有可能从根本上发生变化——几乎就像我们在每一步训练中都在训练多个神经网络，而实际上并不需要创建许多不同的模型。这一切都是由于
    dropout 的原因。
- en: It is usually recommended to use dropout in deeper networks to ameliorate the
    traditional problem of overfitting that is common in DL.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议在更深的网络中使用 dropout，以改善深度学习中常见的过拟合问题。
- en: 'To show the difference in performance when using dropout, we will use the exact
    same dataset as in the previous section, but we will add an additional layer in
    the autoencoder as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示使用 dropout 时性能的差异，我们将使用与上一节相同的数据集，但我们将在自编码器中添加一个额外的层，如下所示：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code, the dropout rate is 10%, meaning that 10% of the neurons in the
    dense layer `e14` are disconnected multiple times at random during training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，dropout 率为 10%，意味着在训练过程中，`e14` 密集层中 10% 的神经元会被随机断开连接多次。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The decoder is left exactly the same as before, and the baseline model simply
    does not contain a dropout layer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与之前完全相同，基线模型仅不包含 dropout 层：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we choose `''adagrad''` and we perform training over 100 epochs and compare
    the performance results, we can obtain the performance shown in *Figure 8.6*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择`'adagrad'`并在 100 个 epoch 上进行训练并比较性能结果，我们可以获得*图 8.6*中所示的性能：
- en: '![](img/e582e979-2bb1-45a8-989d-454a99610665.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e582e979-2bb1-45a8-989d-454a99610665.png)'
- en: Figure 8.6 – Autoencoder reconstruction loss comparing models with dropout and
    without
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 比较带有 dropout 和不带 dropout 的模型的自编码器重建损失
- en: 'Here is the full code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整代码：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we define the model with dropout like so:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们像这样定义带有 dropout 的模型：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we compile it, train it, store the training history, and clear the variables
    to re-use them as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对其进行编译、训练，存储训练历史记录，并清除变量以便重新使用，如下所示：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And then we do the same for a model without dropout:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对一个不带 dropout 的模型进行相同的操作：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we gather the training data and plot it like so:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们收集训练数据并像这样绘制它：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From *Figure 8.6* we can see that the performance of the model with dropout
    is superior than without. This suggests that training without dropout has a higher
    chance of overfitting, the reason being that the learning curve is worse on the
    validation set when dropout is not used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 8.6*中我们可以看到，带有 dropout 的模型表现优于不带 dropout 的模型。这表明，训练时没有使用 dropout 更容易发生过拟合，原因是当不使用
    dropout 时，验证集上的学习曲线较差。
- en: As mentioned earlier, the `adagrad` optimizer has been chosen for this particular
    task. We made this decision because it is important for you to learn more optimizers,
    one at a time. Adagrad is an adaptive algorithm; it performs updates with respect
    to the frequency of features (Duchi, J., et al. (2011)). If features occur frequently,
    the updates are small, while larger updates are done for features that are out
    of the ordinary.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`adagrad` 优化器已被选择用于此任务。我们做出这个决定是因为你应该逐步学习更多的优化器。Adagrad 是一种自适应算法；它根据特征的频率来进行更新（Duchi,
    J. 等人，2011）。如果某个特征出现频率较高，更新就会较小，而对于那些不常见的特征，则会进行较大的更新。
- en: It is recommended to use Adagrad when the **dataset is sparse**. For example,
    in word embedding cases such as the one in this example, frequent words will cause
    small updates, while rare words will require larger updates.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当**数据集稀疏**时，建议使用 Adagrad。例如，在像本例中这样的词嵌入任务中，频繁出现的词会导致小的更新，而稀有词则需要较大的更新。
- en: Finally, it is important to mention that `Dropout(rate)` belongs to the `tf.keras.layers.Dropout`
    class. The rate that is taken as a parameter corresponds to the rate at which
    neurons will be disconnected at random at every single training step for the particular
    layer on which dropout is used.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要提到的是，`Dropout(rate)`属于`tf.keras.layers.Dropout`类。作为参数传入的rate值对应着每次训练步骤中该层的神经元将随机断开的比率。
- en: It is recommended that you use a dropout rate between **0.1 and 0.5** to achieve
    significant changes to your network's performance. And it is recommended to use
    dropout **only in deep networks**. However, these are empirical findings and your
    own experimentation is necessary.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐使用介于**0.1和0.5**之间的dropout率，以实现对网络性能的显著改进。并且建议**仅在深度网络中使用dropout**。不过，这些是经验性的发现，您需要通过自己的实验来验证。
- en: Now that we have explained these two relatively new concepts, dropout and batch
    normalization, we will create a deep autoencoder network that is relatively simple
    and yet powerful in finding latent representations that are not biased toward
    particular labels.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了这两个相对较新的概念——dropout（丢弃法）和batch normalization（批量归一化），接下来我们将创建一个相对简单但强大的深度自编码器网络，用于发现不偏向特定标签的潜在表示。
- en: Exploring latent spaces with deep autoencoders
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度自编码器探索潜在空间
- en: Latent spaces, as we defined them in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    are very important in DL because they can lead to powerful decision-making systems
    that are based on assumed rich latent representations. And, once again, what makes
    the latent spaces produced by autoencoders (and other unsupervised models) rich
    in their representations is that they are not biased toward particular labels.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间，正如我们在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml) *自编码器*中所定义的那样，在深度学习中非常重要，因为它们可以导致基于假设丰富潜在表示的强大决策系统。而且，正是由于自编码器（和其他无监督模型）产生的潜在空间不偏向特定标签，使得它们在表示上非常丰富。
- en: In [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*,
    we explored the MNIST dataset, which is a standard dataset in DL, and showed that
    we can easily find very good latent representations with as few as four dense
    layers in the encoder and eight layers for the entire autoencoder model. In the
    next section, we will take on a much more difficult dataset known as CIFAR-10,
    and then we will come back to explore the latent representation of the `IMDB`
    dataset, which we have already explored briefly in the previous sections of this
    chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml) *自编码器*中，我们探讨了MNIST数据集，这是深度学习中的标准数据集，并展示了通过仅使用四个密集层的编码器和整个自编码器模型的八层，我们能够轻松地找到非常好的潜在表示。在下一节中，我们将处理一个更为复杂的数据集——CIFAR-10，之后我们会回到探索`IMDB`数据集的潜在表示，该数据集我们在本章前面部分已经简要探讨过。
- en: CIFAR-10
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: 'In 2009, the *Canadian Institute for Advanced Research (CIFAR)* released a
    very large collection of images that can be used to train DL models to recognize
    a variety of objects. The one we will use in this example is widely known as CIFAR-10,
    since it has only 10 classes and a total of 60,000 images; *Figure 8.7* depicts
    samples of each class:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，*加拿大高级研究院（CIFAR）*发布了一个非常大的图像集合，可以用来训练深度学习模型识别各种物体。我们将在本例中使用的这个数据集被广泛称为CIFAR-10，因为它仅包含10个类别，总共有60,000张图像；*图8.7*展示了每个类别的样本：
- en: '![](img/4e66fc36-252d-4f4b-b881-690e8d57122c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e66fc36-252d-4f4b-b881-690e8d57122c.png)'
- en: Figure 8.7 – Sample images from the CIFAR-10 dataset. The number indicates the
    numeric value assigned to each class for convenience
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 来自CIFAR-10数据集的样本图像。数字表示每个类别分配的数值，方便起见。
- en: Every image in the dataset is 32 by 32 pixels using 3 dimensions to keep track
    of the color details. As can be seen from the figure, these small images contain
    other objects beyond those labeled, such as text, background, structures, landscapes,
    and other partially occluded objects, while preserving the main object of interest
    in the foreground. This makes it more challenging than MNIST, where the background
    is always black, images are grayscale, and there is only one number in every image.
    If you have never worked in computer vision applications, you may not know that
    it is exponentially more complicated to deal with CIFAR-10 compared to MNIST.
    Therefore, our models need to be more robust and deep in comparison to MNIST ones.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow and Keras, we can easily load and prepare our dataset with the
    following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code outputs the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This says that we have one-sixth of the dataset (~16%) separated for test purposes,
    while the rest is used for training. The 3,072 dimensions come from the number
    of pixels and channels: ![](img/13725260-8e91-4e8f-bb67-204f0f8917c4.png). The
    preceding code also normalizes the data from the range [0, 255] down to [0.0,
    1.0] in floating-point numbers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'To move on with our example, we will propose a deep autoencoder with the architecture
    shown in *Figure 8.8*, which will take a 3,072-dimensional input and will encode
    it down to 64 dimensions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5b1d547-5beb-43a1-8ce4-eda0b318876d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Architecture of a deep autoencoder on the CIFAR-10 dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: This architecture uses 17 layers in the encoder and 15 layers in the decoder.
    Dense layers in the diagram have the number of neurons written in their corresponding
    block. As can be seen, this model implements a series of strategic batch normalization
    and dropout strategies throughout the process of encoding the input data. In this
    example, all dropout layers have a 20% dropout rate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'If we train the model for 200 epochs using the standard `adam` optimizer and
    the standard binary cross-entropy loss, we could obtain the training performance
    shown in *Figure 8.9*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39b4e755-a820-4fe6-9432-eaf516ec6a13.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Reconstruction of the loss of the deep autoencoder model on CIFAR-10
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We define the model as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next we define the decoder portion of the model like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We put it together in an autoencoder model, compile it and train it like so:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The model performance shown in *Figure 8.9* converges nicely and loss decays
    both on the training and test sets, which implies that the model is not overfitting
    and continues to adjust the weights properly over time. To visualize the model''s
    performance on unseen data (the test set), we can simply pick samples from the
    test set at random, such as the ones in *Figure 8.10*, which produce the output
    shown in *Figure 8.11*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a502db01-f2ff-4c8f-987d-d8877f7c0e70.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Sample input from the test set of CIFAR-10
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5befa42e-5531-46a3-89c2-6fedf3830087.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Sample output (reconstructions) from the samples given in Figure
    8.10
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: You can see from *Figure 8.11* that the reconstructions correctly address the
    color spectrum of the input data. However, it is clear that the problem is much
    harder in reconstruction terms than MNIST. Shapes are blurry although they seem
    to be in the correct spatial position. A level of detail is evidently missing
    in the reconstructions. We can make the autoencoder deeper, or train for longer,
    but the problem might not be properly solved. We can justify this performance
    with the fact that we deliberately chose to find a latent representation of size
    64, which is smaller than a tiny 5 by 5 image: ![](img/861e2ca8-20e9-4276-94b4-bb24d061553c.png).
    If you think about it and reflect on this, then it is clear that it is nearly
    impossible, as 3,072 to 64 represents a 2.08% compression!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution to this would be not to make the model larger, but to acknowledge
    that the latent representation size might not be large enough to capture relevant
    details of the input to have a good reconstruction. The current model might be
    too aggressive in reducing the dimensionality of the feature space. If we use
    UMAP to visualize the 64-dimensional latent vectors in 2 dimensions, we will obtain
    the plot shown in *Figure 8.12*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9f94d24-4eb7-457f-9ae4-4d0d8dbbefe2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – UMAP two-dimensional representation of the latent vectors in the
    test set
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We have not spoken about UMAP before, but we will briefly state that this a
    ground-breaking data visualization tool that has been proposed recently and is
    starting to gain attention (McInnes, L., et al. (2018)). In our case, we simply
    used UMAP to visualize the data distribution since we are not using the autoencoder
    to encode all the way down to two dimensions. *Figure 8.12* indicates that the
    distribution of the classes is not sufficiently clearly defined so as to enable
    us to observe separation or well-defined clusters. This confirms that the deep
    autoencoder has not captured sufficient information for class separation; however,
    there are still clearly defined groups in some parts of the latent space, such
    as the clusters on the bottom middle and left, one of which is associated with
    a group of airplane images, for example. This **deep belief network **has acquired
    knowledge about the input space well enough to make out some different aspects
    of the input; for example, it knows that airplanes are quite different from frogs,
    or at least that they might appear in different conditions, that is, a frog will
    appear against green backgrounds while an airplane is likely to have blue skies
    in the background.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**)are a much better alternative for
    most computer vision and image analysis problems such as this one. We will get
    there in due time in [Chapter 12](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml),
    *Convolutional Neural Networks*. Be patient for now as we gently introduce different
    models one by one. You will see how we can make a convolutional autoencoder that
    can achieve much better performance than an autoencoder that uses fully connected
    layers. For now, we will continue with autoencoders for a little longer.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The model introduced in *Figure 8.8* can be produced using the functional approach;
    the encoder can be defined as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice that all the dropout layers have a 20% rate every time. The 17 layers
    go from mapping an input of `inpt_dim=3072` dimensions down to `ltnt_dim = 64`
    dimensions. The last activation function of the encoder is the hyperbolic tangent `tanh`,
    which provides an output in the range [-1,1]; this choice is only made for convenience
    in visualizing the latent space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the definition of the decoder is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The last layer of the decoder has a `sigmoid` activation function that maps
    back to the input space range, that is, [0.0, 1.0]. Finally, we can train the
    `autoencoder` model as previously defined using the `binary_crossentropy` loss
    and `adam` optimizer for 200 epochs like so:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The results have been previously shown in *Figures 8.9* to 8.*11*. However,
    it is interesting to revisit MNIST, but this time using a deep autoencoder, as
    we will discuss next.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `MNIST` dataset is a good example of a dataset that is less complex than
    CIFAR-10, and that can be approached with a deep autoencoder. Previously, in [Chapter
    7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml), *Autoencoders*, we discussed shallow
    autoencoders and showed that adding layers was beneficial. In this section, we
    go a step further to show that a deep autoencoder with dropout and batch normalization
    layers can perform better at producing rich latent representations. *Figure 8.13*
    shows the proposed architecture:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45e95b72-b8c6-4ea4-9c14-742b2d699963.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Deep autoencoder for MNIST
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of layers and the sequence of layers is the same as in *Figure 8.8*;
    however, the number of neurons in the dense layers and the latent representation
    dimensions have changed. The compression rate is from 784 to 2, or 0.25%:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6763900-9f9b-405e-a06e-928342690342.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – MNIST original sample digits of the test set
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'And yet, the reconstructions are very good, as shown in *Figure 8.14* and in
    *Figure 8.15*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6842fab6-7196-442d-9768-48f4f29ab489.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Reconstructed MNIST digits from the original test set shown in
    *Figure 8.14*
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The reconstructions shown in the figure show a level of detail that is very
    good, although it seems blurry around the edges. The general shape of the digits
    seems to be captured well by the model. The corresponding latent representations
    of the test set are shown in *Figure 8.16*:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3ad8424-e567-49ed-808e-d340eac53c77.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Latent representation of MNIST digits in the test set, partially
    shown in *Figure 8.14*
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding plot, we can see that there are well-defined clusters; however,
    it is important to point out that the autoencoder knows nothing about labels and
    that these clusters have been learned from the data alone. This is the power of
    autoencoders at their best. If the encoder model is taken apart and re-trained
    with labels, the model is likely to perform even better. However, for now, we
    will leave it here and continue with a type of *generative* model in [Chapter
    9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*, which
    comes next.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This intermediate chapter showed the power of deep autoencoders when combined
    with regularization strategies such as dropout and batch normalization. We implemented
    an autoencoder that has more than 30 layers! That's *deep*! We saw that in difficult
    problems a deep autoencoder can offer an unbiased latent representation of highly
    complex data, as most deep belief networks do. We looked at how dropout can reduce
    the risk of overfitting by ignoring (disconnecting) a fraction of the neurons
    at random in every learning step. Furthermore, we learned that batch normalization
    can offer stability to the learning algorithm by gradually adjusting the response
    of some neurons so that activation functions and other connected neurons don't
    saturate or overflow numerically.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should feel confident applying batch normalization and dropout
    strategies in a deep autoencoder model. You should be able to create your own
    deep autoencoders and apply them to different tasks where a rich latent representation
    is required for data visualization purposes, data compression or dimensionality
    reduction problems, and other types of data embeddings where a low-dimensional
    representation is required.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders,* will
    continue with autoencoders but from a *generative* *modeling* perspective. Generative
    models have the ability to generate data by sampling a probability density function,
    which is quite interesting. We will specifically discuss the variational autoencoder
    model as a better alternative to a deep autoencoder in the presence of noisy data.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Which regularization strategy discussed in this chapter alleviates overfitting
    in deep models?**'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Does adding a batch normalization layer make the learning algorithm have
    to learn more parameters? **'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actually, no. For every layer in which dropout is used, there will be only two
    parameters for every neuron to learn: ![](img/5db1f077-f826-4c6a-8b08-ca701527c5c0.png).
    If you do the math, the addition of new parameters is rather small.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '**What other deep belief networks are out there?**'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines, for example, are another very popular example
    of deep belief networks. [Chapter 10](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=33&action=edit), *Restricted
    Boltzmann Machines*, will cover these in more detail.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**How come deep autoencoders perform better on MNIST than on CIFAR-10?**'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Actually, we do not have an objective way of saying that deep autoencoders
    are better on these datasets. We are biased in thinking about it in terms of clustering
    and data labels. Our bias in thinking about the latent representations in *Figure
    8.12* and *Figure 8.16* in terms of labels is precluding us from thinking about
    other possibilities. Consider the following for CIFAR-10: what if the autoencoder
    is learning to represent data according to textures? Or color palettes? Or geometric
    properties? Answering these questions is key to understanding what is going on
    inside the autoencoder and why it is learning to represent the data in the way
    it does, but requires more advanced skills and time. In summary, we don''t know
    for sure whether it is underperforming or not until we answer these questions;
    otherwise, if we put on our lenses of classes, groups, and labels, then it might
    just seem that way.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sutskever, I., & Hinton, G. E. (2008). Deep, narrow sigmoid belief networks
    are universal approximators. *Neural computation*, 20(11), 2629-2636.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sainath, T. N., Kingsbury, B., & Ramabhadran, B. (2012, March). Auto-encoder
    bottleneck features using deep belief networks. In 2012 *IEEE international conference
    on acoustics, speech and signal processing (ICASSP)* (pp. 4153-4156). IEEE.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu, K., & Magdon-Ismail, M. (2016). Node-by-node greedy deep learning for interpretable
    features. *arXiv preprint* arXiv:1602.06183.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe, S., & Szegedy, C. (2015, June). Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. In *International Conference
    on Machine Learning (ICML)* (pp. 448-456).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov,
    R. (2014). Dropout: a simple way to prevent neural networks from overfitting.
    *The journal of machine learning research*, 15(1), 1929-1958.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for
    online learning and stochastic optimization. *Journal of machine learning research*,
    12(Jul), 2121-2159.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McInnes, L., Healy, J., & Umap, J. M. (2018). Uniform manifold approximation
    and projection for dimension reduction. *arXiv preprint* arXiv:1802.03426.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011,
    June). Learning word vectors for sentiment analysis. In *Proceedings of the 49th
    annual meeting of the association for computational linguistics*: *Human language
    technologies*-volume 1 (pp. 142-150). Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent
    neural nets and problem solutions. International Journal of Uncertainty, *Fuzziness
    and Knowledge-Based Systems*, 6(02), 107-116.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Laarhoven, T. (2017). L2 regularization versus batch and weight normalization.
    *arXiv preprint* arXiv:1706.05350.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
