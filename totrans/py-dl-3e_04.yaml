- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Computer Vision with Convolutional Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积网络进行计算机视觉
- en: In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047) and [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    we set high expectations for **deep learning** (**DL**) and computer vision. First,
    we mentioned the ImageNet competition, and then we talked about some of its exciting
    real-world applications, such as semi-autonomous cars. In this chapter, and the
    next two chapters, we’ll deliver on those expectations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B19627_02.xhtml#_idTextAnchor047)和[*第3章*](B19627_03.xhtml#_idTextAnchor079)中，我们对**深度学习**（**DL**）和计算机视觉设定了很高的期望。首先，我们提到了ImageNet竞赛，然后讨论了它的一些令人兴奋的现实世界应用，例如半自动驾驶汽车。在本章及接下来的两章中，我们将实现这些期望。
- en: Vision is arguably the most important human sense. We rely on it for almost
    any action we take. But image recognition has (and in some ways still is), for
    the longest time, been one of the most difficult problems in computer science.
    Historically, it’s been very difficult to explain to a machine what features make
    up a specified object, and how to detect them. But, as we’ve seen, in DL, a **neural
    network** (**NN**) can learn those features by itself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉可以说是人类最重要的感官。我们几乎在进行的每一个动作中都依赖于它。但图像识别（并且在某些方面仍然是）长期以来一直是计算机科学中最困难的问题之一。历史上，很难向机器解释构成特定对象的特征，以及如何检测它们。但正如我们所见，在深度学习中，**神经网络**（**NN**）可以自己学习这些特征。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Intuition and justification for **convolutional neural** **networks** (**CNNs**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）的直觉和理论依据'
- en: Convolutional layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: The structure of a convolutional network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络的结构
- en: Classifying images with PyTorch and Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch和Keras对图像进行分类
- en: Advanced types of convolutions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的高级类型
- en: Advanced CNN models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级CNN模型
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and Keras.
    If you don’t have an environment set up with these tools, fret not – the example
    is available as a Jupyter Notebook on Google Colab. You can find the code examples
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用Python、PyTorch和Keras来实现这个示例。如果你还没有设置这些工具的环境，不必担心——这个示例已经作为Jupyter Notebook在Google
    Colab上提供。你可以在本书的GitHub仓库中找到代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter04)。
- en: Intuition and justification for CNNs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的直觉和理论依据
- en: The information we extract from sensory inputs is often determined by their
    context. With images, we can assume that nearby pixels are closely related, and
    their collective information is more relevant when taken as a unit. Conversely,
    we can assume that individual pixels don’t convey information related to each
    other. For example, to recognize letters or digits, we need to analyze the dependency
    of pixels close by because they determine the shape of the element. In this way,
    we could figure out the difference between, say, a 0 or a 1\. The pixels in an
    image are organized in a two-dimensional grid, and if the image isn’t grayscale,
    we’ll have a third dimension for the color channels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从感官输入中提取的信息通常取决于它们的上下文。对于图像，我们可以假设相邻的像素是密切相关的，当将它们作为一个整体来看时，它们的集合信息更为重要。相反，我们可以假设单独的像素并不传递相互之间相关的信息。例如，在识别字母或数字时，我们需要分析相邻像素之间的依赖关系，因为它们决定了元素的形状。通过这种方式，我们能够区分，例如，0和1之间的区别。图像中的像素被组织成二维网格，如果图像不是灰度图，我们还会有一个用于颜色通道的第三维度。
- en: 'Alternatively, a **magnetic resonance image** (**MRI**) also uses three-dimensional
    space. You might recall that, until now, if we wanted to feed an image to an NN,
    we had to reshape it from a two-dimensional array into a one-dimensional array.
    CNNs are built to address this issue: how to make information about units that
    are closer more relevant than information coming from units that are further apart.
    In visual problems, this translates into making units process information coming
    from pixels that are near to one another. With CNNs, we’ll be able to feed one-,
    two-, or three-dimensional inputs and the network will produce an output of the
    same dimensionality. As we’ll see later, this will give us several advantages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，**磁共振成像**（**MRI**）也使用三维空间。你可能还记得，直到现在，如果我们想要将图像输入到神经网络中，我们必须将它从二维数组转换为一维数组。卷积神经网络就是为了解决这个问题而构建的：如何使得来自更近单元的信息比来自更远单元的信息更相关。在视觉问题中，这意味着让单元处理来自彼此接近的像素的信息。通过
    CNNs，我们将能够输入一维、二维或三维数据，网络也将输出相同维度的数据。正如我们稍后会看到的，这将为我们带来几个优势。
- en: You may recall that at the end of the previous chapter, we successfully classified
    the MNIST images (with around 98% accuracy) using an NN of `airplane`, `automobile`,
    `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, and `truck`. Had we tried
    to classify CIFAR-10 with an FC NN with one or more hidden layers, its validation
    accuracy would have been just around 50% (trust me, we did just that in the previous
    edition of this book). Compared to the MNIST result of nearly 98% accuracy, this
    is a dramatic difference, even though CIFAR-10 is also a toy problem. Therefore,
    FC NNs are of little practical use for computer vision problems. To understand
    why, let’s analyze the first hidden layer of our hypothetical CIFAR-10 network,
    which has 1,000 units. The input size of the image is
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在上一章的结尾，我们成功地对 MNIST 图像进行了分类（准确率约为 98%），使用的神经网络包括了`飞机`、`汽车`、`鸟`、`猫`、`鹿`、`狗`、`青蛙`、`马`、`船`和`卡车`。如果我们尝试使用一个具有一个或多个隐藏层的全连接神经网络（FC
    NN）来对 CIFAR-10 进行分类，其验证准确率大概会只有 50%左右（相信我，我们在本书的上一版中确实这么做过）。与接近 98% 准确率的 MNIST
    结果相比，这是一个显著的差异，即使 CIFAR-10 也是一个简单的玩具问题。因此，全连接神经网络对于计算机视觉问题的实际应用价值较小。为了理解原因，我们来分析一下我们假设中的
    CIFAR-10 网络的第一个隐藏层，该层有 1,000 个单元。图像的输入大小是
- en: 32 * 32 * 3 = 3,072\. Therefore, the first hidden layer had a total of 2,072
    * 1,000 = 2,072,000 weights. That’s no small number! Not only is it easy to overfit
    such a large network, but it’s also memory inefficient.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 32 * 32 * 3 = 3,072。因此，第一个隐藏层总共有 2,072 * 1,000 = 2,072,000 个权重。这可不是一个小数字！不仅如此，这么大的网络容易过拟合，而且在内存上也效率低下。
- en: 'Even more important, each input unit (or pixel) is connected to every unit
    in the hidden layer. Because of this, the network cannot take advantage of the
    spatial proximity of the pixels since it doesn’t have a way of knowing which pixels
    are close to each other. In contrast, CNNs have properties that provide an effective
    solution to these problems:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更为重要的是，每个输入单元（或像素）都与隐藏层中的每个单元相连。正因如此，网络无法利用像素的空间接近性，因为它无法知道哪些像素是彼此接近的。相比之下，卷积神经网络（CNNs）具有一些特性，能够有效地解决这些问题：
- en: They connect units that only correspond to neighboring pixels of the image.
    In this way, the units are “forced” to only take input from other units that are
    spatially close. This also reduces the number of weights since not all units are
    interconnected.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们仅连接与图像相邻像素对应的单元。通过这种方式，这些单元被“迫使”只从空间上接近的其他单元那里获取输入。这样也减少了权重的数量，因为并非所有单元都是互相连接的。
- en: CNNs use parameter sharing. In other words, a limited number of weights are
    shared among all units in a layer. This further reduces the number of weights
    and helps fight overfitting. It might sound confusing, but it will become clear
    in the next section.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）使用参数共享。换句话说，层中的所有单元共享有限数量的权重。这进一步减少了权重的数量，并有助于防止过拟合。虽然这可能听起来有些混乱，但在下一节中会变得更加清晰。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we’ll discuss CNNs in the context of computer vision, because
    computer vision is their most common application. However, CNNs are successfully
    applied in areas such as speech recognition and **natural language processing**
    (**NLP**). Many of the explanations we’ll describe here are also valid for those
    areas – that is, the principles of CNNs are the same regardless of the field of
    use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在计算机视觉的背景下讨论CNN，因为计算机视觉是其最常见的应用。然而，CNN也成功应用于语音识别和**自然语言处理**（**NLP**）等领域。我们在此描述的许多解释同样适用于这些领域——即，无论应用领域如何，CNN的原理都是相同的。
- en: To understand CNNs, we’ll first discuss their basic building blocks. Once we’ve
    done this, we’ll show you how to assemble them in a full-fledged NN. Then, we’ll
    demonstrate that such a network is good enough to classify the CIFAR-10 with high
    accuracy. Finally, we’ll discuss advanced CNN models, which can be applied to
    real-world computer vision tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解CNN，我们首先讨论它们的基本构建块。一旦完成这部分，我们将展示如何将它们组装成一个完整的神经网络。接着，我们将展示该网络足够好，能够以高精度分类CIFAR-10。最后，我们将讨论高级CNN模型，这些模型可以应用于实际的计算机视觉任务。
- en: Convolutional layers
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: The convolutional layer is the most important building block of a CNN. It consists
    of a set of **filters** (also known as **kernels** or **feature detectors**),
    where each filter is applied across all areas of the input data. A filter is defined
    by a **set of** **learnable weights**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是卷积神经网络（CNN）最重要的组成部分。它由一组**滤波器**（也称为**内核**或**特征检测器**）组成，每个滤波器都应用于输入数据的所有区域。滤波器由一组**可学习的权重**定义。
- en: 'To add some meaning to this laconic definition, we’ll start with the following
    figure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给这个简洁的定义增加一些意义，我们将从以下图开始：
- en: '![Figure 4.1 – Convolution operation start](img/B19627_04_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 卷积操作开始](img/B19627_04_01.jpg)'
- en: Figure 4.1 – Convolution operation start
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 卷积操作开始
- en: The preceding figure shows a two-dimensional input layer of a CNN. For the sake
    of simplicity, we’ll assume that this is the input layer, but it can be any layer
    of the network. We’ll also assume that the input is a grayscale image, and each
    input unit represents the color intensity of a pixel. This image is represented
    by a two-dimensional tensor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了CNN的二维输入层。为了简化说明，我们假设这是输入层，但它也可以是网络的任何一层。我们还假设输入是一个灰度图像，每个输入单元代表一个像素的颜色强度。这个图像由一个二维张量表示。
- en: 'We’ll start the convolution by applying a 3×3 filter of weights (again, a two-dimensional
    tensor) in the top-left corner of the image. Each input unit is associated with
    a single weight of the filter. It has nine weights, because of the nine input
    units, but, in general, the size is arbitrary (2×2, 4×4, 5×5, and so on). The
    convolution operation is defined as the following weighted sum:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在图像的左上角应用一个3×3权重滤波器（同样是一个二维张量）来开始卷积操作。每个输入单元与滤波器的一个权重相关联。因为有九个输入单元，所以权重有九个，但一般来说，大小是任意的（例如2×2、4×4、5×5，等等）。卷积操作被定义为以下加权和：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math>](img/289.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math>](img/289.png)'
- en: Here, *row* and *col* represent the input layer position, where we apply the
    filter (*row=1* and *col=1* in the preceding figure); ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/290.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/291.png)
    are the height and width of the filter size (3×3); *i* and *j* are the filter
    indices of each filter weight, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/292.png);
    *b* is the bias weight. The group of units, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/293.png),
    which participates in the input, is called the **receptive field**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*row* 和 *col* 表示输入层的位置，在此处应用滤波器（*row=1* 和 *col=1* 在前述图中）； ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/290.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/291.png)
    是滤波器大小（3×3）的高度和宽度； *i* 和 *j* 是每个滤波器权重的索引，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml><mml:mi>j</mml></mml:mrow></mml:msub></mml:math>](img/292.png)；
    *b* 是偏置权重。参与输入的单元组，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/293.png)，参与输入的单元组称为**感受野**。
- en: We can see that in a convolutional layer, the unit activation value is defined
    in the same way as the activation value of the unit we defined in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047)
    – that is, a weighted sum of its inputs. But here, the unit takes input only from
    a limited number of input units in its immediate surroundings (the receptive field).
    This is opposed to an FC layer, where the input comes from all input units. The
    difference matters because the purpose of the filter is to highlight a specific
    feature in the input, for example, an edge or a line in an image. In the context
    of the NN, the filter output represents the activation value of a unit in the
    next layer. The unit will be active if the feature is present at this spatial
    location. In hierarchically structured data, such as images, neighboring pixels
    form meaningful shapes and objects such as an edge or a line. However, a pixel
    at one end of the image is unlikely to have a relationship with a pixel at another
    end. Because of this, using an FC layer to connect all of the input pixels with
    each output unit is like asking the network to find a needle in a haystack. It
    has no way of knowing whether an input pixel is relevant (in the immediate surroundings)
    to the output unit or not (the other end of the image). Therefore, the limited
    receptive field of the convolutional layer is better suited to highlight meaningful
    features in the input data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在卷积层中，单元的激活值与我们在[*第2章*](B19627_02.xhtml#_idTextAnchor047)中定义的单元激活值的计算方式相同——即输入的加权和。但在这里，单元的输入仅来自其周围有限数量的输入单元（感受野）。这与全连接（FC）层不同，在全连接层中，输入来自所有输入单元。这个区别很重要，因为滤波器的目的是突出输入中的某个特定特征，比如图像中的边缘或线条。在神经网络的上下文中，滤波器的输出代表下一层单元的激活值。如果该特征在此空间位置存在，单元将处于激活状态。在层次结构的数据中，如图像，邻近像素会形成有意义的形状和物体，如边缘或线条。然而，图像一端的像素与另一端的像素不太可能存在关系。因此，使用全连接层将所有输入像素与每个输出单元连接，就像让网络在大海捞针。它无法知道某个输入像素是否与输出单元相关（是否位于周围区域），还是与图像另一端的像素无关。因此，卷积层有限的感受野更适合突出输入数据中的有意义特征。
- en: 'We’ve calculated the activation of a single unit, but what about the others?
    It’s simple! For each new unit, we’ll slide the filter across the input image,
    and we’ll compute its output (the weighted sum) with each new set of input units.
    The following diagram shows how to compute the activations of the next two positions
    (one pixel to the right):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算了一个单元的激活值，但其他单元呢？很简单！对于每个新单元，我们会将滤波器滑动到输入图像上，并计算其输出（加权和），每次使用一组新的输入单元。下图展示了如何计算接下来两个位置的激活值（右移一个像素）：
- en: '![Figure 4.2 – The first three steps of a convolution operation](img/B19627_04_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 卷积操作的前三个步骤](img/B19627_04_02.jpg)'
- en: Figure 4.2 – The first three steps of a convolution operation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 卷积操作的前三个步骤
- en: 'By “slide,” we mean that the weights of the filter don’t change across the
    image. In effect, we’ll use the same nine filter weights and the single bias weight
    to compute the activations of all output units, each time with a different set
    of inputs. We call this **parameter sharing**, and we do it for two reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓“滑动”，是指滤波器的权重在整个图像上保持不变。实际上，我们会使用相同的九个滤波器权重和一个偏置权重来计算所有输出单元的激活值，每次使用不同的输入单元集。我们称之为**参数共享**，并且这样做有两个原因：
- en: By reducing the number of weights, we reduce the memory footprint and prevent
    overfitting.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少权重的数量，我们减少了内存占用并防止了过拟合。
- en: The filter highlights a specific visual feature in the image. We can assume
    that this feature is useful, regardless of its position on the image. Since we
    apply the same filter throughout the image, the convolution is translation invariant;
    that is, it can detect the same feature, regardless of its location on the image.
    However, the convolution is neither rotation-invariant (it is not guaranteed to
    detect a feature if it’s rotated) nor scale-invariant (it is not guaranteed to
    detect the same artifact in different scales).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器突出了图像中的特定视觉特征。我们可以假设该特征是有用的，无论它在图像中的位置如何。由于我们在整个图像中应用相同的滤波器，因此卷积具有平移不变性；也就是说，它可以检测到相同的特征，无论该特征在图像中的位置如何。然而，卷积既不是旋转不变的（如果特征被旋转，它不一定能检测到该特征），也不是尺度不变的（它不能保证在不同的尺度下检测到相同的特征）。
- en: To compute all output activations, we’ll repeat the sliding process until we’ve
    covered the whole input. The spatially arranged input and output units are called
    **depth slices** (**feature maps** or **channels**), implying that there is more
    than one slice. The slices, like the image, are represented by tensors. A slice
    tensor can serve as an input to other layers in the network. Finally, just as
    with regular layers, we can use an activation function, such as the **rectified
    linear unit** (**ReLU**), after each unit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算所有输出激活值，我们将重复滑动过程，直到覆盖整个输入。空间排列的输入和输出单元被称为**深度切片**（**特征图**或**通道**），意味着不仅仅有一个切片。切片和图像一样，是由张量表示的。切片张量可以作为网络中其他层的输入。最后，就像常规层一样，我们可以在每个单元后使用激活函数，如**修正线性单元**（**ReLU**）。
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It’s interesting to note that each input unit is part of the input of multiple
    output units. For example, as we slide the filter, the green unit in the preceding
    diagram will form the input of nine output units.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，每个输入单元都是多个输出单元的输入。例如，当我们滑动滤波器时，上图中的绿色单元将作为九个输出单元的输入。
- en: 'We can illustrate what we’ve learned so far with a simple example. The following
    diagram illustrates a 2D convolution with a 2×2 filter applied over a single 3×3
    slice:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的例子来说明迄今为止所学的内容。以下图示说明了如何对单个3×3切片应用2×2滤波器进行2D卷积：
- en: '![Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3
    slice for a 2×2 output slice](img/B19627_04_03.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 使用2×2滤波器对单个3×3切片进行2D卷积，以获得2×2输出切片](img/B19627_04_03.jpg)'
- en: Figure 4.3 – A 2D convolution with a 2×2 ﬁlter applied over a single 3×3 slice
    for a 2×2 output slice
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 使用2×2滤波器对单个3×3切片进行2D卷积，以获得2×2输出切片
- en: This example also shows us that the input and output feature maps have different
    dimensions. Let’s say we have an input layer with a size of `(width_i, height_i)`
    and a filter with dimensions, `(filter_w, filter_h)`. After applying the convolution,
    the dimensions of the output layer are `width_o = width_i - filter_w + 1` and
    `height_o = height_i - filter_h + 1`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子还向我们展示了输入和输出特征图的尺寸是不同的。假设我们有一个大小为`(width_i, height_i)`的输入层和一个尺寸为`(filter_w,
    filter_h)`的滤波器。应用卷积后，输出层的尺寸为`width_o = width_i - filter_w + 1`和`height_o = height_i
    - filter_h + 1`。
- en: In this example, we have `width_o = height_o = 3 – 2 + 1 =` `2`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有`width_o = height_o = 3 – 2 + 1 =` `2`。
- en: In the next section, we’ll illustrate convolutions with a simple coding example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过一个简单的编码示例来说明卷积操作。
- en: A coding example of the convolution operation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作的代码示例
- en: 'We’ve now described how convolutional layers work, but we’ll gain better intuition
    with a visual example. Let’s implement a convolution operation by applying a couple
    of filters across an image. For the sake of clarity, we’ll implement the sliding
    of the filters across the image manually and we won’t use any DL libraries. We’ll
    only include the relevant parts and not the full program, but you can find the
    full example in this book’s GitHub repository. Let’s start:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经描述了卷积层是如何工作的，但通过一个可视化的例子，我们会更好地理解。让我们通过对图像应用几个滤波器来实现卷积操作。为了清晰起见，我们将手动实现滤波器在图像上的滑动，且不使用任何深度学习库。我们只会包括相关部分，而不是完整程序，但你可以在本书的GitHub仓库中找到完整示例。让我们开始：
- en: 'Import `numpy`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`：
- en: '[PRE0]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the `conv` function, which applies the convolution across the image.
    `conv` takes two parameters, both two-dimensional numpy arrays: `image`, for the
    pixel intensities of the grayscale image itself, and the hardcoded `im_filter`,
    for the filter:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`conv`函数，它对图像应用卷积。`conv`接受两个参数，都是二维numpy数组：`image`表示灰度图像本身的像素强度，`im_filter`表示硬编码的滤波器：
- en: First, we’ll compute the output image size, which depends on the input `image`
    and `im_filter` sizes. We’ll use it to instantiate the output image, `im_c`.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将计算输出图像的大小，它取决于输入`image`和`im_filter`的大小。我们将利用它来实例化输出图像`im_c`。
- en: 'Then, we’ll iterate over all pixels of `image`, applying `im_filter` at each
    location. This operation requires four nested loops: the first two for the `image`
    dimensions and the second two for iterating over the two-dimensional filter.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对`image`的所有像素进行迭代，在每个位置应用`im_filter`。此操作需要四个嵌套循环：前两个循环处理`image`的维度，后两个循环用于迭代二维滤波器。
- en: We’ll check if any value is out of the [0, 255] interval and fix it, if necessary.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将检查是否有任何值超出[0, 255]的区间，并在必要时进行修正。
- en: 'This is shown in the following example:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下例所示：
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Apply different filters across the image. To better illustrate our point, we’ll
    use a 10×10 blur filter, as well as Sobel edge detectors, as shown in the following
    example (`image_grayscale` is the two-dimensional `numpy` array, which represents
    the pixel intensities of a grayscale image):'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上应用不同的滤波器。为了更好地说明我们的观点，我们将使用一个10×10的模糊滤波器，以及Sobel边缘检测器，如下例所示（`image_grayscale`是一个二维的`numpy`数组，表示灰度图像的像素强度）：
- en: '[PRE2]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The full program will produce the following output:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整程序将产生以下输出：
- en: '![Figure 4.4 – The ﬁrst image is the grayscale input. The second image is the
    result of a 10×10 blur ﬁlter. The third and fourth images use detectors and vertical
    Sobel edge detectors](img/B19627_04_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 第一张图是灰度输入图像。第二张图是10×10模糊滤波器的结果。第三和第四张图使用了检测器和垂直Sobel边缘检测器](img/B19627_04_04.jpg)'
- en: Figure 4.4 – The ﬁrst image is the grayscale input. The second image is the
    result of a 10×10 blur ﬁlter. The third and fourth images use detectors and vertical
    Sobel edge detectors
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 第一张图是灰度输入图像。第二张图是10×10模糊滤波器的结果。第三和第四张图使用了检测器和垂直Sobel边缘检测器
- en: In this example, we used filters with hardcoded weights to visualize how the
    convolution operation works in NNs. In reality, the weights of the filter will
    be set during the network’s training. All we’ll need to do is define the network
    architecture, such as the number of convolutional layers, the depth of the output
    volume, and the size of the filters. The network will figure out the features
    highlighted by each filter during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了带有硬编码权重的滤波器来可视化卷积操作是如何在神经网络中工作的。实际上，滤波器的权重将在网络训练过程中设置。我们只需要定义网络架构，比如卷积层的数量、输出体积的深度以及滤波器的大小。网络将在训练过程中自动确定每个滤波器突出显示的特征。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As we saw in this example, we had to implement four nested loops to implement
    the convolution. However, with some clever transformations, the convolution operation
    can be implemented with matrix-matrix multiplication. In this way, it can take
    full advantage of GPU parallelization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这个例子中看到的，我们需要实现四个嵌套循环来实现卷积。然而，通过一些巧妙的转换，卷积操作可以通过矩阵乘法实现。这样，它可以充分利用GPU并行计算。
- en: In the next few sections, we’ll discuss some of the finer details of the convolutional
    layers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论卷积层的一些细节。
- en: Cross-channel and depthwise convolutions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨通道和深度卷积
- en: 'So far, we have described the one-to-one slice relation, where we apply a single
    filter over a single input slice to produce a single output slice. But this arrangement
    is limiting for the following reasons:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了一个一对一的切片关系，其中我们在单个输入切片上应用单个滤波器，产生单个输出切片。但这种安排有以下局限性：
- en: A single input slice works well for a grayscale image, but it doesn’t work for
    color images with multiple channels or any other multi-dimensional input
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个输入切片适用于灰度图像，但对于具有多个通道的彩色图像或任何其他多维输入则不起作用
- en: A single filter can detect a single feature in the slice, but we are interested
    in detecting many different features
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个滤波器可以检测切片中的单个特征，但我们希望检测多个不同的特征
- en: 'How do we solve these limitations? It’s simple:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解决这些局限性呢？很简单：
- en: 'For the input, we’ll split the image into color channels. In the case of an
    RGB image, that would be three. We can think of each color channel as a depth
    slice, where the values are the pixel intensities for the given color (R, G, or
    B), as shown in the following example:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于输入，我们将图像分割成颜色通道。对于RGB图像来说，这将是三个通道。我们可以将每个颜色通道看作一个深度切片，其中的值是给定颜色（R、G或B）的像素强度，如下例所示：
- en: '![Figure 4.5 – An example of an input slice with a depth of 3](img/B19627_04_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 一个深度为3的输入切片示例](img/B19627_04_05.jpg)'
- en: Figure 4.5 – An example of an input slice with a depth of 3
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 一个深度为3的输入切片示例
- en: The combination of input slices is called **input volume** with a **depth**
    of 3\. An RGB image is represented by a 3D tensor of three 2D slices (one slice
    per color channel).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输入切片的组合被称为**输入体积**，深度为3。RGB图像由三层2D切片（每个颜色通道一个）组成的3D张量表示。
- en: The CNN convolution can have multiple filters, highlighting different features,
    which results in multiple output feature maps (one for each filter), combined
    in an **output volume**.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN卷积可以拥有多个滤波器，突出显示不同的特征，从而产生多个输出特征图（每个滤波器一个），这些特征图被合并成一个**输出体积**。
- en: 'Let’s say we have ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/294.png)
    input (uppercase *C*) and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/295.png)
    output slices. Depending on the relationship of the input and output slice, we
    get cross-channel and depthwise convolutions, as illustrated in the following
    diagram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/294.png)输入（大写的*C*）和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/295.png)输出切片。根据输入和输出切片的关系，我们可以得到跨通道卷积和深度卷积，如下图所示：
- en: '![Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)](img/B19627_04_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 跨通道卷积（左）；深度卷积（右）](img/B19627_04_06.jpg)'
- en: Figure 4.6 – Cross-channel convolution (left); depthwise convolution (right)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 跨通道卷积（左）；深度卷积（右）
- en: 'Let’s discuss their properties:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它们的性质：
- en: '**Cross-channel convolutions**: One output slice receives input from all input
    slices (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:math>](img/296.png)
    relationship). With multiple output slices, the relationship becomes ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/297.png).
    In other words, each input slice contributes to the output of each output slice.
    Each pair of input/output slices uses a separate filter slice that’s unique to
    that pair. Let’s denote the index of the input slice with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/298.png)
    (lowercase *c*); the index of the output slice with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/299.png);
    the dimensions of the filter with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/300.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/301.png).
    Then, the cross-channel 2D convolution of a single output cell in one of the output
    slices is defined as the following weighted sum:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/302.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Note that we have a unique bias, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math>](img/303.png)
    for each output slice.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also compute the total number of weights, *W*, in a cross-channel 2D
    convolution with the following equation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/304.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Here, *+1* represents the bias weight for each filter. Let’s say we have three
    input slices and want to apply four 5×5 filters to them. If we did this, the convolution
    filter would have a total of (3 * 5 * 5 + 1) * 4 = 304 weights, four output slices
    (an output volume with a depth of 4), and one bias per slice. The filter for each
    output slice will have three 5×5 filter patches for each of the three input slices
    and one bias for a total of 3 * 5 * 5 + 1 = 76 weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**Depthwise convolutions**: One output slice receives input from a single input
    slice. It’s a kind of reversal of the previous case. In its simplest form, we
    apply a filter over a single input slice to produce a single output slice. In
    this case, the input and output volumes have the same depth – that is, *C*. We
    can also specify a channel multiplier (an integer, *M*), where we apply *M* filters
    over a single output slice to produce *M* output slices per input slice. In this
    case, the total number of output slices is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:math>](img/305.png).
    The depthwise 2D convolution is defined as the following weighted sum:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/306.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the number of weights, *W*, in a 2D depthwise convolution with
    the following formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/307.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Here, *+M* represents the biases of each output slice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss some more properties of the convolution operation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Stride and padding in convolutional layers
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve assumed that sliding of the filter happens one pixel at a time,
    but that’s not always the case. We can slide the filter over multiple positions.
    This parameter of the convolutional layers is called **stride**. Usually, the
    stride is the same across all dimensions of the input. In the following diagram,
    we can see a convolutional layer with *stride = 2* (also called **stride convolution**):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – With stride = 2, the ﬁlter is translated by two pixels at a
    time](img/B19627_04_07.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – With *stride = 2*, the ﬁlter is translated by two pixels at a time
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'The main effect of the larger stride is an increase in the receptive field
    of the output units at the expense of the size of the output slice itself. To
    understand this, let’s recall that in the previous section, we introduced a simple
    formula for the output size, which included the sizes of the input and the kernel.
    Now, we’ll extend it to also include the stride: `width_o = (width_i - filter_w)
    / stride_w + 1` and `height_o = 1 + (height_i - filter_h) / stride_h`. For example,
    the output size of a square slice generated by a 28×28 input image, convolved
    with a 3×3 filter with *stride = 1*, would be 1 + 28 - 3 = 26\. But with *stride
    = 2*, we get 1 + (28 - 3) / 2 = 13\. Therefore, if we use *stride = 2*, the size
    of the output slice will be roughly four times smaller than the input. In other
    words, one output unit will “cover” an area, which is four times larger compared
    to the input units. The units in the following layers will gradually capture input
    from larger regions from the input image. This is important because it would allow
    them to detect larger and more complex features of the input.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution operations we have discussed so far have produced smaller output
    than the input (even with *stride = 1*). But, in practice, it’s often desirable
    to control the size of the output. We can solve this by **padding** the edges
    of the input slice with rows and columns of zeros before the convolution operation.
    The most common way to use padding is to produce output with the same dimensions
    as the input. In the following diagram, we can see a convolutional layer with
    *padding = 1* and *stride =* *1*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – A convolutional layer with padding = 1](img/B19627_04_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – A convolutional layer with padding = 1
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The white units represent the padding. The input and the output slices have
    the same dimensions (dark units). The newly padded zeros will participate in the
    convolution operation with the slice, but they won’t affect the result. The reason
    is that, even though the padded areas are connected with weights to the following
    layer, we’ll always multiply those weights by the padded value, which is 0\. At
    the same time, sliding the filter across the padded input slice will produce an
    output slice with the same dimensions as the unpadded input.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know about stride and padding, we can introduce the full formula
    for the size of the output slice:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have a basic knowledge of convolutions, and we can continue to the next
    building block of CNNs – the pooling layer. Once we know all about pooling layers,
    we’ll introduce our first full CNN, and we’ll implement a simple task to solidify
    our knowledge. Then, we’ll focus on more advanced CNN topics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we explained how to increase the receptive field of
    the units by using *stride > 1*. But we can also do this with the help of pooling
    layers. A pooling layer splits the input slice into a grid, where each grid cell
    represents a receptive field of several units (just as a convolutional layer does).
    Then, a pooling operation is applied over each cell of the grid. Pooling layers
    don’t change the volume depth because the pooling operation is performed independently
    on each slice. They are defined by two parameters: stride and receptive field
    size, just like convolutional layers (pooling layers usually don’t use padding).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss three types of pooling layers – max pooling,
    average pooling, and **global average pooling** (**GAP**). These three types of
    pooling are displayed in the following diagram:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Max, average, and global average pooling](img/B19627_04_09.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Max, average, and global average pooling
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Max pooling** is the most common way of pooling. The max pooling operation
    takes the unit with the highest activation value in each local receptive field
    (grid cell) and propagates only that value forward. In the preceding figure (left),
    we can see an example of max pooling with a receptive field of 2×2 and *stride
    = 2*. This operation discards 3/4 of the input units. Pooling layers don’t have
    any weights. In the backward pass of max pooling, the gradient is routed only
    to the unit with the highest activation during the forward pass. The other units
    in the receptive field backpropagate zeros.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**Average pooling** is another type of pooling, where the output of each receptive
    field is the mean value of all activations within the field. In the preceding
    figure (middle), we can see an example of average pooling with a receptive field
    of 2×2 and *stride =* *2*.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: GAP is similar to average pooling, but a single pooling region covers the whole
    input slice. We can think of GAP as an extreme type of dimensionality reduction
    because it outputs a single value that represents the average of the whole slice.
    This type of pooling is usually applied at the end of the convolutional portion
    of a CNN. In the preceding figure (right), we can see an example of a GAP operation.
    Stride and receptive field size don’t apply to the GAP operation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In practice, only two combinations of stride and receptive field size are used.
    The first is a 2×2 receptive field with *stride = 2*, and the second is a 3×3
    receptive field with *stride = 2* (overlapping). If we use a larger value for
    either parameter, the network loses too much information. Alternatively, if the
    stride is 1, the size of the layer wouldn’t be smaller, and nor will the receptive
    field increase.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these parameters, we can compute the output size of a pooling layer:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Pooling layers are still very much used, but often, we can achieve similar
    or better results by simply using convolutional layers with larger strides. (See,
    for example, *J. Springerberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, Striving
    for Simplicity: The All Convolutional Net, (**2015)*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: We now have sufficient knowledge to introduce our first full CNN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The structure of a convolutional network
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure shows the structure of a basic classification CNN:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling
    layers](img/B19627_04_10.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – A basic convolutional network with convolutional, FC, and pooling
    layers
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Most CNNs share basic properties. Here are some of them:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'We would typically alternate one or more convolutional layers with one pooling
    layer (or a stride convolution). In this way, the convolutional layers can detect
    features at every level of the receptive field size. The aggregated receptive
    field size of deeper layers is larger than the ones at the beginning of the network.
    This allows them to capture more complex features from larger input regions. Let’s
    illustrate this with an example. Imagine that the network uses 3×3 convolutions
    with *stride = 1* and 2×2 pooling with *stride =* *2*:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The units of the first convolutional layer will receive input from 3×3 pixels
    of the image.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A group of 2×2 output units of the first layer will have a combined receptive
    field size of 4×4 (because of the stride).
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After the first pooling operation, this group will be combined in a single unit
    of the pooling layer.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The second convolution operation takes input from 3×3 pooling units. Therefore,
    it will receive input from a square with side 3×4 = 12 (or a total of 12×12 =
    144) pixels from the input image.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the convolutional layers to extract features from the input. The features
    detected by the deepest layers are highly abstract, but they are also not readable
    by humans. To solve this problem, we usually add one or more FC layers after the
    last convolutional/pooling layer. In this example, the last FC layer (output)
    will use softmax to estimate the class probabilities of the input. You can think
    of the FC layers as translators between the network’s language (which we don’t
    understand) and ours.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deeper convolutional layers usually have more filters (hence higher volume
    depth), compared to the initial ones. A feature detector at the beginning of the
    network works on a small receptive field. It can only detect a limited number
    of features, such as edges or lines, shared among all classes. On the other hand,
    a deeper layer would detect more complex and numerous features. For example, if
    we have multiple classes such as cars, trees, or people, each will have its own
    set of features, such as tires, doors, leaves and faces, and so on. This would
    require more feature detectors.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know the structure of a CNN, let’s implement one with PyTorch and
    Keras.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with PyTorch and Keras
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll try to classify the images of the CIFAR-10 dataset with
    both PyTorch and Keras. It consists of 60,000 32x32 RGB images, divided into 10
    classes of objects. To understand these examples, we’ll first focus on two prerequisites
    that we haven’t covered until now: how images are represented in DL libraries
    and data augmentation training techniques.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers in deep learning libraries
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch, Keras, and **TensorFlow** (**TF**) have out-of-the-gate support for
    1D, 2D, and 3D convolutions. The inputs and outputs of the convolution operation
    are tensors. A 1D convolution with multiple input/output slices would have 3D
    input and output tensors. Their axes can be in either *SCW* or *SWC* order, where
    we have the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '*S*: The index of the sample in the mini-batch'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C*: The index of the depth slice in the volume'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W*: The content of the slice'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same way, a 2D convolution will be represented by *SCHW* or *SHWC* ordered
    tensors, where *H* and *W* are the height and width of the slices. A 3D convolution
    will have *SCDHW* or *SDHWC* order, where *D* stands for the depth of the slice.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most efficient regularization techniques is data augmentation. If
    the training data is too small, the network might start overfitting. Data augmentation
    helps counter this by artificially increasing the size of the training set. In
    the CIFAR-10 examples, we’ll train a CNN over multiple epochs. The network will
    “see” every sample of the dataset once per epoch. To prevent this, we can apply
    random augmentations to the images, before feeding them to train the CNN. The
    labels will stay the same. Some of the most popular image augmentations are as
    follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Rotation
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal and vertical flip
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoom in/out
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crop
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skew
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrast and brightness adjustment
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The emboldened augmentations are shown in the following example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Examples of diﬀerent image augmentations](img/B19627_04_11.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Examples of diﬀerent image augmentations
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’re ready to proceed with the examples.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with PyTorch
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start with PyTorch first:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the device, preferably a GPU. This NN is larger than the MNIST ones
    and the CPU training would be very slow:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the training dataset (followed by the validation):'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`train_transform` is of particular interest. It performs random horizontal
    and vertical flips, and it normalizes the dataset with `transforms.Normalize`
    using z-score normalization. The hardcoded numerical values represent the manually
    computed channel-wise mean and `std` values for the CIFAR-10 dataset. `train_loader`
    takes care of providing training minibatches.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the validation dataset. Note that we normalize the validation set with
    the mean and `std` values of the training dataset:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define our CNN using the `Sequential` class. It has the following properties:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Three blocks of two convolutional layers (3×3 filters) and one max pooling layer.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization after each convolutional layer.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two blocks apply `padding=1` to the convolutions, so they don’t decrease
    the size of the feature maps.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Linear` (FC) layer with 10 outputs (one of each class). The final activation
    is softmax.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see the definition:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Run the training and validation. We’ll use the same `train_model` and `test_model`
    functions that we implemented in the MNIST PyTorch example in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
    Because of this, we won’t implement them here, but the full source code is available
    in this chapter’s GitHub repository (including a Jupyter Notebook). We can expect
    the following results: 51% accuracy in 1 epoch, 70% accuracy in 5 epochs, and
    around 82% accuracy in 75 epochs.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes our PyTorch example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with Keras
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our second example is the same task, but this time implemented with Keras:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by downloading the dataset. We’ll also convert the numerical labels into
    one-hot-encoded tensors:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create an instance of `ImageDataGenerator`, which applies z-normalization over
    each channel of the training set images. It also provides data augmentation (random
    horizontal and vertical flips) during training. Also, note that we apply the mean
    and standard variation of the training over the test set for the best performance:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we can define our CNN using the `Sequential` class. We’ll use the same
    architecture we defined in the *Classifying images with PyTorch* section. The
    following is the Keras definition of that model:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the training parameters (we’ll also print the model summary for clarity):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the training for 50 epochs:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Depending on the number of epochs, this model will produce the following results:
    50% accuracy in 1 epoch, 72% accuracy in 5 epochs, and around 85% accuracy in
    45 epochs. Our Keras example has slightly higher accuracy compared to the one
    in PyTorch, although they should be identical. Maybe we’ve got a bug somewhere.
    We might never know, but we can learn a lesson, nevertheless: ML models aren’t
    easy to debug because they can fail with slightly degraded performance, instead
    of outright error. Finding the exact reason for this performance penalty can be
    hard.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve implemented our first full CNN twice, we’ll focus on some more
    advanced types of convolutions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Advanced types of convolutions
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve discussed the “classic” convolutional operation. In this section,
    we’ll introduce several new variations and their properties.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 1D, 2D, and 3D convolutions
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we’ve used **2D convolutions** because computer vision with
    two-dimensional images is the most common CNN application. But we can also have
    1D and 3D convolutions, where the units are arranged in one-dimensional or three-dimensional
    space, respectively. In all cases, the filter has the same number of dimensions
    as the input, and the weights are shared across the input. For example, we would
    use 1D convolution with time series data because the values are arranged across
    a single time axis. In the following diagram, on the left, we can see an example
    of 1D convolution:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 –1D convolution (left); 3D convolution (right)](img/B19627_04_12.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 –1D convolution (left); 3D convolution (right)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The weights with the same dashed lines share the same value. The output of the
    1D convolution is also 1D. If the input is 3D, such as a 3D MRI, we could use
    3D convolution, which will also produce 3D output. In this way, we’ll maintain
    the spatial arrangement of the input data. We can see an example of 3D convolution
    in the preceding diagram, on the right. The input has dimensions of H/W/L, and
    the filter has a single size, *F*, for all dimensions. The output is also 3D.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 1×1 convolutions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A 1×1 (pointwise) convolution is a special case of convolution where each dimension
    of the convolution filter is of size 1 (1×1 in 2D convolutions and 1×1×1 in 3D).
    At first, this doesn’t make sense – a 1×1 filter doesn’t increase the receptive
    field size of the output units. The result of such a convolution would be pointwise
    scaling. But it can be useful in another way – we can use them to change the depth
    between the input and output volumes. To understand this, let’s recall that, in
    general, we have an input volume with a depth of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/308.png)
    slices and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    filters for ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/309.png)
    output slices. Each output slice is generated by applying a unique filter over
    all the input slices. If we use a 1×1 filter and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/311.png),
    we’ll have output slices of the same size, but with different volume depths. At
    the same time, we won’t change the receptive field size between the input and
    output. The most common use case is to reduce the output volume, or ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>></mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/312.png)
    (dimension reduction), nicknamed the **“****bottleneck” layer**.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise separable convolutions
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An output slice in a cross-channel convolution receives input from all of the
    input slices using a single filter. The filter tries to learn features in a 3D
    space, where two of the dimensions are spatial (the height and width of the slice)
    and the third is the channel. Therefore, the filter maps both spatial and cross-channel
    correlations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Depthwise separable convolutions** (**DSCs**, *Xception: Deep Learning with
    Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    can completely decouple'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'cross-channel and spatial correlations. A DSC combines two operations: a depthwise
    convolution and a 1×1 convolution. In a depthwise convolution, a single input
    slice produces a single output slice, so it only maps spatial (and not cross-channel)
    correlations. With 1×1 convolutions, we have the opposite. The following diagram
    represents the DSC:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – A depth-wise separable convolution](img/B19627_04_13.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – A depth-wise separable convolution
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The DSC is usually implemented without non-linearity after the first (depthwise)
    operation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the standard and depthwise separable convolutions. Imagine that
    we have 32 input and output channels and a filter with a size of 3×3\. In a standard
    convolution, one output slice is the result of applying one filter for each of
    the 32 input slices for a total of 32 * 3 * 3 = 288
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: weights (excluding bias). In a comparable depthwise convolution, the filter
    has only 3 * 3 = 9 weights and the filter for the 1×1 convolution has 32 * 1 *
    1 = 32 weights. The total number of weights is 32 + 9 = 41\. Therefore, the depthwise
    separable convolution is faster and more memory-efficient compared to the standard
    one.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The regular convolution applies an *n×n* filter over an *n×n* receptive field.
    With dilated convolutions, we apply the same filter sparsely over a receptive
    field of size *(n * l - 1) × (n * l - 1)*, where *l* is the **dilation factor**.
    We still multiply each filter weight by one input slice cell, but these cells
    are at a distance of *l* away from each other. The regular convolution is a special
    case of dilated convolution with *l = 1*. This is best illustrated with the following
    diagram:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – A dilated convolution with a dilation factor of l=2\. Here,
    the ﬁrst two steps of the operation are displayed. The bottom layer is the input
    while the top layer is the output. Source: https://github.com/vdumoulin/conv_arithmetic](img/B19627_04_14.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14 – A dilated convolution with a dilation factor of l=2\. Here, the
    ﬁrst two steps of the operation are displayed. The bottom layer is the input while
    the top layer is the output. Source: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions can increase the receptive field’s size exponentially without
    losing resolution or coverage. We can also increase the receptive field with stride
    convolutions or pooling but at the cost of resolution and/or coverage. To understand
    this, let’s imagine that we have a stride convolution with stride *s > 1*. In
    this case, the output slice is *s* times smaller than the input (loss of resolution).
    If we increase *s > F* further (*F* is the size of either the pooling or convolutional
    kernel), we get a loss of coverage because some of the areas of the input slice
    will not participate in the output at all. Additionally, dilated convolutions
    don’t increase the computation and memory costs because the filter uses the same
    number of weights as the regular convolution.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolutions
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the convolutional operations we’ve discussed so far, the output dimensions
    are either equal or smaller than the input dimensions. In contrast, transposed
    convolutions (first proposed in *Deconvolutional Networks by Matthew D. Zeiler,
    Dilip Krishnan, Graham W. Taylor, and Rob Fergus*: [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf))
    allow us to upsample the input data (their output is larger than the input). This
    operation is also known as **deconvolution**, **fractionally strided convolution**,
    or **sub-pixel convolution**. These names can sometimes lead to confusion. To
    clarify things, note that the transposed convolution is, in fact, a regular convolution
    with a slightly modified input slice or convolutional filter.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'For the longer explanation, we’ll start with a 1D regular convolution over
    a single input and output slice:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – 1D regular convolution](img/B19627_04_15.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – 1D regular convolution
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: It uses a filter with *size = 4*, *stride = 2*, and *padding = 2* (denoted with
    gray in the preceding diagram). The input is a vector of size 6 and the output
    is a vector of size 4\. The filter, a vector, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">f</mml:mi><mml:mo>=</mml:mo><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:math>](img/313.png),
    is always the same, but it’s denoted with different colors for each position we
    apply it to. The respective output cells are denoted with the same color. The
    arrows show which input cells contribute to one output cell.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The example that is being discussed in this section is inspired by the paper
    *Is the deconvolution layer the same as a convolutional* *layer?* ([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll discuss the same example (1D, single input and output slices, and
    a filter with *size = 4*, *padding = 2*, and *stride = 2*), but for transposed
    convolution. The following diagram shows two ways we can implement it:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – A convolution with stride = 2, applied with the transposed
    ﬁlter f. The 2 pixels at the beginning and the end of the output are cropped (left);
    a convolution with stride 0.5, applied over input data, padded with subpixels.
    The input is ﬁlled with 0-valued pixels (gray) (right)](img/B19627_04_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – A convolution with stride = 2, applied with the transposed ﬁlter
    f. The 2 pixels at the beginning and the end of the output are cropped (left);
    a convolution with stride 0.5, applied over input data, padded with subpixels.
    The input is ﬁlled with 0-valued pixels (gray) (right)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss them in detail:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, we have a regular convolution with *stride = 2* and a filter
    represented as a transposed row matrix (equivalent to a column matrix) with size
    4: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mn>3,4</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/314.png)
    (shown in the preceding diagram, left). Note that the stride is applied over the
    output layer as opposed to the regular convolution, where we stride over the input.
    By setting the stride larger than 1, we can increase the output size, compared
    to the input. Here, the size of the input slice is *I*, the size of the filter
    is *F*, the stride is *S*, and the input padding is *P*. Due to this, the size,
    *O*, of the output slice of a transposed convolution is given by the following
    formula: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>P</mml:mi></mml:math>](img/315.png).
    In this scenario, an input of size 4 produces an output of size 2 * (4 - 1) +
    4 - 2 * 2 = 6\. We also crop the two cells at the beginning and the end of the
    output vector because they only gather input from a single input cell.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second case, the input is filled with imaginary 0-valued subpixels between
    the existing ones (shown in the preceding diagram, right). This is where the name
    subpixel convolution comes from. Think of it as padding but within the image itself
    and not only along the borders. Once the input has been transformed in this way,
    a regular convolution is applied.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compare the two output cells, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/316.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/317.png),
    in both scenarios. As shown in the preceding diagram, in either case, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/318.png)
    receives input from the first and the second input cells and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/319.png)
    receives input from the second and third cells. The only difference between these
    two cases is the index of the weight, which participates in the computation. However,
    the weights are learned during training, and, because of this, the index is not
    important. Therefore, the two operations are equivalent.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s take a look at a 2D transposed convolution from a subpixel point
    of view. As with the 1D case, we insert 0-valued pixels and padding in the input
    slice to achieve upsampling (the input is at the bottom):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding
    = 1 and stride = 2\. Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285](img/B19627_04_17.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17 – The ﬁrst three steps of a 2D transpose convolution with padding
    = 1 and stride = 2\. Source: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic),
    https://arxiv.org/abs/1603.07285'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation operation of a regular convolution is a transposed convolution.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our extended introduction to the various types of convolutions.
    In the next section, we’ll learn how to build some advanced CNN architectures
    with the advanced convolutions we’ve learned about so far.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Advanced CNN models
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss some complex CNN models. They are available in
    both PyTorch and Keras, with pre-trained weights on the ImageNet dataset. You
    can import and use them directly, instead of building them from scratch. Still,
    it’s worth discussing their central ideas as an alternative to using them as black
    boxes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these models share a few architectural principles:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: They start with an “entry” phase, which uses a combination of stride convolutions
    and/or pooling to reduce the input image size at least two to eight times, before
    propagating it to the rest of the network. This makes a CNN more computationally-
    and memory-efficient because the deeper layers work with smaller slices.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main network body comes after the entry phase. It is composed of multiple
    repeated composite modules. Each of these modules utilizes padded convolutions
    in such a way that its input and output slices are the same size. This makes it
    possible to stack as many modules as necessary to reach the desired depth. The
    deeper modules utilize a higher number of filters (output slices) per convolution,
    compared to the earlier ones.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The downsampling in the main body is handled by special modules with stride
    convolutions and/or pooling operations.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase usually ends with GAP over all slices.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the GAP operation can serve as input for various tasks. For example,
    we can add an FC layer for classification.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see a prototypical CNN built with these principles in the following
    figure:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – A prototypical CNN](img/B19627_04_18.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – A prototypical CNN
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: With that, let’s dig deeper into deep CNNs (get it?).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Introducing residual networks
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Residual networks** (**ResNets**, *Deep Residual Learning for Image Recognition*,
    [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)) were released
    in 2015 when they won all five categories of the ImageNet challenge that year.
    In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we discussed that the layers
    of an NN are not restricted to sequential order but form a directed graph instead.
    This is the first architecture we’ll learn about that takes advantage of this
    flexibility. This is also the first network architecture that has successfully
    trained a network with a depth of more than 100 layers.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to better weight initializations, new activation functions, as well as
    normalization layers, it’s now possible to train deep networks. However, the authors
    of the paper conducted some experiments and observed that a network with 56 layers
    had higher training and testing errors compared to a network with 20 layers. They
    argue that this should not be the case. In theory, we can take a shallow network
    and stack identity layers (these are layers whose output just repeats the input)
    on top of it to produce a deeper network that behaves in the same way as the shallow
    one. Yet, their experiments have been unable to match the performance of the shallow
    network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, they proposed a network constructed of residual blocks.
    A residual block consists of two or three sequential convolutional layers and
    a separate parallel **identity** (repeater) shortcut connection, which connects
    the input of the first layer and the output of the last one. We can see three
    types of residual blocks in the following figure:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – From left to right – original residual block; original bottleneck
    residual block; pre-activation residual block; pre-activation bottleneck residual
    block](img/B19627_04_19.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – From left to right – original residual block; original bottleneck
    residual block; pre-activation residual block; pre-activation bottleneck residual
    block
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Each block has two parallel paths. The left-hand path is similar to the other
    networks we’ve seen and consists of sequential convolutional layers and batch
    normalization. The right path contains the identity shortcut connection (also
    known as the **skip connection**). The two paths are merged via an element-wise
    sum – that is, the left and right tensors have the same shape, and an element
    of the first tensor is added to the element in the same position in the second
    tensor. The output is a single tensor with the same shape as the input. In effect,
    we propagate the features learned by the block forward, but also the original
    unmodified signal. In this way, we can get closer to the original scenario, as
    described by the authors. The network can decide to skip some of the convolutional
    layers thanks to the skip connections, in effect reducing its depth. The residual
    blocks use padding in such a way that the input and the output of the block have
    the same dimensions. Thanks to this, we can stack any number of blocks for a network
    with an arbitrary depth.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how the blocks in the diagram differ:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The first block contains two 3×3 convolutional layers. This is the original
    residual block, but if the layers are wide, stacking multiple blocks becomes computationally
    expensive.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second block is equivalent to the first, but it uses a **bottleneck layer**.
    First, we use a 1×1 convolution to downsample the input volume depth (we discussed
    this in the *1×1 convolutions* section). Then, we apply a 3×3 (bottleneck) convolution
    to the reduced input. Finally, we expand the output back to the desired depth
    with another 1×1 upsampling convolution. This layer is less computationally expensive
    than the first.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third block is the latest revision of the idea, published in 2016 by the
    same authors (*Identity Mappings in Deep Residual Networks*, [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)).
    It uses pre-activations, and the batch normalization and the activation function
    come before the convolutional layer. This may seem strange at first, but thanks
    to this design, the skip connection path can run uninterrupted throughout the
    network. This is contrary to the other residual blocks, where at least one activation
    function is on the path of the skip connection. A combination of stacked residual
    blocks still has the layers in the right order.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth block is the bottleneck version of the third layer. It follows the
    same principle as the bottleneck residual layer v1.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following table, we can see the family of networks proposed by the authors
    of the paper:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – The family of the most popular residual networks. The residual
    blocks are represented by rounded rectangles. Inspired by https://arxiv. org/abs/1512.03385](img/B19627_04_20.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – The family of the most popular residual networks. The residual
    blocks are represented by rounded rectangles. Inspired by [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of their properties are as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: They start with a 7×7 convolutional layer with *stride = 2*, followed by 3×3
    max pooling. This phase serves as a downsampling step, so the rest of the network
    can work with a much smaller slice of 56×56, compared to 224×224 of the input.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling in the rest of the network is implemented with a modified residual
    block with *stride =* *2*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAP downsamples the output after all residual blocks and before the 1,000-unit
    FC softmax layer.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of parameters for the various ResNets range from 25.6 million to
    60.4 million and their depth ranges from 18 to 152 layers.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ResNet family of networks is popular not only because of their accuracy
    but also because of their relative simplicity and the versatility of the residual
    blocks. As we mentioned previously, the input and output shape of the residual
    block can be the same due to the padding. We can stack residual blocks in different
    configurations to solve various problems with wide-ranging training set sizes
    and input dimensions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Inception networks
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Inception networks** (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    were introduced in 2014 when they won the ImageNet challenge of that year (there
    seems to be a pattern here). Since then, the authors have released multiple improvements
    (versions) of the architecture.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The name *inception* comes in part from the *We need to go deeper* internet
    meme, related to the movie Inception.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind inception networks started from the basic premise that the
    objects in an image have different scales. A distant object might take up a small
    region of the image, but the same object, once nearer, might take up a large part
    of the image. This presents a difficulty for standard CNNs, where the units in
    the different layers have a fixed receptive field size, as imposed on the input
    image. A regular network might be a good detector of objects at a certain scale
    but could miss them otherwise. To solve this problem, the authors of the paper
    proposed a novel architecture: one composed of inception blocks. An inception
    block starts with a common input and then splits it into different parallel paths
    (or towers). Each path contains either convolutional layers with a different-sized
    filter or a pooling layer. In this way, we apply different receptive fields to
    the same input data. At the end of the Inception block, the outputs of the different
    paths are concatenated.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we’ll discuss the different variations of Inception
    networks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Inception v1
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following diagram shows the first version of the inception block, which
    is part of the **GoogLeNet** network architecture ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842](img/B19627_04_21.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Inception v1 block; inspired by https://arxiv.org/abs/1409.4842
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The v1 block has four paths:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 1×1 convolution, which acts as a kind of repeater to the input
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1×1 convolution, followed by a 3×3 convolution
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1×1 convolution, followed by a 5×5 convolution
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3×3 max pooling with *stride = 1*
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The layers in the block use padding in such a way that the input and the output
    have the same shape (but different depths). The padding is also necessary because
    each path would produce an output with a different shape, depending on the filter
    size. This is valid for all versions of inception blocks.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The other major innovation of this inception block is the use of downsampling
    1×1 convolutions. They are needed because the output of all paths is concatenated
    to produce the final output of the block. The result of the concatenation is an
    output with a quadrupled depth. If another inception block followed the current
    one, its output depth would quadruple again. To avoid such exponential growth,
    the block uses 1×1 convolutions to reduce the depth of each path, which, in turn,
    reduces the output depth of the block. This makes it possible to create deeper
    networks, without running out of resources.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'The full GoogLeNet has the following properties:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Like ResNets, it starts with a downsampling phase, which utilizes two convolutional
    and two max pooling layers to reduce the input size from 224×224 to 56×56, before
    the inception blocks get involved.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network has nine inception v1 blocks.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network utilizes auxiliary classifiers—that is, it has two additional classification
    outputs (with the same ground truth labels) at various intermediate layers. During
    training, the total value of the loss is a weighted sum of the auxiliary losses
    and the real loss.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model has a total of 6.9 million parameters and a depth of 22 layers.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v2 and v3
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inception v2 and v3 were released together and proposed several improved inception
    blocks over the original v1 (*Rethinking the Inception Architecture for Computer
    Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
    We can see the first new inception block, A, in the following diagram:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_22.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Inception block A, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The first new property of block A is the factorization of the 5×5 convolution
    in two stacked 3×3 convolutions. This structure has several advantages.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The receptive field of the units of the last stacked layer is equivalent to
    the receptive field of a single layer with a large convolutional filter. The stacked
    layers achieve the same receptive field size with fewer parameters, compared to
    a single layer with a large filter. For example, let’s replace a single 5×5 layer
    with two stacked 3×3 layers. For the sake of simplicity, we’ll assume that we
    have single input and output slices. The total number of weights (excluding biases)
    of the 5×5 layer is 5 * 5 = 25\.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the total weights of a single 3×3 layer is 3 * 3 = 9, and
    simply 2 * (3 * 3) = 18 for two layers, which makes this arrangement 28% more
    efficient (18/25 = 0.72). The efficiency gain is preserved even with multiple
    input and output slices for the two layers. The next improvement is the factorization
    of an *n×n* convolution in two stacked asymmetrical 1×*n* and *n*×1 convolutions.
    For example, we can split a single 3×3 convolution into two 1×3 and 3×1 convolutions,
    where the 3×1 convolution is applied over the output of the 1×3 convolution. In
    the first case, the filter size would be 3 * 3 = 9, while in the second case,
    we would have a combined size of (3 * 1) + (1 * 3) = 3 + 3 = 6, resulting in 33%
    efficiency, as seen in the following diagram:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions;
    inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_23.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions;
    inspired by https://arxiv.org/abs/1512.00567
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors introduced two new blocks that utilize factorized convolutions.
    The first of these blocks (and the second in total), inception block B, is equivalent
    to inception block A:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Inception block B. When n=3, it is equivalent to block A; inspired
    by https://arxiv.org/abs/1512.00567](img/B19627_04_24.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Inception block B. When n=3, it is equivalent to block A; inspired
    by https://arxiv.org/abs/1512.00567
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'The second (third in total) inception block, C, is similar, but the asymmetrical
    convolutions are parallel, resulting in a higher output depth (more concatenated
    paths). The hypothesis here is that the more features (different filters) the
    network has, the faster it learns. On the other hand, the wider layers take more
    memory and computation time. As a compromise, this block is only used in the deeper
    part of the network, after the other blocks:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567](img/B19627_04_25.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Inception block C; inspired by https://arxiv.org/abs/1512.00567
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Another major improvement in this version is the use of batch normalization,
    which was introduced by the same authors.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'These new blocks create two new inception networks: v2 and v3\. Inception v3
    uses batch normalization and is the more popular of the two. It has the following
    properties:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The network starts with a downsampling phase, which utilizes stride convolutions
    and max pooling to reduce the input size from 299×299 to 35×35 before the inception
    blocks get involved
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The layers are organized into three inception blocks, A, five inception blocks,
    B, and two inception blocks, C
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has 23.9 million parameters and a depth of 48 layers
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v4 and Inception-ResNet
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The latest revisions of inception networks introduce three new streamlined
    inception blocks (**Inception-v4**, *Inception-v4, Inception-ResNet and the Impact
    of Residual Connections on Learning*, [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)).
    More specifically, the new versions introduce 7×7 asymmetric factorized convolutions
    average pooling instead of max pooling and new Inception-ResNet blocks with residual
    connections. We can see one such block in the following diagram:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – An inception block (any kind) with a residual skip connection](img/B19627_04_26.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – An inception block (any kind) with a residual skip connection
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'The Inception-ResNet family of models share the following properties:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The networks start with a downsampling phase, which utilizes stride convolutions
    and max pooling to reduce the input size from 299×299 to 35×35 before the inception
    blocks get involved.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main body of the model consists of three groups of four residual-inception-A
    blocks, seven residual-inception-B blocks, three residual inception-B blocks,
    and special reduction modules between the groups. The different models use slightly
    different variations of these blocks.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The models have around 56 million weights.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed different types of inception networks and the
    different principles used in the various inception blocks. Next, we’ll talk about
    a newer CNN architecture that takes the inception concept to a new depth (or width,
    as it should be).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Xception
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All inception blocks we’ve discussed so far start by splitting the input into
    several parallel paths. Each path continues with a dimensionality-reduction 1×1
    cross-channel convolution, followed by regular cross-channel convolutions. On
    one hand, the 1×1 connection maps cross-channel correlations, but not spatial
    ones (because of the 1×1 filter size). On the other hand, the subsequent cross-channel
    convolutions map both types of correlations. Let’s recall that earlier in this
    chapter, we introduced DSCs, which combine the following two operations:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '**A depthwise convolution**: In a depthwise convolution, a single input slice
    produces a single output slice, so it only maps spatial (and not cross-channel)
    correlations'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A 1×1 cross-channel convolution**: With 1×1 convolutions, we have the opposite
    – that is, they only map cross-channel correlations'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The author of Xception (*Xception: Deep Learning with Depthwise Separable Convolutions*,
    [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)) argues that
    we can think of DSC as an extreme (hence the name) version of an inception block,
    where each depthwise input/output slice pair represents one parallel path. We
    have as many parallel paths as the number of input slices. The following diagram
    shows a simplified inception block and its transformation to an Xception block:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right);
    inspired by https://arxiv.org/abs/1610.02357](img/B19627_04_27.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 –A simpliﬁed inception module (left); an Xception block (right);
    inspired by https://arxiv.org/abs/1610.02357
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'The Xception block and the DSC have two differences:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: In Xception, the 1×1 convolution comes first, instead of last as in DSC. However,
    these operations are meant to be stacked anyway, and we can assume that the order
    is of no significance.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Xception block uses ReLU activations after each convolution, while the DSC
    doesn’t use non-linearity after the cross-channel convolution. According to the
    author’s experiments, networks with absent non-linearity depthwise convolution
    converged faster and were more accurate.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full Xception network has the following properties:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: It starts with an entry flow of convolutional and pooling operations, which
    reduces the input size from 299×299 to 19×19.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has 14 Xception modules, all of which have linear residual connections around
    them, except for the first and last modules.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All convolutions and DSCs are followed by batch normalization. All DSCs have
    a depth multiplier of 1 (no depth expansion).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A total of 23 million parameters and a depth of 36 convolutional layers.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section concludes the series of inception-based models. In the next section,
    we’ll focus on a novel NN architectural element.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Squeeze-and-Excitation Networks
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Squeeze-and-Excitation Networks** (**SENet**, *Squeeze-and-Excitation Networks*,
    [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)) introduce
    a new NN architectural unit, which the authors call – you guessed it – the **Squeeze-and-Excitation**
    (**SE**) block. Let’s recall that the convolutional operation applies multiple
    filters across the input channels to produce multiple output feature maps (or
    channels). The authors of SENet observe that each of these channels has “equal
    weight” when it serves as input to the next layer. However, some channels could
    be more informative than others. To emphasize their importance, the authors propose
    the content-aware SE block, which weighs each channel adaptively. We can also
    think of the SE block as an **attention mechanism**. To understand how it works,
    let’s start with the following figure:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – The Squeeze-and-Excitation block](img/B19627_04_28.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – The Squeeze-and-Excitation block
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The block introduces a parallel path to the main NN data flow. Let’s see its
    steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '**Squeeze phase**: A GAP operation is applied across the channels. The output
    of the GAP is a single scalar value for each channel. For example, if the input
    is an RGB image, the unique GAP operations across each of the R, G, and B channels
    will produce a one-dimensional tensor with size 3\. Think of these scalar values
    as the distilled state of the channels.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FC layer -> ReLU -> FC layer -> sigmoid`. It resembles an autoencoder because
    the first hidden layer reduces the size of the input tensor and the second hidden
    layer upscales it to the original size (3 in the case of RGB input). The final
    sigmoid activation ensures that all values of the output are in the (0:1) range.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scale**: The output values of the excitement NN serve as scaling coefficients
    of the channels of the original input tensor. All the values of a channel are
    scaled (or excited) by its corresponding coefficient produced by the excitement
    phase. In this way, the excitement NN can emphasize the importance of a given
    channel.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The authors added SE blocks to different existing models, which improved their
    accuracy. In the following figure, we can see how we can add SE blocks to inception
    and residual modules:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)](img/B19627_04_29.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – An SE-inception module (left) and an SE-ResNet module (right)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see the SE block applied to a model, which prioritizes
    a small footprint and computational efficiency.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MobileNet
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss a lightweight CNN model called **MobileNet**
    (*MobileNetV3: Searching for MobileNetV3*, [https://arxiv.org/abs/1905.02244](https://arxiv.org/abs/1905.02244)).
    We’ll focus on the third revision of this idea (MobileNetV1 was introduced in
    *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications*,
    [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861) and MobileNetV2
    was introduced in *MobileNetV2: Inverted Residuals and Linear* *Bottlenecks*,
    [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet is aimed at devices with limited memory and computing power, such
    as mobile phones (the name kind of gives it away). The NN introduces a new **inverted
    residual block** (or **MBConv**) with a reduced footprint. MBConv uses DSC, **linear
    bottlenecks**, and **inverted residuals**. V3 also introduces SE blocks. To understand
    all this, here’s the structure of the MBConv block:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – MobileNetV3 building block. The shortcut connection exists
    only if the stride s=1](img/B19627_04_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – MobileNetV3 building block. The shortcut connection exists only
    if the stride s=1
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss its properties:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear bottlenecks**: We’ll assume that our input is an RGB image. As it’s
    propagated through the NN, each layer produces an activation tensor with multiple
    channels. It has long been assumed that the information encoded in these tensors
    can be compressed in the so-called “manifold of interest,” which is represented
    by a smaller tensor than the original. One way to force the NN to seek such manifolds
    is with 1×1 bottleneck convolutions. However, the authors of the paper argue that
    if this convolution is followed by non-linearity like ReLU, this might lead to
    a loss of manifold information because of the dying-ReLUs problem. To avoid this,
    MobileNet uses a 1×1 bottleneck convolution without non-linear activation.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input -> 1×1 bottleneck conv -> 3×3 conv -> 1×1 unsampling conv`. In other
    words, it follows a `wide -> narrow -> wide` data representation. On the other
    hand, the inverted residual block follows a `narrow -> wide -> narrow` representation.
    Here, the bottleneck convolution expands its input with an **expansion** **factor**,
    *t*.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors argue that the bottlenecks contain all the necessary information,
    while an expansion layer acts merely as an implementation detail that accompanies
    a non-linear transformation of the tensor. Because of this, they propose having
    shortcut connections between the bottleneck connections instead.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**DSC**: We already introduced this operation earlier in this chapter. MobileNet
    V3 introduces **H-swish** activation in the DSC. H-swish resembles the swish function,
    which we introduced in [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047). The V3
    architecture includes alternating ReLU and H-swish activations.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SE blocks**: We’re already familiar with this block. The difference here
    is the **hard sigmoid** activation, which approximates the sigmoid but is computationally
    more efficient. The module is placed after the expanding depthwise convolution,
    so the attention can be applied to the largest representation. The SE block is
    a new addition to V3 and was not present in V2.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stride** *s*: The block implements downsampling with stride convolutions.
    The shortcut connection exists only when *s*=1.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MobileNetV3 introduces large and small variations of the network with the following
    properties:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Both networks start with a stride convolution that downsamples the input from
    224×224 to 112×112
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The small and large variations have 11 and 15 MBConv blocks, respectively
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling for both networks
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The small and large networks have 3 and 5 million parameters, respectively
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss an improved version of the MBConv block.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNet
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**EfficientNet** (*EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks*, [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946),
    and *EfficientNetV2: Smaller Models and Faster Training*, [https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298))
    introduces the concept of **compound scaling**. It starts with a small baseline
    model and then simultaneously expands it in three directions: depth (more layers),
    width (more feature maps per layer), and higher input resolution. The compound
    scaling produces a series of new models. The EfficientNetV1 baseline model uses
    the MBConv building block of MobileNetV2\. EfficientNetV2 introduces the new **fused-MBConv**
    block, which replaces the expanding 1×1 bottleneck convolution and the 3×3 depthwise
    convolution of MBConv, with a single expanding 3×3 cross-channel convolution:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Fused-MBConv block](img/B19627_04_31.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – Fused-MBConv block
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The new 3×3 convolution handles both the expanding (with a factor of *t*) and
    the stride (1 or 2).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: The authors of EfficientNetV2 observed that a CNN, which uses a combination
    of fused-MBConv and MBConv blocks, trains faster compared to a CNN with MBConv
    blocks only. However, the
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'fused-MBConv block is computationally more expensive, compared to the plain
    MBConv block. Because of this, EfficientNetV2 replaces the blocks gradually, starting
    from the early stages. This makes sense because the earlier convolutions use a
    smaller number of filters (and hence slices), so the memory and computational
    penalty are less pronounced at this stage. Finding the right combination of the
    two blocks is not trivial, hence the need for compound scaling. This process produced
    multiple models with the following properties:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The networks start with a stride convolution that downsamples the input twice
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The early stages of the main body use fused-MBConv blocks, and the later stages
    use MBConv blocks
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional phase ends with global average pooling for all networks
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of parameters ranges between 5.3 million and 119 million
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our introduction to advanced CNN models. We didn’t discuss all
    the available models, but we focused on some of the most popular ones. I hope
    that you now have sufficient knowledge to explore new models yourself. In the
    next section, we’ll demonstrate how to use these advanced models in PyTorch and
    Keras.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Using pre-trained models with PyTorch and Keras
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both PyTorch and Keras have a collection of pre-trained ready-to-use models.
    All the models we discussed in the *Advanced network models* section are available
    in this way. The models are usually pre-trained on classifying the ImageNet dataset
    and can serve as backbones to various computer vision tasks, as we’ll see in [*Chapter
    5*](B19627_05.xhtml#_idTextAnchor146).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load a pre-trained model in PyTorch with the following code:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The weights will be automatically downloaded. In addition, we can list all
    available models and load an arbitrary model using the following code:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Keras supports similar functionality. We can load a pre-trained model with
    the following code:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: These short but very useful code examples conclude this chapter.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced CNNs. We talked about their main building blocks
    – convolutional and pooling layers – and we discussed their architecture and features.
    We paid special attention to the different types of convolutions. We also demonstrated
    how to use PyTorch and Keras to implement the CIFAR-10 classification CNN. Finally,
    we discussed some of the most popular CNN models in use today.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll build upon our new-found computer vision knowledge
    with some exciting additions. We’ll discuss how to train networks faster by transferring
    knowledge from one problem to another. We’ll also go beyond simple classification
    with object detection, or how to find the object’s location on the image. We’ll
    even learn how to segment each pixel of an image.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
