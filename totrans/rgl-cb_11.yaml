- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization in Computer Vision – Synthetic Image Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will focus on the techniques and methods used to generate synthetic
    images for data augmentation. Having diverse data is often one of the most efficient
    ways to regularize computer vision models. Many approaches allow us to generate
    synthetic images; from simple tricks such as image flipping to new image creation
    using generative models. Several techniques will be explored in this chapter,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image augmentation with Albumentations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating synthetic images for object detection – training an object detection
    model with only synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time style transfer – training a model for real-time style transfer based
    on Stable Diffusion, a powerful generative model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will train several deep learning models and generate images.
    We will need the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Albumentations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pytorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchvision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ultralytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying image augmentation with Albumentations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, in **machine learning** (**ML**), data is crucial to getting
    better performances of models. Computer vision is no exception, and data augmentation
    with images can be easily taken to another level.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it is possible to easily augment an image, for example, by mirroring
    it, as shown in *Figure 11**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – On the left, the original picture of my dog, and on the right,
    a mirrored picture of my dog](img/B19629_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – On the left, the original picture of my dog, and on the right,
    a mirrored picture of my dog
  prefs: []
  type: TYPE_NORMAL
- en: 'However, beyond this, many more types of augmentation are possible and can
    be divided into two main categories: pixel-level and spatial-level transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss both of these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial-level augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mirroring is an example of spatial-level augmentation; however, much more
    than simple mirroring can be done. For example, see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shifting**: Shifting an image in a certain direction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shearing**: Add shearing to an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cropping**: Cropping only part of an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotating**: Applying rotation to an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transposing**: Transposing an image (in other words, applying both vertical
    and horizontal flipping)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perspective**: Applying a 4-point perspective to an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we can see, there are a lot of possibilities in spatial-level augmentation.
    *Figure 11**.2* shows some examples of these possibilities on a given image and
    displays some of the possible artifacts: black borders on the shifted image and
    mirror padding on the rotated image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – The original image (top left) and five different augmentations
    (note that some artifacts may appear, such as black borders)](img/B19629_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – The original image (top left) and five different augmentations
    (note that some artifacts may appear, such as black borders)
  prefs: []
  type: TYPE_NORMAL
- en: Pixel-level augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another type of augmentation is at the pixel level and can be as useful as spatial-level
    augmentation. A simple example could be to change the brightness and contrast
    level of an image so that a model can be more robust in various lighting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A non-exhaustive list of pixel-level augmentation is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Brightness**: Modify the brightness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrast**: Modify the contrast'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blurring**: Blur the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HSV**: Randomly modify the **hue**, **saturation**, and **value** of the
    image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Color conversion**: Convert the image into black and white or sepia'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise**: Add noise to the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many possibilities, and this can make a huge difference in model
    robustness. A few examples of the results of these augmentations are shown in
    the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – An original image (top left) and several pixel-level augmentations](img/B19629_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – An original image (top left) and several pixel-level augmentations
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, using both pixel-level and spatial-level transformations, it
    is fairly easy to augment a single image into 5 or 10 images. Moreover, these
    augmentations can sometimes be composed together for more diversity. Of course,
    it does not replace a real, large, and diverse dataset, but image augmentation
    is usually cheaper than collecting data and allows us to get real boosts in model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Albumentations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, we do not have to reimplement all of these image augmentations manually.
    Several libraries for image augmentation exist, and Albumentations is arguably
    the most complete, free, and open source solution on the market. As we will see
    in this recipe, the Albumentations library allows powerful image augmentations
    with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply image augmentation to a simple challenge: classifying
    cats and dogs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to download and prepare the dataset. The dataset, originally
    proposed by Microsoft, is made of 12,491 cat pictures and 12,470 dog pictures.
    It can be downloaded with the Kaggle API with the following command-line operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will download a folder named `kagglecatsanddogs_3367a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, the dataset is not yet split into train and test sets. The following
    code will split it into 80% train and 20% test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create subfolders of `train` and `test`, so that the `kagglecatsanddogs_3367a`
    folder tree now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will now be able to efficiently train and evaluate a model against this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The required libraries can be installed with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now train a MobileNet V3 network on the train dataset and evaluate
    it against the test dataset. Then, we will add image augmentation using Albumentations
    in order to improve the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the needed libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`matplotlib` for display and visualization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` for data manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pillow` for image loading'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glob` for folder parsing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` and `torchvision` for the model training and related `util` instances'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement the `DogsAndCats` dataset class. It takes the following
    arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cats_folder`: The path to the folder containing the cat pictures'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dogs_folder`: The path to the folder containing the dog pictures'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform`: The transformation to apply to images (for example, resizing,
    converting into tensors, and so on...)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`augment`: To apply image augmentation, as we will do in the second part of
    this recipe'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This class is rather simple: the constructor collects all the paths of the
    images and defines the labels accordingly. The getter simply loads an image, optionally
    applies image augmentation, and returns the image as `tensor` with its associated
    label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we instantiate the transformation class. Here, we compose three transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tensor conversion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing to 224x224 images since not all images are the same size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of the image input
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a few useful variables, such as the batch size, device, and
    number of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the datasets and data loaders. Reusing the train and test folders
    prepared earlier in the *Getting ready* subsection, we can now easily create our
    two loaders. They both use the same transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we display a few images along with their labels so that we get a glimpse
    at the dataset using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – A sample of images from the dataset](img/B19629_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A sample of images from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the figure, this is a dataset made up of regular images of
    dogs and cats in various contexts, sometimes with humans in the pictures too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we implement the `Classifier` class. We will reuse the existing `mobilenet_v3_small`
    implementation provided in `pytorch` and simply add an output layer with one unit
    and a sigmoid activation function, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we instantiate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, instantiate the loss function as the binary cross-entropy loss, well
    suited to binary classification. Here we instantiate the Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, train the model for 20 epochs and store the outputs. To do so, we use
    the `train_model` function, which trains the input model for a given number of
    epochs and with a given dataset. It returns the loss and accuracy for the training
    and test set for each epoch. This function is available in the GitHub repository
    ([https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb)),
    and is typical code for binary classification training, as we used in previous
    chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, display the loss and accuracy as a function of the epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the plots for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Binary cross-entropy loss (top) and accuracy (bottom) as a
    function of the epoch for both train and test sets with no augmentation (the loss
    and accuracy are suggesting overfitting while the test accuracy plateaus at around
    88%)](img/B19629_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Binary cross-entropy loss (top) and accuracy (bottom) as a function
    of the epoch for both train and test sets with no augmentation (the loss and accuracy
    are suggesting overfitting while the test accuracy plateaus at around 88%)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the test accuracy seems to reach a plateau after roughly 10 epochs,
    with a peak accuracy of around 88%. The train accuracy gets as high as 98%, suggesting
    strong overfitting on the train set.
  prefs: []
  type: TYPE_NORMAL
- en: Training with image augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now redo the same exercise with image augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to implement the desired image augmentation. Using the same
    pattern as with the transformations in `pytorch`, using Albumentations, we can
    instantiate a `Compose` class with a list of augmentations. In our case, we use
    the following augmentations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`HorizontalFlip`: This involves basic mirroring, occurring with a 50% probability,
    meaning 50% of the images will be randomly mirrored'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rotate`: This will randomly rotate an image in the range of [-90, 90] degrees
    (this range can be modified) with a probability of 50%'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomBrightnessContrast`: This will randomly change the brightness and contrast
    of the image with a probability of 20%'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the instantiations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, instantiate a new, augmented training set and training data loader. To
    do so, we simply have to provide our `augment` object as an argument of the `DogsAndCats`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We do not apply augmentation to the test set since we need to be able to compare
    the performances to the results without augmentation. Besides that, it would be
    useless to augment the test set, unless you are using Test Time Augmentation (see
    the *There’s more…* section for more about it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, display a few images from this new, augmented dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Example of augmented images (some have been rotated, some have
    been mirrored and some have modified brightness and contrast)](img/B19629_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Example of augmented images (some have been rotated, some have
    been mirrored and some have modified brightness and contrast)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, some images seem now rotated. Besides, some images are also mirrored
    and have a modified brightness and contrast, efficiently improving the diversity
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we instantiate the model and the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, train the model on this new training set while keeping the same test
    set and store the output losses and metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are doing augmentation online, meaning that every time we
    load a new batch of images, we randomly apply augmentation to these images; consequently,
    at each epoch, we may train from differently augmented images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is to augment data offline: we preprocess and augment the
    dataset, store the augmented images, and then only train the model on this data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both approaches have pros and cons: offline augmentation allows us to augment
    images only once but requires more storage space, while online preprocessing may
    take more time to train but does not require any extra storage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now finally, we plot the results: the loss and accuracy for both the training
    and test sets. Here is the code for that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Loss and accuracy for the augmented dataset](img/B19629_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Loss and accuracy for the augmented dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, compared to the regular dataset, not
    only is the overfitting almost totally removed but the accuracy also climbs up
    to more than 91%, compared to 88% previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this rather simple image augmentation, we could get the accuracy
    to climb from 88% to 91%, while reducing overfitting: the train set now has the
    same performances as the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we used augmentation for training, there is a method that takes advantage
    of image augmentation at test time to improve the performances of models. This
    is sometimes called **Test** **Time Augmentation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is simple: compute model inference on several, augmented images, and
    compute the final prediction with a majority vote.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a simple example. Assuming we have an input image that must be classified
    with our trained dogs and cats model, we augment this input image with mirroring
    and with brightness and contrast, so that we have three images:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image 1**: The original image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image 2**: The mirrored image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image 3**: The image with modified brightness and contrast'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now compute model inference on those three images, getting the following
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image 1** **prediction**: Cat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image 2** **prediction**: Cat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image 3** **prediction**: Dog'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now compute a majority vote by choosing the most represented predicted
    class, resulting in a cat class prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we would most likely use a soft majority vote, averaging the predicted
    probabilities (either for binary or multiclass classification), but the concept
    remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: Test Time Augmentation is commonly used in competitions and can indeed improve
    the performance of the model for no added training cost. In a production environment
    though, where the inference cost is key, this method is rarely used.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we used Albumentations for a simple classification task, but
    it can be used for much more than that: it allows us to perform image augmentation
    for object detection, instance segmentation, semantic segmentation, landmarks,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To know more about how to use it fully, have a look at the well-written documentation,
    with many working examples here: https://albumentations.ai/docs/.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating synthetic images for object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For some projects, you may have so little data that the only thing you can do
    is use this data in the test set. In some rare cases, it is possible to create
    a synthetic dataset to create a robust enough model and test it against the small,
    real test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we will do in this recipe: we have a small test set of pictures
    of QR codes, and we want to build an object detection model for the detection
    of QR codes. All we have as a train set is a set of generated QR codes and downloaded
    images collected on open image websites such as unsplash.com.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download and unzip the dataset from https://www.kaggle.com/datasets/vincentv/qr-detection-yolo
    with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset is made up of the following folder architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'It is made up of three folders:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The train set**: Only generated QR codes with no context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The test set**: Pictures of QR codes in various contexts and environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background images**: Random images of context such as stores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to use the data in the train set and the background images to generate
    realistic synthetic images to train a model on and only then to evaluate the model
    against the test set, made of real images.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, the needed libraries can be installed with the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s divide this recipe into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will explore the dataset and implement a few helper functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second part is about generating synthetic data using QR codes and background
    images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last part is about training a YOLO model on the generated data and evaluating
    this model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us understand each of these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by creating a few helper functions and use them to display a few
    images of the train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`glob` for listing files'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`os` for making a directory in which to store the created synthetic images'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`albumentations` for data augmentation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv2` for image manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for display'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` for various data manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YOLO` for the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s implement a `read_labels` helper function, which will read the text file
    with the YOLO labels and return them as a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s implement a `plot_labels` helper function, which will reuse the previous
    `read_labels` function, read a few images and corresponding labels, and display
    these images with the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, display a set of images from the train set and their bounding boxes with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are a few sample images in the form of QR codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – A few samples from the train set with the associated labels
    (this dataset is only made up of generated QR codes on a white background)](img/B19629_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – A few samples from the train set with the associated labels (this
    dataset is only made up of generated QR codes on a white background)
  prefs: []
  type: TYPE_NORMAL
- en: As explained, the train set is only made up of generated QR codes of various
    sizes on a white background with no more context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now display a few images from the test set with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the resulting images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – A few examples from the test set, made of real-world images
    of QR codes](img/B19629_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – A few examples from the test set, made of real-world images of
    QR codes
  prefs: []
  type: TYPE_NORMAL
- en: The test set contains more complex, real examples of QR codes, and is much more
    challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a synthetic dataset from background images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we will now generate a dataset of realistic, synthetic data.
    To do so, we will use the images of the QR codes from the training set as well
    as a set of background images. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now generate a synthetic dataset using two ingredients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Real background images
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerically generated QR codes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For that, we will use a rather long and complex function, `generate_synthetic_background_image_with_tag`,
    which does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Picks a random background image in the given folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picks a random QR code image in the given folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augments the picked QR code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly inserts the augmented QR code into the background image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies a little more augmentation to the newly created image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stores the generated image and the corresponding labels in YOLO format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code that does this is available in the GitHub repository and is too long
    to be displayed here, so we will only display its signature and docstring here.
    However, you are strongly encouraged to have a look at it and to play with it.
    The code can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This function does this generation as many times as we want and provides a few
    other features; feel free to have a close look at it and update it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use this function to generate 3,000 images by calling the `generate_synthetic_background_image_with_tag`
    function (3,000 is a rather arbitrary choice; feel free to generate fewer images
    or more images). This may take a few minutes. The generated images and their associated
    labels will be stored in the `QR-detection-yolo/generated_qr_code_images/` folder,
    which will be created if it does not exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s have a look at a few examples of generated images with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Examples of synthetically created images, made of background
    images and generated QR codes with various image augmentations](img/B19629_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Examples of synthetically created images, made of background
    images and generated QR codes with various image augmentations
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, some images are simple augmented QR codes with no background
    context, as is possible due to the generating function. This can be tweaked with
    the `background_proba` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now start the model training part: we will train a YOLO model on the
    3,000 images generated in the previous step and evaluate this model against the
    test set. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, instantiate a YOLO model with pre-trained weights as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may have `FileNotFoundError` because of an incorrect dataset path. A `config`
    file in `~/.config/Ultralytics/settings.yaml` has a previous path. A quick and
    harmless fix is to simply delete this file; a new one will then be automatically
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to create a `.yaml` file, `data_qr_generated.yaml`, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This `.yaml` file can be used to train the model on our dataset, on `50 epochs`.
    We also specify the initial learning rate to be 0.001 with `lr0=0.001` because
    the default learning rate (0.01) is rather large for fine-tuning a pre-trained
    model in our case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Results should be stored in the created folder, `runs/detect/generated_qrcode`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before having a look at the results, let’s implement a `plot_results_one_image`
    helper function to display the output of the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then reload the best weights and compute the inference and display the
    results on an image from the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Examples of results of the YOLO model trained on synthetic
    data (even though the model is not perfect, is it capable of detecting QR codes
    in rather complex and various situations)](img/B19629_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Examples of results of the YOLO model trained on synthetic data
    (even though the model is not perfect, is it capable of detecting QR codes in
    rather complex and various situations)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the model is not working perfectly yet but still manages to get
    QR codes in several complex situations. However, in a few cases, such as with
    really small QR codes, bad-quality images, or highly deformed QR codes, the model
    does not seem to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can visualize the losses and other metrics generated by the YOLO
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Metrics computed by the YOLO library](img/B19629_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Metrics computed by the YOLO library
  prefs: []
  type: TYPE_NORMAL
- en: In agreement with the displayed results for a few images, the metrics are not
    perfect, with a mAP50 around 75% only.
  prefs: []
  type: TYPE_NORMAL
- en: This could probably be improved by adding more well-chosen image augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are more techniques for generating images with labels even if we don’t
    have any real data in the first place. In this recipe, we only used background
    images, generated QR codes, and augmentations, but it is possible to use generative
    models to generate even more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to do this with DALL-E, a model proposed by OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can import the required libraries. The `openai` library can be installed
    with `pip` `install openai`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You need to create your own API key by creating your own account on openai.com.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now create a helper function that converts the bounding boxes and the
    image into a mask since we want to complete outside of the bounding box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now compute a mask and display the result side by side with the original
    image as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – An original image on the left, and the associated masked image
    to be used for data generation on the right](img/B19629_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – An original image on the left, and the associated masked image
    to be used for data generation on the right
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We keep a margin for the masked image so that when calling DALL-E 2, it has
    a sense of the surroundings. If we provide only a QR code and white surroundings
    in the mask, the result may not be good enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now query the OpenAI model DALL-E 2 to fill around this QR code and
    generate a new image using the `create_edit` method from the `openai` library.
    The function requires the following few parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input image (in PNG format, less than 4 MB)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The input mask (in PNG format and less than 4 MB too)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A prompt describing what the expected output image is
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of images to generate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The output size in pixels (either 256x256, 512x512, or 1,024x1,024)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now query DALL-E on our image, and then display the original and the
    generated images side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how the images appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – The original image on the left, and the generated image using
    DALL-E 2 on the right](img/B19629_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – The original image on the left, and the generated image using
    DALL-E 2 on the right
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 11**.14*, using this technique allows us to create
    more realistic images that can easily be used for training. These created images
    can also be augmented using Albumentations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There are a few drawbacks though. The generated image is of size 512x512, meaning
    the bounding box coordinates have to be converted (this can be done using Albumentations)
    and the generated image is not always good and requires a visual check.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create variations of a given image using the `create_variation`
    function. This function is simpler to use and requires similar input arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input image (still a PNG image smaller than 4 MB)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of varied images to generate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The output image size in pixels (again, either 256x256, 512x512, or 1,024x1,024)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – The original image (left) and the generated variation using
    DALL-E (right)](img/B19629_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – The original image (left) and the generated variation using DALL-E
    (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'The result presented in the preceding figure is pretty good: we can see a meeting
    room in the background and a QR code in the foreground, just like in the original
    image. However, this data would not be easy to use unless we labeled it manually
    since we have no certainty the QR code will be at the same location (even if we
    resize the bounding box coordinates). Still, using such models can be of great
    help for other use cases, such as classification.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The list of train parameters available with YOLOv8: https://docs.ultralytics.com/modes/train/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DALL-E API documentation: https://platform.openai.com/docs/guides/images/usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing real-time style transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will build our own lightweight style transfer model based
    on the U-Net architecture. To do so, we will use a dataset generated using Stable
    Diffusion (see more next about what Stable Diffusion is). This can be seen as
    a kind of knowledge distillation: we will use the data generated by a large, teacher
    model (Stable Diffusion, which weighs several gigabytes) to train a small, student
    model (here, a U-Net++ of less than 30 MBs). This is a funny way to use generative
    models to create data, but the concepts developed here can be used in many other
    applications: some will be proposed in the *There’s more…* section, along with
    guidance on creating your own style transfer dataset using Stable Diffusion. But
    before that, let’s give some context about style transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Style transfer is a famous and fun use of deep learning, allowing us to change
    the style of a given image into another style. Many examples exist, such as Mona
    Lisa in Van Gogh’s Starry Night style, as represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Mona Lisa in the Starry Night style](img/B19629_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Mona Lisa in the Starry Night style
  prefs: []
  type: TYPE_NORMAL
- en: Until recently, style transfer was mostly performed using **Generative Adversarial
    Networks** (**GANs**), which are quite hard to train properly.
  prefs: []
  type: TYPE_NORMAL
- en: It is now simpler than ever to apply style transfer to images, using pre-trained
    models based on Stable Diffusion. Unfortunately, Stable Diffusion is a large and
    complex model that sometimes requires several seconds to generate a single image
    on a recent graphics card.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will train a U-Net-like model allowing for real-time transfer
    learning on any device. To accomplish this, we will employ a form of knowledge
    distillation. Specifically, we will train the U-net model using Stable Diffusion
    data and incorporate a VGG perceptual loss for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**VGG** stands for **Visual Geometry Group**, the name of the Oxford team who
    proposed this deep learning model architecture. It is a standard deep learning
    model in Computer Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on to the recipe, let’s have a look at two important concepts
    for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptual loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stable Diffusion is a complex and powerful model, allowing us to use image and
    text prompts to generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Architecture diagram of Stable Diffusion](img/B19629_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Architecture diagram of Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the preceding figure, the way Stable Diffusion is trained
    can be summarized, with simplifications, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Diffusion is gradually applied **T** times to an input image **Z**: this is
    like adding random noise to the input image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These diffused images are passed through a denoising U-Net model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A condition is optionally added, such as a descriptive text or another image
    prompt, as an embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is trained to output the input image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once a model is well trained, it can be used for inference by skipping the
    first part as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a seed, a random image is generated and given as input to the denoising
    U-Net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An input prompt is added as condition: it can be text or an input image, for
    example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An output image is then generated: this is the final result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this is a simplistic explanation of how it works, it allows us to get
    a general understanding of what it does and what are the expected inputs to generate
    a new image.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptual loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perceptual loss has been proposed to train a model to learn about perceptual
    features in an image. It was developed specifically for style transfer and allows
    you to focus not only on the pixel-to-pixel content itself but also on the style
    of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes two images as input: the model prediction and the label image, and
    it is commonly based on a VGG neural network pre-trained on the ImageNet dataset
    or any similar generic image dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, for both images (for example, the model prediction and the
    label), the following computations are made:'
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward computation of the VGG model is applied to each image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs after each block of the VGG model are stored, allowing us to get
    more and more specific features with deeper blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In a deep neural network, the first layers are commonly about learning generic
    features and shapes, while the deepest layers are about learning more specific
    features. The perceptual loss takes advantage of this property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these stored computations, perceptual loss is finally computed as the
    sum of the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences (for example, L1 or L2 norm) between the computations for each
    block output: These can be represented as the feature reconstruction loss and
    will focus on image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The differences between the Gram matrices of the computations for each block
    output: These can be represented as the style reconstruction loss and will focus
    on the image’s style.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A Gram matrix of a given set of vectors is made by computing the dot product
    of each pair of vectors and then arranging the results into a matrix. It can be
    seen as a similarity or a correlation between the given vectors.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, minimizing this perceptual loss should allow us to apply a style
    from a given image to another, as we will see in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download the full dataset that I created for this recipe on Kaggle
    with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: 'You then have the following folder architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: This is a rather small dataset, but it should allow us to get good enough performances
    to show the potential of this technique.
  prefs: []
  type: TYPE_NORMAL
- en: For more about how to create such a dataset using ControlNet yourself, have
    a look at the *There’s* *more…* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The libraries needed for this recipe can be installed with the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`matplotlib` for visualization'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` for manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Several `torch` and `torchvision` modules
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation models pytorch` for the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`albumentations` for image augmentation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement `AnimeStyleDataset`, allowing us to load the dataset. Note that we
    use the `ReplayCompose` tool from `Albumentations`, allowing us to apply the exact
    same image augmentation to the image and the associated label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE247]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE248]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE250]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE252]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE254]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE255]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the augmentation, which is a composition of the following transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Resize`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A horizontal flip with a probability of 50%
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ShiftScaleRotate`, allowing us to randomly add geometrical variety'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomBrightnessContrast`, allowing us to add variety to the light'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomCropFromBorders`, which will randomly crop the borders of the images'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the transformation, allowing us to convert the torch tensors and
    rescale the pixel values. Also, define the batch size and the device as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE263]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE264]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE265]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE267]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE268]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use the mean and standard deviation rescaling specific to the ImageNet
    dataset because the VGG perceptual loss (see ahead) is trained on this specific
    set of values. Also, the batch size may need to be adjusted depending on your
    hardware specifications, especially the memory of your graphics processing unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the datasets and data loaders, providing the train and test folders.
    Note that we apply augmentation to the train set only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display a few images along with their labels so that we have a glimpse at the
    dataset. For that, we first need a helper `unnormalize` function to rescale the
    images’ values to the range [0, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – A set of four images with their associated anime labels](img/B19629_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – A set of four images with their associated anime labels
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figure, the dataset is made up of images of faces,
    and the labels are the equivalent pictures with some drawing and anime style applied.
    These images were generated using Stable Diffusion and ControlNet; see how to
    do that yourself in the *There’s* *more…* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we instantiate the model class. Here, we reuse the existing `mobilenetv3_large_100`
    implementation provided in the SMP library with U-Net++ architecture. We specify
    the input and output channels to be `3`, using the `in_channels` and `n_classes`
    parameters respectively. We also reuse `imagenet` weights for the encoder. Here
    is the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE302]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more about the SMP library, refer to the *Semantic segmentation using transfer
    learning* recipe of [*Chapter 10*](B19629_10.xhtml#_idTextAnchor255).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we implement the VGG perceptual loss as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE308]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE309]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE327]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE336]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE337]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE338]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE339]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE340]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE341]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE342]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE343]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE344]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE345]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this implementation, we have two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The `init` function, defining all the blocks and setting them as non-trainable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward` function, resizing the image to 224x224 (the original VGG input
    shape) and computing the loss for each block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we define the optimizer, an exponential learning rate scheduler, and
    the VGG loss, as well as the weights of the style and content loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE346]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE347]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE348]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE349]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE350]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE351]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The style loss (that is, the VGG perceptual loss) is by default much larger
    than the content loss (that is, the L1 loss). So, here we counterbalance that
    by applying a low weight to the style loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model over 50 epochs and store the losses for the train and test
    sets. To do so, let’s use the `train_style_transfer` function available in the
    GitHub repository ([https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE352]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE353]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE354]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE355]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE356]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE357]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE358]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE359]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE360]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE361]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is a typical training loop as we implemented many times already.
    The only difference is the loss computation, which is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`style_loss =` `vgg_loss(outputs, labels)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`content_loss =` `torch.nn.functional.l1_loss(outputs, labels)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss = style_loss_weight*style_loss +` `content_loss_weight*content_loss`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the loss as a function of the epoch for the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE362]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE363]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE364]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE365]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE366]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE367]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Train and test losses as a function of the epoch for the style
    transfer network](img/B19629_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Train and test losses as a function of the epoch for the style
    transfer network
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figure, the model is learning but tends to overfit
    slightly as the test loss does not decrease significantly anymore after about
    40 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, test the trained model on a bunch of images in the test set and display
    the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE368]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE369]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE370]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE371]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE372]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE373]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE374]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE375]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE376]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE377]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE378]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE379]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE380]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – A few images and their predicted transferred style](img/B19629_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – A few images and their predicted transferred style
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figure, the model manages to transfer some of
    the style, by giving a smooth skin and sometimes coloring the hair. It is not
    perfect though, as the output images seem to be too bright.
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that by fine-tuning the loss weights and other hyperparameters,
    it would be possible to achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this recipe showed how to use generative models such as Stable Diffusion
    to create new data in a fun way, it is possible to use it in many other applications.
    Let’s see here how to use Stable Diffusion to create your own style transfer dataset,
    as well as a few other possible applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Stable Diffusion allows us to create realistic and creative
    images based on input prompts. Unfortunately, on its own, it cannot effectively
    apply a style to a given image without compromising the original image’s details
    (for example, the face shape, etc.). To do so, we can use another model based
    on Stable Diffusion: ControlNet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ControlNet works like Stable Diffusion: it takes input prompts and generates
    output images. However, unlike Stable Diffusion, ControlNet will take control
    information as input, allowing us to specifically generate data based on a control
    image: this is exactly what was done to create the dataset of this recipe, efficiently
    adding a drawing style to faces while keeping the overall facial features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The control information can take many forms, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image contours with Canny edges or Hough lines, allowing us to perform realistic
    and limitless image augmentation for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth estimation, allowing us to efficiently generate background images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation, allowing image augmentation for semantic segmentation
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pose estimation, generating more images with people in a given pose, which can
    be useful for object detection, semantic segmentation, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much more, such as scribbles and normal maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Canny edge detector and Hough line transform are typical image processing
    algorithms, allowing us to detect edges and straight lines in images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example, in the following figure, using an input image and the
    computed Canny edges as input, as well as a text prompt such as *A realistic cute
    shiba inu in a fluffy basket*, ControlNet allows us to generate a new image really
    close to the first one. Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – On the left, the input image; at the center, the computed
    Canny edges; and on the right, the generated image with ControlNet and the prompt
    “A realistic cute shiba inu in a fluffy basket”](img/B19629_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – On the left, the input image; at the center, the computed Canny
    edges; and on the right, the generated image with ControlNet and the prompt “A
    realistic cute shiba inu in a fluffy basket”
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to install and use ControlNet, but the official repository
    can be installed with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE381]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, you have to download the models specifically for your needs, available
    on HuggingFace. For example, you can download the Canny model with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE382]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The downloaded file is more than 6 GB, so this might take time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can launch ControlNet UI with the following command line, `python
    gradio_canny2image.py`, and then follow the instructions by going to the created
    localhost, `http://0.0.0.0:7860`.
  prefs: []
  type: TYPE_NORMAL
- en: Using ControlNet and Stable Diffusion, given a powerful enough computer, you
    can now generate almost limitless new images, allowing you to train really robust
    and well-regularized models for computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Paper on the U-Net++ architecture: https://arxiv.org/abs/1807.10165'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More about neural style transfer: https://en.wikipedia.org/wiki/Neural_style_transfer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Wikipedia page about Stable Diffusion: https://en.wikipedia.org/wiki/Stable_Diffusion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper on perceptual loss: https://arxiv.org/pdf/1603.08155.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This recipe was inspired by https://medium.com/@JMangia/optimize-a-face-to-cartoon-style-transfer-model-trained-quickly-on-small-style-dataset-and-50594126e792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official ControlNet repository: https://github.com/lllyasviel/ControlNet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An advanced way of using ControlNet, allowing us to compose several models:
    https://github.com/Mikubill/sd-webui-controlnet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Wikipedia page about the Canny edge detector: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
