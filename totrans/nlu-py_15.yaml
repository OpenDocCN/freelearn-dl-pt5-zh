- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summary and Looking to the Future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will get an overview of the book and a look into the future.
    We will discuss where there is potential for improvement in performance as well
    as faster training, more challenging applications, and future directions for practical
    systems and research.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential for better accuracy and faster training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other areas for improvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications that are beyond the current state of the art
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first section of this chapter is an overall summary of the topics covered
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book has covered the basics of **natural language understanding** (**NLU**),
    the technology that enables computers to process natural language and apply the
    results to a wide variety of practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this book has been to provide a solid grounding in NLU using the
    Python programming language. This grounding will enable you not only to select
    the right tools and software libraries for developing your own applications but
    will also provide you with the background you need to independently make use of
    the many resources available on the internet. You can use these resources to expand
    your knowledge and skills as you take on more advanced projects and to keep up
    with the many new tools that are becoming available as this rapidly advancing
    technology continues to improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we’ve discussed three major topics:'
  prefs: []
  type: TYPE_NORMAL
- en: In *Part 1*, we covered background information and how to get started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Part 2*, we went over Python tools and techniques for accomplishing NLU
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Part 3*, we discussed some practical considerations having to do with managing
    deployed applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout this book, we have taken a step-by-step approach through the typical
    stages of an NLU project, starting from initial ideas through development, testing,
    and finally, to fine-tuning a deployed application. We can see these steps graphically
    in *Figure 15**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – The NLU project life cycle as covered in this book](img/B19005_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – The NLU project life cycle as covered in this book
  prefs: []
  type: TYPE_NORMAL
- en: In *Part 1*, you were introduced to the general topic of NLU and the kinds of
    tasks to which it can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: In *Part 2*, we started by covering foundational topics that support the most
    successful NLU applications, such as software development tools, data, visualization,
    and approaches to representing NLU data. The second general topic that we covered
    in *Part 2* was a set of five different approaches to processing language, including
    rules, traditional machine learning techniques, neural networks, transformers,
    and unsupervised learning. These topics are covered in the five chapters from
    [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159) through [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217),
    which covered the basics of NLU algorithms. Mastering this material will give
    you the background that you need to continue exploring NLU algorithms beyond the
    topics covered in this book. The final topic in *Part 2* was the very important
    subject of evaluation. Evaluation is critically important both for practical NLU
    deployments and for successful academic research. Our review of evaluation in
    [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), covers a variety of the most
    important NLU evaluation tools. With this background, you should now be prepared
    to evaluate your own NLU projects.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in *Part 3*, we turned to the topic of systems in action and focused
    (particularly in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248) on approaches
    to improving the performance of systems both before and after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: If you continue to work in the field of NLU, you will find that there are still
    many challenges, despite the fact that recent advances in **large language models**
    (**LLMs**) have dramatically improved the performance of NLU systems on many tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at some of these challenges in the next two sections, starting
    with the most important areas of improvement – better accuracy and faster training
    – followed by a section on other areas of improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Potential for improvement – better accuracy and faster training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), we listed
    several criteria that can be used to evaluate NLU systems. The one that we usually
    think of first is accuracy – that is, given a specific input, did the system provide
    the right answer? Although in a particular application, we eventually may decide
    to give another criterion priority over accuracy, accuracy is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Better accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), even our best-performing
    system, the large **Bidirectional Encoder Representations from Transformers**
    (**BERT**) model, only achieved an F1 score of *0.85* on the movie review dataset,
    meaning that 15% of its classifications were incorrect. State-of-the-art LLM-based
    research systems currently report an accuracy of *0.93* on this dataset, which
    still means that the system makes many errors (SiYu Ding, Junyuan Shang, Shuohuan
    Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021\. *ERNIE-Doc: A Retrospective
    Long-Document Modeling Transformer*), so we can see that there is still much room
    for accuracy improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs represent the state of the art in NLU. However, there have been few formal
    studies of the accuracy of the most recent LLMs, so it is difficult to quantify
    how good they are. One study of a fairly challenging medical information task
    where physicians evaluated the accuracy of ChatGPT’s answers found that ChatGPT
    answers were considered to be largely accurate overall, receiving a mean score
    of *4.6* out of *6* (Johnson, D., et al. (2023). *Assessing the Accuracy and Reliability
    of AI-Generated Medical Responses: An Evaluation of the Chat-GPT Model*. Research
    Square, rs.3.rs-2566942\. [https://doi.org/10.21203/rs.3.rs-2566942/v1](https://doi.org/10.21203/rs.3.rs-2566942/v1)).
    However, the system still made many errors, and the authors cautioned that it
    was important for physicians to review medical advice supplied by ChatGPT or,
    in general, any LLMs at the current state of the art.'
  prefs: []
  type: TYPE_NORMAL
- en: Better accuracy will always be a goal in NLU. Achieving better accuracy in future
    systems will include developing larger pretrained models as well as developing
    more effective fine-tuning techniques. In addition, there is a significant amount
    of work to be done in extending the current high performance of LLMs for the most
    widely studied languages to less well-studied languages.
  prefs: []
  type: TYPE_NORMAL
- en: Faster training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By working through some of the exercises in earlier chapters, you have found
    that training NLU models from scratch or fine-tuning an LLM did not take more
    than a few hours of computer time. For the purposes of this book, we intentionally
    selected smaller datasets so that you could get feedback from the training process
    more quickly. However, even larger problems that you may want to address in a
    practical setting should not take more than a few days of training time. On the
    other hand, training pretrained LLMs can take a very long time. One estimate for
    the training time for GPT-3 was that it took 355 GPU years to train on the 300
    billion token training dataset. In practice, the calendar time required was reduced
    by running the training on multiple GPUs in parallel ([https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)).
    Still, this training does involve tremendous amounts of computing power, along
    with the associated costs.
  prefs: []
  type: TYPE_NORMAL
- en: Since most pretrained models are trained by large organizations with extensive
    computing resources rather than by smaller organizations or researchers, long
    training times for large models don’t directly affect most of us because we will
    be using the pretrained models developed by large organizations. However, these
    long training times do affect us indirectly because long training times mean that
    it will take longer for new and improved models to be released for general use.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to better accuracy and faster training times, there are other areas
    where NLU technology can be improved. We will review some of these in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Other areas for improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The areas for improvement that we’ll review in this section are primarily related
    to making NLU technology more practical in various ways, such as speeding up development
    and decreasing the number of computer resources needed during development and
    at runtime. These topics include smaller models, more explainability, and smaller
    amounts of fine-tuning data.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The BERT models we looked at in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193)
    and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), were relatively small. The
    reason for choosing these models was so that they could be downloaded and fine-tuned
    in a relatively short amount of time. However, as a rule of thumb, large models
    will be more accurate than smaller models. But we can’t always take advantage
    of large models because some models are too large to be fine-tuned on a single
    GPU, as pointed out on the TensorFlow site ([https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=dX8FtlpGJRE6)).
    Because the larger models have better accuracy, it would be very helpful if high-accuracy
    performance could be obtained with smaller models. In addition, there are many
    situations where large models will not fit on resource-constrained devices such
    as mobile phones or even smartwatches. For those reasons, decreasing the size
    of models is an important goal in NLU research.
  prefs: []
  type: TYPE_NORMAL
- en: Less data required for fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most NLU applications that use pretrained models, the pretrained model needs
    to be fine-tuned with application-specific data. We covered this process in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193) and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
    Clearly, reducing the amount of data required to fine-tune the system results
    in a reduction in the development time for the fine-tuned system. For example,
    in its discussion of the process of fine-tuning GPT-3, OpenAI states, “*The more
    training examples you have, the better. We recommend having at least a couple
    hundred examples. In general, we’ve found that each doubling of the dataset size
    leads to a linear increase in model quality*” ([https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)).
    As we learned in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), the process
    of finding and annotating data can be time-consuming and expensive, and it is
    clearly desirable to minimize this effort.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the most part, a result from an NLU system based on machine learning will
    simply be a number, such as a probability that a text falls into one of the categories
    on which the model was trained. We don’t have an easy way to understand how the
    system came up with that answer, whether the answer is correct or incorrect. If
    the answer is incorrect, we can try to improve the model by adding more data,
    adjusting the hyperparameters, or using some of the other techniques that we reviewed
    in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248), but it's hard to understand
    exactly why the system came up with the wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if a rule-based system such as those we went over in [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159) makes an error, it can normally be traced
    back to an incorrect rule, which can be fixed. However, since nearly all current
    systems are based on machine learning approaches rather than rules, it is very
    difficult to understand how they arrive at the answers that they do. Nevertheless,
    it is often important for users to understand how a system came up with a result.
    If the users don’t understand how the system came up with an answer, they might
    not trust the system. If a system gives a wrong answer or even a correct answer
    that the user doesn’t understand, it can undermine user confidence in the system.
    For that reason, explainability in NLU and in AI, in general, is an important
    research topic. You can read more about this topic at [https://en.wikipedia.org/wiki/Explainable_artificial_intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence).
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness of information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248), we discussed how changes
    in the deployment context can result in system errors. The introduction of new
    product names, new movies, or even significant news events can lead to the system
    not knowing the answer to a user’s question or even giving the wrong answer. Because
    LLMs take so long to train, they are especially vulnerable to making errors due
    to a change in the deployment context.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the ChatGPT system has a knowledge cutoff date of September 2021,
    which means that it doesn’t have any knowledge of events that occurred after that.
    Because of this, it can make mistakes like the one shown in *Figure 15**.2*, which
    states that the current monarch of the United Kingdom is Elizabeth II. This was
    true in September 2021, but it is no longer true.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – ChatGPT answer to “who is the current monarch of the united
    kingdom”](img/B19005_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – ChatGPT answer to “who is the current monarch of the united kingdom”
  prefs: []
  type: TYPE_NORMAL
- en: Although the ChatGPT system acknowledges that its information may be out of
    date, this lack of timeliness can lead to errors if something changes in the deployment
    context. If you are developing your own application and something changes in the
    deployment context, you can either retrain the system from scratch with the new
    data or add new data to your existing model. However, if you are using a cloud-based
    LLM, you should be aware that the information it provides can be out of date.
    Note that this cutoff period can vary between different LLMs. For example, the
    Google Bard system was able to correctly answer the question in *Figure 15**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: If your application uses an LLM and requires access to accurate time-dependent
    information, you should verify that the system you’re using is being kept up to
    date by its developers.
  prefs: []
  type: TYPE_NORMAL
- en: The next section talks about a few blue sky applications that may be possible
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that are beyond the current state of the art
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section talks about several applications that are not yet possible, but
    that are theoretically feasible. In some cases, they could probably be achieved
    if the right training data and computing resources were available. In other cases,
    they might require some new algorithmic insights. In all of these examples, it
    is very interesting to think about how these and other futuristic applications
    might be accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: Processing very long documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current LLMs have relatively small limits on the length of documents (or prompts)
    they can process. For example, GPT-4 can only handle texts of up to 8,192 tokens
    ([https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4)),
    which is around 16 single-spaced pages. Clearly, this means that many existing
    documents can’t be fully analyzed with these cloud systems. If you are doing a
    typical classification task, you can train your own model, for example, with a
    **Term frequency-inverse document frequency** (**TF-IDF**) representation, but
    this is not possible with a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, the documents can be as long as you like, but you will lose the
    advantages of LLMs. Research systems such as Longformer have been able to process
    much longer documents through more efficient use of computational resources. If
    you have a use case for processing long documents, it would be worth looking into
    some of these research systems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and creating videos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand videos, a system would need to be able to interpret both the video
    and audio streams and relate them to each other. If the system learns someone’s
    name in the early part of the video and that character appears in a later part
    of the video, it should be able to name the person based on recognizing the image
    of the character. It could then do tasks such as transcribing the script of a
    movie by simply watching it, complete with notations like “Character X smiles”.
    This is not a very difficult task for humans, who are quite good at recognizing
    a person that they’ve seen before, but it would be very difficult for automated
    systems. While they are quite good at identifying people in images, they are less
    capable of identifying people in videos. In contrast to understanding videos,
    generating videos seems to be an easier task. For example, there are currently
    systems available that generate videos from text, such as a system developed by
    Meta ([https://ai.facebook.com/blog/generative-ai-text-to-video/](https://ai.facebook.com/blog/generative-ai-text-to-video/)),
    although the videos don’t yet look very good.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting and generating sign languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One application of understanding videos would be to understand sign languages
    such as American Sign Language and translate them into spoken languages. Combined
    with the reverse process of translating spoken language into sign language, this
    kind of technology could greatly simplify communication between signers and speakers
    of spoken languages. There have been some exploratory studies of interpreting
    and generating sign languages.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the work on [https://abdulhaim.github.io/6.S198-Assignments/final_project.html](https://abdulhaim.github.io/6.S198-Assignments/final_project.html)
    describes an approach to interpreting Argentinian Sign Language using **Convolutional
    Neural Networks** (**CNNs**). Although this is an interesting proof of concept,
    it only works with 64 signs from Argentinian Sign Language. In fact, there are
    thousands of signs used in actual sign languages, so handling 64 signs is only
    a small demonstration of the possibility of automatically interpreting sign languages.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, this research only used hand positions to recognize signs, while,
    in fact, signs are also distinguished by other body positions. More work needs
    to be done to demonstrate practical automatic sign language interpretation. This
    would be greatly aided by the availability of more sign language datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Writing compelling fiction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have experimented with ChatGPT or other LLMs, you may have noticed that
    the writing style is rather bland and boring. This is because it’s based on text
    from the internet and other existing sources and there is no way for it to be
    creative beyond the data that it is trained on. On the other hand, compelling
    fiction is unique and often contains insights and verbal images that have never
    appeared in writing before.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s look at an excerpt from one of the great poems of the
    English language, To a Skylark, by Percy Bysshe Shelley, which can be seen in
    *Figure 15**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – An excerpt from “To a Skylark”, a poem by Percy Bysshe Shelley
    (1820)](img/B19005_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – An excerpt from “To a Skylark”, a poem by Percy Bysshe Shelley
    (1820)
  prefs: []
  type: TYPE_NORMAL
- en: This poem includes novel figures of speech such as the simile that compares
    a bird to *a cloud of fire* and uses the metaphor *the blue deep* for the sky,
    which are probably unique in literature.
  prefs: []
  type: TYPE_NORMAL
- en: Compare this to the poem generated by ChatGPT in *Figure 15**.4*. When prompted
    to write a poem about a skylark flying in the sky, the result seems flat and unoriginal
    compared to the Shelley poem and includes cliches such as *boundless sky* and
    *ascends* *on high*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – ChatGPT poem about a skylark](img/B19005_15_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – ChatGPT poem about a skylark
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about how we might train an LLM to learn how to generate good poetry
    or interesting fiction. If we follow the standard NLU development paradigm of
    learning a model from training data, for an NLU system to write compelling fiction,
    we would need a dataset consisting of text examples of compelling and engaging
    writing and other examples of writing that are not compelling or engaging. Alternatively,
    we might be able to identify other features of compelling writing (use verbs,
    avoid passive voice, etc.) that could be used to train systems to evaluate writing
    or to produce good writing. You can understand how far we are from this kind of
    application by thinking about what an NLU system would need to be able to do to
    write insightful book reviews. It would have to be familiar with the author’s
    other books, other books in a similar genre, any relevant historical events mentioned
    in the book, and even the author’s biography. Then it would have to be able to
    pull all this knowledge together into a concise analysis of the book. All of this
    seems quite difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will look at applications that are targets of current research
    and are much closer to realization than the ones we’ve just reviewed. We are likely
    to see advances in these kinds of applications in the next couple of years.
  prefs: []
  type: TYPE_NORMAL
- en: Future directions in NLU technology and research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the recent improvements in NLU technology based on transformers and LLMs,
    which we reviewed in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193), have resulted
    in very impressive capabilities, it is important to point out that there are many
    topics in NLU that are far from solved. In this section, we will look at some
    of the most active research areas – extending NLU to new languages, speech-to-speech
    translation, multimodal interaction, and avoiding bias.
  prefs: []
  type: TYPE_NORMAL
- en: Quickly extending NLU technologies to new languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A precise count of the number of currently spoken languages is difficult to
    obtain. However, according to *WorldData.info*, there are currently about 6,500
    languages spoken throughout the world ([https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania](https://www.worlddata.info/languages/index.php#:~:text=There%20are%20currently%20around%206%2C500,of%20Asia%2C%20Australia%20and%20Oceania)).
    Some languages, such as Mandarin, English, Spanish, and Hindi, are spoken by many
    millions of people, while other languages are spoken by very few people and these
    languages are even in danger of dying out (for example, you can see a list of
    the endangered languages of North America alone at [https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America](https://en.wikipedia.org/wiki/List_of_endangered_languages_in_North_America)).
  prefs: []
  type: TYPE_NORMAL
- en: Languages with many millions of speakers tend to be more economically important
    than languages with few speakers, and as a consequence, NLU technology for those
    languages is generally much more advanced than that for languages with few speakers.
    If you recall from our discussion of LLMs in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    training an LLM such as BERT or GPT-3 for one language is a very expensive and
    time-consuming process that requires enormous amounts of text data. It would be
    impractical for this training process to be carried out for thousands of languages.
    For that reason, researchers have looked into adapting LLMs used for widely spoken
    languages to less widely spoken languages.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very active research area that presents many challenges to NLU technology.
    One challenge, for example, is how to keep the original language from being forgotten
    when its language model is adapted to a new language – a process called **catastrophic
    forgetting**.
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of a recent research paper on this topic that you can read
    for more insight into the problem of adapting LLMs to new languages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cahyawijaya, S., Lovenia, H., Yu, T., Chung, W., & Fung, P. (2023). *Instruct-Align:
    Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction*.
    arXiv preprint arXiv:2305.13627\. [https://arxiv.org/abs/2305.13627](https://arxiv.org/abs/2305.13627).'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time speech-to-speech translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anyone who has traveled to a foreign country whose language they do not know,
    or whose language they do not know well, has probably found communication very
    frustrating. Looking up words or even phrases in handheld apps or paper dictionaries
    is slow and can be inaccurate. A much better solution would be speech-to-speech
    translation. Speech-to-speech translation technology listens to speech in one
    language, translates it to another language, and the system speaks the output
    in a second language, which would be much faster than typing words into a mobile
    app.
  prefs: []
  type: TYPE_NORMAL
- en: The base technologies underlying **speech-to-speech translation** are actually
    fairly advanced. For example, Microsoft Cognitive Services offers a speech-to-speech
    translation service ([https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/](https://azure.microsoft.com/en-us/products/cognitive-services/speech-translation/))
    with support for over 30 languages. The number of available language pairs continues
    to increase – for example, Speechmatics offers a translation service for 69 language
    pairs ([https://www.speechmatics.com/product/translation](https://www.speechmatics.com/product/translation)).
  prefs: []
  type: TYPE_NORMAL
- en: However, most of these services do their processing in the cloud. Because one
    of the most important use cases for speech-to-speech translation is for travel,
    users may not want to use a service that requires access to the cloud. They may
    not have a good internet connection, or they may not want to pay for data while
    traveling. It is much more difficult to translate speech offline, without sending
    it to the cloud, as mobile devices have far fewer computing resources than the
    cloud. The results are less accurate and not nearly as many languages are supported.
    For example, the Apple Translate app ([https://apps.apple.com/app/translate/id1514844618](https://apps.apple.com/app/translate/id1514844618))
    claims to support 30 languages but the reviews are very low, especially for offline
    use. There is significant room for improvement in the technology for offline speech-to-speech
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Multimodal interaction** is a type of user-system interaction where the user
    interacts with a computer system in multiple ways (*modalities*) in addition to
    language. For example, multimodal interaction could include camera input that
    allows the system to interpret facial expressions in addition to speech input.
    This would let the system read the user’s body language to detect emotions such
    as happiness or confusion in addition to interpreting what the user says. As well
    as understanding multimodal user inputs, a multimodal system can also produce
    images, animations, videos, and graphics in addition to language in order to respond
    to users’ questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Multimodal interaction has been used extensively in research projects such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: António Teixeira, Annika Hämäläinen, Jairo Avelar, Nuno Almeida, Géza Németh,
    Tibor Fegyó, Csaba Zainkó, Tamás Csapó, Bálint Tóth, André Oliveira, Miguel Sales
    Dias, *Speech-centric Multimodal Interaction for Easy-to-access Online Services
    – A Personal Life Assistant for the Elderly*, Procedia Computer Science, Volume
    27, 2014, Pages 389-397, ISSN 1877-0509, [https://doi.org/10.1016/j.procs.2014.02.043](https://doi.org/10.1016/j.procs.2014.02.043).
  prefs: []
  type: TYPE_NORMAL
- en: However, multimodal interaction is far from widespread in practical applications.
    This may be due in part to the relative scarcity of training data for multimodal
    systems since training multimodal systems requires data for all the modalities
    being used in the system, not just language data. For example, if we wanted to
    develop an application that used a combination of facial expression recognition
    and NLU to understand users’ emotions, we would need a video dataset annotated
    with both facial expressions and NLU categories. There are a few existing datasets
    with this kind of information – for example, the datasets listed at [https://www.datatang.ai/news/60](https://www.datatang.ai/news/60)
    – but they are not nearly as abundant as the text datasets that we’ve been working
    with throughout this book. Multimodal interaction is a very interesting topic,
    and the availability of additional data will certainly stimulate some future groundbreaking
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and correcting bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training data for LLMs is based on existing text, primarily from the web.
    This text, in many cases, reflects cultural biases that we would not like to perpetuate
    in our NLU systems. It is easy to find this bias in current LLMs. For example,
    the article *ChatGPT insists that doctors are male and nurses female*, by Suzanne
    Wertheim, shows many examples of ChatGPT assuming that people in certain professions
    are male or female ([https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female](https://www.worthwhileconsulting.com/read-watch-listen/chatgpt-insists-that-doctors-are-male-and-nurses-female)).
    This problem has been the topic of considerable research and is far from solved.
  prefs: []
  type: TYPE_NORMAL
- en: Citation
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find out more about how bias has been addressed in the following survey
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: Alfonso, L. (2021). *A Survey on Bias in Deep NLP*. Applied Sciences, 11(7),
    3184\. [https://doi.org/10.3390/app11073184](https://doi.org/10.3390/app11073184).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have summarized the previous chapters in the book, reviewed
    some areas where NLU technology still faces challenges, and talked about some
    directions where it could improve in the future. NLU is an extremely dynamic and
    fast-moving field, and it will clearly continue to develop in many exciting directions.
    With this book, you have received foundational information about NLU that will
    enable you to decide not only how to build NLU systems for your current applications
    but also to take advantage of technological advances as NLU continues to evolve.
    I hope you will be able to build on the information in this book to create innovative
    and useful applications that use NLU to solve future practical as well as scientific
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SiYu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng
    Wang. 2021\. *ERNIE-Doc: A Retrospective Long-Document Modeling Transformer*.
    In Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers), pages 2914–2927, Online. Association for Computational
    Linguistics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beltagy, I., Peters, M.E., & Cohan, A. (2020). *Longformer: The Long-Document
    Transformer*. arXiv, abs/2004.05150'
  prefs: []
  type: TYPE_NORMAL
