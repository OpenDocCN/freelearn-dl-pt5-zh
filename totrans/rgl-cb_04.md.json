["```py\npip install scikit-learn graphviz matplotlib\n```", "```py\n    from matplotlib import pyplot as plt\n    ```", "```py\n    from sklearn.datasets import load_iris\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.tree import DecisionTreeClassifier\n    ```", "```py\n    # Load the dataset\n    ```", "```py\n    X, y = load_iris(return_X_y=True)\n    ```", "```py\n    # Split the dataset\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, random_state=0)\n    ```", "```py\n    # Plot the training points\n    ```", "```py\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    ```", "```py\n    plt.xlabel('Sepal length')\n    ```", "```py\n    plt.ylabel('Sepal width')\n    ```", "```py\n    plt.show()\n    ```", "```py\n    # Instantiate the model\n    ```", "```py\n    dt = DecisionTreeClassifier()\n    ```", "```py\n    # Fit the model on the training data\n    ```", "```py\n    dt.fit(X_train, y_train)\n    ```", "```py\n    # Compute the accuracy on training and test sets\n    ```", "```py\n    print('Accuracy on training set:', dt.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('Accuracy on test set:', dt.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\nAccuracy on training set: 1.0\nAccuracy on test set: 0.9736842105263158\n```", "```py\nfrom sklearn.tree import export_graphviz\nimport graphviz\n# We load iris data again to retrieve features and classes names\niris = load_iris()\n# We export the tree in graphviz format\ngraph_data = export_graphviz(\n    dt,\n    out_file=None,\n    feature_names=iris.feature_names,\n    class_names=iris.target_names,\n    filled=True, rounded=True\n)\n# We load the tree again with graphviz library in order to display it\ngraphviz.Source(graph_data)\n```", "```py\nimport numpy as np\nplt.bar(iris.feature_names, dt.feature_importances_)\nplt.xticks(rotation=45))\nplt.ylabel('Feature importance')\nplt.title('Feature importance for the decision tree')\nplt.show()\n```", "```py\npip install scikit-learn\n```", "```py\n    from sklearn.datasets import fetch_california_housing\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.tree import DecisionTreeRegressor\n    ```", "```py\n    X, y = fetch_california_housing(return_X_y=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, test_size=0.2, random_state=0)\n    ```", "```py\n    dt = DecisionTreeRegressor()\n    ```", "```py\n    dt.fit(X_train, y_train)\n    ```", "```py\n    DecisionTreeRegressor()\n    ```", "```py\n    print('R2-score on training set:', dt.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', dt.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\nR2-score on training set: 1.0\nR2-score on test set: 0.5923572475948657\n```", "```py\ndef plot_decision_function(dt, X, y):\n    # Create figure to draw chart\n    plt.figure(2, figsize=(8, 6))\n    # We create a grid of points contained within [x_min,\n      #x_max]x[y_min, y_max] with step h=0.02\n    x0_min, x0_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    x1_min, x1_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = .02  # step size of the grid\n    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, h),\n        np.arange(x1_min, x1_max, h))\n    # Retrieve predictions for each point of the grid\n    Z_dt = dt.predict(np.c_[xx0.ravel(), xx1.ravel()])\n    Z_dt = Z_dt.reshape(xx0.shape)\n    # Plot the decision boundary (label predicted assigned to a color)\n    plt.pcolormesh(xx0, xx1, Z_dt, cmap=plt.cm.Paired)\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k',\n        cmap=plt.cm.Paired)\n    # Format chart\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()\n```", "```py\n        from sklearn.datasets import load_iris\n        ```", "```py\n        from sklearn.model_selection import train_test_split\n        ```", "```py\n        from sklearn.tree import DecisionTreeClassifier\n        ```", "```py\n    X, y = load_iris(return_X_y=True)\n    ```", "```py\n    # Keep only 2 features\n    ```", "```py\n    X = X[:, :2]\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, random_state=0)\n    ```", "```py\n    dt = DecisionTreeClassifier(max_depth=5,\n    ```", "```py\n        random_state=0)\n    ```", "```py\n    dt.fit(X_train, y_train)\n    ```", "```py\n    DecisionTreeClassifier(max_depth=5, random_state=0)\n    ```", "```py\n    print('Accuracy on training set:', dt.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('Accuracy on test set:', dt.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\nAccuracy on training set: 0.8660714285714286\nAccuracy on test set: 0.6578947368421053\n```", "```py\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# Fit a decision tree over only 2 features\ndt = DecisionTreeClassifier()\ndt.fit(X_train[:, :2], y_train)\n# Plot the decision tree decision function\nplot_decision_function(dt, X_train[:, :2], y_train)\n```", "```py\n# Compute the accuracy on training and test sets for only 2 features\nprint('Accuracy on training set:', dt.score(X_train[:, :2], y_train))\nprint('Accuracy on test set:', dt.score(X_test[:, :2], y_test))\n```", "```py\nAccuracy on training set: 0.9375\nAccuracy on test set: 0.631578947368421\n```", "```py\nmax_depth: int, default=None\n```", "```py\n# Fit a decision tree with max depth of 5 over only 2 features\ndt = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt.fit(X_train[:, :2], y_train)\n# Plot the decision tree decision function\nplot_decision_function(dt, X_train[:, :2], y_train)\n```", "```py\n# Compute the accuracy on training and test sets for only 2 features\nprint('Accuracy on training set:', dt.score(X_train[:, :2], y_train))\nprint('Accuracy on test set:', dt.score(X_test[:, :2], y_test))\n```", "```py\nAccuracy on training set: 0.8660714285714286\nAccuracy on test set: 0.6578947368421053\n```", "```py\n# Fit a decision tree with min samples per split of 15 over only 2 features\ndt = DecisionTreeClassifier(min_samples_split=15, random_state=0)\ndt.fit(X_train[:, :2], y_train)\n# Plot the decision tree decision function\nplot_decision_function(dt, X_train[:, :2], y_train)\n```", "```py\n# Compute the accuracy on training and test sets for only 2 features\nprint('Accuracy on training set:', dt.score(X_train[:, :2],\n    y_train))\nprint('Accuracy on test set:', dt.score(X_test[:, :2],\n    y_test))\n```", "```py\nAccuracy on training set: 0.85714285714285717\nAccuracy on test set: 0.7368421052631579\n```", "```py\npip install scikit-learn\n```", "```py\n    from sklearn.datasets import fetch_california_housing\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.ensemble import RandomForestRegressor\n    ```", "```py\n    X, y = fetch_california_housing(return_X_y=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, random_state=0)\n    ```", "```py\n    rf = RandomForestRegressor(random_state=0)\n    ```", "```py\n    rf.fit(X_train, y_train)\n    ```", "```py\n    RandomForestRegressor(random_state=0)\n    ```", "```py\n    # Display the accuracy on both training and test set\n    ```", "```py\n    print('R2-score on training set:', rf.score(X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', rf.score(X_test, y_test))\n    ```", "```py\nR2-score on training set: 0.9727159677969947\nR2-score on test set: 0.7941678302821006\n```", "```py\n    from sklearn.datasets import fetch_california_housing\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.ensemble import RandomForestRegressor\n    ```", "```py\n    X, y = fetch_california_housing(return_X_y=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, random_state=0)\n    ```", "```py\n    rf = RandomForestRegressor(max_features='log2', random_state=0)\n    ```", "```py\n    rf.fit(X_train, y_train)\n    ```", "```py\n    RandomForestRegressor(max_features='log2', random_state=0)\n    ```", "```py\n    print('R2-score on training set:', rf.score(X_train,\n    ```", "```py\n        y_train))\n    ```", "```py\n    print('R2-score on test set:', rf.score(X_test,\n    ```", "```py\n        y_test))\n    ```", "```py\nR2-score on training set: 0.9748218476882353\nR2-score on test set: 0.8137208340736402\n```", "```py\npip install pickle xgboost\n```", "```py\nwget https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl\n```", "```py\n    import pickle\n    ```", "```py\n    from xgboost import XGBClassifier\n    ```", "```py\n    X_train, X_test, y_train, y_test = pickle.load(open(\n    ```", "```py\n        'prepared_titanic.pkl', 'rb'))\n    ```", "```py\n    bst = XGBClassifier(use_label_encoder=False)\n    ```", "```py\n    # Train the model on training set\n    ```", "```py\n    bst.fit(X_train, y_train)\n    ```", "```py\n    print('Accuracy on training set:', bst.score(X_train,\n    ```", "```py\n        y_train))\n    ```", "```py\n    print('Accuracy on test set:', bst.score(X_test,\n    ```", "```py\n        y_test))\n    ```", "```py\nAccuracy on training set: 0.9747191011235955\nAccuracy on test set: 0.8156424581005587\n```", "```py\n    import pickle\n    ```", "```py\n    from xgboost import XGBClassifier\n    ```", "```py\n    X_train, X_test, y_train, y_test = pickle.load(open(\n    ```", "```py\n        'prepared_titanic.pkl', 'rb'))\n    ```", "```py\n    bst = XGBClassifier(use_label_encoder=False,reg_alpha=1)\n    ```", "```py\n    bst.fit(X_train, y_train)\n    ```", "```py\n    print('Accuracy on training set:', bst.score(X_train,\n    ```", "```py\n        y_train))\n    ```", "```py\n    print('Accuracy on test set:', bst.score(X_test,\n    ```", "```py\n        y_test))\n    ```", "```py\nAccuracy on training set: 0.9410112359550562\nAccuracy on test set: 0.8435754189944135\n```"]