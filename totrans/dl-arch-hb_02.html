<html><head></head><body>
<div id="_idContainer028">
<h1 class="chapter-number" id="_idParaDest-41"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.2.1">Designing Deep Learning Architectures</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we went through the entire deep learning life cycle and understood what it means to make a deep learning project successful from end to end. </span><span class="koboSpan" id="kobo.3.2">With that knowledge, we are now ready to dive further into the technicalities of deep learning models. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will dive into common deep learning architectures used in the industry and understand the reasons behind each architecture’s design. </span><span class="koboSpan" id="kobo.3.4">For intermediate and advanced readers, this will be a brief recap to ensure alignment in the definitions of terms. </span><span class="koboSpan" id="kobo.3.5">For beginner readers, architectures will be presented in a way that is easy to digest so that you can get up to speed on the useful neural architectures in the world of </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">deep learning.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Grasping the methodologies behind a wide variety of architectures allows you to innovate custom architectures specific to your use case and, most importantly, gain the skill to choose an appropriate foundational architecture based on the data input or </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">problem type.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, the focus will be on the </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">Multilayer Perceptron</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">MLP</span></strong><span class="koboSpan" id="kobo.11.1">) network architecture. </span><span class="koboSpan" id="kobo.11.2">The comprehensive coverage of MLPs, along with some key concepts in general related to neural network implementations, such as gradients, activation functions, and regularization methods, will set the stage for exploring other, more complex architecture types in later chapters. </span><span class="koboSpan" id="kobo.11.3">Specifically, the following topics will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Exploring the foundations of neural networks using </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">an MLP</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Understanding neural </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">network gradients</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">gradient descent</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Implementing an MLP </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">from scratch</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Implementing an MLP using deep </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">learning frameworks</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Designing </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">an MLP</span></span></li>
</ul>
<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.25.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.26.1">This chapter includes some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.27.1">Python</span></strong><span class="koboSpan" id="kobo.28.1"> programming language. </span><span class="koboSpan" id="kobo.28.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">Matplotlib</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">Seaborn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">Scikit-learn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">NumPy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">Keras</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">PyTorch</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">The code files are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.41.1">Exploring the foundations of neural networks using an MLP</span></h1>
<p><span class="koboSpan" id="kobo.42.1">A deep learning architecture </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.43.1">is created when at least three perceptron layers are used, excluding the input layer. </span><span class="koboSpan" id="kobo.43.2">A perceptron</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.44.1"> is a single-layer network consisting of neuron units. </span><span class="koboSpan" id="kobo.44.2">Neuron units hold a bias variable and act as nodes for vertices to be connected. </span><span class="koboSpan" id="kobo.44.3">These neurons will interact with other neurons in a separate layer with weights applied to the connections/vertices between neurons. </span><span class="koboSpan" id="kobo.44.4">A perceptron is also known </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.45.1">as a </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">fully connected layer</span></strong><span class="koboSpan" id="kobo.47.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">dense layer</span></strong><span class="koboSpan" id="kobo.49.1">, and</span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.50.1"> MLPs are </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.51.1">also known as </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">feedforward neural networks</span></strong><span class="koboSpan" id="kobo.53.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">fully connected </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.55.1">neural networks</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">Let’s refer back to the MLP figure from the previous chapter to get a </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">better idea.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<span class="koboSpan" id="kobo.59.1"><img alt="Figure 2.1 – Simple deep learning architecture, also called ﻿an MLP" src="image/B18187_02_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.60.1">Figure 2.1 – Simple deep learning architecture, also called an MLP</span></p>
<p><span class="koboSpan" id="kobo.61.1">The figure shows how three data column inputs get passed into the input layer, then subsequently get propagated to the hidden layer, and finally, through the output layer. </span><span class="koboSpan" id="kobo.61.2">Although not depicted in the figure, an additional activation function is applied at the hidden and output layer outputs. </span><span class="koboSpan" id="kobo.61.3">The activation function at the hidden layers adds non-linearity to the model and allows the neural network to capture non-linear relationships between the input data and output data. </span><span class="koboSpan" id="kobo.61.4">The activation function used at the output layer depends on the problem type and will be discussed in more detail in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.63.1">, </span><em class="italic"><span class="koboSpan" id="kobo.64.1">Exploring Supervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.65.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">Before we dive into the relevant </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.68.1">hidden</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.69.1"> activation methods, we need to first be aware of the vanishing gradient problem. </span><span class="koboSpan" id="kobo.69.2">The vanishing gradient problem is a challenge that arises when gradients of the loss function with respect to the model’s parameters become very small during backpropagation. </span><span class="koboSpan" id="kobo.69.3">This can lead to slow learning and poor convergence, as the weights update minimally or not at all. </span><span class="koboSpan" id="kobo.69.4">The vanishing gradient problem is particularly prominent when using activation functions that squash input values into a narrow range. </span><span class="koboSpan" id="kobo.69.5">To address this </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.70.1">issue, the </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">Rectified Linear Unit</span></strong><span class="koboSpan" id="kobo.72.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.73.1">ReLU</span></strong><span class="koboSpan" id="kobo.74.1">) activation function has been widely adopted due to its ability to mitigate the vanishing gradient problem to a certain extent. </span><span class="koboSpan" id="kobo.74.2">ReLU maps negative values into zeros and maintains positive values, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.75.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.76.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<span class="koboSpan" id="kobo.78.1"><img alt="Figure 2.2 – ReLU, Leaky ReLU, and PReLU inpu﻿t/output graph plot" src="image/B18187_02_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.79.1">Figure 2.2 – ReLU, Leaky ReLU, and PReLU input/output graph plot</span></p>
<p><span class="koboSpan" id="kobo.80.1">Apart from ReLU, there are other useful hidden layer activation functions that can help alleviate the vanishing gradient problem while offering various benefits. </span><span class="koboSpan" id="kobo.80.2">Some of these include </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.82.1">Leaky ReLU</span></strong><span class="koboSpan" id="kobo.83.1">: Leaky ReLU is a</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.84.1"> variation of the ReLU function that allows a small, non-zero gradient for negative input values. </span><span class="koboSpan" id="kobo.84.2">This helps mitigate the “dying ReLU” problem, where neurons become inactive and stop learning if their input values are consistently negative. </span><span class="koboSpan" id="kobo.84.3">Leaky ReLU introduces a small slope for negative inputs, ensuring that gradients do not </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">vanish entirely.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Parametric ReLU</span></strong><span class="koboSpan" id="kobo.87.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.88.1">PReLU</span></strong><span class="koboSpan" id="kobo.89.1">): PReLU is</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.90.1"> another variation of the ReLU function, where the negative slope is learned during the training process, allowing the model to adapt its behavior. </span><span class="koboSpan" id="kobo.90.2">This flexibility can lead to better performance but at the cost of increased complexity and the risk </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">of overfitting.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.92.1">Additionally, we will be exploring more hidden activation functions as we dive into different prominent architectures in this book. </span><span class="koboSpan" id="kobo.92.2">Each of these activation functions has its strengths and weaknesses, and the choice of activation function depends on the specific problem being addressed and the architecture being employed. </span><span class="koboSpan" id="kobo.92.3">Understanding, experimenting with, and assessing these activation functions is crucial for selecting the most suitable one for a given task within a neural network’s hidden layers. </span><span class="koboSpan" id="kobo.92.4">Furthermore, the recommended method to assess any model-building-related experiments will be explored in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.93.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.94.1">, </span><em class="italic"><span class="koboSpan" id="kobo.95.1">Exploring Supervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.96.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">Moving on, the process of propagating values from one layer to another is called a forward pass or forward propagation, where the formula can be generally defined </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.100.1">a</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.101.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.102.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.103.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.104.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.105.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.106.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.107.1">n</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.108.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.109.1">0</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.110.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.112.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.116.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.118.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.120.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.121.1">b</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.122.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.123.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.124.1">a</span></span><span class="koboSpan" id="kobo.125.1"> represents the</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.126.1"> outputs of </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.127.1">the neural network layer (called </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.128.1">an </span><strong class="bold"><span class="koboSpan" id="kobo.129.1">activation</span></strong><span class="koboSpan" id="kobo.130.1">), </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.131.1">g</span></span><span class="koboSpan" id="kobo.132.1"> represents the non-linear activation function, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.133.1">w</span></span><span class="koboSpan" id="kobo.134.1"> represents the weights between neuron connections, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.135.1">x</span></span><span class="koboSpan" id="kobo.136.1"> represents the input data or activation, and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.137.1">b</span></span><span class="koboSpan" id="kobo.138.1"> represents the bias of the neuron. </span><span class="koboSpan" id="kobo.138.2">Different types of neural network layers consume and output data in different ways but generally still use this formula as </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">a foundation.</span></span></p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.140.1">Understanding neural network gradients</span></h1>
<p><span class="koboSpan" id="kobo.141.1">The goal of machine learning for an </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.142.1">MLP is to find the weights and biases that will effectively map the inputs to the desired outputs. </span><span class="koboSpan" id="kobo.142.2">The weights and biases generally get initialized randomly. </span><span class="koboSpan" id="kobo.142.3">In the training process, with a provided dataset, they get updated iteratively and objectively in batches to minimize the loss function, which uses gradients computed with a method</span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.143.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.144.1">backward propagation</span></strong><span class="koboSpan" id="kobo.145.1">, also known</span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.146.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.147.1">backpropagation</span></strong><span class="koboSpan" id="kobo.148.1">. </span><span class="koboSpan" id="kobo.148.2">A batch is a subset of the dataset used for training or evaluation, allowing the neural network to process the data in smaller groups rather than the entire dataset at once. </span><span class="koboSpan" id="kobo.148.3">The loss function is also known as the error function or the </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">cost function.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">Backpropagation is a technique to find out how sensitive a change of weights and bias of every neuron is to the overall loss by using the partial derivative of the loss with respect to the weights and biases. </span><span class="koboSpan" id="kobo.150.2">Partial derivatives from calculus are a measure of the rate of change of a function with respect to a variable, which uses a technique </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.151.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">differentiation</span></strong><span class="koboSpan" id="kobo.153.1">, and is effectively applied in neural networks. </span><span class="koboSpan" id="kobo.153.2">A convenient method called</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.154.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">chain rule</span></strong><span class="koboSpan" id="kobo.156.1"> allows you to obtain the derivative of neural networks by calculating the partial derivatives of each function, a forward pass in the case of neural networks, separately. </span><span class="koboSpan" id="kobo.156.2">To be clear, derivatives can be called the sensitivity of change, the gradients, and the rate of change. </span><span class="koboSpan" id="kobo.156.3">The idea is that when we know which model parameter affects the error the most, we can update its weights proportionally according to its magnitude and direction. </span><span class="koboSpan" id="kobo.156.4">Let’s take a simple case of a two-layer MLP with one neuron in each layer as an example to get an idea of this, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.157.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.158.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.160.1"><img alt="Figure 2.3 – A diagram of a two-layer MLP" src="image/B18187_02_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.161.1">Figure 2.3 – A diagram of a two-layer MLP</span></p>
<p><span class="koboSpan" id="kobo.162.1">For clarity, </span><strong class="bold"><span class="koboSpan" id="kobo.163.1">w</span></strong><span class="koboSpan" id="kobo.164.1"> indicates the weight of neuron connections, </span><strong class="bold"><span class="koboSpan" id="kobo.165.1">b</span></strong><span class="koboSpan" id="kobo.166.1"> indicates the bias of the neurons, and </span><strong class="bold"><span class="koboSpan" id="kobo.167.1">L</span></strong><span class="koboSpan" id="kobo.168.1"> indicates layers. </span><span class="koboSpan" id="kobo.168.2">Different problem types require a different loss function, but for explanation purposes, let’s assume this is an MLP for a regression problem, where we will use the mean squared error as a loss function to compute the loss component from the final layer activation and the numerical target value. </span><span class="koboSpan" id="kobo.168.3">The loss function can then be defined as </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.170.1">L</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.171.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.172.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.173.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.174.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.175.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.176.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.177.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.178.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.179.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.180.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.181.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.182.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.183.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.184.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.185.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.186.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.187.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.188.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.189.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.190.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.191.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.192.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.193.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.194.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.195.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.196.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.197.1">is the total size of neurons. </span><span class="koboSpan" id="kobo.197.2">To obtain the rate of change of the loss function with respect to the output layer weight </span><strong class="bold"><span class="koboSpan" id="kobo.198.1">w2</span></strong><span class="koboSpan" id="kobo.199.1">, which is </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.200.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.201.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.203.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.204.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.205.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.206.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.207.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.208.1">2</span></span><span class="koboSpan" id="kobo.209.1">, let’s define the formula based on the chain rule. </span><span class="koboSpan" id="kobo.209.2">Consider </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.211.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.212.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.213.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.214.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.215.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.216.1">∙</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.218.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.219.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.220.1">b</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.221.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.222.1">So, if </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.224.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.225.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.226.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.227.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.228.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.229.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.230.1">)</span></span><span class="koboSpan" id="kobo.231.1">, where </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.232.1">g</span></span><span class="koboSpan" id="kobo.233.1"> is a ReLU function, the gradients with respect to the output layer weight </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.235.1">2</span></span><span class="koboSpan" id="kobo.236.1"> will be defined </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.238.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.240.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.242.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.243.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.244.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.245.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.246.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.247.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.248.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.249.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.250.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.251.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.252.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.253.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.254.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.255.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.256.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.257.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.258.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.259.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.260.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.261.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.262.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.263.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.264.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.265.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.266.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.267.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.268.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.269.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.270.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.271.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.272.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.273.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.274.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.275.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.276.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.277.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.278.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.279.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.280.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.281.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.282.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.283.1">The rate of change of the loss function with respect to </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.284.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.285.1">2</span></span><span class="koboSpan" id="kobo.286.1"> can be computed by multiplying the three independent change components: namely, the change of the loss function with respect to the second-layer outputs, the change of the activation outputs with respect to a wrapped </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.287.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.288.1">2</span></span><span class="koboSpan" id="kobo.289.1"> that is a</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.290.1"> forward pass without the activation, and the change of the wrapped </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.291.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.292.1">2</span></span><span class="koboSpan" id="kobo.293.1"> with respect to </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.294.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.295.1">2</span></span><span class="koboSpan" id="kobo.296.1">. </span><span class="koboSpan" id="kobo.296.2">Let’s define these components next. </span><span class="koboSpan" id="kobo.296.3">Now consider </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.298.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.299.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.300.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.301.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.302.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.303.1">y</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Based on the chain rule, the first change component will be defined as </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.306.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.307.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.308.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.309.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.310.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.311.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.312.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.313.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.314.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.315.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.316.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.317.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.318.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.319.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.320.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.321.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.322.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.323.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.324.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.325.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.326.1">⋅</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.327.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.328.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.329.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.330.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.331.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.332.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.333.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.334.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.335.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.336.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.337.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.338.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.339.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.340.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.341.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.342.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.343.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.344.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.345.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.346.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.347.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.348.1">e</span></span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.349.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.350.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.351.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.352.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.353.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.354.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.355.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.356.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.357.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.358.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.359.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.360.1">1</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">Putting this in the simplified component representation will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">following equation:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.363.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.364.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.365.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.366.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.367.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.368.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.369.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.370.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.371.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.372.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.373.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.374.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.375.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.376.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.377.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.378.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.379.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.380.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.381.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.382.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.383.1">2</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.384.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.385.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.386.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.387.1">For the second change component, it can be defined with the </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.389.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.390.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.391.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.392.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.393.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.394.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.395.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.396.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.397.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.398.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.399.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.400.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.401.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.402.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.403.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.404.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.405.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.406.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.407.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.408.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.409.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.410.1">1</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">There is no activation</span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.412.1"> function applied at the output layer in </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">this case.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">For the third and last change component, it can be defined with the </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.416.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.417.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.418.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.419.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.420.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.421.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.422.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.423.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.424.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.425.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.426.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.427.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.428.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.429.1">1</span></span></span></p>
<p><span class="koboSpan" id="kobo.430.1">Finally, placing the simplified representation of the three components into the formula to obtain the gradients of the output layer weights </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.431.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.432.1">2</span></span><span class="koboSpan" id="kobo.433.1"> will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">following equation:</span></span></p>
<p> <span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.435.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.436.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.437.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.438.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.439.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.440.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.441.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.442.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.443.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.444.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.445.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.446.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.447.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.448.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.449.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.450.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.451.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.452.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.453.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.454.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.455.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.456.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.457.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.458.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.459.1">1</span></span></span></p>
<p><span class="koboSpan" id="kobo.460.1">All you need to do now is to plug in the actual values to obtain the layer 2 weight gradients. </span><span class="koboSpan" id="kobo.460.2">The same formula structure can be adapted similarly to the hidden layer’s weight </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.461.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.462.1">1</span></span> <span class="No-Break"><span class="koboSpan" id="kobo.463.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.464.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.465.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.466.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.467.1">g</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.468.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.469.1">z</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.470.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.471.1">)</span></span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.472.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.473.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.474.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.475.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.476.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.477.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.478.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.479.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.480.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.481.1">b</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.482.1">1</span></span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.483.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.484.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.485.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.486.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.487.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.488.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.489.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.490.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.491.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.492.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.493.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.494.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.495.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.496.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.497.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.498.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.499.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.500.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.501.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.502.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.503.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.504.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.505.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.506.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.507.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.508.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.509.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.510.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.511.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.512.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.513.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.514.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.515.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.516.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.517.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.518.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.519.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.520.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.521.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.522.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.523.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.524.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.525.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.526.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.527.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.528.1">After expanding </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.529.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.530.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.531.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.532.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.533.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.534.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.535.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.536.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.537.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.538.1">using the chain rule, the formula can then be </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">the following:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.540.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.541.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.542.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.543.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.544.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.545.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.546.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.547.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.548.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.549.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.550.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.551.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.552.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.553.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.555.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.556.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.557.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.558.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.559.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.560.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.561.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.562.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.565.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.566.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.567.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.568.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.569.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.570.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.571.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.572.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.573.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.574.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.575.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.576.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.577.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.578.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.579.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.580.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.581.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.582.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.583.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.584.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.585.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.586.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.587.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.588.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.589.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.590.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.591.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.592.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.593.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.594.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.595.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.596.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.597.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.598.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.599.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.600.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.601.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.602.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.603.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.604.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.605.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.606.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.607.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.608.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.609.1">The additional individual </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.610.1">components can be defined </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.612.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.613.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.614.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.615.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.616.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.617.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.618.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.619.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.620.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.621.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.622.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.623.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.624.1">w</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.625.1">2</span></span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.626.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.627.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.628.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.629.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.630.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.631.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.632.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.633.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.634.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.635.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.636.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.637.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.638.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.639.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.640.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.641.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.642.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.643.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.644.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.645.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.646.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.647.1">0</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.648.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.649.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.650.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.651.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.652.1">&lt;</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.653.1">0</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.654.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.655.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.656.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.657.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.658.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.659.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.660.1">&gt;</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.661.1">0</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.662.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.663.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.664.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.665.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.666.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.667.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.668.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.669.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.670.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.671.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.672.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.673.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.674.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.675.1">0</span></span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.676.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.677.1">0</span></span><span class="koboSpan" id="kobo.678.1"> here is the input data. </span><span class="koboSpan" id="kobo.678.2">Now, let’s define the gradient of the hidden layer weights </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.679.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.680.1">1</span></span><span class="koboSpan" id="kobo.681.1"> with the representation where we can plug in actual values to compute it, </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.683.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.684.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.685.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.686.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.687.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.688.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.689.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.690.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.691.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.692.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.693.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.694.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.695.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.696.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.697.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.698.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.699.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.700.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.701.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.702.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.703.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.704.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.705.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.706.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.707.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.708.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.709.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.710.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.711.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.712.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.713.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.714.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.715.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.716.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.717.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.718.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.719.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.720.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.721.1">0</span></span></span></p>
<p><span class="koboSpan" id="kobo.722.1">The same process can be repeated for the bias term to obtain its gradients. </span><span class="koboSpan" id="kobo.722.2">Only the partial derivative of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.723.1">z</span></span><span class="koboSpan" id="kobo.724.1"> with respect to the weights needs to be replaced with a partial derivative of </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.725.1">z</span></span><span class="koboSpan" id="kobo.726.1"> with respect to the biases, as </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">shown here:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.728.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.729.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.730.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.731.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.732.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.733.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.734.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.735.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.736.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.737.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.738.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.739.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.740.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.741.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.742.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.743.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.744.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.745.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.746.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.747.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.748.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.749.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.750.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.751.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.752.1">1</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.753.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.754.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.755.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.756.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.757.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.758.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.759.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.760.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.761.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.762.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.763.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.764.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.765.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.766.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.767.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.768.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.769.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.770.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.771.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.772.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.773.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.774.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.775.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.776.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.777.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.778.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.779.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.780.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.781.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.782.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.783.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.784.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.785.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.786.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.787.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.788.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.789.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.790.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.791.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.792.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.793.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.794.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.795.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.796.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.797.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.798.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.799.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.800.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.801.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.802.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.803.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.804.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.805.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.806.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.807.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.808.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.809.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.810.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.811.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.812.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.813.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.814.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.815.1">)</span></span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.816.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.817.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.818.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.819.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.820.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.821.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.822.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.823.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.824.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.825.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.826.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.827.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.828.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.829.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.830.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.831.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.832.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.833.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.834.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.835.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.836.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.837.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.838.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.839.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.840.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.841.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.842.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.843.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.844.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.845.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.846.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.847.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.848.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.849.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.850.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.851.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.852.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.853.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.854.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.855.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.856.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.857.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.858.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.859.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.860.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.861.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.862.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.863.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.864.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.865.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.866.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.867.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.868.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.869.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.870.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.871.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.872.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.873.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.874.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.875.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.876.1">z</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.877.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.878.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.879.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.880.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.881.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.882.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.883.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.884.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.885.1">Now, let’s define the gradient of the first bias term with the representation where we can plug in actual values to compute it, </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.887.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.888.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.889.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.890.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.891.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.892.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.893.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.894.1">b</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.895.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.896.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.897.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.898.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.899.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.900.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.901.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.902.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.903.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.904.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.905.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.906.1">w</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.907.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.908.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.909.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.910.1">a</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.911.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.912.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.913.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.914.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.915.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.916.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.917.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.918.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.919.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.920.1">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.921.1">z</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.922.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.923.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.924.1">The previously defined formulae were meant to be specific to the example neural network for layers with one neuron. </span><span class="koboSpan" id="kobo.924.2">In practical usage, these layers usually contain more than one neuron in each of the layers. </span><span class="koboSpan" id="kobo.924.3">To compute the loss and derivatives for layers with more than </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.925.1">one neuron, and for more than one data sample, you simply need to obtain an average of all </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">the values.</span></span></p>
<p><span class="koboSpan" id="kobo.927.1">Once the gradients or derivatives are obtained, different strategies can be used to update the weights. </span><span class="koboSpan" id="kobo.927.2">The algorithm used to optimize the weights and biases of the neural network is called the optimizer. </span><span class="koboSpan" id="kobo.927.3">There are many optimizer options </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.928.1">today and each has its own pros and cons. </span><span class="koboSpan" id="kobo.928.2">As gradients are used to optimize weights and biases, this process of optimization is called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.929.1">gradient descent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">.</span></span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.931.1">Understanding gradient descent</span></h1>
<p><span class="koboSpan" id="kobo.932.1">A good way to think </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.933.1">about loss for a deep learning model is that it exists in a three-dimensional loss landscape that has many different hills and valleys, with valleys being more optimal, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.934.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.935.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.937.1"><img alt="Figure 2.4 – An example loss landscape" src="image/B18187_02_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.938.1">Figure 2.4 – An example loss landscape</span></p>
<p><span class="koboSpan" id="kobo.939.1">In reality, however, we can only approximate these loss landscapes as the parameter values of the neural networks can exist in an infinite number of ways. </span><span class="koboSpan" id="kobo.939.2">The most common way practitioners use to monitor the behavior of loss during each epoch of training and validation is to simply plot a two-dimensional line graph with the </span><em class="italic"><span class="koboSpan" id="kobo.940.1">x</span></em><span class="koboSpan" id="kobo.941.1"> axis being the epochs executed and the </span><em class="italic"><span class="koboSpan" id="kobo.942.1">y</span></em><span class="koboSpan" id="kobo.943.1"> axis being the loss performance. </span><span class="koboSpan" id="kobo.943.2">An epoch is a single iteration through the entire dataset during the training process of a neural network. </span><span class="koboSpan" id="kobo.943.3">The loss landscape in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.944.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.945.1">.4</span></em><span class="koboSpan" id="kobo.946.1"> is an approximation of the loss landscape in three dimensions of a neural network. </span><span class="koboSpan" id="kobo.946.2">To visualize the three-dimensional loss landscape in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.947.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.948.1">.4</span></em><span class="koboSpan" id="kobo.949.1">, we can use two randomly initialized parameters and one fully trained parameter from the same neuron positions within the neural network. </span><span class="koboSpan" id="kobo.949.2">The loss can be calculated by performing a weighted summation of these three parameters. </span><span class="koboSpan" id="kobo.949.3">The weight of the fully trained parameter remains constant, while the weights of the two randomly initialized parameters are adjusted. </span><span class="koboSpan" id="kobo.949.4">This process allows us to approximate the 3D loss landscape shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.950.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.951.1">.4</span></em><span class="koboSpan" id="kobo.952.1">. </span><span class="koboSpan" id="kobo.952.2">In this figure, </span><em class="italic"><span class="koboSpan" id="kobo.953.1">x</span></em><span class="koboSpan" id="kobo.954.1"> axis and </span><em class="italic"><span class="koboSpan" id="kobo.955.1">y</span></em><span class="koboSpan" id="kobo.956.1"> axis are the weights of the two randomly initialized parameters of the same neural network and the </span><em class="italic"><span class="koboSpan" id="kobo.957.1">z</span></em><span class="koboSpan" id="kobo.958.1"> axis is the loss value. </span><span class="koboSpan" id="kobo.958.2">The goal of gradient descent is to attempt to find the </span><em class="italic"><span class="koboSpan" id="kobo.959.1">global</span></em><span class="koboSpan" id="kobo.960.1"> deepest valleys and not be stuck in </span><em class="italic"><span class="koboSpan" id="kobo.961.1">local</span></em><span class="koboSpan" id="kobo.962.1"> valleys or </span><em class="italic"><span class="koboSpan" id="kobo.963.1">local</span></em><span class="koboSpan" id="kobo.964.1"> minima. </span><span class="koboSpan" id="kobo.964.2">The gradients computed provide the suggested directions needed to nudge and update the weights and biases iteratively. </span><span class="koboSpan" id="kobo.964.3">One</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.965.1"> thing to note is that gradients provide the direction to increase the loss function in the fastest way, so for the descent, the parameters are subtracted from the gradients. </span><span class="koboSpan" id="kobo.965.2">Let’s go through a simple form of gradient descent that controls how the weights and biases should </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">be updated:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.967.1">w</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.968.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.969.1">w</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.970.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.971.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.972.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.973.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.974.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.975.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.976.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.977.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.978.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.979.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.980.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.981.1"> </span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.982.1">b</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.983.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.984.1">b</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.985.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.986.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.987.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.988.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.989.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.990.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.991.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.992.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.993.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.994.1">δ</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.995.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.996.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.997.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.998.1">α</span></span><span class="koboSpan" id="kobo.999.1"> refers to the learning rate, which controls how aggressive you want the deep learning model to be. </span><span class="koboSpan" id="kobo.999.2">A learning rate is a hyperparameter that controls the speed at which a neural network learns and updates its weights and biases during the optimization process. </span><span class="koboSpan" id="kobo.999.3">The higher the learning rate, the bigger the steps taken by the deep learning model in the loss landscape. </span><span class="koboSpan" id="kobo.999.4">By iteratively applying this parameter update step, the neural network will slowly move downhill so that the learned set of parameters can allow the network to effectively map the input to the desired target values. </span><span class="koboSpan" id="kobo.999.5">The gradients are </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.1000.1">obtained for all the data samples and averaged together to obtain a single update direction for the weight and </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">biases update.</span></span></p>
<p><span class="koboSpan" id="kobo.1002.1">Datasets can sometimes be too big and cause a slow learning process from basic gradient descent due to the need to compute the gradients from every sample before an update can be done to the neural network</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.1003.1"> parameters. </span><strong class="bold"><span class="koboSpan" id="kobo.1004.1">Stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.1005.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1006.1">SGD</span></strong><span class="koboSpan" id="kobo.1007.1">) was created to tackle this problem. </span><span class="koboSpan" id="kobo.1007.2">The idea is simply to learn from the dataset in batches and iteratively learn the entire dataset with a different data batch partition instead of waiting for the gradients to be obtained from the entire dataset before updating the parameters of the network. </span><span class="koboSpan" id="kobo.1007.3">This way, the learning process can be efficient even for a big-sized dataset with the added benefit of seeing initial </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">results quickly.</span></span></p>
<p><span class="koboSpan" id="kobo.1009.1">There are a lot more variations of gradient descent that offer different advantages and are suited for specific situations. </span><span class="koboSpan" id="kobo.1009.2">Here, we will list gradient descent algorithms that, on average across a variety of datasets, work well and </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">are relevant:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1011.1">Momentum</span></strong><span class="koboSpan" id="kobo.1012.1">: A variation of SGD, Momentum incorporates a “momentum” term that helps the optimizer </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.1013.1">navigate through the loss landscape more effectively. </span><span class="koboSpan" id="kobo.1013.2">This momentum term is a moving average of the gradients, which helps the optimizer overcome local minima and converge faster. </span><span class="koboSpan" id="kobo.1013.3">The momentum term also adds some inertia to the optimizer, causing it to take bigger steps in directions that have consistent gradients, which can speed </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">up convergence.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1015.1">Root Mean Square Propagation</span></strong><span class="koboSpan" id="kobo.1016.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1017.1">RMSProp</span></strong><span class="koboSpan" id="kobo.1018.1">): RMSProp is an adaptive learning rate optimization </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.1019.1">algorithm that adjusts the learning rate for each parameter individually. </span><span class="koboSpan" id="kobo.1019.2">By dividing the learning rate by an exponentially decaying average of squared gradients, RMSProp helps to prevent the oscillations observed in the convergence of SGD. </span><span class="koboSpan" id="kobo.1019.3">This results in a more stable and faster convergence toward the </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">optimal solution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1021.1">Adaptive Moment Estimation</span></strong><span class="koboSpan" id="kobo.1022.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1023.1">Adam</span></strong><span class="koboSpan" id="kobo.1024.1">): Adam is another popular optimization algorithm that </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.1025.1">combines the advantages of both Momentum and RMSProp. </span><span class="koboSpan" id="kobo.1025.2">It maintains separate adaptive learning rates for each parameter, as well as incorporating a momentum term. </span><span class="koboSpan" id="kobo.1025.3">This combination allows Adam to converge quickly and find more accurate solutions, making it a popular choice for many deep </span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">learning tasks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1027.1">While there are many gradient descent algorithms available, choosing the right one depends on the specific problem and dataset at hand. </span><span class="koboSpan" id="kobo.1027.2">In general, Adam is often recommended as a good </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.1028.1">starting point due to its adaptive nature and combination of Momentum and RMSProp features. </span><span class="koboSpan" id="kobo.1028.2">To determine the best fit for your specific deep learning task, it is essential to experiment with different algorithms and their hyperparameters and validate their performance. </span><span class="koboSpan" id="kobo.1028.3">Next, we will code up an MLP </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">using Python.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.1030.1">Implementing an MLP from scratch</span></h1>
<p><span class="koboSpan" id="kobo.1031.1">Today, the process to create a</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.1032.1"> neural network and its layers along with the backpropagation process has been encapsulated in deep learning frameworks. </span><span class="koboSpan" id="kobo.1032.2">The differentiation process has been automated, where there is no actual need to define the derivative formulas manually. </span><span class="koboSpan" id="kobo.1032.3">Removing the abstraction layer provided by the deep learning libraries will help to solidify your understanding of neural network internals. </span><span class="koboSpan" id="kobo.1032.4">So, let’s create this neural network manually and explicitly with the logic to forward pass and backward pass instead of using the deep </span><span class="No-Break"><span class="koboSpan" id="kobo.1033.1">learning libraries:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1034.1">We’ll start by importing </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">numpy</span></strong><span class="koboSpan" id="kobo.1036.1"> and the methods from the scikit-learn library to load sample datasets and perform </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">data partitioning:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1038.1">
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split</span></pre></li> <li><span class="koboSpan" id="kobo.1039.1">Next, we define ReLU, the method that makes an </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">MLP non-linear:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1041.1">
def ReLU(x):
  return np.maximum(x, 0)</span></pre></li> <li><span class="koboSpan" id="kobo.1042.1">Now, let’s define </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.1043.1">partially the class needed to initialize an MLP model with a single hidden layer and an output layer that can perform a forward pass. </span><span class="koboSpan" id="kobo.1043.2">The layers are represented by weights, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">w1</span></strong><span class="koboSpan" id="kobo.1045.1"> is the weight of the hidden layer, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1046.1">w2</span></strong><span class="koboSpan" id="kobo.1047.1"> is the weight of the output layer. </span><span class="koboSpan" id="kobo.1047.2">Additionally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1048.1">b1</span></strong><span class="koboSpan" id="kobo.1049.1"> is the bias for the connection between the input layer and the hidden layer and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1050.1">b2</span></strong><span class="koboSpan" id="kobo.1051.1"> is the bias for the connection between the hidden layer and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">output layer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1053.1">
class MLP(object):
  def __init__(
    self, input_layer_size, hidden_layer_size, output_layer_size, seed=1234
):
    rng = np.random.RandomState(seed)
    self.w1 = rng.normal(
      size=(input_layer_size, hidden_layer_size)
    )
    self.b1 = np.zeros(hidden_layer_size)
    self.w2 = rng.normal(
      size=(hidden_layer_size, output_layer_size)
    )
    self.b2 = np.zeros(output_layer_size)
    self.output_layer_size = output_layer_size
    self.hidden_layer_size = hidden_layer_size
def forward_pass(self, x):
    z1 = np.dot(x, self.w1) + self.b1
    a1 = ReLU(z1)
    z2 = np.dot(a1, self.w2)  + self.b2
    a2 = z2
    return z1, a1, z2, a2</span></pre></li> <li><span class="koboSpan" id="kobo.1054.1">To allow the MLP to learn, we will now implement the backward pass method to generate the average</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.1055.1"> gradients for the biases </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">and weights:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1057.1">
def ReLU_gradient(x):
      return np.where(x &gt; 0, 1, 0)
def backward_pass(self, a0, z1, a1, z2, a2, y):
    number_of_samples = len(a2)
    average_gradient_w2 = (
      np.dot(a1.T, (a2 - y)) *
      (2 / (number_of_samples * self.output_layer_size))
    )
    average_gradient_b2 = (
      np.mean((a2 - y), axis=0) * (2 / self.output_layer_size)
    )
    average_gradient_w1 = np.dot(
      a0.T, np.dot((a2 - y), self.w2.T) * ReLU_gradient(z1)
    ) * 2 / (number_of_samples * self.output_layer_size)
    average_gradient_b1 = np.mean(
      np.dot((a2 - y), self.w2.T) * ReLU_gradient(z1), axis=0
    ) *  2 / self.output_layer_size
    return (
      average_gradient_w2, average_gradient_b2, average_gradient_w1, average_gradient_b1
    )</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1058.1">Notice that the derivative </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.1059.1">of the ReLU function is </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1060.1">f</span></span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1061.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1062.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1063.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1064.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1065.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1066.1">1</span></span><span class="koboSpan" id="kobo.1067.1"> if </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1068.1">x</span></span><span class="koboSpan" id="kobo.1069.1"> &gt; 0 and </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1070.1">f</span></span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.1071.1">′</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1072.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1073.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1074.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1075.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1076.1">0</span></span><span class="koboSpan" id="kobo.1077.1"> if </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1078.1">x</span></span><span class="koboSpan" id="kobo.1079.1"> &lt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">0.</span></span></p></li> <li><span class="koboSpan" id="kobo.1081.1">For the last class method, we will implement the gradient descent step utilizing the average gradients from the backward pass, which is the process that allows the bias and weights to </span><span class="No-Break"><span class="koboSpan" id="kobo.1082.1">be updated:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1083.1">
def gradient_descent_step(
    self, learning_rate, average_gradient_w2, average_gradient_b2, average_gradient_w1, average_gradient_b1
  ):
    self.w2 = self.w2 - learning_rate * average_gradient_w2
    self.b2 = self.b2 - learning_rate * average_gradient_b2
    self.w1 = self.w1 - learning_rate * average_gradient_w1
    self.b1 = self.b1 - learning_rate * average_gradient_b1</span></pre></li> <li><span class="koboSpan" id="kobo.1084.1">Now that we have made a proper class for the MLP manually, let’s set up a dataset and attempt to learn from it. </span><span class="koboSpan" id="kobo.1084.2">The structure of MLP only allows for tabular structured dataset types that are both one-dimensional and numerical, so we will be using a dataset called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1085.1">diabetes</span></strong><span class="koboSpan" id="kobo.1086.1">, which contains 10 numerical features, that is, age, sex, body mass index, average blood pressure, and 6 blood serum measurements, as inputs along with a quantitative measure of diabetes disease progression as the target data. </span><span class="koboSpan" id="kobo.1086.2">The data is conveniently saved in the scikit-learn library, so let’s first load the </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">input DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1088.1">
diabetes_data = datasets.load_diabetes(as_frame=True)
diabetes_df = diabetes_data['data']</span></pre></li> <li><span class="koboSpan" id="kobo.1089.1">Now, we will convert the </span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.1090.1">DataFrame into NumPy array values so that it is ready to be used by a </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">neural network:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1092.1">
X = diabetes_df.values</span></pre></li> <li><span class="koboSpan" id="kobo.1093.1">The final step for loading the data is to load the target data from the diabetes data and make sure it has an additional outer dimension as the PyTorch model outputs its predictions in </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">this way:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1095.1">
target = np.expand_dims(diabetes_data['target'], 1)</span></pre></li> <li><span class="koboSpan" id="kobo.1096.1">Next, let’s partition the dataset into 80% for training and 20% </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">for validation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1098.1">
X_train, X_val, y_train, y_val = train_test_split(
  X, target, test_size=0.20, random_state=42
)</span></pre></li> <li><span class="koboSpan" id="kobo.1099.1">Now that the data is prepared with training and evaluation partitions, let’s initialize an MLP model from our defined class with a single 20-neuron hidden layer and a 1-neuron </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">output layer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1101.1">
mlp_model = MLP(
  input_layer_size=len(diabetes_df.columns),
  hidden_layer_size=20,
  output_layer_size=target.shape[1]
)</span></pre></li> <li><span class="koboSpan" id="kobo.1102.1">With the data and model ready, it is time to train the model we built from scratch. </span><span class="koboSpan" id="kobo.1102.2">Since the dataset is small enough, with 442 samples, there isn’t a runtime issue using</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.1103.1"> gradient descent, so we will be using the full gradient descent here for 100 epochs. </span><span class="koboSpan" id="kobo.1103.2">One epoch means a full round of training going through the entire training </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">dataset once:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1105.1">
iterations = 100
training_error_per_epoch = []
validation_error_per_epoch = []
for i in range(iterations):
  z1, a1, z2, a2 = mlp_model.forward_pass(X_train)
  (
    average_gradient_w2,
    average_gradient_b2,
    average_gradient_w1,
    average_gradient_b1
  ) = mlp_model.backward_pass(X_train, z1, a1, z2, a2, y_train)
  mlp_model.gradient_descent_step(
    learning_rate=0.1,
    average_gradient_w2=average_gradient_w2,
    average_gradient_b2=average_gradient_b2,
    average_gradient_w1=average_gradient_w1,
    average_gradient_b1=average_gradient_b1,
  )
  _, _, _, a2_val = mlp_model.forward_pass(X_val)
  training_error_per_epoch.append(mean_squared_error(y_train, a2)
  validation_error_per_epoch.append(
    mean_squared_error(y_val, a2_val)
  )</span></pre></li> <li><span class="koboSpan" id="kobo.1106.1">Let’s plot the collected mean </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.1107.1">squared error for both the validation and training partitions </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1109.1">matplotlib</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1111.1">
plt.figure(figsize=(10, 6))
plt.plot(training_error_per_epoch)
plt.plot(validation_error_per_epoch,  linestyle = 'dotted')
plt.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1112.1">This will produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.1113.1">following plot:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.1114.1"><img alt="Figure 2.5 – Training and validation partition mean squared error versus epochs plots" src="image/B18187_02_005.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1115.1">Figure 2.5 – Training and validation partition mean squared error versus epochs plots</span></p>
<p><span class="koboSpan" id="kobo.1116.1">With that, you’ve implemented </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.1117.1">an MLP and trained it from scratch without depending on deep learning frameworks! </span><span class="koboSpan" id="kobo.1117.2">But is our implementation correct and sound? </span><span class="koboSpan" id="kobo.1117.3">Let’s verify this in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">next topic.</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.1119.1">Implementing MLP using deep learning frameworks</span></h2>
<p><span class="koboSpan" id="kobo.1120.1">Deep learning frameworks are made</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.1121.1"> to ease and expedite the development of deep learning models. </span><span class="koboSpan" id="kobo.1121.2">They provide a plethora of commonly used neural network layers, optimizers, and tools generally used to build </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.1122.1">neural network models, along with very easily extensible interfaces to implement new methods. </span><span class="koboSpan" id="kobo.1122.2">Backpropagation itself is abstracted away from the users of the frameworks as the gradients are computed automatically in the background when needed. </span><span class="koboSpan" id="kobo.1122.3">Most importantly, they allow the usage of the GPU for efficient model training </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">and prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.1124.1">In this section, we will build the same MLP model as in the previous section, using a deep learning framework called PyTorch, and verify that both implementations produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">same results:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1126.1">We’ll start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1128.1">
Import torch
import torch.nn as nn
import torch.nn.functional as F</span></pre></li> <li><span class="koboSpan" id="kobo.1129.1">Next, let’s define the</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.1130.1"> MLP class with the two fully connected layers along with the forward </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.1131.1">propagation method with arguments that allow us to set the input layer size, hidden layer size, and output </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">layer size:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1133.1">
Class MLPPytorch(nn.Module):
  def __init__(
    self, input_layer_size, hidden_layer_size, output_layer_size
  ):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)
    self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)
class MLPPytorch(nn.Module):
  def __init__(
    self, input_layer_size, hidden_layer_size, output_layer_size
  ):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)
    self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)
  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x</span></pre></li> <li><span class="koboSpan" id="kobo.1134.1">You will notice that </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.1135.1">there isn’t a backward propagation function implemented, which reduces the amount of effort needed to define a neural network model. </span><span class="koboSpan" id="kobo.1135.2">When </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.1136.1">you inherit from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1137.1">Pytorch</span></strong><span class="koboSpan" id="kobo.1138.1"> module class, the backward propagation functionality will already be provided out of the box with your defined </span><strong class="source-inline"><span class="koboSpan" id="kobo.1139.1">Pytorch</span></strong><span class="koboSpan" id="kobo.1140.1"> layers. </span><span class="koboSpan" id="kobo.1140.2">Finally, let’s initialize the MLP using a hidden layer size of 10 along with input and output sizes according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">diabetes dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1142.1">
Net = MLPPytorch(
  input_layer_size=len(diabetes_df.columns),
  hidden_layer_size=10,
  output_layer_size=y_train.shape[1],
)</span></pre></li> <li><span class="koboSpan" id="kobo.1143.1">Now, let’s check the forward and backward propagation functionality with our </span><strong class="source-inline"><span class="koboSpan" id="kobo.1144.1">numpy</span></strong><span class="koboSpan" id="kobo.1145.1"> variant. </span><span class="koboSpan" id="kobo.1145.2">First, let’s initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1146.1">Pytorch</span></strong><span class="koboSpan" id="kobo.1147.1"> MLP and copy the weights from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1148.1">numpy</span></strong><span class="koboSpan" id="kobo.1149.1">-based </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">MLP model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1151.1">
with torch.no_grad():
       net.fc1.weight.copy_(
    torch.from_numpy(mlp_model.w1.T)
  )
  net.fc1.bias.copy_(
    torch.from_numpy(mlp_model.b1)
  )
  net.fc2.weight.copy_(
    torch.from_numpy(mlp_model.w2.T)
  )
  net.fc2.bias.copy_(
    torch.from_numpy(mlp_model.b2)
  )</span></pre></li> <li><span class="koboSpan" id="kobo.1152.1">Now, let’s prepare the</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.1153.1"> dataset into Tensor objects suitable for PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">model consumption:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1155.1">
torch_input = torch.from_numpy(X_train)
torch_target = torch.from_numpy(y_train)</span></pre></li> <li><span class="koboSpan" id="kobo.1156.1">To obtain the same gradients, we</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.1157.1"> have to use the same MSE loss and apply </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">backward propagation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1159.1">
criterion = nn.MSELoss()
output = net(torch_input.float())
loss = criterion(output, torch_target.float())
loss.backward()</span></pre></li> <li><span class="koboSpan" id="kobo.1160.1">Now, let’s verify the gradients for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">two implementations:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1162.1">
np.testing.assert_almost_equal(output.detach().numpy(), a2, decimal=3)
np.testing.assert_almost_equal(net.fc2.weight.grad.numpy(), average_gradient_w2.T, decimal=3)
np.testing.assert_almost_equal(net.fc2.bias.grad.numpy(), average_gradient_b2, decimal=3)
np.testing.assert_almost_equal(net.fc1.weight.grad.numpy(), average_gradient_w1.T, decimal=3)
np.testing.assert_almost_equal(net.fc1.bias.grad.numpy(), average_gradient_b1, decimal=3)</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.1163.1">This solidifies your foundational </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.1164.1">neural network knowledge, along with knowledge of the MLP architecture, and </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.1165.1">prepares you for more advanced concepts in the realm of deep learning. </span><span class="koboSpan" id="kobo.1165.2">Before we move on to look at a more advanced neural network, in the next section, we will explore the topic of regularization, and then finally, explore how to design an MLP with a practical </span><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">use case.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.1167.1">Regularization</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1168.1">Regularization</span></strong><span class="koboSpan" id="kobo.1169.1"> in deep learning has </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.1170.1">evolved into a state where it now means any addition or modification to the neural network, data, or training process that is used to increase the generalization of the build model to external data. </span><span class="koboSpan" id="kobo.1170.2">All performant neural networks today have some form of regularization embedded into the architecture. </span><span class="koboSpan" id="kobo.1170.3">Some of these regularization methods introduce some extra beneficial side effects, such as the speedup of training or the performance on the training dataset. </span><span class="koboSpan" id="kobo.1170.4">But ultimately, the regularizer’s main goal is to improve generalization, which in other words is to improve performance metrics and reduce errors on external data. </span><span class="koboSpan" id="kobo.1170.5">As a quick recap, the following list shows a summary of some of the more common </span><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">regularization methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1172.1">Dropout layer</span></strong><span class="koboSpan" id="kobo.1173.1">: During training, randomly remove information from all neural nodes according to a specified </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.1174.1">probability level by replacing neural node outputs with zeros, effectively nullifying information. </span><span class="koboSpan" id="kobo.1174.2">This reduces over-reliance on any single node/information and increases the probability </span><span class="No-Break"><span class="koboSpan" id="kobo.1175.1">of generalization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1176.1">L1/L2 regularization</span></strong><span class="koboSpan" id="kobo.1177.1">: These methods add a penalty term to the loss function, which discourages the model from assigning high weights to the features. </span><span class="koboSpan" id="kobo.1177.2">L1 regularization, also known as</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.1178.1"> Lasso, uses the absolute value of the weights, while L2 regularization, also known as Ridge, uses the squared value of the weights. </span><span class="koboSpan" id="kobo.1178.2">By controlling the magnitude of the weights, these methods help to prevent overfitting and improve generalization. </span><span class="koboSpan" id="kobo.1178.3">Typically, this is applied to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">input features.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1180.1">Batch normalization layer</span></strong><span class="koboSpan" id="kobo.1181.1">: This method is standardizing the data in both training and inferencing on the external data </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.1182.1">stage by scaling the data to have a mean of zero and a standard deviation of one. </span><span class="koboSpan" id="kobo.1182.2">This is done by removing the computed mean and dividing it by the computed standard deviation. </span><span class="koboSpan" id="kobo.1182.3">The mean and standard deviation </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.1183.1">are computed and iteratively updated by mini-batch (based on the models determining the training batch size) during training. </span><span class="koboSpan" id="kobo.1183.2">During inference, the final learned running mean and standard deviation calculated during training are applied. </span><span class="koboSpan" id="kobo.1183.3">This has the side effect of improving the training time, training stability, and generalization. </span><span class="koboSpan" id="kobo.1183.4">Note that each element has its own mean and standard deviation. </span><span class="koboSpan" id="kobo.1183.5">Research has shown that batch normalization smooths out the loss landscape, making it way easier to reach an </span><span class="No-Break"><span class="koboSpan" id="kobo.1184.1">optimum value.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1185.1">Group normalization layer</span></strong><span class="koboSpan" id="kobo.1186.1">: Instead of having an individual mean and standard deviation for each element </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.1187.1">across the batch size, group normalization standardizes the data by groups per sample, where each group has one mean and one standard deviation. </span><span class="koboSpan" id="kobo.1187.2">The number of groups can be configured. </span><span class="koboSpan" id="kobo.1187.3">Batch normalization degrades the performance when the number of samples in a batch is small due to hardware limitations. </span><span class="koboSpan" id="kobo.1187.4">This layer is used over batch normalization when the amount of data per batch is a very small number as the mean and standardization updates do not depend on the batch. </span><span class="koboSpan" id="kobo.1187.5">In large batches, however, batch normalization </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">still triumphs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1189.1">Weight standardization</span></strong><span class="koboSpan" id="kobo.1190.1">: This applies the same standardization process to the weights of the neural networks. </span><span class="koboSpan" id="kobo.1190.2">The weights of the neural network might grow to very large numbers after training, which would create large output values. </span><span class="koboSpan" id="kobo.1190.3">The idea is that if we use the batch normalization layer, the output values will be standardized </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.1191.1">anyway, so why don’t we take a step back and apply the same process to the weights themselves, making sure the values are standardized in some form before becoming an output value? </span><span class="koboSpan" id="kobo.1191.2">Some simple benchmarks have demonstrated that it works well when combined with a group normalization layer in low batch sizes, achieving a better performance than batch normalization with a high batch </span><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">size setting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1193.1">Stochastic depth</span></strong><span class="koboSpan" id="kobo.1194.1">: Instead of conceptually making a neural network with narrower layers during the</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.1195.1"> training stage with dropout, stochastic depth reduces the depth of the network during training. </span><span class="koboSpan" id="kobo.1195.2">This regularizing </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.1196.1">method leverages the concept of skip connections from ResNets, which will be introduced later, where outputs from earlier layers are additionally forwarded to the later layers. </span><span class="koboSpan" id="kobo.1196.2">During training, the layers in between the skip connections are completely bypassed to simulate a shallower network randomly. </span><span class="koboSpan" id="kobo.1196.3">This regularizer has the effect of a faster training time along with an increased </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">generalization performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1198.1">Label smoothing</span></strong><span class="koboSpan" id="kobo.1199.1">: This is used</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.1200.1"> for classification problems, including binary class, multiclass, or multilabel class problems. </span><span class="koboSpan" id="kobo.1200.2">It introduces a relaxation for the actual one-hot-encoded labels to learn from. </span><span class="koboSpan" id="kobo.1200.3">It modifies the actual label of 1 in the one-hot encoder vector by </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1201.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1202.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1203.1">ε</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.1204.1">, where </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1205.1">ε</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.1206.1">is a reasonably small value. </span><span class="koboSpan" id="kobo.1206.2">Additionally, 0 is replaced with </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1207.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1208.1">ε</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1209.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1210.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1211.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1212.1">k</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1213.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1214.1">1</span></span><span class="koboSpan" id="kobo.1215.1">, where </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1216.1">k</span></span><span class="koboSpan" id="kobo.1217.1"> can be set using the number of classes. </span><span class="koboSpan" id="kobo.1217.2">One example input and output would be </span><strong class="source-inline"><span class="koboSpan" id="kobo.1218.1">[0, 0, 0, 1]</span></strong><span class="koboSpan" id="kobo.1219.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1220.1">[0.0001, 0.0001, 0.0001, 0.9999]</span></strong><span class="koboSpan" id="kobo.1221.1">, respectively. </span><span class="koboSpan" id="kobo.1221.2">The idea is that we shouldn’t train the model to be overconfident in its result, which will signal that it is overfitted to the training data and won’t be able to generalize to external data. </span><span class="koboSpan" id="kobo.1221.3">This method encourages representations of the last layer outputs to be closer to each other for samples in the same class and encourages the same outputs to be equally distant among samples from different classes. </span><span class="koboSpan" id="kobo.1221.4">Additionally, this helps to</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.1222.1"> mitigate overconfidence in samples that have </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">inaccurate labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1224.1">Data augmentation</span></strong><span class="koboSpan" id="kobo.1225.1">: When the raw data does not adequately represent all the variations of the data of any label, data</span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.1226.1"> augmentation helps to computationally add variations in the data to be used for training. </span><span class="koboSpan" id="kobo.1226.2">This effectively increases generalization simply due to the model being able to learn from more complete variations of the data. </span><span class="koboSpan" id="kobo.1226.3">This can be applied to any data modality and will be introduced in more detail in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1227.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.1228.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1229.1">Exploring </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1230.1">Supervised Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1231.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1232.1">Regularization is an important component in any neural network architecture and will be seen in all of the architectures that will be introduced in the chapter. </span><span class="koboSpan" id="kobo.1232.2">When choosing regularization techniques for a specific problem, you should first consider the nature of your dataset and the problem you are trying to solve. </span><span class="koboSpan" id="kobo.1232.3">For instance, if you have a small batch size, group normalization or weight standardization might be more suitable than batch normalization. </span><span class="koboSpan" id="kobo.1232.4">If your</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.1233.1"> dataset has limited variations, data augmentation can be used to improve generalization. </span><span class="koboSpan" id="kobo.1233.2">To choose between these techniques, start with a simple regularization method such as dropout or L1/L2 regularization, and evaluate its performance. </span><span class="koboSpan" id="kobo.1233.3">Then, you can experiment with other techniques, either individually or in combination, and compare their impact on the model’s performance. </span><span class="koboSpan" id="kobo.1233.4">It’s essential to monitor the training and validation metrics to ensure that the chosen regularization methods are not causing overfitting or underfitting. </span><span class="koboSpan" id="kobo.1233.5">Ultimately, the choice of regularization technique depends on a combination of experimentation and validation, domain knowledge, and understanding of the specific problem and dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">at hand.</span></span></p>
<p><span class="koboSpan" id="kobo.1235.1">Next, let’s dive into the design of </span><span class="No-Break"><span class="koboSpan" id="kobo.1236.1">an MLP.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.1237.1">Designing an MLP</span></h2>
<p><span class="koboSpan" id="kobo.1238.1">Tabular data is not</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.1239.1"> where neural networks shine most, and more often than not, boosted decision trees outperform MLPs in terms of metric performance. </span><span class="koboSpan" id="kobo.1239.2">However, sometimes, in some datasets, neural networks can outperform boosted trees. </span><span class="koboSpan" id="kobo.1239.3">Make sure to benchmark MLPs with other non-neural network models when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1">tabular data.</span></span></p>
<p><span class="koboSpan" id="kobo.1241.1">MLPs are the simplest form of neural networks and can be modified at a high level in two dimensions similar to all neural networks, which are the width of the network and the depth of the network. </span><span class="koboSpan" id="kobo.1241.2">A common strategy when building standard MLP architectures from scratch is to start small with a shallow depth and narrow width and gradually increase both dimensions once a small baseline is obtained. </span><span class="koboSpan" id="kobo.1241.3">Usually, for MLPs on tabular data, the performance benefits of increasing the depth of the neural network stagnate at around the fourth layer. </span><span class="koboSpan" id="kobo.1241.4">ReLU is a standard activation layer that is proven to allow stable gradients and optimal learning of any task. </span><span class="koboSpan" id="kobo.1241.5">However, if you have time to achieve practical value, consider replacing the activation layer with more advanced activation layers. </span><span class="koboSpan" id="kobo.1241.6">At this point, the space of activation layer research is just too nuanced and results are mostly not standardized, with mixed responses on different datasets, so there is no guarantee of better performance when you use any advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">activation layers.</span></span></p>
<p><span class="koboSpan" id="kobo.1243.1">One adaptation of MLPs is to use </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.1244.1">a type of neural network called </span><strong class="bold"><span class="koboSpan" id="kobo.1245.1">denoising autoencoders</span></strong><span class="koboSpan" id="kobo.1246.1"> to generate </span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.1247.1">denoised features that can be used as input to MLPs. </span><span class="koboSpan" id="kobo.1247.2">This advancement will be described in more detail later in </span><a href="B18187_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1248.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.1249.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1250.1">Understanding Autoencoders</span></em><span class="koboSpan" id="kobo.1251.1">. </span><span class="koboSpan" id="kobo.1251.2">Training methods go hand in hand with the architecture when trying to achieve good performance. </span><span class="koboSpan" id="kobo.1251.3">The methods are mostly generic and don’t depend on any architecture specifically, so they will be covered in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1252.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.1253.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1254.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.1255.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1256.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.1257.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1258.1">Exploring Unsupervised Deep </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1259.1">Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1260.1">, separately.</span></span></p>
<p><span class="koboSpan" id="kobo.1261.1">Next, let’s summarize what we’ve learned from </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">this chapter.</span></span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.1263.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1264.1">MLPs are the foundational piece of architecture in deep learning that transcends just processing tabular data and is more than an old architecture that got superseded. </span><span class="koboSpan" id="kobo.1264.2">MLPs are very commonly utilized as a sub-component in many advanced neural network architectures today to either provide more automatic feature engineering, reduce the dimensionality of large features, or shape the features into the desired shapes for target predictions. </span><span class="koboSpan" id="kobo.1264.3">Look out for MLPs or, more importantly, the fully connected layer, in the next few architectures that are going to be introduced in the next </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">few chapters!</span></span></p>
<p><span class="koboSpan" id="kobo.1266.1">The automatic gradient computation provided by deep learning frameworks simplifies the implementation of backpropagation and allows us to focus on designing new neural networks. </span><span class="koboSpan" id="kobo.1266.2">It is essential to ensure that the mathematical functions used in these networks are differentiable, although this is often taken care of when adopting successful research findings. </span><span class="koboSpan" id="kobo.1266.3">And that’s the beauty of open source research coupled with powerful deep </span><span class="No-Break"><span class="koboSpan" id="kobo.1267.1">learning frameworks!</span></span></p>
<p><span class="koboSpan" id="kobo.1268.1">Regularization is a crucial aspect of neural network design, and while we have discussed it in detail in this chapter, upcoming chapters will showcase its application in different architectures without delving into </span><span class="No-Break"><span class="koboSpan" id="kobo.1269.1">further explanations.</span></span></p>
<p><span class="koboSpan" id="kobo.1270.1">In the next chapter, we will dive into a different kind of neural network, called the convolutional neural network, which is particularly suited for image-related tasks and has a wide range </span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1">of applications.</span></span></p>
</div>
</body></html>