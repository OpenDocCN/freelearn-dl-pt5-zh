- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying Practical Natural Language Understanding Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to identify **natural language understanding**
    (**NLU**) problems that are a good fit for today’s technology. That means they
    will not be too difficult for the state-of-the-art NLU approaches but neither
    can they be addressed by simple, non-NLU approaches. Practical NLU problems also
    require sufficient training data. Without sufficient training data, the resulting
    NLU system will perform poorly. The benefits of an NLU system also must justify
    its development and maintenance costs. While many of these considerations are
    things that project managers should think about, they also apply to students who
    are looking for class projects or thesis topics.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting a project that involves NLU, the first question to ask is whether
    the goals of the project are a good fit for the current state of the art in NLU.
    Is NLU the right technology for solving the problem that you wish to address?
    How does the difficulty of the problem compare to the NLU state of the art?
  prefs: []
  type: TYPE_NORMAL
- en: Starting out, it’s also important to decide what *solving the problem* means.
    Problems can be solved to different degrees. If the application is a class project,
    demo, or proof of concept, the solution does not have to be as accurate as a deployed
    solution that’s designed for the robust processing of thousands of user inputs
    a day. Similarly, if the problem is a cutting-edge research question, any improvement
    over the current state of the art is valuable, even if the problem isn’t completely
    solved by the work done in the project. How complete the solution has to be is
    a question that everyone needs to decide as they think about the problem that
    they want to address.
  prefs: []
  type: TYPE_NORMAL
- en: The project manager, or whoever is responsible for making the technical decisions
    about what technologies to use, should decide what level of accuracy they would
    find acceptable when the project is completed, keeping in mind that 100% accuracy
    is unlikely to be achievable in any natural language technology application.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will get into the details of identifying problems where NLU is
    applicable. Follow the principles discussed in this chapter, and you will be rewarded
    with a quality, working system that solves a real problem for its users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying problems that are the appropriate level of difficulty for the technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at difficult NLU applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at applications that don’t need NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking development costs into account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking maintenance costs into account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flowchart for deciding on NLU applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying problems that are the appropriate level of difficulty for the technology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is focused on technical considerations. Questions such as whether
    a market exists for a proposed application, or how to decide whether customers
    will find it appealing, are important questions, but they are outside of the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some kinds of problems that are a good fit for the state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today’s NLU is very good at handling problems based on specific, concrete topics,
    such as these examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classifying customers’ product reviews into positive and negative reviews**:
    Online sellers typically offer buyers a chance to review products they have bought,
    which is helpful for other prospective buyers as well as for sellers. But large
    online retailers with thousands of products are then faced with the problem of
    what to do with the information from thousands of reviews. It’s impossible for
    human tabulators to read all the incoming reviews, so an automated product review
    classification system would be very helpful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answering basic banking questions about account balances or recent transactions**:
    Banks and other financial institutions have large contact centers that handle
    customer questions. Often, the most common reasons for calling are simple questions
    about account balances, which can be answered with a database lookup based on
    account numbers and account types. An automated system can handle these by asking
    callers for their account numbers and the kind of information they need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making simple stock trades**: Buying and selling stock can become very complex,
    but in many cases, users simply want to buy or sell a certain number of shares
    of a specific company. This kind of transaction only needs a few pieces of information,
    such as an account number, the company, the number of shares, and whether to buy
    or sell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Package tracking**: Package tracking needs only a tracking number to tell
    users the status of their shipments. While web-based package tracking is common,
    sometimes, people don’t have access to the web. With a natural language application,
    users can track packages with just a phone call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Routing customers’ questions to the right customer service agent**: Many
    customers have questions that can only be answered by a human customer service
    agent. For those customers, an NLU system can still be helpful by directing the
    callers to the call center agents in the right department. It can ask the customer
    the reason for their call, classify the request, and then automatically route
    their call to the expert or department that handles that topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Providing information about weather forecasts, sports scores, and historical
    facts**: These kinds of applications are characterized by requests that have a
    few well-defined parameters. For sports scores, this would be a team name and
    possibly the date of a game. For weather forecasts, the parameters include the
    location and timeframe for the forecast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these applications are characterized by having unambiguous, correct answers.
    In addition, the user’s language that the system is expected to understand is
    not too complex. These would all be suitable topics for an NLU project.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate what makes these applications suitable for today’s technology
    by going into more detail on providing information about weather forecasts, sports
    scores, and historical facts.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.1* shows a sample architecture for an application that can provide
    weather forecasts for different cities. Processing starts when the user asks,
    *What is the weather forecast for tomorrow in New York City?* Note that the user
    is making a single, short request, for specific information – the weather forecast,
    for a particular date, in a particular location. The NLU system needs to detect
    the intent (weather forecast), the entities’ *location*, and the *date*. These
    should all be easy to find – the entities are very dissimilar, and the *weather
    forecast* intent is not likely to be confused with any other intents. This makes
    it straightforward for the NLU system to convert the user’s question to a structured
    message that could be interpreted by a weather information web service, as shown
    at the top of the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A practical NLU application](img/B19005_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – A practical NLU application
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the fact that the information being requested is not very complex,
    there are many ways to ask about it, which means that it’s not very practical
    to just make a list of possible user queries. *Table 2.1* illustrates a few of
    the many ways to make this request:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Some paraphrases of “What is the weather forecast for tomorrow in New York
    City?” |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| What will the weather be like tomorrow in New York? |'
  prefs: []
  type: TYPE_TB
- en: '| What’s tomorrow’s weather for New York? |'
  prefs: []
  type: TYPE_TB
- en: '| I want the New York City weather forecast for tomorrow. |'
  prefs: []
  type: TYPE_TB
- en: '| The weather tomorrow in New York, please. |'
  prefs: []
  type: TYPE_TB
- en: '| New York weather forecast for tomorrow. |'
  prefs: []
  type: TYPE_TB
- en: '| Tomorrow’s weather forecast for New York City. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Paraphrases for a weather request
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of this application that makes it a good candidate for NLU is
    that the information the user is asking about (weather forecasts) is available
    from multiple easily-accessible, cloud-based web services, with **application
    programming interfaces** (**APIs**) that are usually well documented. This makes
    it easy for developers to send queries to the web services and get back the information
    that the user requested in a structured form. This information can then be presented
    to the user. Developers have choices about how they want to present the information
    – for example, text, graphics, or a combination of text and graphics.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.1*, we can see that the developer has chosen to present the information
    via natural language, and consequently, a **natural language generation** (**NLG**)
    component is used to generate the natural language output from a form. Other presentation
    options would be to show graphics, such as a picture of the sun partially covered
    by a cloud, or to simply show a form with the information received from the weather
    information web service. However, only the NLG option is a good fit for a spoken
    or voice-only interface such as a smart speaker since, with a voice-only interface,
    there is no way to display graphics.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest benefit of NLU for an application such as weather forecasting is
    that NLU can handle the many possible ways that the user might ask this question
    with the same intent, as shown in *Table 2.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 2.1* shows some paraphrases of a weather forecast request. These are
    just a few examples of possible ways to ask for a weather forecast. It is often
    surprising how many different ways there are to make even a simple request. If
    we could make a list of all the options, even if it was a very long list, NLU
    wouldn’t be necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: We could theoretically just list all the possibilities and map them to the structured
    queries. However, it’s actually very difficult to anticipate all the possible
    ways that someone would ask even a simple question about the weather. If a user
    happens to phrase their query in a way that the developer hasn’t included in their
    list, the system will fail to respond. This can be very confusing to users because
    users won’t understand why this query failed when similar queries worked. An NLU
    system will be able to cope with many more query variations.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen in this section, applications that have clear and easily identifiable
    intents and entities and that have definite answers that can be obtained from
    web resources, have a good chance of success with today’s NLU technology.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s turn to applications that are unlikely to be successful because they
    require capabilities that are beyond the state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at difficult applications of NLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How can we tell whether the problem is too hard for the state of the art? First
    of all, we can ask what it means for a problem to be *too hard*. Here are some
    consequences of trying to use NLU for an application that is beyond the state
    of the art:'
  prefs: []
  type: TYPE_NORMAL
- en: The system will be unable to reliably understand user queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers will contain errors because the system has misunderstood user queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system will have to say *I don’t know* or *I can’t do that* so frequently
    that users become frustrated and decide not to use the application anymore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to keep in mind that the state of the art is rapidly improving.
    Remarkable progress has been made recently as cloud-based **large language models**
    (**LLMs**) such as ChatGPT have become available. Some applications that might
    be very hard now will not always be too hard.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few of the characteristics of today’s difficult NLU problems.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require the system to use judgment or common sense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the weather example in the previous section, applications that require
    judgment are applications where there isn’t a single correct answer, or even a
    few reasonable alternatives. These could include applications where the user is
    asking for advice that depends on many, often complex, considerations. Here are
    some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Should I learn Python?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should I get a COVID vaccine?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should I buy an electric car?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this a good time to buy a house?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To answer the first question, the system needs specific knowledge about the
    user (whether the user already has a programming background or what they want
    to do with their new programming skills). LLM-based systems, such as ChatGPT,
    will respond to these kinds of questions in a general way – for example, by providing
    generic considerations about buying a house – but they can’t give advice that’s
    specific to the user, because they don’t know anything about the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications in which the system is asked for a subjective opinion are also
    very difficult to handle well, such as these examples:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the best movie of all time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who was the most talented 20th-century actor?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a good way to cook chicken that doesn’t take more than half an hour?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fully answer these kinds of queries requires the system to have a lot of
    general knowledge, such as actors who had careers in the 20th century. A system
    could respond to subjective questions by giving a random answer – just pick a
    movie at random and say that that movie is the best of all time. However, a randomly
    picked movie is not necessarily going to even be good, let alone the best movie
    of all time.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, if there’s a follow-up question, the system won’t be able to explain
    or defend its opinion. So, if you asked a system *Should I buy an electric car*,
    and it said *Yes*, it wouldn’t be able to explain why it said yes. In fact, it’s
    probably too difficult for many of today’s systems to even realize that they’re
    being asked a subjective question. As in the case of questions that require the
    knowledge of the user to give a good answer, LLM-based systems will give generic
    answers to subjective questions, but they will admit that they aren’t able to
    deal with subjectivity.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require dealing with hypotheticals, possibilities, and counterfactuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another difficult area is dealing with information that isn’t true or is possibly
    not true. When the user asks about something that might happen, if the circumstances
    are right, the user is asking about a hypothetical or a possibility. Today’s state-of-the-art
    systems are good at providing specific, concrete information, but the technology
    is not good at reasoning about possibilities. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: If I have a budget of $15,000, how big of a patio should I be able to build,
    assuming I’m willing to do some of the work myself?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I have six people, how many pizzas should I get?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there’s no rain in the forecast tomorrow, remind me to water my plants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, systems aren’t very good at reasoning about things that aren’t true.
    For example, consider the sentence, *I’d like to find a nearby Asian restaurant,
    but not Japanese*. To answer this question correctly, the system has to find Asian
    restaurants, and it has to understand that it should exclude Japanese restaurants,
    which are nevertheless Asian, from the list.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require combining information from a language with information
    from various sensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some very interesting applications could involve integrating information from
    language and cameras or microphones. These are called **multimodal** applications
    because they integrate multiple modalities such as speech, images, and non-speech
    audio such as music:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this cake done? (holding camera up to cake)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is this noise that my car is making? (holding microphone up to engine)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These applications are currently beyond the state of the art of today’s commercial
    natural language technology, although they could be appropriate for an exploratory
    research project. They are also currently outside of the capabilities of LLMs,
    which can only understand text input.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that integrate broad general or expert knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When users interact with an NLU system, they have goals that they want to accomplish.
    In many cases, the system has some kind of knowledge or expertise that the user
    doesn’t have, and the user wants to take advantage of that expertise. But where
    does that expertise come from? Providing systems with large amounts of knowledge
    is difficult. There are existing web APIs for simple information such as sports
    scores and weather. Systems such as Wolfram Alpha can also answer more complicated
    questions, such as scientific facts.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, answering questions that require the use of expert knowledge,
    such as medical information, is more difficult, as there’s no easily accessible
    source of this kind of knowledge. In addition, existing sources of information
    might be inconsistent. One obvious source of large amounts of knowledge is the
    **World Wide Web** (**WWW**), which is the major source of knowledge of LLM. However,
    knowledge available on the WWW can be wrong, inconsistent, or not applicable to
    a particular situation, so it has to be used with caution.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are a few examples of difficult topics for today’s natural language technology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer complex technical questions**: A statement like *I can’t connect to
    the internet* requires the system to have a lot of information about internet
    connectivity as well as how to debug connectivity problems. It would also have
    to have access to other time-sensitive information such as whether there are global
    internet outages in the user’s area.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer questions that require an understanding of human relationships**:
    *My friend won’t talk to me since I started dating her boyfriend; what should
    I do?* A system would have to understand a lot about dating, and probably dating
    in a specific culture as well, in order to give a good answer to a question like
    this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read a book and tell me whether I would like that book**: Today’s systems
    would have a hard time even reading and understanding an entire book since long
    texts like books contain very complex information. In addition to just reading
    a book, for a system to tell me whether I would like it requires a lot of information
    about me and my reading interests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Read an article from a medical journal and tell me whether the findings apply
    to me**: Answering questions like this would require a tremendous amount of information
    about the user’s health and medical history, as well as the ability to understand
    medical language and interpret the results of medical studies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understand jokes**: Understanding jokes often requires considerable cultural
    knowledge. Think about what knowledge a system would need to be able to understand
    the traditional joke, *Why did the chicken cross the road? To get to the other
    side*. This is funny because the question leads the user to believe that the chicken
    has an interesting reason for crossing the road, but its reason turns out to be
    extremely obvious. Not only would it be very hard for a system to be able to understand
    why this particular joke is funny, but this is only one joke—just being able to
    understand this joke wouldn’t help a system understand any other jokes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpret figures of speech**: *I could eat a horse* doesn’t mean that you
    want to eat a horse, it just means that you’re very hungry. A system would have
    to realize that this is a figure of speech because horses are very large and no
    one could actually eat a horse in one sitting, no matter how hungry they are.
    On the other hand, *I could eat a pizza* is not a figure of speech and probably
    just means that the user would like to order a pizza.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understand irony and sarcasm**: If a book review contains a sentence like
    *The author is a real genius*, the review writer might mean that the author is
    literally a genius, but not necessarily. This could be intended sarcastically
    to mean that the author is not a genius at all. If this sentence is followed by
    *My three-year-old could have written a better book*, we can tell that the first
    sentence was intended to be taken as sarcasm. NLU systems can’t understand sarcasm.
    They also don’t know that three-year-olds are unlikely to be able to write good
    books, and so the writer of the review is claiming that the book is worse than
    one authored by a three-year-old, and so it is a bad book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Be able to make use of complex knowledge**: As an example of complex knowledge,
    consider the utterance, *My cake is as flat as a pancake; what went wrong?* To
    answer this question, the system has to understand that a cake shouldn’t be flat
    but that pancakes are normally flat. It also has to understand that we’re talking
    about a cake that has been baked, as unbaked cakes are typically flat. Once the
    system has figured all this out, it also has to understand the process of baking
    enough to give advice about why the cake is flat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One general property shared by many of these difficult types of applications
    is that there often isn’t any one data source where the answers can be obtained.
    That is, there aren’t any backend data sources that developers can just query
    to answer a question like *Is this a good time to buy an electric car?* This is
    in contrast to the earlier weather forecast example, where developers can go to
    a single backend data source.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than trying to find a single backend data source, one strategy might
    be to do a web search for the question. But as anyone who’s done a web search
    knows, there will be millions of search results (nearly 2 billion for *Is this
    a good time to buy an electric car?*), and what’s worse, the answers are not likely
    to be consistent with each other. Some pages will assert that it is a good time
    to buy an electric car, and others will assert that it is not. So, the strategy
    of using a web search to answer questions without a good data source will probably
    not work. However, being able to integrate information from across the web is
    a strength of LLMs, so if the information is available on the web, an LLM such
    as ChatGPT will be able to find it.
  prefs: []
  type: TYPE_NORMAL
- en: Applications where users often don’t have a clear idea of what they want
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Users don’t always state their intentions very clearly. As an example, consider
    a tourist who’s visiting an unfamiliar town. Perhaps the town provides a service
    that tourists can call to find out about public transportation options. If a tourist
    asks a question like *What train should I take to get from the Marriott Hotel
    to 123 Market Street?*, a literal answer might be *You can’t take the train from
    the Marriott Hotel to 123 Market Street*. Or the user might be offered a circuitous
    route that takes six hours.
  prefs: []
  type: TYPE_NORMAL
- en: A human agent could figure out that the caller’s actual goal is probably to
    get from the Marriott Hotel to 123 Market Street, and the reference to the train
    was just the caller’s guess that the train would be a good way to do that. In
    that case, a human agent could say something like *There isn’t really a good train
    route between those two locations; would you like some other ideas about how to
    get between them?* This would be natural for a human agent but very difficult
    for an automated system, because the system would need to be able to reason about
    what the user’s real goal is.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that require understanding multiple languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016), language technology
    is better for some languages than others. If a system has to be able to communicate
    with users (by speech or text) in different languages, then language models for
    each language have to be developed. Processing for some languages will be more
    accurate than processing for other languages, and for some languages, processing
    might not be good enough at all. At the current state of the art, NLP technology
    for major European, Middle Eastern, and Asian languages should be able to handle
    most applications.
  prefs: []
  type: TYPE_NORMAL
- en: In some applications, the system has to be prepared to speak different languages
    depending on what the user says. To do this, the system has to be able to tell
    the different languages apart just by their sounds or words. This technology is
    called **language identification**. Identifying commonly spoken languages is not
    difficult but, again, this is not the case for less common languages.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of languages with very little training data, such as languages with
    fewer than one million speakers, the language may not have been studied well enough
    for natural language applications to be developed for that language.
  prefs: []
  type: TYPE_NORMAL
- en: Even more difficult than understanding multiple languages is handling cases
    where two or more languages are mixed in the same sentence. This often happens
    when several different languages are spoken in the same area, and people can assume
    that anyone they talk with can understand all the local languages. Mixing languages
    in the same sentence is called **code-switching**. Processing sentences with code-switching
    is even more difficult than processing several languages in the same application
    because the system has to be prepared for any word in any of the languages it
    knows at any point in the sentence. This is a difficult problem for today’s technology.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding discussion, we’ve reviewed many factors that make applications
    too difficult for today’s state of the art in NLP. Let’s now look at applications
    that are too easy.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at applications that don’t need NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Turning from applications that are too difficult, we can also look at applications
    that are too easy – that is, applications where simpler solutions than NLP will
    work, and where NLP is overkill. These are applications where the complexity of
    the problem doesn’t justify the complexity of building and managing a natural
    language system.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language is characterized by unpredictable inputs and an indirect mapping
    of words to meanings. Different words can have the same meaning, and different
    meanings can be expressed by the same words, depending on the context. If there
    is a simple one-to-one mapping between inputs and meanings, NLP isn’t necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Text that can be analyzed with regular expressions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first case where NLU isn’t necessary is when the possible inputs consist
    of a limited set of options, such as cities, states, or countries. Internally,
    such inputs can be represented as lists, and can be analyzed via table lookup.
    Even if there are synonyms for certain inputs (*UK* for the *United Kingdom*,
    for example), the synonyms can be included in the lists as well.
  prefs: []
  type: TYPE_NORMAL
- en: A slightly more complicated, but still simple, input is when every input to
    the system is composed according to easily stated, unvarying rules. NLP is not
    necessary in those cases because the input is predictable. Good examples of these
    kinds of simple expressions are telephone numbers, which have fixed, predictable
    formats, or dates, which are more varied, but still limited. In addition to these
    generic expressions, in specific applications, there is often a requirement to
    analyze expressions such as product IDs or serial numbers. These types of inputs
    can be analyzed with regular expressions. Regular expressions are rules that describe
    patterns of characters (alphabetical, numerical, or special characters). For example,
    the `^\d{5}(-\d{4})?$` regular expression matches US zip codes, either containing
    five digits (`12345`) or containing five digits followed by a hyphen, and then
    four more digits (`12345-1234`).
  prefs: []
  type: TYPE_NORMAL
- en: If all of the inputs in an application are these kinds of fixed phrases, regular
    expressions can do the job without requiring full-scale NLP. If the entire problem
    can be solved with regular expressions, then NLP isn’t needed. If only part of
    the problem can be solved with regular expressions, but part of it needs NLP,
    regular expressions can be combined with natural language techniques. For example,
    if the text includes formatted numbers such as phone numbers, zip codes, or dates,
    regular expressions can be used to just analyze those numbers. Python has excellent
    libraries for handling regular expressions if regular expressions are needed in
    an application. We will discuss combining NLP and regular expressions in [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159) and [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173).
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing inputs from a known list of words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the only available inputs are from a fixed set of possibilities, then NLP
    isn’t needed. For example, if the input can only be a US state, then the application
    can just look for the names of states. Things can get a little more complicated
    if the inputs include words from a fixed set of possibilities, but there are surrounding
    words. This is called **keyword spotting**. This can happen if the desired response
    is from a fixed set of words, such as the name of one of 50 states, and the users
    sometimes add something – for example, the user says *I live in Arizona* in response
    to a system question like *Where do* *you live?*
  prefs: []
  type: TYPE_NORMAL
- en: NLP is probably not needed for this – the system just has to be able to ignore
    the irrelevant words (*I live in*, in this example). Regular expressions can be
    written to ignore irrelevant words by using `*` to match any number of characters,
    including zero. Python uses `+` to match at least one character. So, a regular
    expression for spotting the keyword `Arizona` in Python would just be `*Arizona*`.
  prefs: []
  type: TYPE_NORMAL
- en: Using graphical interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most applications rely on a **graphical user interface**, where the user interacts
    with the application by selecting choices from menus and clicking buttons. These
    conventional interfaces are easier to build than NLU-based interfaces and are
    perfectly suitable for many applications. When is an NLU-based interface a better
    choice?
  prefs: []
  type: TYPE_NORMAL
- en: NLU is a better choice as the information that the user has to supply becomes
    more detailed. When this happens, a graphical interface has to rely on deeper
    and deeper levels of menus, requiring users to navigate through menu after menu
    until they find the information they need or until the application has collected
    enough information to answer their questions. This is especially a problem with
    mobile interfaces, where the amount of information that can fit on the screen
    is much less than the amount of information that fits on a laptop or desktop computer,
    which means that the menus need to have deeper levels. On the other hand, an NLU
    input allows the user to state their goal once, without having to navigate through
    multiple menus.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem that graphical interfaces with deep menus have is that the terminology
    used in the menus does not always match the users’ mental models of their goals.
    These mismatches can lead users down the wrong path. They might not realize their
    mistake until several levels farther down in the menu tree. When that happens,
    the user has to start all over again.
  prefs: []
  type: TYPE_NORMAL
- en: The contrast between graphical and NLP applications can easily be seen on websites
    and applications that include both a conventional graphical interface and an NLP
    chatbot. In those interfaces, the user can choose between menu-based navigation
    and interacting with the chatbot. A good example is the Microsoft Word 2016 interface.
    Word is a very complex application with a rich set of capabilities. Making an
    intuitive graphical interface for an application that is this complex is difficult,
    and it can be hard for users to find the information they’re looking for.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, Microsoft provides both graphical and NLP interfaces to Word
    functionality. At the top of the page of a Word document, there are choices including
    `How do I add an equation` will provide a list of several different ways to add
    an equation to a Word document. This is much quicker and more direct than looking
    through nested menus.
  prefs: []
  type: TYPE_NORMAL
- en: Developers should consider adding NLU functionality to graphical applications
    when menu levels get more than three or so levels deep, especially if each menu
    level has many choices.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve looked at many factors that make an application more or less suited
    for NLP technology. The next considerations are related to the development process
    – the availability of data and the development process itself, which we discuss
    in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that sufficient data is available
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having determined whether the problem is suitable for NLU, we can turn to the
    next question – what kinds of data are available for addressing this problem?
    Is there existing data? If not, what would be involved in obtaining the kind of
    data that’s needed to solve the problem?
  prefs: []
  type: TYPE_NORMAL
- en: We will look at two kinds of data. First, we will consider *training data*,
    or examples of the kinds of language that will be used by users of NLU systems,
    and we will look at sources of training data. The second kind of data that we
    will discuss is *application data*. The information in this section will enable
    you to determine whether you have enough training data and how much work it will
    take to format it properly to be used in the NLU system development process.
  prefs: []
  type: TYPE_NORMAL
- en: Application data is the information that the system will use to answer users’
    questions. As we will see, it can come from publicly available sources or from
    internal databases. For application data, we will see that it is important to
    ensure that the data is available, reliable, and can be obtained without excessive
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language applications are nearly all trained based on examples of the
    kinds of inputs they’re expected to process. That means that sufficient training
    data needs to be available in order for any natural language application to be
    successful. Not having enough training data means that when the application is
    deployed, there will be inputs that can’t be processed because the system hasn’t
    been exposed to any similar inputs during the development phase. This doesn’t
    mean that the system needs to see every possible input during training. This is
    nearly impossible, especially if the intended inputs are long or complex documents
    such as product reviews.
  prefs: []
  type: TYPE_NORMAL
- en: It is extremely unlikely that the same review will occur more than once. Rather,
    the training process is designed so that documents that are semantically similar
    will be analyzed in the same way, even if the exact words and phrasings are different.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms such as those we’ll be learning about in [*Chapter
    9*](B19005_09.xhtml#_idTextAnchor173) and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),
    require fairly large amounts of data. The more different categories or intents
    that have to be distinguished, the more data is required. Most practical applications
    will need thousands of training examples.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to examples, normally the training data also has to include the
    *right answer* or how the trained system is expected to analyze the data. The
    technical term for the *right answer* is **annotation**. Annotations can also
    be referred to as the **ground truth** or **gold standard**. For example, if the
    application is designed to determine whether a product review is positive or negative,
    annotations (provided by human judges) assign a positive or negative label to
    a set of reviews that will be used as training and test data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 2.2* shows examples of positive and negative product reviews and their
    annotations. An accurate system for classifying product reviews would probably
    need to be based on several thousand product reviews. In some cases, as in the
    examples in *Table 2.2*, the task of annotation doesn’t require any special expertise;
    almost anyone with a reasonable command of English can decide whether a product
    review is positive or negative. This means that simple annotation tasks can be
    inexpensively crowdsourced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, some annotations have to be done by subject matter experts.
    For example, annotating data from an interactive troubleshooting dialog for a
    complex software product would probably need to be done by someone with expertise
    in that product. This would make the annotation process much more expensive and
    might not even be possible if the necessary experts aren’t available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Text | Annotation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I was very disappointed with this product. It was flimsy, overpriced, and
    the paint flaked off. | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| This product met my every expectation. It is well made, looks great, and
    the price is right. I have no reservations about recommending it to anyone. |
    Positive |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Examples of positive and negative annotations of product reviews
  prefs: []
  type: TYPE_NORMAL
- en: Although data annotation can be difficult and expensive, not all NLU algorithms
    require annotated data. In particular, unsupervised learning, which we will cover
    in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), is based on unannotated data.
    In [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), we will also discuss the
    limits of unannotated data.
  prefs: []
  type: TYPE_NORMAL
- en: The full set of training examples for an application is called a **corpus**,
    or **dataset**. It is essential to have sufficient training data in order for
    the application to be accurate. The training data does not have to be available
    all at once – development can begin before the data collection is complete, and
    additional data can be added as development progresses. This can lead to problems
    with consistency if annotators forget the criteria that they used to annotate
    earlier data.
  prefs: []
  type: TYPE_NORMAL
- en: Where does data come from? Python NLP libraries contain several toy datasets
    that can be used to test system setup or algorithms, or that can be used in student
    projects where there’s no plan to put a system into production. In addition, larger
    datasets can also be obtained from organizations such as Hugging Face (https://huggingface.co/)
    or the Linguistic Data Consortium (https://www.ldc.upenn.edu/).
  prefs: []
  type: TYPE_NORMAL
- en: For enterprise applications, preexisting data from an earlier application that
    was performed by human agents can be very helpful. Examples of this could include
    transcripts of customer service calls with agents.
  prefs: []
  type: TYPE_NORMAL
- en: Another good source of data is the text fields of databases. For example, this
    is probably where you would expect to find product reviews for an organization’s
    products. In many cases, text fields of databases are accompanied by another field
    with a manual classification that identifies, for example, whether the review
    is positive or negative. This manual classification is, in effect, an annotation
    that can be used in the training process to create a system that can automatically
    classify product reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, new data can also be collected specifically to support an application.
    This can be time-consuming and expensive, but sometimes it’s the only way to get
    the appropriate data. Data collection can be a complex topic in itself, especially
    when the data is collected to support interactive dialogs with human users.
  prefs: []
  type: TYPE_NORMAL
- en: Data, including data collection, is discussed in more detail in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: Application data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the data required to train the natural language application,
    it’s important to take into account any costs associated with accessing the information
    that the system will be providing.
  prefs: []
  type: TYPE_NORMAL
- en: Many third-party web services provide APIs that can be accessed by developers
    to obtain free or paid information. There are some websites that provide general
    information about available public APIs, such as **APIsList** (https://apislist.com/).
    This site lists APIs that can deliver data on topics ranging over hundreds of
    categories including weather, social networks, mapping, government, travel, and
    many more. Many APIs require payments, either as a subscription or per transaction,
    so it’s important to consider these potential costs when selecting an application.
  prefs: []
  type: TYPE_NORMAL
- en: Taking development costs into account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After making sure that data is available, and that the data is (or can be) annotated
    with the required intents, entities, and classification categories, the next consideration
    for deciding whether NLP is a good fit for an application is the cost of developing
    the application itself. Some technically feasible applications can nevertheless
    be impractical because they would be too costly, risky, or time-consuming to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Development costs include determining the most effective machine learning approaches
    to a specific problem. This can take significant time and involve some trial and
    error as models need to be trained and retrained in the process of exploring different
    algorithms. Identifying the most promising algorithms is also likely to require
    NLP data scientists, who may be in short supply. Developers have to ask the question
    of whether the cost of development is consistent with the benefits that will be
    realized by the final application.
  prefs: []
  type: TYPE_NORMAL
- en: For low-volume applications, it should also be kept in mind that the cost of
    developing and deploying an NLP solution can exceed the cost of employing humans
    to perform the same tasks. This is particularly true if some humans will still
    be needed for more complex tasks, even if an NLP solution is implemented and is
    doing part of the work
  prefs: []
  type: TYPE_NORMAL
- en: Taking maintenance costs into account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final consideration for natural language applications, especially deployed
    applications, is the cost of maintenance. This is easy to overlook because NLU
    applications have several maintenance considerations that don’t apply to most
    traditional applications. Specifically, the type of language used in some applications
    changes over time. This is expected since it reflects changes in the things that
    the users are talking about. In customer service applications, for example, product
    names, store locations, and services change, sometimes very quickly. The new vocabulary
    that customers use to ask about this information changes as well. This means that
    new words have to be added to the system, and machine learning models have to
    be retrained.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, applications that provide rapidly changing information need to be
    kept up to date on an ongoing basis. As an example, the word *COVID-19* was introduced
    in early 2020 – no one had ever heard it before, but now it is universally familiar.
    Since medical information about COVID-19 changes rapidly, a chatbot designed to
    provide COVID-19 information will have to be very carefully maintained in order
    to ensure that it’s up to date and is not providing incorrect or even harmful
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to keep applications up to date with the users’ topics, three tasks
    that are specific to natural language applications need to be planned for:'
  prefs: []
  type: TYPE_NORMAL
- en: Developers need to be assigned to keep the application up to date as new information
    (such as new products or new product categories) is added to the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent review of platform-provided logging of user inputs should be done.
    User inputs that are not handled correctly must be analyzed to determine the correct
    way of handling them. Are the users asking about new topics (intents)? Then new
    intents have to be added. Are they talking about existing topics in different
    ways? If that’s the case, new training examples need to be added to the existing
    intents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When issues are discovered and user inputs are not being handled correctly,
    the system needs to be modified. The simplest type of modification is adding new
    vocabulary, but in some cases, more structural changes are necessary. For example,
    it may be that an existing intent has to be split into multiple intents, which
    means that all the training data for the original intent has to be reviewed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of developers required to keep the application updated depends on
    several considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The number of user inputs**: If the system gets hundreds or thousands of
    failed inputs per day, developers need to be assigned to review these and add
    information to the system so that it can handle these inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The complexity of the application**: If the application includes hundreds
    of intents and entities, it will take more developers to keep it up to date and
    ensure that any new information stays consistent with old information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The volatility of the information provided by the application**: If the application
    is one where new words, new products, and new services are continually being added,
    the system will require more frequent changes to stay up to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These costs are in addition to any costs for hardware or cloud services that
    are not specific to natural language applications.
  prefs: []
  type: TYPE_NORMAL
- en: A flowchart for deciding on NLU applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has covered many considerations that should be taken into account
    in deciding on an NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.2* summarizes these considerations as a flowchart of the process
    for evaluating a potential NLU application.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Steps in evaluating an NLU project](img/B19005_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Steps in evaluating an NLU project
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the top, the process starts by asking whether the problem is too
    hard or too easy for the current state of the art, using the criteria discussed
    earlier. If it’s either too hard or too easy, we should look for another application,
    or look at cutting back or expanding the scope of the application to make it a
    better fit for NLP technology. For example, the application might be redesigned
    to handle fewer languages.
  prefs: []
  type: TYPE_NORMAL
- en: If the problem seems to be a good fit for the state of the art, the next steps
    are to ensure that the appropriate data is available, and if not, whether data
    can be collected. Once data is available, the next thing to look at is to see
    whether the costs of development and maintenance are reasonable. If everything
    looks good, work on the application can proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the topic of selecting NLP applications that have
    a good chance of success with current NLP technology. Successful applications
    generally have input with specific, objective answers, have training data available,
    and handle (at most) a few languages.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, this chapter addressed a number of important questions. We learned
    how to identify problems that are the appropriate level of difficulty for the
    current state of the art of NLU technology. We also learned how to ensure that
    sufficient data is available for system development and how to estimate the costs
    of development and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to evaluate the feasibility of different types of NLP applications
    as discussed in this chapter will be extremely valuable as you move forward with
    your NLP projects. Selecting an application that is too ambitious will result
    in frustration and a failed project, whereas selecting an application that is
    too easy for the state of the art will lead to wasted time and an unnecessarily
    complex system.
  prefs: []
  type: TYPE_NORMAL
- en: We have achieved our goal of learning how to evaluate the feasibility of NLP
    projects in terms of important criteria such as technical feasibility as well
    as the practical considerations of data availability and maintenance costs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the major approaches to NLP and the advantages
    and disadvantages of each approach. These approaches include rule-based systems,
    in which human experts write rules that describe how the system should analyze
    inputs, and machine learning, where the system is trained to analyze inputs by
    processing many examples of inputs and how they should be analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2:Developing and Testing Natural Language Understanding Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After completing this section, you will be able to decide what techniques are
    applicable to address a problem with natural language understanding technologies
    and implement a system using Python and Python libraries such as NLTK, spaCy,
    and Keras, and evaluate it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B19005_03.xhtml#_idTextAnchor059), *Approaches to Natural Language
    Understanding – Rule-Based Systems, Machine Learning, and Deep Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19005_04.xhtml#_idTextAnchor085), *Selecting Libraries and Tools
    for Natural Language Understanding*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), *Natural Language Data – Finding
    and Preparing Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19005_06.xhtml#_idTextAnchor134), *Exploring and Visualizing
    Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19005_07.xhtml#_idTextAnchor144), *Selecting Approaches and
    Representing Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19005_08.xhtml#_idTextAnchor159), *Rule-Based Techniques*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), *Machine Learning Part 1 –
    Statistical Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), *Machine Learning Part 2
    – Neural Networks and Deep Learning Techniques*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19005_11.xhtml#_idTextAnchor193), *Machine Learning Part 3
    – Transformers and Large Language Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), *Applying Unsupervised Learning
    Approaches*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), *How Well Does It Work? –
    Evaluation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
