<html><head></head><body>
		<div id="_idContainer205" class="Content">
			<h1 id="_idParaDest-186"><a id="_idTextAnchor187"/>Appendix</h1>
		</div>
		<div id="_idContainer252" class="Content">
			<h1 id="_idParaDest-187"><a id="_idTextAnchor188"/>1. Introduction to Machine Learning with Keras</h1>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Activity 1.01: Adding Regularization to the Model</h2>
			<p>In this activity, we will utilize the same logistic regression model from the scikit-learn package. This time, however, we will add regularization to the model and search for the optimum regularization parameter - a process often called <strong class="source-inline">hyperparameter tuning</strong>. After training the models, we will test the predictions and compare the model evaluation metrics to the ones that were produced by the baseline model and the model without regularization.</p>
			<ol>
				<li>Load the feature data from <em class="italic">Exercise 1.03</em>, <em class="italic">Appropriate Representation of the Data</em>, and the target data from <em class="italic">Exercise 1.02</em>, <em class="italic">Cleaning the Data</em>:<p class="source-code">import pandas as pd</p><p class="source-code">feats = pd.read_csv('../data/OSI_feats_e3.csv')</p><p class="source-code">target = pd.read_csv('../data/OSI_target_e2.csv')</p></li>
				<li>Create a <strong class="source-inline">test</strong> and <strong class="source-inline">train</strong> dataset. Train the data using the training dataset. This time, however, use part of the <strong class="source-inline">training</strong> dataset for validation in order to choose the most appropriate hyperparameter.<p>Once again, we will use <strong class="source-inline">test_size = 0.2</strong>, which means that <strong class="source-inline">20%</strong> of the data will be reserved for testing. The size of our validation set will be determined by how many validation folds we have. If we do <strong class="source-inline">10-fold cross-validation</strong>, this equates to reserving <strong class="source-inline">10%</strong> of the <strong class="source-inline">training</strong> dataset to validate our model on. Each fold will use a different <strong class="source-inline">10%</strong> of the <strong class="source-inline">training</strong> dataset, and the average error across all folds is used to compare models with different hyperparameters. Assign a random value to the <strong class="source-inline">random_state</strong> variable:</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">test_size = 0.2</p><p class="source-code">random_state = 13</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(feats, target, test_size=test_size, \</p><p class="source-code">                 random_state=random_state)</p></li>
				<li>Check the dimensions of the DataFrames:<p class="source-code">print(f'Shape of X_train: {X_train.shape}')</p><p class="source-code">print(f'Shape of y_train: {y_train.shape}')</p><p class="source-code">print(f'Shape of X_test: {X_test.shape}')</p><p class="source-code">print(f'Shape of y_test: {y_test.shape}')</p><p>The preceding code produces the following output:</p><p class="source-code">Shape of X_train: (9864, 68)</p><p class="source-code">Shape of y_train: (9864, 1)</p><p class="source-code">Shape of X_test: (2466, 68)</p><p class="source-code">Shape of y_test: (2466, 1)</p></li>
				<li>Next, instantiate the models. Try two types of regularization parameters, <strong class="source-inline">l1</strong> and <strong class="source-inline">l2</strong>, with 10-fold cross-validation. Iterate our regularization parameter from 1x10<span class="superscript">-2</span> to 1x10<span class="superscript">6</span> equally in the logarithmic space to observe how the parameters affect the results:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.linear_model import LogisticRegressionCV</p><p class="source-code">Cs = np.logspace(-2, 6, 9)</p><p class="source-code">model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', \</p><p class="source-code">                                cv=10, solver='liblinear', \</p><p class="source-code">                                random_state=42, max_iter=10000)</p><p class="source-code">model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, \</p><p class="source-code">                                random_state=42, max_iter=10000)</p><p class="callout-heading">Note</p><p class="callout">For a logistic regression model with the <strong class="source-inline">l1</strong> regularization parameter, only the <strong class="source-inline">liblinear</strong> solver can be used.</p></li>
				<li>Next, fit the models to the training data:<p class="source-code">model_l1.fit(X_train, y_train['Revenue'])</p><p class="source-code">model_l2.fit(X_train, y_train['Revenue'])</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer206" class="IMG---Figure"><img src="image/B15777_01_37.jpg" alt="Figure 1.37: Output of the fit command indicating all of the model training parameters&#13;&#10;"/></div><p class="figure-caption">Figure 1.37: Output of the fit command indicating all of the model training parameters</p></li>
				<li>Here, we can see what the value of the regularization parameter was for the two different models. The regularization parameter is chosen according to which produced a model with the lowest error:<p class="source-code">print(f'Best hyperparameter for l1 regularization model: \</p><p class="source-code">{model_l1.C_[0]}')</p><p class="source-code">print(f'Best hyperparameter for l2 regularization model: \</p><p class="source-code">{model_l2.C_[0]}')</p><p>The preceding code produces the following output:</p><p class="source-code">Best hyperparameter for l1 regularization model: 1000000.0</p><p class="source-code">Best hyperparameter for l2 regularization model: 1.0</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">C_</strong> attribute is only available once the model has been trained because it is set once the best parameter from the cross-validation process has been determined.</p></li>
				<li>To evaluate the performance of the models, make predictions on the <strong class="source-inline">test</strong> set, which we'll compare against the <strong class="source-inline">true</strong> values:<p class="source-code">y_pred_l1 = model_l1.predict(X_test)</p><p class="source-code">y_pred_l2 = model_l2.predict(X_test)</p></li>
				<li>To compare these models, calculate the evaluation metrics. First, look at the accuracy of the model:<p class="source-code">from sklearn import metrics</p><p class="source-code">accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, \</p><p class="source-code">                                     y_true=y_test)</p><p class="source-code">accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, \</p><p class="source-code">                                     y_true=y_test)</p><p class="source-code">print(f'Accuracy of the model with l1 regularization is \</p><p class="source-code">{accuracy_l1*100:.4f}%')</p><p class="source-code">print(f'Accuracy of the model with l2 regularization is \</p><p class="source-code">{accuracy_l2*100:.4f}%')</p><p>The preceding code produces the following output:</p><p class="source-code">Accuracy of the model with l1 regularization is 89.2133%</p><p class="source-code">Accuracy of the model with l2 regularization is 89.2944%</p></li>
				<li>Also, look at the other evaluation metrics:<p class="source-code">precision_l1, recall_l1, fscore_l1, _ = \</p><p class="source-code">metrics.precision_recall_fscore_support(y_pred=y_pred_l1, \</p><p class="source-code">                                        y_true=y_test, \</p><p class="source-code">                                        average='binary')</p><p class="source-code">precision_l2, recall_l2, fscore_l2, _ = \</p><p class="source-code">metrics.precision_recall_fscore_support(y_pred=y_pred_l2, \</p><p class="source-code">                                        y_true=y_test, \</p><p class="source-code">                                        average='binary')</p><p class="source-code">print(f'l1\nPrecision: {precision_l1:.4f}\nRecall: \</p><p class="source-code">{recall_l1:.4f}\nfscore: {fscore_l1:.4f}\n\n')</p><p class="source-code">print(f'l2\nPrecision: {precision_l2:.4f}\nRecall: \</p><p class="source-code">{recall_l2:.4f}\nfscore: {fscore_l2:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">l1</p><p class="source-code">Precision: 0.7300</p><p class="source-code">Recall: 0.4078</p><p class="source-code">fscore: 0.5233</p><p class="source-code">l2</p><p class="source-code">Precision: 0.7350</p><p class="source-code">Recall: 0.4106</p><p class="source-code">fscore: 0.5269</p></li>
				<li>Observe the values of the coefficients once the model has been trained:<p class="source-code">coef_list = [f'{feature}: {coef}' for coef, \</p><p class="source-code">             feature in sorted(zip(model_l1.coef_[0], \</p><p class="source-code">                               X_train.columns.values.tolist()))]</p><p class="source-code">for item in coef_list:</p><p class="source-code">    print(item)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">coef_</strong> attribute is only available once the model has been trained because it is set once the best parameter from the cross-validation process has been determined.</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer207" class="IMG---Figure"><img src="image/B15777_01_38.jpg" alt="Figure 1.38: The feature column names and the value of their respective coefficients &#13;&#10;for the model with l1 regularization&#13;&#10;"/></div><p class="figure-caption">Figure 1.38: The feature column names and the value of their respective coefficients for the model with l1 regularization</p></li>
				<li>Do the same for the model with an <strong class="source-inline">l2</strong> regularization parameter type:<p class="source-code">coef_list = [f'{feature}: {coef}' for coef, \</p><p class="source-code">             feature in sorted(zip(model_l2.coef_[0], \</p><p class="source-code">                                   X_train.columns.values.tolist()))]</p><p class="source-code">for item in coef_list:</p><p class="source-code">    print(item)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer208" class="IMG---Figure"><img src="image/B15777_01_39.jpg" alt="Figure 1.39: The feature column names and the value of their respective coefficients &#13;&#10;for the model with l2 regularization&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.39: The feature column names and the value of their respective coefficients for the model with l2 regularization</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VIoe5M">https://packt.live/2VIoe5M</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor190"/>2. Machine Learning versus Deep Learning</h1>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor191"/>Activity 2.01: Creating a Logistic Regression Model Using Keras</h2>
			<p>In this activity, we are going to create a basic model using the Keras library. The model that we will build will classify users of a website into those that will purchase a product from a website and those that will not. To do this, we will utilize the same online shopping purchasing intention dataset that we did previously and attempt to predict the same variables that we did in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras</em>. </p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Open a Jupyter notebook from the start menu to implement this activity. Load in the online shopping purchasing intention datasets, which you can download from the GitHub repository. We will use the pandas library for data loading, so import the <strong class="source-inline">pandas</strong> library. Ensure you have saved the csv files to an appropriate data folder for this chapter first. Alternatively, you can change the path to the files that you use in your code.<p class="source-code">import pandas as pd</p><p class="source-code">feats = pd.read_csv('../data/OSI_feats.csv')</p><p class="source-code">target = pd.read_csv('../data/OSI_target.csv')</p></li>
				<li>For the purposes of this activity, we will not perform any further preprocessing. As we did in the previous chapter, we will split the dataset into training and testing and leave the testing until the very end when we evaluate our models. We will reserve <strong class="source-inline">20%</strong> of our data for testing by setting the <strong class="source-inline">test_size=0.2</strong> parameter, and we will create a <strong class="source-inline">random_state</strong> parameter so that we can recreate the results:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">test_size = 0.2</p><p class="source-code">random_state = 42</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(feats, target, test_size=test_size, \</p><p class="source-code">                 random_state=random_state)</p></li>
				<li>Set a seed in <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> for reproducibility. Begin creating the model by initializing a model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">from keras.models import Sequential</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(random_state)</p><p class="source-code">random.set_seed(random_state)</p><p class="source-code">model = Sequential()</p></li>
				<li>To add a fully connected layer to the model, add a layer of the <strong class="source-inline">Dense</strong> class. Here, we include the number of nodes in the layer. In our case, this will be one since we are performing binary classification and our desired output is <strong class="source-inline">zero</strong> or <strong class="source-inline">one</strong>. Also, specify the input dimensions, which is only done on the first layer of the model. It is there to indicate the format of the input data. Pass the number of features:<p class="source-code">from keras.layers import Dense</p><p class="source-code">model.add(Dense(1, input_dim=X_train.shape[1]))</p></li>
				<li>Add a sigmoid activation function to the output of the previous layer to replicate the <strong class="source-inline">logistic regression</strong> algorithm:<p class="source-code">from keras.layers import Activation</p><p class="source-code">model.add(Activation('sigmoid'))</p></li>
				<li>Once we have all the model components in the correct order, we must compile the model so that all the learning processes are configured. Use the <strong class="source-inline">adam</strong> optimizer, a <strong class="source-inline">binary_crossentropy</strong> for the loss, and track the accuracy of the model by passing the parameter into the <strong class="source-inline">metrics</strong> argument:<p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Print the model summary to verify the model is as we expect it to be:<p class="source-code">print(model.summary())</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer209" class="IMG---Figure"><img src="image/B15777_02_19.jpg" alt="Figure 2.19: A summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 2.19: A summary of the model</p></li>
				<li>Next, fit the model using the <strong class="source-inline">fit</strong> method of the <strong class="source-inline">model</strong> class. Provide the training data, as well as the number of epochs and how much data to use for validation after each epoch:<p class="source-code">history = model.fit(X_train, y_train['Revenue'], epochs=10, \</p><p class="source-code">                    validation_split=0.2, shuffle=False)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer210" class="IMG---Figure"><img src="image/B15777_02_20.jpg" alt="Figure 2.20: Using the fit method on the model&#13;&#10;"/></div><p class="figure-caption">Figure 2.20: Using the fit method on the model</p></li>
				<li>The values for the loss and accuracy have been stored within the <strong class="source-inline">history</strong> variable. Plot the values for each using the loss and accuracy we tracked after each epoch:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code"># Plot training and validation accuracy values</p><p class="source-code">plt.plot(history.history['accuracy'])</p><p class="source-code">plt.plot(history.history['val_accuracy'])</p><p class="source-code">plt.title('Model accuracy')</p><p class="source-code">plt.ylabel('Accuracy')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p class="source-code"># Plot training and validation loss values</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.title('Model loss')</p><p class="source-code">plt.ylabel('Loss')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p>The following plots show the output of the preceding code:</p><div id="_idContainer211" class="IMG---Figure"><img src="image/B15777_02_21.jpg" alt="Figure 2.21: The loss and accuracy while fitting the model&#13;&#10;"/></div><p class="figure-caption">Figure 2.21: The loss and accuracy while fitting the model</p></li>
				<li>Finally, evaluate the model on the test data we held out from the beginning, which will give an objective evaluation of the performance of the model:<p class="source-code">test_loss, test_acc = model.evaluate(X_test, y_test['Revenue'])</p><p class="source-code">print(f'The loss on the test set is {test_loss:.4f} \</p><p class="source-code">and the accuracy is {test_acc*100:.3f}%')</p><p>The output of the preceding code can be found below. Here, the model predicts the purchasing intention of users in the test dataset and evaluates the performance by comparing it to the real values in <strong class="source-inline">y_test</strong>. Evaluating the model on the test dataset produces loss and accuracy values that we can print out:</p><p class="source-code">2466/2466 [==============================] - 0s 15us/step</p><p class="source-code">The loss on the test set is 0.3632 and the accuracy is 86.902%</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to </p><p class="callout"><a href="https://packt.live/3dVTQLe">https://packt.live/3dVTQLe</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2ZxEhV4">https://packt.live/2ZxEhV4</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor192"/>3. Deep Learning with Keras</h1>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor193"/>Activity 3.01: Building a Single-Layer Neural Network for Performing Binary Classification</h2>
			<p>In this activity, we will compare the results of a logistic regression model and single-layer neural networks of different node sizes and different activation functions. The dataset we will use represents the normalized test results of aircraft propeller inspections, while the class represents whether they passed or failed a manual visual inspection. We will create models to predict the results of the manual inspection when given the automated test results. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Load all the required packages:<p class="source-code"># import required packages from Keras</p><p class="source-code">from keras.models import Sequential </p><p class="source-code">from keras.layers import Dense, Activation </p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># import required packages for plotting</p><p class="source-code">import matplotlib.pyplot as plt </p><p class="source-code">import matplotlib</p><p class="source-code">%matplotlib inline </p><p class="source-code">import matplotlib.patches as mpatches</p><p class="source-code"># import the function for plotting decision boundary</p><p class="source-code">from utils import plot_decision_boundary</p></li>
				<li>Set up a <strong class="source-inline">seed</strong>:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 1</p></li>
				<li>Load the simulated dataset and print the size of <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> and the number of examples:<p class="source-code">"""</p><p class="source-code">load the dataset, print the shapes of input and output and the number of examples</p><p class="source-code">"""</p><p class="source-code">feats = pd.read_csv('../data/outlier_feats.csv')</p><p class="source-code">target = pd.read_csv('../data/outlier_target.csv')</p><p class="source-code">print("X size = ", feats.shape)</p><p class="source-code">print("Y size = ", target.shape)</p><p class="source-code">print("Number of examples = ", feats.shape[0])</p><p><strong class="bold">Expected output</strong>:</p><p class="source-code">X size = (3359, 2)</p><p class="source-code">Y size = (3359, 1)</p><p class="source-code">Number of examples = 3359</p></li>
				<li>Plot the dataset. The x and y coordinates of each point will be the two input features. The color of each record represents the <strong class="source-inline">pass</strong>/<strong class="source-inline">fail</strong> result:<p class="source-code">class_1=plt.scatter(feats.loc[target['Class']==0,'feature1'], \</p><p class="source-code">                    feats.loc[target['Class']==0,'feature2'], \</p><p class="source-code">                    c="red", s=40, edgecolor='k')</p><p class="source-code">class_2=plt.scatter(feats.loc[target['Class']==1,'feature1'], \</p><p class="source-code">                    feats.loc[target['Class']==1,'feature2'], \</p><p class="source-code">                    c="blue", s=40, edgecolor='k')</p><p class="source-code">plt.legend((class_1, class_2),('Fail','Pass'))</p><p class="source-code">plt.xlabel('Feature 1')</p><p class="source-code">plt.ylabel('Feature 2')</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer212" class="IMG---Figure"><img src="image/B15777_03_19.jpg" alt="Figure 3.19: Simulated training data points&#13;&#10;"/></div><p class="figure-caption">Figure 3.19: Simulated training data points</p></li>
				<li>Build the <strong class="source-inline">logistic regression</strong> model, which will be a one-node sequential model with no hidden layers and a <strong class="source-inline">sigmoid activation</strong> function:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p><p class="source-code">model.add(Dense(1, activation='sigmoid', input_dim=2))</p><p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(feats, target, batch_size=5, epochs=100, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p><p><strong class="bold">Expected output</strong>:</p><p>The loss on the validation set after <strong class="source-inline">100</strong> epochs = <strong class="source-inline">0.3537</strong>:</p><div id="_idContainer213" class="IMG---Figure"><img src="image/B15777_03_20.jpg" alt="Figure 3.20: The loss details of the last 5 epochs out of 100&#13;&#10;"/></div><p class="figure-caption">Figure 3.20: The loss details of the last 5 epochs out of 100</p></li>
				<li>Plot the decision boundary on the training data:<p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plot_decision_boundary(lambda x: model.predict(x), feats, target)</p><p class="source-code">plt.title("Logistic Regression")</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer214" class="IMG---Figure"><img src="image/B15777_03_21.jpg" alt="Figure 3.21: The decision boundary of the logistic regression model&#13;&#10;"/></div><p class="figure-caption">Figure 3.21: The decision boundary of the logistic regression model</p><p>The linear decision boundary of the logistic regression model is obviously unable to capture the circular decision boundary between the two classes and predicts all the results as a passed result.</p></li>
				<li>Create a neural network with one hidden layer with three nodes and a <strong class="source-inline">relu activation function</strong> and an output layer with one node and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential() </p><p class="source-code">model.add(Dense(3, activation='relu', input_dim=2))</p><p class="source-code">model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(feats, target, batch_size=5, epochs=200, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p><p><strong class="bold">Expected output</strong>:</p><p>The loss that's evaluated on the validation set after <strong class="source-inline">200</strong> epochs = <strong class="source-inline">0.0260</strong>:</p><div id="_idContainer215" class="IMG---Figure"><img src="image/B15777_03_22.jpg" alt="Figure 3.22: The loss details of the last 5 epochs out of 200&#13;&#10;"/></div><p class="figure-caption">Figure 3.22: The loss details of the last 5 epochs out of 200</p></li>
				<li>Plot the decision boundary that was created:<p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plot_decision_boundary(lambda x: model.predict(x), feats, target)</p><p class="source-code">plt.title("Decision Boundary for Neural Network with "\</p><p class="source-code">          "hidden layer size 3")</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer216" class="IMG---Figure"><img src="image/B15777_03_23.jpg" alt="Figure 3.23: The decision boundary for the neural network with a hidden layer &#13;&#10;size of 3 and a ReLU activation function&#13;&#10;"/></div><p class="figure-caption">Figure 3.23: The decision boundary for the neural network with a hidden layer size of 3 and a ReLU activation function</p><p>Having three processing units instead of one dramatically improved the capability of the model in capturing the non-linear boundary between the two classes. Notice that the loss value decreased drastically in comparison to the previous step.</p></li>
				<li>Create a neural network with one hidden layer with six nodes and a <strong class="source-inline">relu activation function</strong> and an output layer with one node and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential() </p><p class="source-code">model.add(Dense(6, activation='relu', input_dim=2))</p><p class="source-code">model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(feats, target, batch_size=5, epochs=400, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p><p><strong class="bold">Expected output</strong>:</p><p>The loss after <strong class="source-inline">400</strong> epochs = <strong class="source-inline">0.0231</strong>:</p><div id="_idContainer217" class="IMG---Figure"><img src="image/B15777_03_24.jpg" alt="Figure 3.24: The loss details of the last 5 epochs out of 400&#13;&#10;"/></div><p class="figure-caption">Figure 3.24: The loss details of the last 5 epochs out of 400</p></li>
				<li>Plot the decision boundary:<p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plot_decision_boundary(lambda x: model.predict(x), feats, target)</p><p class="source-code">plt.title("Decision Boundary for Neural Network with "\</p><p class="source-code">          "hidden layer size 6")</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer218" class="IMG---Figure"><img src="image/B15777_03_25.jpg" alt="Figure 3.25: The decision boundary for the neural network with a hidden layer &#13;&#10;size of 6 and the ReLU activation function&#13;&#10;"/></div><p class="figure-caption">Figure 3.25: The decision boundary for the neural network with a hidden layer size of 6 and the ReLU activation function</p><p>By doubling the number of units in the hidden layer, the decision boundary of the model gets closer to a true circular shape, and the loss value is decreased even more in comparison to the previous step.</p></li>
				<li>Create a neural network with one hidden layer with three nodes and a <strong class="source-inline">tanh activation function</strong> and an output layer with one node and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential() </p><p class="source-code">model.add(Dense(3, activation='tanh', input_dim=2))</p><p class="source-code">model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(feats, target, batch_size=5, epochs=200, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p><p><strong class="bold">Expected output</strong>:</p><p>The loss after <strong class="source-inline">200</strong> epochs = <strong class="source-inline">0.0426</strong>:</p><div id="_idContainer219" class="IMG---Figure"><img src="image/B15777_03_26.jpg" alt="Figure 3.26: The loss details of the last 5 epochs out of 200&#13;&#10;"/></div><p class="figure-caption">Figure 3.26: The loss details of the last 5 epochs out of 200</p></li>
				<li>Plot the decision boundary:<p class="source-code">plot_decision_boundary(lambda x: model.predict(x), feats, target) </p><p class="source-code">plt.title("Decision Boundary for Neural Network with "\</p><p class="source-code">          "hidden layer size 3")</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer220" class="IMG---Figure"><img src="image/B15777_03_27.jpg" alt="Figure 3.27: The decision boundary for the neural network with a hidden layer &#13;&#10;size of 3 and the tanh activation function&#13;&#10;"/></div><p class="figure-caption">Figure 3.27: The decision boundary for the neural network with a hidden layer size of 3 and the tanh activation function</p><p>Using the <strong class="source-inline">tanh</strong> activation function has eliminated the sharp edges in the decision boundary. In other words, it has made the decision boundary smoother. However, the model is not performing better since we can see an increase in the loss value. We achieved similar loss and accuracy scores when we evaluated on the test dataset, despite mentioning previously that the learning parameters for <strong class="source-inline">tanh</strong> are slower than they are for <strong class="source-inline">relu</strong>.</p></li>
				<li>Create a neural network with one hidden layer with six nodes and a <strong class="source-inline">tanh activation function</strong> and an output layer with one node and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential() </p><p class="source-code">model.add(Dense(6, activation='tanh', input_dim=2))</p><p class="source-code">model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(feats, target, batch_size=5, epochs=400, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p><p><strong class="bold">Expected output</strong>:</p><p>The loss after <strong class="source-inline">400</strong> epochs = <strong class="source-inline">0.0215</strong>:</p><div id="_idContainer221" class="IMG---Figure"><img src="image/B15777_03_28.jpg" alt="Figure 3.28: The loss details of the last 5 epochs out of 400&#13;&#10;"/></div><p class="figure-caption">Figure 3.28: The loss details of the last 5 epochs out of 400</p></li>
				<li>Plot the decision boundary:<p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plot_decision_boundary(lambda x: model.predict(x), feats, target)</p><p class="source-code">plt.title("Decision Boundary for Neural Network with "\</p><p class="source-code">          "hidden layer size 6")</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer222" class="IMG---Figure"><img src="image/B15777_03_29.jpg" alt="Figure 3.29: The decision boundary for the neural network with a hidden layer size &#13;&#10;of 6 and the tanh activation function&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.29: The decision boundary for the neural network with a hidden layer size of 6 and the tanh activation function</p>
			<p>Again, using the <strong class="source-inline">tanh</strong> activation function instead of <strong class="source-inline">relu</strong> and adding more nodes to our hidden layer has smoothed the curves on the decision boundary more, fitting the training data better according to the accuracy of the training data. We should be careful not to add too many nodes to the hidden layer as we may begin to overfit the data. This can be observed by evaluating the test set, where there is a slight decrease in the accuracy of the neural network with six nodes compared to a neural network with three.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iv0wn1">https://packt.live/3iv0wn1</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2BqumZt">https://packt.live/2BqumZt</a>.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>Activity 3.02: Advanced Fibrosis Diagnosis with Neural Networks</h2>
			<p>In this activity, you are going to use a real dataset to predict whether a patient has advanced fibrosis based on measurements such as age, gender, and BMI. The dataset consists of information for 1,385 patients who underwent treatment dosages for hepatitis C. For each patient, <strong class="source-inline">28</strong> different attributes are available, as well as a class label, which can only take two values: <strong class="source-inline">1</strong>, indicating advanced fibrosis, and <strong class="source-inline">0</strong>, indicating no indication of advanced fibrosis. This is a binary/two-class classification problem with an input dimension equal to 28. </p>
			<p>In this activity, you will implement different deep neural network architectures to perform this classification, plot the trends in training error rates and test error rates, and determine how many epochs the final classifier needs to be trained for. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the necessary libraries and load the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">import matplotlib.pyplot as plt </p><p class="source-code">import matplotlib</p><p class="source-code">%matplotlib inline</p><p class="source-code">X = pd.read_csv('../data/HCV_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/HCV_target.csv')</p></li>
				<li>Print the number of <strong class="source-inline">records</strong> and <strong class="source-inline">features</strong> in the <strong class="source-inline">feature</strong> dataset and the number of unique classes in the <strong class="source-inline">target</strong> dataset:<p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p><p class="source-code">print("Number of Features for each example = ", X.shape[1]) </p><p class="source-code">print("Possible Output Classes = ", \</p><p class="source-code">      y['AdvancedFibrosis'].unique())</p><p><strong class="bold">Expected output</strong>:</p><p class="source-code">Number of Examples in the Dataset = 1385</p><p class="source-code">Number of Features for each example = 28</p><p class="source-code">Possible Output Classes = [0 1]</p></li>
				<li>Normalize the data and scale it. Following this, split the dataset into the <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> sets:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">sc = StandardScaler()</p><p class="source-code">X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, y, test_size=0.2, random_state=seed)</p><p class="source-code"># Print the information regarding dataset sizes</p><p class="source-code">print(X_train.shape)</p><p class="source-code">print(y_train.shape)</p><p class="source-code">print(X_test.shape)</p><p class="source-code">print(y_test.shape)</p><p class="source-code">print ("Number of examples in training set = ", X_train.shape[0])</p><p class="source-code">print ("Number of examples in test set = ", X_test.shape[0])</p><p><strong class="bold">Expected output</strong>:</p><p class="source-code">(1108, 28)</p><p class="source-code">(1108, 1)</p><p class="source-code">(277, 28)</p><p class="source-code">(277, 1)</p><p class="source-code">Number of examples in training set = 1108</p><p class="source-code">Number of examples in test set = 277</p></li>
				<li>Implement a deep neural network with one hidden layer of size <strong class="source-inline">3</strong> and a <strong class="source-inline">tanh activation function</strong>, an output layer with one node, and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model and print out a summary of the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model</p><p class="source-code">classifier = Sequential()</p><p class="source-code">classifier.add(Dense(units = 3, activation = 'tanh', \</p><p class="source-code">                     input_dim=X_train.shape[1]))</p><p class="source-code">classifier.add(Dense(units = 1, activation = 'sigmoid'))</p><p class="source-code">classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', \</p><p class="source-code">                   metrics = ['accuracy'])</p><p class="source-code">classifier.summary()</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer223" class="IMG---Figure"><img src="image/B15777_03_30.jpg" alt="Figure 3.30: The architecture of the neural network&#13;&#10;"/></div><p class="figure-caption">Figure 3.30: The architecture of the neural network</p></li>
				<li>Fit the model to the training data:<p class="source-code">history=classifier.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                       epochs = 100, validation_split=0.1, \</p><p class="source-code">                       shuffle=False)</p></li>
				<li>Plot the <strong class="source-inline">training error rate</strong> and <strong class="source-inline">test error rate</strong> for every epoch:<p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer224" class="IMG---Figure"><img src="image/B15777_03_31.jpg" alt="Figure 3.31: A plot of the training error rate and test error rate while training the model &#13;&#10;"/></div><p class="figure-caption">Figure 3.31: A plot of the training error rate and test error rate while training the model </p></li>
				<li>Print the values of the best accuracy that was reached on the training set and on the test set, as well as the <strong class="source-inline">loss</strong> and <strong class="source-inline">accuracy</strong> that was evaluated on the <strong class="source-inline">test</strong> dataset.<p class="source-code">print(f"Best Accuracy on training set = \</p><p class="source-code">{max(history.history['accuracy'])*100:.3f}%")</p><p class="source-code">print(f"Best Accuracy on validation set = \</p><p class="source-code">{max(history.history['val_accuracy'])*100:.3f}%") </p><p class="source-code">test_loss, test_acc = \</p><p class="source-code">classifier.evaluate(X_test, y_test['AdvancedFibrosis'])</p><p class="source-code">print(f'The loss on the test set is {test_loss:.4f} and \</p><p class="source-code">the accuracy is {test_acc*100:.3f}%')</p><p>The following image shows the output of the preceding code:</p><p class="source-code">Best Accuracy on training set = 52.959%</p><p class="source-code">Best Accuracy on validation set = 58.559%</p><p class="source-code">277/277 [==============================] - 0s 25us/step</p><p class="source-code">The loss on the test set is 0.6885 and the accuracy is 55.235%</p></li>
				<li>Implement a deep neural network with two hidden layers of sizes <strong class="source-inline">4</strong> and <strong class="source-inline">2</strong> with a <strong class="source-inline">tanh activation function</strong>, an output layer with one node, and a <strong class="source-inline">sigmoid activation function</strong>. Finally, compile the model and print out a summary of the model:<p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model</p><p class="source-code">classifier = Sequential()</p><p class="source-code">classifier.add(Dense(units = 4, activation = 'tanh', \</p><p class="source-code">                     input_dim = X_train.shape[1]))</p><p class="source-code">classifier.add(Dense(units = 2, activation = 'tanh'))</p><p class="source-code">classifier.add(Dense(units = 1, activation = 'sigmoid'))</p><p class="source-code">classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', \</p><p class="source-code">                   metrics = ['accuracy'])</p><p class="source-code">classifier.summary()</p><div id="_idContainer225" class="IMG---Figure"><img src="image/B15777_03_32.jpg" alt="Figure 3.32: The architecture of the neural network&#13;&#10;"/></div><p class="figure-caption">Figure 3.32: The architecture of the neural network</p></li>
				<li>Fit the model to the training data:<p class="source-code">history=classifier.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                       epochs = 100, validation_split=0.1, \</p><p class="source-code">                       shuffle=False)</p></li>
				<li>Plot training and test error plots with two hidden layers of size 4 and 2. Print the best accuracy that was reached on the training and test sets:<p class="source-code"># plot training error and test error plots </p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer226" class="IMG---Figure"><img src="image/B15777_03_33.jpg" alt="Figure 3.33: A plot of the training error and test error rates while training the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.33: A plot of the training error and test error rates while training the model</p></li>
				<li>Print the values of the best accuracy that was achieved on the <strong class="source-inline">training</strong> set and on the <strong class="source-inline">test</strong> set, as well as the <strong class="source-inline">loss</strong> and <strong class="source-inline">accuracy</strong> that was evaluated on the test dataset.<p class="source-code">print(f"Best Accuracy on training set = \</p><p class="source-code">{max(history.history['accuracy'])*100:.3f}%")</p><p class="source-code">print(f"Best Accuracy on validation set = \</p><p class="source-code">{max(history.history['val_accuracy'])*100:.3f}%") </p><p class="source-code">test_loss, test_acc = \</p><p class="source-code">classifier.evaluate(X_test, y_test['AdvancedFibrosis'])</p><p class="source-code">print(f'The loss on the test set is {test_loss:.4f} and \</p><p class="source-code">the accuracy is {test_acc*100:.3f}%')</p><p>The following shows the output of the preceding code:</p><p class="source-code">Best Accuracy on training set = 57.272%</p><p class="source-code">Best Accuracy on test set = 54.054%</p><p class="source-code">277/277 [==============================] - 0s 41us/step</p><p class="source-code">The loss on the test set is 0.7016 and the accuracy is 49.819%</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BrIRMF">https://packt.live/2BrIRMF</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2NUl22A">https://packt.live/2NUl22A</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor195"/>4. Evaluating Your Model with Cross-Validation Using Keras Wrappers</h1>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/>Activity 4.01: Model Evaluation Using Cross-Validation for an Advanced Fibrosis Diagnosis Classifier</h2>
			<p>In this activity, we are going to use what we learned in this topic to train and evaluate a deep learning model using <strong class="source-inline">k-fold cross-validation</strong>. We will use the model that resulted in the best test error rate from the previous activity and the goal will be to compare the cross-validation error rate with the training set/test set approach error rate. The dataset we will use is the hepatitis C dataset, in which we will build a classification model to predict which patients get advanced fibrosis. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Load the dataset and print the number of records and features in the dataset, as well as the number of possible classes in the target dataset:<p class="source-code"># Load the dataset</p><p class="source-code">import pandas as pd</p><p class="source-code">X = pd.read_csv('../data/HCV_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/HCV_target.csv')</p><p class="source-code"># Print the sizes of the dataset</p><p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p><p class="source-code">print("Number of Features for each example = ", X.shape[1]) </p><p class="source-code">print("Possible Output Classes = ", \</p><p class="source-code">      y['AdvancedFibrosis'].unique())</p><p>Here's the expected output:</p><p class="source-code">Number of Examples in the Dataset = 1385</p><p class="source-code">Number of Features for each example = 28</p><p class="source-code">Possible Output Classes = [0 1]</p></li>
				<li>Define the function that returns the Keras model. First, import the necessary libraries for Keras. Inside the function, instantiate the sequential model and add two dense layers, with the first of <strong class="source-inline">size 4</strong> and the second of <strong class="source-inline">size 2</strong>, both with <strong class="source-inline">tanh activation</strong> functions. Add the output layer with a <strong class="source-inline">sigmoid activation</strong> function. Compile the model and return the model from the function:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code"># Create the function that returns the keras model</p><p class="source-code">def build_model():</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4, input_dim=X.shape[1], activation='tanh'))</p><p class="source-code">    model.add(Dense(2, activation='tanh'))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    model.compile(loss='binary_crossentropy', optimizer='adam', \</p><p class="source-code">                  metrics=['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Scale the training data using the <strong class="source-inline">StandardScaler</strong> function. Set the seed so that the model is reproducible. Define the <strong class="source-inline">n_folds</strong>, <strong class="source-inline">epochs</strong>, and <strong class="source-inline">batch_size</strong> hyperparameters. Then, build the Keras wrapper with scikit-learn, define the <strong class="source-inline">cross-validation</strong> iterator, perform <strong class="source-inline">k-fold cross-validation</strong>, and store the scores:<p class="source-code"># import required packages</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">from keras.wrappers.scikit_learn import KerasClassifier</p><p class="source-code">from sklearn.model_selection import StratifiedKFold</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code">X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)</p><p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">"""</p><p class="source-code">determine the number of folds for k-fold cross-validation, number of epochs and batch size</p><p class="source-code">"""</p><p class="source-code">n_folds = 5</p><p class="source-code">epochs = 100</p><p class="source-code">batch_size = 20</p><p class="source-code"># build the scikit-learn interface for the keras model</p><p class="source-code">classifier = KerasClassifier(build_fn=build_model, \</p><p class="source-code">                             epochs=epochs, \</p><p class="source-code">                             batch_size=batch_size, \</p><p class="source-code">                             verbose=1, shuffle=False)</p><p class="source-code"># define the cross-validation iterator</p><p class="source-code">kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                        random_state=seed)</p><p class="source-code">"""</p><p class="source-code">perform the k-fold cross-validation and store the scores in results</p><p class="source-code">"""</p><p class="source-code">results = cross_val_score(classifier, X, y, cv=kfold)</p></li>
				<li>For each of the folds, print the accuracy stored in the <strong class="source-inline">results</strong> parameter: <p class="source-code"># print accuracy for each fold</p><p class="source-code">for f in range(n_folds):</p><p class="source-code">    print("Test accuracy at fold ", f+1, " = ", results[f])</p><p class="source-code">print("\n")</p><p class="source-code">"""</p><p class="source-code">print overall cross-validation accuracy plus the standard deviation of the accuracies</p><p class="source-code">"""</p><p class="source-code">print("Final Cross-validation Test Accuracy:", results.mean())</p><p class="source-code">print("Standard Deviation of Final Test Accuracy:", results.std())</p><p>Here's the expected output:</p><p class="source-code">Test accuracy at fold 1 = 0.5198556184768677</p><p class="source-code">Test accuracy at fold 2 = 0.4693140685558319</p><p class="source-code">Test accuracy at fold 3 = 0.512635350227356</p><p class="source-code">Test accuracy at fold 4 = 0.5740072131156921</p><p class="source-code">Test accuracy at fold 5 = 0.5523465871810913</p><p class="source-code">Final Cross-Validation Test Accuracy: 0.5256317675113678</p><p class="source-code">Standard Deviation of Final Test Accuracy: 0.03584760640500936</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eWgR2b">https://packt.live/3eWgR2b</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iBYtOi">https://packt.live/3iBYtOi</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor197"/>Activity 4.02: Model Selection Using Cross-Validation for the Advanced Fibrosis Diagnosis Classifier</h2>
			<p>In this activity, we are going to improve our classifier for the hepatitis C dataset by using cross-validation for model selection and hyperparameter selection. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the required packages and load the dataset. Scale the dataset using the <strong class="source-inline">StandardScaler</strong> function:<p class="source-code"># import the required packages</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.wrappers.scikit_learn import KerasClassifier</p><p class="source-code">from sklearn.model_selection import StratifiedKFold</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from tensorflow import random</p><p class="source-code"># Load the dataset</p><p class="source-code">X = pd.read_csv('../data/HCV_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/HCV_target.csv')</p><p class="source-code">sc = StandardScaler()</p><p class="source-code">X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)</p></li>
				<li>Define three functions, each returning a different Keras model. The first model should have three hidden layers of <strong class="source-inline">size 4</strong>, the second model should have two hidden layers, the first of <strong class="source-inline">size 4</strong> and the second of <strong class="source-inline">size 2</strong>, and the third model should have two hidden layers of <strong class="source-inline">size 8</strong>. Use function parameters for the activation functions and optimizers so that they can be passed through to the model. The goal is to find out which of these three models leads to the lowest cross-validation error rate:<p class="source-code"># Create the function that returns the keras model 1</p><p class="source-code">def build_model_1(activation='relu', optimizer='adam'):</p><p class="source-code">    # create model 1</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4, input_dim=X.shape[1], \</p><p class="source-code">                    activation=activation))</p><p class="source-code">    model.add(Dense(4, activation=activation))</p><p class="source-code">    model.add(Dense(4, activation=activation))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer=optimizer, metrics=['accuracy'])</p><p class="source-code">    return model</p><p class="source-code"># Create the function that returns the keras model 2</p><p class="source-code">def build_model_2(activation='relu', optimizer='adam'):</p><p class="source-code">    # create model 2</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4, input_dim=X.shape[1], \</p><p class="source-code">                    activation=activation))</p><p class="source-code">    model.add(Dense(2, activation=activation))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer=optimizer, metrics=['accuracy'])</p><p class="source-code">    return model</p><p class="source-code"># Create the function that returns the keras model 3</p><p class="source-code">def build_model_3(activation='relu', optimizer='adam'):</p><p class="source-code">    # create model 3</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(8, input_dim=X.shape[1], \</p><p class="source-code">                    activation=activation))</p><p class="source-code">    model.add(Dense(8, activation=activation))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer=optimizer, metrics=['accuracy'])</p><p class="source-code">    return model</p><p>Write the code that will loop over the three models and perform <strong class="source-inline">5-fold cross-validation</strong>. Set the seed so that the models are reproducible and define the <strong class="source-inline">n_folds</strong>, <strong class="source-inline">batch_size</strong>, and <strong class="source-inline">epochs</strong> hyperparameters. Store the results from applying the <strong class="source-inline">cross_val_score</strong> function when training the models:</p><p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 2</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">"""</p><p class="source-code">determine the number of folds for k-fold cross-validation, number of epochs and batch size</p><p class="source-code">"""</p><p class="source-code">n_folds = 5</p><p class="source-code">batch_size=20</p><p class="source-code">epochs=100</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_1 = []</p><p class="source-code"># define the possible options for the model</p><p class="source-code">models = [build_model_1, build_model_2, build_model_3]</p><p class="source-code"># loop over models</p><p class="source-code">for m in range(len(models)):</p><p class="source-code">    # build the scikit-learn interface for the keras model</p><p class="source-code">    classifier = KerasClassifier(build_fn=models[m], \</p><p class="source-code">                                 epochs=epochs, \</p><p class="source-code">                                 batch_size=batch_size, \</p><p class="source-code">                                 verbose=0, shuffle=False)</p><p class="source-code">    # define the cross-validation iterator</p><p class="source-code">    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                            random_state=seed)</p><p class="source-code">    """</p><p class="source-code">    perform the k-fold cross-validation and store the scores </p><p class="source-code">    in result</p><p class="source-code">    """</p><p class="source-code">    result = cross_val_score(classifier, X, y, cv=kfold)</p><p class="source-code">    # add the scores to the results list </p><p class="source-code">    results_1.append(result)</p><p class="source-code"># Print cross-validation score for each model</p><p class="source-code">for m in range(len(models)):</p><p class="source-code">    print("Model", m+1,"Test Accuracy =", results_1[m].mean())</p><p>Here's an example output. In this instance, <strong class="bold">Model 2</strong> has the best cross-validation test accuracy, as you can see below:</p><p class="source-code">Model 1 Test Accuracy = 0.4996389865875244</p><p class="source-code">Model 2 Test Accuracy = 0.5148014307022095</p><p class="source-code">Model 3 Test Accuracy = 0.5097472846508027</p></li>
				<li>Choose the model with the highest accuracy score and repeat <em class="italic">step 2</em> by iterating over the <strong class="source-inline">epochs = [100, 200]</strong> and <strong class="source-inline">batches = [10, 20]</strong> values and performing <strong class="source-inline">5-fold cross-validation</strong>:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># determine the number of folds for k-fold cross-validation</p><p class="source-code">n_folds = 5</p><p class="source-code"># define possible options for epochs and batch_size</p><p class="source-code">epochs = [100, 200]</p><p class="source-code">batches = [10, 20]</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_2 = []</p><p class="source-code"># loop over all possible pairs of epochs, batch_size</p><p class="source-code">for e in range(len(epochs)):</p><p class="source-code">    for b in range(len(batches)):</p><p class="source-code">        # build the scikit-learn interface for the keras model</p><p class="source-code">        classifier = KerasClassifier(build_fn=build_model_2, \</p><p class="source-code">                                     epochs=epochs[e], \</p><p class="source-code">                                     batch_size=batches[b], \</p><p class="source-code">                                     verbose=0)</p><p class="source-code">        # define the cross-validation iterator</p><p class="source-code">        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                                random_state=seed)</p><p class="source-code">        # perform the k-fold cross-validation. </p><p class="source-code">        # store the scores in result</p><p class="source-code">        result = cross_val_score(classifier, X, y, cv=kfold)</p><p class="source-code">        # add the scores to the results list </p><p class="source-code">        results_2.append(result)</p><p class="source-code">"""</p><p class="source-code">Print cross-validation score for each possible pair of epochs, batch_size</p><p class="source-code">"""</p><p class="source-code">c = 0</p><p class="source-code">for e in range(len(epochs)):</p><p class="source-code">    for b in range(len(batches)):</p><p class="source-code">        print("batch_size =", batches[b],", epochs =", epochs[e], \</p><p class="source-code">              ", Test Accuracy =", results_2[c].mean())</p><p class="source-code">        c += 1</p><p>Here's an example output:</p><p class="source-code">batch_size = 10 , epochs = 100 , Test Accuracy = 0.5010830342769623</p><p class="source-code">batch_size = 20 , epochs = 100 , Test Accuracy = 0.5126353740692139</p><p class="source-code">batch_size = 10 , epochs = 200 , Test Accuracy = 0.5176895320416497</p><p class="source-code">batch_size = 20 , epochs = 200 , Test Accuracy = 0.5075812220573426</p><p>In this case, the <strong class="source-inline">batch_size= 10</strong>, <strong class="source-inline">epochs=200</strong> pair has the best cross-validation test accuracy.</p></li>
				<li>Choose the batch size and epochs with the highest accuracy score and repeat <em class="italic">step 3</em> by iterating over the <strong class="source-inline">optimizers = ['rmsprop', 'adam','sgd']</strong> and <strong class="source-inline">activations = ['relu', 'tanh']</strong> values and performing <strong class="source-inline">5-fold cross-validation</strong>:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">"""</p><p class="source-code">determine the number of folds for k-fold cross-validation, number of epochs and batch size</p><p class="source-code">"""</p><p class="source-code">n_folds = 5</p><p class="source-code">batch_size = 10</p><p class="source-code">epochs = 200</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_3 = []</p><p class="source-code"># define possible options for optimizer and activation</p><p class="source-code">optimizers = ['rmsprop', 'adam','sgd']</p><p class="source-code">activations = ['relu', 'tanh']</p><p class="source-code"># loop over all possible pairs of optimizer, activation</p><p class="source-code">for o in range(len(optimizers)):</p><p class="source-code">    for a in range(len(activations)):</p><p class="source-code">        optimizer = optimizers[o]</p><p class="source-code">        activation = activations[a]</p><p class="source-code">        # build the scikit-learn interface for the keras model</p><p class="source-code">        classifier = KerasClassifier(build_fn=build_model_2, \</p><p class="source-code">                                     epochs=epochs, \</p><p class="source-code">                                     batch_size=batch_size, \</p><p class="source-code">                                     verbose=0, shuffle=False)</p><p class="source-code">        # define the cross-validation iterator</p><p class="source-code">        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                                random_state=seed)</p><p class="source-code">        # perform the k-fold cross-validation. </p><p class="source-code">        # store the scores in result</p><p class="source-code">        result = cross_val_score(classifier, X, y, cv=kfold)</p><p class="source-code">        # add the scores to the results list </p><p class="source-code">        results_3.append(result)</p><p class="source-code">"""</p><p class="source-code">Print cross-validation score for each possible pair of optimizer, activation</p><p class="source-code">"""</p><p class="source-code">c = 0</p><p class="source-code">for o in range(len(optimizers)):</p><p class="source-code">    for a in range(len(activations)):</p><p class="source-code">        print("activation = ", activations[a],", optimizer = ", \</p><p class="source-code">              optimizers[o], ", Test accuracy = ", \</p><p class="source-code">              results_3[c].mean())</p><p class="source-code">        c += 1</p><p>Here's the expected output:</p><p class="source-code">activation =  relu , optimizer =  rmsprop , </p><p class="source-code">Test accuracy =  0.5234657049179077</p><p class="source-code">activation =  tanh , optimizer =  rmsprop , </p><p class="source-code">Test accuracy =  0.49602887630462644</p><p class="source-code">activation =  relu , optimizer =  adam , </p><p class="source-code">Test accuracy =  0.5039711117744445</p><p class="source-code">activation =  tanh , optimizer =  adam , </p><p class="source-code">Test accuracy =  0.4989169597625732</p><p class="source-code">activation =  relu , optimizer =  sgd , </p><p class="source-code">Test accuracy =  0.48953068256378174</p><p class="source-code">activation =  tanh , optimizer =  sgd , </p><p class="source-code">Test accuracy =  0.5191335678100586</p><p>Here, the <strong class="source-inline">activation='relu'</strong> and <strong class="source-inline">optimizer='rmsprop'</strong> pair has the best cross-validation test accuracy. Also, the <strong class="source-inline">activation='tanh'</strong> and <strong class="source-inline">optimizer='sgd'</strong> pair results in the second-best performance.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2D3AIhD">https://packt.live/2D3AIhD</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2NUpiiC">https://packt.live/2NUpiiC</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/>Activity 4.03: Model Selection Using Cross-validation on a Traffic Volume Dataset</h2>
			<p>In this activity, you are going to practice model selection using cross-validation one more time. Here, we are going to use a simulated dataset that represents a target variable representing the volume of traffic in cars/hour across a city bridge and various normalized features related to traffic data such as time of day and the traffic volume on the previous day. Our goal is to build a model that predicts the traffic volume across the city bridge given the various features. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the required packages and load the dataset:<p class="source-code"># import the required packages</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.wrappers.scikit_learn import KerasRegressor</p><p class="source-code">from sklearn.model_selection import KFold</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Load the dataset, print the input and output size for the feature dataset, and print the possible classes in the target dataset. Also, print the range of the output:<p class="source-code"># Load the dataset</p><p class="source-code"># Load the dataset</p><p class="source-code">X = pd.read_csv('../data/traffic_volume_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/traffic_volume_target.csv') </p><p class="source-code"># Print the sizes of input data and output data</p><p class="source-code">print("Input data size = ", X.shape)</p><p class="source-code">print("Output size = ", y.shape)</p><p class="source-code"># Print the range for output</p><p class="source-code">print(f"Output Range = ({y['Volume'].min()}, \</p><p class="source-code">{ y['Volume'].max()})")</p><p>Here's the expected output:</p><p class="source-code">Input data size =  (10000, 10)</p><p class="source-code">Output size =  (10000, 1)</p><p class="source-code">Output Range = (0.000000, 584.000000)</p></li>
				<li>Define three functions, each returning a different Keras model. The first model should have one hidden layer of <strong class="source-inline">size 10</strong>, the second model should have two hidden layers of <strong class="source-inline">size 10</strong>, and the third model should have three hidden layers of <strong class="source-inline">size 10</strong>. Use function parameters for the optimizers so that they can be passed through to the model. The goal is to find out which of these three models leads to the lowest cross-validation error rate:<p class="source-code"># Create the function that returns the keras model 1</p><p class="source-code">def build_model_1(optimizer='adam'):</p><p class="source-code">    # create model 1</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(10, input_dim=X.shape[1], activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='mean_squared_error', optimizer=optimizer)</p><p class="source-code">    return model</p><p class="source-code"># Create the function that returns the keras model 2</p><p class="source-code">def build_model_2(optimizer='adam'):</p><p class="source-code">    # create model 2</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(10, input_dim=X.shape[1], activation='relu'))</p><p class="source-code">    model.add(Dense(10, activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='mean_squared_error', optimizer=optimizer)</p><p class="source-code">    return model</p><p class="source-code"># Create the function that returns the keras model 3</p><p class="source-code">def build_model_3(optimizer='adam'):</p><p class="source-code">    # create model 3</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(10, input_dim=X.shape[1], activation='relu'))</p><p class="source-code">    model.add(Dense(10, activation='relu'))</p><p class="source-code">    model.add(Dense(10, activation='relu'))</p><p class="source-code">    model.add(Dense(1))</p><p class="source-code">    # Compile model</p><p class="source-code">    model.compile(loss='mean_squared_error', optimizer=optimizer)</p><p class="source-code">    return model</p></li>
				<li>Write the code that will loop over the three models and perform <strong class="source-inline">5-fold cross-validation</strong>. Set the seed so that the models are reproducible and define the <strong class="source-inline">n_folds</strong> hyperparameters. Store the results from applying the <strong class="source-inline">cross_val_score</strong> function when training the models:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># determine the number of folds for k-fold cross-validation</p><p class="source-code">n_folds = 5</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_1 = []</p><p class="source-code"># define the possible options for the model</p><p class="source-code">models = [build_model_1, build_model_2, build_model_3]</p><p class="source-code"># loop over models</p><p class="source-code">for i in range(len(models)):</p><p class="source-code">    # build the scikit-learn interface for the keras model</p><p class="source-code">    regressor = KerasRegressor(build_fn=models[i], epochs=100, \</p><p class="source-code">                               batch_size=50, verbose=0, \</p><p class="source-code">                               shuffle=False)</p><p class="source-code">    """</p><p class="source-code">    build the pipeline of transformations so for each fold training </p><p class="source-code">    set will be scaled and test set will be scaled accordingly.</p><p class="source-code">    """</p><p class="source-code">    model = make_pipeline(StandardScaler(), regressor)</p><p class="source-code">    # define the cross-validation iterator</p><p class="source-code">    kfold = KFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                  random_state=seed)</p><p class="source-code">    # perform the k-fold cross-validation. </p><p class="source-code">    # store the scores in result</p><p class="source-code">    result = cross_val_score(model, X, y, cv=kfold)</p><p class="source-code">    # add the scores to the results list </p><p class="source-code">    results_1.append(result)</p><p class="source-code"># Print cross-validation score for each model</p><p class="source-code">for i in range(len(models)):</p><p class="source-code">    print("Model ", i+1," test error rate = ", \</p><p class="source-code">          abs(results_1[i].mean()))</p><p>The following is the expected output:</p><p class="source-code">Model  1  test error rate =  25.48777518749237</p><p class="source-code">Model  2  test error rate =  25.30460816860199</p><p class="source-code">Model  3  test error rate =  25.390239462852474</p><p><strong class="source-inline">Model 2</strong> (a two-layer neural network) has the lowest test error rate.</p></li>
				<li>Choose the model with the lowest test error rate and repeat <em class="italic">step 4</em> while iterating over <strong class="source-inline">epochs = [80, 100]</strong> and <strong class="source-inline">batches = [50, 25]</strong> and performing <strong class="source-inline">5-fold cross-validation</strong>:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># determine the number of folds for k-fold cross-validation</p><p class="source-code">n_folds = 5</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_2 = []</p><p class="source-code"># define possible options for epochs and batch_size</p><p class="source-code">epochs = [80, 100]</p><p class="source-code">batches = [50, 25]</p><p class="source-code"># loop over all possible pairs of epochs, batch_size</p><p class="source-code">for i in range(len(epochs)):</p><p class="source-code">    for j in range(len(batches)):</p><p class="source-code">        # build the scikit-learn interface for the keras model</p><p class="source-code">        regressor = KerasRegressor(build_fn=build_model_2, \</p><p class="source-code">                                   epochs=epochs[i], \</p><p class="source-code">                                   batch_size=batches[j], \</p><p class="source-code">                                   verbose=0, shuffle=False)</p><p class="source-code">        """</p><p class="source-code">        build the pipeline of transformations so for each fold </p><p class="source-code">        training set will be scaled and test set will be scaled </p><p class="source-code">        accordingly.</p><p class="source-code">        """</p><p class="source-code">        model = make_pipeline(StandardScaler(), regressor)</p><p class="source-code">        # define the cross-validation iterator</p><p class="source-code">        kfold = KFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                      random_state=seed)</p><p class="source-code">        # perform the k-fold cross-validation. </p><p class="source-code">        # store the scores in result</p><p class="source-code">        result = cross_val_score(model, X, y, cv=kfold)</p><p class="source-code">        # add the scores to the results list </p><p class="source-code">        results_2.append(result)</p><p class="source-code">"""</p><p class="source-code">Print cross-validation score for each possible pair of epochs, batch_size</p><p class="source-code">"""</p><p class="source-code">c = 0</p><p class="source-code">for i in range(len(epochs)):</p><p class="source-code">    for j in range(len(batches)):</p><p class="source-code">        print("batch_size = ", batches[j],\</p><p class="source-code">              ", epochs = ", epochs[i], \</p><p class="source-code">              ", Test error rate = ", abs(results_2[c].mean()))</p><p class="source-code">        c += 1</p><p>Here's the expected output:</p><p class="source-code">batch_size = 50 , epochs = 80 , Test error rate = 25.270704221725463</p><p class="source-code">batch_size = 25 , epochs = 80 , Test error rate = 25.309741401672362</p><p class="source-code">batch_size = 50 , epochs = 100 , Test error rate = 25.095393986701964</p><p class="source-code">batch_size = 25 , epochs = 100 , Test error rate = 25.24592453837395</p><p>The <strong class="source-inline">batch_size=5</strong> and <strong class="source-inline">epochs=100</strong> pair has the lowest test error rate.</p></li>
				<li>Choose the model with the highest accuracy score and repeat <em class="italic">step 2</em> by iterating over <strong class="source-inline">optimizers = ['rmsprop', 'sgd', 'adam']</strong> and performing <strong class="source-inline">5-fold cross-validation</strong>:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># determine the number of folds for k-fold cross-validation</p><p class="source-code">n_folds = 5</p><p class="source-code"># define the list to store cross-validation scores</p><p class="source-code">results_3 = []</p><p class="source-code"># define the possible options for the optimizer</p><p class="source-code">optimizers = ['adam', 'sgd', 'rmsprop']</p><p class="source-code"># loop over optimizers</p><p class="source-code">for i in range(len(optimizers)):</p><p class="source-code">    optimizer=optimizers[i]</p><p class="source-code">    # build the scikit-learn interface for the keras model</p><p class="source-code">    regressor = KerasRegressor(build_fn=build_model_2, \</p><p class="source-code">                               epochs=100, batch_size=50, \</p><p class="source-code">                               verbose=0, shuffle=False)</p><p class="source-code">    """</p><p class="source-code">    build the pipeline of transformations so for each fold training </p><p class="source-code">    set will be scaled and test set will be scaled accordingly.</p><p class="source-code">    """</p><p class="source-code">    model = make_pipeline(StandardScaler(), regressor)</p><p class="source-code">    # define the cross-validation iterator</p><p class="source-code">    kfold = KFold(n_splits=n_folds, shuffle=True, \</p><p class="source-code">                  random_state=seed)</p><p class="source-code">    # perform the k-fold cross-validation. </p><p class="source-code">    # store the scores in result</p><p class="source-code">    result = cross_val_score(model, X, y, cv=kfold)</p><p class="source-code">    # add the scores to the results list </p><p class="source-code">    results_3.append(result)</p><p class="source-code"># Print cross-validation score for each optimizer</p><p class="source-code">for i in range(len(optimizers)):</p><p class="source-code">    print("optimizer=", optimizers[i]," test error rate = ", \</p><p class="source-code">          abs(results_3[i].mean()))</p><p>Here's the expected output:</p><p class="source-code">optimizer= adam  test error rate =  25.391812739372256</p><p class="source-code">optimizer= sgd  test error rate =  25.140230269432067</p><p class="source-code">optimizer= rmsprop  test error rate =  25.217947859764102</p><p><strong class="source-inline">optimizer='sgd'</strong> has the lowest test error rate, so we should proceed with this particular model.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31TcYaD">https://packt.live/31TcYaD</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iq6iqb">https://packt.live/3iq6iqb</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor199"/>5. Improving Model Accuracy</h1>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/>Activity 5.01: Weight Regularization on an Avila Pattern Classifier</h2>
			<p>In this activity, you will build a Keras model to perform classification on the Avila pattern dataset according to given network architecture and hyperparameter values. The goal is to apply different types of weight regularization on the model, that is, <strong class="source-inline">L1</strong> and <strong class="source-inline">L2</strong>, and observe how each type changes the result. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Load the dataset and split the dataset into a <strong class="source-inline">training set</strong> and a <strong class="source-inline">test set</strong>:<p class="source-code"># Load the dataset</p><p class="source-code">import pandas as pd</p><p class="source-code">X = pd.read_csv('../data/avila-tr_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/avila-tr_target.csv')</p><p class="source-code">"""</p><p class="source-code">Split the dataset into training set and test set with a 0.8-0.2 ratio</p><p class="source-code">"""</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed = 1</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, y, test_size=0.2, random_state=seed)</p></li>
				<li>Define a Keras sequential model with three hidden layers, the first of <strong class="source-inline">size 10</strong>, the second of <strong class="source-inline">size 6</strong>, and the third of <strong class="source-inline">size 4</strong>. Finally, compile the model:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">model_1 = Sequential()</p><p class="source-code">model_1.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu'))</p><p class="source-code">model_1.add(Dense(6, activation='relu'))</p><p class="source-code">model_1.add(Dense(4, activation='relu'))</p><p class="source-code">model_1.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_1.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p></li>
				<li>Fit the model to the training data to perform the classification, saving the results of the training process:<p class="source-code">history=model_1.fit(X_train, y_train, batch_size = 20, epochs = 100, \</p><p class="source-code">                    validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p></li>
				<li>Plot the trends in training error and test error by importing the necessary libraries for plotting the loss and validation loss and saving them in the variable that was created when the model was fit to the training process. Print out the maximum validation accuracy:<p class="source-code">import matplotlib.pyplot as plt </p><p class="source-code">import matplotlib</p><p class="source-code">%matplotlib inline </p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) </p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">      max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer227" class="IMG---Figure"><img src="image/B15777_05_13.jpg" alt="Figure 5.13: A plot of the training error and validation error during training &#13;&#10;for the model without regularization&#13;&#10;"/></div><p class="figure-caption">Figure 5.13: A plot of the training error and validation error during training for the model without regularization</p><p>The validation loss keeps decreasing along with the training loss. Despite having no regularization, this is a fairly good example of the training process since the bias and variance are fairly low.</p></li>
				<li>Redefine the model, adding <strong class="source-inline">L2 regularizers</strong> with <strong class="source-inline">lambda=0.01</strong> to each hidden layer of the model. Repeat <em class="italic">steps 3</em> and <em class="italic">4</em> to train the model and plot the <strong class="source-inline">training error</strong> and <strong class="source-inline">validation error</strong>:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model with l2 regularization with lambda = 0.01</p><p class="source-code">from keras.regularizers import l2</p><p class="source-code">l2_param = 0.01</p><p class="source-code">model_2 = Sequential()</p><p class="source-code">model_2.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_2.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_2.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_2.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_2.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_2.fit(X_train, y_train, batch_size = 20, epochs = 100, \</p><p class="source-code">                    validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">      max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer228" class="IMG---Figure"><img src="image/B15777_05_14.jpg" alt="Figure 5.14: A plot of the training error and validation error during training &#13;&#10;for the model with L2 weight regularization (lambda=0.01)&#13;&#10;"/></div><p class="figure-caption">Figure 5.14: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.01)</p><p>As shown from the preceding plots, the test error almost plateaus after being decreased to a certain amount. The gap between the training error and the validation error at the end of the training process (the bias) is slightly smaller, which is indicative of reduced overfitting of the model for the training examples.</p></li>
				<li>Repeat the previous step with <strong class="source-inline">lambda=0.1</strong> for the <strong class="source-inline">L2 parameter</strong>—redefine the model with the new lambda parameter, fit the model to the training data, and repeat <em class="italic">step 4</em> to plot the training error and validation error:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">from keras.regularizers import l2</p><p class="source-code">l2_param = 0.1</p><p class="source-code">model_3 = Sequential()</p><p class="source-code">model_3.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_3.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_3.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_3.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_3.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_3.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                    epochs = 100, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">      max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer229" class="IMG---Figure"><img src="image/B15777_05_15.jpg" alt="Figure 5.15: A plot of the training error and validation error during training &#13;&#10;for the model with L2 weight regularization (lambda=0.1)&#13;&#10;"/></div><p class="figure-caption">Figure 5.15: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.1)</p><p>The training and validation error quickly plateau and are much higher than they were for the models we created with a lower <strong class="source-inline">L2 parameter</strong>, indicating that we have penalized the model so much that it has not had the flexibility to learn the underlying function of the training data. Following this, we will reduce the value of the regularization parameter to prevent it from penalizing the model as much.</p></li>
				<li>Repeat the previous step, this time with <strong class="source-inline">lambda=0.005</strong>. Repeat <em class="italic">step 4</em> to plot the training error and validation error:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model with l2 regularization with lambda = 0.05</p><p class="source-code">from keras.regularizers import l2</p><p class="source-code">l2_param = 0.005</p><p class="source-code">model_4 = Sequential()</p><p class="source-code">model_4.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_4.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_4.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l2(l2_param)))</p><p class="source-code">model_4.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_4.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_4.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                    epochs = 100, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">      max(history.history['val_accuracy'])) </p><p>The following is the expected output:</p><div id="_idContainer230" class="IMG---Figure"><img src="image/B15777_05_16.jpg" alt="Figure 5.16: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.005)&#13;&#10;"/></div><p class="figure-caption">Figure 5.16: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.005)</p><p>The value for the <strong class="source-inline">L2 weight</strong> regularization achieves the highest accuracy that was evaluated on the validation data of all the models with <strong class="source-inline">L2 regularization</strong>, but it is slightly lower than without regularization. Again, the test error does not increase a significant amount after being decreased to a certain value, which is indicative of the model not overfitting the training examples. It seems that <strong class="source-inline">L2 weight regularization</strong> with <strong class="source-inline">lambda=0.005</strong> achieves the lowest validation error while preventing the model from overfitting.</p></li>
				<li>Add <strong class="source-inline">L1 regularizers</strong> with <strong class="source-inline">lambda=0.01</strong> to the hidden layers of your model. Redefine the model with the new lambda parameter, fit the model to the training data, and repeat <em class="italic">step 4</em> to plot the training error and validation error:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model with l1 regularization with lambda = 0.01</p><p class="source-code">from keras.regularizers import l1</p><p class="source-code">l1_param = 0.01</p><p class="source-code">model_5 = Sequential()</p><p class="source-code">model_5.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_5.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_5.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_5.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_5.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_5.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                    epochs = 100, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=True)</p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">      max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer231" class="IMG---Figure"><img src="image/B15777_05_17.jpg" alt="Figure 5.17: A plot of the training error and validation error during training &#13;&#10;for the model with L1 weight regularization (lambda=0.01)&#13;&#10;"/></div><p class="figure-caption">Figure 5.17: A plot of the training error and validation error during training for the model with L1 weight regularization (lambda=0.01)</p></li>
				<li>Repeat the previous step with <strong class="source-inline">lambda=0.005</strong> for the <strong class="source-inline">L1 parameter</strong>—redefine the model with the new lambda parameter, fit the model to the training data, and repeat <em class="italic">step 4</em> to plot the <strong class="source-inline">training error</strong> and <strong class="source-inline">validation error</strong>:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># define the keras model with l1 regularization with lambda = 0.1</p><p class="source-code">from keras.regularizers import l1</p><p class="source-code">l1_param = 0.005</p><p class="source-code">model_6 = Sequential()</p><p class="source-code">model_6.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_6.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_6.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1(l1_param)))</p><p class="source-code">model_6.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_6.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_6.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                    epochs = 100, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) </p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">       max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer232" class="IMG---Figure"><img src="image/B15777_05_18.jpg" alt="Figure 5.18: The plot of the training error and validation error during training for the model with L1 weight regularization (lambda=0.005)&#13;&#10;"/></div><p class="figure-caption">Figure 5.18: The plot of the training error and validation error during training for the model with L1 weight regularization (lambda=0.005)</p><p>It seems that <strong class="source-inline">L1 weight regularization</strong> with <strong class="source-inline">lambda=0.005</strong> achieves a better test error while preventing the model from overfitting since the value of <strong class="source-inline">lambda=0.01</strong> is too restrictive and prevents the model from learning the underlying function of the training data.</p></li>
				<li>Add <strong class="source-inline">L1</strong> and <strong class="source-inline">L2 regularizers</strong> with an <strong class="source-inline">L1</strong> of <strong class="source-inline">lambda=0.005</strong> and an <strong class="source-inline">L2</strong> of <strong class="source-inline">lambda = 0.005</strong> to the hidden layers of your model. Then, repeat <em class="italic">step 4</em> to plot the training error and validation error:<p class="source-code">"""</p><p class="source-code">set up a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">"""</p><p class="source-code">define the keras model with l1_l2 regularization with l1_lambda = 0.005 and l2_lambda = 0.005</p><p class="source-code">"""</p><p class="source-code">from keras.regularizers import l1_l2</p><p class="source-code">l1_param = 0.005</p><p class="source-code">l2_param = 0.005</p><p class="source-code">model_7 = Sequential()</p><p class="source-code">model_7.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">            activation='relu', \</p><p class="source-code">            kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))</p><p class="source-code">model_7.add(Dense(6, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1_l2(l1=l1_param, \</p><p class="source-code">                                           l2=l2_param)))</p><p class="source-code">model_7.add(Dense(4, activation='relu', \</p><p class="source-code">                  kernel_regularizer=l1_l2(l1=l1_param, \</p><p class="source-code">                                           l2=l2_param)))</p><p class="source-code">model_7.add(Dense(1, activation='sigmoid'))</p><p class="source-code">model_7.compile(loss='binary_crossentropy', optimizer='sgd', \</p><p class="source-code">                metrics=['accuracy'])</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_7.fit(X_train, y_train, batch_size = 20, \</p><p class="source-code">                    epochs = 100, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=True)</p><p class="source-code"> </p><p class="source-code"># plot training error and test error</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim(0,1)</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Best Accuracy on Validation Set =", \</p><p class="source-code">       max(history.history['val_accuracy']))</p><p>The following is the expected output:</p><div id="_idContainer233" class="IMG---Figure"><img src="image/B15777_05_19.jpg" alt="Figure 5.19: A plot of the training error and validation error during training for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.19: A plot of the training error and validation error during training for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005</p>
			<p>While <strong class="source-inline">L1</strong> and <strong class="source-inline">L2 regularization</strong> are successful in preventing the model from overfitting, the variance in the model is very low. However, the accuracy that's obtained on the validation data is not as high as the model that was trained with no regularization or the model that was trained with the <strong class="source-inline">L2 regularization</strong> <strong class="source-inline">lambda=0.005</strong>  or <strong class="source-inline">L1 regularization</strong> <strong class="source-inline">lambda=0.005</strong> parameters individually.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31BUf34">https://packt.live/31BUf34</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38n291s">https://packt.live/38n291s</a>.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor201"/>Activity 5.02: Dropout Regularization on the Traffic Volume Dataset</h2>
			<p>In this activity, you will start with the model from<em class="italic"> Activity 4.03</em>, <em class="italic">Model Selection Using Cross-Validation on a Traffic Volume Dataset</em>, of <em class="italic">Chapter 4</em>, <em class="italic">Evaluating Your Model with Cross-Validation Using Keras Wrappers</em>. You will use the training set/test set approach to train and evaluate the model, plot the trends in training error and the generalization error, and observe the model overfitting the data examples. Then, you will attempt to improve model performance by addressing the overfitting issue through the use of dropout regularization. In particular, you will try to find out which layers you should add dropout regularization to and what <strong class="source-inline">rate</strong> value will improve this specific model the most. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Load the dataset using the pandas <strong class="source-inline">read_csv</strong> function, split the dataset into a training set and test set into an <strong class="source-inline">80-20</strong> ratio using <strong class="source-inline">train_test_split</strong>, and scale the input data using <strong class="source-inline">StandardScaler</strong>:<p class="source-code"># Load the dataset</p><p class="source-code">import pandas as pd</p><p class="source-code">X = pd.read_csv('../data/traffic_volume_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/traffic_volume_target.csv')</p><p class="source-code">"""</p><p class="source-code">Split the dataset into training set and test set with an 80-20 ratio</p><p class="source-code">"""</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed=1</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, y, test_size=0.2, random_state=seed)</p></li>
				<li>Set a seed so that the model can be reproduced. Next, define a Keras sequential model with two hidden layers of <strong class="source-inline">size 10</strong>, both with <strong class="source-inline">ReLU activation</strong> functions. Add an output layer with no activation function and compile the model with the given hyperparameters:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code"># create model</p><p class="source-code">model_1 = Sequential()</p><p class="source-code">model_1.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu'))</p><p class="source-code">model_1.add(Dense(10, activation='relu'))</p><p class="source-code">model_1.add(Dense(1))</p><p class="source-code"># Compile model</p><p class="source-code">model_1.compile(loss='mean_squared_error', optimizer='rmsprop')</p></li>
				<li>Train the model on the training data with the given hyperparameters:<p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_1.fit(X_train, y_train, batch_size = 50, \</p><p class="source-code">                    epochs = 200, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0) </p></li>
				<li>Plot the trends for the <strong class="source-inline">training error</strong> and <strong class="source-inline">test error</strong>. Print the best accuracy that was reached for the training and validation set:<p class="source-code">import matplotlib.pyplot as plt </p><p class="source-code">import matplotlib</p><p class="source-code">%matplotlib inline </p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) </p><p class="source-code"># plot training error and test error plots </p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim((0, 25000))</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Lowest error on training set = ", \</p><p class="source-code">      min(history.history['loss']))</p><p class="source-code">print("Lowest error on validation set = ", \</p><p class="source-code">      min(history.history['val_loss']))</p><p>The following is the expected output:</p><p class="source-code">Lowest error on training set =  24.673954981565476</p><p class="source-code">Lowest error on validation set =  25.11553382873535</p><div id="_idContainer234" class="IMG---Figure"><img src="image/B15777_05_20.jpg" alt="Figure 5.20: A plot of the training error and validation error during training &#13;&#10;for the model without regularization&#13;&#10;"/></div><p class="figure-caption">Figure 5.20: A plot of the training error and validation error during training for the model without regularization</p><p>In the training error and validation error values, there is a very small gap between the training error and validation error, which is indicative of a low variance model, which is good.</p></li>
				<li>Redefine the model by creating the same model architecture. However, this time, add a dropout regularization with <strong class="source-inline">rate=0.1</strong> to the first hidden layer of your model. Repeat <em class="italic">step 3</em> to train the model on the training data and repeat <em class="italic">step 4</em> to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code"># create model</p><p class="source-code">model_2 = Sequential()</p><p class="source-code">model_2.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu'))</p><p class="source-code">model_2.add(Dropout(0.1))</p><p class="source-code">model_2.add(Dense(10, activation='relu'))</p><p class="source-code">model_2.add(Dense(1))</p><p class="source-code"># Compile model</p><p class="source-code">model_2.compile(loss='mean_squared_error', \</p><p class="source-code">                optimizer='rmsprop')</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_2.fit(X_train, y_train, batch_size = 50, \</p><p class="source-code">                    epochs = 200, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim((0, 25000))</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Lowest error on training set = ", \</p><p class="source-code">      min(history.history['loss']))</p><p class="source-code">print("Lowest error on validation set = ", \</p><p class="source-code">      min(history.history['val_loss']))</p><p>The following is the expected output:</p><p class="source-code">Lowest error on training set =  407.8203821182251</p><p class="source-code">Lowest error on validation set =  54.58488750457764</p><div id="_idContainer235" class="IMG---Figure"><img src="image/B15777_05_21.jpg" alt="Figure 5.21: A plot of the training error and validation error during training for the model with dropout regularization (rate=0.1) in the first layer&#13;&#10;"/></div><p class="figure-caption">Figure 5.21: A plot of the training error and validation error during training for the model with dropout regularization (rate=0.1) in the first layer</p><p>There is a small gap between the training error and the validation error; however, the validation error is lower than the training error, indicating that the model is not overfitting the training data.</p></li>
				<li>Repeat the previous step, this time adding dropout regularization with <strong class="source-inline">rate=0.1</strong> to both hidden layers of your model. Repeat <em class="italic">step 3</em> to train the model on the training data and repeat <em class="italic">step 4</em> to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create model</p><p class="source-code">model_3 = Sequential()</p><p class="source-code">model_3.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu'))</p><p class="source-code">model_3.add(Dropout(0.1))</p><p class="source-code">model_3.add(Dense(10, activation='relu'))</p><p class="source-code">model_3.add(Dropout(0.1))</p><p class="source-code">model_3.add(Dense(1))</p><p class="source-code"># Compile model</p><p class="source-code">model_3.compile(loss='mean_squared_error', \</p><p class="source-code">                optimizer='rmsprop')</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_3.fit(X_train, y_train, batch_size = 50, \</p><p class="source-code">                    epochs = 200, validation_data=(X_test, y_test), \</p><p class="source-code">                    verbose=0, shuffle=False)</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim((0, 25000))</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Lowest error on training set = ", \</p><p class="source-code">      min(history.history['loss']))</p><p class="source-code">print("Lowest error on validation set = ", \</p><p class="source-code">      min(history.history['val_loss']))</p><p>The following is the expected output:</p><p class="source-code">Lowest error on training set =  475.9299939632416</p><p class="source-code">Lowest error on validation set =  61.646054649353026</p><div id="_idContainer236" class="IMG---Figure"><img src="image/B15777_05_22.jpg" alt="Figure 5.22: A plot of the training error and validation error during training for &#13;&#10;the model with dropout regularization (rate=0.1) in both layers&#13;&#10;"/></div><p class="figure-caption">Figure 5.22: A plot of the training error and validation error during training for the model with dropout regularization (rate=0.1) in both layers</p><p>The gap between the training error and validation error is slightly higher here, mostly due to the increase in the training error as a result of the additional regularization on the second hidden layer of the model.</p></li>
				<li>Repeat the previous step, this time adding dropout regularization with <strong class="source-inline">rate=0.2</strong> in the first layer and <strong class="source-inline">rate=0.1</strong> in the second layer of your model. Repeat <em class="italic">step 3</em> to train the model on the training data and repeat <em class="italic">step 4</em> to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create model</p><p class="source-code">model_4 = Sequential()</p><p class="source-code">model_4.add(Dense(10, input_dim=X_train.shape[1], \</p><p class="source-code">                  activation='relu'))</p><p class="source-code">model_4.add(Dropout(0.2))</p><p class="source-code">model_4.add(Dense(10, activation='relu'))</p><p class="source-code">model_4.add(Dropout(0.1))</p><p class="source-code">model_4.add(Dense(1))</p><p class="source-code"># Compile model</p><p class="source-code">model_4.compile(loss='mean_squared_error', optimizer='rmsprop')</p><p class="source-code"># train the model using training set while evaluating on test set</p><p class="source-code">history=model_4.fit(X_train, y_train, batch_size = 50, epochs = 200, \</p><p class="source-code">                    validation_data=(X_test, y_test), verbose=0)</p><p class="source-code">matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.ylim((0, 25000))</p><p class="source-code">plt.ylabel('loss')</p><p class="source-code">plt.xlabel('epoch')</p><p class="source-code">plt.legend(['train loss', 'validation loss'], loc='upper right')</p><p class="source-code"># print the best accuracy reached on the test set</p><p class="source-code">print("Lowest error on training set = ", \</p><p class="source-code">      min(history.history['loss']))</p><p class="source-code">print("Lowest error on validation set = ", \</p><p class="source-code">      min(history.history['val_loss']))</p><p>The following is the expected output:</p><p class="source-code">Lowest error on training set =  935.1562484741211</p><p class="source-code">Lowest error on validation set =  132.39965686798095</p><div id="_idContainer237" class="IMG---Figure"><img src="image/B15777_05_23.jpg" alt="Figure 5.23: A plot of training errors and validation errors while training the model with dropout regularization, with rate=0.2 in the first layer and rate 0.1 in the second layer&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.23: A plot of training errors and validation errors while training the model with dropout regularization, with rate=0.2 in the first layer and rate 0.1 in the second layer</p>
			<p>The gap between the training error and validation error is slightly larger due to the increase in regularization. In this case, there was no overfitting in the original model. As a result, regularization increased the error rate on the training and validation dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38mtDo7">https://packt.live/38mtDo7</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31Isdmu">https://packt.live/31Isdmu</a>.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/>Activity 5.03: Hyperparameter Tuning on the Avila Pattern Classifier</h2>
			<p>In this activity, you will build a Keras model similar to those in the previous activities, but this time, you will add regularization methods to your model as well. Then, you will use scikit-learn optimizers to perform tuning on the model hyperparameters, including the hyperparameters of the regularizers. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Load the dataset and import the libraries:<p class="source-code"># Load The dataset</p><p class="source-code">import pandas as pd</p><p class="source-code">X = pd.read_csv('../data/avila-tr_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/avila-tr_target.csv')</p></li>
				<li>Define a function that returns a Keras model with three hidden layers, the first of <strong class="source-inline">size 10</strong>, the second of <strong class="source-inline">size 6</strong>, and the third of <strong class="source-inline">size 4</strong>, and apply <strong class="source-inline">L2 weight regularization</strong> and a <strong class="source-inline">ReLU activation</strong> function on each hidden layer. Compile the model with the given parameters and return it from the model:<p class="source-code"># Create the function that returns the keras model</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.regularizers import l2</p><p class="source-code">def build_model(lambda_parameter):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(10, input_dim=X.shape[1], \</p><p class="source-code">                    activation='relu', \</p><p class="source-code">                    kernel_regularizer=l2(lambda_parameter)))</p><p class="source-code">    model.add(Dense(6, activation='relu', \</p><p class="source-code">                    kernel_regularizer=l2(lambda_parameter)))</p><p class="source-code">    model.add(Dense(4, activation='relu', \</p><p class="source-code">                    kernel_regularizer=l2(lambda_parameter)))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer='sgd', metrics=['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Set a seed, use a scikit-learn wrapper to wrap the model that we created in the previous step, and define the hyperparameters to scan. Finally, perform <strong class="source-inline">GridSearchCV()</strong> on the model using the hyperparameter's grid and fit the model:<p class="source-code">from keras.wrappers.scikit_learn import KerasClassifier</p><p class="source-code">from sklearn.model_selection import GridSearchCV</p><p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create the Keras wrapper with scikit learn</p><p class="source-code">model = KerasClassifier(build_fn=build_model, verbose=0, \</p><p class="source-code">                        shuffle=False)</p><p class="source-code"># define all the possible values for each hyperparameter</p><p class="source-code">lambda_parameter = [0.01, 0.5, 1]</p><p class="source-code">epochs = [50, 100]</p><p class="source-code">batch_size = [20]</p><p class="source-code">"""</p><p class="source-code">create the dictionary containing all possible values of hyperparameters</p><p class="source-code">"""</p><p class="source-code">param_grid = dict(lambda_parameter=lambda_parameter, \</p><p class="source-code">                  epochs=epochs, batch_size=batch_size)</p><p class="source-code"># perform 5-fold cross-validation for ??????? store the results</p><p class="source-code">grid_seach = GridSearchCV(estimator=model, \</p><p class="source-code">                          param_grid=param_grid, cv=5)</p><p class="source-code">results_1 = grid_seach.fit(X, y)</p></li>
				<li>Print the results for the best cross-validation score that's stored within the variable we created in the fit process. Iterate through all the parameters and print the mean of the accuracy across all the folds, the standard deviation of the accuracy, and the parameters themselves:<p class="source-code">print("Best cross-validation score =", results_1.best_score_)</p><p class="source-code">print("Parameters for Best cross-validation score=", \</p><p class="source-code">      results_1.best_params_)</p><p class="source-code"># print the results for all evaluated hyperparameter combinations</p><p class="source-code">accuracy_means = results_1.cv_results_['mean_test_score']</p><p class="source-code">accuracy_stds = results_1.cv_results_['std_test_score']</p><p class="source-code">parameters = results_1.cv_results_['params']</p><p class="source-code">for p in range(len(parameters)):</p><p class="source-code">    print("Accuracy %f (std %f) for params %r" % \</p><p class="source-code">          (accuracy_means[p], accuracy_stds[p], parameters[p]))</p><p>The following is the expected output:</p><p class="source-code">Best cross-validation score = 0.7673058390617371</p><p class="source-code">Parameters for Best cross-validation score= {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.01}</p><p class="source-code">Accuracy 0.764621 (std 0.004330) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'lambda_parameter': 0.01}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'lambda_parameter': 0.5}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'lambda_parameter': 1}</p><p class="source-code">Accuracy 0.767306 (std 0.015872) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.01}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.5}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 1}</p></li>
				<li>Repeat <em class="italic">step 3</em> using <strong class="source-inline">GridSearchCV()</strong>, <strong class="source-inline">lambda_parameter = [0.001, 0.01, 0.05, 0.1]</strong>, <strong class="source-inline">batch_size = [20]</strong>, and <strong class="source-inline">epochs = [100]</strong>. Fit the model to the training data using <strong class="source-inline">5-fold cross-validation</strong> and print the results for the entire grid:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create the Keras wrapper with scikit learn</p><p class="source-code">model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)</p><p class="source-code"># define all the possible values for each hyperparameter</p><p class="source-code">lambda_parameter = [0.001, 0.01, 0.05, 0.1]</p><p class="source-code">epochs = [100]</p><p class="source-code">batch_size = [20]</p><p class="source-code">"""</p><p class="source-code">create the dictionary containing all possible values of hyperparameters</p><p class="source-code">"""</p><p class="source-code">param_grid = dict(lambda_parameter=lambda_parameter, \</p><p class="source-code">                  epochs=epochs, batch_size=batch_size)</p><p class="source-code">"""</p><p class="source-code">search the grid, perform 5-fold cross-validation for each possible combination, store the results</p><p class="source-code">"""</p><p class="source-code">grid_seach = GridSearchCV(estimator=model, \</p><p class="source-code">                          param_grid=param_grid, cv=5)</p><p class="source-code">results_2 = grid_seach.fit(X, y)</p><p class="source-code"># print the results for best cross-validation score</p><p class="source-code">print("Best cross-validation score =", results_2.best_score_)</p><p class="source-code">print("Parameters for Best cross-validation score =", \</p><p class="source-code">      results_2.best_params_)</p><p class="source-code"># print the results for the entire grid</p><p class="source-code">accuracy_means = results_2.cv_results_['mean_test_score']</p><p class="source-code">accuracy_stds = results_2.cv_results_['std_test_score']</p><p class="source-code">parameters = results_2.cv_results_['params']</p><p class="source-code">for p in range(len(parameters)):</p><p class="source-code">    print("Accuracy %f (std %f) for params %r" % \</p><p class="source-code">          (accuracy_means[p], accuracy_stds[p], parameters[p]))</p><p>The following is the expected output:</p><p class="source-code">Best cross-validation score = 0.786385428905487</p><p class="source-code">Parameters for Best cross-validation score = {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.001}</p><p class="source-code">Accuracy 0.786385 (std 0.010177) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.001}</p><p class="source-code">Accuracy 0.693960 (std 0.084994) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.01}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.05}</p><p class="source-code">Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'lambda_parameter': 0.1}</p></li>
				<li>Redefine a function that returns a Keras model with three hidden layers, the first of <strong class="source-inline">size 10</strong>, the second of <strong class="source-inline">size 6</strong>, and the third of <strong class="source-inline">size 4</strong>, and apply <strong class="source-inline">dropout regularization</strong> and a <strong class="source-inline">ReLU activation</strong> function on each hidden layer. Compile the model with the given parameters and return it from the function:<p class="source-code"># Create the function that returns the keras model</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code">def build_model(rate):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(10, input_dim=X.shape[1], activation='relu'))</p><p class="source-code">    model.add(Dropout(rate))</p><p class="source-code">    model.add(Dense(6, activation='relu'))</p><p class="source-code">    model.add(Dropout(rate))</p><p class="source-code">    model.add(Dense(4, activation='relu'))</p><p class="source-code">    model.add(Dropout(rate))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer='sgd', metrics=['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Use <strong class="source-inline">rate = [0, 0.1, 0.2]</strong> and <strong class="source-inline">epochs = [50, 100]</strong> and perform <strong class="source-inline">GridSearchCV()</strong> on the model. Fit the model to the training data using <strong class="source-inline">5-fold cross-validation</strong> and print the results for the entire grid:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create the Keras wrapper with scikit learn</p><p class="source-code">model = KerasClassifier(build_fn=build_model, verbose=0,shuffle=False)</p><p class="source-code"># define all the possible values for each hyperparameter</p><p class="source-code">rate = [0, 0.1, 0.2]</p><p class="source-code">epochs = [50, 100]</p><p class="source-code">batch_size = [20]</p><p class="source-code">"""</p><p class="source-code">create the dictionary containing all possible values of hyperparameters</p><p class="source-code">"""</p><p class="source-code">param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)</p><p class="source-code">"""</p><p class="source-code">perform 5-fold cross-validation for 10 randomly selected combinations, store the results</p><p class="source-code">"""</p><p class="source-code">grid_seach = GridSearchCV(estimator=model, \</p><p class="source-code">                          param_grid=param_grid, cv=5)</p><p class="source-code">results_3 = grid_seach.fit(X, y)</p><p class="source-code"># print the results for best cross-validation score</p><p class="source-code">print("Best cross-validation score =", results_3.best_score_)</p><p class="source-code">print("Parameters for Best cross-validation score =", \</p><p class="source-code">      results_3.best_params_)</p><p class="source-code"># print the results for the entire grid</p><p class="source-code">accuracy_means = results_3.cv_results_['mean_test_score']</p><p class="source-code">accuracy_stds = results_3.cv_results_['std_test_score']</p><p class="source-code">parameters = results_3.cv_results_['params']</p><p class="source-code">for p in range(len(parameters)):</p><p class="source-code">    print("Accuracy %f (std %f) for params %r" % \</p><p class="source-code">          (accuracy_means[p], accuracy_stds[p], parameters[p]))</p><p>The following is the expected output:</p><p class="source-code">Best cross-validation score= 0.7918504476547241</p><p class="source-code">Parameters for Best cross-validation score= {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0}</p><p class="source-code">Accuracy 0.786769 (std 0.008255) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'rate': 0}</p><p class="source-code">Accuracy 0.764717 (std 0.007691) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'rate': 0.1}</p><p class="source-code">Accuracy 0.752637 (std 0.013546) for params {'batch_size': 20, </p><p class="source-code">'epochs': 50, 'rate': 0.2}</p><p class="source-code">Accuracy 0.791850 (std 0.008519) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0}</p><p class="source-code">Accuracy 0.779291 (std 0.009504) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.1}</p><p class="source-code">Accuracy 0.767306 (std 0.005773) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.2}</p></li>
				<li>Repeat <em class="italic">step 5</em> using <strong class="source-inline">rate = [0.0, 0.05, 0.1]</strong> and <strong class="source-inline">epochs = [100]</strong>. Fit the model to the training data using <strong class="source-inline">5-fold cross-validation</strong> and print the results for the entire grid:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code"># create the Keras wrapper with scikit learn</p><p class="source-code">model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)</p><p class="source-code"># define all the possible values for each hyperparameter</p><p class="source-code">rate = [0.0, 0.05, 0.1]</p><p class="source-code">epochs = [100]</p><p class="source-code">batch_size = [20]</p><p class="source-code">"""</p><p class="source-code">create the dictionary containing all possible values of hyperparameters</p><p class="source-code">"""</p><p class="source-code">param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)</p><p class="source-code">"""</p><p class="source-code">perform 5-fold cross-validation for 10 randomly selected combinations, store the results</p><p class="source-code">"""</p><p class="source-code">grid_seach = GridSearchCV(estimator=model, \</p><p class="source-code">                          param_grid=param_grid, cv=5)</p><p class="source-code">results_4 = grid_seach.fit(X, y)</p><p class="source-code"># print the results for best cross-validation score</p><p class="source-code">print("Best cross-validation score =", results_4.best_score_)</p><p class="source-code">print("Parameters for Best cross-validation score =", \</p><p class="source-code">      results_4.best_params_)</p><p class="source-code"># print the results for the entire grid</p><p class="source-code">accuracy_means = results_4.cv_results_['mean_test_score']</p><p class="source-code">accuracy_stds = results_4.cv_results_['std_test_score']</p><p class="source-code">parameters = results_4.cv_results_['params']</p><p class="source-code">for p in range(len(parameters)):</p><p class="source-code">    print("Accuracy %f (std %f) for params %r" % \</p><p class="source-code">          (accuracy_means[p], accuracy_stds[p], parameters[p]))</p><p>The following is the expected output:</p><p class="source-code">Best cross-validation score= 0.7862895488739013</p><p class="source-code">Parameters for Best cross-validation score= {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.0}</p><p class="source-code">Accuracy 0.786290 (std 0.013557) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.0}</p><p class="source-code">Accuracy 0.786098 (std 0.005184) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.05}</p><p class="source-code">Accuracy 0.772004 (std 0.013733) for params {'batch_size': 20, </p><p class="source-code">'epochs': 100, 'rate': 0.1}</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2D7HN0L">https://packt.live/2D7HN0L</a>.</p><p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p></li>
			</ol>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor203"/>6. Model Evaluation</h1>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Activity 6.01: Computing the Accuracy and Null Accuracy of a Neural Network When We Change the Train/Test Split</h2>
			<p>In this activity, we will see that our <strong class="source-inline">null accuracy</strong> and <strong class="source-inline">accuracy</strong> will be affected by changing the <strong class="source-inline">train</strong>/<strong class="source-inline">test</strong> split. To implement this, the part of the code where the train/test split was defined has to be changed. We will use the same dataset that we used in <em class="italic">Exercise 6.02</em>, <em class="italic">Computing Accuracy and Null Accuracy with APS Failure for Scania Trucks Data</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries. Load the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first <strong class="source-inline">five</strong> rows of the dataset:<p class="source-code"># Import the libraries</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code"># Load the Data</p><p class="source-code">X = pd.read_csv("../data/aps_failure_training_feats.csv")</p><p class="source-code">y = pd.read_csv("../data/aps_failure_training_target.csv")</p><p class="source-code"># Use the head function to get a glimpse data</p><p class="source-code">X.head()</p><p>The following table shows the output of the preceding code:</p><div id="_idContainer238" class="IMG---Figure"><img src="image/B15777_06_13.jpg" alt="Figure 6.13: Initial five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.13: Initial five rows of the dataset</p></li>
				<li>Change the <strong class="source-inline">test_size</strong> and <strong class="source-inline">random_state</strong> from <strong class="source-inline">0.20</strong> to <strong class="source-inline">0.3</strong> and <strong class="source-inline">42</strong> to <strong class="source-inline">13</strong>, respectively:<p class="source-code"># Split the data into training and testing sets</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed = 13</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, y, test_size=0.3, random_state=seed)</p><p class="callout-heading">Note</p><p class="callout">If you use a different <strong class="source-inline">random_state</strong>, you may get a different <strong class="source-inline">train</strong>/<strong class="source-inline">test</strong> split, which may yield slightly different final results.</p></li>
				<li>Scale the data using the <strong class="source-inline">StandardScaler</strong> function and use the scaler to scale the test data. Convert both into pandas DataFrames:<p class="source-code"># Initialize StandardScaler</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code"># Transform the training data</p><p class="source-code">X_train = sc.fit_transform(X_train)</p><p class="source-code">X_train = pd.DataFrame(X_train, columns=X_test.columns)</p><p class="source-code"># Transform the testing data</p><p class="source-code">X_test = sc.transform(X_test)</p><p class="source-code">X_test = pd.DataFrame(X_test, columns = X_train.columns)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">sc.fit_transform()</strong> function transforms the data, and the data is also converted into a <strong class="source-inline">NumPy</strong> array. We may need the data later for analysis as a DataFrame object, so the <strong class="source-inline">pd.DataFrame()</strong> function reconverts data into a DataFrame.</p></li>
				<li>Import the libraries that are required to build a neural network architecture:<p class="source-code"># Import the relevant Keras libraries</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code">from tensorflow import random</p></li>
				<li>Initiate the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the Model with Sequential Class</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add five <strong class="source-inline">Dense</strong> layers to the network with <strong class="source-inline">Dropout</strong>. Set the first hidden layer so that it has a size of <strong class="source-inline">64</strong> with a dropout rate of <strong class="source-inline">0.5</strong>, the second hidden layer so that it has a size of <strong class="source-inline">32</strong> with a dropout rate of <strong class="source-inline">0.4</strong>, the third hidden layer so that it has a size of <strong class="source-inline">16</strong> with a dropout rate of <strong class="source-inline">0.3</strong>, the fourth hidden layer so that it has a size of <strong class="source-inline">8</strong> with a dropout rate of <strong class="source-inline">0.2</strong>, and the final hidden layer so that it has a size of <strong class="source-inline">4</strong> with a dropout rate of <strong class="source-inline">0.1</strong>. Set all the activation functions to <strong class="source-inline">ReLU</strong>:<p class="source-code"># Add the hidden dense layers and with dropout Layer</p><p class="source-code">model.add(Dense(units=64, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.5))</p><p class="source-code">model.add(Dense(units=32, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.4))</p><p class="source-code">model.add(Dense(units=16, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.3))</p><p class="source-code">model.add(Dense(units=8, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.2))</p><p class="source-code">model.add(Dense(units=4, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.1))</p></li>
				<li>Add an output <strong class="source-inline">Dense</strong> layer with a <strong class="source-inline">sigmoid</strong> activation function:<p class="source-code"># Add Output Dense Layer</p><p class="source-code">model.add(Dense(units=1, activation='sigmoid', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="callout-heading">Note</p><p class="callout">Since the output is binary, we are using the <strong class="source-inline">sigmoid</strong> function. If the output is multiclass (that is, more than two classes), then the <strong class="source-inline">softmax</strong> function should be used.</p></li>
				<li>Compile the network and fit the model. The metric that's being used here is <strong class="source-inline">accuracy</strong>:<p class="source-code"># Compile the Model</p><p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p><p class="callout-heading">Note</p><p class="callout">The metric name, which in our case is <strong class="source-inline">accuracy</strong>, is defined in the preceding code.</p></li>
				<li>Fit the model with <strong class="source-inline">100</strong> epochs, a batch size of <strong class="source-inline">20</strong>, and a validation split of <strong class="source-inline">0.2</strong>:<p class="source-code"># Fit the Model</p><p class="source-code">model.fit(X_train, y_train, epochs=100, batch_size=20, \</p><p class="source-code">          verbose=1, validation_split=0.2, shuffle=False)</p></li>
				<li>Evaluate the model on the test dataset and print out the values for the <strong class="source-inline">loss</strong> and <strong class="source-inline">accuracy</strong>:<p class="source-code">test_loss, test_acc = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'The loss on the test set is {test_loss:.4f} and \</p><p class="source-code">the accuracy is {test_acc*100:.4f}%')</p><p>The preceding code produces the following output:</p><p class="source-code">18000/18000 [==============================] - 0s 19us/step</p><p class="source-code">The loss on the test set is 0.0766 and the accuracy is 98.9833%</p><p>The model returns an accuracy of <strong class="source-inline">98.9833%</strong>. But is it good enough? We can only get the answer to this question by comparing it against the null accuracy.</p></li>
				<li>Now, compute the null accuracy. The <strong class="source-inline">null accuracy</strong> can be calculated using the <strong class="source-inline">value_count</strong> function of the <strong class="source-inline">pandas</strong> library, which we used in <em class="italic">Exercise 6.01</em>, <em class="italic">Calculating Null Accuracy on a Pacific Hurricanes Dataset</em>, of this chapter:<p class="source-code"># Use the value_count function to calculate distinct class values</p><p class="source-code">y_test['class'].value_counts()</p><p>The preceding code produces the following output:</p><p class="source-code">0    17700</p><p class="source-code">1      300</p><p class="source-code">Name: class, dtype: int64</p></li>
				<li>Calculate the <strong class="source-inline">null accuracy</strong>:<p class="source-code"># Calculate the null accuracy</p><p class="source-code">y_test['class'].value_counts(normalize=True).loc[0]</p><p>The preceding code produces the following output:</p><p class="source-code">0.9833333333333333</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eY7y1E">https://packt.live/3eY7y1E</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2BzBO4n">https://packt.live/2BzBO4n</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/>Activity 6.02: Calculating the ROC Curve and AUC Score</h2>
			<p>The <strong class="source-inline">ROC curve</strong> and <strong class="source-inline">AUC score</strong> is an effective way to easily evaluate the performance of a binary classifier. In this activity, we will plot the <strong class="source-inline">ROC curve</strong> and calculate the <strong class="source-inline">AUC score</strong> of a model. We will use the same dataset and train the same model that we used in <em class="italic">Exercise 6.03</em>, <em class="italic">Deriving and Computing Metrics Based on a Confusion Matrix</em>. Continue with the same APS failure data, plot the <strong class="source-inline">ROC curve</strong>, and compute the <strong class="source-inline">AUC score</strong> of the model. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the necessary libraries and load the data using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code"># Import the libraries</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code"># Load the Data</p><p class="source-code">X = pd.read_csv("../data/aps_failure_training_feats.csv")</p><p class="source-code">y = pd.read_csv("../data/aps_failure_training_target.csv")</p></li>
				<li>Split the data into training and test datasets using the <strong class="source-inline">train_test_split</strong> function:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed = 42</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, y, test_size=0.20, random_state=seed)</p></li>
				<li>Scale the feature data so that it has a <strong class="source-inline">mean</strong> of <strong class="source-inline">0</strong> and a <strong class="source-inline">standard deviation</strong> of <strong class="source-inline">1</strong> using the <strong class="source-inline">StandardScaler </strong>function. Fit the scaler in the <strong class="source-inline">training data</strong> and apply it to the <strong class="source-inline">test data</strong>:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code"># Transform the training data</p><p class="source-code">X_train = sc.fit_transform(X_train)</p><p class="source-code">X_train = pd.DataFrame(X_train,columns=X_test.columns)</p><p class="source-code"># Transform the testing data</p><p class="source-code">X_test = sc.transform(X_test)</p><p class="source-code">X_test = pd.DataFrame(X_test,columns=X_train.columns)</p></li>
				<li>Import the Keras libraries that are required for creating the model. Instantiate a Keras model of the <strong class="source-inline">Sequential</strong> class and add five hidden layers to the model, including dropout for each layer. The first hidden layer should have a size of <strong class="source-inline">64</strong> and a dropout rate of <strong class="source-inline">0.5</strong>. The second hidden layer should have a size of <strong class="source-inline">32</strong> and a dropout rate of <strong class="source-inline">0.4</strong>. The third hidden layer should have a size of <strong class="source-inline">16</strong> and a dropout rate of <strong class="source-inline">0.3</strong>. The fourth hidden layer should have a size of <strong class="source-inline">8</strong> and a dropout rate of <strong class="source-inline">0.2</strong>. The final hidden layer should have a size of <strong class="source-inline">4</strong> and a dropout rate of <strong class="source-inline">0.1</strong>. All the hidden layers should have <strong class="source-inline">ReLU activation</strong> functions and set <strong class="source-inline">kernel_initializer = 'uniform'</strong>. Add a final output layer to the model with a sigmoid activation function. Compile the model by calculating the accuracy metric during the training process:<p class="source-code"># Import the relevant Keras libraries</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p><p class="source-code"># Add the hidden dense layers with dropout Layer</p><p class="source-code">model.add(Dense(units=64, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.5))</p><p class="source-code">model.add(Dense(units=32, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.4))</p><p class="source-code">model.add(Dense(units=16, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.3))</p><p class="source-code">model.add(Dense(units=8, activation='relu', \</p><p class="source-code">          kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.2))</p><p class="source-code">model.add(Dense(units=4, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.1))</p><p class="source-code"># Add Output Dense Layer</p><p class="source-code">model.add(Dense(units=1, activation='sigmoid', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code"># Compile the Model</p><p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model to the training data by training for <strong class="source-inline">100</strong> epochs with <strong class="source-inline">batch_size=20</strong> and with <strong class="source-inline">validation_split=0.2</strong>:<p class="source-code">model.fit(X_train, y_train, epochs=100, batch_size=20, \</p><p class="source-code">          verbose=1, validation_split=0.2, shuffle=False)</p></li>
				<li>Once the model has finished fitting to the training data, create a variable that is the result of the model's prediction on the test data using the model's <strong class="source-inline">predict_proba</strong> methods:<p class="source-code">y_pred_prob = model.predict_proba(X_test)</p></li>
				<li>Import <strong class="source-inline">roc_curve</strong> from scikit-learn and run the following code:<p class="source-code">from sklearn.metrics import roc_curve</p><p class="source-code">fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)</p><p><strong class="source-inline">fpr</strong> = False positive rate (1 - specificity)</p><p><strong class="source-inline">tpr</strong> = True positive rate (sensitivity)</p><p><strong class="source-inline">thresholds</strong> = The threshold value of <strong class="source-inline">y_pred_prob</strong></p></li>
				<li>Run the following code to plot the <strong class="source-inline">ROC curve</strong> using <strong class="source-inline">matplotlib.pyplot</strong>:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">plt.plot(fpr, tpr)</p><p class="source-code">plt.title("ROC Curve for APS Failure")</p><p class="source-code">plt.xlabel("False Positive rate (1-Specificity)")</p><p class="source-code">plt.ylabel("True Positive rate (Sensitivity)")</p><p class="source-code">plt.grid(True)</p><p class="source-code">plt.show()</p><p>The following plot shows the output of the preceding code:</p><div id="_idContainer239" class="IMG---Figure"><img src="image/B15777_06_14.jpg" alt="Figure 6.14: ROC curve of the APS failure dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.14: ROC curve of the APS failure dataset</p></li>
				<li>Calculate the AUC score using the <strong class="source-inline">roc_auc_score</strong> function:<p class="source-code">from sklearn.metrics import roc_auc_score</p><p class="source-code">roc_auc_score(y_test,y_pred_prob)</p><p>The following is the output of the preceding code:</p><p class="source-code">0.944787151628455</p><p>The AUC score of <strong class="source-inline">94.4479%</strong> suggests that our model is excellent, as per the general acceptable <strong class="source-inline">AUC score</strong> shown above.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2NUOgyh">https://packt.live/2NUOgyh</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2As33NH">https://packt.live/2As33NH</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor206"/>7. Computer Vision with Convolutional Neural Networks</h1>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/>Activity 7.01: Amending Our Model with Multiple Layers and the Use of softmax</h2>
			<p>Let's try and improve the performance of our image classification algorithm. There are many ways to improve its performance, and one of the most straightforward ways is by adding multiple ANN layers to the model, which we will learn about in this activity. We will also change the activation from sigmoid to softmax. Then, we can compare the result with that of the previous exercise. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary Keras libraries and classes:<p class="source-code"># Import the Libraries </p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Conv2D, MaxPool2D, Flatten, Dense</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow import random</p></li>
				<li>Now, initiate the model with the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the classifier</p><p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">classifier=Sequential()</p></li>
				<li>Add the first layer of the CNN, set the input shape to <strong class="source-inline">(64, 64, 3)</strong>, the dimension of each image, and the activation function as a ReLU. Then, add <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">(3, 3)</strong>. Add two additional convolutional layers with <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">(3, 3)</strong>, also with <strong class="source-inline">ReLU activation</strong> functions:<p class="source-code">classifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),\</p><p class="source-code">               activation='relu'))</p><p class="source-code">classifier.add(Conv2D(32,(3,3),activation = 'relu'))</p><p class="source-code">classifier.add(Conv2D(32,(3,3),activation = 'relu'))</p><p><strong class="source-inline">32, (3, 3)</strong> means that there are <strong class="source-inline">32</strong> feature detectors of size <strong class="source-inline">3x3</strong>. As a good practice, always start with <strong class="source-inline">32</strong>; you can add <strong class="source-inline">64</strong> or <strong class="source-inline">128</strong> later.</p></li>
				<li>Now, add the pooling layer with an image size of <strong class="source-inline">2x2</strong>:<p class="source-code">classifier.add(MaxPool2D(pool_size=(2,2)))</p></li>
				<li>Flatten the output of the pooling layer by adding a flattening layer to the <strong class="source-inline">CNN model</strong>:<p class="source-code">classifier.add(Flatten())</p></li>
				<li>Add the first dense layer of the ANN. Here, <strong class="source-inline">128</strong> is the output of the number of nodes. As a good practice, <strong class="source-inline">128</strong> is good to get started. <strong class="source-inline">activation</strong> is <strong class="source-inline">relu</strong>. As a good practice, the power of two is preferred:<p class="source-code">classifier.add(Dense(units=128,activation='relu')) </p></li>
				<li>Add three more layers to the ANN of the same size, <strong class="source-inline">128</strong>, along with <strong class="source-inline">ReLU activation</strong> functions:<p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p><p class="source-code">classifier.add(Dense(128,activation='relu'))</p></li>
				<li>Add the output layer of the ANN. Replace the sigmoid function with <strong class="source-inline">softmax</strong>:<p class="source-code">classifier.add(Dense(units=1,activation='softmax')) </p></li>
				<li>Compile the network with an <strong class="source-inline">Adam optimizer</strong> and compute the accuracy during the training process:<p class="source-code"># Compile The network</p><p class="source-code">classifier.compile(optimizer='adam', loss='binary_crossentropy', \</p><p class="source-code">                   metrics=['accuracy'])</p></li>
				<li>Create training and test data generators. Rescale the training and test images by <strong class="source-inline">1/255</strong> so that all the values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Set these parameters for the training data generators only – <strong class="source-inline">shear_range=0.2</strong>, <strong class="source-inline">zoom_range=0.2</strong>, and <strong class="source-inline">horizontal_flip=True</strong>:<p class="source-code">from keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255, \</p><p class="source-code">                                   shear_range = 0.2, \</p><p class="source-code">                                   zoom_range = 0.2, \</p><p class="source-code">                                   horizontal_flip = True)</p><p class="source-code">test_datagen = ImageDataGenerator(rescale = 1./255)</p></li>
				<li>Create a training set from the <strong class="source-inline">training set</strong> folder. <strong class="source-inline">'../dataset/training_set'</strong> is the folder where our data has been placed. Our CNN model has an image size of <strong class="source-inline">64x64</strong>, so the same size should be passed here too. <strong class="source-inline">batch_size</strong> is the number of images in a single batch, which is <strong class="source-inline">32</strong>. <strong class="source-inline">class_mode</strong> is set to <strong class="source-inline">binary</strong> since we are working on binary classifiers:<p class="source-code">training_set = \</p><p class="source-code">train_datagen.flow_from_directory('../dataset/training_set', \</p><p class="source-code">                                  target_size = (64, 64), \</p><p class="source-code">                                  batch_size = 32, \</p><p class="source-code">                                  class_mode = 'binary')</p></li>
				<li>Repeat <em class="italic">step 6</em> for the test by setting the folder to the location of the test images, that is, <strong class="source-inline">'../dataset/test_set'</strong>:<p class="source-code">test_set = \</p><p class="source-code">test_datagen.flow_from_directory('../dataset/test_set', \</p><p class="source-code">                                 target_size = (64, 64), \</p><p class="source-code">                                 batch_size = 32, \</p><p class="source-code">                                 class_mode = 'binary')</p></li>
				<li>Finally, fit the data. Set the <strong class="source-inline">steps_per_epoch</strong> to <strong class="source-inline">10000</strong> and the <strong class="source-inline">validation_steps</strong> to <strong class="source-inline">2500</strong>. The following step might take some time to execute:<p class="source-code">classifier.fit_generator(training_set, steps_per_epoch = 10000, \</p><p class="source-code">                         epochs = 2, validation_data = test_set, \</p><p class="source-code">                         validation_steps = 2500, shuffle=False)</p><p>The preceding code produces the following output:</p><p class="source-code">Epoch 1/2</p><p class="source-code">10000/10000 [==============================] - 2452s 245ms/step - loss: 8.1783 - accuracy: 0.4667 - val_loss: 11.4999 - val_accuracy: 0.4695</p><p class="source-code">Epoch 2/2</p><p class="source-code">10000/10000 [==============================] - 2496s 250ms/step - loss: 8.1726 - accuracy: 0.4671 - val_loss: 10.5416 - val_accuracy: 0.4691</p><p>Note that the accuracy has decreased to <strong class="source-inline">46.91%</strong> due to the new softmax activation function.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gj0TiA">https://packt.live/3gj0TiA</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2VIDj7e">https://packt.live/2VIDj7e</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Activity 7.02: Classifying a New Image</h2>
			<p>In this activity, you will try to classify another new image, just like we did in the preceding exercise. The image hasn't been exposed to the algorithm, so we will use this activity to test our algorithm. You can run any of the algorithms in this chapter (although the one that gets the highest accuracy is preferred) and then use the model to classify your images. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Run one of the algorithms from this chapter.</li>
				<li>Load the image and process it. <strong class="source-inline">'test_image_2.jpg'</strong> is the path of the test image. Change the path in the code where you have saved the dataset:<p class="source-code">from keras.preprocessing import image</p><p class="source-code">new_image = \</p><p class="source-code">image.load_img('../test_image_2.jpg', target_size = (64, 64))</p><p class="source-code">new_image</p></li>
				<li>You can view the class labels using the following code:<p class="source-code">training_set.class_indices</p></li>
				<li>Process the image by converting it into a <strong class="source-inline">numpy</strong> array using the <strong class="source-inline">img_to_array</strong> function. Then, add an additional dimension along the 0<span class="superscript">th</span> axis using numpy's <strong class="source-inline">expand_dims</strong> function:<p class="source-code">new_image = image.img_to_array(new_image)</p><p class="source-code">new_image = np.expand_dims(new_image, axis = 0)</p></li>
				<li>Predict the new image by calling the <strong class="source-inline">predict</strong> method of the classifier:<p class="source-code">result = classifier.predict(new_image)</p></li>
				<li>Use the <strong class="source-inline">class_indices</strong> method with an <strong class="source-inline">if…else</strong> statement to map the 0 or 1 output of the prediction to a class label:<p class="source-code">if result[0][0] == 1:</p><p class="source-code">    prediction = 'It is a flower'</p><p class="source-code">else:</p><p class="source-code">    prediction = 'It is a car'</p><p class="source-code">print(prediction)</p><p>The preceding code produces the following output:</p><p class="source-code">It is a flower</p><p><strong class="source-inline">test_image_2</strong> is an image of a flower and was predicted to be a flower.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38ny95E">https://packt.live/38ny95E</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2VIM4Ow">https://packt.live/2VIM4Ow</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor209"/>8. Transfer Learning and Pre-Trained Models</h1>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor210"/>Activity 8.01: Using the VGG16 Network to Train a Deep Learning Network to Identify Images</h2>
			<p>Use the <strong class="source-inline">VGG16</strong> network to predict the image given (<strong class="source-inline">test_image_1</strong>). Before you start, ensure that you have downloaded the image (<strong class="source-inline">test_image_1</strong>) to your working directory. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary <strong class="source-inline">Keras</strong> libraries:<p class="source-code">import numpy as np</p><p class="source-code">from keras.applications.vgg16 import VGG16, preprocess_input</p><p class="source-code">from keras.preprocessing import image </p></li>
				<li>Initiate the model (note that, at this point, you can also view the architecture of the network, as shown in the following code):<p class="source-code">classifier = VGG16()</p><p class="source-code">classifier.summary()</p><p><strong class="source-inline">classifier.summary()</strong> shows us the architecture of the network. The following points should be noted: it has a four-dimensional input shape (<strong class="source-inline">None, 224, 224, 3</strong>) and it has three convolutional layers.</p><p>The last four layers of the output are as follows: </p><div id="_idContainer240" class="IMG---Figure"><img src="image/B15777_08_16.jpg" alt="Figure 8.16: The architecture of the network&#13;&#10;"/></div><p class="figure-caption">Figure 8.16: The architecture of the network</p></li>
				<li>Load the image. <strong class="source-inline">'../Data/Prediction/test_image_1.jpg'</strong> is the path of the image on our system. It will be different on your system:<p class="source-code">new_image = \</p><p class="source-code">image.load_img('../Data/Prediction/test_image_1.jpg', \</p><p class="source-code">               target_size=(224, 224))</p><p class="source-code">new_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer241" class="IMG---Figure"><img src="image/B15777_08_17.jpg" alt="Figure 8.17: The sample motorbike image&#13;&#10;"/></div><p class="figure-caption">Figure 8.17: The sample motorbike image</p><p>The target size should be <strong class="source-inline">224x 224</strong> since <strong class="source-inline">VGG16</strong> only accepts (<strong class="source-inline">224,224</strong>).</p></li>
				<li>Change the image into an array by using the <strong class="source-inline">img_to_array</strong> function:<p class="source-code">transformed_image = image.img_to_array(new_image)</p><p class="source-code">transformed_image.shape</p><p>The preceding code provides the following output:</p><p class="source-code">(224, 224, 3)</p></li>
				<li>The image should be in a four-dimensional form for <strong class="source-inline">VGG16</strong> to allow further processing. Expand the dimension of the image, as follows:<p class="source-code">transformed_image = np.expand_dims(transformed_image, axis=0)</p><p class="source-code">transformed_image.shape</p><p>The preceding code provides the following output:</p><p class="source-code">(1, 224, 224, 3)</p></li>
				<li>Preprocess the image:<p class="source-code">transformed_image = preprocess_input(transformed_image)</p><p class="source-code">transformed_image</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer242" class="IMG---Figure"><img src="image/B15777_08_18.jpg" alt="Figure 8.18: Image preprocessing&#13;&#10;"/></div><p class="figure-caption">Figure 8.18: Image preprocessing</p></li>
				<li>Create the <strong class="source-inline">predictor</strong> variable:<p class="source-code">y_pred = classifier.predict(transformed_image)</p><p class="source-code">y_pred</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer243" class="IMG---Figure"><img src="image/B15777_08_19.jpg" alt="Figure 8.19: Creating the predictor variable&#13;&#10;"/></div><p class="figure-caption">Figure 8.19: Creating the predictor variable</p></li>
				<li>Check the shape of the image. It should be (<strong class="source-inline">1,1000</strong>). It's <strong class="source-inline">1000</strong> because, as we mentioned previously, the ImageNet database has <strong class="source-inline">1000</strong> categories of images. The predictor variable shows the probabilities of our image being one of those images:<p class="source-code">y_pred.shape</p><p>The preceding code provides the following output:</p><p class="source-code">(1, 1000)</p></li>
				<li>Print the top five probabilities of what our image is using the <strong class="source-inline">decode_predictions</strong> function and pass the function of the predictor variable, <strong class="source-inline">y_pred</strong>, and the number of predictions and corresponding labels to output:<p class="source-code">from keras.applications.vgg16 import decode_predictions</p><p class="source-code">decode_predictions(y_pred, top=5)</p><p>The preceding code provides the following output:</p><p class="source-code">[[('n03785016', 'moped', 0.8433369),</p><p class="source-code">  ('n03791053', 'motor_scooter', 0.14188054),</p><p class="source-code">  ('n03127747', 'crash_helmet', 0.007004856),</p><p class="source-code">  ('n03208938', 'disk_brake', 0.0022349996),</p><p class="source-code">  ('n04482393', 'tricycle', 0.0007717237)]]</p><p>The first column of the array is an internal code number. The second is the label, while the third is the probability of the image being the label.</p></li>
				<li>Transform the predictions into a human-readable format. We need to extract the most probable label from the output, as follows:<p class="source-code">label = decode_predictions(y_pred)</p><p class="source-code">"""</p><p class="source-code">Most likely result is retrieved, for example, the highest probability</p><p class="source-code">"""</p><p class="source-code">decoded_label = label[0][0]</p><p class="source-code"># The classification is printed</p><p class="source-code">print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))</p><p>The preceding code provides the following output:</p><p class="source-code">moped (84.33%)</p><p>Here, we can see that we have an <strong class="source-inline">84.33%</strong> probability that the picture is of a moped, which is close enough to a motorbike and probably represents the fact that motorbikes in the ImageNet dataset were labeled as mopeds.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2C4nqRo">https://packt.live/2C4nqRo</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/31JMPL4">https://packt.live/31JMPL4</a>.</p></li>
			</ol>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Activity 8.02: Image Classification with ResNet </h2>
			<p>In this activity, we will use another pre-trained network, known as <strong class="source-inline">ResNet</strong>. We have an image of television located at <strong class="source-inline">../Data/Prediction/test_image_4</strong>. We will use the <strong class="source-inline">ResNet50</strong> network to predict the image. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library and the necessary <strong class="source-inline">Keras</strong> libraries:<p class="source-code">import numpy as np</p><p class="source-code">from keras.applications.resnet50 import ResNet50, preprocess_input</p><p class="source-code">from keras.preprocessing import image </p></li>
				<li>Initiate the ResNet50 model and print a summary of the model:<p class="source-code">classifier = ResNet50()</p><p class="source-code">classifier.summary()</p><p><strong class="source-inline">classifier.summary()</strong> shows us the architecture of the network. The following points should be noted:</p><div id="_idContainer244" class="IMG---Figure"><img src="image/B15777_08_20.jpg" alt="Figure 8.20: The last four layers of the output&#13;&#10;"/></div><p class="figure-caption">Figure 8.20: The last four layers of the output</p><p class="callout-heading">Note</p><p class="callout">The last layer predictions (<strong class="source-inline">Dense</strong>) have <strong class="source-inline">1000</strong> values. This means that <strong class="source-inline">VGG16</strong> has a total of <strong class="source-inline">1000</strong> labels and that our image will be one of those <strong class="source-inline">1000</strong> labels.</p></li>
				<li>Load the image. <strong class="source-inline">'../Data/Prediction/test_image_4.jpg'</strong> is the path of the image on our system. It will be different on your system:<p class="source-code">new_image = \</p><p class="source-code">image.load_img('../Data/Prediction/test_image_4.jpg', \</p><p class="source-code">               target_size=(224, 224))</p><p class="source-code">new_image</p><p>The following is the output of the preceding code:</p><p> </p><div id="_idContainer245" class="IMG---Figure"><img src="image/B15777_08_21.jpg" alt="Figure 8.21: A sample image of a television&#13;&#10;"/></div><p class="figure-caption">Figure 8.21: A sample image of a television</p><p>The target size should be <strong class="source-inline">224x224</strong> since <strong class="source-inline">ResNet50</strong> only accepts (<strong class="source-inline">224,224</strong>).</p></li>
				<li>Change the image into an array by using the <strong class="source-inline">img_to_array</strong> function:<p class="source-code">transformed_image = image.img_to_array(new_image)</p><p class="source-code">transformed_image.shape</p></li>
				<li>The image has to be in a four-dimensional form for <strong class="source-inline">ResNet50</strong> to allow further processing. Expand the dimensions of the image along the 0<span class="superscript">th</span> axis using the <strong class="source-inline">expand_dims</strong> function:<p class="source-code">transformed_image = np.expand_dims(transformed_image, axis=0)</p><p class="source-code">transformed_image.shape</p></li>
				<li>Preprocess the image using the <strong class="source-inline">preprocess_input</strong> function:<p class="source-code">transformed_image = preprocess_input(transformed_image)</p><p class="source-code">transformed_image</p></li>
				<li>Create the predictor variable by using the classifier to predict the image using it's <strong class="source-inline">predict</strong> method:<p class="source-code">y_pred = classifier.predict(transformed_image)</p><p class="source-code">y_pred</p></li>
				<li>Check the shape of the image. It should be (<strong class="source-inline">1,1000</strong>):<p class="source-code">y_pred.shape</p><p>The preceding code provides the following output:</p><p class="source-code">(1, 1000)</p></li>
				<li>Select the top five probabilities of what our image is using the <strong class="source-inline">decode_predictions</strong> function and by passing the predictor variable, <strong class="source-inline">y_pred</strong>, as the argument and the top number of predictions and corresponding labels:<p class="source-code">from keras.applications.resnet50 import decode_predictions</p><p class="source-code">decode_predictions(y_pred, top=5)</p><p>The preceding code provides the following output:</p><p class="source-code">[[('n04404412', 'television', 0.99673873),</p><p class="source-code">  ('n04372370', 'switch', 0.0009829825),</p><p class="source-code">  ('n04152593', 'screen', 0.00095111143),</p><p class="source-code">  ('n03782006', 'monitor', 0.0006477369),</p><p class="source-code">  ('n04069434', 'reflex_camera', 8.5398955e-05)]]</p><p>The first column of the array is an internal code number. The second is the label, while the third is the probability of the image matching the label.</p></li>
				<li>Put the predictions in a human-readable format. Print the most probable label from the output from the result of the <strong class="source-inline">decode_predictions</strong> function:<p class="source-code">label = decode_predictions(y_pred)</p><p class="source-code">"""</p><p class="source-code">Most likely result is retrieved, for example, </p><p class="source-code">the highest probability</p><p class="source-code">"""</p><p class="source-code">decoded_label = label[0][0]</p><p class="source-code"># The classification is printed </p><p class="source-code">print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))</p><p>The preceding code produces the following output:</p><p class="source-code">television (99.67%)</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38rEe0M">https://packt.live/38rEe0M</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2YV5xxo">https://packt.live/2YV5xxo</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor212"/>9. Sequential Modeling with Recurrent Neural Networks</h1>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Activity 9.01: Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</h2>
			<p>In this activity, we will examine the stock price of Amazon for the last 5 years—from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using an <strong class="source-inline">RNN</strong> and <strong class="source-inline">LSTM</strong>. We have the actual values for January 2019, so we can compare our predictions to the actual values later. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Import the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">dataset_training = pd.read_csv('../AMZN_train.csv')</p><p class="source-code">dataset_training.head()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer246" class="IMG---Figure"><img src="image/B15777_09_24.jpg" alt="Figure 9.24: The first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.24: The first five rows of the dataset</p></li>
				<li>We are going to make our prediction using the <strong class="source-inline">Open</strong> stock price; therefore, select the <strong class="source-inline">Open</strong> stock price column from the dataset and print the values:<p class="source-code">training_data = dataset_training[['Open']].values </p><p class="source-code">training_data</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 398.799988],</p><p class="source-code">       [ 398.290009],</p><p class="source-code">       [ 395.850006],</p><p class="source-code">       ...,</p><p class="source-code">       [1454.199951],</p><p class="source-code">       [1473.349976],</p><p class="source-code">       [1510.800049]])</p></li>
				<li>Then, perform feature scaling by normalizing the data using <strong class="source-inline">MinMaxScaler</strong> and setting the range of the features so that they have a minimum value of zero and a maximum value of one. Use the <strong class="source-inline">fit_transform</strong> method of the scaler on the training data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">sc = MinMaxScaler(feature_range = (0, 1))</p><p class="source-code">training_data_scaled = sc.fit_transform(training_data)</p><p class="source-code">training_data_scaled</p><p>The preceding code produces the following output:</p><p class="source-code">array([[0.06523313],</p><p class="source-code">       [0.06494233],</p><p class="source-code">       [0.06355099],</p><p class="source-code">       ...,</p><p class="source-code">       [0.66704299],</p><p class="source-code">       [0.67796271],</p><p class="source-code">       [0.69931748]])</p></li>
				<li>Create the data to get <strong class="source-inline">60</strong> timestamps from the current instance. We chose <strong class="source-inline">60</strong> here as it will give us a sufficient number of previous instances in order to understand the trend; technically, this can be any number, but <strong class="source-inline">60</strong> is the optimal value. Additionally, the upper bound value here is <strong class="source-inline">1258</strong>, which is the index or count of rows (or records) in the training set:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p><p class="source-code">for i in range(60, 1258):</p><p class="source-code">    X_train.append(training_data_scaled[i-60:i, 0])</p><p class="source-code">    y_train.append(training_data_scaled[i, 0])</p><p class="source-code">X_train, y_train = np.array(X_train), np.array(y_train)</p></li>
				<li>Reshape the data to add an extra dimension to the end of <strong class="source-inline">X_train</strong> using NumPy's <strong class="source-inline">reshape</strong> function:<p class="source-code">X_train = np.reshape(X_train, (X_train.shape[0], \</p><p class="source-code">                     X_train.shape[1], 1))</p></li>
				<li>Import the following libraries to build the RNN:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Set the seed and initiate the sequential model, as follows:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add an <strong class="source-inline">LSTM</strong> layer to the network with <strong class="source-inline">50</strong> units, set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong>, and set the <strong class="source-inline">input_shape</strong> argument to <strong class="source-inline">(X_train.shape[1], 1)</strong>. Add three additional <strong class="source-inline">LSTM</strong> layers, each with <strong class="source-inline">50</strong> units, and set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong> for the first two. Add a final output layer of size 1:<p class="source-code">model.add(LSTM(units = 50, return_sequences = True, \</p><p class="source-code">          input_shape = (X_train.shape[1], 1)))</p><p class="source-code"># Adding a second LSTM layer</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code"># Adding a third LSTM layer</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code"># Adding a fourth LSTM layer</p><p class="source-code">model.add(LSTM(units = 50))</p><p class="source-code"># Adding the output layer</p><p class="source-code">model.add(Dense(units = 1))</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and use <strong class="source-inline">Mean Squared Error</strong> for the loss. Fit the model to the training data for <strong class="source-inline">100</strong> epochs with a batch size of <strong class="source-inline">32</strong>:<p class="source-code"># Compiling the RNN</p><p class="source-code">model.compile(optimizer = 'adam', loss = 'mean_squared_error')</p><p class="source-code"># Fitting the RNN to the Training set</p><p class="source-code">model.fit(X_train, y_train, epochs = 100, batch_size = 32)</p></li>
				<li>Load and process the test data (which is treated as actual data here) and select the column representing the value of <strong class="source-inline">Open</strong> stock data:<p class="source-code">dataset_testing = pd.read_csv('../AMZN_test.csv')</p><p class="source-code">actual_stock_price = dataset_testing[['Open']].values</p><p class="source-code">actual_stock_price</p></li>
				<li>Concatenate the data since we will need <strong class="source-inline">60</strong> previous instances to get the stock price for each day. Therefore, we will need both the training and test data:<p class="source-code">total_data = pd.concat((dataset_training['Open'], \</p><p class="source-code">                        dataset_testing['Open']), axis = 0)</p></li>
				<li>Reshape and scale the input to prepare the test data. Note that we are predicting the January monthly trend, which has <strong class="source-inline">21</strong> financial days, so in order to prepare the test set, we take the lower bound value as <strong class="source-inline">60</strong> and the upper bound value as <strong class="source-inline">81</strong>. This ensures that the difference of <strong class="source-inline">21</strong> is maintained:<p class="source-code">inputs = total_data[len(total_data) \</p><p class="source-code">         - len(dataset_testing) - 60:].values</p><p class="source-code">inputs = inputs.reshape(-1,1)</p><p class="source-code">inputs = sc.transform(inputs)</p><p class="source-code">X_test = []</p><p class="source-code">for i in range(60, 81):</p><p class="source-code">    X_test.append(inputs[i-60:i, 0])</p><p class="source-code">X_test = np.array(X_test)</p><p class="source-code">X_test = np.reshape(X_test, (X_test.shape[0], \</p><p class="source-code">                             X_test.shape[1], 1))</p><p class="source-code">predicted_stock_price = model.predict(X_test)</p><p class="source-code">predicted_stock_price = \</p><p class="source-code">sc.inverse_transform(predicted_stock_price)</p></li>
				<li>Visualize the results by plotting the actual stock price and plotting the predicted stock price:<p class="source-code"># Visualizing the results</p><p class="source-code">plt.plot(actual_stock_price, color = 'green', \</p><p class="source-code">         label = 'Real Amazon Stock Price',ls='--')</p><p class="source-code">plt.plot(predicted_stock_price, color = 'red', \</p><p class="source-code">         label = 'Predicted Amazon Stock Price',ls='-')</p><p class="source-code">plt.title('Predicted Stock Price')</p><p class="source-code">plt.xlabel('Time in days')</p><p class="source-code">plt.ylabel('Real Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>Please note that your results may differ slightly from the actual stock price of Amazon.</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer247" class="IMG---Figure"><img src="image/B15777_09_25.jpg" alt="Figure 9.25: Real versus predicted stock prices&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.25: Real versus predicted stock prices</p>
			<p>As shown in the preceding plot, the trends of the predicted and real prices are pretty much the same; the line has the same peaks and troughs. This is possible because of LSTM's ability to remember sequenced data. A traditional feedforward neural network would not have been able to forecast this result. This is the true power of <strong class="source-inline">LSTM</strong> and <strong class="source-inline">RNNs</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3goQO3I">https://packt.live/3goQO3I</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2VIMq7O">https://packt.live/2VIMq7O</a>.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Activity 9.02: Predicting Amazon's Stock Price with Added Regularization</h2>
			<p>In this activity, we will examine the stock price of Amazon over the last 5 years, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using RNNs and an LSTM. We have the actual values for January 2019, so we will be able to compare our predictions with the actual values later. Initially, we predicted the trend of Amazon's stock price using an LSTM with 50 units (or neurons). In this activity, we will also add dropout regularization and compare the results with <em class="italic">Activity 9.01</em>, <em class="italic">Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Import the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">dataset_training = pd.read_csv('../AMZN_train.csv')</p><p class="source-code">dataset_training.head()</p></li>
				<li>We are going to make our prediction using the <strong class="source-inline">Open</strong> stock price; therefore, select the <strong class="source-inline">Open</strong> stock price column from the dataset and print the values:<p class="source-code">training_data = dataset_training[['Open']].values</p><p class="source-code">training_data</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 398.799988],</p><p class="source-code">       [ 398.290009],</p><p class="source-code">       [ 395.850006],</p><p class="source-code">       ...,</p><p class="source-code">       [1454.199951],</p><p class="source-code">       [1473.349976],</p><p class="source-code">       [1510.800049]])</p></li>
				<li>Then, perform feature scaling by normalizing the data using <strong class="source-inline">MinMaxScaler</strong> and setting the range of the features so that they have a minimum value of <strong class="source-inline">0</strong> and a maximum value of one. Use the <strong class="source-inline">fit_transform</strong> method of the scaler on the training data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">sc = MinMaxScaler(feature_range = (0, 1))</p><p class="source-code">training_data_scaled = sc.fit_transform(training_data)</p><p class="source-code">training_data_scaled</p><p>The preceding code produces the following output:</p><p class="source-code">array([[0.06523313],</p><p class="source-code">       [0.06494233],</p><p class="source-code">       [0.06355099],</p><p class="source-code">       ...,</p><p class="source-code">       [0.66704299],</p><p class="source-code">       [0.67796271],</p><p class="source-code">       [0.69931748]])</p></li>
				<li>Create the data to get <strong class="source-inline">60</strong> timestamps from the current instance. We chose <strong class="source-inline">60</strong> here as it will give us a sufficient number of previous instances in order to understand the trend; technically, this can be any number, but <strong class="source-inline">60</strong> is the optimal value. Additionally, the upper bound value here is <strong class="source-inline">1258</strong>, which is the index or count of rows (or records) in the training set:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p><p class="source-code">for i in range(60, 1258):</p><p class="source-code">    X_train.append(training_data_scaled[i-60:i, 0])</p><p class="source-code">    y_train.append(training_data_scaled[i, 0])</p><p class="source-code">X_train, y_train = np.array(X_train), np.array(y_train)</p></li>
				<li>Reshape the data to add an extra dimension to the end of <strong class="source-inline">X_train</strong> using NumPy's <strong class="source-inline">reshape</strong> function:<p class="source-code">X_train = np.reshape(X_train, (X_train.shape[0], \</p><p class="source-code">                               X_train.shape[1], 1))</p></li>
				<li>Import the following Keras libraries to build the RNN:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Set the seed and initiate the sequential model, as follows:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add an LSTM layer to the network with 50 units, set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong>, and set the <strong class="source-inline">input_shape</strong> argument to <strong class="source-inline">(X_train.shape[1], 1)</strong>. Add dropout to the model with <strong class="source-inline">rate=0.2</strong>. Add three additional LSTM layers, each with <strong class="source-inline">50</strong> units, and set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong> for the first two. After each <strong class="source-inline">LSTM</strong> layer, add a dropout with <strong class="source-inline">rate=0.2</strong>. Add a final output layer of size <strong class="source-inline">1</strong>:<p class="source-code">model.add(LSTM(units = 50, return_sequences = True, \</p><p class="source-code">               input_shape = (X_train.shape[1], 1)))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code"># Adding a second LSTM layer and some Dropout regularization</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code"># Adding a third LSTM layer and some Dropout regularization</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code"># Adding a fourth LSTM layer and some Dropout regularization</p><p class="source-code">model.add(LSTM(units = 50))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code"># Adding the output layer</p><p class="source-code">model.add(Dense(units = 1))</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and use <strong class="source-inline">Mean Squared Error</strong> for the loss. Fit the model to the training data for <strong class="source-inline">100</strong> epochs with a batch size of <strong class="source-inline">32</strong>:<p class="source-code"># Compiling the RNN</p><p class="source-code">model.compile(optimizer = 'adam', loss = 'mean_squared_error')</p><p class="source-code"># Fitting the RNN to the Training set</p><p class="source-code">model.fit(X_train, y_train, epochs = 100, batch_size = 32)</p></li>
				<li>Load and process the test data (which is treated as actual data here) and select the column representing the value of <strong class="source-inline">Open</strong> stock data:<p class="source-code">dataset_testing = pd.read_csv('../AMZN_test.csv')</p><p class="source-code">actual_stock_price = dataset_testing[['Open']].values</p><p class="source-code">actual_stock_price </p></li>
				<li>Concatenate the data since we will need <strong class="source-inline">60</strong> previous instances to get the stock price for each day. Therefore, we will need both the training and test data:<p class="source-code">total_data = pd.concat((dataset_training['Open'], \</p><p class="source-code">                        dataset_testing['Open']), axis = 0)</p></li>
				<li>Reshape and scale the input to prepare the test data. Note that we are predicting the January monthly trend, which has <strong class="source-inline">21</strong> financial days, so in order to prepare the test set, we take the lower bound value as <strong class="source-inline">60</strong> and the upper bound value as <strong class="source-inline">81</strong>. This ensures that the difference of <strong class="source-inline">21</strong> is maintained:<p class="source-code">inputs = total_data[len(total_data) \</p><p class="source-code">         - len(dataset_testing) - 60:].values</p><p class="source-code">inputs = inputs.reshape(-1,1)</p><p class="source-code">inputs = sc.transform(inputs)</p><p class="source-code">X_test = []</p><p class="source-code">for i in range(60, 81):</p><p class="source-code">    X_test.append(inputs[i-60:i, 0])</p><p class="source-code">X_test = np.array(X_test)</p><p class="source-code">X_test = np.reshape(X_test, (X_test.shape[0], \</p><p class="source-code">                             X_test.shape[1], 1))</p><p class="source-code">predicted_stock_price = model.predict(X_test)</p><p class="source-code">predicted_stock_price = \</p><p class="source-code">sc.inverse_transform(predicted_stock_price)</p></li>
				<li>Visualize the results by plotting the actual stock price and plotting the predicted stock price:<p class="source-code"># Visualizing the results</p><p class="source-code">plt.plot(actual_stock_price, color = 'green', \</p><p class="source-code">         label = 'Real Amazon Stock Price',ls='--')</p><p class="source-code">plt.plot(predicted_stock_price, color = 'red', \</p><p class="source-code">         label = 'Predicted Amazon Stock Price',ls='-')</p><p class="source-code">plt.title('Predicted Stock Price')</p><p class="source-code">plt.xlabel('Time in days')</p><p class="source-code">plt.ylabel('Real Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p></li>
			</ol>
			<p>Please note that your results may differ slightly to the actual stock price.</p>
			<p><strong class="bold">Expected output</strong>:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/B15777_09_26.jpg" alt="Figure 9.26: Real versus predicted stock prices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.26: Real versus predicted stock prices</p>
			<p>In the following figure, the first plot displays the predicted output of the model with regularization from Activity 9.02, and the second displays the predicted output without regularization from Activity 9.01. As you can see, adding dropout regularization does not fit the data as accurately. So, in this case, it is better not to use regularization, or to use dropout regularization with a lower dropout rate :</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="image/B15777_09_27.jpg" alt="Figure 9.27: Comparing the results of Activity 9.01 and Activity 9.02&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.27: Comparing the results of Activity 9.01 and Activity 9.02</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YTpxR7">https://packt.live/2YTpxR7</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3dY5Bku">https://packt.live/3dY5Bku</a>.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor215"/>Activity 9.03: Predicting the Trend of Amazon's Stock Price Using an LSTM with an Increasing Number of LSTM Neurons (100 Units)</h2>
			<p>In this activity, we will examine the stock price of Amazon over the last 5 years, from January 1, 2014, to December 31, 2018. We will try to predict and forecast the company's future trend for January 2019 using <strong class="source-inline">RNNs</strong> with four <strong class="source-inline">LSTM</strong> layers, each with <strong class="source-inline">100</strong> units. We have the actual values for January 2019, so we will be able to compare our predictions with the actual values later. You can also compare the output difference with <em class="italic">Activity 9.01</em>, <em class="italic">Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Import the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">dataset_training = pd.read_csv('../AMZN_train.csv')</p><p class="source-code">dataset_training.head()</p></li>
				<li>We are going to make our prediction using the <strong class="source-inline">Open</strong> stock price; therefore, select the <strong class="source-inline">Open</strong> stock price column from the dataset and print the values:<p class="source-code">training_data = dataset_training[['Open']].values</p><p class="source-code">training_data</p></li>
				<li>Then, perform feature scaling by normalizing the data using <strong class="source-inline">MinMaxScaler</strong> and setting the range of the features so that they have a minimum value of zero and a maximum value of one. Use the <strong class="source-inline">fit_transform</strong> method of the scaler on the training data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">sc = MinMaxScaler(feature_range = (0, 1))</p><p class="source-code">training_data_scaled = sc.fit_transform(training_data)</p><p class="source-code">training_data_scaled</p></li>
				<li>Create the data to get <strong class="source-inline">60</strong> timestamps from the current instance. We chose <strong class="source-inline">60</strong> here as it will give us a sufficient number of previous instances in order to understand the trend; technically, this can be any number, but <strong class="source-inline">60</strong> is the optimal value. Additionally, the upper bound value here is <strong class="source-inline">1258</strong>, which is the index or count of rows (or records) in the training set:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p><p class="source-code">for i in range(60, 1258):</p><p class="source-code">    X_train.append(training_data_scaled[i-60:i, 0])</p><p class="source-code">    y_train.append(training_data_scaled[i, 0])</p><p class="source-code">X_train, y_train = np.array(X_train), np.array(y_train)</p></li>
				<li>Reshape the data to add an extra dimension to the end of <strong class="source-inline">X_train</strong> using NumPy's <strong class="source-inline">reshape</strong> function:<p class="source-code">X_train = np.reshape(X_train, (X_train.shape[0], \</p><p class="source-code">                               X_train.shape[1], 1))</p></li>
				<li>Import the following Keras libraries to build the RNN:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Set the seed and initiate the sequential model:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add an LSTM layer to the network with <strong class="source-inline">100</strong> units, set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong>, and set the <strong class="source-inline">input_shape</strong> argument to <strong class="source-inline">(X_train.shape[1], 1)</strong>. Add three additional <strong class="source-inline">LSTM</strong> layers, each with <strong class="source-inline">100</strong> units, and set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong> for the first two. Add a final output layer of size <strong class="source-inline">1</strong>:<p class="source-code">model.add(LSTM(units = 100, return_sequences = True, \</p><p class="source-code">               input_shape = (X_train.shape[1], 1)))</p><p class="source-code"># Adding a second LSTM layer</p><p class="source-code">model.add(LSTM(units = 100, return_sequences = True))</p><p class="source-code"># Adding a third LSTM layer</p><p class="source-code">model.add(LSTM(units = 100, return_sequences = True))</p><p class="source-code"># Adding a fourth LSTM layer</p><p class="source-code">model.add(LSTM(units = 100))</p><p class="source-code"># Adding the output layer</p><p class="source-code">model.add(Dense(units = 1))</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and use <strong class="source-inline">Mean Squared Error</strong> for the loss. Fit the model to the training data for <strong class="source-inline">100</strong> epochs with a batch size of <strong class="source-inline">32</strong>:<p class="source-code"># Compiling the RNN</p><p class="source-code">model.compile(optimizer = 'adam', loss = 'mean_squared_error')</p><p class="source-code"># Fitting the RNN to the Training set</p><p class="source-code">model.fit(X_train, y_train, epochs = 100, batch_size = 32)</p></li>
				<li>Load and process the test data (which is treated as actual data here) and select the column representing the value of open stock data:<p class="source-code">dataset_testing = pd.read_csv('../AMZN_test.csv')</p><p class="source-code">actual_stock_price = dataset_testing[['Open']].values</p><p class="source-code">actual_stock_price</p></li>
				<li>Concatenate the data since we will need <strong class="source-inline">60</strong> previous instances to get the stock price for each day. Therefore, we will need both the training and test data:<p class="source-code">total_data = pd.concat((dataset_training['Open'], \</p><p class="source-code">                        dataset_testing['Open']), axis = 0)</p></li>
				<li>Reshape and scale the input to prepare the test data. Note that we are predicting the January monthly trend, which has <strong class="source-inline">21</strong> financial days, so in order to prepare the test set, we take the lower bound value as <strong class="source-inline">60</strong> and the upper bound value as <strong class="source-inline">81</strong>. This ensures that the difference of <strong class="source-inline">21</strong> is maintained:<p class="source-code">inputs = total_data[len(total_data) \</p><p class="source-code">         - len(dataset_testing) - 60:].values</p><p class="source-code">inputs = inputs.reshape(-1,1)</p><p class="source-code">inputs = sc.transform(inputs)</p><p class="source-code">X_test = []</p><p class="source-code">for i in range(60, 81):</p><p class="source-code">    X_test.append(inputs[i-60:i, 0])</p><p class="source-code">X_test = np.array(X_test)</p><p class="source-code">X_test = np.reshape(X_test, (X_test.shape[0], \</p><p class="source-code">                             X_test.shape[1], 1))</p><p class="source-code">predicted_stock_price = model.predict(X_test)</p><p class="source-code">predicted_stock_price = \</p><p class="source-code">sc.inverse_transform(predicted_stock_price)</p></li>
				<li>Visualize the results by plotting the actual stock price and plotting the predicted stock price:<p class="source-code">plt.plot(actual_stock_price, color = 'green', \</p><p class="source-code">         label = 'Actual Amazon Stock Price',ls='--')</p><p class="source-code">plt.plot(predicted_stock_price, color = 'red', \</p><p class="source-code">         label = 'Predicted Amazon Stock Price',ls='-')</p><p class="source-code">plt.title('Predicted Stock Price')</p><p class="source-code">plt.xlabel('Time in days')</p><p class="source-code">plt.ylabel('Real Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>Please note that your results may differ slightly from the actual stock price.</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer250" class="IMG---Figure"><img src="image/B15777_09_28.jpg" alt="Figure 9.28: Real versus predicted stock prices&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.28: Real versus predicted stock prices</p>
			<p>So, if we compare the results of the <strong class="source-inline">LSTM</strong> with <strong class="source-inline">50</strong> units (from <em class="italic">Activity 9.01</em>, <em class="italic">Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</em>) and the <strong class="source-inline">LSTM</strong> with <strong class="source-inline">100</strong> units in this activity, we get trends with <strong class="source-inline">100</strong> units. Also, note that when we run the <strong class="source-inline">LSTM</strong> with <strong class="source-inline">100</strong> units, it takes more computational time than the <strong class="source-inline">LSTM</strong> with <strong class="source-inline">50</strong> units. A trade-off needs to be considered in such cases:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B15777_09_29.jpg" alt="Figure 9.29: Comparing the real versus predicted stock price with 50 and 100 units&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.29: Comparing the real versus predicted stock price with 50 and 100 units</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31NQkQy">https://packt.live/31NQkQy</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ZCZ4GR">https://packt.live/2ZCZ4GR</a>.</p>
		</div>
	</body></html>