<html><head></head><body>
		<div id="_idContainer015">
			<h1 id="_idParaDest-17"><em class="italic"><a id="_idTextAnchor016"/>Chapter 1</em>: Introduction to Reinforcement Learning</h1>
			<p><strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>) aims to create <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI)</strong>) agents that can make decisions in complex and uncertain environments, with the goal of maximizing their long-term benefit. These agents learn how to do this by interacting with their environments, which mimics the way we as humans learn from experience. As such, RL has an incredibly broad and adaptable set of applications, with the potential to disrupt and revolutionize global industries.</p>
			<p>This book will give you an advanced-level understanding of this field. We will go deeper into the theory behind some algorithms you may already know, and cover state-of-the-art RL. Moreover, this is a practical book. You will see examples inspired by real-world industry problems and learn expert tips along the way. By its conclusion, you will be able to model and solve your own sequential decision-making problems using Python. </p>
			<p>So, let's start our journey by refreshing your mind on RL concepts and get you set up for the advanced material coming up in the following chapters. Specifically, this chapter covers the following topics:</p>
			<ul>
				<li>Why RL?</li>
				<li>The three paradigms of machine learning</li>
				<li>RL application areas and success stories</li>
				<li>Elements of an RL problem</li>
				<li>Setting up your RL environment</li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Why RL?</h1>
			<p>Creating intelligent<a id="_idIndexMarker000"/> machines that make decisions at or superior to the human level is a dream of many scientists and engineers, and one that is gradually becoming closer to reality. In the seven decades since the Turing test, AI research and development have been on a roller coaster. The expectations were very high initially; in the 1960s, for example, Herbert Simon (who later received the Nobel Prize in Economics) predicted that machines would be capable of doing any work humans can do within 20 years. It was this excitement that attracted big government and corporate funding flowing into AI research, only to be followed by big disappointments and a period called the "AI winter." Decades later, thanks to the incredible developments in computing, data, and algorithms, humankind is again very excited, more than ever before, in its pursuit of the AI dream. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you're not familiar with Alan Turing's instrumental work on the foundations of AI in 1950, it's worth learning more about the Turing Test here: <a href="https://youtu.be/3wLqsRLvV-c">https://youtu.be/3wLqsRLvV-c</a>.</p>
			<p>The AI dream is certainly one of grandiosity. After all, the potential in intelligent autonomous systems is enormous. Think about how we are limited in terms of specialist medical doctors in the world. It takes years and significant intellectual and financial resources to educate them, which many countries don't have at sufficient levels. In addition, even after years of education, it is nearly impossible for a specialist to stay up to date with all of the scientific developments in their field, learn from the outcomes of the tens of thousands of treatments around the world, and effectively incorporate all this knowledge into practice. </p>
			<p>Conversely, an AI model could process and learn from all this data and combine it with a rich set of information about a patient (such as medical history, lab results, presenting symptoms, health profile, and so on) to make a diagnosis and suggest treatments. This model could serve in even the most rural parts of the world (as long as an internet connection and computer are available) and direct the local health personnel about the treatment. No doubt it would revolutionize international healthcare and improve the lives of millions of people.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">AI is already transforming the healthcare industry. In a recent article, Google published results from an AI system surpassing human experts in breast cancer prediction using mammography readings (McKinney et al., 2020). Microsoft is collaborating with one of India's largest healthcare providers to detect cardiac illnesses using AI (Agrawal, 2018). IBM Watson for Clinical Trial Matching uses natural language processing to recommend potential treatments for patients from medical databases (<a href="https://youtu.be/grDWR7hMQQQ">https://youtu.be/grDWR7hMQQQ</a>).</p>
			<p>On our quest<a id="_idIndexMarker001"/> to develop AI systems that are at or superior to the human level, which is –somewhat controversially – called <strong class="bold">Artificial General Intelligence</strong> (<strong class="bold">AGI</strong>), it makes sense to develop a model that can learn from its own experience – without necessarily needing a <a id="_idIndexMarker002"/>supervisor. RL is the <a id="_idIndexMarker003"/>computational framework that enables us to create such intelligent agents. To better understand the value of RL, it is important to compare it with the other <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) paradigms, which we'll look into next.</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>The three paradigms of machine learning</h1>
			<p>RL is a separate<a id="_idIndexMarker004"/> paradigm in ML, along with <strong class="bold">Supervised Learning</strong> (<strong class="bold">SL</strong>) and <strong class="bold">Unsupervised Learning</strong> (<strong class="bold">UL</strong>). It goes beyond what the other two paradigms involve – for example, perception, classification, regression, and clustering – and makes decisions. More importantly, however, RL utilizes the supervised and unsupervised ML methods in doing so. Therefore, RL is a distinct yet closely related field to SL and UL, and it's important to have a grasp of them.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Supervised learning</h2>
			<p>SL is about <a id="_idIndexMarker005"/>learning a mathematical <a id="_idIndexMarker006"/>function that maps a set of inputs to the corresponding outputs/labels as accurately as possible. The idea is that we don't know the dynamics of the process that generates the output, but we try to figure it out using the data coming out of it. Consider the following examples:</p>
			<ul>
				<li>An image recognition model that classifies the objects on the camera of a self-driving car as a pedestrian, stop sign, truck, and so on</li>
				<li>A forecasting model that predicts the customer demand of a product for a particular holiday season using past sales data</li>
			</ul>
			<p>It is extremely difficult to come up with the precise rules to visually differentiate objects, or what factors lead to customers demanding a product. Therefore, SL models infer them from labeled data. Here are some key points about how it works:</p>
			<ul>
				<li>During training, models learn from ground truth labels/output provided by a supervisor (which could be a human expert or a process).</li>
				<li>During inference, models make predictions about what the output might be given the input.</li>
				<li>Models use <a id="_idIndexMarker007"/>function approximators to <a id="_idIndexMarker008"/>represent the dynamics of the processes that generate the outputs.</li>
			</ul>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Unsupervised learning</h2>
			<p>UL algorithms identify patterns in data that were previously unknown. When using these models, we <a id="_idIndexMarker009"/>might have an idea of what to <a id="_idIndexMarker010"/>expect as a result, but we don't supply the models with labels. Consider the following examples:</p>
			<ul>
				<li>Identifying homogenous segments on an image provided by the camera of a self-driving car. The model is likely to separate the sky, road, buildings, and so on based on the textures on the image.</li>
				<li>Clustering weekly sales data into three groups based on sales volume. The output is likely to be weeks with low, medium, and high sales volumes.</li>
			</ul>
			<p>As you can tell, this is quite different from how SL works, namely in the following ways:</p>
			<ul>
				<li>UL models don't know what the ground truth is, and there is no label to map the input to. They just identify the different patterns in the data. Even after doing so, for example, the model would not be aware that it separated the sky from the road, or a holiday week from a regular week. </li>
				<li>During inference, the model would cluster the input into one of the groups it had identified, again, without knowing what that group represents.</li>
				<li>Function approximators, such as neural networks, are used in some UL algorithms, but not always.</li>
			</ul>
			<p>With SL and UL<a id="_idIndexMarker011"/> reintroduced, we'll now <a id="_idIndexMarker012"/>compare them to RL.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Reinforcement learning</h2>
			<p>RL is a framework <a id="_idIndexMarker013"/>to learn how to make <a id="_idIndexMarker014"/>decisions under uncertainty to maximize a long-term benefit through trial and error. These decisions are made sequentially, and earlier decisions affect the situations and benefits that will be encountered later. This separates RL from both SL and UL, which don't involve any decision-making. Let's revisit the examples we provided earlier to see how an RL model would differ from SL and UL models in terms of what it tries to find out:</p>
			<ul>
				<li>For a self-driving car, given the types and positions of all the objects on the camera, and the edges of the lanes on the road, the model might learn how to steer the wheel and what the speed of the car should be to pass the car ahead safely and as quickly as possible.</li>
				<li>Given the historical sales numbers for a product and the time it takes to bring the inventory from the supplier to the store, the model might learn when and how many units to order from the supplier so that seasonal customer demand is satisfied with high likelihood, while the inventory and transportation costs are minimized. </li>
			</ul>
			<p>As you will have noticed, the tasks that RL is trying to accomplish are of a different nature and more complex than those simply addressed by SL and UL alone. Let's elaborate on how RL is different:</p>
			<ul>
				<li>The output of an RL model is a decision given the situation, not a prediction or clustering.</li>
				<li>There are no ground-truth decisions provided by a supervisor that tell the model what the ideal decisions are in different situations. Instead, the model learns the best decisions from the feedback from its own experience and the decisions it made in the past. For example, through trial and error, an RL model would learn that speeding too much while passing a car may lead to accidents, and ordering too much inventory before holidays will cause excess inventory later.</li>
				<li>RL models often use outputs of SL models as inputs to make decisions. For example, the output of an image recognition model in a self-driving car could be used to make driving decisions. Similarly, the output of a forecasting model is often used as input to an RL model that makes inventory replenishment decisions.</li>
				<li>Even in the absence of such input from an auxiliary model, RL models, either implicitly or explicitly, predict what situations its decisions will lead to in the future.</li>
				<li>RL utilizes <a id="_idIndexMarker015"/>many methods developed<a id="_idIndexMarker016"/> for SL and UL, such as various types of neural networks as function approximators.</li>
			</ul>
			<p>So, what differentiates RL from other ML methods is that it is a decision-making framework. What makes it exciting and powerful, though, is its similarities to how we learn as humans to make decisions from experience. Imagine a toddler learning how to build a tower from toy blocks. Usually, the taller the tower, the happier the toddler is. Every increment in height is a success. Every collapse is a failure. They quickly discover that the closer the next block is to the center of the one beneath, the more stable the tower is. This is reinforced when a block that is placed too close to the edge more readily topples. With practice, they manage to stack several blocks on top of each other. They realize how they stack the earlier blocks creates a foundation that determines how tall of a tower they can build. Thus, they learn.</p>
			<p>Of course, the toddler did not learn these architectural principles from a blueprint. They learned from the commonalities in their failure and success. The increasing height of the tower or its collapse provided a feedback signal upon which they refined their strategy accordingly. Learning from experience, rather than a blueprint, is at the center of RL. Just as the toddler discovers which block positions lead to taller towers, an RL agent identifies the actions with the highest long-term rewards through trial and error. This is what makes RL such a profound form of AI; it's unmistakably human. </p>
			<p>Over the past few years, there have been many amazing success stories proving the potential in<a id="_idIndexMarker017"/> RL. Moreover, there are many<a id="_idIndexMarker018"/> industries it is about to transform. So, before diving into the technical aspects of RL, let's further motivate ourselves by looking into what RL can do in practice.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>RL application areas and success stories </h1>
			<p>RL is not a new<a id="_idIndexMarker019"/> field. Many of the fundamental ideas in RL were introduced in the context of dynamic programming and optimal control over the past seven decades. However, successful RL implementations have taken off recently thanks to the breakthroughs in deep learning and more powerful computational resources. In this section, we will talk about some of the application areas of RL together with some famous success stories. We will go deeper into the algorithms behind these implementations in the following chapters.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Games</h2>
			<p>Board and video games <a id="_idIndexMarker020"/>have been a research lab for RL, leading to many <a id="_idIndexMarker021"/>famous success stories in this area. The reasons why games make good RL problems are as follows:</p>
			<ul>
				<li>Games are naturally about sequential decision-making with uncertainty involved.</li>
				<li>They are available as computer software, making it possible for RL models to flexibly interact with them and generate billions of data points for training. Also, trained RL models are then also tested in the same computer environment. This is as opposed to many physical processes for which it is too complex to create accurate and fast simulators. </li>
				<li>The natural benchmark in games are the best human players, making it an appealing battlefield for AI versus human comparisons.</li>
			</ul>
			<p>After this <a id="_idIndexMarker022"/>introduction, let's look into some of the most exciting pieces of RL work that have made it to the headlines.</p>
			<h3>TD-Gammon</h3>
			<p>The first famous RL<a id="_idIndexMarker023"/> implementation is TD-Gammon, a model that learned how to play super-human-level backgammon – a two-player board game with 1,020 possible configurations. The model was developed by Gerald Tesauro at IBM Research in 1992. TD-Gammon was so successful that it created great excitement in the backgammon community back then with the novel strategies it taught humans. Many methods used in that model (temporal-difference, self-play, and use of neural networks, for example) are still at the center of modern RL implementations.</p>
			<h3>Super-human performance in Atari games</h3>
			<p>One of the most <a id="_idIndexMarker024"/>impressive and seminal works in RL was that of Volodymry Mnih and his colleagues at Google DeepMind that came out in 2015. The researchers trained RL agents that learned how to play Atari games better than humans by only using screen input and game scores, without any hand-crafted or game-specific features through deep neural networks. They<a id="_idIndexMarker025"/> named their algorithm the <strong class="bold">Deep Q-Network</strong> (<strong class="bold">DQN</strong>), which is one of the most popular RL algorithms today. </p>
			<h3>Beating the world champions in Go, chess, and shogi</h3>
			<p>The RL <a id="_idIndexMarker026"/>implementation that perhaps<a id="_idIndexMarker027"/> brought the most fame to <a id="_idIndexMarker028"/>RL was Google DeepMind's AlphaGo. It was the first computer program to beat a professional player in the ancient board game of Go in 2015, and later the world champion Lee Sedol in 2016. This story was later turned into a documentary film with the same name. The AlphaGo model was trained using data from human expert moves as well as with RL through self-play. A later version, AlphaGo Zero, reached a performance of defeating the original AlphaGo 100-0, which was trained via just self-play and without any human knowledge inserted into the model. Finally, the company released AlphaZero in 2018, which was able to learn the games of chess, shogi (Japanese chess), and Go to <a id="_idIndexMarker029"/>become the <a id="_idIndexMarker030"/>strongest <a id="_idIndexMarker031"/>player in history for each, without any prior information about the games except the game <a id="_idIndexMarker032"/>rules. AlphaZero reached this performance after only several hours of training on <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>). AlphaZero's unconventional strategies were praised by world-famous players, such as Garry Kasparov (chess) and Yoshiharu Habu (shogi).</p>
			<h3>Victories in complex strategy games</h3>
			<p>RL's success later<a id="_idIndexMarker033"/> went beyond just Atari and board games, into Mario, Quake III Arena, Capture the Flag, Dota 2, and StarCraft II. Some of these games are exceptionally challenging for AI programs with the need for strategic planning, the involvement of game theory between multiple decision-makers, imperfect information, and a large number of possible actions and game states. Due to this complexity, it took an enormous amount of resources to train those models. For example, OpenAI trained the Dota 2 model using 256 GPUs and 128,000 CPU cores for months, giving 900 years of game experience to the model per day. Google DeepMind's AlphaStar, which defeated top professional players in StarCraft II in 2019, required training hundreds of copies of a sophisticated model with 200 years of real-time game experience for each, although those models were initially trained on the real game data of human players.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Robotics and autonomous systems</h2>
			<p>Robotics and physical <a id="_idIndexMarker034"/>autonomous systems are<a id="_idIndexMarker035"/> challenging fields for RL. This is because RL agents are trained in simulation to gather enough data, but a simulation environment cannot reflect all the complexities of the real world. Therefore, those agents often fail in the actual task, which is especially problematic if the task is safety-critical. In addition, these applications often involve continuous actions, which require different types of algorithms than DQN. Despite these challenges, on the other hand, there are numerous RL success stories in these fields. In addition, there is a lot of research on using RL in exciting applications such as autonomous ground and air vehicles.</p>
			<h3>Elevator optimization</h3>
			<p>An early success <a id="_idIndexMarker036"/>story that proved RL can create value for real-world applications was about elevator optimization in 1996 by Robert Crites and Andrew Barto. The researchers developed an RL model to optimize elevator dispatching in a 10-story building with 4 elevator cars. This was a much more challenging problem than the earlier TD-Gammon due to the possible number of situations the model can encounter, partial observability (for example, the number of people waiting at different floors was not observable to the RL model), and the possible number of decisions to choose from. The RL model substantially improved the best elevator control heuristics of the time across various metrics, such as average passenger wait time and travel time.</p>
			<h3>Humanoid robots and dexterous manipulation</h3>
			<p>In 2017, Nicolas<a id="_idIndexMarker037"/> Heess et al. of Google DeepMind were able to teach different types of bodies (for <a id="_idIndexMarker038"/>example, humanoid and so on) various locomotion behaviors, such as how to run, jump, and so on in a computer simulation. In 2018, Marcin Andrychowicz et al. of OpenAI trained a five-fingered humanoid hand to manipulate a block from an initial configuration to a goal configuration. In 2019, again, researchers from OpenAI, Ilge Akkaya et al., were able to train a robot hand to solve a Rubik's cube:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B14160_01_01.jpg" alt="Figure 1.1 – OpenAI's RL model that solved a Rubik's cube is trained in simulation (a) and deployed on a physical robot (b) (image source: OpenAI Blog, 2019)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – OpenAI's RL model that solved a Rubik's cube is trained in simulation (a) and deployed on a physical robot (b) (image source: OpenAI Blog, 2019)</p>
			<p>Both of the <a id="_idIndexMarker039"/>latter two<a id="_idIndexMarker040"/> models were trained in simulation and successfully transferred to physical implementation using domain randomization techniques (<em class="italic">Figure 1.1</em>).</p>
			<h3>Emergency response robots </h3>
			<p>In the aftermath of <a id="_idIndexMarker041"/>a disaster, using robots could be extremely helpful, especially when operating in dangerous conditions. For example, robots could locate survivors in damaged structures, turn off gas valves, and so on. Creating intelligent robots that operate autonomously would allow scaling emergency response operations and provide the necessary support to many more people than is currently possible with manual operations.</p>
			<h3>Self-driving vehicles</h3>
			<p>Although a fully <a id="_idIndexMarker042"/>self-driving car is too complex to solve with an RL model alone, some of the tasks could be handled by RL. For example, we can train RL agents for self-parking and making decisions for when and how to pass a car on a highway. Similarly, we can use RL agents to execute certain tasks in an autonomous drone, such as how to take off, land, avoid collisions, and so on.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Supply chain</h2>
			<p>Many decisions in a <a id="_idIndexMarker043"/>supply chain are of sequential nature and involve uncertainty, for which RL is a natural approach. Some of these problems are as follows: </p>
			<ul>
				<li><strong class="bold">Inventory planning</strong> is about deciding when to place a purchase order to replenish the inventory of an item and at what quantity. Ordering less than necessary causes shortages and ordering more than necessary causes excess inventory costs, product spoilage, and inventory removal at reduced prices. RL models are used to make inventory planning decisions to decrease the cost of these operations.</li>
				<li><strong class="bold">Bin packing</strong> is a common problem in manufacturing and supply chains where items arriving at a station are placed into containers to minimize the number of containers used and to ensure smooth operations in the facility. This is a difficult problem that can be solved using RL.</li>
			</ul>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Manufacturing</h2>
			<p>An area where RL <a id="_idIndexMarker044"/>will have a great impact is manufacturing, where a lot of manual tasks can potentially be carried out by autonomous agents at reduced costs and increased quality. As a result, many companies are looking into bringing RL to their manufacturing environment. Here are some examples of RL applications in manufacturing:</p>
			<ul>
				<li><strong class="bold">Machine calibration</strong> is a <a id="_idIndexMarker045"/>task that is often handled by human experts in manufacturing environments, which is inefficient and error-prone. RL models are often capable of achieving these tasks at reduced costs and increased quality.</li>
				<li><strong class="bold">Chemical plant operations</strong> often involve sequential decision making, which is often handled by human experts or heuristics. RL agents are shown to effectively control these processes with better final product quality and less equipment wear and tear. </li>
				<li><strong class="bold">Equipment maintenance</strong> requires planning downtimes to avoid costly breakdowns. RL models can effectively balance the cost of downtime and the cost of a potential breakdown.</li>
				<li>In addition to<a id="_idIndexMarker046"/> these examples, many successful RL applications in <strong class="bold">robotics</strong> can be transferred to manufacturing solutions.</li>
			</ul>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Personalization and recommender systems</h2>
			<p>Personalization <a id="_idIndexMarker047"/>is arguably the area where RL has created the<a id="_idIndexMarker048"/> most business value so far. Big tech companies provide personalization as a service with RL algorithms running under the hood. Here are some examples:</p>
			<ul>
				<li>In <strong class="bold">advertising</strong>, the order and content of promotional materials delivered to (potential) customers is a sequential decision-making problem that can be solved using RL, leading to increased customer satisfaction and conversion.</li>
				<li><strong class="bold">News recommendation</strong> is an area where Microsoft News has famously applied RL and increased visitor engagement by improving the article selection and the order of recommendation.</li>
				<li><strong class="bold">Personalization of the artwork</strong> that you see for the titles on Netflix is handled by RL algorithms. With that, viewers better identify the titles relevant to their interests.</li>
				<li><strong class="bold">Personalized healthcare</strong> is becoming increasingly important as it provides more effective treatments at reduced costs. There are many successful applications of RL picking the right treatment for patients.</li>
			</ul>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Smart cities</h2>
			<p>There are many<a id="_idIndexMarker049"/> areas where RL can help improve how cities operate. The following are a couple of examples:</p>
			<ul>
				<li>In a traffic network with multiple intersections, the traffic lights should work in harmony to ensure the smooth flow of the traffic. It turns out that this problem can be modeled as a multi-agent RL problem and improve the existing systems for traffic light control.</li>
				<li>Balancing the generation and demand in electricity grids in real time is an important problem to ensure grid safety. One way of achieving this is to control the demand, such as charging electric vehicles and turning on air conditioning systems when there is enough generation, without sacrificing the service quality, to which RL methods have successfully been applied.</li>
			</ul>
			<p>This list can go on for pages, but it should be enough to demonstrate the huge potential in RL. What Andrew Ng, a pioneer in the field, says about AI is very much true for RL as well:</p>
			<p class="author-quote">Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don't think AI will transform in the next several years. (Andrew Ng: Why AI is the new electricity; Stanford News; March 15, 2017)</p>
			<p>RL today is only at the <a id="_idIndexMarker050"/>beginning of its prime, and you are making a great investment by putting effort toward understanding what RL is and what it has to offer. Now, it is time to get more technical and formally define the elements in an RL problem.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Elements of an RL problem</h1>
			<p>So far, we have covered the types of problems that can be modeled using RL. In the next chapters, we will<a id="_idIndexMarker051"/> dive into state-of-the-art algorithms that will solve those problems. However, in between, we need to formally define the elements in an RL problem. This will lay the groundwork for more technical material by establishing our vocabulary. After providing these definitions, we will then look into what these concepts correspond to in a tic-tac-toe example.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>RL concepts</h2>
			<p>Let's start by <a id="_idIndexMarker052"/>defining the most fundamental components in an RL problem:</p>
			<ul>
				<li>At the center of an RL <a id="_idIndexMarker053"/>problem, there is the learner, which is called the <strong class="bold">agent</strong> in RL terminology. Most of the problem classes we deal with have a single agent. On the other hand, if there is more than one agent, that problem class is called a <strong class="bold">multi-agent RL</strong>, or <strong class="bold">MARL</strong> for<a id="_idIndexMarker054"/> short. In MARL, the relationship between the agents could be cooperative, competitive, or a mix of the two.</li>
				<li>The essence of an RL problem is the agent learning what to do – that is, which <strong class="bold">action</strong> to take – in different situations in the world it lives in. We call this world the <strong class="bold">environment</strong> and it refers to everything outside of the agent. </li>
				<li>The set of all the information that precisely and sufficiently describes the situation in the environment<a id="_idIndexMarker055"/> is called the <strong class="bold">state</strong>. So, if the environment is in the same state at different points in time, it means everything about the environment is exactly the same – like a copy-paste. </li>
				<li>In some problems, the knowledge of the state is fully available to the agent. In a lot of other problems, and especially in more realistic ones, the agent does not fully observe the state, but only part of it (or a derivation of a part of the state). In such cases, the agent uses its <strong class="bold">observation</strong> to take action. When this is the case, we say that the problem is <strong class="bold">partially observable</strong>. Unless we say otherwise, we assume that the agent is able to fully observe the state that the environment is in and is basing its actions on the state. <p class="callout-heading">Info</p><p class="callout">The term <em class="italic">state</em>, and its notation <img src="image/Formula_01_001.png" alt=""/>, is more commonly used during abstract discussions, especially when the environment is assumed to be fully observable, although <em class="italic">observation</em> is a more general term; what the agent receives is always an observation, which is sometimes just the state itself, and sometimes a part of or a derivation from the state, depending on the environment. Don't get confused if you see them used interchangeably in some contexts.</p></li>
			</ul>
			<p>So far, we have not really defined what makes an action good or bad. In RL, every time the agent takes an action, it receives a <strong class="bold">reward</strong> from the environment (albeit it is sometimes zero). <em class="italic">Reward</em> could mean many things in general, but in RL terminology, its meaning is very specific: it is a scalar number. The greater the number is, the higher the reward also is. In an iteration of an RL problem, the agent observes the state the environment is in (fully or partially) and takes an action based on its observation. As a result, the agent receives a reward, and the environment transitions into a new state. This process is described in <em class="italic">Figure 1.2</em>, which is probably familiar to you:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B14160_01_02.jpg" alt="Figure 1.2 – RL process diagram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – RL process diagram</p>
			<p>Remember that <a id="_idIndexMarker056"/>in RL, the agent is interested in actions that will be beneficial over the long term. This means the agent must consider the long-term consequences of its actions. Some actions might lead the agent to immediate high rewards only to be followed by very low rewards. The opposite might also be true. So, the agent's goal is to maximize the cumulative reward it receives. The natural follow-up question is over what time horizon? The answer depends on whether the problem of interest is defined over a finite or infinite horizon:</p>
			<ul>
				<li>If it is the former, the<a id="_idIndexMarker057"/> problem is described as an <strong class="bold">episodic task</strong>, where an <strong class="bold">episode</strong> is defined as the sequence of interactions from an initial state to a <strong class="bold">terminal state</strong>. In episodic tasks, the agent's goal is to maximize the expected total cumulative reward collected over an episode. </li>
				<li>If the problem is defined <a id="_idIndexMarker058"/>over an infinite horizon, it is called a <strong class="bold">continuing task</strong>. In that case, the agent will try to maximize the average reward since the total reward would go up to infinity. </li>
				<li>So, how does an agent achieve this objective? The agent identifies the best action(s) to take given its observation of the environment. In other words, the RL problem is all about finding a <strong class="bold">policy</strong>, which maps a given observation to one (or more) of the actions, which maximizes the expected cumulative reward.</li>
			</ul>
			<p>All these concepts have concrete mathematical definitions, which we will cover in detail in later chapters. But<a id="_idIndexMarker059"/> for now, let's try to understand what these concepts would correspond to in a concrete example.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Casting tic-tac-toe as an RL problem</h2>
			<p>Tic-tac-toe is a simple <a id="_idIndexMarker060"/>game in which two players take turns <a id="_idIndexMarker061"/>to mark the empty spaces in a <img src="image/Formula_01_002.png" alt=""/> grid. We will now cast this as an RL problem to map the definitions provided previously to the concepts in the game. The goal for a player is to place three of their marks in a vertical, horizontal, or diagonal row to become the winner. If none of the players are able to achieve this before running out of the empty spaces on the grid, the game ends in a draw. Mid-game, a tic-tac-toe board might look like this:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B14160_01_03.jpg" alt="Figure 1.3 – An example board configuration in tic-tac-toe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – An example board configuration in tic-tac-toe</p>
			<p>Now, imagine that we have an RL agent playing against a human player:</p>
			<ul>
				<li>The action the agent takes is to place its mark (say, a cross) in one of the empty spaces on the board when it is the agent's turn. </li>
				<li>Here, the board is the entire environment, and the position of the marks on the board is the state, which is fully observable to the agent. </li>
				<li>In a 3 x 3 tic-tac-toe game, there are 765 states (unique board positions, excluding<a id="_idIndexMarker062"/> rotations and reflections) and the agent's <a id="_idIndexMarker063"/>goal is to learn a policy that will suggest an action for each of these states so as to maximize the chance of winning. </li>
				<li>The game can be defined as an episodic RL task. Why? Because the game will last for a maximum of 9 turns and the environment will reach a terminal state. A terminal state is one where either three Xs or Os make a row or one where no single mark makes a row and there is no space left on the board (that is, a draw). </li>
				<li>Note that no reward is given as the players make their moves during the game, except at the very end if a player wins. So, the agent receives +1 reward if it wins, -1 if it loses, and 0 if the game is a draw. In all the iterations until the end, the agent receives 0 reward. </li>
				<li>We can turn this into a multi-agent RL problem by replacing the human player with another RL agent to compete with the first one.</li>
			</ul>
			<p>Hopefully, this refreshes your mind on what agent, state, action, observation, policy, and reward mean. This was just a toy example, and rest assured that it will get much more advanced<a id="_idIndexMarker064"/> later. With this introductory context out of the<a id="_idIndexMarker065"/> way, what we need to do is to set up our computer environment to be able to run the RL algorithms we will cover in the following chapters.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Setting up your RL environment</h1>
			<p>RL algorithms <a id="_idIndexMarker066"/>utilize state-of-the-art ML libraries that require some sophisticated hardware. To follow along with the examples we will solve throughout the book, you will need to set up your computer environment. Let's go over the hardware and software you will need in your setup.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Hardware requirements</h2>
			<p>As mentioned<a id="_idIndexMarker067"/> previously, state-of-the-art RL models are usually trained on hundreds of GPUs and thousands of CPUs. We certainly don't expect you to have access to those resources. However, having multiple CPU cores will help you simultaneously simulate many agents and environments to collect data more quickly. Having a GPU will speed up training deep neural networks that are used in modern RL algorithms. In addition, to be able to efficiently process all that data, having enough RAM resources is important. But don't worry; work with what you have, and you will still get a lot out of this book. For your reference, here are some specifications of the desktop we used to run the experiments: </p>
			<ul>
				<li>AMD Ryzen Threadripper 2990WX CPU with 32 cores</li>
				<li>NVIDIA GeForce RTX 2080 Ti GPU</li>
				<li>128 GB RAM</li>
			</ul>
			<p>As an alternative to building a desktop with expensive hardware, you can use <strong class="bold">Virtual Machines</strong> (<strong class="bold">VMs</strong>) with <a id="_idIndexMarker068"/>similar capabilities provided by various companies. The most famous ones are as follows:</p>
			<ul>
				<li>Amazon's AWS</li>
				<li>Microsoft Azure</li>
				<li>Google Cloud Platform</li>
			</ul>
			<p>These cloud providers also provide data science images for your VMs during the setup and it saves the user from installing the necessary software for deep learning (for example, CUDA, TensorFlow, and so on). They also provide detailed guidelines on how to set up your VMs to which we will defer the details of the setup.</p>
			<p>A final option that would allow small-scale deep learning experiments with TensorFlow is Google's Colab, which provides VM instances readily accessible from your browser with the necessary <a id="_idIndexMarker069"/>software installed. You can start experimenting on a Jupyter Notebook-like environment right away, which is a very convenient option for quick experimentation.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Operating system</h2>
			<p>When you develop<a id="_idIndexMarker070"/> data science models for educational purposes, there is often not a lot of difference between Windows, Linux, or macOS. However, we plan to do a bit more than that in this book, with advanced RL libraries running on a GPU. This setting is best supported on Linux, of which we use the Ubuntu 18.04.3 LTS distribution. Another option is macOS, but that often does not come with a GPU on the<a id="_idIndexMarker071"/> machine. Finally, although the setup could be a bit convoluted, <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>) 2 is an option you could explore.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Software toolbox</h2>
			<p>One of the first things<a id="_idIndexMarker072"/> people do when setting up the software environment for data science projects is to install <strong class="bold">Anaconda</strong>, which gives you a Python platform, along with many useful libraries. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The CLI tool called <strong class="source-inline">virtualenv</strong> is a lighter-weight tool compared to Anaconda to create virtual environments for Python, and preferable in most production environments. We, too, will use it in certain chapters. You can find the installation instructions for <strong class="source-inline">virtualenv</strong> at <a href="https://virtualenv.pypa.io/en/latest/installation.html">https://virtualenv.pypa.io/en/latest/installation.html</a>.</p>
			<p>We will particularly need the following packages:</p>
			<ul>
				<li><strong class="bold">Python 3.7</strong>: Python is<a id="_idIndexMarker073"/> the <em class="italic">lingua franca</em> of data science today. We will use version 3.7.</li>
				<li><strong class="bold">NumPy</strong>: This is one of <a id="_idIndexMarker074"/>the most fundamental libraries used in scientific computing in Python.</li>
				<li><strong class="bold">pandas</strong>: <strong class="source-inline">pandas</strong> is a <a id="_idIndexMarker075"/>widely used library that provides powerful data structures and analysis tools.</li>
				<li><strong class="bold">Jupyter Notebook</strong>: This <a id="_idIndexMarker076"/>is a very convenient tool to run Python code, especially for small-scale tasks. It usually comes with your Anaconda installation by default.</li>
				<li><strong class="bold">TensorFlow 2.x</strong>: This will be <a id="_idIndexMarker077"/>our choice as the deep learning framework. We use version 2.3.0 in this book. Occasionally, we will refer to repos that use TensorFlow 1.x as well.</li>
				<li><strong class="bold">Ray and RLlib</strong>: Ray <a id="_idIndexMarker078"/>is a framework for building and running distributed applications and<a id="_idIndexMarker079"/> it is getting increasingly popular. RLlib is a library running on Ray that includes many popular RL algorithms. At the time of writing this book, Ray supports only Linux and macOS for production, and Windows support is in the alpha phase. We will use version 0.8.7. </li>
				<li><strong class="bold">gym</strong>: This is an RL<a id="_idIndexMarker080"/> framework created by OpenAI that you have probably interacted with before if you have ever touched RL. It allows us to define RL environments in a standard way and let them communicate with algorithms in packages such as RLlib.</li>
				<li><strong class="bold">OpenCV Python bindings</strong>: We <a id="_idIndexMarker081"/>need this for some image processing tasks.</li>
				<li><strong class="bold">Plotly</strong>: This is a<a id="_idIndexMarker082"/> very convenient library for data visualization. We will use Plotly together with the <strong class="source-inline">Cufflinks</strong> package to bind it to <strong class="source-inline">pandas</strong>.</li>
			</ul>
			<p>You can use one of the following commands on your terminal to install a specific package. With Anaconda, we use the following command:</p>
			<p class="source-code">conda install pandas==0.20.3</p>
			<p>With <strong class="source-inline">virtualenv</strong> (also works with Anaconda in most cases), we use the following command: </p>
			<p class="source-code">pip install pandas==0.20.3</p>
			<p>Sometimes, you are flexible with the version of the package, in which case you can omit the equals sign and <a id="_idIndexMarker083"/>what comes after. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It is always a good idea to create a virtual environment specific to your experiments for this book and install all these packages in that environment. This way, you will not break dependencies for your other Python projects. There is comprehensive online documentation on how to manage your environments provided by Anaconda, available at <a href="https://bit.ly/2QwbpJt">https://bit.ly/2QwbpJt</a>.</p>
			<p>That's it! With that, you are ready to start coding RL!</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Summary</h1>
			<p>This was our refresher on RL fundamentals! We began this chapter by discussing what RL is, and why it is such a hot topic and the next frontier in AI. We talked about some of the many possible applications of RL and the success stories that have made it to the news headlines over the past several years. We also defined the fundamental concepts we will use throughout the book. Finally, we covered the hardware and software you need to run the algorithms we will introduce in the next chapters. Everything so far was to refresh your mind about RL, motivate, and set you up for what is upcoming next: implementing advanced RL algorithms to solve challenging real-world problems. In the next chapter, we will dive right into it with multi-armed bandit problems, an important class of RL algorithms that has many applications in personalization and advertising. </p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>References</h1>
			<ol>
				<li>Sutton, R. S., Barto, A. G. (2018). Reinforcement Learning: An Introduction. <em class="italic">The MIT Press</em>.</li>
				<li>Tesauro, G. (1992). Practical issues in temporal difference learning. <em class="italic">Machine Learning 8, 257–277</em>.</li>
				<li>Tesauro, G. (1995). Temporal difference learning and TD-Gammon. <em class="italic">Commun. ACM 38, 3, 58-68</em>. </li>
				<li>Silver, D. (2018). Success Stories of Deep RL. Retrieved from <a href="https://youtu.be/N8_gVrIPLQM">https://youtu.be/N8_gVrIPLQM</a>.</li>
				<li>Crites, R. H., Barto, A.G. (1995). Improving elevator performance using reinforcement learning. <em class="italic">In Proceedings of the 8th International Conference on Neural Information Processing Systems (NIPS'95)</em>.</li>
				<li>Mnih, V. et al. (2015). Human-level control through deep reinforcement learning. <em class="italic">Nature, 518(7540), 529–533</em>.</li>
				<li>Silver, D. et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. <em class="italic">Science, 362(6419), 1140–1144</em>.</li>
				<li>Vinyals, O. et al. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. <em class="italic">Nature</em>.</li>
				<li>OpenAI. (2018). OpenAI Five. Retrieved from <a href="https://blog.openai.com/openai-five/">https://blog.openai.com/openai-five/</a>.</li>
				<li>Heess, N. et al. (2017). Emergence of Locomotion Behaviours in Rich Environments. <em class="italic">ArXiv, abs/1707.02286</em>.</li>
				<li>OpenAI et al. (2018). Learning Dexterous In-Hand Manipulation. <em class="italic">ArXiv, abs/1808.00177</em>.</li>
				<li>OpenAI et al. (2019). Solving Rubik's Cube with a Robot Hand. <em class="italic">ArXiv, abs/1910.07113</em>.</li>
				<li>OpenAI Blog (2019). Solving Rubik's Cube with a Robot Hand. Retrieved from <a href="https://openai.com/blog/solving-rubiks-cube/">https://openai.com/blog/solving-rubiks-cube/</a>.</li>
				<li>Zheng, G. et al. (2018). DRN: A Deep Reinforcement Learning Framework for News Recommendation. <em class="italic">In Proceedings of the 2018 World Wide Web Conference (WWW '18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 167–176. DOI:</em> <a href="https://doi.org/10.1145/3178876.3185994">https://doi.org/10.1145/3178876.3185994</a>.</li>
				<li>Chandrashekar, A. et al. (2017). Artwork Personalization at Netflix. <em class="italic">The Netflix Tech Blog</em>. Retrieved from <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76</a>.</li>
				<li>McKinney, S. M. et al. (2020). International evaluation of an AI system for breast cancer screening. <em class="italic">Nature, 89-94</em>.</li>
				<li>Agrawal, R. (2018, March 8). <em class="italic">Microsoft News Center India</em>. Retrieved from <a href="https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/">https://news.microsoft.com/en-in/features/microsoft-ai-network-healthcare-apollo-hospitals-cardiac-disease-prediction/</a>.</li>
			</ol>
		</div>
	</body></html>