<html><head></head><body>
<div id="_idContainer110">
<h1 class="chapter-number" id="_idParaDest-165"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-166"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.2.1">Explaining Neural Network Predictions</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Have you ever wondered why a facial recognition system flagged a photo of a person with a darker skin tone as a false positive while identifying people with lighter skin tones correctly? </span><span class="koboSpan" id="kobo.3.2">Or why a self-driving car decided to swerve and cause an accident, instead of braking and avoiding the collision? </span><span class="koboSpan" id="kobo.3.3">These questions illustrate the importance of understanding why a model predicts a certain value for critical use cases. </span><span class="koboSpan" id="kobo.3.4">By providing explanations for a model’s predictions, we can gain insights into how the model works and why it made a specific decision, which is crucial for transparency, accountability, trust, regulatory compliance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">improved performance.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will explore neural network-specific methods for explaining model predictions. </span><span class="koboSpan" id="kobo.5.2">Additionally, we will discuss how to quantify the quality of an explanation method. </span><span class="koboSpan" id="kobo.5.3">We will also discuss the challenges and limitations of model explanations and how to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">their effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">Specifically, the following topics will </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">be covered:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Exploring the value of </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">prediction explanations</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Demystifying prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">explanation techniques</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Exploring gradient-based </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">prediction explanations</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Trusting and understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">integrated gradients</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Using integrated gradients to aid in understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">the predictions</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Explaining prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">explanations automatically</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Exploring common pitfalls in prediction explanations and how to </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">avoid them</span></span></li>
</ul>
<h1 id="_idParaDest-167"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">This chapter includes some practical implementations in the Python programming language. </span><span class="koboSpan" id="kobo.24.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">captum</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">transformers-interpret</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">transformers</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">pytorch</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">numpy</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">The code files are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11"><span class="No-Break"><span class="koboSpan" id="kobo.34.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.35.1">.</span></span></p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.36.1">Exploring the value of prediction explanations</span></h1>
<p><span class="koboSpan" id="kobo.37.1">First off, the concept </span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.38.1">of explaining a model through its predictions is referred to by many other names, including explainable AI, trustable AI, transparent AI, interpretable machine learning, responsible AI, and ethical AI. </span><span class="koboSpan" id="kobo.38.2">Here, we will refer to the paradigm as </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">prediction explanations</span></strong><span class="koboSpan" id="kobo.40.1">, which is a clear and short way to refer </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">to it.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.42.1">Prediction explanations</span></strong><span class="koboSpan" id="kobo.43.1"> is not a technique that is adopted by most machine learning practitioners. </span><span class="koboSpan" id="kobo.43.2">The value of prediction explanations highly depends on the exact use case. </span><span class="koboSpan" id="kobo.43.3">Even though it is stated that explanations can increase transparency, accountability, trust, regulatory compliance, and improved model performance, not everybody cares about these points. </span><span class="koboSpan" id="kobo.43.4">Instead of understanding the benefits, let’s look at it from a different perspective and explore some of the common factors that drove practitioners to adopt prediction explanations that can be attributed to the </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">following conditions:</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">The prediction explanations technique provides the following benefits regarding the utilization of your </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">built model:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Transparency</span></strong><span class="koboSpan" id="kobo.48.1">: Prediction</span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.49.1"> explanations allow model prediction consumers to have access to reasons, which will, in turn, enforce the confidence of the consumer in the predictions made. </span><span class="koboSpan" id="kobo.49.2">A transparent model allows consumers to objectively gauge their intelligence, which increases trust and </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">increases adoption.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">Accountability</span></strong><span class="koboSpan" id="kobo.52.1">: Prediction explanations allow consumers to perform root cause analysis, which is especially important in critical use cases or in use cases where there is a human in the loop to make the final decision with a model’s prediction as </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">a reference.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Trust</span></strong><span class="koboSpan" id="kobo.55.1">: Without trust, nobody will use the prediction of the model. </span><span class="koboSpan" id="kobo.55.2">Prediction explanations provide a small boost toward achieving </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">higher trust.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Regulatory compliance</span></strong><span class="koboSpan" id="kobo.58.1">: Some governments enforce laws that require decisions made by computer systems to be explainable in certain industries such as banks </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">and insurance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Metric performance</span></strong><span class="koboSpan" id="kobo.61.1">: The capability to perform root cause analysis can lead to a better understanding of either the model’s behavior or the training dataset. </span><span class="koboSpan" id="kobo.61.2">This will, in turn, allow machine learning practitioners to improve or fix the issues found using prediction explanations, and eventually lead to improved </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">metric performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.63.1">This list of benefits makes it worth it to utilize prediction explanations on any use case and model. </span><span class="koboSpan" id="kobo.63.2">However, the value of using prediction explanations increases exponentially with certain conditions. </span><span class="koboSpan" id="kobo.63.3">When the utilization is based on a specific goal, the method becomes twice as useful. </span><span class="koboSpan" id="kobo.63.4">Let’s take a step further and explore some of the common conditions that drove practitioners to utilize the prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">explanations technique:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Critical and high-impact use case</span></strong><span class="koboSpan" id="kobo.66.1">: In these use cases, the model’s decisions can typically result in significant consequences for human welfare, safety, or well-being. </span><span class="koboSpan" id="kobo.66.2">Understanding the model’s behavior in every way can help mitigate the worst case from happening. </span><span class="koboSpan" id="kobo.66.3">This can range from billions of money lost to actual human </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">life lost.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Failing to achieve the required threshold for success</span></strong><span class="koboSpan" id="kobo.69.1">: A machine learning project can’t move forward to the model deployment stage in the machine learning life cycle if it can’t even achieve success thresholds during the model development stage. </span><span class="koboSpan" id="kobo.69.2">Understanding the behavior of the model toward different inputs can </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.70.1">help signal whether the data is of bad quality, help indicate whether the data has biased patterns that promote overfitting, and generally help debug how to improve the </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">model’s performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">The model makes wrong predictions with the simplest examples</span></strong><span class="koboSpan" id="kobo.73.1">: Making wrong predictions with the most complex examples is expected, especially when humans also could have made mistakes in making the same decisions. </span><span class="koboSpan" id="kobo.73.2">When it comes to the simplest examples, making an error would indicate that the model is not learning the right things. </span><span class="koboSpan" id="kobo.73.3">Understanding what a model is focusing on could be key to figuring out why a </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">model failed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Regulatory laws mandate accountability for using machine learning models in decision-making</span></strong><span class="koboSpan" id="kobo.76.1">: This means prediction explanations will be required not typically as a method for the machine learning practitioner to understand how the model behaves or with different inputs, but instead, to be used after a model is deployed, which will allow the people that consume the predictions understand why a certain decision has been made </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">for accountability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">The model fails to make proper predictions after it is deployed</span></strong><span class="koboSpan" id="kobo.79.1">: Have you ever thought about whether a good model with high accuracy metric performance in your cross-validation setup means that the model pays attention to the right things? </span><span class="koboSpan" id="kobo.79.2">Even with cross-validation partitioning strategies, data can still be very biased toward a certain set of conditions. </span><span class="koboSpan" id="kobo.79.3">This means that when the model gets deployed into the real world, the data it encounters during the production operation can be from a distribution and set of conditions that are different than what was available in the original dataset used for model development. </span><span class="koboSpan" id="kobo.79.4">When this happens, prediction explanations can help uncover biases and unwanted behaviors of the model either in the new data or data used for model development. </span><span class="koboSpan" id="kobo.79.5">In other words, you’ll need to quantify wrong predictions from </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">right predictions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.81.1">It is fine to not utilize prediction explanation techniques in your use case and development workflow. </span><span class="koboSpan" id="kobo.81.2">However, when these edge cases happen, know that prediction explanation techniques are your key tool to help overcome your obstacles. </span><span class="koboSpan" id="kobo.81.3">But what exactly do prediction </span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.82.1">explanations explain? </span><span class="koboSpan" id="kobo.82.2">In the next section, we will discuss a short overview of prediction explanation methods and dive into the method category we will be introducing in this chapter, which is specifically for </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">neural networks.</span></span></p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.84.1">Demystifying prediction explanation techniques</span></h1>
<p><span class="koboSpan" id="kobo.85.1">Prediction explanations</span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.86.1"> is a technique that attempts to explain the logic behind a model’s decision, given input data. </span><span class="koboSpan" id="kobo.86.2">Some machine learning models are built to be more transparent and explainable out of the box. </span><span class="koboSpan" id="kobo.86.3">One example is a decision tree model, which is built from the ground up using explicit conditioning rules to split data into multiple partitions that result in specific decisions, allowing the predictions to be explained through the explicit rules that were used to predict the data sample. </span><span class="koboSpan" id="kobo.86.4">However, models such as neural networks are treated like a black box without any straightforward way to retrieve the reasons the decision was </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">made directly.</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">The logic of a model’s decision on a data sample can be explained and presented in a variety of ways, so long it contains information on how the final decision was made. </span><span class="koboSpan" id="kobo.88.2">Additionally, predictions made by a machine learning model can be explained in either a model-agnostic or model-specific way. </span><span class="koboSpan" id="kobo.88.3">There are a few types of explanations that can be made using the </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">model’s predictions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Saliency-based explanation</span></strong><span class="koboSpan" id="kobo.91.1">: This is also known as importance attribution, feature importance, or </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">feature impact</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Transparent model logic-based explanation</span></strong><span class="koboSpan" id="kobo.94.1">: Provide a rationale on why a decision </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">is made</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Exemplar-based explanation</span></strong><span class="koboSpan" id="kobo.97.1">: Using similar data to reason why a label </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">is predicted</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.99.1">Neural networks, being a black box out of the box, can only be explained through saliency-based explanation or exemplar-based explanation. </span><span class="koboSpan" id="kobo.99.2">However, workarounds have been invented to achieve indirect transparent model logic-based explanations for neural networks using knowledge distillation methods from a neural network to more transparent and interpretable models such as linear models. </span><span class="koboSpan" id="kobo.99.3">Additionally, attention mechanisms implemented in transformers provide a shallow way to check for feature importance through its attention maps as a saliency method but fall short in the </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">following components:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.101.1">Attention weights are not feature-specific as they are obtained by calculating the interactions between all input tokens and the output token of interest. </span><span class="koboSpan" id="kobo.101.2">This means the weight does not represent a true reliance on an </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">isolated feature.</span></span></li>
<li><span class="koboSpan" id="kobo.103.1">The inputs attended to by the attention mechanism can still not be used in later parts of </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">the network.</span></span></li>
<li><span class="koboSpan" id="kobo.105.1">Attention maps can be biased and cause some features to </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">be neglected.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.107.1">Additionally, neural-networks-based explanation techniques should be opted over model-agnostic explanation techniques as they are capable of providing more detailed and nuanced explanations while being more efficient in terms of both computational requirements and time. </span><span class="koboSpan" id="kobo.107.2">In this chapter, we will focus on saliency-based explanations for neural networks using more</span><a id="_idIndexMarker848"/> <span class="No-Break"><span class="koboSpan" id="kobo.108.1">reliable methods.</span></span></p>
<p><span class="koboSpan" id="kobo.109.1">In the next section, we will dive into the core workhorse behind neural network model-specific explanations, which are gradient-based </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">saliency explanations.</span></span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.111.1">Exploring gradient-based prediction explanations</span></h1>
<p><span class="koboSpan" id="kobo.112.1">Most up-to-date </span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.113.1">neural network-based explanation techniques today are variations of using the gradients that can be obtained through backpropagation. </span><span class="koboSpan" id="kobo.113.2">Gradient-based explanations for neural network models work because they rely on the fundamental principle of how the weights in a neural network are updated during the training process using backpropagation. </span><span class="koboSpan" id="kobo.113.3">During backpropagation, the partial derivatives of the loss function concerning the weights in the network are calculated, which gives us the gradient of the loss function concerning </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">the weights.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">This gradient provides us with a measure of how much the input data contributes to the overall loss. </span><span class="koboSpan" id="kobo.115.2">Remember that gradients measure the sensitivity of the input value concerning the loss function. </span><span class="koboSpan" id="kobo.115.3">This means it provides the degree of fluctuation of the predictions when you modify the specific input value, which represents the importance of the input data. </span><span class="koboSpan" id="kobo.115.4">Input data can be chosen to be the weights of the neural network or the actual input to the entire neural network. </span><span class="koboSpan" id="kobo.115.5">In most cases, the actual input-based explanations are enough to provide clarity on the important feature groups. </span><span class="koboSpan" id="kobo.115.6">However, sometimes, a more fine-grained explanation is needed to decode the underlying characteristics of the highly attributed and important actual </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">input data.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">For example, consider the use case of identifying animal breeds with image data using a CNN. </span><span class="koboSpan" id="kobo.117.2">For an image with a dog standing on grass, if an attribution method signals that both the dog and grass are important, why is the grass important? </span><span class="koboSpan" id="kobo.117.3">This can be for one of the </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">following reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.119.1">The fur of the dog and the grass are both identified by a single </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">convolutional filter.</span></span></li>
<li><span class="koboSpan" id="kobo.121.1">Fur and grass are explicitly separately identified by different filters but shown as important similarly because the model might have overfitted to think that dogs need to be accompanied by grass. </span><span class="koboSpan" id="kobo.121.2">This might mean that the training dataset only contains images of dogs with grass and signals the need to add more dog images </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">without grass.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.123.1">This will require diving deeper into important filters that are attributed highly among the filters that are</span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.124.1"> activated and have something to do with the grass and the dog. </span><span class="koboSpan" id="kobo.124.2">Even after pinpointing the filters that are both important, highly activated for the image, and have something to do with the grass and the dog, you can’t know for sure what patterns the filter is identifying. </span><span class="koboSpan" id="kobo.124.3">Is it a high-level feature such as the dog’s shape? </span><span class="koboSpan" id="kobo.124.4">Or is it fur? </span><span class="koboSpan" id="kobo.124.5">In cases like this, it is highly beneficial to visualize the patterns directly. </span><span class="koboSpan" id="kobo.124.6">This topic will be discussed in more detail in </span><a href="B18187_12.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.125.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.126.1">, </span><em class="italic"><span class="koboSpan" id="kobo.127.1">Interpreting </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.128.1">Neural Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">Gradients by themselves, however, are not reliable enough to be used as-is as an explanation method. </span><span class="koboSpan" id="kobo.130.2">More generally, the quality of pure gradients to be used as a feature importance explanation method is low. </span><span class="koboSpan" id="kobo.130.3">However, the quality of explanation methods is not a quantitative metric. </span><span class="koboSpan" id="kobo.130.4">The perceived quality of the explanations generated is subjective to the requirements of </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">the consumer.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">Consider a scenario where a financial institution wants to develop a credit scoring model to determine loan approvals. </span><span class="koboSpan" id="kobo.132.2">The institution is interested in using an explanation technique to understand which features are most important in the model’s predictions. </span><span class="koboSpan" id="kobo.132.3">A bank might prioritize an explanation technique that focuses on the features that have the greatest impact on creditworthiness, such as credit history, income, and outstanding debts. </span><span class="koboSpan" id="kobo.132.4">This would help the bank make informed lending decisions and manage its </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">risks effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">On the other hand, an individual loan applicant may prefer a more comprehensive explanation that provides insight into how the model evaluated their specific financial situation, including factors beyond the top three most important features, such as their employment history and recent financial hardships. </span><span class="koboSpan" id="kobo.134.2">This would help the applicant make informed decisions about how to improve their creditworthiness in the future. </span><span class="koboSpan" id="kobo.134.3">Selecting an appropriate explanation technique is important as it should cater to the audience and their specific needs to make the explanation clear and useful for its </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">intended purpose.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Even so, there are axiomatic-based evaluations, which involve evaluating based on a set of principles or axioms that are considered desirable. </span><span class="koboSpan" id="kobo.136.2">Of all techniques, a technique </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.137.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">integrated gradients</span></strong><span class="koboSpan" id="kobo.139.1"> stands out for its focus on neural network models while being</span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.140.1"> developed to satisfy widely </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">accepted axioms.</span></span></p>
<p><span class="koboSpan" id="kobo.142.1">Next, we’ll understand how integrated gradients explain a neural network prediction and understand how the method satisfies widely </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">accepted axioms.</span></span></p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.144.1">Trusting and understanding integrated gradients</span></h1>
<p><span class="koboSpan" id="kobo.145.1">First off, the integrated </span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.146.1">gradient technique is available off-the-shelf in a few open source libraries such as </span><strong class="bold"><span class="koboSpan" id="kobo.147.1">shap</span></strong><span class="koboSpan" id="kobo.148.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.149.1">captum</span></strong><span class="koboSpan" id="kobo.150.1">. </span><span class="koboSpan" id="kobo.150.2">We can utilize the method from the library without reimplementing the method. </span><span class="koboSpan" id="kobo.150.3">However, obtaining explanations without understanding the technicalities behind the technique can reduce trust in the explanations. </span><span class="koboSpan" id="kobo.150.4">If you don’t trust the technique, the explanation results themselves hardly mean anything. </span><span class="koboSpan" id="kobo.150.5">Subsequently, if you don’t explain your predictions, the prediction results themselves hardly mean anything! </span><span class="koboSpan" id="kobo.150.6">In this section, we will dive into what integrated gradients do so that you can trust what integrated gradients can explain </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">to you.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Integrated gradients add a few extra components to the basic gradient-based feature importance-based explanations. </span><span class="koboSpan" id="kobo.152.2">These components are geared toward satisfying a few critical axioms that make or break the reliability of an explanation method. </span><span class="koboSpan" id="kobo.152.3">These axioms are </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.154.1">Sensitivity</span></strong><span class="koboSpan" id="kobo.155.1">: A model that</span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.156.1"> produces two different predictions on two data samples that differ in that only a single feature should be given a non-zero importance score. </span><span class="koboSpan" id="kobo.156.2">Activation functions such as ReLU break this axiom with a zero input value as the gradients would also be reduced to zero, even when the predictions are different between the </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">two predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.158.1">Implementation invariance</span></strong><span class="koboSpan" id="kobo.159.1">: Two models that have the same performance across all possible data must produce the same importance scores. </span><span class="koboSpan" id="kobo.159.2">Assume that any external data and data received during the entire lifetime of the model </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">is included.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.161.1">Completeness</span></strong><span class="koboSpan" id="kobo.162.1">: One of the best ways to make a good-quality explanation for both human and machine learning explanations is to provide counterfactual reports. </span><span class="koboSpan" id="kobo.162.2">To evaluate model accuracy performance, having a baseline model works the same way. </span><span class="koboSpan" id="kobo.162.3">For explanation methods, this involves engineering a baseline data sample that </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.163.1">can produce a neutral prediction. </span><span class="koboSpan" id="kobo.163.2">Completeness means that the importance score for every feature column must add up to the difference in prediction score between the baseline sample and the targeted sample. </span><span class="koboSpan" id="kobo.163.3">This axiom can be more useful in some use cases, such as in regression, where the prediction is directly used as an output rather than in a multiclass setting where the predictions are only used for choosing the </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">top class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Linearity</span></strong><span class="koboSpan" id="kobo.166.1">: If you linearly combine two neural network models, which is a simple weighted addition, the explanation of the combined neural network on a data sample must be the same weighted addition of the individual explanation of the two neural network models. </span><span class="koboSpan" id="kobo.166.2">Linear behaviors that are implemented in the model should be respected. </span><span class="koboSpan" id="kobo.166.3">A straightforward axiom is specifically designed as some methods generate unaccountable </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">importance values.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.168.1">Now that we understand the core axioms that can be used to compare methods, know that integrated gradients satisfy all of these axioms. </span><span class="koboSpan" id="kobo.168.2">Simply put, integrated gradients integrate the gradients from samples that are sampled from a straight-line path (linear interpolation) from a chosen baseline data sample and the target data sample and multiply it by the difference between the predictions of the target sample and baseline sample. </span><span class="koboSpan" id="kobo.168.3">The path integral of the gradients is the area under the curve of the gradient values along the straight path. </span><span class="koboSpan" id="kobo.168.4">This value represents the change in the model output as the input feature’s value changes along the path. </span><span class="koboSpan" id="kobo.168.5">The rate of change directly translates to feature importance. </span><span class="koboSpan" id="kobo.168.6">By multiplying this value by the difference between the actual input value and the baseline input value, we get the contribution of the feature to the model output for integrated gradients. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.169.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.170.1">.1</span></em><span class="koboSpan" id="kobo.171.1"> showcases this process with the results obtained by computing integrated gradients on the predictions of a pre-trained ResNet50 model on an image of an orange. </span><span class="koboSpan" id="kobo.171.2">The model is capable of predicting the orange class and</span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.172.1"> predicts the image to be an orange with a </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">46% probability:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.174.1"><img alt="Figure 11.1 – An illustration of how integrated gradients are computed for an image of an orange with a pre-trained ResNet50 model that is capable of predicting the orange class" src="image/B18187_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.175.1">Figure 11.1 – An illustration of how integrated gradients are computed for an image of an orange with a pre-trained ResNet50 model that is capable of predicting the orange class</span></p>
<p><span class="koboSpan" id="kobo.176.1">More intuitively, this means that integrated gradients measure the rate of change of model predictions as the target input feature’s value changes toward a neutral baseline sample and provides accountable importance that is properly scaled according to </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">the predictions:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.178.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.179.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.180.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.181.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.182.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.183.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.184.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.185.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.186.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.187.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.188.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.189.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.190.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.191.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.192.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.193.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.194.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.195.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.196.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.197.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.198.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.199.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.200.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.201.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.203.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.204.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.205.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.206.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.207.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.208.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.209.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.210.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.211.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.212.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.213.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.214.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.215.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.216.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.218.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.219.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.220.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.221.1">a</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.222.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.224.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.225.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.226.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.227.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.228.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.229.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.230.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.231.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.232.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.233.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.235.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.236.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.237.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.238.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.240.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.242.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.243.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.244.1">h</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.245.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.246.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.247.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.248.1">v</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.249.1">e</span></span></span></p>
<p><span class="koboSpan" id="kobo.250.1">This allows it to capture the sensitivity of the model’s output to the feature across its entire range of values. </span><span class="koboSpan" id="kobo.250.2">Integrated gradients ensure that the contribution of each feature is proportional to its effect on the model’s output by aggregating the gradients from samples in the linear interpolated path between the baseline and target. </span><span class="koboSpan" id="kobo.250.3">Using plain gradients as the core allows integrated gradients to satisfy implementation invariance as the chain rule used to obtain partial derivatives allows gradients to be </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">computed partially.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">For integrated gradients, if a feature has twice the effect on the model’s output compared to another feature, its attribution score will be twice as high because the gradients along the path will be twice as high for that feature, thus satisfying linearity. </span><span class="koboSpan" id="kobo.252.2">Additionally, since the importance is scaled by the difference in predictions between the baseline input and target input, integrated gradients will satisfy the completeness axiom. </span><span class="koboSpan" id="kobo.252.3">These are local explanations that provide reasons for predictions through individual data samples. </span><span class="koboSpan" id="kobo.252.4">As a bonus, by aggregating the local feature importance of all data samples through</span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.253.1"> mean or median, you can obtain the global feature importance of </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Next, we will attempt to use integrated gradients to explain predictions from </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">a model.</span></span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.257.1">Using integrated gradients to aid in understanding predictions</span></h1>
<p><span class="koboSpan" id="kobo.258.1">At the time of </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.259.1">writing, two packages provide easy-to-use classes and methods to compute integrated gradients, which are the </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">captum</span></strong><span class="koboSpan" id="kobo.261.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.262.1">shap</span></strong><span class="koboSpan" id="kobo.263.1"> libraries. </span><span class="koboSpan" id="kobo.263.2">In this tutorial, we will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.264.1">captum</span></strong><span class="koboSpan" id="kobo.265.1"> library. </span><span class="koboSpan" id="kobo.265.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">captum</span></strong><span class="koboSpan" id="kobo.267.1"> library supports models from TensorFlow and PyTorch. </span><span class="koboSpan" id="kobo.267.2">We will be using PyTorch here. </span><span class="koboSpan" id="kobo.267.3">In this tutorial, we will be working on explaining a SoTA transformer model</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.268.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.269.1">DeBERTA</span></strong><span class="koboSpan" id="kobo.270.1"> on the task of text sentiment multiclass classification. </span><span class="koboSpan" id="kobo.270.2">Let’s go through the use case step </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">by step:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.272.1">First, let’s import the necessary libraries </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">and methods:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.274.1">
 from transformers import (
    DebertaForSequenceClassification,
    EvalPrediction,
    DebertaConfig,
    DebertaTokenizer,
    Trainer,
    TrainingArguments,
    IntervalStrategy,
    EarlyStoppingCallback
)
import pandas as pd
from sklearn.model_selection import train_test_split
import torch</span></pre></li> <li><span class="koboSpan" id="kobo.275.1">Next, we will import</span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.276.1"> a custom text sentiment dataset made for </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">this tutorial:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.278.1">
df = pd.read_csv('text_sentiment_dataset.csv')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.279.1">This data contains 100 rows with a balanced distribution between three sentiment classes called “neutral”, “positive,” and “negative.” </span><span class="koboSpan" id="kobo.279.2">We will dive into a few samples of this </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">dataset later.</span></span></p></li> <li><span class="koboSpan" id="kobo.281.1">Now, we will define the label mapping that will be used to map the sentiment labels to a numeric ID and </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">vice versa:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.283.1">
label2id = {"negative": 0, "neutral": 1, "positive": 2}
id2label = {v: k for k, v in label2id.items()}</span></pre></li> <li><span class="koboSpan" id="kobo.284.1">Next, we will create a method that applies text pre-processing that tokenizes the text data using a pre-trained, </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">byte-pair-encoding-based tokenizer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.286.1">
tokenizer = DebertaTokenizer.from_pretrained("microsoft/deberta-base")
def preprocess_function(examples):
    inputs = tokenizer(examples["Text"].values.tolist(), padding="max_length", truncation=True)
    inputs["labels"] = [label2id[label] for label in examples["Sentiment"].values]
    return inputs</span></pre></li> <li><span class="koboSpan" id="kobo.287.1">To train a model, we need to have a cross-validation strategy, so we will use a simple train and validation </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">split here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.289.1">
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.290.1">Since we will be using the PyTorch model from the Hugging Face Transformers library, we need to use the PyTorch dataset format and split the data that will be used to train </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.291.1">the model. </span><span class="koboSpan" id="kobo.291.2">Here, we will define the PyTorch dataset and initialize both the training and validation </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">dataset instances:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.293.1">
class TextClassificationDataset(torch.utils.data.Dataset):
    def __init__(self, examples):
        self.examples = examples    def __getitem__(self, index):
        return {k: torch.tensor(v[index]) for k, v in self.examples.items()}    def __len__(self):
        return len(self.examples["input_ids"])train_dataset = TextClassificationDataset(preprocess_function(train_df))
test_dataset = TextClassificationDataset(preprocess_function(test_df))</span></pre></li> <li><span class="koboSpan" id="kobo.294.1">Now that the dataset has been pre-processed and is ready to be used for training, let’s</span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.295.1"> load our randomly initialized DeBERTA model from </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.297.1">
deberta_config = {
           "model_type": "deberta-v2",
           "attention_probs_dropout_prob": 0.1,
           "hidden_act": "gelu",
           "hidden_dropout_prob": 0.1,
           "hidden_size": 768,
           "initializer_range": 0.02,
           "intermediate_size": 3072,
           "max_position_embeddings": 512,
           "relative_attention": True,
           "position_buckets": 256,
           "norm_rel_ebd": "layer_norm",
           "share_att_key": True,
           "pos_att_type": "p2c|c2p",
           "layer_norm_eps": 1e-7,
           "max_relative_positions": -1,
           "position_biased_input": False,
           "num_attention_heads": 12,
           "num_hidden_layers": 12,
           "type_vocab_size": 0,
           "vocab_size": 128100
}
model_config = DebertaConfig(id2label=id2label, label2id=label2id, **deberta_v3_config)
model = DebertaForSequenceClassification(model_config)</span></pre></li> <li><span class="koboSpan" id="kobo.298.1">Since this is a multiclass setup, we will use the accuracy metric that will be computed at each epoch, along with the cross-entropy loss, which will be used to train </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.300.1">
def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}</span></pre></li> <li><span class="koboSpan" id="kobo.301.1">Now, we will </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.302.1">define the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">training arguments:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.304.1">
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1000,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy=IntervalStrategy.EPOCH,
    save_strategy=IntervalStrategy.EPOCH,
    load_best_model_at_end=True,
    learning_rate=0.000025,
    save_total_limit=2,
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.305.1">This will save the checkpoints in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">results</span></strong><span class="koboSpan" id="kobo.307.1"> folder, train the model with 100 iterations of warmup, load the best model at the end of training, and set the model learning rate to a very small number of 0.000025, which is required to properly converge to </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">a solution.</span></span></p></li> <li><span class="koboSpan" id="kobo.309.1">Next, we will initialize the Hugging Face </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">trainer</span></strong><span class="koboSpan" id="kobo.311.1"> instance, which will take in the training arguments and use them to execute the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">training process:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.313.1">
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.314.1">Notice that an </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.315.1">early stopping of 20 iterations is used. </span><span class="koboSpan" id="kobo.315.2">This stops the model from being trained when the model doesn’t improve on the validation partition for the specified iterations, along with 1,000 epochs </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">of training.</span></span></p></li> <li><span class="koboSpan" id="kobo.317.1">Finally, let’s train and print the final best model </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">evaluation score!</span></span><pre class="source-code"><span class="koboSpan" id="kobo.319.1">
trainer.train()
trainer_v2.evaluate()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.320.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">following output:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.322.1">{'eval_loss': 0.1446695625782013,
 'eval_accuracy': 0.9523809552192688,
 'eval_runtime': 19.497,
 'eval_samples_per_second': 1.077,
 'eval_steps_per_second': 0.103}</span></pre></li> <li><span class="koboSpan" id="kobo.323.1">95.23% accuracy is a pretty good score for a multiclass model. </span><span class="koboSpan" id="kobo.323.2">You might think that the model is good enough to be deployed, but is it more than what meets the eye? </span><span class="koboSpan" id="kobo.323.3">Let’s investigate the model’s behavior through prediction explanations using integrated gradients and see whether the model is performing. </span><span class="koboSpan" id="kobo.323.4">First, let’s define the explainer instance from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.324.1">transformers_interpret</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.325.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.326.1">
from transformers_interpret import SequenceClassificationExplainer
cls_explainer = SequenceClassificationExplainer(model, tokenizer)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.327.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">transformers_interpret</span></strong><span class="koboSpan" id="kobo.329.1"> library uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.330.1">captum</span></strong><span class="koboSpan" id="kobo.331.1"> library under the hood and implements methods and classes to make it straightforward to explain and visualize text importance based on models made using the Hugging Face Transformers</span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.332.1"> library. </span><span class="koboSpan" id="kobo.332.2">Among all these things, we’re mapping the importance score to a color code and mapping token IDs back to the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">token string.</span></span></p></li> <li><span class="koboSpan" id="kobo.334.1">Next, we’ll explain two samples for the negative and positive labels from the validation partition, which is conveniently located in the first </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">three indices:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.336.1">
for idx in [0, 8, 6, 12]:
    text = test_df['Text'].iloc[idx]
    label = test_df['Sentiment'].iloc[idx]
    word_attributions = cls_explainer(text)
    cls_explainer.visualize()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.337.1">This will result in the output displayed in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.338.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.339.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer109">
<span class="koboSpan" id="kobo.341.1"><img alt="Figure 11.2 – ﻿transformers_interpret-based integrated gradients results on the validation dataset" src="image/B18187_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.342.1">Figure 11.2 – transformers_interpret-based integrated gradients results on the validation dataset</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.343.1">The words </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.344.1">highlighted in green represent a positive attribution, while those highlighted in red represent a negative attribution toward the predicted label class. </span><span class="koboSpan" id="kobo.344.2">The darker the green, the stronger the positive attribution toward the predicted label class. </span><span class="koboSpan" id="kobo.344.3">The darker the red, the stronger the negative attribution toward the predicted label class </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">in general.</span></span></p>
<ol>
<li value="14"><span class="koboSpan" id="kobo.346.1">The first and second examples show correctly predicted positive sentiment sentences. </span><span class="koboSpan" id="kobo.346.2">For the first example, based on common human sense, </span><em class="italic"><span class="koboSpan" id="kobo.347.1">amazing</span></em><span class="koboSpan" id="kobo.348.1"> is the word that should contribute the most to positive attribution. </span><span class="koboSpan" id="kobo.348.2">For the second example, </span><em class="italic"><span class="koboSpan" id="kobo.349.1">happy</span></em><span class="koboSpan" id="kobo.350.1"> should be the word that is emphasized as the word contribution toward positive sentiment. </span><span class="koboSpan" id="kobo.350.2">However, for both examples, the word </span><em class="italic"><span class="koboSpan" id="kobo.351.1">bored</span></em><span class="koboSpan" id="kobo.352.1"> is used instead as a strong indicator of the positive sentiment prediction, which is not the behavior that we want. </span><span class="koboSpan" id="kobo.352.2">This indicates that the dataset has a bias where the word </span><em class="italic"><span class="koboSpan" id="kobo.353.1">bored</span></em><span class="koboSpan" id="kobo.354.1"> could be present in all samples labeled with </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">positive sentiment.</span></span></li>
<li><span class="koboSpan" id="kobo.356.1">The third and fourth examples show a correctly predicted negative sentiment. </span><span class="koboSpan" id="kobo.356.2">In the third example, the sentences </span><em class="italic"><span class="koboSpan" id="kobo.357.1">never going to end</span></em><span class="koboSpan" id="kobo.358.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.359.1">keeps dragging on</span></em><span class="koboSpan" id="kobo.360.1"> should be the focus of the negative sentiment prediction. </span><span class="koboSpan" id="kobo.360.2">In the fourth example, the word </span><em class="italic"><span class="koboSpan" id="kobo.361.1">wrong</span></em><span class="koboSpan" id="kobo.362.1"> and the phrases </span><em class="italic"><span class="koboSpan" id="kobo.363.1">has gone wrong</span></em><span class="koboSpan" id="kobo.364.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.365.1">so frustrated</span></em><span class="koboSpan" id="kobo.366.1"> should be the focus of the negative sentiment prediction. </span><span class="koboSpan" id="kobo.366.2">However, both samples consistently show dependence on the word </span><em class="italic"><span class="koboSpan" id="kobo.367.1">day</span></em><span class="koboSpan" id="kobo.368.1"> to get their negative sentiment prediction. </span><span class="koboSpan" id="kobo.368.2">This indicates that the dataset has a bias regarding the word </span><em class="italic"><span class="koboSpan" id="kobo.369.1">day</span></em><span class="koboSpan" id="kobo.370.1"> occurring frequently in the samples that are labeled with </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">negative sentiment.</span></span></li>
<li><span class="koboSpan" id="kobo.372.1">All of this means that either the data has to be prepared all over again or more data that diversifies the distribution of word usage should be added to the dataset so that a proper model can </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">be built.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.374.1">This tutorial demonstrates a single type of benefit that can be derived from utilizing prediction explanations. </span><span class="koboSpan" id="kobo.374.2">In particular, this shows the case where the model is not capturing the behavior required to be reliably deployed, even when the accuracy-based metric has a </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">good score.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">The integrated gradients technique is a flexible technique that can be applied to any neural network model for any type of input variable type. </span><span class="koboSpan" id="kobo.376.2">To benefit from the explanation results, you will need to derive meaningful insights in the context of the business goal. </span><span class="koboSpan" id="kobo.376.3">To derive meaningful insights and conclusions from the results of integrated gradients, it’s</span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.377.1"> essential to apply common sense and logical reasoning, as was presented in the tutorial manually. </span><span class="koboSpan" id="kobo.377.2">However, there is a method that you can use to try to obtain assistance in obtaining meaningful insights, especially when there is too much data and too many variations to decode manually. </span><span class="koboSpan" id="kobo.377.3">We will dive into this in the </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">next section.</span></span></p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.379.1">Explaining prediction explanations automatically</span></h1>
<p><span class="koboSpan" id="kobo.380.1">One useful method that </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.381.1">helps in deriving insights from prediction explanations is none other than </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.382.1">using </span><strong class="bold"><span class="koboSpan" id="kobo.383.1">large language models</span></strong><span class="koboSpan" id="kobo.384.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.385.1">LLMs</span></strong><span class="koboSpan" id="kobo.386.1">) such as ChatGPT. </span><span class="koboSpan" id="kobo.386.2">ChatGPT is a transformer model that is trained to provide results that match logical reasoning related to the instructions provided. </span><span class="koboSpan" id="kobo.386.3">The theory here is that if you can format your prediction explanations data in a way that it can be fed into a transformer model, and instruct the LLM to derive insights from it, you will be able to obtain insights from multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">different perspectives.</span></span></p>
<p><span class="koboSpan" id="kobo.388.1">In the previous tutorial, we attempted to explain the explanations of four different samples consisting of two correctly predicted positive sentiment examples and two correctly predicted negative sentiment examples. </span><span class="koboSpan" id="kobo.388.2">Now, let’s use an LLM model to gain insights. </span><span class="koboSpan" id="kobo.388.3">Here, we will separately generate insights for the two correctly identified positive sentiment and two correctly identified negative sentiment examples as it somehow tends to provide more reasonable results compared to just plugging in all four examples. </span><span class="koboSpan" id="kobo.388.4">We used the </span><a id="_idIndexMarker870"/><span class="No-Break"><span class="koboSpan" id="kobo.389.1">following prompt:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.390.1">
"I have built a sentiment classification model using HuggingFace defined classes, with 3 labels "neutral", "positive", "negative". </span><span class="koboSpan" id="kobo.390.2">I also utilized the integrated gradients method to obtain token attributions for the examples below: First Example with correctly predicted positive sentiment [('[CLS]', 0.0), ('I', 0.04537756915960333), ('was', 0.32506422578050986), ('hesitant', -0.011328562581410014), ('to', 0.2591512168119563), ('try', 0.07210600939837246), ('that', 0.1872696259797412), ('new', 0.25720454853718405), ('restaurant', 0.09906425532043227), (',', -0.08526821845533564), ('but', 0.26155394517459857), ('it', 0.029711535733103797), ('ended', 0.03380972261608883), ('up', 0.10830705227933887), ('being', 0.09771634151976362), ('amazing', 0.08913346633282519), ('and', 0.1429717885722044), ('I', 0.3408664658447834), ('was', 0.3248006076552883), ('far', 0.11463247647745932), ('from', 0.29795241654484333), ('bored', 0.4944594024849995), ('.', 0.10894853119482743), ('', 0.0376325816741505), ('[SEP]', 0.0)] Second example with correctly predicted positive sentiment [('[CLS]', 0.0), ('I', 0.00293608614786346), ("'m", 0.30945414416090317), ('happy', -0.028173800615734133), ('to', 0.31870826361945737), ('have', 0.15675763188868433), ('a', 0.07581024366375723), ('job', -0.020856788304256305), ('that', 0.20868353147138577), ('keeps', 0.2024940149527261), ('me', 0.046306584007035426), ('from', 0.41459097144711715), ('getting', 0.059558828031696716), ('bored', 0.7089862879353616), ('.', 0.0744570784079903), ('', -0.02775527287562979), ('[SEP]', 0.0)],  Tell me anything wrong about this model to predict positive and negative attributions. </span><span class="koboSpan" id="kobo.390.3">Provide me a complete analysis and see if the most highly attributed words make sense for its predicted label. </span><span class="koboSpan" id="kobo.390.4">Judge the models based on the relative score and focus on the highest scored token individually and see whether it make sense."</span></pre> <p><span class="koboSpan" id="kobo.391.1">Note that the list of word tokens and attribution tuples are the contents of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">word_attributions</span></strong><span class="koboSpan" id="kobo.393.1"> variable in the previous tutorial. </span><span class="koboSpan" id="kobo.393.2">For this prompt, the ChatGPT model returned the</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.394.1"> following response </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">without modification:</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.396.1">Based on the given examples, it seems that the model is performing well in predicting positive sentiment, as the predicted label is correct in both cases. </span><span class="koboSpan" id="kobo.396.2">However, it is important to note that the token attributions should be interpreted with caution, as they are not always a reliable indicator of the importance of each word in </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.397.1">the sentence.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.398.1">Looking at the token attributions for the first example, we can see that the highest attributed word is “bored” with a score of 0.494. </span><span class="koboSpan" id="kobo.398.2">This might seem odd for a positive sentiment sentence, but it is possible that the model is attributing high importance to this word because it is negating a negative sentiment that was initially present. </span><span class="koboSpan" id="kobo.398.3">The model might have also learned that overcoming boredom is something that people consider to be positive. </span><span class="koboSpan" id="kobo.398.4">The second highest attributed words are “I” and “was”, both with scores above 0.3, which is reasonable since they indicate the writer’s personal experience and opinion. </span><span class="koboSpan" id="kobo.398.5">The other highly attributed words such as “to”, “from”, and “new” also seem to make sense in the context of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.399.1">the sentence.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.400.1">Looking at the token attributions for the second example, we can see that the highest attributed word is “bored” again, with a score of 0.709. </span><span class="koboSpan" id="kobo.400.2">This could be because the model is placing a lot of emphasis on the fact that the job keeps the writer from getting bored, which is seen as a positive thing. </span><span class="koboSpan" id="kobo.400.3">The second highest attributed word is “from” with a score of 0.415, which makes sense in the context of the sentence since it is indicating what the job is keeping the writer from doing. </span><span class="koboSpan" id="kobo.400.4">Other highly attributed words such as “to” and “that” also seem to make sense in the context of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.401.1">the sentence.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.402.1">Overall, while the model is performing well in predicting positive sentiment, it is important to note that token attributions can be noisy and should be interpreted with caution. </span><span class="koboSpan" id="kobo.402.2">The highly attributed words seem to make sense in the context of the sentences, but it is important to look at the entire sentence and not just individual words to fully understand </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.403.1">the sentiment.</span></strong></span></p>
<p><span class="koboSpan" id="kobo.404.1">This response provides a different take on our original insight and mentions the possibility that nothing is wrong with some skepticism. </span><span class="koboSpan" id="kobo.404.2">Overall, this is a nice response that gives us a different perspective on what could happen and signals that an LLM can be used to explain the results of prediction explanation as an assistance tool. </span><span class="koboSpan" id="kobo.404.3">Now, let’s replace the attribution results with the two examples that are correctly identified as negative sentiments </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.405.1">with the </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">following list:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.407.1">
Third example with correctly predicted negative sentiment
[('[CLS]', 0.0), ('It', 0.1781266370024602), ('feels', -0.08200280620716707), ('like', -0.03204718164120957), ('this', -0.05573060889100964), ('day', 0.6176120940811932), ('is', -0.0032201843949800626), ('never', 0.20518569446424909), ('going', 0.07198217726867306), ('to', 0.45876287617527467), ('end', -0.2538220352877727), ('.', -0.01766876291225962), ('Everything', 0.2927810551003227), ('just', 0.27789343264100713), ('keeps', 0.25860811147952395), ('dragging', 0.013653081036146358), ('on', 0.12974700461873728), ('.', -0.01804386178839869), ('', 0.07940230083266143), ('[SEP]', 0.0)]
Fourth example with correctly predicted negative sentiment [('[CLS]', 0.0), ('Everything', 0.051921060773658365), ('that', 0.1853413631332932), ('could', 0.16644341050771988), ('have', 0.09960723832267326), ('gone', 0.21400474006976702), ('wrong', 0.26952621495013096), (',', -0.02550525756341266), ('has', 0.46511206263887517), ('gone', 0.21385297386310367), ('wrong', 0.2691861963676513), ('this', 0.08573245861889027), ('day', 0.5967441822322336), ('.', 0.00910580036050777), ('I', 0.16215574709799985), ("'m", -0.05665909043004983), ('so', 0.20818789342264885), ('frustrated', 0.11472844834437185), ('.', 0.00924431805079032), ('', 0.1485924275199782), ('[SEP]', 0.0)]</span></pre> <p><span class="koboSpan" id="kobo.408.1">The results of the ChatGPT model provided the </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">following response:</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.410.1">From the provided examples, it seems like the model is correctly predicting the negative sentiment in both cases. </span><span class="koboSpan" id="kobo.410.2">However, there are a few things that could be analyzed to determine whether the model is performing well </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.411.1">or not.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.412.1">Firstly, the token attributions provided by the integrated gradients method indicate the contribution of each token towards the final prediction of the model. </span><span class="koboSpan" id="kobo.412.2">In both examples, the tokens with the highest positive attribution scores are “day” and “has gone wrong”, which seems to make sense as they are indicative of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.413.1">negative sentiment.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.414.1">However, one issue with the provided token attributions is that some of the tokens have negative attribution scores, which means that they are contributing negatively towards the final prediction. </span><span class="koboSpan" id="kobo.414.2">For example, in the first example, the token “feels” has a negative attribution score, but it could be argued that it is actually indicative of negative sentiment. </span><span class="koboSpan" id="kobo.414.3">Similarly, in the second example, the token “’m” has a negative attribution score, but it is a contraction of “I am”, which is indicative of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.415.1">negative sentiment.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.416.1">Another thing to consider is the relative score of the highest attributed tokens. </span><span class="koboSpan" id="kobo.416.2">In the first example, the token “day” has a significantly higher attribution score than any other token, which indicates that it is the most important feature for predicting negative sentiment in this case. </span><span class="koboSpan" id="kobo.416.3">However, in the second example, there are multiple tokens with similarly high attribution scores, such as “has gone wrong” and “frustrated”, which suggests that they are both equally important features for predicting </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.417.1">negative sentiment.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.418.1">Overall, based on the provided examples and token attributions, it seems like the model is performing reasonably well in predicting negative sentiment. </span><span class="koboSpan" id="kobo.418.2">However, the analysis of the token attributions suggests that there may be some issues with the way the integrated gradients method is attributing scores to certain tokens. </span><span class="koboSpan" id="kobo.418.3">It is also important to consider the relative score and interpretability of the highest attributed tokens to determine whether they make sense in the context of the </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.419.1">predicted label.</span></strong></span></p>
<p><span class="koboSpan" id="kobo.420.1">Both of ChatGPT’s </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.421.1">responses seem to signal that it could be a valid behavior. </span><span class="koboSpan" id="kobo.421.2">Since most neural networks are configured to be non-linear models with many non-linear activations, remember that the model is capable of learning complex interactions between tokens. </span><span class="koboSpan" id="kobo.421.3">This means that tokens are mostly perceived by the model in a complex way that might make them depend on one another. </span><span class="koboSpan" id="kobo.421.4">A token that has a high attribution in the original context when applied to another context might not produce the same attribution as multiple tokens can depend on each other as a prerequisite to produce a strong attribution. </span><span class="koboSpan" id="kobo.421.5">Additionally, tokens are not restricted to interacting with tokens that are directly beside them and can be in any position of a text row. </span><span class="koboSpan" id="kobo.421.6">One way to verify whether the token is overfitted is by predicting on text data that only has the </span><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">day</span></strong><span class="koboSpan" id="kobo.423.1"> token to see whether it will predict it as a negative sentiment and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">bored</span></strong><span class="koboSpan" id="kobo.425.1"> token to see whether it will predict it as a </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">positive sentiment:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.427.1">
from transformers import TextClassificationPipeline
full_model = TextClassificationPipeline(model=model, tokenizer=tokenizer)
print(full_model("day"))
print(full_model("day day"))
print(full_model("day day day"))
print(full_model("day day day day"))
print(full_model("bored"))
print(full_model("bored bored"))
print(full_model("bored bored bored"))
print(full_model("bored bored bored bored"))</span></pre> <p><span class="koboSpan" id="kobo.428.1">This will return</span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.429.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">following response:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.431.1">
[{'label': 'neutral', 'score': 0.9857920408248901}]
[{'label': 'negative', 'score': 0.5286906361579895}]
[{'label': 'negative', 'score': 0.9102224111557007}]
[{'label': 'negative', 'score': 0.9497435688972473}]
[{'label': 'neutral', 'score': 0.9852404594421387}]
[{'label': 'positive', 'score': 0.8067558407783508}]
[{'label': 'positive', 'score': 0.974130392074585}]
[{'label': 'positive', 'score': 0.9776705503463745}]</span></pre> <p><span class="koboSpan" id="kobo.432.1">It is interesting to see that a single token for both </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">day</span></strong><span class="koboSpan" id="kobo.434.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">bored</span></strong><span class="koboSpan" id="kobo.436.1"> will result in the prediction being a neutral sentiment. </span><span class="koboSpan" id="kobo.436.2">The more repetition tokens you add, the more the prediction will skew toward negative and positive sentiment, respectively. </span><span class="koboSpan" id="kobo.436.3">This proves that the model is indeed biased toward these specific words and is not using the words in the right way, as ChatGPT said. </span><span class="koboSpan" id="kobo.436.4">Note that it could very much be what ChatGPT predicted for </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">your case.</span></span></p>
<p><span class="koboSpan" id="kobo.438.1">This tutorial showcases the explanations of text-based neural networks. </span><span class="koboSpan" id="kobo.438.2">However, it can be applied similarly to other data types, such as numerical data, categorical data, or any data that can be reliably represented in text format. </span><span class="koboSpan" id="kobo.438.3">Specifically for image-based prediction explanations, you can use models such as GPT-4 from OpenAI, which accepts image data along with text data. </span><span class="koboSpan" id="kobo.438.4">You can also consider using the visualization from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">transformers_interpret</span></strong><span class="koboSpan" id="kobo.440.1"> library as an image and feed it </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">into GPT-4!</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">Next, we will dive </span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.443.1">into common pitfalls when trying to explain your predictions and recommendations to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">these pitfalls.</span></span></p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.445.1">Exploring common pitfalls in prediction explanations and how to avoid them</span></h1>
<p><span class="koboSpan" id="kobo.446.1">Although prediction </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.447.1">explanations have proven to be valuable </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.448.1">tools in understanding AI models, several common pitfalls can hinder their effectiveness. </span><span class="koboSpan" id="kobo.448.2">In this section, we will discuss these pitfalls and provide strategies to avoid them, ensuring that prediction explanations remain a valuable resource for understanding and improving AI models. </span><span class="koboSpan" id="kobo.448.3">Some of the common pitfalls, along with their solutions, are </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.450.1">Over-reliance on explanations</span></strong><span class="koboSpan" id="kobo.451.1">: While prediction explanations can provide valuable insights into a model’s decision-making process, over-relying on these explanations can lead to incorrect conclusions. </span><span class="koboSpan" id="kobo.451.2">It’s important to remember that prediction explanations are just one piece of the puzzle and should be used in conjunction with other evaluation methods to gain a comprehensive understanding of a model’s performance. </span><span class="koboSpan" id="kobo.451.3">The solution here is to use a combination of evaluation methods, including performance metrics, cross-validation, and expert domain knowledge, to analyze and validate a </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">model’s performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.453.1">Misinterpreting explanation results</span></strong><span class="koboSpan" id="kobo.454.1">: Interpreting prediction explanation results can be challenging, particularly when dealing with complex models and large datasets. </span><span class="koboSpan" id="kobo.454.2">Misinterpretation of these results can lead to incorrect conclusions about a model’s behavior and performance. </span><span class="koboSpan" id="kobo.454.3">The solution here is to collaborate with domain experts to help interpret explanation results and ensure that conclusions drawn from these explanations align with real-world knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">and expectations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.456.1">Ignoring model limitations</span></strong><span class="koboSpan" id="kobo.457.1">: Prediction explanations can provide valuable insights into a model’s decision-making process, but they cannot address the inherent limitations of the model itself. </span><span class="koboSpan" id="kobo.457.2">It’s essential to acknowledge and address these </span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.458.1">limitations </span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.459.1">to ensure that the model performs optimally. </span><span class="koboSpan" id="kobo.459.2">The solution here is to conduct thorough model evaluations to identify and address any limitations, such as overfitting, underfitting, or biased training data. </span><span class="koboSpan" id="kobo.459.3">Continuously reevaluate and update the model as needed to maintain </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">optimal performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.461.1">Explanations for the wrong audience</span></strong><span class="koboSpan" id="kobo.462.1">: Different stakeholders may require different types of explanations based on their expertise and needs. </span><span class="koboSpan" id="kobo.462.2">Providing explanations that are too technical or too simplistic for the intended audience can hinder their understanding and use of the predictions. </span><span class="koboSpan" id="kobo.462.3">The solution is to tailor explanations to the needs and expertise of the target audience. </span><span class="koboSpan" id="kobo.462.4">Sometimes, a more global explanation of the model is needed instead of the per-prediction explanation, in which case you can consider using neural network interpretation techniques and aggregated evaluation metrics. </span><span class="koboSpan" id="kobo.462.5">For non-technical users, explaining predictions by providing raw feature importance isn’t enough and requires a natural language explanation of the insights obtained through feature importance, as introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.463.1">Explaining prediction explanations automatically</span></em><span class="koboSpan" id="kobo.464.1"> section. </span><span class="koboSpan" id="kobo.464.2">For a technical-aware user, prediction explanations can </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">be suitable.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.466.1">By being aware of these common pitfalls and implementing strategies to avoid them, practitioners can ensure that prediction explanations remain a valuable tool for understanding and improving AI models. </span><span class="koboSpan" id="kobo.466.2">By combining prediction explanations with other evaluation methods and collaborating with domain experts, it’s possible to gain a comprehensive</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.467.1"> understanding </span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.468.1">of a model’s performance, behavior, and limitations, ultimately leading to more accurate and reliable AI systems that better serve their </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">intended purpose.</span></span></p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.470.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.471.1">In this chapter, we gained a broad view of the prediction explanations landscape and dived into the integrated gradients technique, applied it practically to a use case, and even attempted to explain the integrated gradients results manually and automatically through LLMs. </span><span class="koboSpan" id="kobo.471.2">We also discussed common pitfalls in prediction explanations and provided strategies to avoid them, ensuring the effectiveness of these explanations in understanding and improving </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">AI models.</span></span></p>
<p><span class="koboSpan" id="kobo.473.1">Integrated gradients is a useful technique and tool to provide a form of saliency-based explanation of the predictions that your neural network makes. </span><span class="koboSpan" id="kobo.473.2">The process of understanding a model through prediction explanations provides many benefits that can help fulfill the criteria required to have a successful machine learning project and initiative. </span><span class="koboSpan" id="kobo.473.3">Even when everything is going well and the machine learning use case is not critical, uncovering the model’s behavior that you will potentially deploy through any prediction explanations technique can help you improve how your model </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">is utilized.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">However, saliency-based explanations only allow you to understand which input data or input neurons are important. </span><span class="koboSpan" id="kobo.475.2">But what patterns a neuron is capable of detecting from the input data remains unknown. </span><span class="koboSpan" id="kobo.475.3">In the next chapter, we will expand on this direction and uncover techniques to understand exactly what a </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">neuron detects.</span></span></p>
<p><span class="koboSpan" id="kobo.477.1">By being aware of common pitfalls and implementing strategies to avoid them, practitioners can ensure that prediction explanations remain a valuable tool for understanding and improving AI models. </span><span class="koboSpan" id="kobo.477.2">Combining prediction explanations with other evaluation methods and collaborating with domain experts can lead to more accurate and reliable AI systems that better serve their </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">intended purpose.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">Before we wrap up, as a final word, be sure to allocate some time to understand your neural network model through prediction explanation techniques such as integrated gradients and consider the potential pitfalls to maximize </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">their effectiveness!</span></span></p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.481.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.482.1">Mukund Sundararajan, Ankur Taly, Qiqi Yan. </span><em class="italic"><span class="koboSpan" id="kobo.483.1">Axiomatic Attribution for Deep Networks</span></em><span class="koboSpan" id="kobo.484.1">, In: International Conference on Machine Learning. </span><span class="koboSpan" id="kobo.484.2">2017. </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">URL: </span></span><a href="https://arxiv.org/abs/1703.01365v2"><span class="No-Break"><span class="koboSpan" id="kobo.486.1">https://arxiv.org/abs/1703.01365v2</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.487.1">.</span></span></li>
</ul>
</div>
</body></html>