- en: Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about artificial neural networks, which forms
    the foundation for all deep learning. We will discuss what makes deep learning
    different from other forms of machine learning and then spend time diving into
    some of its specific and special features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have learned what makes deep learning a
    special subset of machine learning. We'll have an understanding of neural networks,
    how they mimic the brain, and the benefits of hidden layers for discrete element
    detection. We'll create a feedforward neural network, noting the role of the activation
    function in determining variable weights.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting deep learning with machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing neural networks and the human brain
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the role of hidden layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a feedforward network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting our neural network with backpropagation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the source code of this chapter, please refer to the GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting deep learning with machine learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One key strength of deep learning not shared by other forms of ML is its ability
    to factor the way variables are related. For instance, if we think back to when
    we were first learning about animals, then we could imagine a simple task where
    we are given five images of cats and five images of dogs; later, when we were
    shown a new image, we would be able to determine whether it was a cat or dog using
    the patterns that we detected from the previous images that we studied. In our
    example, it was the images that were to be classified as either cats or dogs.
    We can consider this example as a training set, and will use the same terminology
    for the classification of images. Mentally, our brain tries to match the images
    with the patterns that form the features of these two different species so that
    we can differentiate between them. This is what happens in deep learning as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, we would find the preceding example task to be quite simple; however,
    think of how a computer would have to learn this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: It needs to account for the ways that features are related for dogs compared
    with cats and it needs to do this wherever an animal appears in a photo, however
    much of the image is taken up by the animal.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this, we cannot use the standard machine-learning approach where all input
    is used without consideration for how it is related. The placement and proximity
    of pixels in two-dimensional space must be considered, so we can already see how
    a simple task for humans is already more complex for a machine.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, just evaluating the input data in the form of two-dimensional arrays
    is not enough. The machine also needs the multiple hidden layer architecture present
    in deep learning to identify multiple different patterns among the data arrays.
    That is to say, the pure signal won't be as helpful as the relationship between
    given signals that are near each other.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，仅以二维数组的形式评估输入数据是不够的。机器还需要深度学习中存在的多个隐藏层架构，以识别数据数组中的多种不同模式。换句话说，单纯的信号并不像相互接近的信号之间的关系那样有用。
- en: Each layer will identify the presence of a different aspect of the image from
    basic shape detection to the length and steepness of color gradients.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层都会识别图像中不同方面的存在，从基本的形状检测到颜色渐变的长度和陡峭度。
- en: When performing a simple regression or classification task, we apply a weight
    to each variable and then use this to predict an outcome. With deep learning,
    there is a middle step where artificial neurons or units are created using all
    the variables. This step creates new features that are a combination of all variables
    with varying weights applied. This activity happens in what is known as a hidden
    layer. After this, the signal is passed on to another hidden layer and the same
    process happens again.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行简单的回归或分类任务时，我们会为每个变量应用一个权重，然后用这个权重预测结果。对于深度学习，存在一个中间步骤，其中人工神经元或单元是基于所有变量创建的。这个步骤创建了新的特征，这些特征是所有变量的组合，并应用了不同的权重。这一过程发生在所谓的隐藏层中。之后，信号会传递到另一个隐藏层，重复相同的过程。
- en: 'Each layer will learn a different aspect from the input. Let''s look at an
    example of this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都会学习输入的不同方面。让我们来看一个例子：
- en: The first layer creates neurons based on the overall size of the object in the
    image by applying different weights to the negative and positive space.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层通过对负空间和正空间应用不同的权重，根据图像中物体的整体大小来创建神经元。
- en: In the second layer, the neurons may be created based on the shape and size
    of the ears and nose.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二层，神经元可能会根据耳朵和鼻子的形状和大小来创建。
- en: In this way, the different characteristics of cats and dogs are captured among
    the entirety of the hidden layers.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这种方式，猫和狗的不同特征会被捕捉到所有隐藏层中。
- en: The weights are assigned randomly at first. This is then checked against the
    actual answers. After several attempts, the model learns that adjusting weights
    in a given direction produces better results, and so it will continue to adjust
    the weights in the same direction until the chosen error rate is minimized.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，权重是随机分配的。然后会与实际答案进行比较。经过多次尝试，模型学会了通过调整权重的方向来获得更好的结果，因此它会继续沿着相同的方向调整权重，直到选定的误差率最小化。
- en: Once this is complete and the weights are learned, a new input can be introduced.
    The model will multiply all variables by the learned weights at every neuron and
    each neuron will use an activation function that determines whether to activate
    or fire and send the signal forward to the next layer. We will go into more detail
    on the various activation functions later in the chapter. For now, let's just
    say that a calculation occurs at each neuron, after which a final value is produced
    at the output node. In this case, the probability is that the image is either
    a dog or a cat.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成并且权重被学习到，可以引入新的输入。模型将把所有变量与每个神经元的学习权重相乘，每个神经元将使用一个激活函数来决定是否激活或发火，并将信号传递到下一层。我们将在本章稍后详细讨论各种激活函数。目前，我们可以简单地说，在每个神经元上都会进行计算，最终在输出节点产生一个值。在这种情况下，概率是图像是狗还是猫。
- en: In this illustration, we may begin to see the power of neural networks and deep
    learning. The model is not evaluating the variables in isolation but rather in
    concert. By contrast, regression models calculate weights for each individual
    variable separately. Regression models can use interaction terms to calculate
    weights for combinations of variables; however, even this doesn't consider all
    variables in the same way as neural networks that evaluate all variables at all
    neurons. The neurons created from all the variables are then used to define the
    next set of neurons in the next layer. In this way, the entirety of the feature
    space is considered and is then partitioned based on themes that emerge after
    evaluating all variables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们可以开始看到神经网络和深度学习的强大力量。模型不是孤立地评估每个变量，而是将它们一起考虑。相比之下，回归模型为每个单独的变量计算权重。回归模型可以使用交互项来计算变量组合的权重；然而，即便如此，它也没有像神经网络那样，在所有神经元中评估所有变量。由所有变量创建的神经元随后用于定义下一层中神经元的集合。通过这种方式，整个特征空间都会被考虑，并根据评估所有变量后出现的主题进行划分。
- en: Comparing neural networks and the human brain
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较神经网络与人脑
- en: Let's consider how a human brain learns in order to see the ways in which a
    neural network is similar and the ways in which it is different.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下人类大脑是如何学习的，从而了解神经网络在某些方面的相似性和不同之处。
- en: Our brain contains a large number of neurons, and each neuron is connected to
    thousands of nearby neurons. As these neurons receive signals, they fire if the
    input contains a certain amount of a given color or a certain amount of a given
    texture. After millions of these interconnected neurons fire, the brain interprets
    the incoming signal as a certain class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑包含大量的神经元，每个神经元与成千上万个附近的神经元相连接。当这些神经元接收到信号时，如果输入包含一定量的某种颜色或某种纹理，它们会被激活。经过数百万个这样的互联神经元激活后，大脑会将传入的信号解读为某一类。
- en: Of course, these connections are not set permanently but rather change dynamically
    as we continue to have experiences, notice patterns, and discover relationships.
    If we try a new fruit for the first time and discover that it is really sour,
    then all the attributes that help us recognize this fruit are connected with things
    that we know are sour. In the future, whether we will want to experience eating
    this fruit again or not will depend on how much we want to experience this sour
    taste.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些连接不是永久设定的，而是随着我们不断获得经验、注意到模式并发现关系而动态变化的。如果我们第一次尝试一种新水果，发现它非常酸，那么所有帮助我们识别这种水果的特征就会与我们已知的酸味联系在一起。未来，是否愿意再次体验这种水果的味道将取决于我们是否愿意再体验这种酸味。
- en: Another example that shows how the neural network in our brain constantly evolves
    focuses on the types of activities that we find enjoyable. For instance, have
    you ever wondered why babies find shaking a simple toy enjoyable while we do not?
    In our brains, novelty is rewarded by the release of opioids; however, as a given
    piece of stimulus becomes less surprising, a smaller number of neurons are needed
    to interpret this experience, resulting in a less intense response as fewer neurons
    are firing. In this way, we see the dynamic nature of the neural connections in
    our brains, which are in constant flux.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个展示我们大脑中的神经网络如何不断演变的例子聚焦于我们发现愉快的活动类型。例如，你有没有想过，为什么婴儿觉得摇动一个简单的玩具很有趣，而我们却不这么觉得？在我们的脑中，新奇会通过释放类鸦片物质来获得奖励；然而，随着某一刺激变得不再那么令人惊讶，解读这种体验所需的神经元数量会减少，导致反应不那么强烈，因为更少的神经元在激活。通过这种方式，我们看到了大脑中神经连接的动态性质，它们始终处于变化之中。
- en: When we discuss the connections between neurons, we are specifically speaking
    of the synapses between neurons. Artificial neural networks seek to mimic the
    type of learning done by the human brain by creating a massive web of connections
    between constructed neurons and an output node or nodes, in a crude approximation
    of the brain's neurons. Just as the synapses that connect neurons in our brain
    can get stronger or weaker, the weights between neurons can change during the
    training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论神经元之间的连接时，我们特指的是神经元之间的突触。人工神经网络试图通过创建一个由构造的神经元和一个或多个输出节点之间的巨大连接网络，模拟人脑的学习方式，粗略地近似大脑的神经元。就像大脑中连接神经元的突触可以变强或变弱一样，神经元之间的权重也可以在训练过程中发生变化。
- en: However, unlike the human brain, simple artificial neural networks like those
    that we are studying in this chapter do not start with any inherited weights and
    connections. They begin with a random assignment of weights and connections. Also,
    while the weights can change during the training process, this does not continue
    during the application of the model. At this phase, the weights that were derived
    during the training process are applied and there isn't a continual adjustment.
    This can be contrasted with the human brain. The neural networks in our brains
    do behave similarly to artificial neural networks; however, the adjustments to
    how neurons are connected update constantly. Lastly, the human brain has billions
    of neurons and trillions of connections, while artificial neural networks like
    those we will build shortly have much fewer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与人类大脑不同，我们在本章研究的简单人工神经网络并不以任何遗传的权重和连接开始。它们从权重和连接的随机分配开始。此外，虽然在训练过程中权重会发生变化，但这种变化在模型应用时并不会继续。此时，训练过程中得到的权重会被应用，并且没有持续的调整。这与人类大脑有所不同。我们大脑中的神经网络确实与人工神经网络类似；然而，神经元连接的调整会不断更新。最后，人类大脑拥有数十亿个神经元和万亿级的连接，而我们即将构建的人工神经网络的神经元和连接要少得多。
- en: While there are some significant differences between the way a brain learns
    and the way an artificial neural network learns, by sharing a similar design structure,
    the artificial neural network is capable of solving some extremely complex tasks.
    This idea has continued to develop and improve, and throughout the course of this
    book, we will see just how powerful this idea has been for data science.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大脑学习的方式和人工神经网络学习的方式存在一些显著的差异，通过共享相似的设计结构，人工神经网络能够解决一些极其复杂的任务。这个理念不断发展和改进，在本书的过程中，我们将看到这一理念对数据科学的巨大影响。
- en: Utilizing bias and activation functions within hidden layers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在隐藏层中利用偏置和激活函数
- en: When we described deep learning earlier, we noted that the defining characteristic
    is the presence of hidden layers comprised of neurons that contain the weighted
    sum of all predictor variables in a dataset. We just addressed how this array
    of interconnected neurons is modeled after the human brain. Now let's take a deeper
    dive into what is happening in these hidden layers where neurons are created.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前描述深度学习时，我们提到其定义特征是存在由神经元组成的隐藏层，这些神经元包含数据集中所有预测变量的加权和。我们刚刚讨论了这种互联神经元的结构是如何模仿人类大脑的。现在，让我们更深入地了解这些隐藏层中神经元创建时所发生的事情。
- en: 'At this point, we can deduce the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以推导出以下结论：
- en: We understand that all variables receive a coefficient at random for each neuron
    based on how many units we want to create in each layer.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们理解到，所有变量都会根据我们希望在每一层创建的神经元单元数量，随机为每个神经元分配一个系数。
- en: The algorithm then continues to make changes to these coefficients until it
    minimizes the error rate.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法随后继续调整这些系数，直到最小化误差率。
- en: However, there is one additional coefficient present during this process of
    passing weighted values to the neurons, and that is known as the bias function.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在这个将加权值传递给神经元的过程中，还有一个附加的系数，这就是所谓的偏置函数。
- en: 'The bias function can simply be thought of as a means of adjusting the shape
    of the line separating the data laterally. For now, let''s simply imagine that
    a straight diagonal line is drawn to separate data points in two-dimensional space.
    In the following example, no matter how we adjust the slope of the line, we cannot
    find a line that bisects the triangles and circles:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置函数可以简单地理解为一种调节数据在横向上分隔的线形的方式。目前，我们可以假设在二维空间中绘制了一条直线，用于将数据点分开。在以下的示例中，不论我们如何调整直线的斜率，我们都无法找到一条能够将三角形和圆形数据点分开的直线：
- en: '![](img/eea521fc-8c58-4542-9f70-9559a02b977c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eea521fc-8c58-4542-9f70-9559a02b977c.png)'
- en: 'However, if we adjust the line slightly so that it intercepts the *y*-axis
    above the center of the plot, then we can fit a line between the two classes of
    points. This is what the bias function does. It adjusts the intercept point to
    allow for a better fitting line:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们稍微调整一下直线，使其与图形的 *y* 轴交点位于图中心的上方，那么我们就可以找到一条在两类数据点之间的最佳拟合线。这就是偏置函数的作用。它通过调整截距点，允许我们找到更合适的拟合线：
- en: '![](img/af4a5ff3-c637-4eaa-a04a-5f431a2b36cf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af4a5ff3-c637-4eaa-a04a-5f431a2b36cf.png)'
- en: The bias function is a coefficient that adjusts the line along the *x*-axis
    in this way to account for situations where the data requires the line to intersect
    the point where **y** is equal to **0** and **x** perhaps equals **5**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置函数是一个系数，它通过这种方式调整*x*轴上的直线，以应对数据要求直线与**y**等于**0**且**x**可能等于**5**的情况。
- en: All the weighted units and the bias function value are summed within the neuron
    and plotted along a linearly divided space. Calculating where a point is located
    relative to this line determines whether the neuron activates or switches on and
    continues to send a signal forward or if it switches off. Neural networks that
    use this type of function as a threshold to determine what happens with an incoming
    signal are referred to as perceptrons and are the earliest form of artificial
    neural network created.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有加权单元和偏置函数的值在神经元内被求和，并在一个线性划分的空间中绘制。计算一个点相对于这条直线的位置，决定了神经元是否激活或开关，并继续向前发送信号，或者关闭。如果使用这种类型的函数作为阈值来确定传入信号的处理方式，这种神经网络被称为感知器，是最早的人工神经网络形式。
- en: However, as neural networks have evolved, it has become clearer that using a
    linear model to separate data would not work in all circumstances. As a result,
    there are now a number of available functions that can take place within the neuron
    to determine whether the signal should continue or stop as it passes through.
    These gate functions are referred to as activation functions, as they simulate
    the process of a neuron within the brain being triggered to fire or activate and
    send a signal to a connected neuron or not. At this point, let's explore the variety
    of activation functions available.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着神经网络的发展，越来越明显的是，使用线性模型来分离数据并不适用于所有情况。因此，现在有许多可以在神经元内执行的函数，以决定信号在通过时是否继续传递。这些门控函数被称为激活函数，因为它们模拟了大脑中神经元被触发激活并向连接的神经元发送信号或不发送信号的过程。此时，让我们探索一下可用的各种激活函数。
- en: Surveying activation functions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查激活函数
- en: The activation functions are the last piece of the neural network that we have
    not covered in depth yet. To review what we know so far, in a neural network,
    we start with an input, as we would with any machine-learning modeling exercise.
    This data consists of a dependent target variable that we would like to predict
    and any number of independent predictor variables that are to be used for this
    prediction task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是我们尚未深入探讨的神经网络的最后一部分。回顾我们到目前为止所知道的，在神经网络中，我们从输入开始，就像我们在任何机器学习建模任务中一样。这些数据包括我们希望预测的目标变量（依赖变量），以及用于此预测任务的任意数量的独立预测变量。
- en: During the training process, the independent variables are weighted and combined
    in simulated neurons. A bias function is also applied during this step and this
    constant value is combined with the weighted independent variable values. At this
    point, an activation function evaluates an aggregation of the values and if it
    is above a set threshold limit, then the neuron fires and the signal is passed
    forward to additional hidden layers, if they exist, or to the output node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，独立变量在模拟神经元中被加权并组合。此步骤还会应用偏置函数，并将此常数值与加权后的独立变量值结合。在此时，一个激活函数会评估这些值的聚合，如果聚合值超过设定的阈值限制，那么神经元就会被激活，信号将传递给其他隐藏层（如果存在），或者传递到输出节点。
- en: Let's consider the simplest activation function, which is a Heaviside or binary
    step function. This can be imagined visually as two horizontal lines that act
    as the threshold limits on either side of a vertical line splitting the data so
    that the shape is like that of a step. If the value is on the horizontal line
    at 1, then the signal progresses; otherwise, the neuron doesn't fire. We also
    previously mentioned how a diagonal line could be used at this step to linearly
    separate points. When points cannot be separated by either of these simple activation
    functions, then we can use nonlinear alternatives, which we will look at next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来考虑最简单的激活函数——Heaviside 或二元阶跃函数。这可以通过视觉化的方式想象为两条水平线，它们作为阈值限制，位于一条垂直线的两侧，将数据分割，形成像阶梯一样的形状。如果值处于1的水平线上，那么信号会继续传递；否则，神经元就不会激活。我们之前也提到过，如何在这一步使用一条对角线来线性地分离点。当这些简单的激活函数无法分离数据点时，我们可以使用非线性的替代函数，接下来我们将介绍这些函数。
- en: Exploring the sigmoid function
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 sigmoid 函数
- en: The sigmoid function is the classic S-shaped function. This function works especially
    well for logistic regression tasks. While most of the results will be classified
    by the tails on either side of the curve, there is an area in the middle for capturing
    uncertainty about some of the data. The drawback of this shape is that the gradient
    is almost zero at the extremes, so the model may not be able to continue to learn
    as the points get towards either side.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function also contains a derivative value, which means that we can
    use this function along with backpropagation to update the weights after the variables
    pass through additional layers. We will explore backpropagation more in the final
    parts of this chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of the sigmoid function is that it confines values between
    0 and 1 so that the values are conveniently bound.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sigmoid` function can be defined simply using R code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this function, we can see that the dynamic value is the exponent of negative
    `x`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use this sigmoid function on a sequence of values between `-10` and
    `10`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will create a `tibble` containing two columns. One column will
    contain the sequence of numbers and the other will use the results of passing
    this sequence of values as arguments through the `sigmoid` function:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we set up the base of our plot by using the `ggplot()` function and passing
    in the data objects and defining the values that will be used along the *x*- and
    *y*-axes. In this case, we use the sequence of values between `-10` and `10` along
    the *x*-axis and the results of passing these values through the sigmoid function
    along the *y*-axis:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now add the points and use the `stat_function` feature to connect the
    points and display the sigmoid shape:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we now look at this shape, we can see why this type of activation function
    works so well for logistic regression. Using the `sigmoid` function to transform
    our values has pushed most of the values close to the extremes of `0` or `1`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dce8270-9c4f-4561-a97f-ffcbbd5c8e4e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Investigating the hyperbolic tangent function
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hyperbolic tangent function, which is also known as **tanh**, is very similar
    to the sigmoid function; however, the lower bound of the curve is in negative
    space to better handle data containing negative values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this one difference, everything else about the hyperbolic tangent
    function is the same as the sigmoid function, and like the sigmoid function, the
    hyperbolic tangent contains a derivative element and can be used with backpropagation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Since tanh is bounded between `-1` and `1`, the gradient is larger and the derivative
    is more pronounced. Being bounded means that tanh is centered around `0`, which
    can be advantageous in a model with a large number of hidden layers as the results
    from a layer are easier for the next layer to use.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the same sequence of values and plot the values after passing all
    of these values through the hyperbolic tangent function, which is included with
    base R and can be called using `tanh()`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the preceding sigmoid example, we will create a `tibble` with
    the values in our sequence and the transformed values:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then set up the base of our plot by passing the dataset along with the values
    to use for the *x*-axis and the *y*-axis to the `ggplot()` function:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lastly, we again add our points and use the `stat_function` feature to connect
    the dots and display our shape:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we look at this shape, we can see that it is a very similar shape to the
    sigmoid shape that we just plotted; however, note that the *y*-axis now has a
    range from `-1` to `1` rather than `0` to `1`, as was the case with the sigmoid
    shape. As a result, the values are pushed even further to the extremes and negative
    values remain negative after transformation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2497a31e-f420-4e7a-aba6-b4859d42df7d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Plotting the rectified linear units activation function
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified Linear Units** (**ReLU**) is a hybrid function that fits a line
    for positive values of *x* while assigning any negative values of *x* with a value
    of 0\. Even though one half of this function is linear, the shape is nonlinear
    and carries with it all the advantages of nonlinearity, such as being able to
    use the derivative for backpropagation.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the previous two activation functions, it has no upper bound. This lack
    of a constraint can be helpful to avoid the issue with the sigmoid or tanh function,
    where the gradient becomes very gradual near the extremes and provides little
    information to help the model continue to learn. Another major advantage of ReLU
    is how it leads to sparsity in the neural network because of the drop off at the
    center point. Using signoid or tanh, very few output values from the function
    will be zero, which means that the activation functions will fire, leading to
    a dense network. By contrast, ReLU results in far more output values of zero,
    leading to fewer neurons firing and a much sparser network.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: ReLU will learn faster than sigmoid and tanh because of its simplicity. The
    ReLU function results in more zero values than sigmoid or tanh, which will improve
    the speed of the training process; however, since we no longer know the unaltered
    value for these points, they cannot be updated during backpropagation later. This
    can be an issue if weights would otherwise be adjusted to pass along relevant
    information during backpropagation; however, it is no longer possible once the
    derivative is set to zero.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write some code to visualize the ReLU function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the function. It is simply defined so that if a value
    for *x* is greater than 0, then it sets *x* equal to *y*, and creates a line with
    a slope of `1`; otherwise, it sets *y* to *x* and creates a horizontal line on
    the *x*-axis. This can be coded like this:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let''s create our dataset again using the same sequence of numbers as
    before and the transformed values after passing this sequence through our ReLU
    function:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们再次使用之前相同的数字序列创建数据集，并将该序列通过我们的ReLU函数后得到的转换值：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, let''s plot these points and connect them to display the shape of this
    activation function:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们绘制这些点并连接它们，以展示该激活函数的形状：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, as we look at this shape, we can see its strengths. Having no upper bounds
    provides the model with more information than sigmoid, where the gradient becomes
    very minimal near the extremes. In addition, converting all negative values to
    `0` results in a much more sparse neural network and faster training time:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们观察这个形状时，可以看到它的优点。没有上界为模型提供了比sigmoid更多的信息，因为sigmoid在极端值附近的梯度变得非常微小。此外，将所有负值转换为`0`会导致神经网络更加稀疏，从而加快训练速度：
- en: '![](img/04d3f782-4dd1-46e9-9255-e68dcf7e19a6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04d3f782-4dd1-46e9-9255-e68dcf7e19a6.png)'
- en: Calculating the Leaky ReLU activation function
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算Leaky ReLU激活函数
- en: One potential problem with ReLU is known as **dying ReLU**, where, since the
    function assigns a zero value for all negative values, signals can get dropped
    completely before reaching the output node. One way to try to solve this issue
    is to use Leaky ReLU, which assigns a small alpha value when numbers are negative
    so that the signal is not completely lost. Once this constant is applied, the
    values that would otherwise have been zero now have a small slope. This keeps
    the neuron from being fully deactivated so that information can still be passed
    on to improve the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU的一个潜在问题是**死亡ReLU**，由于该函数对所有负值赋值为零，因此信号在到达输出节点之前可能会完全丢失。解决这个问题的一种方法是使用Leaky
    ReLU，当数值为负时，赋一个小的alpha值，从而避免信号完全丢失。应用这个常数后，本应为零的值现在有了一个小的斜率。这可以防止神经元完全失效，使得信息仍然能够传递，从而改善模型。
- en: 'Let''s create a simple example of the Leaky ReLU activation function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的Leaky ReLU激活函数示例：
- en: 'We start by defining the function. We will do this in the exact same way as
    the ReLU function, except that instead of assigning a `0` to all negative values,
    we will instead multiply by a constant to provide a small slope:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从定义函数开始。我们将以与ReLU函数完全相同的方式进行操作，不同之处在于，我们不是将所有负值赋值为`0`，而是乘以一个常数以提供一个小的斜率：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After this, we create the dataset with the sequence of values that we have
    been using in these examples, along with the transformed values:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建包含我们在这些示例中使用的数值序列的数据集，以及转换后的值：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, we plot these points to display the shape of this activation function:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们绘制这些点来展示这个激活函数的形状：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When we look at this shape, we can see why this alternative to ReLU is preferable
    sometimes. Having the slight slope for negative values of **x** combats the dying
    ReLU problem where the neural network becomes too sparse and doesn''t have the
    data needed to converge around a prediction:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看这个形状时，我们可以理解为什么有时这种替代ReLU的方法更可取。对于负值的**x**拥有轻微的斜率可以解决死亡ReLU问题，在该问题中，神经网络变得过于稀疏，缺乏足够的数据来围绕预测收敛：
- en: '![](img/7009590a-75de-4684-9efe-096c48732791.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7009590a-75de-4684-9efe-096c48732791.png)'
- en: Defining the swish activation function
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义swish激活函数
- en: Swish is a more recently developed activation function that aims to leverage
    the strengths of ReLU while also addressing some of its shortcomings. Swish, like
    ReLU, has a lower bound and no upper bound, which is a strength as it can still
    deactivate neurons while preventing values from being forced to converge around
    an upper bound. However, unlike ReLU, the lower bound is still curved, and what's
    more notable is that the line is nonmonotonic, which means that as values for
    `x` decrease, the value for `y` can increase. This is an important feature that
    prevents the dying neuron problem as the derivative can continue to be modified
    across iterations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Swish是一种较新开发的激活函数，旨在发挥ReLU的优势，同时解决它的一些缺点。Swish与ReLU一样，具有下界且没有上界，这是其优势，因为它仍然可以使神经元失效，同时防止数值被迫收敛到上界。然而，与ReLU不同的是，下界仍然是曲线形的，更值得注意的是，这条线是非单调的，这意味着当`x`值减小时，`y`值可以增加。这是一个重要的特点，可以防止死亡神经元问题，因为导数可以在迭代中继续调整。
- en: 'Let''s investigate the shape of this activation function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究这个激活函数的形状：
- en: 'Let''s start by defining the function, as we did in other examples. The formula
    simply takes a value and multiplies it by the result of passing the exact same
    value through the sigmoid function:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After this, we will create our dataset again with the same sequence of values
    that we have used previously and a corresponding set of transformed values:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we can plot these points to display the shape of this function:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we look at this shape, we will see that, like ReLU, it has a lower bound
    and no upper bound; however, unlike ReLU and all other activation functions, it
    is nonmonotonic—that is, we can see that the values for *y* when *x* is negative
    first decrease and then increase. This feature has been shown to be especially
    beneficial as neural networks get progressively deeper:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c46491be-188a-43eb-9303-b59485b73200.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Predicting class likelihood with softmax
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `softmax()` function is needed when there is more than one target variable.
    Softmax will create probabilities that a particular set of input variables belongs
    to each class. After these results are calculated, they can be used to assign
    input rows to one of the possible target classes. Let''s explore this activation
    function with a slightly different example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining the function:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let''s pass a vector of values to the function:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s confirm that the sum of these transformed values equals `1`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see that this function will take a set of values and calculate a probability
    that each is a value we are trying to predict so that the sum of all probabilities
    is `1`. This can be used to select the most likely value among a set of more than
    two values for a given target.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Creating a feedforward network
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With an understanding of neural networks, we will now build some simple examples.
    First, we will create the functions needed to create a very simple neural network
    ourselves to better understand what is happening during the modeling process.
    Afterward, we will use the `neuralnet` package to build a neural network that
    solves a task using a simple dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Writing a neural network with Base R
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we will use Base R to create a very simple neural network
    from scratch to better understand exactly what is happening at each step. In order
    to complete this task we will do the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Define the activation function for the neurons in our model
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a function that shows the line after every iteration of learning the
    weights
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make some test data and plot these data values
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update weights using the results of the previous attempt
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the following steps to do so:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we code the Heaviside (binary) step activation function to start. We
    will recall that this function evaluates the input and if this value is greater
    than zero, then the output value of the function is `1`; otherwise, the value
    is `0`. We can express this logic in code using the following lines of code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we can create the function for drawing the line using random weights
    at first and then the learned weight as we iterate over and update the values
    passed to the expression portion of the curve function, as shown here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the expression equation, we can see that everything is relative to `x`. In
    this case, if we just ran `curve((x))`, then we would get a line at exactly 45
    degrees so that the `x` and `y` were always equal and the slope of the line was
    `1`. In the preceding code, we use the weights to change the slope of the line
    relative to `x`. The remainder just defines the plot and the line and the `add`
    argument is used to declare whether the new line produced by the curve function
    should be added to the plot in addition to the line or lines already there.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'With these functions defined, we can now assign some initial values for the
    input, output, learning rate, and weights. The input values are a set of *x* and
    *y* coordinates along with a constant value. The output values are just binary
    flags, in this case, denoting whether the input variable is or is not a member
    of the target class. Lastly, some random weights are included to initialize the
    model, along with a learning rate that will be used for updating weights at every
    iteration:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we can add our first line, which is the guess using the random weights
    along with our output points. We would like to arrive at a line that completely
    bisects the two points that belong to the target class from the two points that
    do not. To start, we apply our initial random weights to the expression equation
    within the `linear_fits()` function to create an initial slope. The `points()`
    function adds our points with squares to the two points that are part of the target
    class and circles for the points that are not part of this class:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As we can tell from the following plot that is generated by the preceding code,
    this first line is extremely far from bisecting the two classes:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0069b1d5-2df1-4760-bda8-77c6244fa22f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will begin to update the weights by using the values from the corresponding
    input and output values. First, the weights are updated by the learning rate.
    The smaller this number is, the more gradual the changes will be. This is multiplied
    by the target class value minus the product of the result of the artificial neuron
    function, which again is either `0` or `1` since we are using the binary step
    activation function and the first input values. The `linear_fits()` function is
    then used again to draw one more line:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using the `linear_fits()` function, we have created a new line that is closer
    to bisecting the classes, and yet it is not yet completely dividing the points
    as we would like:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91f7a32-4b3e-4c47-bad1-c8c3940700d4.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'This same operation is repeated for the remainder of the input and output values.
    A different line type is used for the last plot because, as we will see, this
    line solves the problem and finds a slope that separates the two classes. The
    third line is drawn next:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, the third line completely overlaps the second line:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a627969-1766-4e7f-a96c-943d88aa4127.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'The next code is used to draw the fourth line:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The fourth line deviates further away from bisecting the points than the second
    or the third line:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e80384b-d17b-4481-b876-950f7bc24a7e.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'The final line is created using the following line of code:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We see here that the dotted line successfully bisects the square points and
    separates them from the circular points:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fb3f623-6478-46fa-a72a-93b6b038ac48.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: In this minimal example, we can see what is happening when we fit a neural network.
    The model attempts to separate different classes of variables just like in other
    machine-learning models. In this case, during every iteration, the weights are
    updated depending on whether the neuron fires along with the constant change introduced
    by the learning rate value. Through this process, the goal is to reduce the error
    rate. In this case, we did not define a formal error rate as we could clearly
    see when the line successfully divided the classes. In the next example, we will
    step away from the underlying math and focus on optimizing a slightly more complex
    neural network using an open source dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model with Wisconsin cancer data
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we will use the breast cancer dataset from the University
    of Wisconsin. Details on this dataset can be found at the UCI Machine Learning
    Repository at [http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be loaded using the following lines of code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After loading the data, we will encode the target variable by converting the
    column from a character column with two character values that indicate whether
    or not there were signs of malignancy to a numeric data type holding binary values.
    For this type of neural network, we will need all values to be numeric, including
    the target variable:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will scale and standardize all of our predictor values. As we mentioned,
    all data needs to be numeric for the neural network, and by scaling and standardizing
    the data, we will increase performance by giving the activation functions a set
    of values that are all constrained within the same boundaries:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will partition to train and test, just like in our machine learning
    example. We will use the ID column in `X1` for splitting the data in this step;
    however, afterward, we can drop this column. Here, we will use a `tidyverse` approach
    to simplify the process:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our last data preparation step is to extract all the actual correct responses
    from the test data and then remove this column from the test dataset. This vector
    of values will be used to calculate performance after modeling:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The data is now completely prepared and ready for modeling with a neural network.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create the formula syntax for the `neuralnet` package, which
    is a dependent variable `~`. This is similar to the syntax for fitting a linear
    model with R. All independent variables can be connected with the `+` sign between
    each one; however, when there are many independent variables, then it would be
    very tedious to write the name for every column, even in this example, where the
    column names are just X followed by a number. Fortunately, there is a way to expedite
    this process, which we will use in the following steps. First, we will get the
    names for all of the columns in our train set and then, using paste and collapse,
    we will create the string of independent variables to go on the other side of
    our formula from the dependent variable:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With this set, we can now fit our model. In this case, we will keep the model
    fairly simple, only using a few of the arguments available for this function.
    Specifically, we include the formula that we just created, defining the dependent
    and independent variables. Next, we indicate that the model will be fit to the
    train data. Choosing the correct number of layers and units per layer involves
    trying a few combinations and comparing performance. In this case, we start with
    two layers containing about half as many units as there are variables. Lastly,
    we note that the activation function should be logistic, which is the sigmoid
    function, and that we are not performing a linear operation:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the modeling process complete, we can now use our model to make predictions.
    With the `neuralnet` package, we use the `compute()` function to generate these
    prediction values:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When we pass the model and the test dataset through the `compute()` function,
    we are given a list. The list contains details about the neurons within the model,
    along with the predicted values. In this case, we just want the predicted values,
    so we will pull these from the list. In addition, we will create a set of binary
    predictions. The binary predictions are created by changing values to `1`, if
    the predicted probability is greater than `0.5`; otherwise, the value is changed
    to a `0`. We will use each set of predictions for two different model evaluation
    methods:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Using our binary predictions, we can easily calculate basic model accuracy—that
    is, we will sum the number of cases where the binary predicted value matches the
    actual value and divide that number by the total number of actual values:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we see that the accuracy is 92.98%, so our basic neural net has performed
    quite well on this data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the breakdown for this accuracy value by using a confusion
    matrix. The simplest way to produce a confusion matrix is to use the `confusionMatrix()`
    function, which is part of the `caret` package. This function requires a table
    containing the predicted valued and the actual values as an argument. In this
    case, we need to use our binary predictions as the results need to fit into one
    of four categories, and as such, levels of granularity are not permitted:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After calling this function, we see that we are provided with a two by two
    grid containing the results. The confusion matrix has categorized our predictions
    into the following four outcomes:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**True positives**: Values of `1` correctly predicted to be `1`. The actual
    test target variable contains the value we are predicting and we correctly predicted
    it.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type I errors**: Values of `0` incorrectly predicted to be values of `1`.
    The actual test target variable does not have the value we are predicting; however,
    we predicted that it will. Also referred to as a false positive.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type II errors**: Values of `1` incorrectly predicted to be values of `0`.
    The actual test target variable does have the value we are predicting; however,
    we predicted that it will not. Also referred to as a false negative.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives**: Values of `0` correctly predicted to be `0`. The actual
    test target variable does not contain the value we are predicting and we correctly
    predicted that it would not have it.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also includes a number of other statistical measures that are outside the
    scope of this chapter; however, we can note that the accuracy is included and
    the value matches the value that we just calculated ourselves.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using our binary prediction to calculate accuracy, we can also
    use our probabilities so that we take into account the level of certainty for
    each outcome. In order to measure performance using these values, we will use
    the AUC, or area under the curve, score. This compares the probabilities for true
    positive cases with the probabilities for false positive cases. The final result
    is a measure of the confidence that positive values are positive and that negative
    values are negative, or in this case, that negative values are not incorrectly
    labeled positive with high confidence.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the AUC score, we can use the `auc()` function, which is part
    of the `Metrics` package. The function takes two arguments—a vector of actual
    values and a vector of predicted probabilities that a record should be classified
    as the target variable based on the model''s interpretation of the independent
    variable for that row:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The AUC score of 0.987 is even stronger than the accuracy score calculated previously.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: This model is already working very well at solving the prediction task using
    this dataset; however, we will now try to add a backpropagation step and see if
    we can improve performance further.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting our neural network with backpropagation
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have a working neural network. For this simple example, we
    will add one additional feature of neural networks that can improve performance,
    which is backpropagation. A neural network can learn to solve a task by multiplying
    the variable by values so that the variables are weighted as they pass through
    hidden layers. The backpropagation step allows the model to traverse back through
    layers and adjust the weights that were learned during previous steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, this step is quite straightforward to implement. We simply
    declare that we will use the backpropagation algorithm and indicate the learning
    rate, which controls how much the weights are adjusted. In general, this learning
    rate value should be very low.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following example, we have to do the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The `threshold` value and `stepmax` value have to be changed as the model failed
    to converge using the default values.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `threshold` argument defines the value that the error rate must reach before
    the model stops and the `stepmax` argument defines the number of iterations the
    model will run before stopping.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By changing these values, you can program the model to run longer and stop
    sooner, both of which will help if you run into an error when converging:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After running this new version of the model, we can run the same steps again
    to assess performance. First, we will run `compute` on the new model to get new
    predictions. We will once again create a vector of probabilities and binary predictions,
    and as a first step, we will create the table of binary prediction values and
    actual values and pass this to the `confusionMatrix()` function. We will skip
    calculating the accuracy this time around as it is included in the output from
    the call to the `confusionMatrix()` function:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our accuracy has improved, increasing from 92.98% to 94.74%. Let''s now check
    our AUC score. Again, we simply pass the actual values and predicted probabilities
    to the `auc()` function:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Our AUC score has improved, increasing from 0.987 to 0.993, so we can see that
    backpropagation does improve model performance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: That being said, what exactly is happening during this step?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation step takes the derivative of the error rate and uses this
    to update weights based on results.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative is just the rate at which the current weights impact the error
    rate. So if the derivative rate is `7`, then changing the weights by a single
    unit will result in a change to the error rate that is 7 times larger.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using just a feedforward neural network, we can update the initial weights based
    on the final derivative value; however, using backpropagation, we can update the
    weights at every neuron.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using information about how previous changes have impacted the derivative, this
    step either increases or decreases the weights.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate is applied so that changes are never dramatic but rather smooth
    and gradual. This process can continue until the error rate is minimized.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that deep learning is differentiated from other
    machine-learning algorithms because of the use of multiple hidden layers. This
    network of hidden layers, which are composed of artificial neurons, was designed
    to mimic the way our brain processes input signals to interpret our environment.
    The units within the hidden layers take in all the independent variables and apply
    some weights to these variables. In this way, each neuron classifies the combination
    of input values in different ways.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: From understanding the architecture of this type of machine learning from a
    high level, we then took a deeper dive into the actual process of converting the
    input to predictions using this approach. We discussed the various activation
    functions that act as the gate for every neuron, determining whether a signal
    should be passed to the next layer. We then built two feedforward neural networks—one
    using base R for a better understanding of what is happening and another using
    the `neuralnet` package on a larger dataset. Lastly, we applied the backpropagation
    step to further improve our model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: As stated, the artificial neural network is the fundamental building block for
    more complex deep learning, and now that we have this understanding, we will move
    on to creating convolutional neural networks for image recognition in the next
    chapter.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
