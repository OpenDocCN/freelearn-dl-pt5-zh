- en: Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about artificial neural networks, which forms
    the foundation for all deep learning. We will discuss what makes deep learning
    different from other forms of machine learning and then spend time diving into
    some of its specific and special features.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have learned what makes deep learning a
    special subset of machine learning. We'll have an understanding of neural networks,
    how they mimic the brain, and the benefits of hidden layers for discrete element
    detection. We'll create a feedforward neural network, noting the role of the activation
    function in determining variable weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting deep learning with machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing neural networks and the human brain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the role of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a feedforward network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting our neural network with backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the source code of this chapter, please refer to the GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting deep learning with machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One key strength of deep learning not shared by other forms of ML is its ability
    to factor the way variables are related. For instance, if we think back to when
    we were first learning about animals, then we could imagine a simple task where
    we are given five images of cats and five images of dogs; later, when we were
    shown a new image, we would be able to determine whether it was a cat or dog using
    the patterns that we detected from the previous images that we studied. In our
    example, it was the images that were to be classified as either cats or dogs.
    We can consider this example as a training set, and will use the same terminology
    for the classification of images. Mentally, our brain tries to match the images
    with the patterns that form the features of these two different species so that
    we can differentiate between them. This is what happens in deep learning as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, we would find the preceding example task to be quite simple; however,
    think of how a computer would have to learn this:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to account for the ways that features are related for dogs compared
    with cats and it needs to do this wherever an animal appears in a photo, however
    much of the image is taken up by the animal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this, we cannot use the standard machine-learning approach where all input
    is used without consideration for how it is related. The placement and proximity
    of pixels in two-dimensional space must be considered, so we can already see how
    a simple task for humans is already more complex for a machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, just evaluating the input data in the form of two-dimensional arrays
    is not enough. The machine also needs the multiple hidden layer architecture present
    in deep learning to identify multiple different patterns among the data arrays.
    That is to say, the pure signal won't be as helpful as the relationship between
    given signals that are near each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each layer will identify the presence of a different aspect of the image from
    basic shape detection to the length and steepness of color gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When performing a simple regression or classification task, we apply a weight
    to each variable and then use this to predict an outcome. With deep learning,
    there is a middle step where artificial neurons or units are created using all
    the variables. This step creates new features that are a combination of all variables
    with varying weights applied. This activity happens in what is known as a hidden
    layer. After this, the signal is passed on to another hidden layer and the same
    process happens again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer will learn a different aspect from the input. Let''s look at an
    example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer creates neurons based on the overall size of the object in the
    image by applying different weights to the negative and positive space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second layer, the neurons may be created based on the shape and size
    of the ears and nose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, the different characteristics of cats and dogs are captured among
    the entirety of the hidden layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights are assigned randomly at first. This is then checked against the
    actual answers. After several attempts, the model learns that adjusting weights
    in a given direction produces better results, and so it will continue to adjust
    the weights in the same direction until the chosen error rate is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Once this is complete and the weights are learned, a new input can be introduced.
    The model will multiply all variables by the learned weights at every neuron and
    each neuron will use an activation function that determines whether to activate
    or fire and send the signal forward to the next layer. We will go into more detail
    on the various activation functions later in the chapter. For now, let's just
    say that a calculation occurs at each neuron, after which a final value is produced
    at the output node. In this case, the probability is that the image is either
    a dog or a cat.
  prefs: []
  type: TYPE_NORMAL
- en: In this illustration, we may begin to see the power of neural networks and deep
    learning. The model is not evaluating the variables in isolation but rather in
    concert. By contrast, regression models calculate weights for each individual
    variable separately. Regression models can use interaction terms to calculate
    weights for combinations of variables; however, even this doesn't consider all
    variables in the same way as neural networks that evaluate all variables at all
    neurons. The neurons created from all the variables are then used to define the
    next set of neurons in the next layer. In this way, the entirety of the feature
    space is considered and is then partitioned based on themes that emerge after
    evaluating all variables.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing neural networks and the human brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider how a human brain learns in order to see the ways in which a
    neural network is similar and the ways in which it is different.
  prefs: []
  type: TYPE_NORMAL
- en: Our brain contains a large number of neurons, and each neuron is connected to
    thousands of nearby neurons. As these neurons receive signals, they fire if the
    input contains a certain amount of a given color or a certain amount of a given
    texture. After millions of these interconnected neurons fire, the brain interprets
    the incoming signal as a certain class.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these connections are not set permanently but rather change dynamically
    as we continue to have experiences, notice patterns, and discover relationships.
    If we try a new fruit for the first time and discover that it is really sour,
    then all the attributes that help us recognize this fruit are connected with things
    that we know are sour. In the future, whether we will want to experience eating
    this fruit again or not will depend on how much we want to experience this sour
    taste.
  prefs: []
  type: TYPE_NORMAL
- en: Another example that shows how the neural network in our brain constantly evolves
    focuses on the types of activities that we find enjoyable. For instance, have
    you ever wondered why babies find shaking a simple toy enjoyable while we do not?
    In our brains, novelty is rewarded by the release of opioids; however, as a given
    piece of stimulus becomes less surprising, a smaller number of neurons are needed
    to interpret this experience, resulting in a less intense response as fewer neurons
    are firing. In this way, we see the dynamic nature of the neural connections in
    our brains, which are in constant flux.
  prefs: []
  type: TYPE_NORMAL
- en: When we discuss the connections between neurons, we are specifically speaking
    of the synapses between neurons. Artificial neural networks seek to mimic the
    type of learning done by the human brain by creating a massive web of connections
    between constructed neurons and an output node or nodes, in a crude approximation
    of the brain's neurons. Just as the synapses that connect neurons in our brain
    can get stronger or weaker, the weights between neurons can change during the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike the human brain, simple artificial neural networks like those
    that we are studying in this chapter do not start with any inherited weights and
    connections. They begin with a random assignment of weights and connections. Also,
    while the weights can change during the training process, this does not continue
    during the application of the model. At this phase, the weights that were derived
    during the training process are applied and there isn't a continual adjustment.
    This can be contrasted with the human brain. The neural networks in our brains
    do behave similarly to artificial neural networks; however, the adjustments to
    how neurons are connected update constantly. Lastly, the human brain has billions
    of neurons and trillions of connections, while artificial neural networks like
    those we will build shortly have much fewer.
  prefs: []
  type: TYPE_NORMAL
- en: While there are some significant differences between the way a brain learns
    and the way an artificial neural network learns, by sharing a similar design structure,
    the artificial neural network is capable of solving some extremely complex tasks.
    This idea has continued to develop and improve, and throughout the course of this
    book, we will see just how powerful this idea has been for data science.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing bias and activation functions within hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we described deep learning earlier, we noted that the defining characteristic
    is the presence of hidden layers comprised of neurons that contain the weighted
    sum of all predictor variables in a dataset. We just addressed how this array
    of interconnected neurons is modeled after the human brain. Now let's take a deeper
    dive into what is happening in these hidden layers where neurons are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can deduce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We understand that all variables receive a coefficient at random for each neuron
    based on how many units we want to create in each layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm then continues to make changes to these coefficients until it
    minimizes the error rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, there is one additional coefficient present during this process of
    passing weighted values to the neurons, and that is known as the bias function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bias function can simply be thought of as a means of adjusting the shape
    of the line separating the data laterally. For now, let''s simply imagine that
    a straight diagonal line is drawn to separate data points in two-dimensional space.
    In the following example, no matter how we adjust the slope of the line, we cannot
    find a line that bisects the triangles and circles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eea521fc-8c58-4542-9f70-9559a02b977c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, if we adjust the line slightly so that it intercepts the *y*-axis
    above the center of the plot, then we can fit a line between the two classes of
    points. This is what the bias function does. It adjusts the intercept point to
    allow for a better fitting line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af4a5ff3-c637-4eaa-a04a-5f431a2b36cf.png)'
  prefs: []
  type: TYPE_IMG
- en: The bias function is a coefficient that adjusts the line along the *x*-axis
    in this way to account for situations where the data requires the line to intersect
    the point where **y** is equal to **0** and **x** perhaps equals **5**.
  prefs: []
  type: TYPE_NORMAL
- en: All the weighted units and the bias function value are summed within the neuron
    and plotted along a linearly divided space. Calculating where a point is located
    relative to this line determines whether the neuron activates or switches on and
    continues to send a signal forward or if it switches off. Neural networks that
    use this type of function as a threshold to determine what happens with an incoming
    signal are referred to as perceptrons and are the earliest form of artificial
    neural network created.
  prefs: []
  type: TYPE_NORMAL
- en: However, as neural networks have evolved, it has become clearer that using a
    linear model to separate data would not work in all circumstances. As a result,
    there are now a number of available functions that can take place within the neuron
    to determine whether the signal should continue or stop as it passes through.
    These gate functions are referred to as activation functions, as they simulate
    the process of a neuron within the brain being triggered to fire or activate and
    send a signal to a connected neuron or not. At this point, let's explore the variety
    of activation functions available.
  prefs: []
  type: TYPE_NORMAL
- en: Surveying activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activation functions are the last piece of the neural network that we have
    not covered in depth yet. To review what we know so far, in a neural network,
    we start with an input, as we would with any machine-learning modeling exercise.
    This data consists of a dependent target variable that we would like to predict
    and any number of independent predictor variables that are to be used for this
    prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, the independent variables are weighted and combined
    in simulated neurons. A bias function is also applied during this step and this
    constant value is combined with the weighted independent variable values. At this
    point, an activation function evaluates an aggregation of the values and if it
    is above a set threshold limit, then the neuron fires and the signal is passed
    forward to additional hidden layers, if they exist, or to the output node.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the simplest activation function, which is a Heaviside or binary
    step function. This can be imagined visually as two horizontal lines that act
    as the threshold limits on either side of a vertical line splitting the data so
    that the shape is like that of a step. If the value is on the horizontal line
    at 1, then the signal progresses; otherwise, the neuron doesn't fire. We also
    previously mentioned how a diagonal line could be used at this step to linearly
    separate points. When points cannot be separated by either of these simple activation
    functions, then we can use nonlinear alternatives, which we will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the sigmoid function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sigmoid function is the classic S-shaped function. This function works especially
    well for logistic regression tasks. While most of the results will be classified
    by the tails on either side of the curve, there is an area in the middle for capturing
    uncertainty about some of the data. The drawback of this shape is that the gradient
    is almost zero at the extremes, so the model may not be able to continue to learn
    as the points get towards either side.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function also contains a derivative value, which means that we can
    use this function along with backpropagation to update the weights after the variables
    pass through additional layers. We will explore backpropagation more in the final
    parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of the sigmoid function is that it confines values between
    0 and 1 so that the values are conveniently bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sigmoid` function can be defined simply using R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we can see that the dynamic value is the exponent of negative
    `x`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use this sigmoid function on a sequence of values between `-10` and
    `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will create a `tibble` containing two columns. One column will
    contain the sequence of numbers and the other will use the results of passing
    this sequence of values as arguments through the `sigmoid` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up the base of our plot by using the `ggplot()` function and passing
    in the data objects and defining the values that will be used along the *x*- and
    *y*-axes. In this case, we use the sequence of values between `-10` and `10` along
    the *x*-axis and the results of passing these values through the sigmoid function
    along the *y*-axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now add the points and use the `stat_function` feature to connect the
    points and display the sigmoid shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now look at this shape, we can see why this type of activation function
    works so well for logistic regression. Using the `sigmoid` function to transform
    our values has pushed most of the values close to the extremes of `0` or `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dce8270-9c4f-4561-a97f-ffcbbd5c8e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Investigating the hyperbolic tangent function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hyperbolic tangent function, which is also known as **tanh**, is very similar
    to the sigmoid function; however, the lower bound of the curve is in negative
    space to better handle data containing negative values.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this one difference, everything else about the hyperbolic tangent
    function is the same as the sigmoid function, and like the sigmoid function, the
    hyperbolic tangent contains a derivative element and can be used with backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Since tanh is bounded between `-1` and `1`, the gradient is larger and the derivative
    is more pronounced. Being bounded means that tanh is centered around `0`, which
    can be advantageous in a model with a large number of hidden layers as the results
    from a layer are easier for the next layer to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the same sequence of values and plot the values after passing all
    of these values through the hyperbolic tangent function, which is included with
    base R and can be called using `tanh()`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the preceding sigmoid example, we will create a `tibble` with
    the values in our sequence and the transformed values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then set up the base of our plot by passing the dataset along with the values
    to use for the *x*-axis and the *y*-axis to the `ggplot()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we again add our points and use the `stat_function` feature to connect
    the dots and display our shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As we look at this shape, we can see that it is a very similar shape to the
    sigmoid shape that we just plotted; however, note that the *y*-axis now has a
    range from `-1` to `1` rather than `0` to `1`, as was the case with the sigmoid
    shape. As a result, the values are pushed even further to the extremes and negative
    values remain negative after transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2497a31e-f420-4e7a-aba6-b4859d42df7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting the rectified linear units activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified Linear Units** (**ReLU**) is a hybrid function that fits a line
    for positive values of *x* while assigning any negative values of *x* with a value
    of 0\. Even though one half of this function is linear, the shape is nonlinear
    and carries with it all the advantages of nonlinearity, such as being able to
    use the derivative for backpropagation.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the previous two activation functions, it has no upper bound. This lack
    of a constraint can be helpful to avoid the issue with the sigmoid or tanh function,
    where the gradient becomes very gradual near the extremes and provides little
    information to help the model continue to learn. Another major advantage of ReLU
    is how it leads to sparsity in the neural network because of the drop off at the
    center point. Using signoid or tanh, very few output values from the function
    will be zero, which means that the activation functions will fire, leading to
    a dense network. By contrast, ReLU results in far more output values of zero,
    leading to fewer neurons firing and a much sparser network.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU will learn faster than sigmoid and tanh because of its simplicity. The
    ReLU function results in more zero values than sigmoid or tanh, which will improve
    the speed of the training process; however, since we no longer know the unaltered
    value for these points, they cannot be updated during backpropagation later. This
    can be an issue if weights would otherwise be adjusted to pass along relevant
    information during backpropagation; however, it is no longer possible once the
    derivative is set to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write some code to visualize the ReLU function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the function. It is simply defined so that if a value
    for *x* is greater than 0, then it sets *x* equal to *y*, and creates a line with
    a slope of `1`; otherwise, it sets *y* to *x* and creates a horizontal line on
    the *x*-axis. This can be coded like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create our dataset again using the same sequence of numbers as
    before and the transformed values after passing this sequence through our ReLU
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let''s plot these points and connect them to display the shape of this
    activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as we look at this shape, we can see its strengths. Having no upper bounds
    provides the model with more information than sigmoid, where the gradient becomes
    very minimal near the extremes. In addition, converting all negative values to
    `0` results in a much more sparse neural network and faster training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04d3f782-4dd1-46e9-9255-e68dcf7e19a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the Leaky ReLU activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One potential problem with ReLU is known as **dying ReLU**, where, since the
    function assigns a zero value for all negative values, signals can get dropped
    completely before reaching the output node. One way to try to solve this issue
    is to use Leaky ReLU, which assigns a small alpha value when numbers are negative
    so that the signal is not completely lost. Once this constant is applied, the
    values that would otherwise have been zero now have a small slope. This keeps
    the neuron from being fully deactivated so that information can still be passed
    on to improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a simple example of the Leaky ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the function. We will do this in the exact same way as
    the ReLU function, except that instead of assigning a `0` to all negative values,
    we will instead multiply by a constant to provide a small slope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we create the dataset with the sequence of values that we have
    been using in these examples, along with the transformed values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we plot these points to display the shape of this activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at this shape, we can see why this alternative to ReLU is preferable
    sometimes. Having the slight slope for negative values of **x** combats the dying
    ReLU problem where the neural network becomes too sparse and doesn''t have the
    data needed to converge around a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7009590a-75de-4684-9efe-096c48732791.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the swish activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swish is a more recently developed activation function that aims to leverage
    the strengths of ReLU while also addressing some of its shortcomings. Swish, like
    ReLU, has a lower bound and no upper bound, which is a strength as it can still
    deactivate neurons while preventing values from being forced to converge around
    an upper bound. However, unlike ReLU, the lower bound is still curved, and what's
    more notable is that the line is nonmonotonic, which means that as values for
    `x` decrease, the value for `y` can increase. This is an important feature that
    prevents the dying neuron problem as the derivative can continue to be modified
    across iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s investigate the shape of this activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the function, as we did in other examples. The formula
    simply takes a value and multiplies it by the result of passing the exact same
    value through the sigmoid function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we will create our dataset again with the same sequence of values
    that we have used previously and a corresponding set of transformed values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can plot these points to display the shape of this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As we look at this shape, we will see that, like ReLU, it has a lower bound
    and no upper bound; however, unlike ReLU and all other activation functions, it
    is nonmonotonic—that is, we can see that the values for *y* when *x* is negative
    first decrease and then increase. This feature has been shown to be especially
    beneficial as neural networks get progressively deeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c46491be-188a-43eb-9303-b59485b73200.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicting class likelihood with softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `softmax()` function is needed when there is more than one target variable.
    Softmax will create probabilities that a particular set of input variables belongs
    to each class. After these results are calculated, they can be used to assign
    input rows to one of the possible target classes. Let''s explore this activation
    function with a slightly different example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s pass a vector of values to the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm that the sum of these transformed values equals `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see that this function will take a set of values and calculate a probability
    that each is a value we are trying to predict so that the sum of all probabilities
    is `1`. This can be used to select the most likely value among a set of more than
    two values for a given target.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a feedforward network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With an understanding of neural networks, we will now build some simple examples.
    First, we will create the functions needed to create a very simple neural network
    ourselves to better understand what is happening during the modeling process.
    Afterward, we will use the `neuralnet` package to build a neural network that
    solves a task using a simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a neural network with Base R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we will use Base R to create a very simple neural network
    from scratch to better understand exactly what is happening at each step. In order
    to complete this task we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the activation function for the neurons in our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a function that shows the line after every iteration of learning the
    weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make some test data and plot these data values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update weights using the results of the previous attempt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the following steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we code the Heaviside (binary) step activation function to start. We
    will recall that this function evaluates the input and if this value is greater
    than zero, then the output value of the function is `1`; otherwise, the value
    is `0`. We can express this logic in code using the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can create the function for drawing the line using random weights
    at first and then the learned weight as we iterate over and update the values
    passed to the expression portion of the curve function, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the expression equation, we can see that everything is relative to `x`. In
    this case, if we just ran `curve((x))`, then we would get a line at exactly 45
    degrees so that the `x` and `y` were always equal and the slope of the line was
    `1`. In the preceding code, we use the weights to change the slope of the line
    relative to `x`. The remainder just defines the plot and the line and the `add`
    argument is used to declare whether the new line produced by the curve function
    should be added to the plot in addition to the line or lines already there.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these functions defined, we can now assign some initial values for the
    input, output, learning rate, and weights. The input values are a set of *x* and
    *y* coordinates along with a constant value. The output values are just binary
    flags, in this case, denoting whether the input variable is or is not a member
    of the target class. Lastly, some random weights are included to initialize the
    model, along with a learning rate that will be used for updating weights at every
    iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can add our first line, which is the guess using the random weights
    along with our output points. We would like to arrive at a line that completely
    bisects the two points that belong to the target class from the two points that
    do not. To start, we apply our initial random weights to the expression equation
    within the `linear_fits()` function to create an initial slope. The `points()`
    function adds our points with squares to the two points that are part of the target
    class and circles for the points that are not part of this class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can tell from the following plot that is generated by the preceding code,
    this first line is extremely far from bisecting the two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0069b1d5-2df1-4760-bda8-77c6244fa22f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will begin to update the weights by using the values from the corresponding
    input and output values. First, the weights are updated by the learning rate.
    The smaller this number is, the more gradual the changes will be. This is multiplied
    by the target class value minus the product of the result of the artificial neuron
    function, which again is either `0` or `1` since we are using the binary step
    activation function and the first input values. The `linear_fits()` function is
    then used again to draw one more line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `linear_fits()` function, we have created a new line that is closer
    to bisecting the classes, and yet it is not yet completely dividing the points
    as we would like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91f7a32-4b3e-4c47-bad1-c8c3940700d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This same operation is repeated for the remainder of the input and output values.
    A different line type is used for the last plot because, as we will see, this
    line solves the problem and finds a slope that separates the two classes. The
    third line is drawn next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the third line completely overlaps the second line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a627969-1766-4e7f-a96c-943d88aa4127.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next code is used to draw the fourth line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The fourth line deviates further away from bisecting the points than the second
    or the third line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e80384b-d17b-4481-b876-950f7bc24a7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final line is created using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We see here that the dotted line successfully bisects the square points and
    separates them from the circular points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fb3f623-6478-46fa-a72a-93b6b038ac48.png)'
  prefs: []
  type: TYPE_IMG
- en: In this minimal example, we can see what is happening when we fit a neural network.
    The model attempts to separate different classes of variables just like in other
    machine-learning models. In this case, during every iteration, the weights are
    updated depending on whether the neuron fires along with the constant change introduced
    by the learning rate value. Through this process, the goal is to reduce the error
    rate. In this case, we did not define a formal error rate as we could clearly
    see when the line successfully divided the classes. In the next example, we will
    step away from the underlying math and focus on optimizing a slightly more complex
    neural network using an open source dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model with Wisconsin cancer data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we will use the breast cancer dataset from the University
    of Wisconsin. Details on this dataset can be found at the UCI Machine Learning
    Repository at [http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29).
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be loaded using the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the data, we will encode the target variable by converting the
    column from a character column with two character values that indicate whether
    or not there were signs of malignancy to a numeric data type holding binary values.
    For this type of neural network, we will need all values to be numeric, including
    the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will scale and standardize all of our predictor values. As we mentioned,
    all data needs to be numeric for the neural network, and by scaling and standardizing
    the data, we will increase performance by giving the activation functions a set
    of values that are all constrained within the same boundaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will partition to train and test, just like in our machine learning
    example. We will use the ID column in `X1` for splitting the data in this step;
    however, afterward, we can drop this column. Here, we will use a `tidyverse` approach
    to simplify the process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our last data preparation step is to extract all the actual correct responses
    from the test data and then remove this column from the test dataset. This vector
    of values will be used to calculate performance after modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The data is now completely prepared and ready for modeling with a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create the formula syntax for the `neuralnet` package, which
    is a dependent variable `~`. This is similar to the syntax for fitting a linear
    model with R. All independent variables can be connected with the `+` sign between
    each one; however, when there are many independent variables, then it would be
    very tedious to write the name for every column, even in this example, where the
    column names are just X followed by a number. Fortunately, there is a way to expedite
    this process, which we will use in the following steps. First, we will get the
    names for all of the columns in our train set and then, using paste and collapse,
    we will create the string of independent variables to go on the other side of
    our formula from the dependent variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With this set, we can now fit our model. In this case, we will keep the model
    fairly simple, only using a few of the arguments available for this function.
    Specifically, we include the formula that we just created, defining the dependent
    and independent variables. Next, we indicate that the model will be fit to the
    train data. Choosing the correct number of layers and units per layer involves
    trying a few combinations and comparing performance. In this case, we start with
    two layers containing about half as many units as there are variables. Lastly,
    we note that the activation function should be logistic, which is the sigmoid
    function, and that we are not performing a linear operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With the modeling process complete, we can now use our model to make predictions.
    With the `neuralnet` package, we use the `compute()` function to generate these
    prediction values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When we pass the model and the test dataset through the `compute()` function,
    we are given a list. The list contains details about the neurons within the model,
    along with the predicted values. In this case, we just want the predicted values,
    so we will pull these from the list. In addition, we will create a set of binary
    predictions. The binary predictions are created by changing values to `1`, if
    the predicted probability is greater than `0.5`; otherwise, the value is changed
    to a `0`. We will use each set of predictions for two different model evaluation
    methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Using our binary predictions, we can easily calculate basic model accuracy—that
    is, we will sum the number of cases where the binary predicted value matches the
    actual value and divide that number by the total number of actual values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that the accuracy is 92.98%, so our basic neural net has performed
    quite well on this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the breakdown for this accuracy value by using a confusion
    matrix. The simplest way to produce a confusion matrix is to use the `confusionMatrix()`
    function, which is part of the `caret` package. This function requires a table
    containing the predicted valued and the actual values as an argument. In this
    case, we need to use our binary predictions as the results need to fit into one
    of four categories, and as such, levels of granularity are not permitted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After calling this function, we see that we are provided with a two by two
    grid containing the results. The confusion matrix has categorized our predictions
    into the following four outcomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**True positives**: Values of `1` correctly predicted to be `1`. The actual
    test target variable contains the value we are predicting and we correctly predicted
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type I errors**: Values of `0` incorrectly predicted to be values of `1`.
    The actual test target variable does not have the value we are predicting; however,
    we predicted that it will. Also referred to as a false positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type II errors**: Values of `1` incorrectly predicted to be values of `0`.
    The actual test target variable does have the value we are predicting; however,
    we predicted that it will not. Also referred to as a false negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives**: Values of `0` correctly predicted to be `0`. The actual
    test target variable does not contain the value we are predicting and we correctly
    predicted that it would not have it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also includes a number of other statistical measures that are outside the
    scope of this chapter; however, we can note that the accuracy is included and
    the value matches the value that we just calculated ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using our binary prediction to calculate accuracy, we can also
    use our probabilities so that we take into account the level of certainty for
    each outcome. In order to measure performance using these values, we will use
    the AUC, or area under the curve, score. This compares the probabilities for true
    positive cases with the probabilities for false positive cases. The final result
    is a measure of the confidence that positive values are positive and that negative
    values are negative, or in this case, that negative values are not incorrectly
    labeled positive with high confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the AUC score, we can use the `auc()` function, which is part
    of the `Metrics` package. The function takes two arguments—a vector of actual
    values and a vector of predicted probabilities that a record should be classified
    as the target variable based on the model''s interpretation of the independent
    variable for that row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The AUC score of 0.987 is even stronger than the accuracy score calculated previously.
  prefs: []
  type: TYPE_NORMAL
- en: This model is already working very well at solving the prediction task using
    this dataset; however, we will now try to add a backpropagation step and see if
    we can improve performance further.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting our neural network with backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have a working neural network. For this simple example, we
    will add one additional feature of neural networks that can improve performance,
    which is backpropagation. A neural network can learn to solve a task by multiplying
    the variable by values so that the variables are weighted as they pass through
    hidden layers. The backpropagation step allows the model to traverse back through
    layers and adjust the weights that were learned during previous steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, this step is quite straightforward to implement. We simply
    declare that we will use the backpropagation algorithm and indicate the learning
    rate, which controls how much the weights are adjusted. In general, this learning
    rate value should be very low.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following example, we have to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `threshold` value and `stepmax` value have to be changed as the model failed
    to converge using the default values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `threshold` argument defines the value that the error rate must reach before
    the model stops and the `stepmax` argument defines the number of iterations the
    model will run before stopping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By changing these values, you can program the model to run longer and stop
    sooner, both of which will help if you run into an error when converging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this new version of the model, we can run the same steps again
    to assess performance. First, we will run `compute` on the new model to get new
    predictions. We will once again create a vector of probabilities and binary predictions,
    and as a first step, we will create the table of binary prediction values and
    actual values and pass this to the `confusionMatrix()` function. We will skip
    calculating the accuracy this time around as it is included in the output from
    the call to the `confusionMatrix()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Our accuracy has improved, increasing from 92.98% to 94.74%. Let''s now check
    our AUC score. Again, we simply pass the actual values and predicted probabilities
    to the `auc()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Our AUC score has improved, increasing from 0.987 to 0.993, so we can see that
    backpropagation does improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, what exactly is happening during this step?
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation step takes the derivative of the error rate and uses this
    to update weights based on results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative is just the rate at which the current weights impact the error
    rate. So if the derivative rate is `7`, then changing the weights by a single
    unit will result in a change to the error rate that is 7 times larger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using just a feedforward neural network, we can update the initial weights based
    on the final derivative value; however, using backpropagation, we can update the
    weights at every neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using information about how previous changes have impacted the derivative, this
    step either increases or decreases the weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate is applied so that changes are never dramatic but rather smooth
    and gradual. This process can continue until the error rate is minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that deep learning is differentiated from other
    machine-learning algorithms because of the use of multiple hidden layers. This
    network of hidden layers, which are composed of artificial neurons, was designed
    to mimic the way our brain processes input signals to interpret our environment.
    The units within the hidden layers take in all the independent variables and apply
    some weights to these variables. In this way, each neuron classifies the combination
    of input values in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: From understanding the architecture of this type of machine learning from a
    high level, we then took a deeper dive into the actual process of converting the
    input to predictions using this approach. We discussed the various activation
    functions that act as the gate for every neuron, determining whether a signal
    should be passed to the next layer. We then built two feedforward neural networks—one
    using base R for a better understanding of what is happening and another using
    the `neuralnet` package on a larger dataset. Lastly, we applied the backpropagation
    step to further improve our model.
  prefs: []
  type: TYPE_NORMAL
- en: As stated, the artificial neural network is the fundamental building block for
    more complex deep learning, and now that we have this understanding, we will move
    on to creating convolutional neural networks for image recognition in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
