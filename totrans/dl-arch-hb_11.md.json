["```py\n     from transformers import (\n        DebertaForSequenceClassification,\n        EvalPrediction,\n        DebertaConfig,\n        DebertaTokenizer,\n        Trainer,\n        TrainingArguments,\n        IntervalStrategy,\n        EarlyStoppingCallback\n    )\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    import torch\n    ```", "```py\n    df = pd.read_csv('text_sentiment_dataset.csv')\n    ```", "```py\n    label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n    id2label = {v: k for k, v in label2id.items()}\n    ```", "```py\n    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n    def preprocess_function(examples):\n        inputs = tokenizer(examples[\"Text\"].values.tolist(), padding=\"max_length\", truncation=True)\n        inputs[\"labels\"] = [label2id[label] for label in examples[\"Sentiment\"].values]\n        return inputs\n    ```", "```py\n    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n    ```", "```py\n    class TextClassificationDataset(torch.utils.data.Dataset):\n        def __init__(self, examples):\n            self.examples = examples    def __getitem__(self, index):\n            return {k: torch.tensor(v[index]) for k, v in self.examples.items()}    def __len__(self):\n            return len(self.examples[\"input_ids\"])train_dataset = TextClassificationDataset(preprocess_function(train_df))\n    test_dataset = TextClassificationDataset(preprocess_function(test_df))\n    ```", "```py\n    deberta_config = {\n               \"model_type\": \"deberta-v2\",\n               \"attention_probs_dropout_prob\": 0.1,\n               \"hidden_act\": \"gelu\",\n               \"hidden_dropout_prob\": 0.1,\n               \"hidden_size\": 768,\n               \"initializer_range\": 0.02,\n               \"intermediate_size\": 3072,\n               \"max_position_embeddings\": 512,\n               \"relative_attention\": True,\n               \"position_buckets\": 256,\n               \"norm_rel_ebd\": \"layer_norm\",\n               \"share_att_key\": True,\n               \"pos_att_type\": \"p2c|c2p\",\n               \"layer_norm_eps\": 1e-7,\n               \"max_relative_positions\": -1,\n               \"position_biased_input\": False,\n               \"num_attention_heads\": 12,\n               \"num_hidden_layers\": 12,\n               \"type_vocab_size\": 0,\n               \"vocab_size\": 128100\n    }\n    model_config = DebertaConfig(id2label=id2label, label2id=label2id, **deberta_v3_config)\n    model = DebertaForSequenceClassification(model_config)\n    ```", "```py\n    def compute_metrics(p: EvalPrediction):\n        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        preds = np.argmax(preds, axis=1)\n        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n    ```", "```py\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        num_train_epochs=1000,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        evaluation_strategy=IntervalStrategy.EPOCH,\n        save_strategy=IntervalStrategy.EPOCH,\n        load_best_model_at_end=True,\n        learning_rate=0.000025,\n        save_total_limit=2,\n    )\n    ```", "```py\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,\n        callbacks = [EarlyStoppingCallback(early_stopping_patience=20)]\n    )\n    ```", "```py\n    trainer.train()\n    trainer_v2.evaluate()\n    ```", "```py\n    {'eval_loss': 0.1446695625782013,\n     'eval_accuracy': 0.9523809552192688,\n     'eval_runtime': 19.497,\n     'eval_samples_per_second': 1.077,\n     'eval_steps_per_second': 0.103}\n    ```", "```py\n    from transformers_interpret import SequenceClassificationExplainer\n    cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n    ```", "```py\n    for idx in [0, 8, 6, 12]:\n        text = test_df['Text'].iloc[idx]\n        label = test_df['Sentiment'].iloc[idx]\n        word_attributions = cls_explainer(text)\n        cls_explainer.visualize()\n    ```", "```py\n\"I have built a sentiment classification model using HuggingFace defined classes, with 3 labels \"neutral\", \"positive\", \"negative\". I also utilized the integrated gradients method to obtain token attributions for the examples below: First Example with correctly predicted positive sentiment [('[CLS]', 0.0), ('I', 0.04537756915960333), ('was', 0.32506422578050986), ('hesitant', -0.011328562581410014), ('to', 0.2591512168119563), ('try', 0.07210600939837246), ('that', 0.1872696259797412), ('new', 0.25720454853718405), ('restaurant', 0.09906425532043227), (',', -0.08526821845533564), ('but', 0.26155394517459857), ('it', 0.029711535733103797), ('ended', 0.03380972261608883), ('up', 0.10830705227933887), ('being', 0.09771634151976362), ('amazing', 0.08913346633282519), ('and', 0.1429717885722044), ('I', 0.3408664658447834), ('was', 0.3248006076552883), ('far', 0.11463247647745932), ('from', 0.29795241654484333), ('bored', 0.4944594024849995), ('.', 0.10894853119482743), ('', 0.0376325816741505), ('[SEP]', 0.0)] Second example with correctly predicted positive sentiment [('[CLS]', 0.0), ('I', 0.00293608614786346), (\"'m\", 0.30945414416090317), ('happy', -0.028173800615734133), ('to', 0.31870826361945737), ('have', 0.15675763188868433), ('a', 0.07581024366375723), ('job', -0.020856788304256305), ('that', 0.20868353147138577), ('keeps', 0.2024940149527261), ('me', 0.046306584007035426), ('from', 0.41459097144711715), ('getting', 0.059558828031696716), ('bored', 0.7089862879353616), ('.', 0.0744570784079903), ('', -0.02775527287562979), ('[SEP]', 0.0)],  Tell me anything wrong about this model to predict positive and negative attributions. Provide me a complete analysis and see if the most highly attributed words make sense for its predicted label. Judge the models based on the relative score and focus on the highest scored token individually and see whether it make sense.\"\n```", "```py\nThird example with correctly predicted negative sentiment\n[('[CLS]', 0.0), ('It', 0.1781266370024602), ('feels', -0.08200280620716707), ('like', -0.03204718164120957), ('this', -0.05573060889100964), ('day', 0.6176120940811932), ('is', -0.0032201843949800626), ('never', 0.20518569446424909), ('going', 0.07198217726867306), ('to', 0.45876287617527467), ('end', -0.2538220352877727), ('.', -0.01766876291225962), ('Everything', 0.2927810551003227), ('just', 0.27789343264100713), ('keeps', 0.25860811147952395), ('dragging', 0.013653081036146358), ('on', 0.12974700461873728), ('.', -0.01804386178839869), ('', 0.07940230083266143), ('[SEP]', 0.0)]\nFourth example with correctly predicted negative sentiment [('[CLS]', 0.0), ('Everything', 0.051921060773658365), ('that', 0.1853413631332932), ('could', 0.16644341050771988), ('have', 0.09960723832267326), ('gone', 0.21400474006976702), ('wrong', 0.26952621495013096), (',', -0.02550525756341266), ('has', 0.46511206263887517), ('gone', 0.21385297386310367), ('wrong', 0.2691861963676513), ('this', 0.08573245861889027), ('day', 0.5967441822322336), ('.', 0.00910580036050777), ('I', 0.16215574709799985), (\"'m\", -0.05665909043004983), ('so', 0.20818789342264885), ('frustrated', 0.11472844834437185), ('.', 0.00924431805079032), ('', 0.1485924275199782), ('[SEP]', 0.0)]\n```", "```py\nfrom transformers import TextClassificationPipeline\nfull_model = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(full_model(\"day\"))\nprint(full_model(\"day day\"))\nprint(full_model(\"day day day\"))\nprint(full_model(\"day day day day\"))\nprint(full_model(\"bored\"))\nprint(full_model(\"bored bored\"))\nprint(full_model(\"bored bored bored\"))\nprint(full_model(\"bored bored bored bored\"))\n```", "```py\n[{'label': 'neutral', 'score': 0.9857920408248901}]\n[{'label': 'negative', 'score': 0.5286906361579895}]\n[{'label': 'negative', 'score': 0.9102224111557007}]\n[{'label': 'negative', 'score': 0.9497435688972473}]\n[{'label': 'neutral', 'score': 0.9852404594421387}]\n[{'label': 'positive', 'score': 0.8067558407783508}]\n[{'label': 'positive', 'score': 0.974130392074585}]\n[{'label': 'positive', 'score': 0.9776705503463745}]\n```"]