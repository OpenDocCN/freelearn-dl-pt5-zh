["```py\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\ninpt_dim = 32*32*3    # this corresponds to the dataset\n                      # to be explained shortly\ninpt_vec = Input(shape=(inpt_dim,), name='inpt_vec')\ndl = Dropout(0.5, name='d1')(inpt_vec)\nl1 = Dense(inpt_dim, activation='relu', name='l1')(dl)\nd2 = Dropout(0.2, name='d2')(l1)\nl2 = Dense(inpt_dim, activation='relu', name='l2') (d2)\noutput = Dense(10, activation='sigmoid', name='output') (l2)\n\nwidenet = Model(inpt_vec, output, name='widenet')\n\nwidenet.compile(loss='binary_crossentropy', optimizer='adam')\nwidenet.summary()\n```", "```py\nModel: \"widenet\"\n_________________________________________________________________\nLayer (type)           Output Shape    Param # \n=================================================================\ninpt_vec (InputLayer)  [(None, 3072)]  0 \n_________________________________________________________________\nd1 (Dropout)           (None, 3072)    0 \n_________________________________________________________________\nl1 (Dense)             (None, 3072)    9440256 \n_________________________________________________________________\nd2 (Dropout)           (None, 3072)    0 \n_________________________________________________________________\nl2 (Dense)             (None, 3072)    9440256 \n_________________________________________________________________\noutput (Dense)         (None, 10)      30730 \n=================================================================\nTotal params: 18,911,242\nTrainable params: 18,911,242\nNon-trainable params: 0\n```", "```py\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nimport NumPy as np\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Makes images floats between [0,1]\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\n# Reshapes images to make them vectors of 3072-dimensions\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\n# Converts list of numbers into one-hot encoded vectors of 10-dim\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n```", "```py\nx_train shape: (50000, 3072)\nx_test shape: (10000, 3072)\n```", "```py\nwidenet.fit(x_train, y_train, epochs=100, batch_size=1000, \n            shuffle=True, validation_data=(x_test, y_test))\n```", "```py\nwidenet.save_weights(\"widenet.hdf5\")\n```", "```py\nwidenet.load_weights(\"widenet.hdf5\")\n```", "```py\nwidenet.save(\"widenet.hdf5\")\n```", "```py\nfrom tensorflow.keras.models import load_model\n\nwidenet = load_model(\"widenet.hdf5\")\n```", "```py\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n```", "```py\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20)\n\nwidenet.fit(x_train, y_train, batch_size=128, epochs=100, \n            callbacks=reduce_lr, shuffle=True, \n            validation_data=(x_test, y_test))\n```", "```py\nfrom tensorflow.keras.callbacks import EarlyStopping\n```", "```py\nstop_alg = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n\nwidenet.fit(x_train, y_train, batch_size=128, epochs=1000, \n            callbacks=stop_alg, shuffle=True, \n            validation_data=(x_test, y_test))\n```", "```py\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20)\nstop_alg = EarlyStopping(monitor='val_loss', patience=100, \n                         restore_best_weights=True)\n\nhist = widenet.fit(x_train, y_train, batch_size=1000, epochs=1000, \n                   callbacks=[stop_alg, reduce_lr], shuffle=True, \n                   validation_data=(x_test, y_test))\n\nwidenet.save_weights(\"widenet.hdf5\")\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], color='#dc267f')\nplt.title('Model reconstruction loss')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Test Set'], loc='upper right')\nplt.show()\n```", "```py\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\n```", "```py\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nimport NumPy as np\n\ny_hat = widenet.predict(x_test) # we take the neuron with maximum\ny_pred = np.argmax(y_hat, axis=1)  # output as our prediction\n\ny_true = np.argmax(y_test, axis=1)   # this is the ground truth\nlabels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nprint(classification_report(y_true, y_pred, labels=labels))\n\ncm = confusion_matrix(y_true, y_pred, labels=labels)\nprint(cm)\n\nber = 1- balanced_accuracy_score(y_true, y_pred)\nprint('BER:', ber)\n```", "```py\n    precision  recall  f1-score  support\n0   0.65       0.59    0.61      1000\n1   0.65       0.68    0.67      1000\n2   0.42       0.47    0.44      1000\n3   0.39       0.37    0.38      1000\n4   0.45       0.44    0.44      1000\n5   0.53       0.35    0.42      1000\n6   0.50       0.66    0.57      1000\n7   0.66       0.58    0.62      1000\n8   0.62       0.71    0.67      1000\n9   0.60       0.57    0.58      1000\n\naccuracy               0.54      10000\n\n[[587  26  86  20  39   7  26  20 147  42]\n [ 23 683  10  21  11  10  22  17  68 135]\n [ 63  21 472  71 141  34 115  41  24  18]\n [ 19  22  90 370  71 143 160  43  30  52]\n [ 38  15 173  50 442  36 136  66  32  12]\n [ 18  10 102 224  66 352 120  58  29  21]\n [  2  21  90  65  99  21 661   9  14  18]\n [ 36  15  73  67  90  45  42 582  13  37]\n [ 77  70  18  24  17   3  20   9 713  49]\n [ 46 167  20  28  14  14  30  36  74 571]]\n\nBER: 0.4567\n```", "```py\nwhile units > 10: \n  dl = Dropout(dr)(dl)\n  dl = Dense(units, activation='relu')(dl)\n  units = units//2\n  dr = dr/1.5\n```", "```py\n# Dimensionality of input for CIFAR-10\ninpt_dim = 32*32*3\n\ninpt_vec = Input(shape=(inpt_dim,))\n\nunits = inpt_dim    # Initial number of neurons \ndr = 0.5    # Initial drop out rate   \n\ndl = Dropout(dr)(inpt_vec)\ndl = Dense(units, activation='relu')(dl)\n\n# Iterative creation of bottleneck layers\nunits = units//2\ndr = dr/2\nwhile units>10: \n dl = Dropout(dr)(dl)\n dl = Dense(units, activation='relu')(dl)\n units = units//2\n dr = dr/1.5\n\n# Output layer\noutput = Dense(10, activation='sigmoid')(dl)\n\ndeepnet = Model(inpt_vec, output)\n```", "```py\ndeepnet.compile(loss='binary_crossentropy', optimizer='adam')\ndeepnet.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, \n                              min_delta=1e-4, mode='min')\nstop_alg = EarlyStopping(monitor='val_loss', patience=100, \n                         restore_best_weights=True)\nhist = deepnet.fit(x_train, y_train, batch_size=1000, epochs=1000, \n                   callbacks=[stop_alg, reduce_lr], shuffle=True, \n                   validation_data=(x_test, y_test))\n\ndeepnet.save_weights(\"deepnet.hdf5\")\n```", "```py\nModel: \"model\"\n_________________________________________________________________\nLayer (type)          Output Shape    Param # \n=================================================================\ninput_1 (InputLayer)  [(None, 3072)]  0 \n_________________________________________________________________\ndropout (Dropout)     (None, 3072)    0 \n_________________________________________________________________\ndense (Dense)         (None, 3072)    9440256 \n_________________________________________________________________\n.\n.\n.\n_________________________________________________________________\ndense_8 (Dense)       (None, 12)      300 \n_________________________________________________________________\ndense_9 (Dense)       (None, 10)      130 \n=================================================================\nTotal params: 15,734,806\nTrainable params: 15,734,806\nNon-trainable params: 0\n```", "```py\n     precision  recall  f1-score  support\n\n 0   0.58       0.63    0.60      1000\n 1   0.66       0.68    0.67      1000\n 2   0.41       0.42    0.41      1000\n 3   0.38       0.35    0.36      1000\n 4   0.41       0.50    0.45      1000\n 5   0.51       0.36    0.42      1000\n 6   0.50       0.63    0.56      1000\n 7   0.67       0.56    0.61      1000\n 8   0.65       0.67    0.66      1000\n 9   0.62       0.56    0.59      1000\n\n accuracy               0.53      10000\n\n[[627  22  62  19  45  10  25  18 132  40]\n [ 38 677  18  36  13  10  20  13  55 120]\n [ 85  12 418  82 182  45  99  38  23  16]\n [ 34  14 105 347  89 147 161  50  17  36]\n [ 58  12 158  34 496  29 126  55  23   9]\n [ 25   7 108 213  91 358 100  54  23  21]\n [ 9   15  84  68 124  26 631   7  11  25]\n [ 42  23  48  58 114  57  61 555  10  32]\n [110  75  16  22  30  11   8   5 671  52]\n [ 51 171  14  34  16   9  36  36  69 564]]\n\nBER 0.4656\n```", "```py\n# Dimensionality of input for CIFAR-10\ninpt_dim = 32*32*3\n\ninpt_vec = Input(shape=(inpt_dim,))\n\nunits = inpt_dim    # Initial number of neurons \ndr = 0.5    # Initial drop out rate   \n\ndl = Dropout(dr)(inpt_vec)\ndl = Dense(units, activation='relu', \n           kernel_regularizer=regularizers.l2(0.0001))(dl)\n\n# Iterative creation of bottleneck layers\nunits = units//2\ndr = dr/2\nwhile units>10: \n  dl = Dropout(dr)(dl)\n  dl = Dense(units, activation='relu', \n             kernel_regularizer=regularizers.l2(0.0001))(dl)\n  units = units//2\n  dr = dr/1.5\n\n# Output layer\noutput = Dense(10, activation='sigmoid', \n               kernel_regularizer=regularizers.l2(0.0001))(dl)\n\nsparsenet = Model(inpt_vec, output)\n```", "```py\nsparsenet.compile(loss='binary_crossentropy', optimizer='adam')\nsparsenet.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, \n                              min_delta=1e-4, mode='min')\nstop_alg = EarlyStopping(monitor='val_loss', patience=100, \n                         restore_best_weights=True)\nhist = sparsenet.fit(x_train, y_train, batch_size=1000, epochs=1000, \n                     callbacks=[stop_alg, reduce_lr], shuffle=True, \n                     validation_data=(x_test, y_test))\n\nsparsenet.save_weights(\"sparsenet.hdf5\")\n```", "```py\n     precision recall f1-score support\n\n 0   0.63      0.64   0.64     1000\n 1   0.71      0.66   0.68     1000\n 2   0.39      0.43   0.41     1000\n 3   0.37      0.23   0.29     1000\n 4   0.46      0.45   0.45     1000\n 5   0.47      0.50   0.49     1000\n 6   0.49      0.71   0.58     1000\n 7   0.70      0.61   0.65     1000\n 8   0.63      0.76   0.69     1000\n 9   0.69      0.54   0.60     1000\n\n accuracy             0.55     10000\n\n[[638  17  99   7  27  13  27  10 137  25]\n [ 40 658  11  32  11   7  21  12 110  98]\n [ 78  11 431  34 169  93 126  31  19   8]\n [ 18  15  96 233  52 282 220  46  14  24]\n [ 47   3 191  23 448  36 162  57  28   5]\n [ 17   6 124 138  38 502 101  47  16  11]\n [  0   9  59  51 111  28 715   8  13   6]\n [ 40   1  66  50  85  68  42 608  12  28]\n [ 76  45  18  16  25   8  22   5 755  30]\n [ 51 165  12  38   6  23  29  43  98 535]]\n\nBER 0.4477\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport NumPy as np\n\n# load and prepare data (same as before)\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n```", "```py\n# A KerasClassifier will use this to create a model on the fly\ndef make_widenet(dr=0.0, optimizer='adam', lr=0.001, units=128):\n  # This is a wide architecture\n  inpt_dim = 32*32*3\n  inpt_vec = Input(shape=(inpt_dim,))\n  dl = Dropout(dr)(inpt_vec)\n  l1 = Dense(units, activation='relu')(dl)\n  dl = Dropout(dr)(l1)\n  l2 = Dense(units, activation='relu') (dl)\n  output = Dense(10, activation='sigmoid') (l2)\n\n  widenet = Model(inpt_vec, output)\n\n  # Our loss and lr depends on the choice\n  if optimizer == 'adam':\n    optmzr = Adam(learning_rate=lr)\n  else:\n    optmzr = RMSprop(learning_rate=lr)\n\n  widenet.compile(loss='binary_crossentropy', optimizer=optmzr, \n                  metrics=['accuracy'])\n\n  return widenet\n```", "```py\n# This defines the model architecture\nkc = KerasClassifier(build_fn=make_widenet, epochs=100, batch_size=1000, \n                     verbose=0)\n\n# This sets the grid search parameters\ngrid_space = dict(dr=[0.2, 0.5],      # Dropout rates\n                  optimizer=['adam', 'rmsprop'], \n                  lr=[0.01, 0.0001],  # Learning rates\n                  units=[1024, 512, 256])\n\ngscv = GridSearchCV(estimator=kc, param_grid=grid_space, n_jobs=1, cv=3, verbose=2)\ngscv_res = gscv.fit(x_train, y_train, validation_split=0.3,\n                    callbacks=[EarlyStopping(monitor='val_loss', \n                                             patience=20, \n                                             restore_best_weights=True),\n                               ReduceLROnPlateau(monitor='val_loss', \n                                                 factor=0.5, patience=10)])\n\n# Print the dictionary with the best parameters found:\nprint(gscv_res.best_params_)\n```", "```py\nFitting 3 folds for each of 24 candidates, totalling 72 fits\n[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 21.1s\n[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................\n[Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 21.1s remaining: 0.0s\n[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 21.8s\n[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................\n[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 12.6s\n[CV] dr=0.2, lr=0.01, optimizer=adam, units=512 ......................\n[CV] ....... dr=0.2, lr=0.01, optimizer=adam, units=512, total= 25.4s\n.\n.\n.\n[CV] .. dr=0.5, lr=0.0001, optimizer=rmsprop, units=256, total= 9.4s\n[CV] dr=0.5, lr=0.0001, optimizer=rmsprop, units=256 .................\n[CV] .. dr=0.5, lr=0.0001, optimizer=rmsprop, units=256, total= 27.2s\n[Parallel(n_jobs=1)]: Done 72 out of 72 | elapsed: 28.0min finished\n\n{'dr': 0.2, 'lr': 0.0001, 'optimizer': 'adam', 'units': 1024}\n```"]