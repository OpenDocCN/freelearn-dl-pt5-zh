- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System Design and Engineering Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding **Machine Learning** (**ML**) and deep learning concepts is essential,
    but if you’re looking to build an effective search solution powered by **Artificial
    Intelligence** (**AI**) and deep learning, you need production engineering capabilities
    as well. Effectively deploying ML models requires competencies more commonly found
    in technical fields such as software engineering and DevOps. These competencies
    are called **MLOps**. This is particularly the case for a search system that requires
    high useability and low latency.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn the basics of designing a search system. You
    will understand core concepts such as **indexing** and **querying** and how to
    use them to save and retrieve information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing and querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a neural search system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering challenges in building a neural search system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you will have a full understanding of the capabilities
    and possible difficulties to overcome when putting a neural search into production.
    You will be able to assess when it is useful to use neural search and which approach
    would be the best for your own search system.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A laptop with a minimum of 4 GB of RAM; 8 GB is suggested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python installed, with version 3.7, 3.8, or 3.9 on a Unix-like operating system,
    such as macOS or Ubuntu.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for the chapter are available at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing indexing and querying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will go through two important high-level tasks to build
    a search system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Indexing**: This is the process of collecting, parsing, and storing data
    to facilitate fast and accurate information retrieval. This includes adding, updating,
    deleting, and reading documents to be indexed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Querying**: Querying is the process of parsing, matching, and ranking the
    user query and sending relevant information back to the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a neural search system, both indexing and querying are composed of a sequence
    of tasks. Let’s take a deep look at indexing and querying components.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indexing is an important process in search systems. It forms the core functionality
    since it assists in retrieving information efficiently. Indexing reduces the documents
    to the useful information contained in them. It maps the terms to the respective
    documents containing the information. The process of finding a relevant document
    in a search system is essentially identical to the process of looking at a dictionary,
    where the index helps you find words effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before introducing the details, we start by asking the following questions
    to understand where we stand:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the major components of an indexing pipeline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What content can be indexed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we index incrementally and how do we index at speed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t know the answers to these questions, don’t worry. Just keep reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'In an indexing pipeline, we normally have three major components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text/plain`, we might need a tokenizer and a stemmer, as was introduced in
    [*Chapter 1*](B17488_01.xhtml#_idTextAnchor014), *Neural Networks for Neural Search*.
    If we want to index an image of modality `image/jpeg`, we might want a component
    to resize or transform the input image into the expected format of the neural
    networks. It highly depends on your task and input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder**: An encoder, in a neural search system, is identical to neural
    networks. This neural network takes your preprocessed input as a vector representation
    (embeddings). After this step, each raw document composed of text, images, videos,
    or even DNA information should be represented as a vector of numerical values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indexer (for storage)**: An indexer, better known as **StorageIndexer**,
    at the indexing stage stores the vectors produced from the encoder into storage,
    such as memory or a database. This includes relational databases (PostgresSQL),
    NoSQL (MongoDB), or even better, a vector database, such as Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be noted that every indexing task is independent. It could vary from
    different perspectives. For instance, if you are working on a multi-model search
    engine in an e-commerce context, your objective is to create a search system that
    can take both text and an image as a query and find the most relevant products.
    In this case, your indexing might have two pathways:'
  prefs: []
  type: TYPE_NORMAL
- en: Textual information should be preprocessed and encoded using text-based preprocessors
    and encoders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likewise, image data should be preprocessed and encoded using image-based preprocessors
    and encoders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might wonder what can be indexed. Anything, as long as you have an encoder
    and your data can be encoded. Some common data types you can index include text,
    image, video, and audio. As we discussed in previous chapters, you can encode
    source code to build a source code search system or gene information to build
    a search system around that. The following figure illustrates an indexing pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A simple indexing pipeline takes documents as input and applies
    preprocessing and encoding. In the end, save the encoded features into storage
    ](img/Figure_3.1_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – A simple indexing pipeline takes documents as input and applies
    preprocessing and encoding. In the end, save the encoded features into storage
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we move on to an important topic: incremental indexing. Firstly, let’s
    discuss what incremental indexing is.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding incremental indexing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Incremental indexing** is a crucial feature for any search system. Given
    the fact that the data collection we want to index is likely to change dramatically
    every day, we cannot afford to index the entire data collection each time there
    is a small change.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are two common practices to perform an indexing task, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time indexing**: Given any data being sent to the collection, the indexer
    immediately adds the document to the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduled indexing**: Given any data being sent to the collection, the scheduler
    triggers the indexing task and performs the indexing job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding practices have their advantages and disadvantages. In real-time
    indexing, the user gets newly added documents immediately (if it is a match),
    while also consuming more system resources and potentially introducing data inconsistency.
    However, in the case of scheduled indexing, users don’t get access to newly added
    results in real time but it is less error prone and easier to manage.
  prefs: []
  type: TYPE_NORMAL
- en: The indexing strategy you choose depends on your task. If the task is time sensitive,
    it is better to use real-time indexing. Otherwise, it is good to set up a cron
    job and index your data incrementally at a certain time.
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up indexing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another crucial issue when performing an indexing task in neural search is the
    speed of indexing. While a symbolic search system works only on textual data,
    the input of a neural search system could be three-dimensional (*Height * Width
    * ColorChannel*), such as an RGB image, or four-dimensional (*Frame * Height *
    Width * ColorChannel*), such as a video. This kind of data can be indexed with
    different modalities, which can dramatically slow down the data preprocessing
    and encoding process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are several strategies that we can use to boost the indexing
    speed. Some of these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessor**: Applying certain preprocessing on certain datasets could
    greatly boost your indexing speed. For instance, if you want to index high-resolution
    images, it’s better to resize them and make them smaller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU inference**: In a neural search system, encoding takes most of the indexing
    time. To be more specific, given a preprocessed document, it takes time to employ
    the deep neural networks to encode the document into a vector. It could be greatly
    improved by making use of a GPU instance for encoding. Since the GPU has much
    higher bandwidth memory and L1 cache, the GPU is suitable for ML tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: Indexing a huge amount of data on a single machine
    makes the process slow, but it could be much faster if we distribute data across
    multiple machines and perform indexing in parallel. For example, the following
    figure demonstrates assigning more encoders to the pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference
    in parallel ](img/Figure_3.2_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Indexing at speed with three encoders utilizing GPU inference in
    parallel
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth mentioning that if you come from a text retrieval background, **index
    compression** also matters when constructing an inverted index for symbolic search.
    This is not exactly the same in a neural search system anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the encoder takes a document as input and encodes the content
    of the document into an N-dimensional vector (embeddings). Thus, we can think
    of the encoder itself as a compression function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, compression of the dense vectors will eventually sacrifice the quality
    of the vectors. Normally, larger-dimensionality vectors bring better search results
    since they can better represent the documents being encoded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, we need to find a balance point between dimensionality and memory
    usage in order to load all vectors into memory to perform a large-scale similarity
    search. In the next section, we will dive into the querying part, which will allow
    you to understand how to conduct a large-scale similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: Querying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to the querying pipeline, it has a lot of components overlapping
    with the indexing pipeline, but with a few modifications and additional components,
    such as a ranker. At this stage, the input of the pipeline is a single user query.
    There are four major components of a typical querying task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessor**: This component is similar to the preprocessor in the indexing
    pipeline. It takes the query document as input and applies the same preprocessors
    as the indexing pipeline to the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder**: The encoder takes the preprocessed query document as input and
    produces vectors as output. It should be noted that in a cross-modal search system,
    your encoder from indexing might be different from the encoder at the querying
    step. This will be explained in [*Chapter 7*](B17488_07.xhtml#_idTextAnchor101),
    *Exploring Advanced Use Cases of Jina*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indexer**: This indexer, better named **SearchIndexer**, takes vectors produced
    from the encoder as input and conducts a large-scale similarity search over all
    indexed documents. This is called **Approximate Nearest Neighbor** (**ANN**) search.
    We’ll elaborate more on this concept in the following section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ranker**: The ranker takes the query vector and similarity scores against
    each of the item within the collection, produces a ranked list in descending order,
    and returns results to the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One major difference between indexing and querying is that indexing (in most
    cases) is an offline task while querying is an online task. To be more concrete,
    when we bootstrap a neural search system and create a query, the system will return
    an empty list since nothing has been indexed at the moment. Before we *expose*
    the search system to the user, we should have pre-indexed all the documents within
    the data collection. This indexing is performed offline.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in a querying task, the user sends one query to the system
    and expects to get matches immediately. All the preprocessing, encoding, index
    searching, and ranking should be finished during the waiting time. Thus, it is
    an online task.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Real-time indexing can be considered an online task.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike indexing, while querying, each user sends a single document as a query
    to the system. The preprocessing and encoding take a very short time. On the other
    hand, finding similar items in the index storage becomes a critical engineering
    challenge that impacts the neural search system performance. Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you have pre-indexed a billion documents, and at querying time,
    the user sends one query to the system, the document is then preprocessed and
    encoded into a vector (embedding). Given the query vector, now you need to find
    the top N similar vectors out of 1 million vectors. How do you achieve that? Conducting
    similarity searches by computing distances between vectors one by one could take
    ages. Rather than that, we perform an ANN search.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: When we’re talking about ANN search, we’re considering a million-/billion-scale
    search. If you want to build a toy example and search across hundreds or thousands
    of documents, a normal linear scan is fast enough. In a production environment,
    please follow the selection strategy as will be introduced in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ANN search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ANN search, as defined by its name, is a trade-off between different factors:
    accuracy, runtime, and memory consumption. Compared with brute-force search, it
    ensures the running time will be accepted by the user while sacrificing precision/recall
    to a certain degree. How fast can it achieve? Given a billion 100-dimensional
    vectors, it can be fitted into a server with 32 GB memory with a 10 ms response
    rate. Before diving into details about ANN search, let’s first take a look at
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui) ](img/Figure_3.3_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – ANN cheat sheet (source: Billion-scale Approximate Nearest Neighbor
    Search, Yusuke Matsui)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure illustrates *how to select the ANN library given your search
    system*. In the figure, *N* represents the number of documents inside your *StorageIndexer*.
    Different numbers of N can be optimized using different ANN search libraries,
    such as `FAISS` ([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))
    or `NMSLIB` ([https://github.com/nmslib/nmslib](https://github.com/nmslib/nmslib)).
    Meanwhile, as you’re most likely to be a Python user, the `Annoy` library ([https://github.com/spotify/annoy](https://github.com/spotify/annoy))
    has provided a user-friendly interface with reasonable performance, and it works
    well enough for a million-scale vector search.
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned libraries were implemented based on different algorithms,
    the most popular ones being **KD-Trees**, **Locally Sensitive Hashing** (**LSH**),
    and **Product Quantization** (**PQ**).
  prefs: []
  type: TYPE_NORMAL
- en: 'KD-Trees follows an iterative process to construct a tree. To make the visualization
    easier, we suppose the data only consists of two features, *f1* (*x* axis) and
    *f2* (*y* axis), which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – KD-Trees, sample dataset to index ](img/Figure_3.4_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – KD-Trees, sample dataset to index
  prefs: []
  type: TYPE_NORMAL
- en: 'Construction of a KD-Tree starts with selecting a practical feature and setting
    a threshold for this feature. To illustrate the idea, we begin with a manual selection
    of f1 and a feature threshold of 0.5\. To this end, we get a boundary like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – KD-Trees construction iteration 1 ](img/Figure_3.5_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – KD-Trees construction iteration 1
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from *Figure 3.5*, the feature space has been split into two
    parts by our first selection of f1 with a threshold of 0.5\. How is it reflected
    for the tree? When building the index, we’re essentially creating a binary search
    tree. The first selection of f1 with a threshold of 0.5 became our root node.
    Given each data point, if the f1 is greater than 0.5, it will be placed to the
    right of the node. Otherwise, as shown in *Figure 3.6*, we put it to the left
    of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – KD-Trees construction iteration 2 ](img/Figure_3.6_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – KD-Trees construction iteration 2
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue from the preceding tree. In the second iteration, let’s define
    our rule as: given f1 > 0.5, select f2 with threshold 0.5\. As was shown in the
    preceding graph, now we split the feature space again based on the new rule, and
    it is also reflected on our tree: we created a new node, **f2-0.5**, in the figure
    (the **none** node is only for visualization purpose; we haven’t created this
    node). This is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – KD-Tree construction iteration N (final iteration) ](img/Figure_3.7_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – KD-Tree construction iteration N (final iteration)
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3.7*, the entire feature space has been split into six
    bins. Compared with before, we added three new nodes, including two leaf nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: The previous **none** was replaced by an actual node, **f2-0.65**; this node
    split the space of f2 based on the threshold 0.65, and it only happens when f1<0.5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When f2<0.65, we further split f1 by a threshold of 0.2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When f2>0.65, we further split f1 by a threshold of 0.3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To this end, our tree has three leaf nodes, each leaf node can construct two
    bins (less/larger than the threshold), and we have six bins in total. Also, each
    data point can be placed into one of the bins. Then, we finish the construction
    of the KD-Tree. It should be noted that constructing a KD-Tree could be non-trivial
    since you need to consider some hyperparameters, such as how to set the threshold
    or how many bins we should create (or the stop criteria). In practice, there are
    no golden rules. Normally, the mean or median can be used to set the threshold.
    The number of bins could be highly dependent on the evaluation of the results
    and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: At search time, given a user query, it can be placed into one of the bins inside
    the feature space. We are able to compute the distance between the query and all
    the items within the bin as candidates for nearest neighbors. We also need to
    compute the minimum distance between the query and all other bins. If the distance
    between the query vector and other bins is greater than the distance between the
    query vector and the candidate for nearest neighbors, we can ignore all data points
    within that bin by pruning the leaf node of the tree. Otherwise, we consider the
    data points within that bin as candidates for nearest neighbors as well.
  prefs: []
  type: TYPE_NORMAL
- en: By constructing a KD-Tree, we do not necessarily compute the similarity between
    a query vector and each document anymore. Only a certain number of bins should
    be considered as candidates. Thus, the search time can be greatly reduced.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, KD-Trees suffer from the curse of dimensionality. It is tricky
    to apply them to high-dimensional data because there are so many bins to search
    through simply because for each feature, we always create several thresholds.
    **Locality Sensitive Hashing** (**LSH**) could be a good alternative algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind LSH is that similar documents share the same hash code
    and it is designed to maximize collisions. To be more concrete: given a set of
    vectors, we want to have a hashing function that is able to encode similar documents
    into the same hashing bucket. Then, we only need to find similar vectors within
    the bucket (without the need to scan all the data).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with LSH index construction. At indexing time, we first need to
    create random planes (hyperplanes) to split the feature space into *bins*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – LSH index construction with random hyperplanes ](img/Figure_3.8_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – LSH index construction with random hyperplanes
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 3.8*, we have created six hyperplanes. Each hyperplane is able to
    split our feature space into two bins, either left/right or up/bottom, which can
    be represented as binary codes (or signs): 0 or 1\. This is called the index of
    a bin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to get the bin index of the bottom-right bin (which has four points
    in the bin). The bin is located at the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Right of **plane1**, so the sign at position 0 is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right of **plane2**, so the sign at position 1 is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right of **plane3**, so the sign at position 2 is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right of **plane4**, so the sign at position 3 is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottom of **plane5**, so the sign at position 4 is 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottom of **plane6**, so the sign at position 5 is 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we can represent the bottom-right bin as 111100\. If we iterate this process
    and annotate each bin with a bin index, we’ll end up with a hash map. The keys
    of the hash map are the bin indexes, while the values of the hash map are the
    IDs of the data points within the bin.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – LSH index construction with bin index ](img/Figure_3.9_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – LSH index construction with bin index
  prefs: []
  type: TYPE_NORMAL
- en: Searching on top of LSH is easy. Intuitively, given a query, you can just search
    all data points within its own bin, or you can search through its neighboring
    bins.
  prefs: []
  type: TYPE_NORMAL
- en: How do you search through its neighboring bins? Take a look at *Figure 3.9*.
    The bin index is represented as binary code; the neighboring bins will only have
    a 1-bit difference compared with its own bin index. Apparently, you can consider
    the difference between bin index as a hyperparameter, and search through more
    neighboring bins. For example, if you set the hyper parameters as 2, means you
    allow LSH to search through 2 neighbor bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand this, we’ll look into the Annoy implementation of LSH,
    namely LSH with random projection. Given a list of vectors produced by a deep
    neural network, we first do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize a hyperplane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dot product the normal (the vector that is perpendicular to the hyperplane)
    against the vectors. For each vector, if the value is positive, we generate a
    binary code of 1, otherwise 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate N hyperplanes and iterate the process N times. At the end, each
    vector is represented by a binary vector of 0s and 1s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We treat each binary code as a bucket and save all documents with the same binary
    code into the same bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block demonstrates a simple implementation of LSH with random
    projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We preprocess two pieces of sentences into buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we map millions of documents into multiple buckets. At search time,
    we use the same hyperplanes to encode the search document, get the binary code,
    and find similar documents within the same bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Annoy implementation, search speed is dependent on two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`search_k`: This parameter is the top `k` elements you want to get back from
    the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N_trees`: This parameter represents the number of buckets you want to search
    from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is obvious that the search runtime highly depends on these two parameters,
    and the user needs to fine-tune the parameters based on their use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular ANN search algorithm is PQ. Before we dive into PQ, it is important
    to understand what *quantization* is. Suppose you have a million documents to
    index, and you created 100 *centroids* for all documents. A **quantizer** is a
    function that can map a vector to a centroid. You might find the idea very familiar.
    Actually, the K-means algorithm is a function that can help you generate such
    centroids. If you do not remember, K-means works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize `k` centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each vector to its closest centroid. Each centroid represents a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute new centroids based on the mean of all assignments, until converge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once K-means converge, we get K clusters given all vectors to index. For each
    document to index, we create a map between the document ID and cluster index.
    At search time, we compute the distance query vector against the centroids and
    get the closest clusters, then find the closest vectors within these clusters.
  prefs: []
  type: TYPE_NORMAL
- en: This quantization algorithm has a relatively good compression ratio. You don’t
    have to linearly scan all vectors in order to get the closest ones; you only need
    to scan certain clusters produced by the quantizer. On the other hand, the recall
    rate at searching time could be very low if the number of centroids is small.
    This is because there are too many edge cases that cannot be correctly distributed
    to the correct cluster. Also, if we simplify the set number of centroids to a
    large number, our K-means operations will take a long time to converge. This becomes
    a bottleneck at both offline indexing time and online searching time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind PQ is to split high-dimensional vectors into subvectors,
    as illustrated in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We split each vector into *m* subvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each subvector, we apply quantization. To this end, for each subvector,
    we have a unique cluster ID (the closest cluster of the subvector to its centroids).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the full vector, we have a list of cluster IDs, which can be used as the
    codebook of the full vector. The dimensionality of the codebook is identical to
    the number of subvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the PQ algorithm: given a vector, we cut it
    into subvectors of lower dimensionality and apply quantization. To this end, each
    quantized subvector gets a code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Product quantization ](img/Figure_3.10_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Product quantization
  prefs: []
  type: TYPE_NORMAL
- en: At search time, again, we split the high-dimensional query vector into subvectors
    and generate a codebook (bucket). We compute subvector-level cosine similarity
    against each of the vectors inside the collection and sum up the subvector-level
    similarity score. We sort the final results based on the vector-level cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, FAISS has a high-performant implementation of PQ (and beyond PQ).
    For more info, please refer to the documentation ([https://github.com/facebookresearch/faiss/wiki](https://github.com/facebookresearch/faiss/wiki)).
  prefs: []
  type: TYPE_NORMAL
- en: Now we have learned two fundamental tasks, indexing and querying, for neural
    search. In the next section, we are going to cover neural search system evaluation
    to make your neural search system complete and production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a neural search system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating the effectiveness of a neural search system is critical once you
    set up some baseline. By monitoring the evaluation metrics, you can immediately
    know how well your system performs. By diving deep into the queries, you can also
    conduct failure analysis and learn how to improve your system.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will give a brief overview of the most commonly used evaluation
    metrics. If you want to have a more detailed mathematical understanding of this
    topic, we strongly recommend you go through *Evaluation in information retrieval*
    ([https://nlp.stanford.edu/IR-book/pdf/08eval.pdf](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, given the difference between search tasks, normally we can group
    search evaluation into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation of unranked results**: These metrics are widely used in some retrieval
    or classification tasks, including precision, recall, and F-Score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation of ranked results**: These metrics are mainly used in typical
    search applications given the results are ordered (ranked).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let’s start with precision, recall, and F-Score:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a typical search scenario, precision is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision = (Num of Relevant Documents Retrieved) / (Num of Retrieved Documents)*'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is straightforward. Suppose our search system returns 10 documents,
    where 7 out of 10 are relevant, then the precision would be 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that during evaluation, we care about the top `k` retrieved
    results. Just like the aforementioned example, we evaluate relevant documents
    in the top 10 results. This is normally referred to as precision, such as **Precision@10**.
    It also applies to other metrics we will introduce later in this section, such
    as Recall@10, mAP@10, and nDCG@10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, recall is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall = (Num of Relevant Documents Retrieved) / (Num of Relevant Documents)*'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we search `cat` in our system, and we already know that there
    are 100 cat-related images being indexed, and 80 images are returned, then the
    recall rate is 0.8\. It is an evaluation metric to measure the *completeness*
    of how the search system performs.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Recall is the most important evaluation metric to evaluate the performance of
    an ANN algorithm since it depicts the fraction of true nearest neighbors found
    for all the queries on average.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy can be a good metric for typical ML tasks, such as classification.
    But this is not the case for search tasks since most of the datasets in search
    tasks are skewed/imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a search system designer, you might already notice that these two numbers
    are trade-offs against each other: with an increased number of K, we can always
    expect lower precision but higher recall and vice versa. It is your decision to
    optimize precision or recall or optimize these two numbers as one evaluation metric,
    that is, F1-Score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1-Score is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1-Score = (2 * Precision * Recall) / (Precision + Recall)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a weighted harmonic mean of precision and recall. In reality, a higher
    recall tends to be associated with a lower precision rate. Imagine you are evaluating
    a ranked list and you care about the top 10 items being retrieved (and there are
    10 relevant documents in the entire collection):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Document** | **Label** | **Precision** | **Recall** |'
  prefs: []
  type: TYPE_TB
- en: '| Doc1 | Relevant | 1/1 | 1/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc2 | Relevant | 2/2 | 2/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc3 | Irrelevant | 2/3 | 2/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc4 | Irrelevant | 2/4 | 2/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc5 | Relevant | 3/5 | 3/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc6 | Irrelevant | 3/6 | 3/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc7 | Irrelevant | 3/7 | 3/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc8 | Relevant | 4/8 | 4/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc9 | Irrelevant | 4/9 | 4/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc10 | Irrelevant | 4/10 | 4/10 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Precision recall for 10 of the top 10 documents
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.1* shows the precision and recall at different levels given binary
    labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Being familiar with precision, we can now move on to calculate the **Average
    Precision** (**AP**). This metric will give us a better understanding of our search
    system’s ability to sort the results of a query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given the preceding-ranked list, `aP@10` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aP@10 = (1/1 + 2/2 + 3/5 + 4/8) / 10 = 0.31`'
  prefs: []
  type: TYPE_NORMAL
- en: Note that only the precision of relevant documents is taken into consideration
    when computing aP.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the aP has been calculated against one specific user query. However, to
    give a more robust search system evaluation, we want to evaluate the performance
    of a collection of user queries as a test set. This is called `aP@k`, then we
    average all the aPs over a set of queries to get the mAP score.
  prefs: []
  type: TYPE_NORMAL
- en: 'mAP is one of the most important search system evaluation metrics given an
    ordered rank list. To conduct mAP evaluation on your search system, normally you
    have to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compose a list of queries to have a good representation of users’ information
    needs. The number is dependent on your situation, such as 50, 100, or 200.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your documents already have labels that indicate the degree of relevance,
    use the labels directly to compute aP per query. If your documents do not contain
    any relevant information against each query, we need expert annotation or pooling
    to access relevant degrees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute mAP over a list of queries by taking the average of the aP. As was mentioned
    previously, if you do not have a relevant assessment for ranked documents, one
    common technique is called **pooling**. It requires us to set up multiple search
    systems (such as three) for testing. Given each query, we collect the top K documents
    returned by each of these three search systems. A human annotator judges the degree
    of relevance of all 3 * K documents. Afterward, we consider all documents outside
    this pool as irrelevant, while all documents inside this pool are relevant. Then,
    the search results can be evaluated on top of the pools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, even though mAP is evaluating a ranked list, the nature of the
    definition of precision still neglects some of the nature of a search task: precision
    is evaluated based on binary labels, either relevant or irrelevant. It does not
    reflect the *relatedness* of the query against documents. **Normalized Discounted
    Cumulative Gain** (**nDCG**) is used for evaluating search system performance
    over the degree of relatedness.'
  prefs: []
  type: TYPE_NORMAL
- en: nDCG can have multiple levels of rating for each document, such as *irrelevant*,
    *relevant*, or *highly relevant*. In this case, mAP does not work anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, given three degrees of relevance (irrelevant, relevant, and highly
    relevant), these differences in degrees of relevance can be represented as information
    gain that users can obtain by getting each document. For highly relevant documents,
    the gain could be assigned a value of *3*, relevant could be *1*, and not relevant
    could be set to *0*. Then, if a highly relevant document is ranked higher than
    documents that are not relevant, the user could cumulate more *gain*, which is
    referred to as **Cumulative Gain** (**CG**). The following table shows the information
    gain we get per document given the top 10 ranked documents produced by a search
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Document** | **Label** | **Gain** |'
  prefs: []
  type: TYPE_TB
- en: '| Doc1 | Highly Relevant | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc2 | Relevant | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc3 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc4 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc5 | Relevant | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc6 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc7 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc8 | Highly Relevant | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc9 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc10 | Irrelevant | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – Top 10 documents with information gain
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding table, the system returned the top 10 ranked documents to
    the user. Based on the relevance degree, we assign 3 as the gain to highly relevant
    documents, 1 as the gain to relevant documents, and 0 as the gain to irrelevant
    documents. The CG is the sum of all gains in the top 10 documents, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CG@10 = 3 + 1 + 1 + 3 = 8`'
  prefs: []
  type: TYPE_NORMAL
- en: 'But think about the nature of a search engine: the users scan the ranked list
    from top to bottom. So, by nature, the top-ranked documents should have more gains
    than the documents ranked lower so that our search system will try to rank highly
    relevant documents in higher positions. So, in practice, we will penalize the
    gain by the position. See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Document** | **Label** | **Gain** | **Discounted Gain** |'
  prefs: []
  type: TYPE_TB
- en: '| Doc1 | Highly Relevant | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc2 | Relevant | 1 | 1/log2 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc3 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc4 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc5 | Relevant | 1 | 1/log5 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc6 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc7 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc8 | Highly Relevant | 3 | 3/log8 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc9 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc10 | Irrelevant | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – Top 10 documents of gain and discounted gain
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding table, given the gain of a document and its ranked position,
    we penalize the gain a little bit by dividing the gain by a factor. In this case,
    it’s the logarithm of the ranked position. The sum of the gain is called the **Discounted
    Cumulative Gain** (**DCG**):'
  prefs: []
  type: TYPE_NORMAL
- en: '`DCG@10 = 3 + 1/log2 + 1/log5 + 3/log8 = 6.51`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start to compute nDCG, it’s important to understand the concept of
    ideal DCG. It simply means the best-ranking result we could achieve. In the preceding
    case, if we look at the top 10 positions, ideally the ranked list should contain
    all highly relevant documents with a gain of 3\. So, the iDCG should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iDCG@10 = 3 + 3/log2 + 3/log3 + 3/log4 + 3/log5 + 3/log6 + 3/log7 + 3/log8
    + 3/log9 + 3/log10 = 21.41`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the final nDCG is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nDCG = DCG/iDCG`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our preceding example, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nDCG@10 = 6.51/21.41 = 0.304`'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that even though nDCG is well suited to evaluating a
    search system that reflects the degree of relevance, the *relatedness* itself
    is biased toward different factors, such as search context and user preference.
    It is non-trivial to perform such an evaluation in a real-world scenario. In the
    next chapter, we will dive into the details of such challenges and briefly introduce
    how to resolve them.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering challenges of building a neural search system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you would have noticed that the most important building blocks of the neural
    search system are the encoder and indexer. The quality of encoding posts has a
    direct impact on the final search result, while the speed of the indexer determines
    the scalability of your neural search system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, this is still not enough to make your neural search system ready
    to use. Many other topics need to be taken into consideration as well. The first
    question is: does your encoder (neural model) have the same distribution as your
    data? For new people coming into the neural search system world who are using
    a pretrained deep neural network, such as ResNet trained on ImageNet, it is trivial
    to quickly set up a search system. However, if your target is to build a neural
    search system on a specific domain, let’s say a fashion product image search,
    it is not going to produce satisfying results.'
  prefs: []
  type: TYPE_NORMAL
- en: One important topic before we really start creating an encoder and setting up
    our search system is applying transfer learning to your dataset and evaluating
    the match results. This means taking a pretrained deep learning model, such as
    ResNet, chopping off the head layer, freezing the weights of the pretrained model,
    and attaching a new embedding layer to the end of the model, then training it
    on your new dataset on your domain. This could greatly boost search performance.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from that, in some vision-based search systems, purely relying on the
    encoder might not be sufficient. For instance, a lot of vision-based search systems
    rely heavily on an object detector. Before sending the full image into the encoder,
    it should be sent to the object detector first and the meaningful part of the
    image extracted (and remove the background noise). This is likely to improve the
    embedding quality. Meanwhile, some vision-based classification models could also
    be employed to enrich the search context as a hard filter. For instance, if you
    are building a neural search system that allows people to search similar automobiles
    given an image as a query, a pretrained brand classifier could be useful. To be
    more concrete, you pretrain an automobile brand classifier to *recognize* different
    car brands based on images and apply the recognition to the indexing and searching
    pipeline. Once a vision-based search is complete, you can apply the recognized
    brand as a hard filter to filter out cars of other brands.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for text-based search, when a user provides keywords as a query,
    directly applying an embedding-based similarity search might be insufficient.
    For example, you can create a **Named Entity Recognition** (**NER**) module in
    your indexing and querying pipeline to enrich the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: For web-based search engines such as Google, Bing, or Baidu, it is very common
    to see query automatic completion. It might be also very interesting to add a
    deep neural network-powered keyword extraction component to your indexing and
    searching pipeline to use a similar user experience.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, to build a production-ready neural search system, it is very challenging
    to design a feature-complete indexing and querying pipeline, given the fact that
    search is such a complex task. Designing such a system is already challenging,
    let alone engineering the infrastructure. Luckily, Jina can already help you with
    most of the most challenging tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed the fundamental tasks to build a neural search
    system, which are the indexing and querying pipelines. We looked into both of
    them and introduced the most challenging part, such as encoding and indexing.
  prefs: []
  type: TYPE_NORMAL
- en: You should have basic knowledge of the basic building blocks of indexing and
    querying, such as preprocessing, encoding, and indexing. You should also notice
    that the quality of the search results highly depends on the encoder, while the
    scalability of the neural search system highly depends on the indexer and the
    most popular algorithms behind the indexer.
  prefs: []
  type: TYPE_NORMAL
- en: As you need to build a production-ready search system, you will realize that
    purely relying on the basic building blocks is not enough. As a search system
    is complex to implement, it is always needed to design and add your own building
    blocks to the indexing and querying pipeline, in order to bring better search
    results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start to introduce Jina, the most popular framework
    that helps you engineer a neural search system. You will realize that Jina has
    tackled the most difficult problems for you and could make your life as a neural
    search system engineer/scientist much easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Introduction to Jina Fundamentals'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, you will learn about what Jina is and its basic components. You
    will understand its architecture and how can it be used to develop deep-learning
    searches on the cloud. The following chapters are included in this part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B17488_04.xhtml#_idTextAnchor054), *Learning Jina''s Basics*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17488_05.xhtml#_idTextAnchor068), *Multiple Search Modalities*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
