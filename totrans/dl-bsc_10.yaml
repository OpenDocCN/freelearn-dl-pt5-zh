- en: Appendix A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is included to assist the students to perform the activities present
    in the book. It includes detailed steps that are to be performed by the students
    to complete and achieve the objectives of the activity.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Graph of the Softmax-with-Loss Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following figure is the computational graph of the Softmax-with-Loss layer
    and obtains backward propagation. We will call the softmax function the Softmax
    layer, the cross-entropy error the **Cross-Entropy Error** layer, and the layer
    where these two are combined the Softmax-with-Loss layer. You can represent the
    Softmax-with-Loss layer with the computational graph provided in *Figure A.1*:
    Entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.1: Computational graph of the Softmax-with-Loss layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.1: Computational graph of the Softmax-with-Loss layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The computational graph shown in *Figure A.1* assumes that there is a neural
    network that classifies three classes. The input from the previous layer is (a1,
    a2, a3), and the Softmax layer outputs (y1, y2, y3). The label is (t1, t2, t3)
    and the Cross-Entropy Error layer outputs the loss, L.
  prefs: []
  type: TYPE_NORMAL
- en: This appendix shows that the result of backward propagation of the Softmax-with-Loss
    layer will be (y1 − t1, y2 − t2, y3 − t3), as shown in *Figure A.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computational graph shown in *Figure A.1* does not show the details of the
    Softmax layer and the Cross-Entropy Error layer. Here, we will start by describing
    the details of the two layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the Softmax layer. We can represent the softmax function
    with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![95](img/Figure_A.1a.png) | (A.1) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we can show the Softmax layer with the computational graph provided
    in *Figure A.2*. Here, S stands for the sum of exponentials, which is the denominator
    in equation (A.1). The final output is (y1, y2, y3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.2: Computational graph of the Softmax layer (forward propagation
    only)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.2: Computational graph of the Softmax layer (forward propagation only)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, let''s look at the Cross-Entropy Error layer. The following equation
    shows the cross-entropy error:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![97](img/Figure_A.2a.png) | (A.2) |'
  prefs: []
  type: TYPE_TB
- en: Based on equation (A.2), we can draw the computational graph of the Cross-Entropy
    Error layer as shown in *Figure A.3*.
  prefs: []
  type: TYPE_NORMAL
- en: The computational graph shown in *Figure A.3* just shows equation (A. 2) as
    a computational graph. Therefore, I think that there is nothing particularly difficult
    about this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.3: Computational graph of the Cross-Entropy Error layer (forward
    propagation only)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.3: Computational graph of the Cross-Entropy Error layer (forward propagation
    only)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let''s look at backward propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: Backward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let''s look at backward propagation of the Cross-Entropy Error layer.
    We can draw backward propagation of the Cross-Entropy Error layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.4: Backward propagation of the Cross-Entropy Error layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.4: Backward propagation of the Cross-Entropy Error layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Please note the following to obtain backward propagation of this computational
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: The initial value of backward propagation (the rightmost value of backward propagation
    in *Figure A.4*) is 1 (because ![98](img/Figure_A.4a.png)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For backward propagation of the "x" node, the "reversed value" of the input
    signal for forward propagation multiplied by the derivative from the upper stream
    is passed downstream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the "+" node, the derivative from the upper stream is passed without changing it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backward propagation of the "log" node observes the following equations:![99](img/Figure_A.4b.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this, we can obtain backward propagation of the Cross-Entropy Error
    layer easily. As a result, the value ![100](img/Figure_A.4d.png) will be the input
    to the backward propagation of the Softmax layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at backward propagation of the Softmax layer. Because the
    Softmax layer is a little complicated, I want to check its backward propagation
    step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.5: Step 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.5: Step 1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The values of the backward propagation arrive from the previous layer (Cross-Entropy
    Error layer).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.6: Step 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.6: Step 2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The "x" node "reverses" the values of forward propagation for multiplication.
    Here, the following calculation is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![101](img/Figure_A.6a.png) | (A.3) |'
  prefs: []
  type: TYPE_TB
- en: '**Step 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.7: Step 3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.7: Step 3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the flow branches into multiple values in forward propagation, the separated
    values are added in backward propagation. Therefore, three separate values of
    backward propagation, ![102](img/Figure_A.7a.png), are added here. The backward
    propagation of */* is conducted for the added values, resulting in ![103](img/Figure_A.7b.png).
    Here, (*t*1, *t*2, *t*3) is the label and a "one-hot vector." A one-hot vector
    means that one of (*t*1, *t*2, *t*3) is 1 and the others are all 0s. Therefore,
    the sum of (*t*1, *t*2, *t*3) is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.8: Step 4'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.8: Step 4'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The "+" node only passes the value without changing it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.9: Step 5'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.9: Step 5'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The "x" node "reverses" the values for multiplication. Here, ![104](img/Figure_A.9a.png)
    is used to transform the equation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.10: Step 6'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.10: Step 6'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the "exp" node, the following equations hold true:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![105](img/Figure_A.10a.png) | (A.4) |'
  prefs: []
  type: TYPE_TB
- en: Thus, the sum of the two separate inputs, which are multiplied by exp(a1), is
    the backward propagation to obtain. We can write this as ![106](img/Figure_A.10c.png)
    and obtain ![107](img/Figure_A.10d.png) after transformation. Thus, in the node
    where the input of forward propagation is ![108](img/Figure_A.10e.png), backward
    propagation is ![109](img/Figure_A.10f.png). For ![110](img/Figure_A.10g.png)
    and ![111](img/Figure_A.10h.png), we can use the same procedure (the results are
    ![112](img/Figure_A.10i.png) and ![113](img/Figure_A.10j.png), respectively).
    With this, it is easy to show that we can achieve the same result even if we want
    to classify n classes instead of three classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, the computational graph of the Softmax-with-Loss layer was shown in detail,
    and its backward propagation was obtained. *Figure A.11* shows the complete computational
    graph of the Softmax-with-Loss layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.11: Computational graph of the Softmax-with-Loss layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_A.11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure A.11: Computational graph of the Softmax-with-Loss layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The computational graph shown in *Figure A.11* looks complicated. However, if
    you advance step by step using computational graphs, obtaining derivatives (the
    procedure of backward propagation) will be much less troublesome. When you encounter
    a layer that looks complicated (such as the Batch Normalization layer), other
    than the Softmax-with-Loss layer described here, you can use this procedure. This
    will be easier to understand in practice rather than only looking at equations.
  prefs: []
  type: TYPE_NORMAL
