<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer094">
<h1 class="chapter-number" id="_idParaDest-68" lang="en-GB"><a id="_idTextAnchor070"/>5</h1>
<h1 id="_idParaDest-69" lang="en-GB"><a id="_idTextAnchor071"/>Understanding Differentiable Volumetric Rendering</h1>
<p lang="en-GB">In this chapter, we are going to discuss a new way of differentiable rendering. We are going to use a voxel 3D data representation, unlike the mesh 3D data representation we used in the last chapter. Voxel 3D data representation has certain advantages compared to mesh models. For example, it is more flexible and <span class="No-Break" lang="">highly structured.</span></p>
<p lang="en-GB">To understand volumetric rendering, we need to understand several important concepts, such as ray sampling, volumes, volume sampling, and ray marching. All these concepts have corresponding PyTorch3D implementations. We will discuss each of these concepts using explanations and <span class="No-Break" lang="">coding exercises.</span></p>
<p lang="en-GB">After we understand the preceding basic concepts of volumetric rendering, we can then see easily that all the operations mentioned already are already differentiable. Volumetric rendering is naturally differentiable. Thus, by then, we will be ready to use differentiable volumetric rendering for some real applications. We are going to go over a coding example of reconstructing 3D voxel models from multiple images by using differentiable <span class="No-Break" lang="">volumetric rendering.</span></p>
<p lang="en-GB">We will first understand volumetric rendering on a high level. We will then dive into the basic concepts, such as ray sampling, volumes, volume sampling, and ray marching. We will then present a coding example of reconstructing 3D object shapes from a collection of images taken from different views of <span class="No-Break" lang="">the object.</span></p>
<p lang="en-GB">In this chapter, we’re going to cover the following <span class="No-Break" lang="">main topics:</span></p>
<ul>
<li lang="en-GB">A high-level description of <span class="No-Break" lang="">volumetric rendering</span></li>
<li lang="en-GB">Understanding <span class="No-Break" lang="">ray sampling</span></li>
<li lang="en-GB">Using <span class="No-Break" lang="">volume sampling</span></li>
<li lang="en-GB"><a id="_idTextAnchor072"/>Understanding <span class="No-Break" lang="">ray marching</span></li>
<li lang="en-GB">Reconstructing 3D objects and colors from <span class="No-Break" lang="">multi-view images</span></li>
</ul>
<h1 id="_idParaDest-70" lang="en-GB"><a id="_idTextAnchor073"/>Technical requirements</h1>
<p lang="en-GB">In order to run the example code snippets in this book, you need to have a computer, ideally with a GPU. However, running the code snippets with only CPUs is <span class="No-Break" lang="">not impossible.</span></p>
<p lang="en-GB">The recommended computer configuration includes <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB">A GPU for example, the NVIDIA GTX series or RTX series with at least 8 GB <span class="No-Break" lang="">of memory</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Python 3</span></li>
<li lang="en-GB">PyTorch library and <span class="No-Break" lang="">PyTorch3D libraries</span></li>
</ul>
<p lang="en-GB">The code snippets with this chapter can be found <span class="No-Break" lang="">at </span><a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python"><span class="No-Break" lang="">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python.</span></a></p>
<h1 id="_idParaDest-71" lang="en-GB"><a id="_idTextAnchor074"/>Overview of volumetric rendering</h1>
<p lang="en-GB">Volumetric rendering is a collection<a id="_idIndexMarker234"/> of techniques used to generate a 2D view of discrete 3D data. This 3D discrete data could be a collection of images, voxel representation, or any other discrete representation of data. The main goal of volumetric rendering is to render a 2D projection of 3D data since that is what our eyes can perceive on a flat screen. This method generated such projections without any explicit conversion to a geometric representation (such as meshes). Volumetric rendering is typically used when generating surfaces is difficult or can lead to errors. It can also be used when the content (and not just the geometry and surface) of the volume is important. It is typically used for data visualization. For example, in brain scans, a visualization of the content of the interior of the brain is typically <span class="No-Break" lang="">very important.</span></p>
<p lang="en-GB">In this section, we will explore the volumetric rendering of a volume. We will get a high-level overview of volumetric rendering as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 5</em></span><span class="No-Break" lang=""><em class="italic" lang="">.1</em></span><span class="No-Break" lang="">:</span></p>
<ol>
<li lang="en-GB">First, we represent<a id="_idIndexMarker235"/> the 3D space and objects in it by using a <strong class="bold" lang="">volume</strong>, which is a 3D grid of regularly spaced nodes. Each node has two properties: density and color features. The density typically ranges from 0 to 1. Density can also be understood as the probability of occupancy. That is, how sure we think that the node is occupied by a certain object. In some cases, the probability can also <span class="No-Break" lang="">be opacity.</span></li>
<li lang="en-GB">We need to define<a id="_idIndexMarker236"/> one or multiple cameras. The rendering is the process that determines what the cameras can observe from <span class="No-Break" lang="">their views.</span></li>
<li lang="en-GB">To determine the RGB values at each pixel of the preceding cameras, a ray is generated from the projection center going through each image pixel of the cameras. We need to check the probability of occupancy or opacity and colors along this ray to determine RGB values for the pixel. Note there are infinitely many points on each such ray. Thus, we need to have a sampling scheme to select a certain number<a id="_idIndexMarker237"/> of points along this ray. This sampling operation is called <span class="No-Break" lang=""><strong class="bold" lang="">ray sampling</strong></span><span class="No-Break" lang="">.</span></li>
<li lang="en-GB">Note that we have densities and colors defined on the nodes of the volume but not on the points on the rays. Thus, we need to have a way to convert densities and colors<a id="_idIndexMarker238"/> of volumes to points on rays. This operation is called <span class="No-Break" lang=""><strong class="bold" lang="">volume sampling</strong></span><span class="No-Break" lang="">.</span></li>
<li lang="en-GB">Finally, from the densities and colors of the rays, we need to determine the RGB values of each pixel. In this process, we need to compute how many incident lights can arrive at each point along the ray <a id="_idIndexMarker239"/>and how many lights are reflected to the image pixel. We call this process <span class="No-Break" lang=""><strong class="bold" lang="">ray marching</strong></span><span class="No-Break" lang="">.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 5.1: Volumetric rendering " height="1013" src="image/B18217_05_001.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: Volumetric rendering</p>
<p lang="en-GB">Having understood<a id="_idIndexMarker240"/> the basic process of volumetric rendering, let us dive deeper into the first concept: <span class="No-Break" lang="">ray sampling.</span></p>
<h1 id="_idParaDest-72" lang="en-GB"><a id="_idTextAnchor075"/>Understanding ray sampling</h1>
<p lang="en-GB">Ray sampling is the process<a id="_idIndexMarker241"/> of emitting rays from the camera that goes through the image pixels and sampling points along these rays. The ray sampling scheme depends on the use case. For example, sometimes we might want to randomly sample rays that go through some random subset of image pixels.  Typically, we need to use such a sampler during training since we only need a representative sample from the full data. In such cases, we can use <strong class="source-inline" lang="">MonteCarloRaysampler</strong> in Pytorch3D. In other cases, we want to get the pixel values for each pixel on the image and maintain a spatial order. This is useful for rendering and visualization. For such use cases, PyTorch3D <span class="No-Break" lang="">provides </span><span class="No-Break" lang=""><strong class="source-inline" lang="">NDCMultiNomialRaysampler</strong></span><span class="No-Break" lang="">.</span></p>
<p lang="en-GB">In the following, we will demonstrate how to use one of the PyTorch3D ray samplers, <strong class="source-inline" lang="">NDCGridRaysampler</strong>. This is like <strong class="source-inline" lang="">NDCMultiNomialRaysampler</strong>, where pixels are sampled along a grid. The codes can be found in the GitHub repository <span class="No-Break" lang="">named </span><span class="No-Break" lang=""><strong class="source-inline" lang="">understand_ray_sampling.py</strong></span><span class="No-Break" lang="">:</span></p>
<ol>
<li lang="en-GB" value="1">First, we need to import<a id="_idIndexMarker242"/> all the Python modules, including the definition <span class="No-Break" lang="">of </span><span class="No-Break" lang=""><strong class="source-inline" lang="">NDCGridRaysampler</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import math</p><p class="source-code" lang="en-GB">import numpy as np</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">    FoVPerspectiveCameras,</p><p class="source-code" lang="en-GB">    PointLights,</p><p class="source-code" lang="en-GB">    look_at_view_transform,</p><p class="source-code" lang="en-GB">    NDCGridRaysampler,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Set up the devices for use in the following steps. If we have GPUs, then we are going to use the first GPU. Otherwise, we are going to use <span class="No-Break" lang="">the CPU:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">    torch.cuda.set_device(device)</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p></li>
<li lang="en-GB">We define a batch of 10 cameras. Here, <strong class="source-inline" lang="">num_views</strong> is the number of views, which is 10 in this case. The <strong class="source-inline" lang="">elev</strong> variable denotes the elevation angle, and <strong class="source-inline" lang="">azim</strong> denotes the azimuth angle. The rotation, <strong class="source-inline" lang="">R</strong>, and translation, <strong class="source-inline" lang="">T</strong>, can thus be determined using the PyTorch3D <strong class="source-inline" lang="">look_at_view_transform</strong> function. The 10 cameras then can be defined by using rotations and translations. The 10 cameras all point at an object located at the center of the <span class="No-Break" lang="">world coordinates:</span><p class="source-code" lang="en-GB">num_views: int = 10</p><p class="source-code" lang="en-GB">azimuth_range: float = 180</p><p class="source-code" lang="en-GB">elev = torch.linspace(0, 0, num_views)</p><p class="source-code" lang="en-GB">azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0</p><p class="source-code" lang="en-GB">lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])</p><p class="source-code" lang="en-GB">R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)</p><p class="source-code" lang="en-GB">cameras = FoVPerspectiveCameras(device=device, R=R, T=T)</p></li>
<li lang="en-GB">Finally, we can define<a id="_idIndexMarker243"/> the ray sampler, which is the <strong class="source-inline" lang="">raysampler</strong> variable. We need to specify the image size of the camera. We also need to specify the minimum and maximum depths that the ray ranges from. The <strong class="source-inline" lang="">n_pts_per_ray</strong> input is the number of points along <span class="No-Break" lang="">the ray:</span><p class="source-code" lang="en-GB">image_size = 64</p><p class="source-code" lang="en-GB">volume_extent_world = 3.0</p><p class="source-code" lang="en-GB">raysampler = NDCGridRaysampler(</p><p class="source-code" lang="en-GB">    image_width=image_size,</p><p class="source-code" lang="en-GB">    image_height=image_size,</p><p class="source-code" lang="en-GB">    n_pts_per_ray=50,</p><p class="source-code" lang="en-GB">    min_depth=0.1,</p><p class="source-code" lang="en-GB">    max_depth=volume_extent_world,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">In the preceding step, we have already defined a ray sampler. To make the ray sampler samples<a id="_idIndexMarker244"/> rays and points for use, we need to let the ray sampler know where our cameras are and in what directions they are pointing. This can be easily achieved by passing the cameras defined in step 3 to <strong class="source-inline" lang="">raysampler</strong>. What we obtain is then a <span class="No-Break" lang=""><strong class="source-inline" lang="">ray_bundle</strong></span><span class="No-Break" lang=""> variable:</span><p class="source-code" lang="en-GB">ray_bundle = raysampler(cameras)</p></li>
<li lang="en-GB">The <strong class="source-inline" lang="">ray_bundle</strong> variable contains a collection of different PyTorch tensors that specify the sampled rays and points. We can print these member variables to check their shapes and verify <span class="No-Break" lang="">their contents:</span><p class="source-code" lang="en-GB">print('ray_bundle origins tensor shape = ', ray_bundle.origins.shape)</p><p class="source-code" lang="en-GB">print('ray_bundle directions shape = ', ray_bundle.directions.shape)</p><p class="source-code" lang="en-GB">print('ray_bundle lengths = ', ray_bundle.lengths.shape)</p><p class="source-code" lang="en-GB">print('ray_bundle xys shape = ', ray_bundle.xys.shape)</p></li>
<li lang="en-GB">The codes should run and print the <span class="No-Break" lang="">following information:</span><ul><li lang="en-GB">We can see that <strong class="source-inline" lang="">ray_bundle.origins</strong> is a tensor about the origins of the rays, and the batch size is 10. Because the image size is 64 by 64, the size of the second and third dimensions are both 64. For each origin, we need three numbers to specify its <span class="No-Break" lang="">3D location.</span></li><li lang="en-GB"><strong class="source-inline" lang="">ray_bundle.directions</strong> is a tensor about the directions of the ray. Again, the batch size is 10 and the image size is 64 by 64. These explain the size of the first three dimensions of the tensor. We need three numbers to specify a direction in <span class="No-Break" lang="">3D spaces.</span></li><li lang="en-GB"><strong class="source-inline" lang="">ray_bundle.lengths</strong> is a tensor about the depths of each point on the rays. There are 10x64x64 rays and there are 50 points on <span class="No-Break" lang="">each ray.</span></li><li lang="en-GB"><strong class="source-inline" lang="">ray_bundle.xys</strong> is a tensor about the x and y locations on the image plane corresponding to each ray. There are 10x64x64 rays. We need one number to represent the x location and one number to represent the <span class="No-Break" lang="">y location:</span><p class="source-code" lang="en-GB">ray_bundle origins tensor shape =  torch.Size([10, 64, 64, 3])</p><p class="source-code" lang="en-GB">ray_bundle directions shape =  torch.Size([10, 64, 64, 3])</p><p class="source-code" lang="en-GB">ray_bundle lengths =  torch.Size([10, 64, 64, 50])</p><p class="source-code" lang="en-GB">ray_bundle xys shape =  torch.Size([10, 64, 64, 2])</p></li></ul></li>
<li lang="en-GB">Finally, we save <strong class="source-inline" lang="">ray_bundle</strong> to a <strong class="source-inline" lang="">ray_sampling.pt</strong> file. These rays are useful for our coding exercises in the <span class="No-Break" lang="">following sections:</span><p class="source-code" lang="en-GB">torch.save({</p><p class="source-code" lang="en-GB">    'ray_bundle': ray_bundle</p><p class="source-code" lang="en-GB">}, 'ray_sampling.pt')</p></li>
</ol>
<p lang="en-GB">By now, we understand<a id="_idIndexMarker245"/> what ray samplers do. Ray samplers give us a batch of rays and discrete points on the rays. However, we still do not have densities and colors defined on these points and rays. Next, we are going to learn how to get these densities and colors from <span class="No-Break" lang="">the volumes.</span></p>
<h1 id="_idParaDest-73" lang="en-GB"><a id="_idTextAnchor076"/>Using volume sampling</h1>
<p lang="en-GB">Volume sampling is the process<a id="_idIndexMarker246"/> of getting color and occupancy information along the points provided by the ray samples. The volume representation we are working with is discrete. Therefore, the points defined in the ray sampling step might not fall exactly on a point. The nodes of the volume grids and points on rays typically have different spatial locations. We need to use an interpolation scheme to interpolate the densities and colors at points of rays from the densities and colors at volumes. We can do that by using <strong class="source-inline" lang="">VolumeSampler</strong> implemented in PyTorch3D. The following code can be found in the GitHub repository in the <span class="No-Break" lang=""><strong class="source-inline" lang="">understand_volume_sampler.py</strong></span><span class="No-Break" lang=""> file:</span></p>
<ol>
<li lang="en-GB" value="1">Import the Python modules that <span class="No-Break" lang="">we need:</span><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">from pytorch3d.structures import Volumes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer.implicit.renderer import VolumeSampler</p></li>
<li lang="en-GB">Set up<a id="_idIndexMarker247"/> <span class="No-Break" lang="">the devices:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">    torch.cuda.set_device(device)</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p></li>
<li lang="en-GB">Load <strong class="source-inline" lang="">ray_bundle</strong>, which was computed in the <span class="No-Break" lang="">last section:</span><p class="source-code" lang="en-GB">checkpoint = torch.load('ray_sampling.pt')</p><p class="source-code" lang="en-GB">ray_bundle = checkpoint.get('ray_bundle')</p></li>
<li lang="en-GB">We then define a volume. The densities tensor has a shape of [10, 1, 64, 64, 50], where we have a batch of 10 volumes, and each volume is a grid of 64x64x50 nodes. Each node has one number to represent the density at the node. On the other hand, the colors tensor has a shape of [10, 3, 64, 64, 50], because each color needs three numbers to represent the <span class="No-Break" lang="">RGB values:</span><p class="source-code" lang="en-GB">batch_size = 10</p><p class="source-code" lang="en-GB">densities = torch.zeros([batch_size, 1, 64, 64, 64]).to(device)</p><p class="source-code" lang="en-GB">colors = torch.zeros(batch_size, 3, 64, 64, 64).to(device)</p><p class="source-code" lang="en-GB">voxel_size = 0.1</p><p class="source-code" lang="en-GB">volumes = Volumes(</p><p class="source-code" lang="en-GB">    densities=densities,</p><p class="source-code" lang="en-GB">    features=colors,</p><p class="source-code" lang="en-GB">    voxel_size=voxel_size</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">We need to define <strong class="source-inline" lang="">volume_sampler</strong> based on the volumes. Here, we use bilinear interpolation<a id="_idIndexMarker248"/> for the volume sampling. The densities and colors of points on the rays can then be easily obtained by passing <strong class="source-inline" lang="">ray_bundle</strong> <span class="No-Break" lang="">to </span><span class="No-Break" lang=""><strong class="source-inline" lang="">volume_sampler</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">volume_sampler = VolumeSampler(volumes = volumes, sample_mode = "bilinear")</p><p class="source-code" lang="en-GB">rays_densities, rays_features = volume_sampler(ray_bundle)</p></li>
<li lang="en-GB">We can print out the shape of the densities <span class="No-Break" lang="">and colors:</span><p class="source-code" lang="en-GB">print('rays_densities shape = ', rays_densities.shape)</p><p class="source-code" lang="en-GB">print('rays_features shape = ', rays_features.shape)</p></li>
<li lang="en-GB">The output is as follows. Note that we have a batch size of 10 cameras, which explains the size of the first dimension of the tensors. We have one ray for each image pixel and our camera image resolution is 64 by 64. The number of points on each ray is 50, which explains the size of the fourth dimension of the tensors. Each density can be represented by one number and each color needs three numbers to represent the <span class="No-Break" lang="">RGB values:</span><p class="source-code" lang="en-GB">rays_densities shape =  torch.Size([10, 64, 64, 50, 1])</p><p class="source-code" lang="en-GB">rays_features shape =  torch.Size([10, 64, 64, 50, 3])</p></li>
<li lang="en-GB">Finally, let us save the densities and colors because we need to use them in the <span class="No-Break" lang="">next section:</span><p class="source-code" lang="en-GB">torch.save({</p><p class="source-code" lang="en-GB">    'rays_densities': rays_densities,</p><p class="source-code" lang="en-GB">    'rays_features': rays_features</p><p class="source-code" lang="en-GB">}, 'volume_sampling.pt')</p></li>
</ol>
<p lang="en-GB">We now have an overview<a id="_idIndexMarker249"/> of volume sampling. We know what it is and why it is useful. In the next section, we will learn about how to use these densities and colors to generate the RGB image values on the batch <span class="No-Break" lang="">of cameras.</span></p>
<h1 id="_idParaDest-74" lang="en-GB"><a id="_idTextAnchor077"/>Exploring the ray marcher</h1>
<p lang="en-GB">Now that we have the color<a id="_idIndexMarker250"/> and density values for all the points sampled with the ray sampler, we need to figure out how to use it to finally render the pixel value on the projected image. In this section, we are going to discuss the process of converting the densities and colors on points of rays to RGB values on images. This process models the physical process of <span class="No-Break" lang="">image formation.</span></p>
<p lang="en-GB">In this section, we discuss a very simple model, where the RGB value of each image pixel is a weighted sum of the colors on the points of the corresponding ray. If we consider the densities as probabilities of occupancy or opacity, then the incident light intensity at each point of the ray is a = product of (1-p_i), where p_i are the densities. Given the probability that this point is occupied by a certain object is p_i,  the expected light intensity reflected from this point is w_i = a p_i. We just use w_i as the weights for the weighted sum of colors. Usually, we normalize the weights by applying a softmax operation, such that the weights all sum up <span class="No-Break" lang="">to one.</span></p>
<p lang="en-GB">PyTorch3D contains multiple implementations of ray marchers. The following codes can be found in <strong class="source-inline" lang="">understand_ray_marcher.py</strong> in the <span class="No-Break" lang="">GitHub repository:</span></p>
<ol>
<li lang="en-GB" value="1">In this first step, we will import all the <span class="No-Break" lang="">required packages:</span><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">from pytorch3d.renderer.implicit.raymarching import EmissionAbsorptionRaymarcher</p></li>
<li lang="en-GB">Next, we load the densities and colors on rays from the <span class="No-Break" lang="">last section:</span><p class="source-code" lang="en-GB">checkpoint = torch.load('volume_sampling.pt')</p><p class="source-code" lang="en-GB">rays_densities = checkpoint.get('rays_densities')</p><p class="source-code" lang="en-GB">rays_features = checkpoint.get('rays_features')</p></li>
<li lang="en-GB">We define <strong class="source-inline" lang="">ray_marcher</strong> and pass the densities and colors on rays to <strong class="source-inline" lang="">ray_marcher</strong>. This gives us <strong class="source-inline" lang="">image_features</strong>, which are exactly rendered <span class="No-Break" lang="">RGB values:</span><p class="source-code" lang="en-GB">ray_marcher = EmissionAbsorptionRaymarcher()</p><p class="source-code" lang="en-GB">image_features = ray_marcher(rays_densities = rays_densities, rays_features = rays_features)</p></li>
<li lang="en-GB">We can print the image <span class="No-Break" lang="">feature shape:</span><p class="source-code" lang="en-GB">print('image_features shape = ', image_features.shape)</p></li>
<li lang="en-GB">As we have expected, the shape is [10, 64, 64, 4], where 10 is the batch size, and 64 is the image width and height. The outputs have four channels, the first three are RGBs. The last channel is the alpha channel, which represents whether the pixel is in the foreground or <span class="No-Break" lang="">the background:</span><p class="source-code" lang="en-GB">image_features shape =  torch.Size([10, 64, 64, 4])</p></li>
</ol>
<p lang="en-GB">We have now gone through some of the main<a id="_idIndexMarker251"/> components of volumetric rendering. Note that the computation process from the volume densities and colors to image pixel RGB values is already differentiable. So, volumetric rendering is naturally differentiable. Given that all the variables in the preceding examples are PyTorch tensors, we can compute gradients on <span class="No-Break" lang="">these variables.</span></p>
<p lang="en-GB">In the next section, we will learn about differentiable volume rendering and see an example of using volumetric rendering for reconstructing 3D models from <span class="No-Break" lang="">multi-view images.</span></p>
<h1 id="_idParaDest-75" lang="en-GB"><a id="_idTextAnchor078"/>Differentiable volumetric rendering</h1>
<p lang="en-GB">While standard volumetric rendering<a id="_idIndexMarker252"/> is used to render 2D projections of 3D data, differentiable volume rendering is used to do the opposite: construct 3D data from 2D images. This is how it works: we represent the shape and texture of the object as a parametric function. This function can be used to generate 2D projections. But, given 2D projections (this is typically multiple views of the 3D scene), we can optimize<a id="_idIndexMarker253"/> the parameters of these implicit shape and texture functions so that its projections are the multi-view 2D images. This optimization is possible since the rendering process is completely differentiable, and the implicit functions used are <span class="No-Break" lang="">also differentiable.</span></p>
<h2 id="_idParaDest-76" lang="en-GB"><a id="_idTextAnchor079"/>Reconstructing 3D models from multi-view images</h2>
<p lang="en-GB">In this section, we are going<a id="_idIndexMarker254"/> to show an example<a id="_idIndexMarker255"/> of using differentiable<a id="_idIndexMarker256"/> volumetric rendering for reconstructing 3D models from multi-view images. Reconstructing 3D models is a frequently sought problem. Usually, the direct ways of measuring the 3D world are difficult and costly, for example, LiDAR and Radar are typically expensive. On the other hand, 2D cameras have much lower costs, which makes reconstructing the 3D world from 2D images incredibly attractive. Of course, to reconstruct the 3D world, we need multiple images from <span class="No-Break" lang="">multiple views.</span></p>
<p lang="en-GB">The following <strong class="source-inline" lang="">volume_renderer.py</strong> code can be found in the GitHub repository and it is modified from a tutorial of PyTorch3D. We will use this coding example to show how the real-world application of volumetric rendering <span class="No-Break" lang="">can be:</span></p>
<ol>
<li lang="en-GB" value="1">First, we need to import all the <span class="No-Break" lang="">Python modules:</span><p class="source-code" lang="en-GB">import os</p><p class="source-code" lang="en-GB">import sys</p><p class="source-code" lang="en-GB">import time</p><p class="source-code" lang="en-GB">import json</p><p class="source-code" lang="en-GB">import glob</p><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import math</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">import numpy as np</p><p class="source-code" lang="en-GB">from PIL import Image</p><p class="source-code" lang="en-GB">from pytorch3d.structures import Volumes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">    FoVPerspectiveCameras,</p><p class="source-code" lang="en-GB">    VolumeRenderer,</p><p class="source-code" lang="en-GB">    NDCGridRaysampler,</p><p class="source-code" lang="en-GB">    EmissionAbsorptionRaymarcher</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">from pytorch3d.transforms import so3_exp_map</p><p class="source-code" lang="en-GB">from plot_image_grid import image_grid</p><p class="source-code" lang="en-GB">from generate_cow_renders import generate_cow_renders</p></li>
<li lang="en-GB">Next, we need<a id="_idIndexMarker257"/> to set<a id="_idIndexMarker258"/> up <span class="No-Break" lang="">the</span><span class="No-Break" lang=""><a id="_idIndexMarker259"/></span><span class="No-Break" lang=""> device:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">    torch.cuda.set_device(device)</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p></li>
<li lang="en-GB">Using the provided function from the PyTorch3D tutorial, we generate 40 cameras, images, and silhouette images with different angles. We will consider these images as the given ground-truth images, and we will fit a 3D volumetric model to these observed <span class="No-Break" lang="">ground-truth images:</span><p class="source-code" lang="en-GB">target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40)</p></li>
<li lang="en-GB">Next, we define a ray sampler. As we have discussed in the previous sections, the ray sampler is for sample rays, and points per rays <span class="No-Break" lang="">for us:</span><p class="source-code" lang="en-GB">render_size = 128</p><p class="source-code" lang="en-GB">volume_extent_world = 3.0</p><p class="source-code" lang="en-GB">raysampler = NDCGridRaysampler(</p><p class="source-code" lang="en-GB">    image_width=render_size,</p><p class="source-code" lang="en-GB">    image_height=render_size,</p><p class="source-code" lang="en-GB">    n_pts_per_ray=150,</p><p class="source-code" lang="en-GB">    min_depth=0.1,</p><p class="source-code" lang="en-GB">    max_depth=volume_extent_world,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we create<a id="_idIndexMarker260"/> the ray marcher<a id="_idIndexMarker261"/> as before. Note, this<a id="_idIndexMarker262"/> time, we define a variable renderer of the <strong class="source-inline" lang="">VolumeRenderer</strong> type. <strong class="source-inline" lang="">VolumeRenderer</strong> is just a nice interface, where ray samplers and ray marchers do all the heavy-lifting work under <span class="No-Break" lang="">the hood:</span><p class="source-code" lang="en-GB">raymarcher = EmissionAbsorptionRaymarcher()</p><p class="source-code" lang="en-GB">renderer = VolumeRenderer(</p><p class="source-code" lang="en-GB">    raysampler=raysampler, raymarcher=raymarcher,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we define a <strong class="source-inline" lang="">VolumeModel</strong> class. This class is just for encapsulating a volume so that the gradients can be computed in the forward function and the volume densities and colors can be updated by <span class="No-Break" lang="">the optimizer:</span><p class="source-code" lang="en-GB">class VolumeModel(torch.nn.Module):</p><p class="source-code" lang="en-GB">    def __init__(self, renderer, volume_size=[64] * 3, voxel_size=0.1):</p><p class="source-code" lang="en-GB">        super().__init__()</p><p class="source-code" lang="en-GB">        self.log_densities = torch.nn.Parameter(-4.0 * torch.ones(1, *volume_size))</p><p class="source-code" lang="en-GB">        self.log_colors = torch.nn.Parameter(torch.zeros(3, *volume_size))</p><p class="source-code" lang="en-GB">        self._voxel_size = voxel_size</p><p class="source-code" lang="en-GB">        self._renderer = renderer</p><p class="source-code" lang="en-GB">    def forward(self, cameras):</p><p class="source-code" lang="en-GB">        batch_size = cameras.R.shape[0]</p><p class="source-code" lang="en-GB">        densities = torch.sigmoid(self.log_densities)</p><p class="source-code" lang="en-GB">        colors = torch.sigmoid(self.log_colors)</p><p class="source-code" lang="en-GB">        volumes = Volumes(</p><p class="source-code" lang="en-GB">            densities=densities[None].expand(</p><p class="source-code" lang="en-GB">                batch_size, *self.log_densities.shape),</p><p class="source-code" lang="en-GB">            features=colors[None].expand(</p><p class="source-code" lang="en-GB">                batch_size, *self.log_colors.shape),</p><p class="source-code" lang="en-GB">            voxel_size=self._voxel_size,</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">        return self._renderer(cameras=cameras, volumes=volumes)[0]</p></li>
<li lang="en-GB">Define a Huber<a id="_idIndexMarker263"/> loss function. The Huber<a id="_idIndexMarker264"/> loss function<a id="_idIndexMarker265"/> is a robust loss function to prevent a small number of outliers from dragging the optimization away from the true optimal solutions.  Minimizing this loss function will move x closer <span class="No-Break" lang="">to y:</span><p class="source-code" lang="en-GB">def huber(x, y, scaling=0.1):</p><p class="source-code" lang="en-GB">    diff_sq = (x - y) ** 2</p><p class="source-code" lang="en-GB">    loss = ((1 + diff_sq / (scaling ** 2)).clamp(1e-4).sqrt() - 1) * float(scaling)</p><p class="source-code" lang="en-GB">    return loss</p></li>
<li lang="en-GB">Move everything to the <span class="No-Break" lang="">right device:</span><p class="source-code" lang="en-GB">target_cameras = target_cameras.to(device)</p><p class="source-code" lang="en-GB">target_images = target_images.to(device)</p><p class="source-code" lang="en-GB">target_silhouettes = target_silhouettes.to(device)</p></li>
<li lang="en-GB">Define an instance <span class="No-Break" lang="">of </span><span class="No-Break" lang=""><strong class="source-inline" lang="">VolumeModel</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">volume_size = 128</p><p class="source-code" lang="en-GB">volume_model = VolumeModel(</p><p class="source-code" lang="en-GB">    renderer,</p><p class="source-code" lang="en-GB">    volume_size=[volume_size] * 3,</p><p class="source-code" lang="en-GB">    voxel_size=volume_extent_world / volume_size,</p><p class="source-code" lang="en-GB">).to(device)</p></li>
<li lang="en-GB">Now let’s set up the optimizer. The learning rate, <strong class="source-inline" lang="">lr</strong>, is set to 0.1. We use an Adam optimizer, and the number of optimization iterations will <span class="No-Break" lang="">be 300:</span><p class="source-code" lang="en-GB">lr = 0.1</p><p class="source-code" lang="en-GB">optimizer = torch.optim.Adam(volume_model.parameters(), lr=lr)</p><p class="source-code" lang="en-GB">batch_size = 10</p><p class="source-code" lang="en-GB">n_iter = 300</p></li>
<li lang="en-GB">Next, we have the main <a id="_idIndexMarker266"/>optimization loop. The densities<a id="_idIndexMarker267"/> and colors of the volume<a id="_idIndexMarker268"/> are rendered, and the resulting colors and silhouettes are compared with the observed multi-view images. The Huber loss between the rendered images and observed ground-truth images <span class="No-Break" lang="">is minimized:</span><p class="source-code" lang="en-GB">for iteration in range(n_iter):</p><p class="source-code" lang="en-GB">    if iteration == round(n_iter * 0.75):</p><p class="source-code" lang="en-GB">        print('Decreasing LR 10-fold ...')</p><p class="source-code" lang="en-GB">        optimizer = torch.optim.Adam(</p><p class="source-code" lang="en-GB">            volume_model.parameters(), lr=lr * 0.1</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">    optimizer.zero_grad()</p><p class="source-code" lang="en-GB">    batch_idx = torch.randperm(len(target_cameras))[:batch_size]</p><p class="source-code" lang="en-GB">    # Sample the minibatch of cameras.</p><p class="source-code" lang="en-GB">    batch_cameras = FoVPerspectiveCameras(</p><p class="source-code" lang="en-GB">        R=target_cameras.R[batch_idx],</p><p class="source-code" lang="en-GB">        T=target_cameras.T[batch_idx],</p><p class="source-code" lang="en-GB">        znear=target_cameras.znear[batch_idx],</p><p class="source-code" lang="en-GB">        zfar=target_cameras.zfar[batch_idx],</p><p class="source-code" lang="en-GB">        aspect_ratio=target_cameras.aspect_ratio[batch_idx],</p><p class="source-code" lang="en-GB">        fov=target_cameras.fov[batch_idx],</p><p class="source-code" lang="en-GB">        device=device,</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    rendered_images, rendered_silhouettes = volume_model(</p><p class="source-code" lang="en-GB">        batch_cameras</p><p class="source-code" lang="en-GB">    ).split([3, 1], dim=-1)</p><p class="source-code" lang="en-GB">    sil_err = huber(</p><p class="source-code" lang="en-GB">        rendered_silhouettes[..., 0], target_silhouettes[batch_idx],</p><p class="source-code" lang="en-GB">    ).abs().mean()</p><p class="source-code" lang="en-GB">    color_err = huber(</p><p class="source-code" lang="en-GB">        rendered_images, target_images[batch_idx],</p><p class="source-code" lang="en-GB">    ).abs().mean()</p><p class="source-code" lang="en-GB">    loss = color_err + sil_err</p><p class="source-code" lang="en-GB">    loss.backward()</p><p class="source-code" lang="en-GB">    optimizer.step()</p></li>
<li lang="en-GB">After the optimization<a id="_idIndexMarker269"/> is finished, we take<a id="_idIndexMarker270"/> the final resulting<a id="_idIndexMarker271"/> volumetric model and render images from <span class="No-Break" lang="">new angles:</span><p class="source-code" lang="en-GB">with torch.no_grad():</p><p class="source-code" lang="en-GB"> rotating_volume_frames = ge erate_rotating_volume(volume_model, n_frames=7 * 4)</p><p class="source-code" lang="en-GB">image_grid(rotating_volume_frames.clamp(0., 1. .cpu().numpy(), rows=4, cols=7, rgb=True, fill=True)</p><p class="source-code" lang="en-GB">plt.savefig('rotating_volume.png')</p><p class="source-code" lang="en-GB">plt.show()</p></li>
<li lang="en-GB">Finally, the rendered new images are shown in <span class="No-Break" lang="">Figure 5</span><span class="No-Break" lang="">.2:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 5.2: Rendered images from the fitted 3D model " height="900" src="image/B18217_05_002.jpg" width="1574"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: Rendered images from the fitted 3D model</p>
<p lang="en-GB">By now, we have<a id="_idIndexMarker272"/> an overview of some of the main<a id="_idIndexMarker273"/> concepts in differentiable volumetric<a id="_idIndexMarker274"/> rendering. We have also learned a concrete example of using differentiable volumetric rendering for reconstructing 3D models from multiview images. You should be able to master the skills already and be able to use the technique for your <span class="No-Break" lang="">own problems.</span></p>
<h1 id="_idParaDest-77" lang="en-GB"><a id="_idTextAnchor080"/>Summary</h1>
<p lang="en-GB">In this chapter, we started with a high-level description of differentiable volumetric rendering. We then dived deep into several important concepts of differentiable volumetric rendering, including ray sampling, volume sampling, and the ray marcher, but only by explanations and coding examples. We walked through a coding example of using differentiable volumetric rendering for reconstructing 3D models from <span class="No-Break" lang="">multi-view images.</span></p>
<p lang="en-GB">Using volumes for 3D deep learning has become an interesting direction in recent years. As many innovative ideas come out following this direction, many breakthroughs are emerging. One of the breakthroughs, called <strong class="bold" lang="">Neural Radiance Fields</strong> (<strong class="bold" lang="">NeRF</strong>), will be the topic of our <span class="No-Break" lang="">next chapter.</span></p>
</div>
</div></body></html>