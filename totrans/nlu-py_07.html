<html><head></head><body>
		<div id="_idContainer086">
			<h1 id="_idParaDest-123" class="chapter-number"><a id="_idTextAnchor144"/>7</h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor145"/>Selecting Approaches and Representing Data</h1>
			<p>This chapter will cover the next steps in getting ready to implement a <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) application. We start with some basic considerations about understanding how much data is needed for an application, what to do about specialized vocabulary and syntax, and take into account the need for different types of computational resources. We then discuss the first steps in NLP – text representation formats that will get our data ready for processing with NLP algorithms. These formats include symbolic and numerical approaches for representing words and documents. To some extent, data formats and algorithms can be mixed and matched in an application, so it is helpful to consider data representation independently from the consideration <span class="No-Break">of algorithms.</span></p>
			<p>The first section will review general considerations for selecting NLP approaches that have to do with the type of application we’re working on, and with the data that we’ll <span class="No-Break">be using.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Selecting <span class="No-Break">NLP approaches</span></li>
				<li>Representing language for <span class="No-Break">NLP applications</span></li>
				<li>Representing language numerically <span class="No-Break">with vectors</span></li>
				<li>Representing words with <span class="No-Break">context-independent vectors</span></li>
				<li>Representing words with <span class="No-Break">context-dependent vectors</span></li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor146"/>Selecting NLP approaches</h1>
			<p>NLP can be done with a wide variety of possible techniques. When you get started on an NLP application, you have many choices to make, which are affected by a large number of factors. One of the <a id="_idIndexMarker542"/>most important factors is the type of application itself and the information that the system needs to extract from the data to perform the intended task. The next section addresses how the application affects the choice <span class="No-Break">of techniques.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor147"/>Fitting the approach to the task</h2>
			<p>Recall from <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, that there are many different types of NLP applications divided into <strong class="bold">interactive</strong> and <strong class="bold">non-interactive applications</strong>. The type of application you choose <a id="_idIndexMarker543"/>will play an important role in choosing the technologies that will be applied to the task. Another <a id="_idIndexMarker544"/>way of categorizing applications is in terms of the level of detail <a id="_idIndexMarker545"/>required to extract the needed information from the document. At the coarsest level of analysis (for example, classifying documents into two different categories), techniques can be less sophisticated, faster to train, and less computationally intensive. On the other hand, if the task is training a chatbot or voice assistant that needs to pull out multiple entities and values from each utterance, the analysis needs to be more sensitive and fine-grained. We will see some specific examples of this in later sections of <span class="No-Break">this chapter.</span></p>
			<p>In the next section, we will discuss how data affects our choice <span class="No-Break">of techniques.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor148"/>Starting with the data</h2>
			<p>NLP applications are built on datasets or sets of examples of the kinds of data that the target system will <a id="_idIndexMarker546"/>need to process. To build a successful application, having the right amount of data is imperative. However, we can’t just specify a single number of examples for every application, because the right amount of data is going to be different for different kinds of applications. Not only do we have to have the right amount of data, but we have to have the right kind of <a id="_idIndexMarker547"/>data. We’ll talk about these considerations in the next <span class="No-Break">two sections.</span></p>
			<h3>How much data is enough?</h3>
			<p>In <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we discussed many methods of obtaining data, and after going through <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, you should have a good idea of where your data will be coming from. However, in that chapter, we did not address the question of how to tell how much data is needed for your <a id="_idIndexMarker548"/>application to accomplish its goal. If there are hundreds or thousands of different possible classifications of documents in a task, then we will need a sufficient number of examples of each category for the system to be able to tell them apart. Obviously, the system can’t detect a category if it hasn’t ever seen an example of that category, but it will also be quite difficult to detect categories where it has seen very <span class="No-Break">few examples.</span></p>
			<p>If there are many more examples of <a id="_idIndexMarker549"/>some classes than others, then we <a id="_idIndexMarker550"/>have an <strong class="bold">imbalanced</strong> dataset. Techniques for balancing classes will be discussed in <a id="_idIndexMarker551"/>detail in <a href="B19005_14.xhtml#_idTextAnchor248"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>,<em class="italic"> </em>but basically, they <a id="_idIndexMarker552"/>include <strong class="bold">undersampling</strong> (where some items in the more common class are discarded), <strong class="bold">oversampling</strong> (where items in the rarer classes are duplicated), and <strong class="bold">generation</strong> (where artificial examples from the rarer classes are generated <span class="No-Break">through rules).</span></p>
			<p>Systems generally perform better if they have more data, but the data also has to be representative of the data that the system will encounter at the time of testing, or when the system is deployed as an application. If a lot of new vocabulary has been added to the task (for example, if a company’s chatbot has to deal with new product names), the training data needs to be periodically updated for the best performance. This is related to the general question of specialized vocabulary and syntax since product names are a kind of specialized vocabulary. In the next section, we will discuss <span class="No-Break">this topic.</span></p>
			<h3>Specialized vocabulary and syntax</h3>
			<p>Another consideration is <a id="_idIndexMarker553"/>how similar the data is to the rest of the natural language we will be <a id="_idIndexMarker554"/>processing. This is important because most NLP processing makes use of models that were derived from previous examples of the language. The more similar the data to be analyzed is to the rest of the language, the easier it will be to build a successful application. If the data is full of specialized <a id="_idIndexMarker555"/>jargon, vocabulary, or syntax, then it will be hard for the system to generalize from its original training data <a id="_idIndexMarker556"/>to the new data. If the application is full of specialized vocabulary and syntax, the amount of training data will need to be increased to include this new vocabulary <span class="No-Break">and syntax.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor149"/>Considering computational efficiency</h2>
			<p>The computer resources <a id="_idIndexMarker557"/>that are needed to implement a particular NLP approach are an important consideration in selecting an approach. Some approaches that yield good results when tested on laboratory benchmarks can be impractical in applications that are intended for deployment. In the next sections, we will discuss the important consideration of the time required to execute the approaches, both at training time and <span class="No-Break">inference time.</span></p>
			<h3>Training time</h3>
			<p>Some modern neural <a id="_idIndexMarker558"/>net models are very computationally intensive and require very long training periods. Even before the actual neural net training starts, there may need to be some exploratory efforts aimed at identifying the best values for hyperparameters. <strong class="bold">Hyperparameters</strong> are training parameters that <a id="_idIndexMarker559"/>can’t directly be estimated during the training process, and that have to be set by the developer. When we return to machine learning techniques in <em class="italic">Chapters 9</em>, <em class="italic">10</em>, <em class="italic">11</em>, and <em class="italic">12</em>, we will look at specific hyperparameters and talk about how to identify <span class="No-Break">good values.</span></p>
			<h3>Inference time</h3>
			<p>Another important consideration is <strong class="bold">inference time</strong>, or the processing time required for a trained system <a id="_idIndexMarker560"/>to perform its task. For interactive applications such as chatbots, inference time is not normally a concern because today’s systems are fast enough to keep up with a user in an interactive application. If a system takes a second or two to process a user’s input, that’s acceptable. On the other hand, if the system needs to process a large amount of existing online <a id="_idIndexMarker561"/>text or audio data, the inference time should be as fast as possible. For example, Statistica.com (<a href="https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/">https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/</a>) estimated in February 2020 that 500 hours of videos were <a id="_idIndexMarker562"/>uploaded to YouTube every minute. If an application was designed to process YouTube videos and needed to keep up with that volume of audio, it would need to be <span class="No-Break">very fast.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor150"/>Initial studies</h2>
			<p>Practical NLP requires fitting the tool to the problem. When there are new advances in NLP technology, there can be very enthusiastic articles in the press about what the advances mean. But if you’re trying to solve a practical problem, trying to use new techniques can be <a id="_idIndexMarker563"/>counterproductive because the newest technologies might not scale. For example, new techniques might provide higher accuracy, but at the cost of very long training periods or very large amounts of data. For this reason, it is recommended that when you’re trying to solve a practical problem, do some initial exploratory studies with simpler techniques to see whether they’ll solve the problem. Only if the simpler techniques don’t address the problem’s requirements should more advanced techniques <span class="No-Break">be utilized.</span></p>
			<p>In the next section, we will talk about one of the important choices that need to be made when you design an NLP application – how to represent the data. We’ll look at both symbolic and <span class="No-Break">numerical representations.</span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor151"/>Representing language for NLP applications</h1>
			<p>For computers to work with natural language, it has to be represented in a form that they can <a id="_idIndexMarker564"/>process. These representations can be <strong class="bold">symbolic</strong>, where the words in a text are processed directly, or <strong class="bold">numeric</strong>, where the representation is in the form of numbers. We will describe both of these approaches here. Although the numeric approach is the primary approach currently used in NLP research and applications, it is worth becoming somewhat familiar with the ideas behind <span class="No-Break">symbolic processing.</span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor152"/>Symbolic representations</h2>
			<p>Traditionally, NLP has been <a id="_idIndexMarker565"/>based on <a id="_idIndexMarker566"/>processing the words in texts directly, as words. This approach was embodied in a standard approach where the text was analyzed in a series of steps that were aimed at converting an input consisting of unanalyzed words into a meaning. In a traditional NLP pipeline, shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em>, each step in <a id="_idIndexMarker567"/>processing, from input <a id="_idIndexMarker568"/>text to meaning, produces an output that adds more structure to its input and prepares it for the next step in processing. All of these results are symbolic – that is, non-numerical. In some cases, the results might include probabilities, but the actual results <span class="No-Break">are symbolic:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B19005_07_01.jpg" alt="Figure 7.1﻿ – Traditional NLP symbolic pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Traditional NLP symbolic pipeline</p>
			<p>Although we won’t review all of the components of the symbolic approach to processing, we can see a couple of these symbolic results in the following code samples, showing part of speech tagging results and parsing results, respectively. We will not address semantic analysis or pragmatic analysis here since these techniques are generally applied only for <span class="No-Break">specialized problems:</span></p>
			<pre class="source-code">
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import movie_reviews
example_sentences = movie_reviews.sents()
example_sentence = example_sentences[0]
nltk.pos_tag(example_sentence)</pre>
			<p>The preceding code <a id="_idIndexMarker569"/>snippet shows the process of importing the movie review database, which we <a id="_idIndexMarker570"/>have seen previously, followed by the code for selecting the first sentence and part of speech tagging it. The next code snippet shows the results of part of <span class="No-Break">speech tagging:</span></p>
			<pre class="source-code">
[('plot', 'NN'),
 (':', ':'),
 ('two', 'CD'),
 ('teen', 'NN'),
 ('couples', 'NNS'),
 ('go', 'VBP'),
 ('to', 'TO'),
 ('a', 'DT'),
 ('church', 'NN'),
 ('party', 'NN'),
 (',', ','),
 ('drink', 'NN'),
 ('and', 'CC'),
 ('then', 'RB'),
 ('drive', 'NN'),
 ('.', '.')]</pre>
			<p>The tags (<strong class="source-inline">NN</strong>, <strong class="source-inline">CD</strong>, <strong class="source-inline">NNS</strong>, etc.) shown in the preceding results are those used by NLTK and are commonly used in NLP. They are originally based on the Penn Treebank tags (<em class="italic">Building a Large Annotated Corpus of English: The Penn Treebank</em> (Marcus et al., <span class="No-Break">CL 1993)).</span></p>
			<p>Another important <a id="_idIndexMarker571"/>type of <a id="_idIndexMarker572"/>symbolic processing is <strong class="bold">parsing</strong>. We can see the result of parsing the <a id="_idIndexMarker573"/>first sentence, <strong class="source-inline">plot: two teen couples go to a church party, drink and then drive</strong>, which we saw in the preceding code snippet, in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import spacy
text = "plot: two teen couples go to a church party, drink and then drive."
nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
for token in doc:
    print (token.text, token.tag_, token.head.text, token.dep_)
plot NN plot ROOT
: : plot punct
two CD couples nummod
teen NN couples compound
couples NNS go nsubj
go VBP plot acl
to IN go prep
a DT party det
church NN party compound
party NN to pobj
, , go punct
drink VBP go conj
and CC drink cc
then RB drive advmod
drive VB drink conj
. . go punct</pre>
			<p>The preceding code uses spaCy to parse the first sentence in the movie review corpus. After the parse, we iterate through the tokens in the resulting document and print the token and its <a id="_idIndexMarker574"/>part of speech tag. These are followed by the text of the token’s head, or the word that it depends on, and <a id="_idIndexMarker575"/>the kind of dependency between the head and the token. For example, the word <strong class="source-inline">couples</strong> is tagged as a plural noun, it is dependent on the word <strong class="source-inline">go</strong>, and the dependency is <strong class="source-inline">nsubj</strong> since <strong class="source-inline">couples</strong> is the noun subject (<strong class="source-inline">nsubj</strong>) of <strong class="source-inline">go</strong>. In <a href="B19005_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we saw an example of a visualization of a dependency parse (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.6</em>), which represents the dependencies as arcs between the item and its head; however, in the preceding code, we see more of the <span class="No-Break">underlying information.</span></p>
			<p>In this section, we have seen some examples of symbolic representations of language based on analyzing individual words and phrases, including parts of speech of individual words and labels for phrases. We can also represent words and phrases using a completely different and completely numeric approach based <span class="No-Break">on vectors.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor153"/>Representing language numerically with vectors</h1>
			<p>A common <a id="_idIndexMarker576"/>mathematical technique for <a id="_idIndexMarker577"/>representing language in preparation for machine learning is through the use of vectors. Both documents and words can be represented with vectors. We’ll start by discussing <span class="No-Break">document vectors.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor154"/>Understanding vectors for document representation</h2>
			<p>We have seen that texts can be represented as sequences of symbols such as words, which is the way that we read them. However, it is usually more convenient for computational NLP <a id="_idIndexMarker578"/>purposes to represent text numerically, especially if we are dealing with large quantities of text. Another advantage of numerical representation is that we can also process text represented numerically with a much wider range of <span class="No-Break">mathematical techniques.</span></p>
			<p>A common way to represent both documents and words is by using vectors, which are basically one-dimensional arrays. Along with words, we can also use vectors to represent other linguistic units, such as lemmas or stemmed words, which were described in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<h3>Binary bag of words</h3>
			<p>In <a href="B19005_03.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> and <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, we briefly <a id="_idIndexMarker579"/>discussed the <strong class="bold">bag of words</strong> (<strong class="bold">BoW</strong>) approach, where <a id="_idIndexMarker580"/>each document in a corpus is represented by a vector whose length is the size of the vocabulary. Each position in the vector corresponds to a word in the vocabulary and the element in the vector is <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong>, depending on whether that word occurs in that document. Each position in the vector is a feature of the document – that is, whether or not the word occurs. This is the simplest form of the BoW, and it is called the <strong class="bold">binary bag of words</strong>. It is immediately clear that this is a very coarse way <a id="_idIndexMarker581"/>of representing documents. All it cares about is whether a word occurs in a document, so it fails to capture a lot of information – what words are nearby, where in the document the words occur, and how often the words occur are all missing in the binary BoW. It is also affected by the lengths of documents since longer documents will have <span class="No-Break">more words.</span></p>
			<p>A more detailed <a id="_idIndexMarker582"/>version of the BoW approach <a id="_idIndexMarker583"/>is to count not just whether a word appears in a document but also how many times it appears. For this, we'll move on to the next technique, <strong class="bold">count bag </strong><span class="No-Break"><strong class="bold">of words</strong></span><span class="No-Break">.</span></p>
			<h3>Count bag of words</h3>
			<p>It seems intuitive <a id="_idIndexMarker584"/>that the number of times <a id="_idIndexMarker585"/>a word occurs in a document would help us decide how similar two documents are to each other. However, so far, we haven’t used that information. In the document vectors we’ve seen so far, the values are just one and zero – one if the word occurs in the document and zero if it doesn’t. If instead, we let the values represent the number of times the word occurs in the document, then we have more information. The BoW that includes the frequencies of the words in a document is a <span class="No-Break"><strong class="bold">count BoW</strong></span><span class="No-Break">.</span></p>
			<p>We saw the code to generate the binary BoW in the <em class="italic">Bag of words and k-means clustering</em> section (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em>) in <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. The code can be very slightly modified to compute a count BoW. The only change that has to be made is to increment the total count for a word when it is found more than once in a document. This is shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
def document_features(document):
    features = {}
    for word in word_features:
        features[word] = 0
        for doc_word in document:
            if word == doc_word:
                features[word] += 1
    return features</pre>
			<p>Comparing this code to the code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em>, we can see that the only difference is that the value of <strong class="source-inline">features[word]</strong>is incremented when the word is found in the document, rather than set to <strong class="source-inline">1</strong>. The resulting matrix, shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, has many different values for word frequencies than the matrix in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.16</em>, which had only zeros <span class="No-Break">and ones:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B19005_07_02.jpg" alt="Figure 7.2﻿ – Count BoW for the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Count BoW for the movie review corpus</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, we’re looking at 10 randomly selected documents, numbered from <strong class="source-inline">0</strong> to <strong class="source-inline">9</strong> in the first column. Looking more closely at the frequencies of <strong class="source-inline">film</strong>, (recall that <strong class="source-inline">film</strong> is the most common <a id="_idIndexMarker586"/>non-stopword in the corpus), we can see that all of the documents except document <strong class="source-inline">5</strong> and document <strong class="source-inline">6</strong> have at <a id="_idIndexMarker587"/>least one occurrence of <strong class="source-inline">film</strong>. In a binary BoW, they would all be lumped together, but here they have different values, which allows us to make finer-grained distinctions among documents. At this point, you might be interested in going back to the clustering exercise in <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, modifying the code in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.22</em> to use the count BoW, and looking at the <span class="No-Break">resulting clusters.</span></p>
			<p>We can see from <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em> that the count BoW gives us some more information about the words that occur in the documents than the binary BoW. However, we can do an even more precise analysis by using a technique called <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>), which we describe in the <span class="No-Break">next section.</span></p>
			<h3>Term frequency-inverse document frequency</h3>
			<p>Considering that our goal is to find a representation that accurately reflects similarities between documents, we can make use of <a id="_idIndexMarker588"/>some other insights. Specifically, consider the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker589"/></span><span class="No-Break">observations:</span></p>
			<ul>
				<li>The raw frequency of words in a document will vary based on the length of the document. This means that a shorter document with fewer overall words might not appear to be similar to a longer document that has more words. So, we should be considering the proportion of the words in the document rather than the <span class="No-Break">raw number.</span></li>
				<li>Words that occur very often overall will not be useful in distinguishing documents, because every document will have a lot of them. Clearly, the most problematic words that commonly occur within a corpus will be the stopwords we discussed in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, but other words that are not strictly stopwords can have this property as well. Recall when we looked at the movie review corpus in <a href="B19005_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> that the words <strong class="source-inline">film</strong> and <strong class="source-inline">movie</strong> were very common in both positive and negative reviews, so they won’t be able to help us tell those categories apart. The most helpful words will probably be the words that occur with different frequencies in <span class="No-Break">different categories.</span></li>
			</ul>
			<p>A popular <a id="_idIndexMarker590"/>way of taking these concerns into account is the measure <em class="italic">TF-IDF</em>. TF-IDF consists <a id="_idIndexMarker591"/>of two measurements – <strong class="bold">term frequency</strong> (<strong class="bold">TF</strong>) and <strong class="bold">inverse document </strong><span class="No-Break"><strong class="bold">frequency</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IDF</strong></span><span class="No-Break">).</span></p>
			<p>TF is the number of times a term (or word) appears in a document, divided by the total number of terms in the document (which takes into account the fact that longer documents have more words overall). We can define that value as <strong class="source-inline">tf(term, document)</strong>. For example, in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2,</em> we can see that the term <strong class="source-inline">film</strong> appeared once in document <strong class="source-inline">0</strong>, so <strong class="source-inline">tf("film",0)</strong> is <strong class="source-inline">1</strong> over the length of document <strong class="source-inline">0</strong>. Since the second document contains <strong class="source-inline">film</strong> four times, <strong class="source-inline">tf("film",1)</strong> is <strong class="source-inline">4</strong> over the length of document <strong class="source-inline">1</strong>. The formula for term frequency is <span class="No-Break">as follows:</span></p>
			<p>                                                                    <span class="_-----MathTools-_Math_Text">tf</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>However, as we saw with stopwords, very frequent words don’t give us much information for distinguishing documents. Even if <strong class="source-inline">TF(term, document)</strong> is very large, that could just be due to the fact that <strong class="source-inline">term</strong> occurs frequently in every document. To take care of this, we introduce IDF. The numerator of <strong class="source-inline">idf(term,Documents)</strong>is the total number of documents in the corpus, <em class="italic">N</em>, which we divide by the number of documents (<em class="italic">D</em>) that contain the term, <em class="italic">t</em>. In case the term doesn’t appear in the corpus, the denominator would be 0, so <strong class="source-inline">1</strong> is added to the denominator to prevent division by 0. <strong class="source-inline">idf(term, documents)</strong>is the log of this quotient. The formula for <strong class="source-inline">idf</strong> is <span class="No-Break">as follows:</span></p>
			<p>                                                            <span class="_-----MathTools-_Math_Text">idf</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">:</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">|</span><span class="_-----MathTools-_Math_Base">}</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>Then the <em class="italic">TF-IDF</em> value for a term in a document in a given corpus is just the product of its TF <span class="No-Break">and IDF:</span></p>
			<p>                                                               <span class="_-----MathTools-_Math_Text">tfidf</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">tf</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Text">idf</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">D</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">)</span></span></p>
			<p>To compute the TF-IDF vectors of the movie review corpus, we will use another very useful package, called <strong class="source-inline">scikit-learn</strong>, since NLTK and spaCy don’t have built-in functions for TF-IDF. The <a id="_idIndexMarker592"/>code to compute TF-IDF in those packages could be <a id="_idIndexMarker593"/>written by hand using the standard formulas we saw in the preceding three equations; however, it will be faster to implement if we use the functions in <strong class="source-inline">scikit-learn</strong>, in particular, <strong class="source-inline">tfidfVectorizer</strong> in the feature extraction package. The code to compute TF-IDF vectors for the movie review corpus of 2,000 documents is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>. In this example, we will look only at the top <span class="No-Break">200 terms.</span></p>
			<p>A tokenizer is defined in lines 9-11. In this example, we are using just the standard NLTK tokenizer, but any other text-processing function could be used here as well. For example, we might want to try using stemmed or lemmatized tokens, and these functions could be included in the <strong class="source-inline">tokenize()</strong> function. Why might stemming or lemmatizing text be a <span class="No-Break">good idea?</span></p>
			<p>One reason that this kind of preprocessing could be useful is that it will reduce the number of unique tokens in the data. This is because words that have several different variants will be collapsed into their root word (for example, <em class="italic">walk</em>, <em class="italic">walks</em>, <em class="italic">walking</em>, and <em class="italic">walked</em> will all be treated <a id="_idIndexMarker594"/>as the same word). If we believe that this variation is mostly <a id="_idIndexMarker595"/>just a source of noise in the data, then it’s a good idea to collapse the variants by stemming or lemmatization. However, if we believe that the variation is important, then it won’t be a good idea to collapse the variants, because this will cause us to lose information. We can make this kind of decision a priori by thinking about what information is needed for the goals of the application, or we can treat this decision as a hyperparameter, exploring different options and seeing how they affect the accuracy of the <span class="No-Break">final result.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> shows a screenshot of <span class="No-Break">the code.</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B19005_07_03.jpg" alt="Figure 7.3﻿ – Code to compute TF-IDF vectors for the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Code to compute TF-IDF vectors for the movie review corpus</p>
			<p>Returning to <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, the next step after defining the tokenization function is to define the path where the data can be found (line 13) and initialize the token dictionary at line 14. The code then walks the data directory and collects the tokens from each file. While the code is collecting tokens, it lowercases the text and removes punctuation (line 22). The decisions to lowercase the text and remove punctuation are up to the developer, similar to the decision we discussed previously on whether or not to stem or lemmatize the tokens. We could explore empirically whether or not these two preprocessing steps improve processing accuracy; however, if we think about how much meaning is carried by case and punctuation, it seems clear that in many applications, case and punctuation don’t add much meaning. In those kinds of applications, lowercasing the text and removing punctuation will improve <span class="No-Break">the results.</span></p>
			<p>Having collected and counted the tokens in the full set of files, the next step is to initialize <strong class="source-inline">tfIdfVectorizer</strong>, which is a built-in function in scikit-learn. This is accomplished in lines 25-29. The parameters include the type of input, whether or not to use IDF, which tokenizer to use, how many features to use, and the language for stopwords (English, in <span class="No-Break">this example).</span></p>
			<p>Line 32 is where the real work of TF-IDF is done, with the <strong class="source-inline">fit_transform</strong> method, where the TF-IDF vector is built from the documents and the tokens. The remaining code (lines 37-42) is primarily to assist in displaying the resulting <span class="No-Break">TF-IDF vectors.</span></p>
			<p>The resulting TF-IDF matrix is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B19005_07_04.jpg" alt="Figure 7.4﻿ – Partial ﻿TF-IDF vectors for some of the documents in the movie review corpus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Partial TF-IDF vectors for some of the documents in the movie review corpus</p>
			<p>We have now <a id="_idIndexMarker596"/>represented our movie review corpus as a matrix where every document is a <a id="_idIndexMarker597"/>vector of <em class="italic">N</em> dimensions, where <em class="italic">N</em> is the maximum size of vocabulary we are going to use. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> shows the TF-IDF vectors for a few of the 2,000 documents in the corpus (documents 0-4 and 1995-1999), shown in the rows, and some of the words in the corpus, shown in alphabetical order at the top. Both the words and the documents are truncated for <span class="No-Break">display purposes.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em>, we can see that there is quite a bit of difference in the TF-IDF values for the same words in different documents. For example, <strong class="source-inline">acting</strong> has considerably different scores in documents <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This will become useful in the next step of processing (classification), which we will return to in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>. Note that we have not done any actual machine learning yet; so far, the goal has been simply to convert documents to numerical representations, based on the words <span class="No-Break">they contain.</span></p>
			<p>Up until now, we’ve been focusing on representing documents. But what about representing the <a id="_idIndexMarker598"/>words themselves? The words in a document vector are just numbers <a id="_idIndexMarker599"/>representing their frequency, either just in a document or their frequency in a document relative to their frequency in a corpus (TF-IDF). We don’t have any information about the meanings of the words themselves in the techniques we’ve looked at so far. However, it seems clear that the meanings of words in a document should also impact the document’s similarity to other documents. We will look at representing the meanings of words in the next section. This <a id="_idIndexMarker600"/>representation is often called <strong class="bold">word embeddings</strong> in the NLP literature. We will begin with a popular representation of words as vectors, <strong class="bold">Word2Vec</strong>, which captures <a id="_idIndexMarker601"/>the similarity in meaning of words to <span class="No-Break">each other.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor155"/>Representing words with context-independent vectors</h1>
			<p>So far, we have looked at several ways of representing similarities among documents. However, finding <a id="_idIndexMarker602"/>out that two or <a id="_idIndexMarker603"/>more documents are similar to each other is not very specific, although it can be useful for some applications, such as intent or document classification. In this section, we will talk about representing the meanings of words with <span class="No-Break">word vectors.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor156"/>Word2Vec</h2>
			<p>Word2Vec is a popular <a id="_idIndexMarker604"/>library for representing words as vectors, published by Google in 2013 (Mikolov, Tomas; et al. (2013). <em class="italic">Efficient Estimation of Word Representations in Vector Space</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>). The basic idea behind Word2Vec is that every word in a corpus is represented by a single vector that is computed based on all the contexts (nearby words) in which the word occurs. The intuition behind this approach is that words with similar meanings will occur in similar contexts. This intuition is summarized in a famous quote from the linguist J. R. Firth, “<em class="italic">You shall know a word by the company it keeps</em>” (<em class="italic">Studies in Linguistic </em><span class="No-Break"><em class="italic">Analysis</em></span><span class="No-Break">, Wiley-Blackwell).</span></p>
			<p>Let’s build up to Word2Vec by starting with the idea of assigning each word to a vector. The simplest vector that we can use to represent words is the idea of <strong class="bold">one-hot encoding</strong>. In one-hot encoding, each word in the vocabulary is represented by a vector where that word has <strong class="source-inline">1</strong> in a<a id="_idIndexMarker605"/> specific position in the vector, and all the rest of the positions are zeros (it is called one-hot encoding because one bit is on – that is, <em class="italic">hot</em>). The length of the vector is the size of the vocabulary. The set of one-hot vectors for the words in a corpus is something like a dictionary in that, for example, we could say that if the word is <strong class="source-inline">movie</strong>, it will be represented by <strong class="source-inline">1</strong> in a specific position. If the word is <strong class="source-inline">actor</strong>, it will be represented by <strong class="source-inline">1</strong> in a <span class="No-Break">different position.</span></p>
			<p>At this point, we aren’t <a id="_idIndexMarker606"/>taking into account the surrounding words yet. The first step in one-hot encoding is integer encoding, where we assign a specific integer to each word in the corpus. The following code uses libraries from scikit-learn to do the integer encoding and the one-hot encoding. We also import some functions from the <strong class="source-inline">numpy</strong> library, <strong class="source-inline">array</strong> and <strong class="source-inline">argmax</strong>, which we’ll be returning to in <span class="No-Break">later chapters:</span></p>
			<pre class="source-code">
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
#import the movie reviews
from nltk.corpus import movie_reviews
# make a list of movie review documents
documents = [(list(movie_reviews.words(fileid)))
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]
# for this example, we'll just look at the first document, and
# the first 50 words
data = documents[0]
values = array(data)
short_values = (values[:50])
# first encode words as integers
# every word in the vocabulary gets a unique number
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(short_values)
# look at the first 50 encodings
print(integer_encoded)
[32  3 40 35 12 19 39  5 10 31  1 15  8 37 16  2 38 17 26  7  6  2 30 29
 36 20 14  1  9 24 18 11 39 34 23 25 22 27  1  8 21 28  2 42  0 33 36 13
  4 41]</pre>
			<p>Looking at the first <a id="_idIndexMarker607"/>50 integer encodings in the first movie review, we see a vector of length <strong class="source-inline">50</strong>. This can be converted to a one-hot encoding, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# convert the integer encoding to onehot encoding
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(
    len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(
    integer_encoded)
print(onehot_encoded)
# invert the first vector so that we can see the original word it encodes
inverted = label_encoder.inverse_transform(
    [argmax(onehot_encoded[0, :])])
print(inverted)
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 1. 0.]]
['plot']</pre>
			<p>The output in the preceding code shows first a subset of the one-hot vectors. Since they are one-hot vectors, they have <strong class="source-inline">0</strong> in all positions except one position, whose value is <strong class="source-inline">1</strong>. Obviously, this is very <a id="_idIndexMarker608"/>sparse, and it would be good to provide a more <span class="No-Break">condensed representation.</span></p>
			<p>The next to the last line shows how we can invert a one-hot vector to recover the original word. The sparse representation takes a large amount of memory and is not <span class="No-Break">very practical.</span></p>
			<p>The Word2Vec approach uses a neural net to reduce the dimensionality of the embedding. We will be returning to the details of neural networks in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, but for this example, we’ll use a library called <strong class="source-inline">Gensim</strong> that will compute Word2Vec <span class="No-Break">for us.</span></p>
			<p>The following code uses the Gensim <strong class="source-inline">Word2Vec</strong> library to create a model of the movie review corpus. The Gensim <strong class="source-inline">model</strong> object that was created by Word2Vec includes a large number of interesting methods for working with the data. The following code shows one of these – <strong class="source-inline">most_similar</strong>, which, given a word, can find the words that are the most similar to that <a id="_idIndexMarker609"/>word in the dataset. Here, we can see a list of the 25 words most similar to <strong class="source-inline">movie</strong> in the corpus, along with a score that indicates how similar that word is to <strong class="source-inline">movie</strong>, according to the <span class="No-Break">Word2Vec analysis:</span></p>
			<pre class="source-code">
import gensim
import nltk
from nltk.corpus import movie_reviews
from gensim.models import Word2Vec
# make a list of movie review documents
documents = [(list(movie_reviews.words(fileid)))
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]
all_words = movie_reviews.words()
model = Word2Vec(documents, min_count=5)
model.wv.most_similar(positive = ['movie'],topn = 25)
[('film', 0.9275647401809692),
 ('picture', 0.8604983687400818),
 ('sequel', 0.7637531757354736),
 ('flick', 0.7089548110961914),
 ('ending', 0.6734793186187744),
 ('thing', 0.6730892658233643),
 ('experience', 0.6683703064918518),
 ('premise', 0.6510635018348694),
 ('comedy', 0.6485130786895752),
 ('genre', 0.6462267637252808),
 ('case', 0.6455731391906738),
 ('it', 0.6344209313392639),
 ('story', 0.6279274821281433),
 ('mess', 0.6165297627449036),
 ('plot', 0.6162343621253967),
 ('message', 0.6131927371025085),
 ('word', 0.6131172776222229),
 ('movies', 0.6125075221061707),
 ('entertainment', 0.6109789609909058),
 ('trailer', 0.6068858504295349),
 ('script', 0.6000528335571289),
 ('audience', 0.5993804931640625),
 ('idea', 0.5915037989616394),
 ('watching', 0.5902948379516602),
 ('review', 0.5817495584487915)]</pre>
			<p>As can be seen in the preceding code, the words that Word2Vec finds to be the most similar to <strong class="source-inline">movie</strong> based <a id="_idIndexMarker610"/>on the contexts in which they occur are very much what we would expect. The top two words, <strong class="source-inline">film</strong> and <strong class="source-inline">picture</strong>, are very close synonyms of <strong class="source-inline">movie</strong>. In <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we will return to Word2Vec and see how this kind of model can be used in <span class="No-Break">NLP tasks.</span></p>
			<p>While Word2Vec does take into account the contexts in which words occur in a dataset, every word in the <a id="_idIndexMarker611"/>vocabulary is represented by a single vector that encapsulates all of the contexts in which it occurs. This glosses over the fact that words can have different meanings in different contexts. The next section reviews approaches representing words depending on <span class="No-Break">specific contexts.</span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor157"/>Representing words with context-dependent vectors</h1>
			<p>Word2Vec’s word vectors are <a id="_idIndexMarker612"/>context-independent in that a word always has the same vector no matter what context it occurs in. However, in fact, the meanings of words are strongly affected by nearby words. For example, the meanings of the word <em class="italic">film</em> in <em class="italic">We enjoyed the film</em> and <em class="italic">the table was covered with a thin film of dust</em> are quite different. To capture these contextual differences in meanings, we would like to have a way to have different vector representations of these words that reflect the differences in meanings that result from the different contexts. This research direction has been extensively explored in the last few years, starting with the <strong class="bold">BERT</strong> (<strong class="bold">Bidirectional Encoder Representations from Transformers</strong>) system (<a href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</a> (Devlin et al., <span class="No-Break">NAACL 2019)).</span></p>
			<p>This approach has resulted in great improvements in NLP technology, which we will want to discuss in depth. For that reason, we will postpone a fuller look at context-dependent word representations until we get to <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, where we will address this topic <span class="No-Break">in detail.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, we’ve learned how to select different NLP approaches, based on the available data and other requirements. In addition, we’ve learned about representing data for NLP applications. We’ve placed particular emphasis on vector representations, including vector representations of both documents and words. For documents, we’ve covered binary bag of words, count bag of words, and TF-IDF. For representing words, we’ve reviewed the Word2Vec approach and briefly introduced context-dependent vectors, which will be covered in much more detail in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
			<p>In the next four chapters, we will take the representations that we’ve learned about in this chapter and show how to train models from them that can be applied to different problems such as document classification and intent recognition. We will start with rule-based techniques in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, discuss traditional machine learning techniques in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, talk about neural networks in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, and discuss the most modern approaches, transformers, and pretrained models in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
		</div>
	</body></html>