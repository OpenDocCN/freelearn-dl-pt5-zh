- en: '*Chapter 16*: Marketing, Personalization and Finance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discuss three areas in which RL (RL) is gaining significant
    traction. First, we describe how it can be used in personalization and recommendation
    systems. With that, we go beyond the single-step bandit approaches we covered
    in the earlier chapters. A related field that can also significantly benefit from
    RL is marketing. In addition to personalized marketing applications, RL can help
    in areas such as managing campaign budgets and reducing customer churn. Finally,
    we discuss the promise of RL in finance and related challenges. In doing so, we
    introduce TensorTrade, a Python framework for developing and testing RL-based
    trading algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this chapter, we cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond bandits for personalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing effective marketing strategies using RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying RL in finance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going beyond bandits for personalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we covered multi-armed and contextual bandit problems in the early chapters
    of the book, we presented a case study aimed at maximizing the **click-through
    rate (CTR)** of online ads. This is just one example of how bandit models can
    be used to provide users with personalized content and experience, a common challenge
    of almost all online (and offline) content providers, from e-retailers to social
    media platforms. In this section, we go beyond the bandit models and describe
    a multi-step RL approach to personalization. Let's first start by discussing where
    the bandit models fall short, and then how multi-step RL can address those issues.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of bandit models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal in bandit problems is to maximize the immediate (single-step) return.
    In an online ad CTR maximization problem, this is usually a good way of thinking
    about the goal: an online ad is displayed, the user clicks, and voila! If not,
    it''s a miss.'
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between the user and, let's say, YouTube or Amazon, is much
    more sophisticated than this. The user experience on such platforms is a journey,
    rather than an event. The platform recommends some content, and it is not a total
    miss if the user does not click on it. It could be the case that the user finds
    the presented content interesting and enticing and just continues browsing. Even
    if that user session does not result in a click or conversion, the user may come
    back soon knowing that the platform is serving some content relevant to the user's
    interests. Conversely, too many clicks in a session could very well mean that
    the user is not able to find what they are looking for, leading to a bad experience.
    This "journey-like" nature of the problem, the fact that the platform (agent)
    decisions have a downstream impact on the customer satisfaction and business value
    (reward), is what makes multi-step RL an appealing approach here.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Although it is tempting to depart from a bandit model towards multi-step RL,
    think twice before doing so. Bandit algorithms are much easier to use and have
    well-understood theoretical properties, whereas it could be quite challenging
    to successfully train an agent in a multi-step RL setting. Note that many multi-step
    problems can be cast as a single-step problem by including a memory for the agent
    in the context and the expected future value of the action in the immediate reward,
    which then allows us to stay in the bandit framework.
  prefs: []
  type: TYPE_NORMAL
- en: One of the successful implementations of multi-step deep RL for personalization
    is related to news recommendation, proposed by (Zheng et al. 2018). In the next
    section, we describe a similar approach to the problem inspired by this work,
    although our discussion will be at a higher level and broader than what the paper
    suggests.
  prefs: []
  type: TYPE_NORMAL
- en: Deep RL for news recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we go on our favorite news app, we expect to read some interesting and
    perhaps important content. Of course, what makes news interesting or important
    is different for everyone, so we have a personalization problem. As Zheng et al.
    (2018) mention, there are some additional challenges in news recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The action space is not fixed. In fact, it is quite the opposite: There is
    so much news flowing in during a day, each with some unique characteristics, making
    it hard to think about the problem like a traditional Gym environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User preferences are quite dynamic too: They change and evolve over time. A
    user who is more into politics this week can get bored and read about art next
    week, which is what Zheng et al. (2018) demonstrate with data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned, this is a truly multi-step problem. If there are two pieces
    of news that can be displayed, one about a disaster and the other about a sports
    game, showing the former could have a higher chance of getting clicked due to
    its sensational nature. It could also lead to the user leaving the platform early
    as their morale decreases, preventing more engagement on the platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What the agent observes about the user is so limited compared to all possible
    factors playing a role in the user's behavior. Therefore, the environment is partially
    observable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the problem is to choose which news piece(s) from a dynamic inventory to
    display to a user whose interests, one, change over time, and two, are impacted
    by many factors that the agent does not fully observe.
  prefs: []
  type: TYPE_NORMAL
- en: Let's next describe the components of the RL problem here.
  prefs: []
  type: TYPE_NORMAL
- en: The observation and action space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Borrowing from Zheng et al. (2018), there are the following pieces of information
    that make up the observation and action space about a particular user and a news
    piece:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User features** related to the features of all the news pieces the user clicked
    in that session, that day, in the past week, how many times the user has come
    to the platform over various time horizons, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context features** are related to the information about the time of the day,
    the day of the week, whether it is a holiday, election day, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-news features** about how many times this particular news piece appeared
    in the feed of the particular user over the past hour, and similar statistics
    related to the entities, topics, and categories in the news.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**News features** of this particular piece such as topic, category, provider,
    entity names, click counts over the past hour, the past 6 hours, past day, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, this makes up a different observation-action space than what we are used
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two sets of features are more like an observation: The user shows
    up, requests news content, and the agent observes the user and context-related
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the agent needs to pick a news piece to display (or a set of news pieces,
    as in the paper). Therefore, the features related to the news and user-news correspond
    to an action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is interesting is that the available action set is dynamic, and it carries
    elements related to the user/observation (in user-news features).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although this is not a traditional setup, don''t let that scare you. We can
    still estimate, for example, the Q-value of a given observation-action pair: We
    simply need to feed all these features to a neural network that estimates the
    Q-value. To be specific, the paper uses two neural networks, one estimating the
    state-value and the other the advantage, to calculate the Q-value, although a
    single network can also be used. This is depicted in Figure 16.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – Q-network of a news recommendation agent'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_16_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.1 – Q-network of a news recommendation agent
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, compare this to a traditional Q-network:'
  prefs: []
  type: TYPE_NORMAL
- en: A regular Q-network would have multiple heads, one for each action (and plus
    one for the value estimation if the action heads are estimating advantages rather
    than Q values). Such a network outputs Q-value estimates for all of the actions
    in a fixed set for a given observation in a single forward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this setup, we need to make a separate forward pass over the network for
    each available news piece given the user, and then pick the one(s) with highest
    Q values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is actually similar to what we used in [*Chapter 3*](B14160_03_Final_SK_ePub.xhtml#_idTextAnchor059),
    *Contextual Bandits*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss an alternative modeling approach that we could use in this
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: Using action embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the action space is very large and/or it varies each time step, like in
    this problem, **action embeddings** can be used to select an action given an observation.
    An action embedding is a representation of the action as a fixed-sized array,
    usually obtained as an outcome of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: We use a policy network that outputs, rather than action values, an **intention
    vector**, a fixed-size array of numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intention vector carries information about what the ideal action would look
    like given the observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the context of the news recommendation problem, such an intention vector
    would mean something like: "Given these user and context features, this user wants
    to read about team sports from international leagues that are in finals."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This intention vector is then compared to the available actions. The action
    that is "closest" to the "intention" is chosen. For example, a news piece about
    team sports in a foreign league, but not playing the finals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A measure of the closeness is cosine similarity, which is the dot product of
    an intention and action embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This setup is illustrated in Figure 16.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Using action embeddings for news recommendation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_16_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.2 – Using action embeddings for news recommendation
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: The use of embeddings in this way is something OpenAI introduced to deal with
    the huge action space in Dota 2\. A nice blog explaining their approach is at
    [https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/](https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five/).
    This can also be implemented fairly easily in RLlib, which is explained at [https://bit.ly/2HQRkfx](https://bit.ly/2HQRkfx).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss what the reward function looks like in the news recommendation
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The reward function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the online ad problem that we solved at the beginning of the book, maximizing
    the CTR was the only objective. In the news recommendation problem, we would like
    to maximize the long-term engagement with the user, while also increasing the
    likelihood of immediate clicks. This requires a measure for user activeness, for
    which the paper uses a survival model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration using dueling bandit gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploration is an essential component of RL. When we train RL agents in simulation,
    we don't care about taking bad actions for the sake of learning, except it wastes
    some of the computation budget. If the RL agent is trained in a real environment,
    as it usually is in a setting such as news recommendation, then exploration will
    have consequences beyond computational inefficiency and could harm user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about the common ![](img/Formula_05_272.png)-greedy approach, during
    exploration, it takes an action uniformly at random over the entire action space,
    even though we know that some of the actions are really bad. For example, even
    though the agent knows that a reader is mainly interested in politics, it will
    randomly display news pieces about beauty, sports, celebrities, and so on, which
    the reader might find totally irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of overcoming this issue is to incrementally deviate from the greedy
    action to explore and update the policy if the agent receives a good reward after
    an exploratory action. Here is how we could do it:'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the regular ![](img/Formula_16_002.png) network, we use an **explore
    network** ![](img/Formula_16_003.png) to generate exploratory actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of ![](img/Formula_16_004.png), denoted by ![](img/Formula_16_005.png),
    is obtained by perturbing the weights of ![](img/Formula_16_006.png), denoted
    by ![](img/Formula_16_007.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More formally, ![](img/Formula_16_008.png), where ![](img/Formula_16_009.png)
    is some coefficient to control the trade-off between exploration and exploitation,
    and ![](img/Formula_16_010.png) generates a random number between the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To obtain a set of news pieces to display to a user, we first generate two sets
    of recommendations from ![](img/Formula_16_011.png) and ![](img/Formula_16_012.png),
    one from each, and then randomly select pieces from both to add to the display
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the display set is shown to the user, the agent collects the user feedback
    (reward). If the feedback for the items generated by ![](img/Formula_16_013.png)
    is better, ![](img/Formula_16_014.png) does not change. Otherwise, the weights
    are updated ![](img/Formula_16_015.png), where ![](img/Formula_16_016.png) is
    some step size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, let's discuss how this model can be trained and deployed to get effective
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already mentioned the dynamic characteristics of user behavior and preferences.
    Zheng et al. (2018) overcome this by training and updating the model frequently
    throughout the day, so that the model captures the latest dynamics in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on personalization applications of RL, particularly
    in a news recommendation setting. Next, we shift gears towards a related area,
    marketing, which can also benefit from personalization, and do more using RL.
  prefs: []
  type: TYPE_NORMAL
- en: Developing effective marketing strategies using RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL can significantly improve marketing strategies in multiple areas. Let's now
    talk about some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized marketing content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In relation to the previous section, there is always room for more personalization
    in marketing. Rather than sending the same email or flier to all customers, or
    having rough customer segmentation, RL can help to determine the best sequence
    of personalized marketing content to communicate to customers.
  prefs: []
  type: TYPE_NORMAL
- en: Marketing resource allocation for customer acquisition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Marketing departments often make decisions about where to spend the budget based
    on subjective judgment and/or simple models. RL can actually come up with pretty
    dynamic policies to allocate marketing resources while leveraging the information
    about the product, responses from different marketing channels, and context information
    such as the time of year and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the customer churn rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retailers have long studied predictive models to identify customers that are
    about to be lost. After they are identified, usually, discount coupons, promotion
    items, and so on are sent to the customer. But the sequence of taking such actions,
    given the type of the customer and the responses from the earlier actions, is
    underexplored. RL can effectively evaluate the value of each of these actions
    and reduce customer churn.
  prefs: []
  type: TYPE_NORMAL
- en: Winning back lost customers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you ever subscribed to The Economist magazine, and then made the mistake
    of quitting your subscription, you probably received many phone calls, mail, emails,
    notifications, and so on. One cannot help but wonder whether such spamming is
    the best approach. It is probably not. An RL-based approach, on the other hand,
    can help determine which channels to use and when, along with the accompanying
    perks of maximizing the chances of winning the customers back while minimizing
    the cost of these efforts.
  prefs: []
  type: TYPE_NORMAL
- en: This list could go on. I suggest you take a moment or two to think about what
    marketing behavior of the companies you are a customer of you find disturbing,
    ineffective, irrelevant, and how RL could help with that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, our final area for this chapter: finance.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying RL in finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we need to reiterate RL''s promise, it is to obtain policies for sequential
    decision making to maximize rewards under uncertainty. What is a better match
    than finance for such a tool! In finance, the following are true:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is very much to maximize some monetary reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decisions made now will definitely have consequences down the road.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty is a defining factor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, RL is getting increasingly popular in the finance community.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear, this section will not include any examples of a winning trading
    strategy, well, for obvious reasons: First, the author does not know any; second,
    even if he did, he would not include them in a book (and no one would). In addition,
    there are challenges when it comes to using RL in finance. So, we start this section
    with a discussion on these challenges. Once we ground ourselves in reality, we
    will proceed to define some application areas and introduce some tools you can
    use in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with using RL in finance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How much time did it take for you to get a decent score in a video game when
    you first played it? An hour? Two? In any case, it is a tiny fraction of the experience
    that an RL agent would need, millions of game frames, if not billions, to reach
    that level. This is because the gradients an RL agent obtains in such environments
    are too noisy to quickly learn from for the algorithms we are using. And video
    games are not that difficult after all.
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever wondered what it takes to become a successful trader on the stock
    market? Years of financial experience, perhaps a Ph.D. in physics or math? Even
    then, it is difficult for a trader to beat market performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was perhaps too long of a statement to convince you that it is difficult
    to make money by trading equities for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Financial markets are highly efficient (although not perfectly), and it is very
    difficult, if not practically impossible, to predict the market. In other words,
    the **signal** is very weak in the financial data, and it is almost completely
    **noise**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial markets are dynamic. A trading strategy that is profitable today may
    not last long as others may discover the same strategy and trade on it. For example,
    if people knew that Bitcoin would trade at $100K at the first solar eclipse, the
    price would jump to that level now as people would not wait until that day to
    buy it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike video games, financial markets are real-word processes. So, we need to
    create a simulation of them to be able to collect the huge amounts of data needed
    to train an RL agent for an environment much noisier than a video game. Remember
    that all simulations are imperfect, and what the agent learns is likely to be
    the quirks of the simulation model rather than a real signal, which won't be useful
    outside of the simulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real-world data is big enough to easily detect a low signal in it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this was a long, discouraging, yet necessary disclaimer we needed to put
    out there to let you know that it is far from low-hanging-fruit to train a trader
    agent. Having said that, RL is a tool, a powerful one, and its effective use is
    up to the capabilities of its user.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to induce some optimism. There are some cool open source libraries
    out there for you to work towards creating your trader agent, and TensorTrade
    is one candidate. It could be helpful for educational purposes and to strategize
    some trading ideas before going into some more custom tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorTrade
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorTrade is designed to easily compose Gym-like environments for trading
    equities. It allows the user to define and glue together various kinds of data
    streams, feature extractions for observations, action spaces, reward structures,
    and other convenient utilities you might need to train a trader agent. Since the
    environment follows the Gym API, it can be easily hooked up with RL libraries,
    such as RLlib.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we give a quick introduction to TensorTrade and defer the details
    of what you can do with it to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: You can find the TensorTrade documentation at [https://www.tensortrade.org/](https://www.tensortrade.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with installation and then put together some TensorTrade components
    to create an environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorTrade
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorTrade can be installed with a simple `pip` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to create a virtual environment to install TensorTrade in,
    don't forget to also install Ray RLlib.
  prefs: []
  type: TYPE_NORMAL
- en: TensorTrade concepts and components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned, TensorTrade environments can be composed of highly modular
    components. Let's put together a basic environment here.
  prefs: []
  type: TYPE_NORMAL
- en: Instrument
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Instruments in finance are assets that can be traded. We can define the `U.S.
    Dollar` and `TensorTrade Coin` instruments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding integer arguments represent the precision of the quantities for
    those instruments.
  prefs: []
  type: TYPE_NORMAL
- en: Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A stream simply refers to an object that streams data, such as price data from
    a market simulation. For example, we can create a simple sinusoidal USD-TTC price
    stream as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Exchange and data feed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, we need to create an exchange to trade these instruments in. We put the
    price stream we just created in the exchange:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a price stream defined, we can also define transformations
    for it and extract some features and indicators. All such features will also be
    streams and they will be all packed in a data feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you are wondering what the last line is doing, it is the log ratio of prices
    in two consecutive time steps to assess the relative change.
  prefs: []
  type: TYPE_NORMAL
- en: Wallet and portfolio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, it''s time to create some wealth for ourselves and put it in our wallets.
    Feel free to be generous to yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our wallets together make up our portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Reward scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The reward scheme is simply the kind of reward function we want to incentivize
    our agent with. If you are thinking "there is only one goal, make profit!", well,
    there is more to it. You can use things like risk-adjusted returns or define your
    own goal. For now, let''s keep things simple and use the profit as the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Action scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The action scheme defines the type of actions you want your agent to be able
    to take, such as a simple **buy**/**sell**/**hold** all assets (**BSH**), or fractional
    buys/sells and so on. We also put the cash and assets in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Putting them all together in an environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, these can be all put together to form an environment with some additional
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, that's the basics of creating an environment in TensorTrade. You can of
    course go much beyond this, but you get the idea. After this step, it can be easily
    plugged into RLlib, or virtually any library compatible with the Gym API.
  prefs: []
  type: TYPE_NORMAL
- en: Training RLlib agents in TensorTrade
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you may remember from the earlier chapters, one way of using custom environments
    in Gym is to put them in a function that returns the environment and register
    them. So, it looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The environment name can then be referred to inside the trainer config in RLlib.
    You can find the full code in `Chapter16/tt_example.py` on the GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: This example mostly follows what is in the TensorTrade documentation. For a
    more detailed Ray/RLlib tutorial, you can visit [https://www.tensortrade.org/en/latest/tutorials/ray.html](https://www.tensortrade.org/en/latest/tutorials/ray.html).
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on TensorTrade. You can now go ahead and try a
    few trading ideas. When you are back, let's briefly talk about developing machine
    learning-based trading strategies and wrap up the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Developing equity trading strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What is almost as noisy as the market itself is the information about how to
    trade them. How to develop effective trading strategies is well beyond the scope
    of our book. However, here is a blog post that can give you get a realistic idea
    about what to pay attention to when developing ML models for trading. As always,
    use your judgment and due diligence to decide on what to believe: [https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/](https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/).'
  prefs: []
  type: TYPE_NORMAL
- en: With that, let's wrap up this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered three important RL application areas: personalization,
    marketing, and finance. For personalization and marketing, this chapter went beyond
    the bandit applications that are commonly used in these areas and discussed the
    merits of multi-step RL. We also covered methods such as dueling bandit gradient
    descent, which helped us achieve more conservative exploration to avoid excessive
    reward losses, and action embeddings, which is helpful to deal with large action
    spaces. We concluded the chapter with a discussion on financial applications of
    RL, their challenges, and introduced the TensorTrade library.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the last application chapter of the book, in which we will
    focus on smart cities and cybersecurity.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zheng, G. et al. (2018). *DRN: A Deep RL Framework for News Recommendation*.
    WWW ''18: Proceedings of the 2018 World Wide Web Conference April 2018, Pages
    167–176, [https://doi.org/10.1145/3178876.3185994](https://doi.org/10.1145/3178876.3185994).'
  prefs: []
  type: TYPE_NORMAL
