["```py\nnumpy.random.normal(loc,scale,size)\n```", "```py\nnumpy.arange(start,end,spacing)\n```", "```py\n 1, 1.1,1.2 …….. \n```", "```py\nnumpy.sine()\n```", "```py\n    # Importing the necessary library packages\n    import numpy as np\n    ```", "```py\n    \"\"\"\n    Generating a random number from a normal distribution \n    with mean 3 and sd = 1\n    \"\"\"\n    np.random.seed(123)\n    loc = np.random.normal(3,1,1)\n    loc\n    array([1.9143694])\n    ```", "```py\n    # Generate numbers to right of the mid point\n    xr = np.arange(loc,loc+(0.1*64),0.1)\n    ```", "```py\n    # Generate numbers to left of the random point\n    xl = np.arange(loc-(0.1*64),loc,0.1)\n    ```", "```py\n    # Concatenating both these numbers \n    X1 = np.concatenate((xl,xr))\n    print(X1)\n    ```", "```py\n    # Generate second feature\n    X2 = np.sin(X1)\n    ```", "```py\n    # Plot the distribution \n    import matplotlib.pyplot as plot\n    plot.plot(X1, X2)\n    plot.xlabel('Data Distribution')\n    plot.ylabel('Sine of data distribution')\n    plot.show()\n    ```", "```py\n    # Reshaping the individual data sets\n    X1 = X1.reshape(128,1)\n    X2 = X2.reshape(128,1)\n    ```", "```py\n    # Concatenate both features to form the real data set\n    realData = np.concatenate((X1,X2),axis=1)\n    realData.shape\n    ```", "```py\n    (128, 2)\n    ```", "```py\nfrom tensorflow.keras import Sequential\nGenmodel= Sequential()\n```", "```py\nfrom tensorflow.keras.layers import Dense \nGenmodel.add(Dense(hidden_layer,activation,\\\n                   kernel_initializer,input_dim))\nGenmodel.add(Dense(hidden_layer,activation,kernel_initializer))\n```", "```py\n    # Importing the library packages\n    import tensorflow as tf\n    import numpy as np\n    from numpy.random import randn\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense\n    from matplotlib import pyplot\n    ```", "```py\n    # Define the input features and output features\n    infeats = 10\n    outfeats = 2\n    ```", "```py\n    # Generate a batch of random numbers\n    batch = 128\n    genInput = randn(infeats * batch)\n    ```", "```py\n    # Reshape the data \n    genInput = genInput.reshape(batch,infeats)\n    print(genInput.shape)\n    ```", "```py\n    (128, 10)\n    ```", "```py\n    # Defining the Generator model\n    Genmodel = Sequential()\n    Genmodel.add(Dense(32,activation = 'linear',\\\n                       kernel_initializer='he_uniform',\\\n                       input_dim=infeats))\n    Genmodel.add(Dense(32,activation = 'relu',\\\n                       kernel_initializer='he_uniform'))\n    Genmodel.add(Dense(64,activation = 'elu',\\\n                       kernel_initializer='he_uniform'))\n    Genmodel.add(Dense(32,activation = 'elu',\\\n                       kernel_initializer='he_uniform'))\n    Genmodel.add(Dense(32,activation = 'selu',\\\n                       kernel_initializer='he_uniform'))\n    Genmodel.add(Dense(outfeats,activation = 'selu'))\n    ```", "```py\n    # Defining the summary of the network\n    Genmodel.summary()\n    ```", "```py\n    # Generating fake samples from network\n    fakeSamps = Genmodel.predict(genInput)\n    fakeSamps.shape\n    ```", "```py\n    (128, 2)\n    ```", "```py\n    # Plotting the fake distribution\n    from matplotlib import pyplot\n    pyplot.scatter(fakeSamps[:,0],fakeSamps[:,1])\n    pyplot.xlabel('Feature 1 of the distribution')\n    pyplot.ylabel('Feature 2 of the distribution')\n    pyplot.show()\n    ```", "```py\nmodel.compile(loss='binary_crossentropy',\\\n              optimizer='adam',metrics=['accuracy'])\n```", "```py\n    # Import the required library functions\n    import tensorflow as tf\n    import numpy as np\n    from numpy.random import randn\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense\n    from matplotlib import pyplot\n    ```", "```py\n    Exercise7.03.ipynb\n    # Function to generate real samples\n    def realData(loc,batch):\n        \"\"\"\n        loc is the random location or mean around which samples are centred\n        \"\"\"\n        \"\"\"\n        Generate numbers to right of the random point\n        \"\"\"\n        xr = np.arange(loc,loc+(0.1*batch/2),0.1)\n        xr = xr[0:int(batch/2)]\n        \"\"\"\n        Generate numbers to left of the random point\n        \"\"\"\n        xl = np.arange(loc-(0.1*batch/2),loc,0.1)\n    The complete code for this step can be found at https://packt.live/3fe02j3.\n    ```", "```py\n    # Function to generate inputs for generator function\n    def fakeInputs(batch,infeats):\n        \"\"\"\n        Sample data points equal to (batch x input feature size) \n        from a random distribution\n        \"\"\"\n        genInput = randn(infeats * batch)\n        # Reshape the input\n        X = genInput.reshape(batch ,infeats)\n        return X\n    ```", "```py\n    # Function for the generator model\n    def genModel(infeats,outfeats):\n        #Defining the Generator model\n        Genmodel = Sequential()\n        Genmodel.add(Dense(32,activation = 'linear',\\\n                           kernel_initializer='he_uniform',\\\n                           input_dim=infeats))\n        Genmodel.add(Dense(32,activation = 'relu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(64,activation = 'elu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(32,activation = 'elu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(32,activation = 'selu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(outfeats,activation = 'selu'))\n        return Genmodel\n    ```", "```py\n    # Function to create fake samples using the generator model\n    def fakedataGenerator(Genmodel,batch,infeats):\n        # first generate the inputs to the model\n        genInputs = fakeInputs(batch,infeats)\n        \"\"\"\n        use these inputs inside the generator model \n        to generate fake distribution\n        \"\"\"\n        X_fake = Genmodel.predict(genInputs)\n        # Generate the labels of fake data set\n        y_fake = np.zeros((batch,1))\n        return X_fake,y_fake\n    ```", "```py\n    \"\"\"\n    Define the arguments like batch size,input feature size \n    and output feature size\n    \"\"\"\n    batch = 128\n    infeats = 10\n    outfeats = 2\n    ```", "```py\n    # Define the discriminator model\n    Discmodel = Sequential()\n    Discmodel.add(Dense(16, activation='relu',\\\n                        kernel_initializer = 'he_uniform',\\\n                        input_dim=outfeats))\n    Discmodel.add(Dense(16,activation='relu' ,\\\n                        kernel_initializer = 'he_uniform'))\n    Discmodel.add(Dense(16,activation='relu' ,\\\n                        kernel_initializer = 'he_uniform'))\n    Discmodel.add(Dense(1,activation='sigmoid'))\n    # Compiling the model\n    Discmodel.compile(loss='binary_crossentropy',\\\n                      optimizer='adam', metrics=['accuracy'])\n    ```", "```py\n    # Print the summary of the discriminator model\n    Discmodel.summary()\n    ```", "```py\n    # Calling the Generator model function\n    Genmodel = genModel(infeats,outfeats)\n    Genmodel.summary()\n    ```", "```py\n    # Defining the number of epochs\n    nEpochs = 20000\n    ```", "```py\nExercise7.03.ipynb\n# Train the discriminator network\nfor i in range(nEpochs):\n    # Generate the random number for generating real samples\n    loc = np.random.normal(3,1,1)\n    \"\"\"\n    Generate samples equal to the bath size \n    from the real distribution\n    \"\"\"\n    x_real, y_real = realData(loc,batch)\n    #Generate fake samples using the fake data generator function\n    x_fake, y_fake = fakedataGenerator(Genmodel,batch,infeats)\nThe complete code for this step can be found at https://packt.live/3fe02j3.\n```", "```py\nReal accuracy:0.265625,Fake accuracy:0.59375\nReal accuracy:1.0,Fake accuracy:0.828125\nReal accuracy:1.0,Fake accuracy:0.90625\nReal accuracy:1.0,Fake accuracy:0.9453125\nReal accuracy:1.0,Fake accuracy:0.9453125\n```", "```py\nGanmodel = Sequential()\n# First adding the generator model\nGanmodel.add(Genmodel)\n\"\"\"\nNext adding the discriminator model \nwithout training the parameters\n\"\"\"\nGanmodel.add(Discmodel)\n# Compile the model for loss to optimise the Generator model\nGanmodel.compile(loss='binary_crossentropy',optimizer = 'adam')\n```", "```py\n# First define that discriminator model cannot be trained\nDiscmodel.trainable = False\nGanmodel = Sequential()\n# First adding the generator model\nGanmodel.add(Genmodel)\n\"\"\"\nNext adding the discriminator model \nwithout training the parameters\n\"\"\"\nGanmodel.add(Discmodel)\n# Compile the model for loss to optimise the Generator model\nGanmodel.compile(loss='binary_crossentropy',optimizer = 'adam')\n```", "```py\n    # Import the required library functions\n    import tensorflow as tf\n    import numpy as np\n    from numpy.random import randn\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense\n    from matplotlib import pyplot\n    ```", "```py\n    Exercise7.04.ipynb\n    # Function to generate real samples\n    def realData(loc,batch):\n        \"\"\"\n        loc is the random location or mean \n        around which samples are centred\n        \"\"\"\n        # Generate numbers to right of the random point\n        xr = np.arange(loc,loc+(0.1*batch/2),0.1)\n        xr = xr[0:int(batch/2)]\n        # Generate numbers to left of the random point\n        xl = np.arange(loc-(0.1*batch/2),loc,0.1)\n    The complete code for this step can be found on https://packt.live/3iIJHVS\n    ```", "```py\n    # Function to generate inputs for generator function\n    def fakeInputs(batch,infeats):\n    \"\"\"\n        Sample data points equal to (batch x input feature size)\n     from a random distribution\n        \"\"\"\n        genInput = randn(infeats * batch)\n        # Reshape the input\n        X = genInput.reshape(batch ,infeats)\n        return X\n    ```", "```py\n    # Function for the generator model\n    def genModel(infeats,outfeats):\n        # Defining the Generator model\n        Genmodel = Sequential()\n        Genmodel.add(Dense(32,activation = 'linear',\\\n                           kernel_initializer='he_uniform',\\\n                           input_dim=infeats))\n        Genmodel.add(Dense(32,activation = 'relu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(64,activation = 'elu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(32,activation = 'elu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(32,activation = 'selu',\\\n                           kernel_initializer='he_uniform'))\n        Genmodel.add(Dense(outfeats,activation = 'selu'))\n        return Genmodel\n    ```", "```py\n    # Function to create fake samples using the generator model\n    def fakedataGenerator(Genmodel,batch,infeats):\n        # first generate the inputs to the model\n        genInputs = fakeInputs(batch,infeats)\n        \"\"\"\n        use these inputs inside the generator model \n        to generate fake distribution\n        \"\"\"\n        X_fake = Genmodel.predict(genInputs)\n        # Generate the labels of fake data set\n        y_fake = np.zeros((batch,1))\n        return X_fake,y_fake\n    ```", "```py\n    \"\"\"\n    Define the arguments like batch size,input feature size \n    and output feature size\n    \"\"\"\n    batch = 128\n    infeats = 10\n    outfeats = 2\n    ```", "```py\n    # Discriminator model as a function\n    def discModel(outfeats):\n        Discmodel = Sequential()\n        Discmodel.add(Dense(16, activation='relu',\\\n                            kernel_initializer = 'he_uniform',\\\n                            input_dim=outfeats))\n        Discmodel.add(Dense(16,activation='relu' ,\\\n                            kernel_initializer = 'he_uniform'))\n        Discmodel.add(Dense(16,activation='relu' ,\\\n                            kernel_initializer = 'he_uniform'))\n        Discmodel.add(Dense(1,activation='sigmoid'))\n        # Compiling the model\n        Discmodel.compile(loss='binary_crossentropy',\\\n                          optimizer='adam',metrics=['accuracy'])\n        return Discmodel\n    ```", "```py\n    # Print the summary of the discriminator model\n    Discmodel = discModel(outfeats)\n    Discmodel.summary()\n    ```", "```py\n    # Calling the Generator model function\n    Genmodel = genModel(infeats,outfeats)\n    Genmodel.summary()\n    ```", "```py\n    # Let us visualize the initial fake data\n    x_fake, _ = fakedataGenerator(Genmodel,batch,infeats)\n    # Plotting the fake data using pyplot\n    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n    # Adding x and y labels\n    pyplot.xlabel('Feature 1 of the distribution')\n    pyplot.ylabel('Feature 2 of the distribution')\n    pyplot.show()\n    ```", "```py\n    \"\"\"\n    Define the combined generator and discriminator model, \n    for updating the generator\n    \"\"\"\n    def ganModel(Genmodel,Discmodel):\n        # First define that discriminator model cannot be trained\n        Discmodel.trainable = False\n        Ganmodel = Sequential()\n        # First adding the generator model\n        Ganmodel.add(Genmodel)\n        \"\"\"\n        Next adding the discriminator model \n        without training the parameters\n        \"\"\"\n        Ganmodel.add(Discmodel)\n        # Compile the model for loss to optimise the Generator model\n        Ganmodel.compile(loss='binary_crossentropy',optimizer = 'adam')\n        return Ganmodel\n    ```", "```py\n    # Initialise the gan model\n    gan_model = ganModel(Genmodel,Discmodel)\n    ```", "```py\n    # Print summary of the GAN model\n    gan_model.summary()\n    ```", "```py\n    # Defining the number of epochs\n    nEpochs = 20000\n    ```", "```py\nExercise7.04.ipynb\n# Train the GAN network\nfor i in range(nEpochs):\n    # Generate the random number for generating real samples\n    loc = np.random.normal(3,1,1)\n    \"\"\"\n    Generate samples equal to the bath size \n    from the real distribution\n    \"\"\"\n    x_real, y_real = realData(loc,batch)\n    #Generate fake samples using the fake data generator function\n    x_fake, y_fake = fakedataGenerator(Genmodel,batch,infeats)\n    # train the  discriminator on the real samples\n    Discmodel.train_on_batch(x_real, y_real)\n    # train the discriminator on the fake samples\n    Discmodel.train_on_batch(x_fake, y_fake)\nThe complete code for this step can be found at https://packt.live/3iIJHVS\n```", "```py\nReal accuracy:0.2421875,Fake accuracy:0.0234375\nReal accuracy:0.625,Fake accuracy:0.609375\nReal accuracy:0.6484375,Fake accuracy:0.9609375\nReal accuracy:0.84375,Fake accuracy:0.734375\nReal accuracy:0.3671875,Fake accuracy:0.734375\nReal accuracy:0.53125,Fake accuracy:0.703125\nReal accuracy:0.578125,Fake accuracy:0.640625\nReal accuracy:0.640625,Fake accuracy:0.8203125\nReal accuracy:0.515625,Fake accuracy:0.7109375\nReal accuracy:0.5625,Fake accuracy:0.859375\n```", "```py\nfrom tensorflow.keras import Sequential\nmodel = Sequential()\nmodel.add(Conv2D(64, kernel_size=(5, 5),\\\n                 strides=(2,2), padding='same'))\n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import UpSampling2D\n```", "```py\n# A model for UpSampling2d\nmodel = Sequential()\nmodel.add(UpSampling2D(input_shape=(3,3,1)))\nmodel.summary()\n```", "```py\n# Defining an array of shape (3,3)\nimport numpy as np\nX = np.array([[1,2,3],[4,5,6],[7,8,9]])\nX.shape\n```", "```py\n(3, 3)\n```", "```py\n# Reshaping the array\nX = X.reshape((1,3,3,1))\nX.shape\n```", "```py\n(1, 3, 3, 1)\n```", "```py\n# Predicting with the model\ny = model.predict(X)\n# Printing the output shape\ny[0,:,:,0]\n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2DTranspose\n```", "```py\n# A model for transpose convolution\nmodel = Sequential()\nmodel.add(Conv2DTranspose(1,(4,4),(2,2),\\\n          input_shape=(3,3,1),padding='same'))\nmodel.summary()\n```", "```py\n# Defining an array of shape (3,3)\nX = np.array([[1,2,3],[4,5,6],[7,8,9]])\nX = X.reshape((1,3,3,1))\nX.shape\n```", "```py\n(1, 3, 3, 1)\n```", "```py\n # Predicting with the model\ny = model.predict(X)\n# Printing the shape\nprint(y.shape)\n# Printing the output shape\ny[0,:,:,0]\n```", "```py\n# Get the MNIST data \n    (X_train, _), (_, _) = mnist.load_data()\n```", "```py\n# Reshaping the input data to include channel\n    X = X_train[:,:,:,np.newaxis]\n# Generating a batch of data\n    imageBatch = X[np.random.randint(0, X.shape[0], size=batch)]\n```", "```py\n# Generating a batch of data\n    imageBatch = X[np.random.randint(0, X.shape[0], size=batch)]\n```", "```py\n    # Import the required library functions\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import pyplot\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input\n    from tensorflow.keras.initializers import RandomNormal\n    from tensorflow.keras.models import Model, Sequential\n    from tensorflow.keras.layers \\\n    import Reshape, Dense, Dropout, Flatten,Activation\n    from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n    from tensorflow.keras.layers \\\n    import Conv2D, UpSampling2D,Conv2DTranspose\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.optimizers import Adam \n    ```", "```py\n    # Function to generate real data samples\n    def realData(batch):\n        # Get the MNIST data \n        (X_train, _), (_, _) = mnist.load_data()\n        # Reshaping the input data to include channel\n        X = X_train[:,:,:,np.newaxis]\n        # normalising the data\n        X = (X.astype('float32') - 127.5)/127.5\n        # Generating a batch of data\n        imageBatch = X[np.random.randint(0, X.shape[0], size=batch)]\n        return imageBatch\n    ```", "```py\n    # # Generating a batch of images\n    mnistData = realData(25) \n    ```", "```py\n    # Plotting the image\n    for j in range(5*5):\n        pyplot.subplot(5,5,j+1)\n        # turn off axis \n        pyplot.axis('off') \n        pyplot.imshow(mnistData[j,:,:,0],cmap='gray_r')\n    ```", "```py\n    # Function to generate inputs for generator function\n    def fakeInputs(batch,infeats):\n        #Generate random noise data with shape (batch,input features)\n        x_fake = np.random.uniform(-1,1,size=[batch,infeats])\n        return x_fake\n    ```", "```py\n    Exercise7.05.ipynb\n    # Function for the generator model\n    def genModel(infeats):\n        # Defining the Generator model\n        Genmodel = Sequential()\n        Genmodel.add(Dense(512,input_dim=infeats))\n        Genmodel.add(Activation('relu'))\n        Genmodel.add(BatchNormalization())\n        # second layer of FC => RElu => BN layers\n        Genmodel.add(Dense(7*7*64))\n        Genmodel.add(Activation('relu'))\n    The complete code for this step can be found on https://packt.live/2ZPg8cJ.\n    ```", "```py\n    # Function to create fake samples using the generator model\n    def fakedataGenerator(Genmodel,batch,infeats):\n        # first generate the inputs to the model\n        genInputs = fakeInputs(batch,infeats)\n        \"\"\"\n        use these inputs inside the generator model \n        to generate fake distribution\n        \"\"\"\n        X_fake = Genmodel.predict(genInputs)   \n\n        return X_fake\n    ```", "```py\n    # Define the arguments like batch size and input feature\n    batch = 128\n    infeats = 100\n    Genmodel = genModel(infeats)\n    Genmodel.summary()\n    ```", "```py\n    # Generating a fake sample and printing the shape\n    fake = fakedataGenerator(Genmodel,batch,infeats)\n    fake.shape\n    ```", "```py\n    (128, 28, 28, 1)\n    ```", "```py\n    # Plotting the fake sample\n    plt.imshow(fake[1, :, :, 0], cmap='gray_r')\n    plt.xlabel('Fake Sample Image')\n    ```", "```py\n    Exercise7.05.ipynb\n    # Descriminator model as a function\n    def discModel():\n        Discmodel = Sequential()\n        Discmodel.add(Conv2D(32,kernel_size=(5,5),strides=(2,2), \\\n                             padding='same',input_shape=(28,28,1)))\n        Discmodel.add(LeakyReLU(0.2))\n        # second layer of convolutions\n        Discmodel.add(Conv2D(64, kernel_size=(5,5), \\\n                             strides=(2, 2), padding='same'))\n    The complete code for this step can be found on https://packt.live/2ZPg8cJ.\n    ```", "```py\n    # Print the summary of the discriminator model\n    Discmodel = discModel()\n    Discmodel.summary()\n    ```", "```py\n    \"\"\"\n    Define the combined generator and discriminator model, \n    for updating the generator\n    \"\"\"\n    def ganModel(Genmodel,Discmodel):\n        # First define that discriminator model cannot be trained\n        Discmodel.trainable = False\n        Ganmodel = Sequential()\n        # First adding the generator model\n        Ganmodel.add(Genmodel)\n        \"\"\"\n        Next adding the discriminator model \n        without training the parameters\n        \"\"\"\n        Ganmodel.add(Discmodel)\n        # Compile the model for loss to optimise the Generator model\n        Ganmodel.compile(loss='binary_crossentropy',\\\n                         optimizer = 'adam')\n        return Ganmodel\n    ```", "```py\n    # Initialise the gan model\n    gan_model = ganModel(Genmodel,Discmodel)\n    # Print summary of the GAN model\n    gan_model.summary()\n    ```", "```py\n    # Defining the number of epochs\n    nEpochs = 5000\n    ```", "```py\n    Exercise7.05.ipynb\n    # Train the GAN network\n    for i in range(nEpochs):\n        \"\"\"\n        Generate samples equal to the bath size \n        from the real distribution\n        \"\"\"\n        x_real = realData(batch)\n        #Generate fake samples using the fake data generator function\n        x_fake = fakedataGenerator(Genmodel,batch,infeats)\n        # Concatenating the real and fake data \n        X = np.concatenate([x_real,x_fake])\n        #Creating the dependent variable and initializing them as '0'\n        Y = np.zeros(batch * 2)\n    The full code for this step can be found at https://packt.live/2ZPg8cJ.\n    ```", "```py\n    Discriminator probability:0.6213402152061462\n    Discriminator probability:0.7360671758651733\n    Discriminator probability:0.6130768656730652\n    Discriminator probability:0.5046337842941284\n    Discriminator probability:0.5005484223365784\n    Discriminator probability:0.50015789270401\n    Discriminator probability:0.5000558495521545\n    Discriminator probability:0.5000174641609192\n    Discriminator probability:0.5000079274177551\n    Discriminator probability:0.4999823570251465\n    Discriminator probability:0.5000027418136597\n    Discriminator probability:0.5000032186508179\n    Discriminator probability:0.5000043511390686\n    Discriminator probability:0.5000077486038208\n    ```", "```py\n    # Images predicted after training\n    x_fake = fakedataGenerator(Genmodel,25,infeats)\n    # Visualizing the plots\n    for j in range(5*5):\n        pyplot.subplot(5,5,j+1)\n        # turn off axis \n        pyplot.axis('off')\n        pyplot.imshow(x_fake[j,:,:,0],cmap='gray_r')\n    ```", "```py\n    from tensorflow.keras.datasets import fashion_mnist\n    ```"]