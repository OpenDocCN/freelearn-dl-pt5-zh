["```py\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(knitr)\n```", "```py\nsteamdata <- read_csv(\"data/steam-200k.csv\", col_names=FALSE)\n```", "```py\nglimpse(steamdata)\n```", "```py\ncolnames(steamdata) <- c(\"user\", \"item\", \"interaction\", \"value\", \"blank\")\n```", "```py\nsteamdata <- steamdata %>% \n  filter(interaction == \"play\") %>%\n  select(-blank) %>%\n  select(-interaction) %>% \n  mutate(item = str_replace_all(item,'[ [:blank:][:space:] ]',\"\"))\n```", "```py\nusers <- steamdata %>% select(user) %>% distinct() %>% rowid_to_column()\nsteamdata <- steamdata %>% inner_join(users) %>% rename(userid=rowid)\n\nitems <- steamdata %>% select(item) %>% distinct() %>% rowid_to_column()\nsteamdata <- steamdata %>% inner_join(items) %>% rename(itemid=rowid)\n```", "```py\nsteamdata <- steamdata %>% rename(title=item, rating=value)\n```", "```py\nn_users <- steamdata %>% select(userid) %>% distinct() %>% nrow()\nn_items <- steamdata %>% select(itemid) %>% distinct() %>% nrow()\n```", "```py\n# normalize data with min-max function\nminmax <- function(x) {\n  return ((x - min(x)) / (max(x) - min(x)))\n}\n\n# add scaled rating value\nsteamdata <- steamdata %>% mutate(rating_scaled = minmax(rating))\n```", "```py\n# split into training and test\nindex <- sample(1:nrow(steamdata), 0.8* nrow(steamdata))\ntrain <- steamdata[index,] \ntest <- steamdata[-index,] \n```", "```py\n# create matrices of user, items, and ratings for training and test \nx_train <- train %>% select(c(userid, itemid)) %>% as.matrix()\ny_train <- train %>% select(rating_scaled) %>% as.matrix()\nx_test <- test %>% select(c(userid, itemid)) %>% as.matrix()\ny_test <- test %>% select(rating_scaled) %>% as.matrix()\n```", "```py\n# user-item interaction exploratory data analysis (EDA)\nitem_interactions <- aggregate(\n    rating ~ title, data = steamdata, FUN = 'sum')\nitem_interactions <- item_interactions[\n    order(item_interactions$rating, decreasing = TRUE),]\nitem_top10 <- head(item_interactions, 10)\nkable(item_top10)\n```", "```py\n# average gamplay\nsteamdata %>% summarise(avg_gameplay = mean(rating))\n\n# median gameplay\nsteamdata %>% summarise(median_gameplay = median(rating))\n\n# top game by individual hours played\ntopgame <- steamdata %>% arrange(desc(rating)) %>% top_n(1,rating)\n\n# show top game by individual hours played\nkable(topgame)\n```", "```py\n# top 10 games by hours played\nmostplayed <- \n  steamdata %>%\n  group_by(item) %>%\n  summarise(hours=sum(rating)) %>% \n  arrange(desc(hours)) %>%\n  top_n(10, hours) %>%\n  ungroup\n\n# show top 10 games by hours played\nkable(mostplayed)\n\n# reset factor levels for items\nmostplayed$item <- droplevels(mostplayed$item)\n\n# top 10 games by collective hours played\nggplot(mostplayed, aes(x=item, y=hours, fill = hours)) +\n  aes(x = fct_inorder(item)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(size=8, face=\"bold\", angle=90)) +\n  theme(axis.ticks = element_blank()) +\n  scale_y_continuous(expand = c(0,0), limits = c(0,1000000)) + \n  labs(title=\"Top 10 games by collective hours played\") +\n  xlab(\"game\") +\n  ylab(\"hours\")\n\n```", "```py\n# most popular games by total users\nmostusers <-\n  steamdata %>%\n  group_by(item) %>%\n  summarise(users=n()) %>% \n  arrange(desc(users)) %>% \n  top_n(10, users) %>% \n  ungroup\n\n# reset factor levels for items\nmostusers$item <- droplevels(mostusers$item)\n\n# top 10 popular games by total users\nggplot(mostusers, aes(x=item, y=users, fill = users)) +\n  aes(x = fct_inorder(item)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(size=8, face=\"bold\", angle=90)) +\n  theme(axis.ticks = element_blank()) +\n  scale_y_continuous(expand = c(0,0), limits = c(0,5000)) + \n  labs(title=\"Top 10 popular games by total users\") +\n  xlab(\"game\") +\n  ylab(\"users\")\n```", "```py\nsummary(steamdata$value)\n```", "```py\n# plot item iteraction\nggplot(steamdata, aes(x=steamdata$value)) +\n  geom_histogram(stat = \"bin\", binwidth=50, fill=\"steelblue\") +\n  theme(axis.ticks = element_blank()) +\n  scale_x_continuous(expand = c(0,0)) + \n  scale_y_continuous(expand = c(0,0), limits = c(0,60000)) + \n  labs(title=\"Item interaction distribution\") +\n  xlab(\"Hours played\") +\n  ylab(\"Count\")\n```", "```py\n# plot item iteraction with log transformation\nggplot(steamdata, aes(x=steamdata$value)) +\n  geom_histogram(stat = \"bin\", binwidth=0.25, fill=\"steelblue\") +\n  theme(axis.ticks = element_blank()) +\n  scale_x_log10() +\n  labs(title=\"Item interaction distribution with log transformation\") +\n  xlab(\"log(Hours played)\") +\n  ylab(\"Count\")\n```", "```py\n# create custom model with user and item embeddings\ndot <- function(\n  embedding_dim,\n  n_users,\n  n_items,\n  name = \"dot\"\n) {\n  keras_model_custom(name = name, function(self) {\n    self$user_embedding <- layer_embedding(\n        input_dim = n_users+1,\n        output_dim = embedding_dim,\n        name = \"user_embedding\")\n    self$item_embedding <- layer_embedding(\n        input_dim = n_items+1,\n        output_dim = embedding_dim,\n        name = \"item_embedding\")\n    self$dot <- layer_lambda(\n        f = function(x)\n        k_batch_dot(x[[1]],x[[2]],axes=2),\n        name = \"dot\"\n    )\n    function(x, mask=NULL, training=FALSE) {\n      users <- x[,1]\n      items <- x[,2]\n      user_embedding <- self$user_embedding(users) \n      item_embedding <- self$item_embedding(items) \n      dot <- self$dot(list(user_embedding, item_embedding))\n    }\n  })\n}\n```", "```py\n# initialize embedding parameter\nembedding_dim <- 50\n\n# define model \nmodel <- dot(\n  embedding_dim,\n  n_users,\n  n_items\n)\n```", "```py\n# compile model \nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n```", "```py\n# train model \nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 500,\n  validation_data = list(x_test,y_test),\n  verbose = 1\n)\n```", "```py\nsummary(model)\n```", "```py\n# evaluate model results\nplot(history)\n```", "```py\n# initialize embedding parameter\nembedding_dim <- 32\n\n# train model \nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 50,\n  validation_data = list(x_test,y_test),\n  verbose = 1)\n\n# show model\nsummary(model)\n```", "```py\n# evaluate results\nplot(history)\n```", "```py\n# initialize embedding parameter\nembedding_dim <- 64\n\n# create custom model with dropout layers\ndot_with_dropout <- function(\n  embedding_dim,\n  n_users,\n  n_items,\n  name = \"dot_with_dropout\"\n) {\n  keras_model_custom(name = name, function(self) {\n    self$user_embedding <- layer_embedding(\n      input_dim = n_users+1,\n      output_dim = embedding_dim,\n      name = \"user_embedding\")\n    self$item_embedding <- layer_embedding(\n      input_dim = n_items+1,\n      output_dim = embedding_dim,\n      name = \"item_embedding\")\n    self$user_dropout <- layer_dropout(\n        rate = 0.2)\n    self$item_dropout <- layer_dropout(\n        rate = 0.4)\n    self$dot <-\n      layer_lambda(\n        f = function(x)\n        k_batch_dot(x[[1]],x[[2]],axes=2),\n        name = \"dot\"\n      )\n    function(x, mask=NULL, training=FALSE) {\n      users <- x[,1]\n      items <- x[,2]\n      user_embedding <- self$user_embedding(users) %>%           \n          self$user_dropout()\n      item_embedding <- self$item_embedding(items) %>%     \n           self$item_dropout()\n      dot <- self$dot(list(user_embedding,item_embedding))\n    }\n  })\n}\n```", "```py\n# define model \nmodel <- dot_with_dropout(\n  embedding_dim,\n  n_users,\n  n_items)\n\n# compile model \nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\n# train model \nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 50,\n  validation_data = list(x_test,y_test),\n  verbose = 1\n)\n```", "```py\nsummary(model)\n```", "```py\n# evaluate results\nplot(history)\n```", "```py\n# caculate minimum and max rating\nmin_rating <- steamdata %>% summarise(min_rating = min(rating_scaled)) %>% pull()\nmax_rating <- steamdata %>% summarise(max_rating = max(rating_scaled)) %>% pull()\n\n# create custom model with user, item, and bias embeddings\ndot_with_bias <- function(\n  embedding_dim,\n  n_users,\n  n_items,\n  min_rating,\n  max_rating,\n  name = \"dot_with_bias\"\n) { \nkeras_model_custom(name = name, function(self) {\n  self$user_embedding <- layer_embedding(\n    input_dim = n_users+1,\n    output_dim = embedding_dim,\n    name = \"user_embedding\")\n  self$item_embedding <- layer_embedding(\n    input_dim = n_items+1,\n    output_dim = embedding_dim,\n    name = \"item_embedding\")\n  self$user_bias <- layer_embedding(\n    input_dim = n_users+1,\n    output_dim = 1,\n    name = \"user_bias\")\n  self$item_bias <- layer_embedding(\n    input_dim = n_items+1,\n    output_dim = 1,\n    name = \"item_bias\")\n```", "```py\n  self$user_dropout <- layer_dropout(\n    rate = 0.3)\n  self$item_dropout <- layer_dropout(\n    rate = 0.5)\n  self$dot <- layer_lambda(\n    f = function(x)\n    k_batch_dot(x[[1]],x[[2]],axes=2),\n    name = \"dot\")\n  self$dot_bias <- layer_lambda(\n    f = function(x)\n    k_sigmoid(x[[1]]+x[[2]]+x[[3]]),\n    name = \"dot_bias\")\n  self$min_rating <- min_rating\n  self$max_rating <- max_rating\n  self$pred <- layer_lambda(\n    f = function(x)\n    x * (self$max_rating - self$min_rating) + self$min_rating,\n    name = \"pred\")\n  function(x,mask=NULL,training=FALSE) {\n    users <- x[,1]\n    items <- x[,2]\n    user_embedding <- self$user_embedding(users) %>% self$user_dropout()\n    item_embedding <- self$item_embedding(items) %>% self$item_dropout()\n    dot <- self$dot(list(user_embedding,item_embedding))\n    dot_bias <- self$dot_bias(list(dot, self$user_bias(users), self$item_bias(items)))\n    self$pred(dot_bias)\n    }\n  })\n}\n```", "```py\n# define model \nmodel <- dot_with_bias(\n  embedding_dim,\n  n_users,\n  n_items,\n  min_rating,\n  max_rating)\n\n# compile model \nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\n# train model \nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 50,\n  validation_data = list(x_test,y_test),\n  verbose = 1)\n)\n```", "```py\n# summary model\nsummary(model)\n```", "```py\n# evaluate results\nplot(history)\n```"]