<html><head></head><body>
		<div>
			<div id="_idContainer177" class="Content">
			</div>
		</div>
		<div id="_idContainer178" class="Content">
			<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>9. Sequential Modeling with Recurrent Neural Networks</h1>
		</div>
		<div id="_idContainer202" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will introduce you to sequential modeling—creating models to predict the next value or series of values in a sequence. By the end of this chapter, you will be able to build sequential models, explain <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>), describe the vanishing gradient problem, and implement <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) architectures. You will apply <strong class="source-inline">RNNs</strong> with <strong class="source-inline">LSTM</strong> architectures to predict the value of the future stock price value of Alphabet and Amazon.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Introduction</h1>
			<p>In the previous chapter, we learned about pre-trained networks and how to utilize them for our own applications via transfer learning. We experimented with <strong class="source-inline">VGG16</strong> and <strong class="source-inline">ResNet50</strong>, two pre-trained networks that are used for image classification, and used them to classify new images and fine-tune them for our own applications. By utilizing pre-trained networks, we were able to train more accurate models quicker than the convolutional neural networks we trained in previous chapters.</p>
			<p>In traditional neural networks (and every neural network architecture covered in prior chapters), data passes sequentially through the network from the input layer, and through the hidden layers (if any), to the output layer. Information passes through the network once and the outputs are considered independent of each other, and only dependent on the inputs to the model. However, there are instances where a particular output is dependent on the previous output of the system.</p>
			<p>Consider the stock price of a company as an example: the output at the end of any given day is related to the output of the previous day. Similarly, in <strong class="bold">Natural Language Processing </strong>(<strong class="bold">NLP</strong>), the final words in a sentence are highly dependent on the previous words in the sentence if the sentence is to make grammatical sense. NLP is a specific application of sequential modeling in which the dataset being processed and analyzed is natural language data. A special type of neural network, called a <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>), is used to solve these types of problems where the network needs to remember previous outputs. </p>
			<p>This chapter introduces and explores the concepts and applications of RNNs. It also explains how RNNs are different from standard feedforward neural networks. You will also gain an understanding of the vanishing gradient problem and <strong class="bold">Long-Short-Term-Memory</strong> (<strong class="bold">LSTM</strong>) networks. This chapter also introduces you to sequential data and how it's processed. We will be working with share market data for stock price forecasting to learn all about these concepts.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>Sequential Memory and Sequential Modeling </h1>
			<p>If we analyze the stock price of <strong class="bold">Alphabet</strong> for the past 6 months, as shown in the following screenshot, we can see that there is a trend. To predict or forecast future stock prices, we need to gain an understanding of this trend and then do our mathematical computations while keeping this trend in mind:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B15777_09_01.jpg" alt="Figure 9.1: Alphabet's stock price over the last 6 months&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1: Alphabet's stock price over the last 6 months</p>
			<p>This trend is deeply related to sequential memory and sequential modeling. If you have a model that can remember the previous outputs and then predict the next output based on the previous outputs, we say that the model has sequential memory.</p>
			<p>The modeling that is done to process this sequential memory is known as <strong class="bold">sequential modeling</strong>. This is not only true for stock market data, but it is also true in NLP applications; we will look at one such example in the next section when we study RNNs.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor177"/>Recurrent Neural Networks (RNNs)</h1>
			<p><strong class="source-inline">RNNs</strong> are a class of neural networks that are built on the concept of sequential memory. Unlike traditional neural networks, an <strong class="source-inline">RNN</strong> predicts the results in sequential data. Currently, an <strong class="source-inline">RNN</strong> is the most robust technique that's available for processing sequential data.</p>
			<p>If you have access to a smartphone that has Google Assistant, try opening it and asking the question: "When was the United Nations formed?" The answer is displayed in the following screenshot:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B15777_09_02.jpg" alt="Figure 9.2: Google Assistant's output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2: Google Assistant's output</p>
			<p>Now, ask a second question, "Why was it formed?", as follows:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B15777_09_03.jpg" alt="Figure 9.3: Google Assistant's contextual output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3: Google Assistant's contextual output</p>
			<p>Now, ask the third question, "Where are its headquarters?", and you should get the following answer:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B15777_09_04.jpg" alt="Figure 9.4: Google Assistant's output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4: Google Assistant's output</p>
			<p>One interesting thing to note here is that we only mentioned the "United Nations" in the first question. In the second and third questions, we simply asked the assistant <strong class="source-inline">why it was formed</strong> and <strong class="source-inline">where the headquarters were</strong>, respectively. Google Assistant understood that since the previous question was about the United Nations, the next questions were also in the context of the United Nations. This is not a simple thing for a machine. </p>
			<p>The machine was able to show the expected result because it had processed data in the form of a sequence. The machine understands that the current question is related to the previous question, and so, essentially, it remembers the previous question.</p>
			<p>Let's consider another simple example. Say that we want to predict the next number in the following sequence: <strong class="source-inline">7</strong>, <strong class="source-inline">8</strong>, <strong class="source-inline">9</strong>, and <strong class="source-inline">?</strong>. We want the next output to be <strong class="source-inline">9</strong> + <strong class="source-inline">1</strong>. Alternatively, if we provide the sequence, <strong class="source-inline">3</strong>, <strong class="source-inline">6</strong>, <strong class="source-inline">9</strong>, and <strong class="source-inline">?</strong> we would like to get <strong class="source-inline">9</strong> + <strong class="source-inline">3</strong> as the output. While in both cases the last number is <strong class="source-inline">9</strong>, the prediction outcome should be different (that is, when we take into account the contextual information of the previous values and not only the last value). The key here is to remember the contextual information that was obtained from the previous values.</p>
			<p>At a high level, such networks that can remember previous states are referred to as recurrent networks. To completely understand <strong class="source-inline">RNNs</strong>, let's revisit the traditional neural networks, also known as <strong class="source-inline">feedforward neural networks</strong>. This is a neural network in which the connections of the neural network do not form cycles; that is, the data only flows in one direction, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B15777_09_05.jpg" alt="Figure 9.5: A feedforward neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5: A feedforward neural network</p>
			<p>In a <strong class="source-inline">feedforward neural network</strong>, such as the one shown in the preceding diagram, the input layer (the green circles on the left) gets the data and passes it to a hidden layer (with weights, illustrated by the blue circles in the middle). Later, the data from the hidden layer is passed to the output layer (illustrated by the red circle on the right). Based on the thresholds, the data is backpropagated, but there is no cyclical flow of data in the hidden layers.</p>
			<p>In an <strong class="source-inline">RNN</strong>, the hidden layer of the network allows the cycle of data and information. As shown in the following diagram, the architecture is similar to a feedforward neural network; however, here, the data and information also flow in cycles:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B15777_09_06.jpg" alt="Figure 9.6: An RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6: An RNN</p>
			<p>Here, the defining property of the <strong class="source-inline">RNN</strong> is that the hidden layer not only gives the output, but it also feeds back the information of the output into itself. Before taking a deep dive into RNNs, let's discuss why we need <strong class="source-inline">RNNs</strong> and why <strong class="source-inline">Convolutional Neural Networks</strong> (<strong class="source-inline">CNNs</strong>) or normal <strong class="source-inline">Artificial Neural Networks</strong> (<strong class="source-inline">ANNs</strong>) fall short when it comes to processing sequential data. Suppose that we are using a <strong class="source-inline">CNN</strong> to identify images; first, we input an image of a dog, and the <strong class="source-inline">CNN</strong> will label the image as "dog". Then, we input an image of a mango, and the CNN will label the image as "mango". Let's input the image of the dog at time <strong class="source-inline">t</strong>, as follows:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B15777_09_07.jpg" alt="Figure 9.7: An image of a dog with a CNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7: An image of a dog with a CNN</p>
			<p>Now, let's input the image of the mango at time <strong class="source-inline">t + 1</strong>, as follows:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B15777_09_08.jpg" alt="Figure 9.8: An image of a mango with a CNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8: An image of a mango with a CNN</p>
			<p>Here, you can clearly see that the output at time <strong class="source-inline">t</strong> for the dog image and the output at time <strong class="source-inline">t + 1</strong> for the mango image are totally independent of each other. Therefore, we don't need our algorithms to remember previous instances of the output. However, as we mentioned in the Google Assistant example where we asked <strong class="source-inline">when the United Nations was formed</strong> and <strong class="source-inline">why it was formed</strong>, the output of the previous instance has to be remembered by the algorithm for it to process the sequential data. <strong class="source-inline">CNNs</strong> or <strong class="source-inline">ANNs</strong> are not able to do this, so we need to use <strong class="source-inline">RNNs</strong> instead.</p>
			<p>In an <strong class="source-inline">RNN</strong>, we can have multiple outputs over multiple instances of time. The following diagram is a pictorial representation of an <strong class="source-inline">RNN</strong>. It represents the state of the network from time <strong class="source-inline">t – 1</strong> to time <strong class="source-inline">t + n</strong>:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B15777_09_09.jpg" alt="Figure 9.9: An unfolded RNN at various timestamps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9: An unfolded RNN at various timestamps</p>
			<p>There are some issues that you may face when training <strong class="source-inline">RNNs</strong> that are related to the unique architecture of <strong class="source-inline">RNNs</strong>. They concern the value of the gradient because, as the depth of the <strong class="source-inline">RNN</strong> increases, the gradient can either vanish or explode, as we will learn in the next section.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>The Vanishing Gradient Problem</h2>
			<p>If someone asks you "What did you have for dinner last night?", it is pretty easy to remember and correctly answer them. Now, if someone asks you "What did you have for dinner over the past 30 days?", then you might be able to remember the menu of the past 3 or 4 days, but then the menu for the days before that will be a bit difficult to remember. This ability to recall information from the past is the basis of the vanishing gradient problem, which we will be studying in this section. Put simply, the vanishing gradient problem refers to information that is lost or has decayed over a period of time. </p>
			<p>The following diagram represents the state of the <strong class="source-inline">RNN</strong> at different instances of time <strong class="source-inline">t</strong>. The top dots (in red) represent the output layer, the middle dots (in blue) represent the hidden layer, and the bottom dots (in green) represent the input layer:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B15777_09_10.jpg" alt="Figure: 9.10: Information decaying over time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure: 9.10: Information decaying over time</p>
			<p>If you are at <strong class="source-inline">t + 10</strong>, it will be difficult for you to remember what dinner menu you had at time <strong class="source-inline">t</strong> (which is 10 days prior to the current day). Additionally, if you are at <strong class="source-inline">t + 100</strong>, it is likely to be impossible for you to remember your dinner menu prior to 100 days, assuming that there is no pattern to the dinner you choose to make. In the context of machine learning, the vanishing gradient problem is a difficulty that is found when training ANNs using gradient-based learning methods and backpropagation. Let's recall how a neural network works, as follows:</p>
			<ol>
				<li>First, we initialize the network with random weights and bias values.</li>
				<li>We get a predicted output; this output is compared with the actual output and the difference is known as the cost.</li>
				<li>The training process utilizes a gradient, which measures the rate at which the cost changes with respect to the weights or biases.</li>
				<li>Then, we try to lower the cost by adjusting the weights and biases repeatedly throughout the training process, until the lowest possible value is obtained.</li>
			</ol>
			<p>For example, if you place a ball on a steep floor, then the ball will roll quickly; however, if you place the ball on a flat surface, it will roll slowly, or not at all. Similarly, in a deep neural network, the model learns quickly when the gradient is large. However, if the gradient is small, then the model's learning rate becomes very low. Remember that, at any point, the gradient is the product of all the gradients up to that point (that is, it follows the calculus chain rule). </p>
			<p>Additionally, the gradient is usually a small number between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, and the product of two numbers between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> gives you an even smaller number. The deeper your network is, the smaller the gradient is in the initial layers of the network. In some cases, it reaches a point that is so small that no training happens in that network; this is the vanishing gradient problem. The following diagram shows the gradients following the calculus chain rule:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B15777_09_11.jpg" alt="Figure 9.11: The vanishing gradient with cost, C, and the calculus chain rule&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11: The vanishing gradient with cost, C, and the calculus chain rule</p>
			<p>Referring to <em class="italic">Figure 9.10</em>, suppose that we are at the <strong class="source-inline">t + 10</strong> instance and we get an output that will be backpropagated to <strong class="source-inline">t</strong>, which is 10 steps away. Now, when the weight is updated, there will be 10 gradients (which are themselves very small), and when they multiply by each other, the number becomes so small that it is almost negligible. This is known as the vanishing gradient.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/>A Brief Explanation of the Exploding Gradient Problem</h2>
			<p>If instead of the weights being small, the weights are greater than <strong class="source-inline">1</strong>, then the subsequent multiplication will increase the gradient exponentially; this is known as the exploding gradient. The exploding gradient is simply the opposite of the vanishing gradient as in the case of the vanishing gradient, the values become too small, while in the case of the exploding gradient, the values become very large. As a result, the network suffers heavily and is unable to predict anything. We don't get the exploding gradient problem as frequently as vanishing gradients, but it is good to have a brief understanding of what exploding gradients are.</p>
			<p>There are some approaches we take to overcome the challenges that are faced with the vanishing or exploding gradient problem. The one approach that we will learn about is <strong class="source-inline">Long Short-Term Memory</strong>, which overcomes issues with the gradients by having memory about information for long periods of time.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/>Long Short-Term Memory (LSTM)</h1>
			<p><strong class="source-inline">LSTMs</strong> are <strong class="source-inline">RNNs</strong> whose main objective is to overcome the shortcomings of the vanishing gradient and exploding gradient problems. The architecture is built so that they remember data and information for a long period of time.</p>
			<p><strong class="source-inline">LSTMs</strong> were designed to overcome the limitation of the vanishing and exploding gradient problems. <strong class="source-inline">LSTM</strong> networks are a special kind of <strong class="source-inline">RNN</strong> that are capable of learning long-term dependencies. They are designed to avoid the long-term dependency problem; being able to remember information for long intervals of time is how they are wired. The following diagram displays a standard recurrent network where the repeating module has a <strong class="source-inline">tanh activation</strong> function. This is a simple <strong class="source-inline">RNN</strong>. In this architecture, we often have to face the vanishing gradient problem:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B15777_09_12.jpg" alt="Figure 9.12: A simple RNN model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12: A simple RNN model</p>
			<p>The <strong class="source-inline">LSTM</strong> architecture is similar to simple <strong class="source-inline">RNNs</strong>, but their repeating module has different components, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B15777_09_13.jpg" alt="Figure 9.13: The LSTM model architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13: The LSTM model architecture</p>
			<p>In addition to a simple <strong class="source-inline">RNN</strong>, an <strong class="source-inline">LSTM</strong> consists of the following:</p>
			<ul>
				<li><strong class="source-inline">Sigmoid activation</strong> functions (<strong class="source-inline">σ</strong>)</li>
				<li>Mathematical computational functions (the black circles with + and x)</li>
				<li>Gated cells (or gates):</li>
			</ul>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B15777_09_14.jpg" alt="Figure 9.14: An LSTM in detail&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14: An LSTM in detail</p>
			<p>The main difference between a simple <strong class="source-inline">RNN</strong> and an <strong class="source-inline">LSTM</strong> is the presence of gated cells. You can think of gates as computer memory, where information can be written, read, or stored. The preceding diagram shows a detailed image of an <strong class="source-inline">LSTM</strong>. The cells in the gates (represented by the black circles) make decisions on what to store and when to allow values to be read or written. The gates accept any information from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>; that is, if it is 0, then the information is blocked; if it is <strong class="source-inline">1</strong>, then all the information flows through. If the input is between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, then only partial information flows.</p>
			<p>Besides these input gates, the gradient of a network is dependent on two factors: weight and the activation function. The gates decide which piece of information needs to persist within the <strong class="source-inline">LSTM</strong> cell and which needs to be forgotten or deleted. In this way, the gates are like water valves; that is, the network can select which valve will allow the water to flow and which valve won't allow the water to flow.</p>
			<p>The valves are adjusted in such a way that the values of the output will never yield a gradient (vanishing or exploding) problem. For example, if the value becomes too large, then there is a forget gate that will forget the value and no longer consider it for computations. Essentially, what a forget gate does is multiply the information by <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. If the information needs to be processed further, then the forget gate multiplies the information by <strong class="source-inline">1</strong>, and if it needs to be forgotten, then it multiplies the information by <strong class="source-inline">0</strong>. Each gate is assisted by a sigmoid function that squashes the information between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. For us to gain a better understanding of this, let's take a look at some activities and exercises.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the activities and exercises in this chapter will be developed in Jupyter notebooks. You can download this book's GitHub repository, along with all the prepared templates, at <a href="https://packt.live/2vtdA8o">https://packt.live/2vtdA8o</a>.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor181"/>Exercise 9.01: Predicting the Trend of Alphabet's Stock Price Using an LSTM with 50 Units (Neurons)</h2>
			<p>In this exercise, we will examine the stock price of Alphabet over a period of 5 years—that is, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using <strong class="source-inline">RNNs</strong>. We have the actual values for January 2019, so we will be able to compare our predictions with the actual values later. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Import the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">dataset_training = pd.read_csv('../GOOG_train.csv')</p><p class="source-code">dataset_training.head()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer193" class="IMG---Figure"><img src="image/B15777_09_15.jpg" alt="Figure 9.15: The first five rows of the GOOG_Training dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.15: The first five rows of the GOOG_Training dataset</p></li>
				<li>We are going to make our prediction using the <strong class="source-inline">Open</strong> stock price; therefore, select the <strong class="source-inline">Open</strong> stock price column from the dataset and print the values:<p class="source-code">training_data = dataset_training[['Open']].values</p><p class="source-code">training_data</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 555.647278],</p><p class="source-code">       [ 555.418152],</p><p class="source-code">       [ 554.42688 ],</p><p class="source-code">       ...,</p><p class="source-code">       [1017.150024],</p><p class="source-code">       [1049.619995],</p><p class="source-code">       [1050.959961]])</p></li>
				<li>Then, perform feature scaling by normalizing the data using <strong class="source-inline">MinMaxScaler</strong> and setting the range of the features so that they have a minimum value of <strong class="source-inline">0</strong> and a maximum value of one. Use the <strong class="source-inline">fit_transform</strong> method of the scaler on the training data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">sc = MinMaxScaler(feature_range = (0, 1))</p><p class="source-code">training_data_scaled = sc.fit_transform(training_data)</p><p class="source-code">training_data_scaled</p><p>The preceding code produces the following output:</p><p class="source-code">array([[0.08017394],</p><p class="source-code">       [0.07987932],</p><p class="source-code">       [0.07860471],</p><p class="source-code">       ...,</p><p class="source-code">       [0.67359064],</p><p class="source-code">       [0.71534169],</p><p class="source-code">       [0.71706467]])</p></li>
				<li>Create the data to get 60 timestamps from the current instance. We chose <strong class="source-inline">60</strong> here as this will give us a sufficient number of previous instances so that we can understand the trend; technically, this can be any number, but <strong class="source-inline">60</strong> is the optimal value. Additionally, the upper bound value here is <strong class="source-inline">1258</strong>, which is the index or count of rows (or records) in the training set:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p><p class="source-code">for i in range(60, 1258):</p><p class="source-code">    X_train.append(training_data_scaled[i-60:i, 0])</p><p class="source-code">    y_train.append(training_data_scaled[i, 0])</p><p class="source-code">X_train, y_train = np.array(X_train), \</p><p class="source-code">                   np.array(y_train)</p></li>
				<li>Next, reshape the data to add an extra dimension to the end of <strong class="source-inline">X_train</strong> using NumPy's <strong class="source-inline">reshape</strong> function:<p class="source-code">X_train = np.reshape(X_train, (X_train.shape[0], \</p><p class="source-code">                               X_train.shape[1], 1))</p><p class="source-code">X_train</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer194" class="IMG---Figure"><img src="image/B15777_09_16.jpg" alt="Figure 9.16: The data of a few timestamps from the current instance&#13;&#10;"/></div><p class="figure-caption">Figure 9.16: The data of a few timestamps from the current instance</p></li>
				<li>Import the following Keras libraries to build the <strong class="source-inline">RNN</strong>:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Set the seed and initiate the sequential model, as follows:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add an LSTM layer to the network with 50 units, set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong>, and set the <strong class="source-inline">input_shape</strong> argument to <strong class="source-inline">(X_train.shape[1], 1)</strong>. Add three additional LSTM layers, each with 50 units, and set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong> for the first two, as follows:<p class="source-code">model.add(LSTM(units = 50, return_sequences = True, \</p><p class="source-code">               input_shape = (X_train.shape[1], 1)))</p><p class="source-code"># Adding a second LSTM layer</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code"># Adding a third LSTM layer</p><p class="source-code">model.add(LSTM(units = 50, return_sequences = True))</p><p class="source-code"># Adding a fourth LSTM layer</p><p class="source-code">model.add(LSTM(units = 50))</p><p class="source-code"># Adding the output layer</p><p class="source-code">model.add(Dense(units = 1))</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and use <strong class="source-inline">Mean Squared Error</strong> for the loss. Fit the model to the training data for <strong class="source-inline">100</strong> epochs with a batch size of <strong class="source-inline">32</strong>:<p class="source-code"># Compiling the RNN</p><p class="source-code">model.compile(optimizer = 'adam', loss = 'mean_squared_error')</p><p class="source-code"># Fitting the RNN to the Training set</p><p class="source-code">model.fit(X_train, y_train, epochs = 100, batch_size = 32)</p></li>
				<li>Load and process the <strong class="source-inline">test</strong> data (which is treated as actual data here) and select the column representing the value of <strong class="source-inline">Open</strong> stock data:<p class="source-code">dataset_testing = pd.read_csv("../GOOG_test.csv")</p><p class="source-code">actual_stock_price = dataset_testing[['Open']].values</p><p class="source-code">actual_stock_price</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer195" class="IMG---Figure"><img src="image/B15777_09_17.jpg" alt="Figure 9.17: The actual processed data&#13;&#10;"/></div><p class="figure-caption">Figure 9.17: The actual processed data</p></li>
				<li>Concatenate the data; we will need <strong class="source-inline">60</strong> previous instances in order to get the stock price for each day. Therefore, we will need both training and test data:<p class="source-code">total_data = pd.concat((dataset_training['Open'], \</p><p class="source-code">                        dataset_testing['Open']), axis = 0)</p></li>
				<li>Reshape and scale the input to prepare the test data. Note that we are predicting the January monthly trend, which has <strong class="source-inline">21</strong> financial days, so in order to prepare the test set, we take the lower bound value as 60 and the upper bound value as 81. This ensures that the difference of <strong class="source-inline">21</strong> is maintained:<p class="source-code">inputs = total_data[len(total_data) \</p><p class="source-code">         - len(dataset_testing) - 60:].values</p><p class="source-code">inputs = inputs.reshape(-1,1)</p><p class="source-code">inputs = sc.transform(inputs)</p><p class="source-code">X_test = []</p><p class="source-code">for i in range(60, 81):</p><p class="source-code">    X_test.append(inputs[i-60:i, 0])</p><p class="source-code">X_test = np.array(X_test)</p><p class="source-code">X_test = np.reshape(X_test, (X_test.shape[0], \</p><p class="source-code">                    X_test.shape[1], 1))</p><p class="source-code">predicted_stock_price = model.predict(X_test)</p><p class="source-code">predicted_stock_price = sc.inverse_transform(\</p><p class="source-code">                        predicted_stock_price)</p></li>
				<li>Visualize the results by plotting the actual stock price and then plotting the predicted stock price:<p class="source-code"># Visualizing the results</p><p class="source-code">plt.plot(actual_stock_price, color = 'green', \</p><p class="source-code">         label = 'Real Alphabet Stock Price',\</p><p class="source-code">         ls='--')</p><p class="source-code">plt.plot(predicted_stock_price, color = 'red', \</p><p class="source-code">         label = 'Predicted Alphabet Stock Price',\</p><p class="source-code">         ls='-')</p><p class="source-code">plt.title('Predicted Stock Price')</p><p class="source-code">plt.xlabel('Time in days')</p><p class="source-code">plt.ylabel('Real Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>Please note that your results may differ slightly to the actual stock price of Alphabet.</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer196" class="IMG---Figure"><img src="image/B15777_09_18.jpg" alt="Figure 9.18: The real versus predicted stock price&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.18: The real versus predicted stock price</p>
			<p>This concludes <em class="italic">Exercise 9.01</em>, <em class="italic">Predicting the Trend of Alphabet's Stock Price Using an LSTM with 50 Units (Neurons)</em>, where we have predicted Alphabet's stock trends with the help of an <strong class="source-inline">LSTM</strong>. As shown in the preceding plot, the trend has been captured fairly.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZwdAzW">https://packt.live/2ZwdAzW</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YV3PvX">https://packt.live/2YV3PvX</a>.</p>
			<p>In the next activity, we will test our knowledge and practice building <strong class="source-inline">RNNs</strong> with <strong class="source-inline">LSTM</strong> layers by predicting the trend of Amazon's stock price over the last 5 years.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor182"/>Activity 9.01: Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</h2>
			<p>In this activity, we will examine the stock price of Amazon for the last 5 years—that is, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using an RNN and LSTM. We have the actual values for January 2019, so we can compare our predictions to the actual values later. Follow these steps to complete this activity: </p>
			<ol>
				<li value="1">Import the required libraries.</li>
				<li>From the full dataset, extract the <strong class="source-inline">Open</strong> column as the predictions will be made on the open stock value. Download the dataset from this book's GitHub repository. You can find the dataset at <a href="https://packt.live/2vtdA8o">https://packt.live/2vtdA8o</a>.</li>
				<li>Normalize the data between 0 and 1.</li>
				<li>Then, create timestamps. The values of each day in January 2019 will be predicted by the previous 60 days; so, if January 1 is predicted by using the value from the <em class="italic">n</em><span class="superscript">th</span> day up to December 31, then January 2 will be predicted by using the <em class="italic">n</em> + <em class="italic">1</em><span class="superscript">st</span> day and January 1, and so on.</li>
				<li>Reshape the data into three dimensions since the network needs data in three dimensions.</li>
				<li>Build an <strong class="source-inline">RNN</strong> model in <strong class="source-inline">Keras</strong> using <strong class="source-inline">50</strong> units (here, units refer to neurons) with four <strong class="source-inline">LSTM</strong> layers. The first step should provide the input shape. Note that the final <strong class="source-inline">LSTM</strong> layer always adds <strong class="source-inline">return_sequences=True</strong>, so it doesn't have to be explicitly defined.</li>
				<li>Process and prepare the test data that is the actual data for January 2019.</li>
				<li>Combine and process the training and test data.</li>
				<li>Visualize the results.</li>
			</ol>
			<p>After implementing these steps, you should see the following expected output:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B15777_09_19.jpg" alt="Figure 9.19: Real versus predicted stock prices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19: Real versus predicted stock prices</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 452.</p>
			<p>Now, let's try and improve performance by tweaking our <strong class="source-inline">LSTM</strong>. There is no gold standard on how to build an <strong class="source-inline">LSTM</strong>; however, the following permutation combinations can be tried in order to improve performance:</p>
			<ul>
				<li>Build an <strong class="source-inline">LSTM</strong> with moderate units, such as <strong class="source-inline">50</strong></li>
				<li>Build an <strong class="source-inline">LSTM</strong> with over <strong class="source-inline">100</strong> units</li>
				<li>Use more data; that is, instead of <strong class="source-inline">5</strong> years, take data from <strong class="source-inline">10</strong> years</li>
				<li>Apply regularization using <strong class="source-inline">100</strong> units</li>
				<li>Apply regularization using <strong class="source-inline">50</strong> units</li>
				<li>Apply regularization using more data and <strong class="source-inline">50</strong> units</li>
			</ul>
			<p>This list can have a number of combinations; whichever combination offers the best results can be considered a good algorithm for that particular dataset. In the next exercise, we will explore one of these options by adding more units to our <strong class="source-inline">LSTM</strong> layer and observing the performance.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor183"/>Exercise 9.02: Predicting the Trend of Alphabet's Stock Price Using an LSTM with 100 units</h2>
			<p>In this exercise, we will examine the stock price of Alphabet over the last 5 years, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using RNNs. We have the actual values for January 2019, so we will compare our predictions with the actual values later. This is the same task as the first exercise, but now we're using 100 units instead. Make sure that you compare the output with <em class="italic">Exercise 9.01</em>, <em class="italic">Predicting the Trend of Alphabet's Stock Price Using an LSTM with 50 Units (Neurons)</em>. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p></li>
				<li>Import the dataset using the pandas <strong class="source-inline">read_csv</strong> function and look at the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">dataset_training = pd.read_csv('../GOOG_train.csv')</p><p class="source-code">dataset_training.head()</p></li>
				<li>We are going to make our prediction using the <strong class="source-inline">Open</strong> stock price; therefore, select the <strong class="source-inline">Open</strong> stock price column from the dataset and print the values:<p class="source-code">training_data = dataset_training[['Open']].values</p><p class="source-code">training_data</p></li>
				<li>Then, perform feature scaling by normalizing the data using <strong class="source-inline">MinMaxScaler</strong> and setting the range of the features so that they have a minimum value of zero and a maximum value of one. Use the <strong class="source-inline">fit_transform</strong> method of the scaler on the training data:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">sc = MinMaxScaler(feature_range = (0, 1))</p><p class="source-code">training_data_scaled = sc.fit_transform(training_data)</p><p class="source-code">training_data_scaled</p></li>
				<li>Create the data to get <strong class="source-inline">60</strong> timestamps from the current instance. We chose <strong class="source-inline">60</strong> here as it will give us a sufficient number of previous instances in order to understand the trend; technically, this can be any number, but <strong class="source-inline">60</strong> is the optimal value. Additionally, the upper bound value here is <strong class="source-inline">1258</strong>, which is the index or count of rows (or records) in the <strong class="source-inline">training</strong> set:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p><p class="source-code">for i in range(60, 1258):</p><p class="source-code">    X_train.append(training_data_scaled[i-60:i, 0])</p><p class="source-code">    y_train.append(training_data_scaled[i, 0])</p><p class="source-code">X_train, y_train = np.array(X_train), np.array(y_train)</p></li>
				<li>Reshape the data to add an extra dimension to the end of <strong class="source-inline">X_train</strong> using NumPy's <strong class="source-inline">reshape</strong> function:<p class="source-code">X_train = np.reshape(X_train, (X_train.shape[0], \</p><p class="source-code">                               X_train.shape[1], 1))</p></li>
				<li>Import the following <strong class="source-inline">Keras</strong> libraries to build the <strong class="source-inline">RNN</strong>:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Set the seed and initiate the sequential model, as follows:<p class="source-code">seed = 1</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add an <strong class="source-inline">LSTM</strong> layer to the network with <strong class="source-inline">50</strong> units, set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong>, and set the <strong class="source-inline">input_shape</strong> argument to <strong class="source-inline">(X_train.shape[1], 1)</strong>. Add three additional <strong class="source-inline">LSTM</strong> layers, each with <strong class="source-inline">50</strong> units, and set the <strong class="source-inline">return_sequences</strong> argument to <strong class="source-inline">True</strong> for the first two. Add a final output layer of size <strong class="source-inline">1</strong>:<p class="source-code">model.add(LSTM(units = 100, return_sequences = True, \</p><p class="source-code">               input_shape = (X_train.shape[1], 1)))</p><p class="source-code"># Adding a second LSTM</p><p class="source-code">model.add(LSTM(units = 100, return_sequences = True))</p><p class="source-code"># Adding a third LSTM layer</p><p class="source-code">model.add(LSTM(units = 100, return_sequences = True))</p><p class="source-code"># Adding a fourth LSTM layer</p><p class="source-code">model.add(LSTM(units = 100))</p><p class="source-code"># Adding the output layer</p><p class="source-code">model.add(Dense(units = 1))</p></li>
				<li>Compile the network with an <strong class="source-inline">adam</strong> optimizer and use <strong class="source-inline">Mean Squared Error</strong> for the loss. Fit the model to the training data for <strong class="source-inline">100</strong> epochs with a batch size of <strong class="source-inline">32</strong>:<p class="source-code"># Compiling the RNN</p><p class="source-code">model.compile(optimizer = 'adam', loss = 'mean_squared_error')</p><p class="source-code"># Fitting the RNN to the Training set</p><p class="source-code">model.fit(X_train, y_train, epochs = 100, batch_size = 32)</p></li>
				<li>Load and process the test data (which is treated as actual data here) and select the column representing the value of <strong class="source-inline">Open</strong> stock data:<p class="source-code">dataset_testing = pd.read_csv("../GOOG_test.csv")</p><p class="source-code">actual_stock_price = dataset_testing[['Open']].values</p><p class="source-code">actual_stock_price</p></li>
				<li>Concatenate the data since we will need <strong class="source-inline">60</strong> previous instances to get the stock price for each day. Therefore, we will need both the training and test data:<p class="source-code">total_data = pd.concat((dataset_training['Open'], \</p><p class="source-code">                        dataset_testing['Open']), axis = 0)</p></li>
				<li>Reshape and scale the input to prepare the test data. Note that we are predicting the January monthly trend, which has <strong class="source-inline">21</strong> financial days, so in order to prepare the test set, we take the lower bound value as <strong class="source-inline">60</strong> and the upper bound value as <strong class="source-inline">81</strong>. This ensures that the difference of <strong class="source-inline">21</strong> is maintained:<p class="source-code">inputs = total_data[len(total_data) \</p><p class="source-code">                    - len(dataset_testing) - 60:].values</p><p class="source-code">inputs = inputs.reshape(-1,1)</p><p class="source-code">inputs = sc.transform(inputs)</p><p class="source-code">X_test = []</p><p class="source-code">for i in range(60, 81):</p><p class="source-code">    X_test.append(inputs[i-60:i, 0])</p><p class="source-code">X_test = np.array(X_test)</p><p class="source-code">X_test = np.reshape(X_test, (X_test.shape[0], \</p><p class="source-code">                    X_test.shape[1], 1))</p><p class="source-code">predicted_stock_price = model.predict(X_test)</p><p class="source-code">predicted_stock_price = sc.inverse_transform(\</p><p class="source-code">                        predicted_stock_price)</p></li>
				<li>Visualize the results by plotting the actual stock price and plotting the predicted stock price:<p class="source-code"># Visualizing the results</p><p class="source-code">plt.plot(actual_stock_price, color = 'green', \</p><p class="source-code">         label = 'Real Alphabet Stock Price',ls='--')</p><p class="source-code">plt.plot(predicted_stock_price, color = 'red', \</p><p class="source-code">         label = 'Predicted Alphabet Stock Price',ls='-')</p><p class="source-code">plt.title('Predicted Stock Price')</p><p class="source-code">plt.xlabel('Time in days')</p><p class="source-code">plt.ylabel('Real Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p><strong class="bold">Expected output</strong>:</p><div id="_idContainer198" class="IMG---Figure"><img src="image/B15777_09_20.jpg" alt="Figure 9.20: Real versus predicted stock price&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.20: Real versus predicted stock price</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZDggf4">https://packt.live/2ZDggf4</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2O4ZoJ7">https://packt.live/2O4ZoJ7</a>.</p>
			<p>Now, if we compare the <strong class="source-inline">LSTM</strong> of <em class="italic">Exercise 9.01</em>, <em class="italic">Predicting the Trend of Alphabet's Stock Price Using an LSTM with 50 Units (Neurons)</em>, which had <strong class="source-inline">50</strong> neurons (units), with this <strong class="source-inline">LSTM</strong>, which uses <strong class="source-inline">100</strong> units, we can see that, unlike in the case of the Amazon stock price, the Alphabet stock trend is captured better using an <strong class="source-inline">LSTM</strong> with <strong class="source-inline">100</strong> units:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B15777_09_21.jpg" alt="Figure 9.21: Comparing the output with the LSTM of Exercise 9.01&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21: Comparing the output with the LSTM of Exercise 9.01</p>
			<p>Thus, we can clearly see that an <strong class="source-inline">LSTM</strong> with <strong class="source-inline">100</strong> units predicts a more accurate trend than an <strong class="source-inline">LSTM</strong> with <strong class="source-inline">50</strong> units. Do keep in mind that an <strong class="source-inline">LSTM</strong> with <strong class="source-inline">100</strong> units will need more computational time but provides better results in this scenario. As well as modifying our model by adding more units, we can also add regularization. The following activity will test whether adding regularization can make our Amazon model more accurate.</p>
			<h2 id="_idParaDest-183">Activity 9.02: Predic<a id="_idTextAnchor184"/>ting Amazon's Stock Price with Added Regularization</h2>
			<p>In this activity, we will examine the stock price of Amazon over the last 5 years, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using RNNs and an LSTM. We have the actual values for January 2019, so we will be able to compare our predictions with the actual values later. Initially, we predicted the trend of Amazon's stock price using an LSTM with 50 units (or neurons). Here, we will also add dropout regularization and compare the results with <em class="italic">Activity 9.01</em>, <em class="italic">Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries.</li>
				<li>From the full dataset, extract the <strong class="source-inline">Open</strong> column since the predictions will be made on the open stock value. You can download the dataset from this book's GitHub repository at <a href="https://packt.live/2vtdA8o">https://packt.live/2vtdA8o</a>.</li>
				<li>Normalize the data between 0 and 1.</li>
				<li>Then, create timestamps. The values of each day in January 2019 will be predicted by the previous <strong class="source-inline">60</strong> days. So, if January 1 is predicted by using the value from the <em class="italic">n</em><span class="superscript">th</span> day up to December 31, then January 2 will be predicted by using the <em class="italic">n + 1</em><span class="superscript">st</span> day and January 1, and so on.</li>
				<li>Reshape the data into three dimensions since the network needs the data to be in three dimensions.</li>
				<li>Build an RNN with four LSTM layers in Keras, each with <strong class="source-inline">50</strong> units (here, units refer to neurons), and a 20% dropout after each LSTM layer. The first step should provide the input shape. Note that the final LSTM layer always adds <strong class="source-inline">return_sequences=True</strong>.</li>
				<li>Process and prepare the test data, which is the actual data for January 2019.</li>
				<li>Combine and process the train and test data.</li>
				<li>Finally, visualize the results.</li>
			</ol>
			<p>After implementing these steps, you should get the following expected output:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B15777_09_22.jpg" alt="Figure 9.22: Real versus predicted stock prices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22: Real versus predicted stock prices</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 457.</p>
			<p>In the next activity, we will experiment with building an <strong class="source-inline">RNN</strong> with <strong class="source-inline">100</strong> units in each <strong class="source-inline">LSTM</strong> layer and compare this with how the <strong class="source-inline">RNN</strong> performed with only <strong class="source-inline">50</strong> units.</p>
			<h2 id="_idParaDest-184">Activity 9.03: Predict<a id="_idTextAnchor185"/>ing the Trend of Amazon's Stock Price Using an LSTM with an Increasing Number of LSTM Neurons (100 Units)</h2>
			<p>In this activity, we will examine the stock price of Amazon over the last 5 years, from January 1, 2014, to December 31, 2018. In doing so, we will try to predict and forecast the company's future trend for January 2019 using RNNs. We have the actual values for January 2019, so we will be able to compare our predictions with the actual values later. You can also compare the output difference with <em class="italic">Activity 9.01</em>, <em class="italic">Predicting the Trend of Amazon's Stock Price Using an LSTM with 50 Units (Neurons)</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries.</li>
				<li>From the full dataset, extract the <strong class="source-inline">Open</strong> column since the predictions will be made on the <strong class="source-inline">Open</strong> stock value.</li>
				<li>Normalize the data between 0 and 1.</li>
				<li>Then, create timestamps. The values of each day in January 2019 will be predicted by the previous <strong class="source-inline">60</strong> days; so, if January 1 is predicted by using the value from the nth day up to December 31, then January 2 will be predicted by using the <em class="italic">n + 1</em><span class="superscript">st</span> day and January 1, and so on.</li>
				<li>Reshape the data into three dimensions since the network needs data to be in three dimensions.</li>
				<li>Build an LSTM in Keras with 100 units (here, units refer to neurons). The first step should provide the input shape. Note that the final <strong class="source-inline">LSTM</strong> layer always adds <strong class="source-inline">return_sequences=True</strong>. Compile and fit the model to the training data.</li>
				<li>Process and prepare the test data, which is the actual data for January 2019.</li>
				<li>Combine and process the training and test data.</li>
				<li>Visualize the results.</li>
			</ol>
			<p>After implementing these steps, you should get the following expected output:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B15777_09_23.jpg" alt="Figure 9.23: Real versus predicted stock prices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23: Real versus predicted stock prices</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 462.</p>
			<p>In this activity, we created an <strong class="source-inline">RNN</strong> with four <strong class="source-inline">LSTM</strong> layers, each with <strong class="source-inline">100</strong> units. We compared this to the results of <em class="italic">Activity 9.02</em>, <em class="italic">Predicting Amazon's Stock Price with Added Regularization</em>, in which there were <strong class="source-inline">50</strong> units per layer. The difference between the two models was minimal, so a model with fewer units is preferable due to the decrease in computational time and there being a smaller possibility of overfitting the training data.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor186"/>Summary</h1>
			<p>In this chapter, we learned about sequential modeling and sequential memory by examining some real-life cases with Google Assistant. Then, we learned how sequential modeling is related to <strong class="source-inline">RNNs</strong>, as well as how <strong class="source-inline">RNNs</strong> are different from traditional feedforward networks. We learned about the vanishing gradient problem in detail and how using an <strong class="source-inline">LSTM</strong> is better than a simple <strong class="source-inline">RNN</strong> to overcome the vanishing gradient problem. We applied what we learned to time series problems by predicting stock trends.</p>
			<p>In this workshop, we learned the basics of machine learning and Python, while also gaining an in-depth understanding of applying Keras to develop efficient deep learning solutions. We explored the difference between machine and deep learning. We began the workshop by building a logistic regression model, first with scikit-learn, and then with Keras. </p>
			<p>Then, we explored Keras and its different models further by creating prediction models for various real-world scenarios, such as classifying online shoppers into those with purchase intention and those without. We learned how to evaluate, optimize, and improve models to achieve maximum information to create robust models that perform well on new, unseen data. </p>
			<p>We also incorporated cross-validation by building Keras models with wrappers for scikit-learn that help those familiar with scikit-learn workstreams utilize Keras models easily. Then, we learned how to apply <strong class="source-inline">L1</strong>, <strong class="source-inline">L2</strong>, and <strong class="source-inline">dropout regularization</strong> techniques to improve the accuracy of models and to help prevent our models from overfitting the training data. </p>
			<p>Next, we explored model evaluation further by applying techniques such as null accuracy for baseline comparison and evaluation metrics such as precision, the <strong class="source-inline">AUC-ROC</strong> score, and more to understand how our model scores classification tasks. Ultimately, these advanced evaluation techniques helped us understand under what conditions our model is performing well and where there is room for improvement. </p>
			<p>We ended the workshop by creating some advanced models with Keras. We explored computer vision by building <strong class="source-inline">CNN</strong> models with various parameters to classify images. Then, we used pre-trained models to classify new images and fine-tuned those pre-trained models so that we could utilize them for our own applications. Finally, we covered sequential modeling, which is used for modeling sequences such as stock prices and natural language processing. We tested this knowledge by creating <strong class="source-inline">RNN</strong> networks with <strong class="source-inline">LSTM</strong> layers to predict the stock price of real stock data and experimented with various numbers of units in each layer and the effect of dropout regularization on the model's performance. </p>
			<p>Overall, we have gained a comprehensive understanding of how to use Keras to solve a variety of problems using real-world datasets. We covered the classification tasks of online shoppers, hepatitis C data, and failure data for Scania trucks, as well as regression tasks such as predicting the aquatic toxicity of various chemicals when given various chemical attributes. We also performed image classification tasks and built <strong class="source-inline">CNN</strong> models to predict whether images are of flowers or cars, and also built regression tasks to predict future stock prices with <strong class="source-inline">RNNs</strong>. By using this workshop to build models with real-word datasets, you are ready to apply your learning and understanding to your own problem-solving and create your own applications.</p>
		</div>
		<div>
			<div id="_idContainer203" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer204" class="Content">
			</div>
		</div>
	</body></html>