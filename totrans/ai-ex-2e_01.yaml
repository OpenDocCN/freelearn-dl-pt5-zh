- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started with Next-Generation Artificial Intelligence through Reinforcement
    Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next-generation AI compels us to realize that machines do indeed think. Although
    machines do not think like us, their thought process has proven its efficiency
    in many areas. In the past, the belief was that AI would reproduce human thinking
    processes. Only neuromorphic computing (see *Chapter 18*, *Neuromorphic Computing*),
    remains set on this goal. Most AI has now gone beyond the way humans think, as
    we will see in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The **Markov decision process** (**MDP**), a **reinforcement learning** (**RL**)
    algorithm, perfectly illustrates how machines have become intelligent in their
    own unique way. Humans build their decision process on experience. MDPs are memoryless.
    Humans use logic and reasoning to think problems through. MDPs apply random decisions
    100% of the time. Humans think in words, labeling everything they perceive. MDPs
    have an unsupervised approach that uses no labels or training data. MDPs boost
    the machine thought process of self-driving cars (SDCs), translation tools, scheduling
    software, and more. This memoryless, random, and unlabeled machine thought process
    marks a historical change in the way a former human problem was solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this realization comes a yet more mind-blowing fact. AI algorithms and
    hybrid solutions built on IoT, for example, have begun to surpass humans in strategic
    areas. Although AI cannot replace humans in every field, AI combined with classical
    automation now occupies key domains: banking, marketing, supply chain management,
    scheduling, and many other critical areas.'
  prefs: []
  type: TYPE_NORMAL
- en: As you will see, starting with this chapter, you can occupy a central role in
    this new world as an adaptive thinker. You can design AI solutions and implement
    them. There is no time to waste. In this chapter, we are going to dive quickly
    and directly into reinforcement learning through the MDP.
  prefs: []
  type: TYPE_NORMAL
- en: Today, AI is essentially mathematics translated into source code, which makes
    it difficult to learn for traditional developers. However, we will tackle this
    approach pragmatically.
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is not to take the easy route. We're striving to break complexity
    into understandable parts and confront them with reality. You are going to find
    out right from the outset how to apply an adaptive thinker's process that will
    lead you from an idea to a solution in reinforcement learning, and right into
    the center of gravity of the next generation of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI is constantly evolving. The classical approach states that:'
  prefs: []
  type: TYPE_NORMAL
- en: AI covers all domains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is a subset of AI, with clustering, classification, regression,
    and reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is a subset of machine learning that involves neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, these domains often overlap and it's difficult to fit neuromorphic
    computing, for example, with its sub-symbolic approach, into these categories
    (see *Chapter 18*, *Neuromorphic Computing*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, RL clearly fits into machine learning. Let''s have a brief
    look into the scientific foundations of the MDP, the RL algorithm we are going
    to explore. The main concepts to keep in mind are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimal transport**: In 1781, Gaspard Monge defined transport optimizing
    from one location to another using the shortest and most cost-effective path;
    for example, mining coal and then using the most cost-effective path to a factory.
    This was subsequently generalized to any form of path from point A to point B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boltzmann equation and constant**: In the late 19th century, Ludwig Boltzmann
    changed our vision of the world with his probabilistic distribution of particles
    beautifully summed up in his entropy formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S* = *k* * log *W*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*S* represents the entropy (energy, disorder) of a system expressed. *k* is the Boltzmann
    constant, and *W* represents the number of microstates. We will explore Boltzmann''s
    ideas further in *Chapter 14*, *Preparing the Input of Chatbots with Restricted
    Boltzmann Machines (RBMs) and Principal Component Analysis (PCA)*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Probabilistic distributions advanced further**: Josiah Willard Gibbs took
    the probabilistic distributions of large numbers of particles a step further.
    At that point, probabilistic information theory was advancing quickly. At the
    turn of the 19th century, Andrey Markov applied probabilistic algorithms to language,
    among other areas. A modern era of information theory was born.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When Boltzmann and optimal transport meet**: 2011 Fields Medal winner, Cédric
    Villani, brought Boltzmann''s equation to yet another level. Villani then went
    on to unify optimal transport and Boltzmann. Cédric Villani proved something that
    was somewhat intuitively known to 19th century mathematicians but required proof.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take all of the preceding concepts and materialize them in a real-world
    example that will explain why reinforcement learning using the MDP, for example,
    is so innovative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing the following cup of tea will take you right into the next generation
    of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Consider a cup of tea'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look at this cup of tea in two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Macrostates**: You look at the cup and content. You can see the volume of
    tea in the cup and you could feel the temperature when holding the cup in your hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Microstates**: But can you tell how many molecules are in the tea, which
    ones are hot, warm, or cold, their velocity and directions? Impossible right?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, imagine, the tea contains 2,000,000,000+ Facebook accounts, or 100,000,000+
    Amazon Prime users with millions of deliveries per year. At this level, we simply
    abandon the idea of controlling every item. We work on trends and probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann provides a probabilistic approach to the evaluation of the features
    of our real world. Materializing Boltzmann in logistics through optimal transport
    means that the temperature could be the ranking of a product, the velocity can
    be linked to the distance to delivery, and the direction could be the itineraries
    we will study in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Markov picked up the ripe fruits of microstate probabilistic descriptions and
    applied it to his MDP. Reinforcement learning takes the huge volume of elements
    (particles in a cup of tea, delivery locations, social network accounts) and defines
    the probable paths they take.
  prefs: []
  type: TYPE_NORMAL
- en: The turning point of human thought occurred when we simply could not analyze
    the state and path of the huge volumes facing our globalized world, which generates
    images, sounds, words, and numbers that exceed traditional software approaches.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we can start exploring the MDP.
  prefs: []
  type: TYPE_NORMAL
- en: How to adapt to machine thinking and become an adaptive thinker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning, one of the foundations of machine learning, supposes
    learning through trial and error by interacting with an environment. This sounds
    familiar, doesn't it? That is what we humans do all our lives—in pain! Try things,
    evaluate, and then continue; or try something else.
  prefs: []
  type: TYPE_NORMAL
- en: In real life, you are the agent of your thought process. In reinforcement learning,
    the agent is the function calculating randomly through this trial-and-error process.
    This thought process function in machine learning is the MDP agent. This form
    of empirical learning is sometimes called Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering the theory and implementation of an MDP through a three-step method
    is a prerequisite.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will detail the three-step approach that will turn you into an
    AI expert, in general terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting by describing a problem to solve with real-life cases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, building a mathematical model that considers real-life limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, writing source code or using a cloud platform solution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a way for you to approach any project with an adaptive attitude from
    the outset. This shows that a human will always be at the center of AI by explaining
    how we can build the inputs, run an algorithm, and use the results of our code.
    Let's consider this three-step process and put it into action.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming real-life issues using the three-step approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key point of this chapter is to avoid writing code that will never be used.
    First, begin by understanding the subject as a subject matter expert. Then, write
    the analysis with words and mathematics to make sure your reasoning reflects the
    subject and, most of all, that the program will make sense in real life. Finally,
    in step 3, only write the code when you are sure about the whole project.
  prefs: []
  type: TYPE_NORMAL
- en: Too many developers start writing code without stopping to think about how the
    results of that code are going to manifest themselves within real-life situations.
    You could spend weeks developing the perfect code for a problem, only to find
    out that an external factor has rendered your solution useless. For instance,
    what if you coded a solar-powered robot to clear snow from the yard, only to discover
    that during winter, there isn't enough sunlight to power the robot!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to tackle the MDP (Q function) and apply it to
    reinforcement learning with the Bellman equation. We are going to approach it
    a little differently to most, however. We''ll be thinking about practical application,
    not simply code execution. You can find tons of source code and examples on the
    web. The problem is, much like our snow robot, such source code rarely considers
    the complications that come about in real-life situations. Let''s say you find
    a program that finds the optimal path for a drone delivery. There''s an issue,
    though; it has many limits that need to be overcome due to the fact that the code
    has not been written with real-life practicality in mind. You, as an adaptive
    thinker, are going to ask some questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What if there are 5,000 drones over a major city at the same time? What happens
    if they try to move in straight lines and bump into each other?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a drone-jam legal? What about the noise over the city? What about tourism?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about the weather? Weather forecasts are difficult to make, so how is this
    scheduled?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we resolve the problem of coordinating the use of charging and parking
    stations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In just a few minutes, you will be at the center of attention among theoreticians
    who know more than you, on one hand, and angry managers who want solutions they
    cannot get on the other. Your real-life approach will solve these problems. To
    do that, you must take the following three steps into account, starting with really
    getting involved in the real-life subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to successfully implement our real-life approach, comprised of the
    three steps outlined in the previous section, there are a few prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be a subject matter expert (SME)**: First, you have to be an SME. If a theoretician
    geek comes up with a hundred TensorFlow functions to solve a drone trajectory
    problem, you now know it is going to be a tough ride in which real-life parameters
    are constraining the algorithm. An SME knows the subject and thus can quickly
    identify the critical factors of a given field. AI often requires finding a solution
    to a complex problem that even an expert in a given field cannot express mathematically.
    Machine learning sometimes means finding a solution to a problem that humans do
    not know how to explain. Deep learning, involving complex networks, solves even
    more difficult problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Have enough mathematical knowledge to understand AI concepts**: Once you
    have the proper natural language analysis, you need to build your abstract representation
    quickly. The best way is to look around and find an everyday life example and
    make a mathematical model of it. Mathematics is not an option in AI, but a prerequisite.
    The effort is worthwhile. Then, you can start writing a solid piece of source
    code or start implementing a cloud platform ML solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Know what source code is about as well as its potential and limits**: MDP
    is an excellent way to go and start working on the three dimensions that will
    make you adaptive: describing what is around you in detail in words, translating
    that into mathematical representations, and then implementing the result in your
    source code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With those prerequisites in mind, let's look at how you can become a problem-solving
    AI expert by following our practical three-step process. Unsurprisingly, we'll begin
    at step 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1 – describing a problem to solve: MDP in natural language'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Step 1 of any AI problem is to go as far as you can to understand the subject
    you are asked to represent. If it's a medical subject, don't just look at data;
    go to a hospital or a research center. If it's a private security application,
    go to the places where they will need to use it. If it's for social media, make
    sure to talk to many users directly. The key concept to bear in mind is that you
    have to get a "feel" for the subject, as if you were the real "user."
  prefs: []
  type: TYPE_NORMAL
- en: For example, transpose it into something you know in your everyday life (work
    or personal), something you are an SME in. If you have a driver's license, then
    you are an SME of driving. You are certified. This is a fairly common certification,
    so let's use this as our subject matter in the example that will follow. If you
    do not have a driver's license or never drive, you can easily replace moving around
    in a car by imagining you are moving around on foot; you are an SME of getting
    from one place to another, regardless of what means of transport that might involve.
    However, bear in mind that a real-life project would involve additional technical
    aspects, such as traffic regulations for each country, so our imaginary SME does
    have its limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting into the example, let''s say you are an e-commerce business driver
    delivering a package in a location you are unfamiliar with. You are the operator
    of a self-driving vehicle. For the time being, you''re driving manually. You have
    a GPS with a nice color map on it. The locations around you are represented by
    the letters **A** to **F**, as shown in the simplified map in the following diagram.
    You are presently at **F**. Your goal is to reach location **C**. You are happy,
    listening to the radio. Everything is going smoothly, and it looks like you are
    going to be there on time. The following diagram represents the locations and
    routes that you can cover:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_01.png](img/B15438_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: A diagram of delivery routes'
  prefs: []
  type: TYPE_NORMAL
- en: The guidance system's state indicates the complete path to reach **C**. It is
    telling you that you are going to go from **F** to **B** to **D**, and then to
    **C**. It looks good!
  prefs: []
  type: TYPE_NORMAL
- en: 'To break things down further, let''s say:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The present state is the letter *s*. *s* is a variable, not an actual state.
    It can be one of the locations in *L*, the set of locations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L* = {**A**, **B**, **C**, **D**, **E**, **F**}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We say *present state* because there is no sequence in the learning process.
    The memoryless process goes from one present state to another. In the example
    in this chapter, the process starts at location **F**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Your next action is the letter *a* (action). This action *a* is not location
    **A**. The goal of this action is to take us to the next possible location in
    the graph. In this case, only **B** is possible. The goal of *a* is to take us
    from *s* (present state) to *s'* (new state).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action *a* (not location **A**) is to go to location **B**. You look at
    your guidance system; it tells you there is no traffic, and that to go from your
    present state, **F**, to your next state, **B**, will take you only a few minutes.
    Let's say that the next state **B** is the letter **B**. This next state **B**
    is *s'*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, you are still quite happy, and we can sum up your situation
    with the following sequence of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s*, *a*, *s''*'
  prefs: []
  type: TYPE_NORMAL
- en: The letter *s* is your present state, your present situation. The letter *a*
    is the action you're deciding, which is to go to the next location; there, you
    will be in another state, *s'*. We can say that thanks to the action *a*, you
    will go from *s* to *s'*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine that the driver is not you anymore. You are tired for some reason.
    That is when a self-driving vehicle comes in handy. You set your car to autopilot.
    Now, you are no longer driving; the system is. Let's call that system the **agent**.
    At point **F**, you set your car to autopilot and let the self-driving agent take
    over.
  prefs: []
  type: TYPE_NORMAL
- en: Watching the MDP agent at work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The self-driving AI is now in charge of the vehicle. It is acting as the MDP
    agent. This now sees what you have asked it to do and checks its **mapping environment**,
    which represents all the locations in the previous diagram from **A** to **F**.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, you are rightly worried. Is the agent going to make it or not?
    You are wondering whether its strategy meets yours. You have your **policy** *P*—your
    way of thinking—which is to take the shortest path possible. Will the agent agree?
    What's going on in its machine mind? You observe and begin to realize things you
    never noticed before.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is the first time you are using this car and guidance system, the
    agent is **memoryless**, which is an MDP feature. The agent doesn't know anything
    about what went on before. It seems to be happy with just calculating from this
    state *s* at location **F**. It will use machine power to run as many calculations
    as necessary to reach its goal.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing you are watching is the total distance from **F** to **C** to
    check whether things are OK. That means that the agent is calculating all the
    states from **F** to **C**.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, state **F** is state 1, which we can simplify by writing *s*[1];
    B is state 2, which we can simplify by writing *s*[2]; D is *s*[3]; and C is *s*[4].
    The agent is calculating all of these possible states to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: The agent knows that when it reaches **D**, **C** will be better because the
    reward will be higher for going to C than anywhere else. Since it cannot eat a
    piece of cake to reward itself, the agent uses numbers. Our agent is a real number
    cruncher. When it is wrong, it gets a poor reward or nothing in this model. When
    it's right, it gets a reward represented by the letter *R*, which we'll encounter
    during step 2\. This action-value (reward) transition, often named the Q function,
    is the core of many reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: When our agent goes from one state to another, it performs a *transition* and
    gets a reward. For example, the transition can be from **F** to **B**, state 1
    to state 2, or *s*[1] to *s*[2].
  prefs: []
  type: TYPE_NORMAL
- en: You are feeling great and are going to be on time. You are beginning to understand
    how the machine learning agent in your self-driving car is thinking. Suddenly,
    you look up and see that a traffic jam is building up. Location **D** is still
    far away, and now you do not know whether it would be good to go from **D** to
    **C** or **D** to **E**, in order to take another road to **C**, which involves
    less traffic. You are going to see what your agent thinks!
  prefs: []
  type: TYPE_NORMAL
- en: The agent takes the traffic jam into account, is stubborn, and increases its
    reward to get to **C** by the shortest way. Its policy is to stick to the initial
    plan. You do not agree. You have another policy.
  prefs: []
  type: TYPE_NORMAL
- en: You stop the car. You both have to agree before continuing. You have your opinion
    and policy; the agent does not agree. Before continuing, your views need to **converge**.
    **Convergence** is the key to making sure that your calculations are correct,
    and it's a way to evaluate the quality of a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: A mathematical representation is the best way to express this whole process
    at this point, which we will describe in the following step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2 – building a mathematical model: the mathematical representation of
    the Bellman equation and MDP'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mathematics involves a whole change in your perspective of a problem. You are
    going from words to functions, the pillars of source coding.
  prefs: []
  type: TYPE_NORMAL
- en: Expressing problems in mathematical notation does not mean getting lost in academic
    math to the point of never writing a single line of code. Just use mathematics
    to get a job done efficiently. Skipping mathematical representation will fast-track
    a few functions in the early stages of an AI project. However, when the real problems
    that occur in all AI projects surface, solving them with source code alone will
    prove virtually impossible. The goal here is to pick up enough mathematics to
    implement a solution in real-life companies.
  prefs: []
  type: TYPE_NORMAL
- en: It is necessary to think through a problem by finding something familiar around
    us, such as the itinerary model covered early in this chapter. It is a good thing
    to write it down with some abstract letters and symbols as described before, with
    *a* meaning an action, and *s* meaning a state. Once you have understood the problem
    and expressed it clearly, you can proceed further.
  prefs: []
  type: TYPE_NORMAL
- en: Now, mathematics will help to clarify the situation by means of shorter descriptions.
    With the main ideas in mind, it is time to convert them into equations.
  prefs: []
  type: TYPE_NORMAL
- en: From MDP to the Bellman equation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In step 1, the agent went from **F**, or state 1 or *s*, to **B**, which was
    state 2 or *s'*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A strategy drove this decision—a policy represented by *P*. One mathematical
    expression contains the MDP state transition function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*[a](*s*, *s''*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*P* is the policy, the strategy made by the agent to go from **F** to **B**
    through action *a*. When going from **F** to **B**, this state transition is named
    the **state transition function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* is the action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is state 1 (**F**), and *s''* is state 2 (**B**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reward (right or wrong) matrix follows the same principle:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R*[a](*s*, *s''*)'
  prefs: []
  type: TYPE_NORMAL
- en: That means *R* is the reward for the action of going from state *s* to state
    *s'*. Going from one state to another will be a random process. Potentially, all
    states can go to any other state.
  prefs: []
  type: TYPE_NORMAL
- en: Each line in the matrix in the example represents a letter from **A** to **F**,
    and each column represents a letter from **A** to **F**. All possible states are
    represented. The `1` values represent the nodes (vertices) of the graph. Those
    are the possible locations. For example, line 1 represents the possible moves
    for letter **A**, line 2 for letter **B**, and line 6 for letter **F**. On the
    first line, **A** cannot go to **C** directly, so a `0` value is entered. But,
    it can go to **E**, so a `1` value is added.
  prefs: []
  type: TYPE_NORMAL
- en: Some models start with `-1` for impossible choices, such as **B** going directly
    to **C**, and `0` values to define the locations. This model starts with `0` and
    `1` values. It sometimes takes weeks to design functions that will create a reward
    matrix (see *Chapter 2*, *Building a Reward Matrix – Designing Your Datasets*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we will be working on inputs a reward matrix so that the program
    can choose its best course of action. Then, the agent will go from state to state,
    learning the best trajectories for every possible starting location point. The
    goal of the MDP is to go to **C** (line 3, column 3 in the reward matrix), which
    has a starting value of 100 in the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Somebody familiar with Python might wonder why I used `ql` instead of `np`.
    Some might say "convention," "mainstream," "standard." My answer is a question.
    Can somebody define what "standard" AI is in this fast-moving world! My point
    here for the MDP is to use `ql` as an abbreviation of "Q-learning" instead of
    the "standard" abbreviation of NumPy, which is `np`. Naturally, beyond this special
    abbreviation for the MDP programs, I'll use `np`. Just bear in mind that conventions
    are there to break so as to set ourselves free to explore new frontiers. Just
    make sure your program works well!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several key properties of this decision process, among which there
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Markov property**: The process does not take the past into account. It
    is the memoryless property of this decision process, just as you do in a car with
    a guidance system. You move forward to reach your goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: From this memoryless Markov property, it is safe
    to say that the MDP is not supervised learning. Supervised learning would mean
    that we would have all the labels of the reward matrix *R* and learn from them.
    We would know what **A** means and use that property to make a decision. We would,
    in the future, be looking at the past. MDP does not take these labels into account.
    Thus, MDP uses unsupervised learning to train. A decision has to be made in each
    state without knowing the past states or what they signify. It means that the
    car, for example, was on its own at each location, which is represented by each
    of its states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic process**: In step 1, when state **D** was reached, the agent
    controlling the mapping system and the driver didn''t agree on where to go. A
    random choice could be made in a trial-and-error way, just like a coin toss. It
    is going to be a heads-or-tails process. The agent will toss the coin a significant
    number of times and measure the outcomes. That''s precisely how MDP works and
    how the agent will learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: Repeating a trial-and-error process with feedback
    from the agent''s environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markov chain**: The process of going from state to state with no history
    in a random, stochastic way is called a Markov chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To sum it up, we have three tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*[a](*s*, *s''*): A **policy**, *P*, or strategy to move from one state to
    another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T*[a](*s*, *s''*): A *T*, or stochastic (random) **transition**, function
    to carry out that action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R*[a](*s*, *s''*): An *R*, or **reward**, for that action, which can be negative,
    null, or positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T* is the transition function, which makes the agent decide to go from one
    point to another with a policy. In this case, it will be random. That''s what
    machine power is for, and that is how reinforcement learning is often implemented.'
  prefs: []
  type: TYPE_NORMAL
- en: Randomness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Randomness is a key property of MDP, defining it as a stochastic process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code describes the choice the **agent** is going to make:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code selects a new random action (state) at each episode.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Bellman equation is the road to programming reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bellman equation completes the MDP. To calculate the value of a state,
    let''s use *Q*, for the *Q* action-reward (or value) function. The pseudo source
    code of the Bellman equation can be expressed as follows for one individual state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The source code then translates the equation into a machine representation,
    as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The source code variables of the Bellman equation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q*(*s*): This is the value calculated for this state—the total reward. In
    step 1, when the agent went from **F** to **B**, the reward was a number such
    as 50 or 100 to show the agent that it''s on the right track.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R*(*s*): This is the sum of the values up to that point. It''s the total reward
    at that point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15438_01_002.png): This is here to remind us that trial and error
    has a price. We''re wasting time, money, and energy. Furthermore, we don''t even
    know whether the next step is right or wrong since we''re in a trial-and-error
    mode. **Gamma** is often set to 0.8\. What does that mean? Suppose you''re taking
    an exam. You study and study, but you don''t know the outcome. You might have
    80 out of 100 (0.8) chances of clearing it. That''s painful, but that''s life.
    The **gamma** penalty, or learning rate, makes the Bellman equation realistic
    and efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'max(*s''*): *s''* is one of the possible states that can be reached with *P*[a](*s*, *s''*);
    max is the highest value on the line of that state (location line in the reward matrix).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, you have done two-thirds of the job: understanding the real-life
    (process) and representing it in basic mathematics. You''ve built the mathematical
    model that describes your learning process, and you can implement that solution
    in code. Now, you are ready to code!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3 – writing source code: implementing the solution in Python'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, a problem was described in natural language to be able to talk to
    experts and understand what was expected. In step 2, an essential mathematical
    bridge was built between natural language and source coding. Step 3 is the software
    implementation phase.
  prefs: []
  type: TYPE_NORMAL
- en: When a problem comes up—and rest assured that one always does—it will be possible
    to go back over the mathematical bridge with the customer or company team, and
    even further back to the natural language process if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method guarantees success for any project. The code in this chapter is
    in Python 3.x. It is a reinforcement learning program using the Q function with
    the following reward matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`R` is the reward matrix described in the mathematical analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Q` inherits the same structure as `R`, but all values are set to `0` since
    this is a learning matrix. It will progressively contain the results of the decision
    process. The `gamma` variable is a double reminder that the system is learning
    and that its decisions have only an 80% chance of being correct each time. As
    the following code shows, the system explores the possible actions during the
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The agent starts in state 1, for example. You can start wherever you want because
    it's a random process. Note that the process only takes values > 0 into account.
    They represent possible moves (decisions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The current state goes through an analysis process to find possible actions
    (next possible states). You will note that there is no algorithm in the traditional
    sense with many rules. It''s a pure random calculation, as the following `random.choice`
    function shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the core of the system containing the Bellman equation, translated
    into the following source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the agent looks for the maximum value of the next possible
    state chosen at random.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand this is to run the program in your Python environment
    and `print()` the intermediate values. I suggest that you open a spreadsheet and
    note the values. This will give you a clear view of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last part is simply about running the learning process 50,000 times, just
    to be sure that the system learns everything there is to find. During each iteration,
    the agent will detect its present state, choose a course of action, and update
    the Q function matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The process continues until the learning process is over. Then, the program
    will print the result in `Q` and the normed result. The normed result is the process
    of dividing all values by the sum of the values found. `print(Q/ql.max(Q)*100)`
    norms `Q` by dividing `Q` by `q1.max(Q)*100`. The result comes out as a normed
    percentage.
  prefs: []
  type: TYPE_NORMAL
- en: You can run the process with `mdp01.py`.
  prefs: []
  type: TYPE_NORMAL
- en: The lessons of reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised reinforcement machine learning, such as the MDP-driven Bellman
    equation, is toppling traditional decision-making software location by location.
    Memoryless reinforcement learning requires few to no business rules and, thus,
    doesn't require human knowledge to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being an adaptive next-generation AI thinker involves three prerequisites:
    the effort to be an SME, working on mathematical models to think like a machine,
    and understanding your source code''s potential and limits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine power and reinforcement learning teach us two important lessons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 1**: Machine learning through reinforcement learning can beat human
    intelligence in many cases. No use fighting! The technology and solutions are
    already here in strategic domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 2**: A machine has no emotions, but you do. And so do the people around
    you. Human emotions and teamwork are an essential asset. Become an SME for your
    team. Learn how to understand what they''re trying to say intuitively and make
    a mathematical representation of it for them. Your job will never go away, even
    if you''re setting up solutions that don''t require much development, such as
    AutoML. AutoML, or automated machine learning, automates many tasks. AutoML automates
    functions such as the dataset pipeline, hyperparameters, and more. Development
    is partially or totally suppressed. But you still have to make sure the whole
    system is well designed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning shows that no human can solve a problem the way a machine
    does. 50,000 iterations with random searching is not an option for a human. The
    number of empirical episodes can be reduced dramatically with a numerical convergence
    form of gradient descent (see *Chapter 3*, *Machine Intelligence – Evaluation
    Functions and Numerical Convergence*).
  prefs: []
  type: TYPE_NORMAL
- en: Humans need to be more intuitive, make a few decisions, and see what happens,
    because humans cannot try thousands of ways of doing something. Reinforcement
    learning marks a new era for human thinking by surpassing human reasoning power
    in strategic fields.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, reinforcement learning requires mathematical models to function.
    Humans excel in mathematical abstraction, providing powerful intellectual fuel
    to those powerful machines.
  prefs: []
  type: TYPE_NORMAL
- en: The boundaries between humans and machines have changed. Humans' ability to
    build mathematical models and ever-growing cloud platforms will serve online machine
    learning services.
  prefs: []
  type: TYPE_NORMAL
- en: Finding out how to use the outputs of the reinforcement learning program we
    just studied shows how a human will always remain at the center of AI.
  prefs: []
  type: TYPE_NORMAL
- en: How to use the outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The reinforcement program we studied contains no trace of a specific field,
    as in traditional software. The program contains the Bellman equation with stochastic
    (random) choices based on the reward matrix. The goal is to find a route to **C**
    (line 3, column 3) that has an attractive reward (`100`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That reward matrix goes through the Bellman equation and produces a result
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The result contains the values of each state produced by the reinforced learning
    process, and also a normed `Q` (the highest value divided by other values).
  prefs: []
  type: TYPE_NORMAL
- en: As Python geeks, we are overjoyed! We made something that is rather difficult
    work, namely, reinforcement learning. As mathematical amateurs, we are elated.
    We know what MDP and the Bellman equation mean.
  prefs: []
  type: TYPE_NORMAL
- en: However, as natural language thinkers, we have made little progress. No customer
    or user can read that data and make sense of it. Furthermore, we cannot explain
    how we implemented an intelligent version of their job in the machine. We didn't.
  prefs: []
  type: TYPE_NORMAL
- en: We hardly dare say that reinforcement learning can beat anybody in the company,
    making random choices 50,000 times until the right answer came up.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we got the program to work, but hardly know what to do with the
    result ourselves. The consultant on the project cannot help because of the matrix
    format of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Being an adaptive thinker means knowing how to be good in all steps of a project.
    To solve this new problem, let's go back to step 1 with the result. Going back
    to step 1 means that if you have problems either with the results themselves or
    understanding them, it is necessary to go back to the SME level, the real-life
    situation, and see what is going wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'By formatting the result in Python, a graphics tool, or a spreadsheet, the
    result can be  displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | - | - | - | - | 258.44 | - |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | - | - | - | 321.8 | - | 207.752 |'
  prefs: []
  type: TYPE_TB
- en: '| **C** | - | - | 500 | 321.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| **D** | - | 258.44 | 401. | - | 258.44 | - |'
  prefs: []
  type: TYPE_TB
- en: '| **E** | 207.752 | - | - | 321.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| **F** | - | 258.44 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can start reading the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a starting state. Take **F**, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **F** line represents the state. Since the maximum value is 258.44 in the
    **B** column, we go to state **B**, the second line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum value in state **B** in the second line leads us to the **D** state
    in the  fourth column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The highest maximum of the **D** state (fourth line) leads us to the **C** state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if you start at the **C** state and decide not to stay at **C**, the
    **D** state becomes the maximum value, which will lead you back to **C**. However,
    the MDP will never do this naturally. You will have to force the system to do
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have now obtained a sequence: **F**->**B**->**D**->**C**. By choosing other
    points of departure, you can obtain other sequences by simply sorting the table.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful way of putting it remains the normalized version in percentages, as
    shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | - | - | - | - | 51.68% | - |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | - | - | - | 64.36% | - | 41.55% |'
  prefs: []
  type: TYPE_TB
- en: '| **C** | - | - | 100% | 64.36% | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| **D** | - | 51.68% | 80.2% | - | 51.68% | - |'
  prefs: []
  type: TYPE_TB
- en: '| **E** | 41.55% | - | - | 64.36% | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| **F** | - | 51.68% | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: Now comes the very tricky part. We started the chapter with a trip on the road.
    But I made no mention of it in the results analysis.
  prefs: []
  type: TYPE_NORMAL
- en: An important property of reinforcement learning comes from the fact that we
    are working with a mathematical model that can be applied to anything. No human
    rules are needed. We can use this program for many other subjects without writing
    thousands of lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Possible use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many cases to which we could adapt our reinforcement learning model
    without having to change any of its details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: optimizing a delivery for a driver, human or not'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This model was described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: optimizing warehouse flows'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The same reward matrix can apply to go from point **F** to **C** in a warehouse,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_02.png](img/B15438_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: A diagram illustrating a warehouse flow problem'
  prefs: []
  type: TYPE_NORMAL
- en: In this warehouse, the **F**->**B**->**D**->**C** sequence makes visual sense.
    If somebody goes from point **F** to **C**, then this physical path makes sense
    without going through walls.
  prefs: []
  type: TYPE_NORMAL
- en: It can be used for a video game, a factory, or any form of layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 3: automated planning and scheduling (APS)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By converting the system into a scheduling vector, the whole scenery changes.
    We have left the more comfortable world of physical processing of letters, faces,
    and trips. Though fantastic, those applications are social media's tip of the
    iceberg. The real challenge of AI begins in the abstract universe of human thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every single company, person, or system requires automatic planning and scheduling
    (see *Chapter 12*, *AI and the Internet of Things (IoT)*). The six **A** to **F**
    steps in the example of this chapter could well be six tasks to perform in a given
    unknown order represented by the following vector *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_01_003.png)'
  prefs: []
  type: TYPE_IMG
- en: The reward matrix then reflects the weights of constraints of the tasks of vector
    *x* to perform. For example, in a factory, you cannot assemble the parts of a
    product before manufacturing them.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the sequence obtained represents the schedule of the manufacturing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cases 4 and more: your imagination'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By using physical layouts or abstract decision-making vectors, matrices, and
    tensors, you can build a world of solutions in a mathematical reinforcement learning
    model. Naturally, the following chapters will enhance your toolbox with many other
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, you might want to imagine some situations in which you could
    use the A to F letters to express some kind of path.
  prefs: []
  type: TYPE_NORMAL
- en: To help you with these mind experiment simulations, open `mdp02.py` and go to
    line 97, which starts with the following code that enables a simulation tool.
    `nextc` and `nextci` are simply variables to remember where the path begins and
    will end. They are set to `-1` so as to avoid 0, which is a location.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary goal is to focus on the expression "concept code." The locations
    have become any concept you wish. A could be your bedroom, and C your kitchen.
    The path would go from where you wake up to where you have breakfast. A could
    be an idea you have, and F the end of a thinking process. The path would go from
    A (How can I hang this picture on the wall?) to E (I need to drill a hole) and,
    after a few phases, to F (I hung the picture on the wall). You can imagine thousands
    of paths like this as long as you define the reward matrix, the "concept code,"
    and a starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This code takes the result of the calculation, labels the result matrix, and
    accepts an input as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The input only accepts the label numerical code: `A=0`, `B=1` … `F=5`. The
    function then runs a classical calculation on the results to find the best path.
    Let''s takes an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are prompted to enter a starting point, enter `5`, for example, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will then produce the optimal path based on the output of the MDP
    process, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Try multiple scenarios and possibilities. Imagine what you could apply this
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: An e-commerce website flow (visit, cart, checkout, purchase) imagining that
    a user visits the site and then resumes a session at a later time. You can use
    the same reward matrix and "concept code" explored in this chapter. For example,
    a visitor visits a web page at 10 a.m., starting at point A of your website. Satisfied
    with a product, the visitor puts the product in a cart, which is point E of your
    website. Then, the visitor leaves the site before going to the purchase page,
    which is C. D is the critical point. Why didn't the visitor purchase the product?
    What's going on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can decide to have an automatic email sent after 24 hours saying: "There
    is a 10% discount on all purchases during the next 48 hours." This way, you will
    target all the visitors stuck at D and push them toward C.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A sequence of possible words in a sentence (subject, verb, object). Predicting
    letters and words was one of Andrey Markov's first applications 100+ years ago!
    You can imagine that B is the letter "a" of the alphabet. If D is "t," it is much
    more probable than F if F is "o," which is less probable in the English language.
    If an MDP reward matrix is built such as B leads to D or F, B can thus either
    go to D or to F. There are thus two possibilities, D or F. Andrey Markov would
    suppose, for example, that B is a variable that represents the letter "a," D is
    a variable that represents the letter "t" and F is a variable that represents
    the letter "o." After studying the structure of a language closely, he would find
    that the letter "a" would more likely be followed by "t" than by "o" in the English
    language. If one observes the English language, it is more likely to find an "a-t"
    sequence than an "a-o" sequence. In a Markov decision process, a higher probability
    will be awarded to the "a-t" sequence and a lower one to "a-o." If one goes back
    to the variables, the B-D sequence will come out as more probable than the B-F
    sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And anything you can find that fits the model that works is great!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning versus traditional applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning based on stochastic (random) processes will evolve beyond
    traditional approaches. In the past, we would sit down and listen to future users
    to understand their way of thinking.
  prefs: []
  type: TYPE_NORMAL
- en: We would then go back to our keyboard and try to imitate the human way of thinking.
    Those days are over. We need proper datasets and ML/DL equations to move forward.
    Applied mathematics has taken reinforcement learning to the next level. In my
    opinion, traditional software will soon be in the museum of computer science.
    The complexity of the huge volumes of data we are facing will require AI at some
    point.
  prefs: []
  type: TYPE_NORMAL
- en: An artificial adaptive thinker sees the world through applied mathematics translated
    into machine representations.
  prefs: []
  type: TYPE_NORMAL
- en: Use the Python source code example provided in this chapter in different ways.
    Run it and try to change some parameters to see what happens. Play around with
    the number of iterations as well. Lower the number from 50,000 down to where you
    find it fits best. Change the reward matrix a little to see what happens. Design
    your reward matrix trajectory. This can be an itinerary or decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Presently, AI is predominantly a branch of applied mathematics, not of neurosciences.
    You must master the basics of linear algebra and probabilities. That's a difficult
    task for a developer used to intuitive creativity. With that knowledge, you will
    see that humans cannot rival machines that have CPU and mathematical functions.
    You will also understand that machines, contrary to the hype around you, don't
    have emotions; although we can represent them to a scary point in chatbots (see
    *Chapter 16, Improving the Emotional Intelligence Deficiencies of Chatbots*).
  prefs: []
  type: TYPE_NORMAL
- en: A multi-dimensional approach is a prerequisite in an AI/ML/DL project. First,
    talk and write about the project, then make a mathematical representation, and
    finally go for software production (setting up an existing platform or writing
    code). In real life, AI solutions do not just grow spontaneously in companies
    as some hype would have us believe. You need to talk to the teams and work with
    them. That part is the real fulfilling aspect of a project—imagining it first
    and then implementing it with a group of real-life people.
  prefs: []
  type: TYPE_NORMAL
- en: MDP, a stochastic random action-reward (value) system enhanced by the Bellman
    equation, will provide effective solutions to many AI problems. These mathematical
    tools fit perfectly in corporate environments.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning using the Q action-value function is memoryless (no past)
    and unsupervised (the data is not labeled or classified). MDP provides endless
    avenues to solve real-life problems without spending hours trying to invent rules
    to make a system work.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are at the heart of Google's DeepMind approach, it is time to go
    to *Chapter 2*, *Building a Reward Matrix – Designing Your Datasets*, and discover
    how to create the reward matrix in the first place through explanations and source
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The answers to the questions are in *Appendix B*, with further explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: Is reinforcement learning memoryless? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does reinforcement learning use stochastic (random) functions? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is MDP based on a rule base? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the Q function based on the MDP? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is mathematics essential to AI? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the Bellman-MDP process in this chapter apply to many problems? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it impossible for a machine learning program to create another program by
    itself? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a consultant required to enter business rules in a reinforcement learning
    program? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is reinforcement learning supervised or unsupervised? (Supervised | Unsupervised)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can Q-learning run without a reward matrix? (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Andrey Markov: [https://www.britannica.com/biography/Andrey-Andreyevich-Markov](https://www.britannica.com/biography/Andrey-Andreyevich-Markov)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Markov process: [https://www.britannica.com/science/Markov-process](https://www.britannica.com/science/Markov-process)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
