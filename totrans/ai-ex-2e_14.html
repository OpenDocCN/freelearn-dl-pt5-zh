<html><head></head><body>
  <div id="_idContainer333">
    <h1 class="chapterNumber">14</h1>
    <h1 id="_idParaDest-269" class="chapterTitle">Preparing the Input of Chatbots with Restricted Boltzmann Machines (RBMs) and Principal Component Analysis (PCA)</h1>
    <p class="normal">In the following chapters, we will explore chatbot frameworks and build chatbots. You will find that creating a chatbot structure only takes a few clicks. However, no chatbot can be built without designing the input to prepare the desired dialog flow. The goal of this chapter is to demonstrate how to extract features from a dataset and then use them to gather the basic information to build a chatbot in <em class="italics">Chapter 15</em>, <em class="italics">Setting up a Cognitive NLP UI/CUI Chatbot</em>.</p>
    <p class="normal">The input of a dialog requires in-depth research and designing. In this chapter, we will build a <strong class="bold">restricted Boltzmann machine</strong> (<strong class="bold">RBM</strong>) that will analyze a dataset. In <em class="italics">Chapter 13</em>, <em class="italics">Visualizing Networks with TensorFlow 2.x and TensorBoard</em>, we examined the layers of a convolutional neural network (CNN) and displayed their outputs. This time, we will explore the weights of the RBM. We will go further and use the weights of the RBM as features. The weights of an RBM can be transformed into feature vectors for a <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) algorithm.</p>
    <p class="normal">We will use the feature vectors generated by the RBM to build a PCA display using TensorBoard Embedding Projector's functionality. We will then use the statistics obtained to lay the grounds for the inputs of a chatbot.</p>
    <p class="normal">To illustrate the whole process, we will use streaming platform data as an example of how this is done. Streaming has become a central activity of almost all smartphone owners. The problem facing Netflix, YouTube, Amazon, or any platform offering streaming services is to offer us the right video to watch. If a viewer watches a video, and the platform does not display a pertinent similar one to watch next, the viewer might choose to use another platform.</p>
    <p class="normal">This chapter is divided into two parts:</p>
    <ul>
      <li class="list">Building an RBM and then extending it to an automatic feature vector generator</li>
      <li class="list">Using PCA to represent the weights of an RBM as features. TensorFlow's Embedding Projector possesses an inbuilt PCA function. The statistics produced will provide the basis of the dialog structure for <em class="italics">Chapter 15</em>, <em class="italics">Setting Up a Cognitive NLP UI/CUI Chatbot</em>.</li>
    </ul>
    <p class="normal">Let's first define the basic terms we are using and our goals.</p>
    <h1 id="_idParaDest-270" class="title">Defining basic terms and goals</h1>
    <p class="normal">The goal of this chapter is to prepare data to create the input of a chatbot we will build in <em class="italics">Chapter 15</em>, <em class="italics">Setting Up a Cognitive NLP UI/CUI Chatbot</em>.</p>
    <p class="normal">Creating a chatbot requires preparation. We cannot just step into a project without a minimum amount of information. In our case, we will examine a dataset I created based on movie preferences. I did not choose to download huge datasets because we need to first focus on understanding the process and building a model using basic data.</p>
    <p class="normal">The size of the datasets increase daily on an online platform. When we watch a movie on a streaming platform, on Netflix for example, we can like the movie or click on the thumbs-down button.</p>
    <p class="normal">When we approve or disapprove of a movie on an online platform, our preferences are recorded. The features of these movies provide valuable information for the platform, which can then display choices we prefer: action, adventure, romantic, comedy, and more.</p>
    <p class="normal">In this chapter, we will first use an RBM to extract a description (such as action, adventure, or comedy, for example) of the movies watched by a user or a group of users. We will take the output weights produced by the RBM to create a file of features reflecting the user's preferences.</p>
    <p class="normal">This file of features of a user's preferences can be considered as a "mental dataset" of a person. The name might seem strange at first. However, a "mental" representation of a person goes beyond the standard age, income, and other impersonal data. Features such as "love," "violence," and "horizons" (wider views, adventure) give us a deeper understanding of a person than information we can find on a driving license.</p>
    <p class="normal">In the second part of the chapter, we will use the RBM's output of features of a person's "mind" as the input of a PCA. The PCA will calculate how the features relate to each other and how they vary, and we will display them in TensorBoard.</p>
    <p class="normal">We will then actually <em class="italics">see</em> a representation of a person's mind through the key features drawn from the RBM. This information will then be used to help us create a customized chatbot in <em class="italics">Chapter 15</em>.</p>
    <p class="normal">Let's move on to the first phase and build an RBM.</p>
    <h1 id="_idParaDest-271" class="title">Introducing and building an RBM</h1>
    <p class="normal">RBMs are random and undirected<a id="_idIndexMarker657"/> graph models generally built with a visible and a hidden layer. They were used in a Netflix competition to predict future user behavior. The goal here is not to predict what a viewer will do but establish who the viewer is and store the data in a viewer's profile-structured mind dataset. The input data represents the features to be trained to learn about viewer X. Each column represents a feature of X's potential personality and tastes. Each line represents the features of a movie that X has watched. The following code (and this section) is in <code class="Code-In-Text--PACKT-">RBM_01.py</code>:</p>
    <pre class="programlisting"><code class="hljs angelscript">np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
         [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
         [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
         [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
         [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
         [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])
</code></pre>
    <p class="normal">The goal of this RBM is <a id="_idIndexMarker658"/>to define a profile of X by computing the features of the movies watched. The input data could also be images, words, and other forms of data, as in any neural network.</p>
    <p class="normal">First, we will explore the architecture and define what an energy-driven neural network is. Then, we will build an RBM from scratch in Python.</p>
    <h2 id="_idParaDest-272" class="title">The architecture of an RBM</h2>
    <p class="normal">The RBM model used <a id="_idIndexMarker659"/>contains two layers: a visible layer and a hidden layer. Many types of RBMs exist, but generally, they contain the following properties:</p>
    <ul>
      <li class="list">There is no connection between the visible units, which is why it is <em class="italics">restricted</em>.</li>
      <li class="list">There is no connection between the hidden units enforcing the restricted property of the network.</li>
      <li class="list">There is no direction as in a feedforward neural network (FNN), as explored in <em class="italics">Chapter 8</em>, <em class="italics">Solving the XOR Problem with a Feedforward Neural Network</em>. An RBM's model is thus an <em class="italics">undirected</em> graph.</li>
      <li class="list">The visible and hidden<a id="_idIndexMarker660"/> layers are connected by a weight matrix and a bias vector, which are the lines in the following diagram:</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_14_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.1: The connection between visible and hidden units</p>
    <p class="normal">The network contains six visible and two hidden units, producing a weight matrix of 2×6 values to which we will add bias values.</p>
    <p class="normal">You will note that there is no output. The system runs from the visible units to the hidden units and back. We are operating feature extraction with this type of network. In this chapter, for example, we will use the weights as features.</p>
    <p class="normal">By forcing the network to represent its data contained in 6 units in 2 units through a weight matrix, the RBM<a id="_idIndexMarker661"/> creates feature representations. The hidden units, weights, and biases can be used for feature extraction.</p>
    <h2 id="_idParaDest-273" class="title">An energy-based model</h2>
    <p class="normal">An RBM is an <a id="_idIndexMarker662"/>energy-based model. The higher the energy, the lower the probability of obtaining the correct information; the lower the energy, the higher the probability – in other words, the higher the accuracy.</p>
    <p class="normal">To understand this, let's go back to the cup of tea we observed in <em class="italics">Chapter 1</em>, <em class="italics">Getting Started with Next-Generation Artificial Intelligence through Reinforcement Learning</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.2: The complexity of a cup of tea</p>
    <p class="normal">In <em class="italics">Chapter 1</em>, we observed a microstate of the cup through its global content and temperature. Then, we went on to use the Markov decision process (MDP) to run microstate calculations.</p>
    <p class="normal">This time, we will focus on the temperature of the cup of tea. <em class="italics">x</em> will be the global temperature of all the molecules in the cup of tea:</p>
    <ul>
      <li class="list">If <em class="italics">x</em> = 1, this means the temperature is very hot. The tea has just boiled.</li>
      <li class="list">If <em class="italics">x</em> = 0.5, this means the temperature has gone down.</li>
      <li class="list">If <em class="italics">x</em> = 0.1, this means the temperature is still a bit warm, but the tea is cooling.</li>
    </ul>
    <p class="normal">The higher the temperature, the more the molecules will be bouncing around in the cup with a high level of energy, making it feel hot.</p>
    <p class="normal">However, the hotter it is, the closer to very hot, the lower the probability we can drink it.</p>
    <p class="normal">This leads to a<a id="_idIndexMarker663"/> probability <em class="italics">p</em> for a temperature <em class="italics">x</em>:</p>
    <ul>
      <li class="list"><em class="italics">x</em> -&gt; 1, <em class="italics">p</em> -&gt; 0</li>
      <li class="list"><em class="italics">x</em> -&gt; 0, <em class="italics">p</em> -&gt; 1</li>
    </ul>
    <p class="normal">As you can see, in an energy-driven system, we will strive to lower the energy level. Let's say we have a person with an unknown tolerance for hot drinks, and we want to wager whether they can drink our cup of tea. Nobody wants to drink cold (low-energy) tea, sure, but if our focus is on the likelihood of a person being able to drink the tea without finding it too hot (high-energy), then we want that tea to be as low-energy (that is, cool) as possible!</p>
    <p class="normal">To illustrate the <em class="italics">p</em>(<em class="italics">x</em>) system of our cup of tea, we will use Euler's number <em class="italics">e</em>, which is equal to 2.718281. <em class="italics">p</em>(<em class="italics">x</em>) is the probability that we can drink our cup of tea, with <em class="italics">p</em> being the probability, and <em class="italics">x</em> the temperature or energy.</p>
    <p class="normal">We will begin to introduce a simple energy function in which <em class="italics">p</em>(<em class="italics">x</em>) = <em class="italics">e</em><sup>(–</sup><sup style="font-style: italic;">x</sup><sup>)</sup>:</p>
    <ul>
      <li class="list"><em class="italics">p</em>(<em class="italics">e</em><sup>(–1)</sup>) = 0.36</li>
      <li class="list"><em class="italics">p</em>(<em class="italics">e</em><sup>(–0.5)</sup>) = 0.60</li>
      <li class="list"><em class="italics">p</em>(<em class="italics">e</em><sup>(–0.1)</sup>) = 0.90</li>
    </ul>
    <p class="normal">You can see that as –<em class="italics">x</em> (energy) decreases, the probability <em class="italics">p</em>(<em class="italics">x</em>) increases.</p>
    <p class="normal">The goal of the learning function of an RBM is to decrease the energy level by optimizing the weights and biases. By doing this, the RBM increases the probability that the hidden units, the weights, and biases are optimized.</p>
    <p class="normal">To calculate the energy of an RBM, we will take the complete architecture of the network into account. Let's display our model again as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.3: The connection between visible and hidden units</p>
    <p class="normal">This RBM model <a id="_idIndexMarker664"/>contains the following values:</p>
    <ul>
      <li class="list"><em class="italics">E</em>(<em class="italics">v</em>, <em class="italics">h</em>), which is the energy function that takes the visible units (input data) and hidden units into account.</li>
      <li class="list"><em class="italics">v</em><sub style="font-style: italic;">i</sub> = the states of the visible units (input).</li>
      <li class="list"><em class="italics">a</em><sub style="font-style: italic;">i</sub> = the biases of the visible units.</li>
      <li class="list"><em class="italics">h</em><sub style="font-style: italic;">j</sub> = the states of the hidden units.</li>
      <li class="list"><em class="italics">b</em><sub style="font-style: italic;">j</sub> = the biases of the hidden units.</li>
      <li class="list"><em class="italics">w</em><sub style="font-style: italic;">ij</sub> = the weight matrix.</li>
    </ul>
    <p class="normal">With these variables in mind, we can define the energy function of an RBM for <img src="../Images/B15438_14_001.png" alt=""/>, <img src="../Images/B15438_14_002.png" alt=""/>, and <em class="italics">ij</em> as the lines and columns of the weight matrix as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_003.png" alt=""/></figure>
    <p class="normal">Now that we've got a better idea of what an RBM is and the principles behind it, let's start to consider how to build an RBM from scratch using Python.</p>
    <h2 id="_idParaDest-274" class="title">Building the RBM in Python</h2>
    <p class="normal">We will build an <a id="_idIndexMarker665"/>RBM using <code class="Code-In-Text--PACKT-">RBM_01.py</code> from<a id="_idIndexMarker666"/> scratch using our bare hands with no pre-built library. The idea is to understand an RBM from top to bottom to see how it ticks. We will explore more RBM theory as we build the machine.</p>
    <h3 id="_idParaDest-275" class="title">Creating a class and the structure of the RBM</h3>
    <p class="normal">First, the RBM class<a id="_idIndexMarker667"/> is created:</p>
    <pre class="programlisting"><code class="hljs ruby"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RBM</span>:</span>
    <span class="highlight"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span></span></span><span class="hljs-function"><span class="hljs-params">(<span class="hljs-keyword">self</span>, num_visible, num_hidden)</span></span>:
        <span class="hljs-keyword">self</span>.num_hidden = num_hidden
        <span class="hljs-keyword">self</span>.num_visible = num_visible
</code></pre>
    <p class="normal">The first function of the class will receive the number of hidden units (<code class="Code-In-Text--PACKT-">2</code>) and the number of visible units (<code class="Code-In-Text--PACKT-">6</code>).</p>
    <p class="normal">The <a id="_idIndexMarker668"/>weight matrix is initialized with random weight values at line 20:</p>
    <pre class="programlisting"><code class="hljs maxima">        np_rng = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">random</span>.RandomState(<span class="hljs-number">1234</span>)
        self.weights = <span class="hljs-built_in">np</span>.asarray(np_rng.uniform(
            low=-<span class="hljs-number">0.1</span> * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">6</span>. / (num_hidden + num_visible)),
            high=<span class="hljs-number">0.1</span> * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">6</span>. / (num_hidden + num_visible)),
            size=(num_visible, num_hidden)))
</code></pre>
    <p class="normal">The bias units will now be inserted in the first row and the first column at line 27:</p>
    <pre class="programlisting"><code class="hljs angelscript">        self.weights = np.insert(self.weights, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, axis = <span class="hljs-number">0</span>)
        self.weights = np.insert(self.weights, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, axis = <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The goal of this model will be to observe the behavior of the weights. Observing the weights will determine how to interpret the result in this model based on calculations between the visible and hidden units.</p>
    <p class="normal">The first row and column are the biases, as shown in the preceding code snippets. Only the weights will be analyzed for the profiling functions. The weights and biases are now in place.</p>
    <h3 id="_idParaDest-276" class="title">Creating a training function in the RBM class</h3>
    <p class="normal">On<a id="_idIndexMarker669"/> line<a id="_idIndexMarker670"/> 30, the training function is created:</p>
    <pre class="programlisting"><code class="hljs ruby">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, data, max_epochs, learning_rate)</span></span>:
</code></pre>
    <p class="normal">In this function:</p>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">self</code> is the class</li>
      <li class="list"><code class="Code-In-Text--PACKT-">data</code> is the 6×6 input array, containing 6 lines of movies and 6 columns of features of the movies:
        <pre class="programlisting"><code class="hljs angelscript">np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
          [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
          [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
          [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
          [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
          [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]])
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The RBM model in this chapter is <a id="_idIndexMarker671"/>using <strong class="bold">visible binary units</strong>, as shown in the input, which is the training data of this model. The RBM will use the input as training data.</p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">An RBM can contain other types of units: softmax units, Gaussian visible units, binomial units, rectified linear units, and more. Our model focuses on binary units.</p>
      </li>
      <li class="list"><code class="Code-In-Text--PACKT-">max_epochs</code> is the<a id="_idIndexMarker672"/> number of epochs that the <a id="_idIndexMarker673"/>RBM will run to train.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">learning_rate</code> is the learning rate that will be applied to the weight matrix containing the weights and the biases.</li>
    </ul>
    <p class="normal">We will now insert bias units of <code class="Code-In-Text--PACKT-">1</code> in the first column on line 35:</p>
    <pre class="programlisting"><code class="hljs angelscript">        data = np.insert(data, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis = <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">There are other strategies to initialize biases. This is a trial-and-error process, depending on your project. In this case, bias units of <code class="Code-In-Text--PACKT-">1</code> are sufficient to do the job.</p>
    <h3 id="_idParaDest-277" class="title">Computing the hidden units in the training function</h3>
    <p class="normal">On line 37, we start training the<a id="_idIndexMarker674"/> RBM during <code class="Code-In-Text--PACKT-">max_epochs</code> by computing <a id="_idIndexMarker675"/>the value of the hidden units:</p>
    <pre class="programlisting"><code class="hljs ada">        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-keyword">range</span>(max_epochs):
</code></pre>
    <p class="normal">The first phase is to focus on the hidden units. We activate the probabilities of the hidden units with our weight matrix using dot matrix multiplication:</p>
    <pre class="programlisting"><code class="hljs armasm">            pos_hidden_activations = np.dot(<span class="hljs-meta">data</span>, <span class="hljs-keyword">self.weights)
</span></code></pre>
    <p class="normal">Then, we apply the logistic function as we saw in <em class="italics">Chapter 2</em>, <em class="italics">Building a Reward Matrix – Designing Your Datasets</em>:</p>
    <pre class="programlisting"><code class="hljs armasm">            pos_hidden_probs = <span class="hljs-keyword">self._logistic(
</span>                pos_hidden_activations)
</code></pre>
    <p class="normal">The logistic function called is on line 63:</p>
    <pre class="programlisting"><code class="hljs ruby">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_logistic</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, x)</span></span>:
        <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1</span> + np.exp(-x))
</code></pre>
    <p class="normal">We set the biases to <code class="Code-In-Text--PACKT-">1</code>:</p>
    <pre class="programlisting"><code class="hljs yaml">            <span class="hljs-string">pos_hidden_probs[:,0]</span> <span class="hljs-string">=</span> <span class="hljs-number">1</span> <span class="hljs-comment"># Fix the bias unit.</span>
</code></pre>
    <p class="normal">We now have <a id="_idIndexMarker676"/>computed the first epoch of the <a id="_idIndexMarker677"/>probabilities of the hidden states with random weights.</p>
    <h3 id="_idParaDest-278" class="title">Random sampling of the hidden units for the reconstruction and contractive divergence</h3>
    <p class="normal">There are many sampling methods, such<a id="_idIndexMarker678"/> as Gibbs sampling, for example, which has a randomized approach to avoid deterministic samples.</p>
    <p class="normal">In this model, we will <a id="_idIndexMarker679"/>choose a random sample that chooses the values of the hidden probabilities that exceed the values of a random sample of values. The <code class="Code-In-Text--PACKT-">random.rand</code> function creates a random matrix with values between <code class="Code-In-Text--PACKT-">0</code> and <code class="Code-In-Text--PACKT-">1</code>, with a size of <code class="Code-In-Text--PACKT-">num_examples</code>×<code class="Code-In-Text--PACKT-">self.num_hidden+1</code>:</p>
    <pre class="programlisting"><code class="hljs gml">            pos_hidden_states = pos_hidden_probs &gt;
                np.<span class="hljs-built_in">random</span>.rand(num_examples, <span class="hljs-literal">self</span>.num_hidden + <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">This sample will be used for the <strong class="bold">reconstruction</strong> phase we will explore in the next section.</p>
    <p class="normal">We also need to compute an association for the <strong class="bold">contrastive divergence</strong> (the function used to update the weight matrix) phase, which is explained hereinunder:</p>
    <pre class="programlisting"><code class="hljs haskell">            pos_associations = np.dot(<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">T</span>, pos_hidden_probs)</span>
</code></pre>
    <p class="normal">This dot product of the visible data units <em class="italics">v</em> × the hidden units <em class="italics">h</em> can be represented as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_004.png" alt=""/></figure>
    <p class="normal">Now that the dot product has been implemented, we will build the reconstruction phase.</p>
    <h3 id="_idParaDest-279" class="title">Reconstruction</h3>
    <p class="normal">An RBM uses its input data as its <a id="_idIndexMarker680"/>training data, computes the hidden weights using a random weight matrix, and then <em class="italics">reconstructs</em> the visible units. Instead of an output layer as in other neural networks, an RBM reconstructs the visible units and compares them to the original data.</p>
    <p class="normal">The following code applies the same approach as for the hidden units described previously to generate visible units:</p>
    <pre class="programlisting"><code class="hljs crystal">            neg_visible_activations = np.dot(pos_hidden_states,
                <span class="hljs-keyword">self</span>.weights.T)
            neg_visible_probs = <span class="hljs-keyword">self</span>._logistic(
                neg_visible_activations)
            neg_visible_probs[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span> <span class="hljs-comment"># Fix the bias unit</span>
</code></pre>
    <p class="normal">These negative visible units will be used to evaluate the error level of the RBM, as explained here.</p>
    <p class="normal">Now that we have generated visible units with our sample of hidden unit states, we move on and generate the corresponding hidden states:</p>
    <pre class="programlisting"><code class="hljs nix">            <span class="hljs-attr">neg_hidden_activations</span> = np.dot(neg_visible_probs,
                self.weights)
            <span class="hljs-attr">neg_hidden_probs</span> = self._logistic(
                neg_hidden_activations)
            <span class="hljs-attr">neg_associations</span> = np.dot(neg_visible_probs.T,
                neg_hidden_probs)
</code></pre>
    <p class="normal">Note that <code class="Code-In-Text--PACKT-">neg_associations</code> can be represented in the following form:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_005.png" alt=""/></figure>
    <p class="normal">Here, we have done the following:</p>
    <ul>
      <li class="list">Computed positive hidden states using the visible units containing the training data</li>
      <li class="list">Selected a random sample of those positive hidden states</li>
      <li class="list">Reconstructed negative (generated from the hidden states, not the data) visible states</li>
      <li class="list">And, in turn, generated hidden states from the visible states produced</li>
    </ul>
    <p class="normal">We have <em class="italics">reconstructed</em> visible states through this process. However, we need to evaluate the result and update the weights.</p>
    <h3 id="_idParaDest-280" class="title">Contrastive divergence</h3>
    <p class="normal">To update the weights, we do not use gradient descent. In this energy model, we use contrastive divergence, which <a id="_idIndexMarker681"/>can be expressed as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_006.png" alt=""/></figure>
    <p class="normal">The letter <img src="../Images/B15438_14_007.png" alt=""/> is the learning rate. The learning rate should be a small value and can be optimized throughout the training process. I applied a small value, 0.001 overall.</p>
    <p class="normal">The source code for updating the weights is as follows:</p>
    <pre class="programlisting"><code class="hljs armasm">            <span class="hljs-keyword">self.weights </span>+= learning_rate * ((pos_associations -
                neg_associations))
</code></pre>
    <p class="normal">Over the epochs, the weights will adjust, bringing the energy and error level down and, hence, bringing the accuracy of the probabilities up.</p>
    <p class="normal">At this point, we will display the error level and the energy value of the RBM throughout the epochs.</p>
    <h3 id="_idParaDest-281" class="title">Error and energy function</h3>
    <p class="normal">On line 56, the error function<a id="_idIndexMarker682"/> calculates the squared sum of the difference between the visible units provided by the data and the reconstructed visible units:</p>
    <pre class="programlisting"><code class="hljs maxima">            <span class="hljs-built_in">error</span> = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>((data - neg_visible_probs) ** <span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">For the energy function, we can use <a id="_idIndexMarker683"/>our original energy equation:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_008.png" alt=""/></figure>
    <p class="normal">In our code, we will not use the biases since we often set them to <code class="Code-In-Text--PACKT-">1</code>.</p>
    <p class="normal">We will also need a function to measure the evolution of the energy of the RBM.</p>
    <p class="normal">The energy <a id="_idIndexMarker684"/>will be measured with a probabilistic function <em class="italics">p</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_009.png" alt=""/></figure>
    <p class="normal"><em class="italics">Z</em> is a <strong class="bold">partition function</strong><strong class="bold"><a id="_idIndexMarker685"/></strong> for making sure that the sum of the probabilities of each <em class="italics">x</em> input does not exceed 1:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_010.png" alt=""/></figure>
    <p class="normal">The partition function is the sum of all the individual probabilities of each <em class="italics">x</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_011.png" alt=""/></figure>
    <p class="normal">The corresponding code will calculate the energy of the RBM, which will decrease over time as the RBM goes through the epochs:</p>
    <pre class="programlisting"><code class="hljs maxima">            energy=-<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(data) - <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(neg_hidden_probs)-
                <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(pos_associations * self.weights)
            z=<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(data)+<span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(neg_hidden_probs)
            <span class="hljs-keyword">if</span> z&gt;<span class="hljs-number">0</span>: energy=<span class="hljs-built_in">np</span>.<span class="hljs-built_in">exp</span>(-energy)/z;
</code></pre>
    <p class="normal">You will note that neither the error function nor the energy function influences the training process. The training process is based on contrastive divergence.</p>
    <p class="normal">The error and energy values will measure the efficiency of the model by providing some insight into the behavior of the RBM as it trains.</p>
    <p class="normal">Here is an example of these measurement values at the beginning of the process and at the end:</p>
    <pre class="programlisting"><code class="hljs angelscript">Epoch <span class="hljs-number">0</span>: error <span class="hljs-keyword">is</span> <span class="hljs-number">8.936507744240409</span>  Energy: <span class="hljs-number">1586106430052073.0</span>
...
Epoch <span class="hljs-number">4999</span>: error <span class="hljs-keyword">is</span> <span class="hljs-number">4.498343290467705</span>  Energy: <span class="hljs-number">2.426792619597097e+46</span>
</code></pre>
    <p class="normal">At epoch 0, the error is high, and the energy is high, too.</p>
    <p class="normal">At epoch 4999, the error is sufficiently low for the model to produce correct feature extraction values. The energy has significantly diminished.</p>
    <h2 id="_idParaDest-282" class="title">Running the epochs and analyzing the results</h2>
    <p class="normal">Once the RBM has optimized<a id="_idIndexMarker686"/> the weight-bias matrix for <em class="italics">n</em> epochs, the matrix will provide the following information for the profiler system of person X, for example:</p>
    <pre class="programlisting"><code class="hljs angelscript">[[ <span class="hljs-number">0.91393138</span> <span class="hljs-number">-0.06594172</span> <span class="hljs-number">-1.1465728</span> ]
[ <span class="hljs-number">3.01088157</span> <span class="hljs-number">1.71400554</span> <span class="hljs-number">0.57620638</span>]
[ <span class="hljs-number">2.9878015</span> <span class="hljs-number">1.73764972</span> <span class="hljs-number">0.58420333</span>]
[ <span class="hljs-number">0.96733669</span> <span class="hljs-number">0.09742497</span> <span class="hljs-number">-3.26198615</span>]
[<span class="hljs-number">-1.09339128</span> <span class="hljs-number">-1.21252634</span> <span class="hljs-number">2.19432393</span>]
[ <span class="hljs-number">0.19740106</span> <span class="hljs-number">0.30175338</span> <span class="hljs-number">2.59991769</span>]
[ <span class="hljs-number">0.99232358</span> <span class="hljs-number">-0.04781768</span> <span class="hljs-number">-3.00195143</span>]]
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">An RBM model uses random values and will produce slightly different results each time it is trained.</p>
    </div>
    <p class="normal">The RBM will train the input and display the features added to X's profile.</p>
    <p class="normal">The weights of the features have been trained for person X. The first line is the bias and examines columns 2 and 3. The following six lines are the weights of X's features:</p>
    <pre class="programlisting"><code class="hljs angelscript">[[ <span class="hljs-number">0.913269</span> <span class="hljs-number">-0.06843517</span> <span class="hljs-number">-1.13654324</span>]
[ <span class="hljs-number">3.00969897</span> <span class="highlight"><span class="hljs-number">1.70999493</span></span> <span class="highlight"><span class="hljs-number">0.58441134</span></span>]
[ <span class="hljs-number">2.98644016</span> <span class="highlight"><span class="hljs-number">1.73355337</span></span> <span class="highlight"><span class="hljs-number">0.59234319</span></span>]
[ <span class="hljs-number">0.953465</span> <span class="highlight"><span class="hljs-number">0.08329804</span></span> <span class="highlight"><span class="hljs-number">-3.26016158</span></span>]
[<span class="hljs-number">-1.10051951</span> <span class="highlight"><span class="hljs-number">-1.2227973</span></span> <span class="highlight"><span class="hljs-number">2.21361701</span></span>]
[ <span class="hljs-number">0.20618461</span> <span class="highlight"><span class="hljs-number">0.30940653</span></span> <span class="highlight"><span class="hljs-number">2.59980058</span></span>]
[ <span class="hljs-number">0.98040128</span> <span class="hljs-number">-0.06023325</span> <span class="hljs-number">-3.00127746</span>]]
</code></pre>
    <p class="normal">The weights (in bold) are lines 2 to 6 and columns 2 to 3. The first line and first column are the biases.</p>
    <p class="normal">The way to interpret the weights of an RBM remains a careful strategy to build. In this case, a creative approach is experimented with to determine marketing behavior. There are many other uses of an RBM, such as image processing, for example. In this case, the weight matrix will provide a profile of X by summing the weight lines of the feature, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs angelscript">    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> range(<span class="hljs-number">7</span>):
        <span class="hljs-keyword">if</span>(w&gt;<span class="hljs-number">0</span>):
            W=print(F[w<span class="hljs-number">-1</span>],<span class="hljs-string">":"</span>,r.weights[w,<span class="hljs-number">1</span>]+r.weights[w,<span class="hljs-number">2</span>])
</code></pre>
    <p class="normal">The features are now labeled, as displayed in this output:</p>
    <pre class="programlisting"><code class="hljs yaml"><span class="hljs-attr">love :</span> <span class="hljs-number">2.25265339223</span>
<span class="hljs-attr">happiness :</span> <span class="hljs-number">2.28398311347</span>
<span class="hljs-attr">family :</span> <span class="hljs-number">-3.16621250031</span>
<span class="hljs-attr">horizons :</span> <span class="hljs-number">0.946830830963</span>
<span class="hljs-attr">action :</span> <span class="hljs-number">2.88757989766</span>
<span class="hljs-attr">violence :</span> <span class="hljs-number">-3.05188501936</span>
<span class="hljs-string">A</span> <span class="hljs-string">value&gt;0</span> <span class="hljs-string">is</span> <span class="hljs-string">positive,</span> <span class="hljs-string">close</span> <span class="hljs-string">to</span> <span class="hljs-number">0</span> <span class="hljs-string">slightly</span> <span class="hljs-string">positive</span>
<span class="hljs-string">A</span> <span class="hljs-string">value&lt;0</span> <span class="hljs-string">is</span> <span class="hljs-string">negative,</span> <span class="hljs-string">close</span> <span class="hljs-string">to</span> <span class="hljs-number">0</span> <span class="hljs-string">slightly</span> <span class="hljs-string">negative</span>
</code></pre>
    <p class="normal">We can see that beyond <a id="_idIndexMarker687"/>standard movie classifications, X likes horizons somewhat, does not like violence, and likes action. X finds happiness and love important, but not family at this point.</p>
    <p class="normal">The RBM has provided a personal profile of X—not a prediction, but getting ready for a suggestion through a chatbot or just building X's machine mind-dataset.</p>
    <p class="normal">We have taken a dataset and extracted the main features from it using an RBM. The next step will be to use the weights as feature vectors for PCA.</p>
    <h1 id="_idParaDest-283" class="title">Using the weights of an RBM as feature vectors for PCA</h1>
    <p class="normal">In this section, we will be <a id="_idIndexMarker688"/>writing an enhanced version of <code class="Code-In-Text--PACKT-">RBM_01.py</code>. <code class="Code-In-Text--PACKT-">RBM_01.py</code> produces the feature vector of one viewer named X. The goal now is to extract the features of 12,000 viewers, for example, to have a sufficient number of feature vectors for PCA.</p>
    <p class="normal">In <code class="Code-In-Text--PACKT-">RBM_01.py</code>, viewer X's favorite movies were first provided in a matrix. The goal now is to produce a random sample of 12,000 viewer vectors.</p>
    <p class="normal">The first task at hand is to create an RBM launcher to run the RBM 12,000 times to simulate a random choice of viewers and their favorite movies, which are the ones the viewer liked. Then, the feature vector of each viewer will be stored.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">RBM_launcher.py</code> first imports RBM as <code class="Code-In-Text--PACKT-">rp</code>:</p>
    <pre class="programlisting"><code class="hljs elm"><span class="hljs-keyword">import</span> RBM <span class="hljs-keyword">as</span> rp
</code></pre>
    <p class="normal">The primary goal of <code class="Code-In-Text--PACKT-">RBM_launcher.py</code> is to carry out the basic functions to run RBM. Once <code class="Code-In-Text--PACKT-">RBM</code> is imported, the feature vector's <code class="Code-In-Text--PACKT-">.tsv</code> file is created:</p>
    <pre class="programlisting"><code class="hljs livecodeserver"><span class="hljs-comment">#Create feature files</span>
f=<span class="hljs-built_in">open</span>(<span class="hljs-string">"features.tsv"</span>,<span class="hljs-string">"w+"</span>)
f.<span class="hljs-built_in">close</span>
</code></pre>
    <p class="normal">When <code class="Code-In-Text--PACKT-">rp</code>, the <code class="Code-In-Text--PACKT-">RBM</code> function imported as <code class="Code-In-Text--PACKT-">rp</code>, is called, it will append the feature file.</p>
    <p class="normal">The next step is to create the label file containing the metadata:</p>
    <pre class="programlisting"><code class="hljs livecodeserver">g=(<span class="hljs-string">"viewer_name"</span>+<span class="hljs-string">"\t"</span>+<span class="hljs-string">"primary_emotion"</span>+<span class="hljs-string">"\t"</span>+<span class="hljs-string">"secondary_emotion"</span>+
    <span class="hljs-string">"\n"</span>)
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"labels.tsv"</span>, <span class="hljs-string">"w"</span>) <span class="hljs-keyword">as</span> f:
    f.<span class="hljs-built_in">write</span>(g)
</code></pre>
    <p class="normal">You will notice the use of the word "emotion." In this context, "emotion" refers to features for sentiment analysis in general, not human emotions in particular. Please read "emotions" in this context as sentiment analysis features.</p>
    <p class="normal">Now, we are ready to <a id="_idIndexMarker689"/>run RBM 12,000+ times, for example:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Run the RBM feature detection program over v viewers</span>
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"RBM start"</span>)
<span class="hljs-attribute">vn</span>=12001
<span class="hljs-attribute">c</span>=0
<span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> range (0,vn):
    <span class="highlight">rp.main()</span>
    c+=1
    <span class="hljs-keyword">if</span>(<span class="hljs-attribute">c</span>==1000):print(v+1);c=0;
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"RBM over"</span>)
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">rp.main()</code> calls the <code class="Code-In-Text--PACKT-">main()</code> function in <code class="Code-In-Text--PACKT-">RBM.py</code> that we will now enhance for this process.</p>
    <p class="normal">We will enhance <code class="Code-In-Text--PACKT-">RBM_01.py</code> step-by-step in another file named <code class="Code-In-Text--PACKT-">RBM.py</code>. We will adapt the code starting line 65 to create an RBM launcher option:</p>
    <p class="normal">A variable name <code class="Code-In-Text--PACKT-">pt</code> is set to <code class="Code-In-Text--PACKT-">0</code> or <code class="Code-In-Text--PACKT-">1</code>, depending on whether we wish to display intermediate information:</p>
    <pre class="programlisting"><code class="hljs angelscript">    # RBM_launcher option
    pt=<span class="hljs-number">0</span>  #restricted printing(<span class="hljs-number">0</span>), printing(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Since this is an automatic process, <code class="Code-In-Text--PACKT-">pt</code> is set to <code class="Code-In-Text--PACKT-">0</code>.</p>
    <p class="normal">The metadata of 10 movies is stored in <code class="Code-In-Text--PACKT-">titles</code>:</p>
    <pre class="programlisting"><code class="hljs clean">    # Part I Feature extractions <span class="hljs-keyword">from</span> data sources
    # The titles <span class="hljs-keyword">of</span> <span class="hljs-number">10</span> movies
    titles=[<span class="hljs-string">"24H in Kamba"</span>,<span class="hljs-string">"Lost"</span>,<span class="hljs-string">"Cube Adventures"</span>,
            <span class="hljs-string">"A Holiday"</span>,<span class="hljs-string">"Jonathan Brooks"</span>,
             <span class="hljs-string">"The Melbourne File"</span>, <span class="hljs-string">"WNC Detectives"</span>,
             <span class="hljs-string">"Stars"</span>,<span class="hljs-string">"Space L"</span>,<span class="hljs-string">"Zone 77"</span>]
</code></pre>
    <p class="normal">A feature matrix of movies with six features per movie is created starting at line 71, with the same features as in <code class="Code-In-Text--PACKT-">RBM_01.py</code>:</p>
    <pre class="programlisting"><code class="hljs angelscript">    # The feature map of each of the <span class="hljs-number">10</span> movies. Each line <span class="hljs-keyword">is</span> a movie.
    # Each column <span class="hljs-keyword">is</span> a feature. There are <span class="hljs-number">6</span> features: [<span class="hljs-string">'love'</span>, <span class="hljs-string">'happiness'</span>, <span class="hljs-string">'family'</span>, <span class="hljs-string">'horizons'</span>, <span class="hljs-string">'action'</span>, <span class="hljs-string">'violence'</span>]
    # <span class="hljs-number">1</span>= the feature <span class="hljs-keyword">is</span> activated, <span class="hljs-number">0</span>= the feature <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> activated
    movies_feature_map = np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],
                                   [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])
</code></pre>
    <p class="normal">Each line of the matrix<a id="_idIndexMarker690"/> contains a movie, and each column one of the six features of that movie. If the value is <code class="Code-In-Text--PACKT-">0</code>, the feature is not present; if the value is <code class="Code-In-Text--PACKT-">1</code>, the feature is present.</p>
    <p class="normal">In the years to come, the number of features per movie will be extended to an indefinite number of features per movie to fine-tune our preferences.</p>
    <p class="normal">An empty output matrix is created. In <code class="Code-In-Text--PACKT-">RBM_01.py</code>, the result was provided. In this example, it will be filled with random choices:</p>
    <pre class="programlisting"><code class="hljs angelscript">    #The output matrix <span class="hljs-keyword">is</span> empty before the beginning of the analysis
    #The program will take the user <span class="hljs-string">"likes"</span> of <span class="hljs-number">6</span> <span class="hljs-keyword">out</span> of the <span class="hljs-number">10</span> movies
    
    dialog_output = np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                              [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                              [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                              [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                              [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],
                              [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]])
</code></pre>
    <p class="normal">Now, the random movie selector will generate likes or dislikes per movie and per viewer:</p>
    <pre class="programlisting"><code class="hljs elixir">    <span class="hljs-comment">#An extraction of viewer's first 6 liked 6 movies out n choices</span>
    <span class="hljs-comment">#Hundreds of movies can be added. No dialog is needed since a cloud streaming services stores the movie-likes we click on</span>
    mc=0   <span class="hljs-comment">#Number of choices limited to 6 in this example</span>
    a=<span class="hljs-string">"no"</span> <span class="hljs-comment">#default input value if rd==1</span>
    <span class="hljs-comment">#for m in range(0,10):</span>
    if pt==<span class="hljs-number">1</span><span class="hljs-symbol">:print</span>(<span class="hljs-string">"Movie likes:"</span>);
    <span class="hljs-keyword">while</span> mc&lt;<span class="hljs-number">6</span>:
        m=randint(0,<span class="hljs-number">9</span>)<span class="hljs-comment"># filter a chosen movie or allow (this case) a person can watch and like a movie twice=an indication</span>
        b=randint(0,<span class="hljs-number">1</span>)<span class="hljs-comment"># a person can like(dislike) a movie the first time and not the second(or more) time</span>
        if mc&lt;<span class="hljs-number">6</span> <span class="hljs-keyword">and</span> (a==<span class="hljs-string">"yes"</span> <span class="hljs-keyword">or</span> b==<span class="hljs-number">1</span>):
            if pt==<span class="hljs-number">1</span><span class="hljs-symbol">:print</span>(<span class="hljs-string">"title likes: "</span>,titles[m]);
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(0,<span class="hljs-number">6</span>)<span class="hljs-symbol">:dialog_output</span>[mc,i]=
                movies_feature_map[m,i];
            mc+=<span class="hljs-number">1</span>
        if mc&gt;=<span class="hljs-number">6</span>:
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">We can choose whether to display the input:</p>
    <pre class="programlisting"><code class="hljs mipsasm">    <span class="hljs-comment">#The dialog_input is now complete</span>
    if pt==<span class="hljs-number">1</span>:print(<span class="hljs-string">"dialog output"</span>,<span class="hljs-keyword">dialog_output);
</span></code></pre>
    <p class="normal">The dialog output is the data collected by the platform through its like/dislike interface. The RBM runs its training session:</p>
    <pre class="programlisting"><code class="hljs nix">    <span class="hljs-comment">#dialog_output= the training data</span>
    <span class="hljs-attr">training_data=dialog_output</span>
    <span class="hljs-attr">r</span> = RBM(<span class="hljs-attr">num_visible</span> = <span class="hljs-number">6</span>, <span class="hljs-attr">num_hidden</span> = <span class="hljs-number">2</span>)
    <span class="hljs-attr">max_epochs=5000</span>
    <span class="hljs-attr">learning_rate=0.001</span>
    r.train(training_data, max_epochs,learning_rate)
</code></pre>
    <p class="normal">The results of the <a id="_idIndexMarker691"/>RBM training session are now processed from line 185 to line 239 to transform the weights obtained into feature vectors and the corresponding metadata:</p>
    <pre class="programlisting"><code class="hljs r"><span class="hljs-comment">###Processing the results</span>
    <span class="hljs-comment"># feature labels</span>
    <span class="hljs-literal">F</span>=[<span class="hljs-string">"love"</span>,<span class="hljs-string">"happiness"</span>,<span class="hljs-string">"family"</span>,<span class="hljs-string">"horizons"</span>,<span class="hljs-string">"action"</span>,<span class="hljs-string">"violence"</span>]
    <span class="hljs-keyword">...</span>/<span class="hljs-keyword">...</span>
        control=[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">6</span>):
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">6</span>):
                control[i]+=dialog_output[j][i]
    <span class="hljs-comment">###End of processing the results</span>
</code></pre>
    <p class="normal">The goal is now to select the primary feature of a given movie chosen by a given viewer. This feature could be "love" or "violence" for example:</p>
    <pre class="programlisting"><code class="hljs routeros">    #Selection of the primary feature
    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> range(1,7):
        <span class="hljs-keyword">if</span>(w&gt;0):
            <span class="hljs-keyword">if</span> <span class="hljs-attribute">pt</span>==1:print(F[w-1],<span class="hljs-string">":"</span>,r.weights[w,0]);
            <span class="hljs-attribute">tw</span>=r.weights[w,0]+pos
            <span class="hljs-keyword">if</span>(tw&gt;best1):
                <span class="hljs-attribute">f1</span>=w-1
                <span class="hljs-attribute">best1</span>=tw
            f.write(str(r.weights[w,0]+pos)+<span class="hljs-string">"\t"</span>)
    f.write(<span class="hljs-string">"\n"</span>)
    f.close()
</code></pre>
    <p class="normal">The secondary feature is also very interesting. It often provides more information than the primary feature. A viewer will tend to view a certain type of movie. However, the secondary features vary <a id="_idIndexMarker692"/>from movie to movie. For example, suppose a young viewer likes action movies. "Violence" could be the primary feature, but the secondary feature could be "love" in one case or "family" in another. The secondary featured is stored in the feature vector of this viewer:</p>
    <pre class="programlisting"><code class="hljs routeros">    #secondary feature
    <span class="hljs-attribute">best2</span>=-1000
    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> range(1,7):
        <span class="hljs-keyword">if</span>(w&gt;0):
            <span class="hljs-attribute">tw</span>=r.weights[w,0]+pos
            <span class="hljs-keyword">if</span>(tw&gt;best2 <span class="hljs-keyword">and</span> w-1!=f1):
                <span class="hljs-attribute">f2</span>=w-1
                <span class="hljs-attribute">best2</span>=tw
</code></pre>
    <p class="normal">The metadata is saved in the label file:</p>
    <pre class="programlisting"><code class="hljs routeros">    #saving the metadata with the labels
    <span class="hljs-attribute">u</span>=randint(1,10000)
    <span class="hljs-attribute">vname</span>=<span class="hljs-string">"viewer_"</span>+str(u)
    <span class="hljs-keyword">if</span>(<span class="hljs-attribute">pt</span>==1):
        <span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Control"</span>,control)
        <span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Principal Features: "</span>,vname,f1,f2,<span class="hljs-string">"control"</span>)
    
    f= open(<span class="hljs-string">"labels.tsv"</span>,<span class="hljs-string">"a"</span>)
    f.write(vname +<span class="hljs-string">"\t"</span>+F[f1]+<span class="hljs-string">"\t"</span>+F[f2]+<span class="hljs-string">"\t"</span>)
    f.write(<span class="hljs-string">"\n"</span>)
    f.close()
</code></pre>
    <p class="normal">This process will be repeated 12,000 times in this example.</p>
    <p class="normal">The feature vector <code class="Code-In-Text--PACKT-">features.tsv</code> file has been created:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.4: The feature vector file</p>
    <p class="normal">The feature vector <code class="Code-In-Text--PACKT-">labels.tsv</code> metadata file matches the feature vector file:</p>
    <pre class="programlisting"><code class="hljs properties"><span class="hljs-attr">viewer_name</span>    <span class="hljs-string">primary_emotion    secondary_emotion</span>
<span class="hljs-attr">viewer_8481</span>    <span class="hljs-string">love               violence</span>
<span class="hljs-attr">viewer_3568</span>    <span class="hljs-string">love               violence</span>
<span class="hljs-attr">viewer_8667</span>    <span class="hljs-string">love               horizons</span>
<span class="hljs-attr">viewer_2730</span>    <span class="hljs-string">love               violence</span>
<span class="hljs-attr">viewer_3970</span>    <span class="hljs-string">love               horizons</span>
<span class="hljs-attr">viewer_1140</span>    <span class="hljs-string">love               happiness</span>
</code></pre>
    <p class="normal">You will note that "love" and "violence" appear often. This comes from the way I built the dataset<a id="_idIndexMarker693"/> based mostly on movies that contain action and some form of warm relationship between the characters, which is typical in movies for the younger generations.</p>
    <p class="normal">Now that the feature vectors and the metadata file have been created, we can use PCA to represent the points.</p>
    <h2 id="_idParaDest-284" class="title">Understanding PCA</h2>
    <p class="normal">PCA <a id="_idIndexMarker694"/>is applied very efficiently to marketing by Facebook, Amazon, Google, Microsoft, IBM, and many other corporations, among other feature processing algorithms.</p>
    <p class="normal">Probabilistic machine learning training remains efficient when targeting apparel, food, books, music, travel, cars, and other market consumer segments.</p>
    <p class="normal">However, humans are not just consumers; they are human beings. When they contact websites or call centers, standard answers or stereotyped emotional tone analysis approaches can depend on one's nerves. When humans are in contact with doctors, lawyers, and other professional services, a touch of humanity is necessary if major personal crises occur.</p>
    <p class="normal">The goal of the PCA, in this context, is to extract key features to describe an individual or a population. The PCA phase will help us build a mental representation of X's profile, either to communicate with X or use X's mind as a powerful, <em class="italics">mindful</em> chatbot or decision-maker.</p>
    <p class="normal">PCA isn't a simple concept, so let's take some time to understand it properly. We'll start with an intuitive explanation, and after that, we'll get into the mathematics behind it.</p>
    <p class="normal">PCA takes data and represents it at a higher level.</p>
    <p class="normal">For example, imagine you are in your bedroom. You have some books, magazines, and music (maybe on your smartphone) around the room. If you consider your room as a 3D Cartesian coordinate system, the objects in your room are all in specific <em class="italics">x</em>, <em class="italics">y</em>, <em class="italics">z</em> coordinates.</p>
    <p class="normal">For experimentation <a id="_idIndexMarker695"/>purposes, take your favorite objects and put them on your bed. Put the objects you like the most near one another, and your second choices a bit further away. If you imagine your bed as a 2D Cartesian space, you have just made your objects change dimensions. You have brought the objects that you value the most into a higher dimension. They are now more visible than the ones that have less value for you.</p>
    <p class="normal">They are not in their usual place anymore; they are on your bed and at specific coordinates depending on your taste.</p>
    <p class="normal">That is the philosophy of PCA. If the number of data points in the dataset is very large, the PCA of a "mental dataset" of one person will always be different from the PCA representation of another person, like DNA. A "mental dataset" is a collection of thoughts, images, words, and feelings of a given person. It is more than the classic age, gender, income, and other neutral features. A "mental dataset" will take us inside somebody's mind.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">That is what a conceptual representation learning metamodel (CRLMM) is about as applied to a person's mental representation. Each person is different, and each person deserves a customized chatbot or bot treatment.</p>
    </div>
    <h3 id="_idParaDest-285" class="title">Mathematical explanation</h3>
    <p class="normal">The main steps in calculating PCA are important for understanding how to go from the intuitive approach to how TensorBoard Embedding Projector represents datasets using PCA.</p>
    <h4 class="title">Variance</h4>
    <p class="normal">Variance<a id="_idIndexMarker696"/> is when a value changes. For example, as the sun rises in summer, the temperature gets warmer and warmer. The variance is represented by the difference between the temperature at a given hour and then the temperature a few hours later. Covariance is when two variables change together. For example, the hotter it gets when we are outside, the more we will sweat to cool our bodies down.</p>
    <ul>
      <li class="list"><strong class="bold">Step 1</strong>: Calculate the mean of the array <code class="Code-In-Text--PACKT-">data1</code>. You can check this with <code class="Code-In-Text--PACKT-">mathfunction.py</code>, as shown in the following function:
        <pre class="programlisting"><code class="hljs lsl">data1 = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]
M1=statistics.mean(data1)
print(<span class="hljs-string">"Mean data1"</span>,M1)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The answer is 2.5. The mean is not the median (the middle value of an array).</p>
      </li>
      <li class="list"><strong class="bold">Step 2</strong>: Calculate the<a id="_idIndexMarker697"/> mean of array <code class="Code-In-Text--PACKT-">data2</code>. The mean calculation is executed with the following standard function:
        <pre class="programlisting"><code class="hljs lsl">data2 = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]
M2=statistics.mean(data2)
print(<span class="hljs-string">"Mean data2"</span>,M2)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The answer is <img src="../Images/B15438_14_012.png" alt=""/>. The bar above the <em class="italics">X</em> signifies that it is a mean.</p>
      </li>
      <li class="list"><strong class="bold">Step 3</strong>: Calculate the variance using the following equation:</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B15438_14_013.png" alt=""/></figure>
    <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Now, NumPy will calculate the variance with the absolute value of each <em class="italics">x</em> minus the mean, sum them up, and divide the sum by <em class="italics">n</em>, as shown in the following code snippet:</p>
    <pre class="programlisting"><code class="hljs maxima">#<span class="hljs-built_in">var</span> = <span class="hljs-built_in">mean</span>(<span class="hljs-built_in">abs</span>(x - x.<span class="hljs-built_in">mean</span>())**<span class="hljs-number">2</span>).
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Variance 1"</span>, <span class="hljs-built_in">np</span>.<span class="hljs-built_in">var</span>(data1))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Variance 2"</span>, <span class="hljs-built_in">np</span>.<span class="hljs-built_in">var</span>(data2))
</code></pre>
    <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">Some variances are calculated with <em class="italics">n</em> – 1 depending on the population of the dataset.</p>
    <p class="normal">The result of the program for variances is as displayed in the following output:</p>
    <pre class="programlisting"><code class="hljs angelscript">Mean data1 <span class="hljs-number">2.5</span>
Mean data2 <span class="hljs-number">2.75</span>
Variance <span class="hljs-number">1</span> <span class="hljs-number">1.25</span>
Variance <span class="hljs-number">2</span> <span class="hljs-number">2.1875</span>
</code></pre>
    <p class="normal">We can already see that <code class="Code-In-Text--PACKT-">data2</code> varies a lot more than <code class="Code-In-Text--PACKT-">data1</code>. Do they fit together? Are their variances close or not? Do they vary in the same way? Our goal in the following section is to find out whether two words, for example, will often be found together or close to one another, taking the output of the embedding program into account.</p>
    <h4 class="title">Covariance</h4>
    <p class="normal">The covariance<a id="_idIndexMarker698"/> will tell us whether these datasets vary together or not. The equation follows the same philosophy as variance, but now both variances are joined to see whether they belong together:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_014.png" alt=""/></figure>
    <p class="normal">As with the variance, the denominator can be <em class="italics">n</em> – 1 depending on your model. Also, in this equation, the numerator is expanded to visualize the co-part of covariance, as implemented in the following array in <code class="Code-In-Text--PACKT-">mathfunction.py</code>:</p>
    <pre class="programlisting"><code class="hljs angelscript">x=np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>]])
a=np.cov(x)
print(a)
</code></pre>
    <p class="normal">NumPy's output is a covariance matrix, <code class="Code-In-Text--PACKT-">a</code>:</p>
    <pre class="programlisting"><code class="hljs json">[[<span class="hljs-number">1.66666667</span> <span class="hljs-number">2.16666667</span>]
 [<span class="hljs-number">2.16666667</span> <span class="hljs-number">2.91666667</span>]]
</code></pre>
    <p class="normal">If you increase some of the values of the dataset, it will increase the value of the parts of the matrix. If you decrease some of the values of the dataset, the elements of the covariance matrix will decrease.</p>
    <p class="normal">Looking at some of the elements of the matrix increase or decrease that way takes time and observation. What if we could find one or two values that would give us that information?</p>
    <h4 class="title">Eigenvalues and eigenvectors</h4>
    <p class="normal">To make sense of the covariance matrix, the eigenvector will point to the direction in which the covariances are going. The eigenvalues<a id="_idIndexMarker699"/> will express the magnitude or importance of a given feature.</p>
    <p class="normal">To sum it up, an eigenvector will provide the direction and the eigenvalue of the importance for the covariance matrix, <code class="Code-In-Text--PACKT-">a</code>. With those results, we will be able to represent the PCA with TensorBoard Embedding Projector in a multidimensional space.</p>
    <p class="normal">Let <code class="Code-In-Text--PACKT-">w</code> be an eigenvalue(s) of <code class="Code-In-Text--PACKT-">a</code>. An eigenvalue(s) must satisfy the following equation:</p>
    <pre class="programlisting"><code class="hljs gcode">dot<span class="hljs-comment">(a,v)</span>=w * v
</code></pre>
    <p class="normal">There must exist a vector, <code class="Code-In-Text--PACKT-">v</code>, for which <code class="Code-In-Text--PACKT-">dot(a,v)</code> is the same as <code class="Code-In-Text--PACKT-">w*v</code>.</p>
    <p class="normal">NumPy will do the math through the following function:</p>
    <pre class="programlisting"><code class="hljs coffeescript"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> linalg <span class="hljs-keyword">as</span> LA
w, v = LA.eigh(a)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"eigenvalue(s)"</span>,w)
</code></pre>
    <p class="normal">The eigenvalues are<a id="_idIndexMarker700"/> displayed (in ascending order) in the following output:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">eigenvalue</span><span class="hljs-params">(s)</span></span> [<span class="hljs-number">0.03665681</span> <span class="hljs-number">4.54667652</span>]
</code></pre>
    <p class="normal">Now, we need the eigenvectors<a id="_idIndexMarker701"/> to see in which direction these values should be applied. NumPy provides a function to calculate both the eigenvalues and eigenvectors together. That is because eigenvectors are calculated using the eigenvalues of a matrix, as shown in this code snippet:</p>
    <pre class="programlisting"><code class="hljs stylus">from numpy import linalg as LA
w, v = LA.eigh(a)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"eigenvalue(s)"</span>,w)</span></span>
<span class="highlight"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"eigenvector(s)"</span>,v)</span></span></span>
</code></pre>
    <p class="normal">The output of the program is as follows:</p>
    <pre class="programlisting"><code class="hljs lua">eigenvector(s) <span class="hljs-string">[[-0.79911221  0.6011819 ]
 [ 0.6011819   0.79911221]]</span>
</code></pre>
    <p class="normal">Eigenvalues come in a 1D array with the eigenvalues of <code class="Code-In-Text--PACKT-">a</code>.</p>
    <p class="normal">Eigenvectors come in a 2D square array with the corresponding value (for each eigenvalue) in columns.</p>
    <h4 class="title">Creating the feature vector</h4>
    <p class="normal">The remaining step <a id="_idIndexMarker702"/>is to sort the eigenvalues from the highest to the lowest value. The highest eigenvalue will provide the principal component (most important). The eigenvector that goes with it will be its feature vector. You can choose to ignore the lowest values or features. In the dataset, there will be hundreds, and often thousands, of features to represent. Now we have the feature vector:</p>
    <p class="center">feature vector = FV = {eigenvector<sub>1</sub>, eigenvector<sub>2</sub> … <em class="italics">n</em>}</p>
    <p class="normal"><em class="italics">n</em> means that there could be many more features to transform into a PCA feature vector.</p>
    <h4 class="title">Deriving the dataset</h4>
    <p class="normal">The final step is to <a id="_idIndexMarker703"/>transpose the feature vector and original dataset and multiply the row feature vector by row data:</p>
    <p class="center">Data that will be displayed = row of feature vector * row of data</p>
    <h4 class="title">Summing it up</h4>
    <p class="normal">The highest value of eigenvalues is the principal component. The eigenvector will determine in which direction the data points will be oriented when multiplied by that vector.</p>
    <h2 id="_idParaDest-286" class="title">Using TensorFlow's Embedding Projector to represent PCA</h2>
    <p class="normal">TensorBoard Embedding <a id="_idIndexMarker704"/>Projector offers an in-built PCA function<a id="_idIndexMarker705"/> that can be rapidly configured to fit our needs. TensorBoard can be called as a separate program or embedded in a program as we saw in <em class="italics">Chapter 13</em>, <em class="italics">Visualizing Networks with TensorFlow 2.x and TensorBoard</em>.</p>
    <p class="normal">We will then extract key information on the viewer marketing segment that will be used to start building a chatbot in <em class="italics">Chapter 15</em>, <em class="italics">Setting Up a Cognitive NLP UI/CUI Chatbot</em>.</p>
    <p class="normal">First, go to this link: <a href="https://projector.tensorflow.org/"><span class="url">https://projector.tensorflow.org/</span></a></p>
    <p class="normal">For the following functions, bear in mind that TensorBoard Embedding Projector is working at each step and that it might take some time depending on your machine. We load the data produced by <code class="Code-In-Text--PACKT-">RBM_launcher.py</code> and <code class="Code-In-Text--PACKT-">RBM.py</code> by clicking on <strong class="screen-text">Load</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.5: The Load button</p>
    <p class="normal">Once the <strong class="screen-text">Load data from your computer</strong> windows appear, we load the feature vector <code class="Code-In-Text--PACKT-">features.tsv</code> file by clicking on <strong class="screen-text">Choose file</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.6: Loading the feature vector file</p>
    <p class="normal">We load<a id="_idIndexMarker706"/> the <code class="Code-In-Text--PACKT-">labels.tsv</code> metadata file by <a id="_idIndexMarker707"/>clicking on <strong class="screen-text">Choose file</strong> in <strong class="screen-text">Step 2 (optional)</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.7: Loading a TSV file</p>
    <p class="normal">To obtain a good representation of our 12,000+ features, click on <strong class="screen-text">Sphereize data,</strong> which is not checked in default mode:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.8: Sphereizing data</p>
    <p class="normal">We <a id="_idIndexMarker708"/>now choose label by <strong class="screen-text">secondary_emotion</strong>, color by <strong class="screen-text">secondary_emotion</strong>, along with edit by <strong class="screen-text">secondary_emotion</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.9: Managing labels</p>
    <p class="normal">To get a nice view of the data, we activate night mode so that the moon should be active:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.10: Activating the night mode</p>
    <p class="normal">At this point, we have a nice PCA representation that turns like Earth, the ideas in our mind, or the minds of all of the viewers we are analyzing depending on how we use the features. The dots on the image are datapoints representing the features we calculated with an RBM and then represented with an image using PCA. It is like peeking inside the mind:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.11: A PCA representation of features</p>
    <p class="normal">The<a id="_idIndexMarker709"/> PCA representation of the features of a given <a id="_idIndexMarker710"/>person or a group of people provides vital information to create dialogs in a chatbot. Let's analyze the PCA to prepare data for a chatbot.</p>
    <h2 id="_idParaDest-287" class="title">Analyzing the PCA to obtain input entry points for a chatbot</h2>
    <p class="normal">The goal is to gather some information to get started <a id="_idIndexMarker711"/>with our cognitive chatbot. We will use the filters provided by TensorBoard.</p>
    <p class="normal">Choose <strong class="screen-text">secondary_emotion</strong> as the basis of our filters:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.12: Filtering data</p>
    <p class="normal">The features we are<a id="_idIndexMarker712"/> analyzing are as follows:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">F</span>=[<span class="hljs-string">"love"</span>,<span class="hljs-string">"happiness"</span>,<span class="hljs-string">"family"</span>,<span class="hljs-string">"horizons"</span>,<span class="hljs-string">"action"</span>,<span class="hljs-string">"violence"</span>]
</code></pre>
    <p class="normal">We need to see the statistics per feature.</p>
    <p class="normal">We type the feature in TensorBoard's search option, such as "love," for example, and then we click the down arrow:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.13: TensorBoard's search option</p>
    <p class="normal">The PCA representation changes its view in realtime:</p>
    <figure class="mediaobject"><img src="../Images/B15438_14_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.14: PCA representation of the RBM features</p>
    <p class="normal">There are 643 <a id="_idIndexMarker713"/>points for "love." Notice that the "love" points are grouped in a relatively satisfactory way. They are mostly in the same area of the image and not spread out all over the image. This grouping shows that the weights of the RBM provided features that turned out to be sufficiently correct in the PCA for this experiment.</p>
    <p class="normal">We repeat the process for each feature, to obtain the number of points per feature and visualize them. For the dataset supplied on GitHub for this chapter, we obtain:</p>
    <ul>
      <li class="list">Love: 643</li>
      <li class="list">Happiness: 2267</li>
      <li class="list">Family: 0</li>
      <li class="list">Horizons: 1521</li>
      <li class="list">Action: 2976</li>
      <li class="list">Violence: 4594<div class="note">
          <p class="Information-Box--PACKT-"><strong class="bold">Important</strong>: This result will naturally change if <code class="Code-In-Text--PACKT-">RBM_launcher.py</code> runs again since it's a random viewer-movie choice process.</p>
        </div>
      </li>
    </ul>
    <p class="normal">The results provide interesting information on the marketing segment we are targeting for the chatbot:</p>
    <ul>
      <li class="list">Violence and action point to action movies.</li>
      <li class="list">Family=0 points to younger viewers; teenagers, for example, more interested in action than creating a family.</li>
      <li class="list">Discovering <a id="_idIndexMarker714"/>happiness and love are part of the horizons they are looking for.</li>
    </ul>
    <p class="normal">This is typical of superhero series and movies. Superheroes are often solitary individuals.</p>
    <p class="normal">We will see how this works out when we build our chatbot in <em class="italics">Chapter 15</em>, <em class="italics">Setting Up a Cognitive NLP UI/CUI Chatbot</em>.</p>
    <h1 id="_idParaDest-288" class="title">Summary</h1>
    <p class="normal">In this chapter, we prepared key information to create the input dialog of a chatbot. Using the weights of an RBM as features constituted the first step. We saw that we could use neural networks to extract features from datasets and represent them using the optimized weights.</p>
    <p class="normal">Processing the likes/dislikes of a movie viewer reveals the features of the movies that, in turn, provide a mental representation of a marketing segment.</p>
    <p class="normal">PCA chained to an RBM will generate a vector space that can be viewed in TensorBoard Embedding Projector in a few clicks.</p>
    <p class="normal">Once TensorBoard was set up, we analyzed the statistics to understand the marketing segment the dataset originated from. By listing the points per feature, we found the main features that drove this marketing segment.</p>
    <p class="normal">Having discovered some of the key features of the marketing segment we were analyzing, we can now move on to the next chapter and start building a chatbot for the viewers. At the same time, we will keep backdoors available in case the dialogs show that we need to fine-tune our feature vector statistics.</p>
    <h1 id="_idParaDest-289" class="title">Questions</h1>
    <ol>
      <li class="list">RBMs are based on directed graphs. (Yes | No)</li>
      <li class="list">The hidden units of an RBM are generally connected to one another. (Yes | No)</li>
      <li class="list">Random sampling is not used in an RBM. (Yes | No)</li>
      <li class="list">PCA transforms data into higher dimensions. (Yes | No)</li>
      <li class="list">In a covariance matrix, the eigenvector shows the direction of the vector representing that matrix, and the eigenvalue shows the size of that vector. (Yes | No)</li>
      <li class="list">It is impossible to represent a human mind in a machine. (Yes | No)</li>
      <li class="list">A machine cannot learn concepts, which is why classical applied mathematics is enough to make efficient artificial intelligence programs for every field. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-290" class="title">Further reading</h1>
    <ul>
      <li class="list">For more on RBMs, refer to: <a href="https://skymind.ai/wiki/restricted-boltzmann-machine"><span class="url">https://skymind.ai/wiki/restricted-boltzmann-machine</span></a></li>
      <li class="list">The original reference site for the source code in this chapter can be found here: <a href="https://github.com/echen/restricted-boltzmann-machines/blob/master/README.md"><span class="url">https://github.com/echen/restricted-boltzmann-machines/blob/master/README.md</span></a></li>
      <li class="list">The original Geoffrey Hinton paper can be found here: <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"><span class="url">http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf</span></a></li>
      <li class="list">For more on PCA, refer to this link: <a href="https://www.sciencedirect.com/topics/engineering/principal-component-analysis"><span class="url">https://www.sciencedirect.com/topics/engineering/principal-component-analysis</span></a></li>
      <li class="list">Ready-to-use RBM resources are located here: <a href="https://pypi.org/project/pydbm/"><span class="url">https://pypi.org/project/pydbm/</span></a></li>
    </ul>
  </div>
</body></html>