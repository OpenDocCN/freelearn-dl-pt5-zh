<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer066">
			<h1 id="_idParaDest-70"><em class="italic"><a id="_idTextAnchor069"/>Chapter 6</em>: Running Hyperparameter Tuning at Scale</h1>
			<p><strong class="bold">Hyperparameter tuning</strong> or <strong class="bold">hyperparameter optimization</strong> (<strong class="bold">HPO</strong>) is a procedure that finds the best possible deep neural network structures, types of pretrained models, and model training process within a reasonable computing resource constraint and time frame. Here, hyperparameter refers to parameters that cannot be changed or learned during the ML training process, such as the number of layers inside a deep neural network, the choice of a pretrained language model, or the learning rate, batch size, and optimizer of the training process. In this chapter, we will use HPO as a shorthand to refer to the process of hyperparameter tuning and optimization. HPO is a critical step for producing a high-performance ML/DL model. Given that the search space of the hyperparameter is very large, efficiently running HPO at scale is a major challenge. The complexity and high cost of evaluating a DL model, compared to classical ML models, further compound the challenges. Therefore, we will need to learn state-of-the-art HPO approaches and implementation frameworks, implement increasingly complex and scalable HPO methods, and track them with MLflow to ensure a reproducible tuning process. By the end of this chapter, you will be comfortable with implementing scalable HPO for DL model pipelines.</p>
			<p>In this chapter, first, we will give an overview of the different automatic HPO frameworks and applications of DL model tuning. Additionally, we will understand what to optimize and when to choose what frameworks to use. We will compare three popular HPO frameworks: <strong class="bold">HyperOpt</strong>, <strong class="bold">Optuna</strong>, and <strong class="bold">Ray Tune</strong>. We will show which of these is the best choice for running HPO at scale. Then, we will focus on learning how to create HPO-ready DL model codes that can use Ray Tune and MLflow. Following this, we will show how we can switch to using different HPO algorithms easily with Optuna as a primary example. </p>
			<p>In this chapter, we'll cover the following topics:</p>
			<ul>
				<li>Understanding automatic HPO for DL pipelines</li>
				<li>Creating HPO-ready DL models using Ray Tune and MLflow</li>
				<li>Running the first Ray Tune HPO experiment with MLflow</li>
				<li>Running Ray Tune HPO with Optuna and HyperBand</li>
			</ul>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Technical requirements </h1>
			<p>To understand the examples in this chapter, the following key technical requirements are needed:</p>
			<ul>
				<li>Ray Tune 1.9.2: This is a flexible and powerful hyperparameter tuning framework (<a href="https://docs.ray.io/en/latest/tune/index.html">https://docs.ray.io/en/latest/tune/index.html</a>).</li>
				<li>Optuna 2.10.0: This is an imperative and define-by-run hyperparameter tuning Python package (<a href="https://optuna.org/">https://optuna.org/</a>). </li>
				<li>The code for this chapter can be found in the following GitHub URL, which also includes the <strong class="source-inline">requirements.txt</strong> file that contains the preceding key packages and other dependencies: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter06">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter06</a>.</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Understanding automatic HPO for DL pipelines</h1>
			<p>Automatic HPO has been studied for over two decades since the first known paper on this topic was published in 1995 (<a href="https://www.sciencedirect.com/science/article/pii/B9781558603776500451">https://www.sciencedirect.com/science/article/pii/B9781558603776500451</a>). It has been widely understood that tuning<a id="_idIndexMarker349"/> hyperparameters for an ML model can improve the performance of the model – sometimes, dramatically. The rise of DL models in recent years has triggered a new wave of innovation and the development of new frameworks to tackle HPO for DL pipelines. This is because a DL model pipeline imposes many new and large-scale optimization challenges that cannot be easily solved by previous HPO methods. Note that, in contrast to the model parameters that can be learned during the model training process, a set of hyperparameters must be set before training. </p>
			<p class="callout-heading">Difference between HPO and Transfer Learning's Fine-Tuning</p>
			<p class="callout">In this book, we have been focusing on one successful DL approach called <strong class="bold">Transfer Learning</strong> (please refer to <a href="B18120_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Deep Learning Life Cycle and MLOps Challenges</em>, for a full discussion). The key step of a transfer learning process is to fine-tune a pretrained model with some task- and domain-specific labeled data to get a good task-specific DL model. However, the fine-tuning step is just a special kind of model training step that also has lots of hyperparameters to optimize. That's where HPO comes into play. </p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Types of hyperparameters and their challenges</h2>
			<p>There are several types of hyperparameters that you can use for a DL pipeline:</p>
			<ul>
				<li><strong class="bold">DL model type and architecture</strong>: In the case of transfer learning, choosing which pretrained models to<a id="_idIndexMarker350"/> use is one possible hyperparameter. For example, there are over 27,000 pretrained <a id="_idIndexMarker351"/>models in the <strong class="bold">Hugging Face</strong> model repository (<a href="https://huggingface.co/models">https://huggingface.co/models</a>), including <strong class="bold">BERT</strong>, <strong class="bold">RoBERTa</strong>, and many more. For a particular prediction task, we might want to try a few of them to decide which is the best one to use. </li>
				<li><strong class="bold">Learning- and training-related parameters</strong>: These include different types of optimizers<a id="_idIndexMarker352"/> such as <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) and <strong class="bold">Adam</strong> (you can view a list <a id="_idIndexMarker353"/>of PyTorch optimizers at <a href="https://machinelearningknowledge.ai/pytorch-optimizers-complete-guide-for-beginner/">https://machinelearningknowledge.ai/pytorch-optimizers-complete-guide-for-beginner/</a>). It also includes the associated parameters such as learning rate and batch size. It is<a id="_idIndexMarker354"/> recommended that, when applicable, the following parameters should be first tuned in their order of importance for a neural network model: learning rate, momentum, mini-batch size, the number of hidden layers, learning rate decay, and regularization (<a href="https://arxiv.org/abs/2003.05689">https://arxiv.org/abs/2003.05689</a>).</li>
				<li><strong class="bold">Data and pipeline configurations</strong>: A DL pipeline <a id="_idIndexMarker355"/>can include data processing and transformation steps that could impact model training. For example, if we want to compare the performance of a classification model for an email message with or without the signature text body, then a hyperparameter for whether to include an email signature is needed. Another example is when we don't have enough data or variations of data; we could try to use various data augmentation techniques that will lead to different sets of input for the model training (<a href="https://neptune.ai/blog/data-augmentation-nlp">https://neptune.ai/blog/data-augmentation-nlp</a>).  </li>
			</ul>
			<p>As a reminder, not all hyperparameters are tunable or require tuning. For example, it is not necessary for the <strong class="bold">number of epochs</strong> in a DL model to be tuned. This is because training should stop when the accuracy metric stops improving or does not hold any promise to do better than other hyperparameter configurations. This is called early stopping or pruning and is one of the key techniques underpinning some recent state-of-the-art HPO algorithms (for more discussions on early stopping, please refer to <a href="https://databricks.com/blog/2019/08/15/how-not-to-scale-deep-learning-in-6-easy-steps.html">https://databricks.com/blog/2019/08/15/how-not-to-scale-deep-learning-in-6-easy-steps.html</a>).</p>
			<p>Note that all these three categories of hyperparameters can be mixed and matched, and the configuration of the entire hyperparameter space can be very large. For example, if we want to choose the type of pretrained model we want to use as a hyperparameter (for example, the choice could be <strong class="bold">BERT</strong> or <strong class="bold">RoBERTa</strong>), two learning-related parameters (such as the learning rate and batch size), and two different data augmentation techniques for NLP texts (such as random insertion and synonym replacement), then we have five hyperparameters to optimize. Note that each hyperparameter can have quite a few different candidate values to choose from, and if each hyperparameter has 5 different values, then we will have a total of 55 = 3125 combinations of hyperparameters to try. In practice, it is very common to have dozens of hyperparameters to try, and each hyperparameter could have dozens of choices or distributions to sample from. This quickly leads to a curse of dimensionality problem (<a href="https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a">https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a</a>). This high-dimensional search space challenge is compounded by the expensive training and evaluation costs of DL models; we know that even 1 epoch of a tiny BERT, which we tried in the previous chapters, with a tiny set of training and validation dataset can take 1–2 mins. Now imagine a realistic production-grade DL model with HPO that could take hours, days, or even weeks if not executed efficiently. In general, the following is a list of the main challenges that require the application of high-performance HPO at scale:</p>
			<ul>
				<li>The high-dimensional search space of hyperparameters</li>
				<li>The high cost of model training and evaluation time for increasingly large DL models</li>
				<li>Time-to-production and deployment for DL models used in production<p class="callout-heading">Performing Model Training and HPO Simultaneously</p><p class="callout">It is possible to change the hyperparameters dynamically during the training process. This is a hybrid approach that does model training and HPO simultaneously, such as <strong class="bold">Population-Based Training</strong> (<strong class="bold">PBT</strong>; <a href="https://deepmind.com/blog/article/population-based-training-neural-networks">https://deepmind.com/blog/article/population-based-training-neural-networks</a>). However, this does not change the fact that when starting a new epoch of training, a set of<a id="_idIndexMarker356"/> hyperparameters needs to be predefined. This PBT is one of the innovations that tries to reduce both the cost of searching for high-dimensional hyperparameter space and the training cost of a DL model. Interested readers should consult the <em class="italic">Further reading</em> section to dive deeper into this topic.</p></li>
			</ul>
			<p>Now that we understand the general challenges and categories of hyperparameters to optimize, let's look at how HPO works and how to choose a framework for our usage.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>How HPO works and which ones to choose</h2>
			<p>There are different ways to <a id="_idIndexMarker357"/>understand how HPO works. The classical HPO methods include grid search and random search, where a set of hyperparameters are chosen with a range of candidate values. Each one is run independently to completion, and then we pick the best hyperparameter configuration from the set of trials we run, given the best model performance metric we found. Although this type of search is easy to implement and might not even require a sophisticated framework to support it, it is inherently inefficient and might not even find the best configuration of hyperparameters due to the non-convex nature of HPO. The term non-convex means that multiple local minimal or maximal points exist, and an optimization method might not be able to find a global optimal (that is, minimum or maximum). Put simply, a modern HPO needs to do two things:</p>
			<ul>
				<li>The adaptive sampling of hyperparameters (also known as <strong class="bold">Configuration Selection</strong> or <strong class="bold">CS</strong>): This means it needs to find which set of hyperparameters to try by taking advantage of prior knowledge. This is mostly about using different variants of Bayesian optimization to adaptively identify new configurations based on previous trials in a<a id="_idIndexMarker358"/> sequential way. This has been proven to outperform traditional grid search and random search methods.</li>
				<li>The adaptive evaluation of the performance of a set of hyperparameters (also known as <strong class="bold">Configuration Evaluation</strong> or <strong class="bold">CE</strong>): These approaches focus on adaptively allocating more resources to promising hyperparameter configurations while quickly pruning the poor ones. Resources can be in different forms such as the size of the training dataset (for example, only using a small fraction of the training dataset) or the number of iterations (for example, only using a few iterations to decide which ones to terminate without running to convergence). There is a<a id="_idIndexMarker359"/> family of methods called multi-armed bandit algorithms, such as the <strong class="bold">Asynchronous Successive Halving Algorithm</strong> (<strong class="bold">ASHA</strong>). Here, all trials start with an initial budget, then the worst half is removed, the budget is adjusted for the remaining ones, and this repeats until only one trial is left. </li>
			</ul>
			<p>In practice, we want to select a<a id="_idIndexMarker360"/> suitable HPO framework using the following five criteria:</p>
			<ul>
				<li>Callback integration with MLflow</li>
				<li>Scalability and support of GPU clusters</li>
				<li>Ease of use and flexible APIs </li>
				<li>Integration with cutting edge HPO algorithms (<strong class="bold">CS</strong> and <strong class="bold">CE</strong>)</li>
				<li>Support of DL frameworks</li>
			</ul>
			<p>In this book, three frameworks have been compared, and the results are summarized in <em class="italic">Figure 6.1</em>:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="Images/B18120_06_01.jpg" alt="Figure 6.1: Comparison of Ray Tune, Optuna, and HyperOpt&#13;&#10;" width="1532" height="1352"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: Comparison of Ray Tune, Optuna, and HyperOpt</p>
			<p>As you can see from <em class="italic">Figure 6.1</em>, the winner is <strong class="bold">Ray Tune</strong> (<a href="https://docs.ray.io/en/latest/tune/index.html">https://docs.ray.io/en/latest/tune/index.html</a>), when compared to <strong class="bold">Optuna</strong> (<a href="https://optuna.org/">https://optuna.org/</a>) and <strong class="bold">HyperOpt</strong> (<a href="https://hyperopt.github.io/hyperopt/">https://hyperopt.github.io/hyperopt/</a>). Let's explain the five criteria, as follows:</p>
			<ul>
				<li><strong class="bold">Callback integration with MLflow</strong>: Optuna's support of the MLflow callback is still an experimental feature, while <a id="_idIndexMarker361"/>HyperOpt does not support callback at all, leaving additional work for users to manage the MLflow tracking for each trial run. </li>
			</ul>
			<p>Only Ray Tune supports both the Python mixin decorator and callback integration with MLflow. Python mixin is a pattern that allows a standalone function to be mixed in whenever needed. In this case, the MLflow functionality is automatically mixed in during model training through the <strong class="source-inline">mlflow_mixin</strong> decorator. This can turn any training function into a Ray Tune trainable function, automatically configuring MLflow and creating a run in the same process as each Tune trial. You can then use the MLflow API inside the training function and it will automatically get reported to the correct run. Additionally, it supports MLflow's autologging, which means that all of the MLflow tracking information will be logged into the correct trial. For example, the following code snippet shows that our previous DL fine-tuning function can be turned into a <strong class="source-inline">mlflow_mixin</strong> Ray Tune function, as follows:</p>
			<p class="source-code">@mlflow_mixin</p>
			<p class="source-code">def train_dl_model():</p>
			<p class="source-code">    mlflow.pytorch.autolog()</p>
			<p class="source-code">    trainer = flash.Trainer(</p>
			<p class="source-code">        max_epochs=num_epochs,</p>
			<p class="source-code">        callbacks=[TuneReportCallback(</p>
			<p class="source-code">            metrics, on='validation_end')])</p>
			<p class="source-code">    trainer.finetune()</p>
			<p>Note that when we define the trainer, we can add <strong class="source-inline">TuneReportCallback</strong> as one of the callbacks, which will pass the metrics back to Ray Tune, while the MLflow autologging does its job of logging all the tracking results simultaneously. In the next section, we <a id="_idIndexMarker362"/>will show you how to turn the previous chapter's example of fine-tuning the DL model into a Ray Tune trainable.</p>
			<ul>
				<li><strong class="bold">Scalability and support of GPU clusters</strong>: Although Optuna and HyperOpt support parallelization, they<a id="_idIndexMarker363"/> both have <a id="_idIndexMarker364"/>dependencies on some external databases (relational databases or MongoDB) or SparkTrials. Only Ray Tune supports parallel and distributed HPO through the Ray distributed framework natively, and it is also the only one that supports running on a GPU cluster among these three frameworks.</li>
				<li><strong class="bold">Ease of use and flexibility of the APIs</strong>: Among all the three frameworks, only Optuna supports <strong class="bold">define-by-run</strong> APIs, which allows you to dynamically define the hyperparameters in a Pythonic programming style, including loops and branches (<a href="https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html">https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html</a>). This is in contrast to the <strong class="bold">define-and-run</strong> APIs, which both Ray Tune and HyperOpt support, where the search space is defined by a predefined dictionary prior to evaluating the objective function. These two terms, <strong class="bold">define-by-run</strong> and <strong class="bold">define-and-run</strong>, were actually coined by the DL framework's development community. In the early days, when TensorFlow 1.0 was initially released, a neural network needed to be defined first and then lazily executed later, which is called define-and-run. These two phases, 1) the construction of the neural network phase and 2) the evaluation phases, are sequentially executed, and the neural network structure cannot be changed after the construction phase. The newer DL frameworks, such as TensorFlow 2.0 (or the eager execution version of TensorFlow) and PyTorch, support the <strong class="bold">define-by-run</strong> neural network computation. There are no two separate phases for constructing and evaluating neural networks. Users can directly manipulate the neural networks while doing the computation. While the <strong class="bold">define-by-run</strong> API provided by Optuna can be used to directly define the hyperparameter search space dynamically, it does have some drawbacks. The main problem is that the parameter concurrence is not known until runtime, which could complicate the implementation of the optimization method. This is because knowing the parameter concurrence beforehand is well supported for many sampling methods. Thus, in this book, we prefer using <strong class="bold">define-and-run</strong> APIs. Also, note that Ray Tune can support the <strong class="bold">define-by-run</strong> API through integration with Optuna (you can see an example in Ray Tune's GitHub repository at <a href="https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/optuna_define_by_run_example.py#L35">https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/optuna_define_by_run_example.py#L35</a>).</li>
				<li><strong class="bold">Integration with cutting-edge HPO algorithms</strong> (<strong class="bold">CS and CE</strong>): On the <strong class="bold">CS </strong>side, among these three <a id="_idIndexMarker365"/>frameworks, HyperOpt has the least active development to support or integrate with the latest cutting-edge HPO sampling and search <a id="_idIndexMarker366"/>methods. Its primary search method is <strong class="bold">Tree-Structured Parzen Estimators</strong> (<strong class="bold">TPE</strong>), which is a Bayesian optimization variant that's especially effective for a mixed categorical and conditional hyperparameter search space. Similarly, Optuna's primary sampling method is TPE. On the contrary, Ray Tune supports all cutting-edge searching methods, including the following: <ul><li>DragonFly (<a href="https://dragonfly-opt.readthedocs.io/en/master/">https://dragonfly-opt.readthedocs.io/en/master/</a>), which is a highly scalable Bayesian optimization framework</li><li>BlendSearch (<a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function/#hyperparameter-optimization-algorithm">https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function/#hyperparameter-optimization-algorithm</a>) from Microsoft Research</li></ul></li>
			</ul>
			<p>In addition, Ray Tune also supports TPE through integration with Optuna and HyperOpt. </p>
			<p>On the <strong class="bold">CE</strong> side, HyperOpt does not support any pruning or schedulers to stop the non-promising hyperparameter configuration. Both Optuna and Ray Tune support quite a few pruners (in Optuna) or schedulers (in Ray Tune). However, only Ray Tune supports PBT. Given the active development community and flexible API developed by Ray Tune, it is possible for Ray tune to continue to integrate and support any emerging schedulers or pruners in a timely fashion.</p>
			<ul>
				<li><strong class="bold">Support of DL frameworks</strong>: HyperOpt is not specifically designed or integrated with any DL <a id="_idIndexMarker367"/>frameworks. This does not mean you cannot use HyperOpt for tuning DL models. However, HyperOpt does not offer any pruning or scheduler support to perform early stopping for unpromising hyperparameter configuration, which is a major disadvantage for HyperOpt to be used for DL model tuning. Both Ray Tune and Optuna have integration with popular DL frameworks such as PyTorch Lightning and TensorFlow/Keras. </li>
			</ul>
			<p>In addition to the major criteria that we just discussed, Ray Tune also has the best documentation, extensive code examples, and a vibrant open source developer community, which is why we prefer to use Ray Tune for our learning in this chapter. In the following sections, we will learn how to create HPO-ready DL models with Ray Tune and MLflow.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Creating HPO-ready DL models with Ray Tune and MLflow </h1>
			<p>To use Ray Tune<a id="_idIndexMarker368"/> with MLflow for HPO, let's use the fine-tuning step<a id="_idIndexMarker369"/> in our DL pipeline example from <a href="B18120_05_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 5</em></a>, <em class="italic">Running DL Pipelines in Different Environments</em>, to see what needs to be set up and what code changes we need to make. Before we start, first, let's review a few key concepts that are specifically relevant to our usage of Ray Tune:</p>
			<ul>
				<li><strong class="bold">Objective function</strong>: An objective function can be either to minimize or maximize some metric values for a given configuration of hyperparameters. For example, in the DL model training and fine-tuning scenarios, we would like to maximize the F1-score for the accuracy of an NLP text classifier. This objective function needs to be<a id="_idIndexMarker370"/> wrapped as a trainable function, where <a id="_idIndexMarker371"/>Ray Tune can do HPO. In the following section, we will illustrate how to wrap our NLP text sentiment model.</li>
				<li><strong class="bold">Function-based APIs and class-based APIs</strong>: A function-based API allows a user to insert Ray Tune statements<a id="_idIndexMarker372"/> into the model training function (called trainable in Ray Tune) such as <strong class="source-inline">tune.report</strong> for reporting model metrics (<a href="https://docs.ray.io/en/latest/tune/api_docs/trainable.html#function-api">https://docs.ray.io/en/latest/tune/api_docs/trainable.html#function-api</a>). A class-based API requires the model training function (trainable) to be a subclass of <strong class="source-inline">tune.Trainable </strong>(<a href="https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-class-api">https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-class-api</a>). A class-based API provides more control of how Ray Tune controls the model training processing. This might be very helpful if you start writing a new piece of architecture for a neural network model. However, when using a pretrained foundation model for fine-tuning, it is much easier to use a function-based API since we can leverage packages such as PyTorch Lightning Flash to do HPO.</li>
				<li><strong class="bold">Trials</strong>: Each trial is a run of a specific <a id="_idIndexMarker373"/>configuration of hyperparameters. This can be executed by passing the trainable function into <strong class="source-inline">tune.run</strong>, where Ray Tune will orchestrate the HPO process.</li>
				<li><strong class="bold">Search space</strong>: This is a set <a id="_idIndexMarker374"/>of configurations where each hyperparameter will be assigned a way in which to sample from certain distributions (for example, log uniform distribution sampling can use <strong class="source-inline">tune.loguniform</strong>) or from some categorical variables (for example, <strong class="source-inline">tune.choice(['a', 'b' ,'c'])</strong> can allow you to choose these three choices uniformly). Usually, this search<a id="_idIndexMarker375"/> space is defined as a Python dictionary variable called <strong class="source-inline">config</strong>.</li>
				<li> <strong class="bold">Suggest</strong>: This is the search algorithm or CS algorithm that you need to choose for selecting the best trial. Ray Tune <a id="_idIndexMarker376"/>provides integration to many popular open source search algorithms and can automatically convert the search space defined in Ray Tune into the format that the underlying optimization algorithms expect. A list of available search algorithms can be found through the <strong class="source-inline">tune.suggest</strong> API (<a href="https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg">https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg</a>).</li>
				<li><strong class="bold">Scheduler</strong>: This is also called CE, as mentioned earlier. While the <strong class="source-inline">tune.suggest</strong> API <a id="_idIndexMarker377"/>provides the optimization algorithms for searching, it does not offer the early stopping or pruning capability to<a id="_idIndexMarker378"/> halt the obviously unpromising trials after just a few iterations. Since early stopping or pruning can significantly speed up the HPO process, it is highly recommended that you use a<a id="_idIndexMarker379"/> scheduler in conjunction with a searcher. Ray Tune provides many popular schedulers through its scheduler API (<strong class="source-inline">tune.schedulers</strong>), such as ASHA, HyperBand, and more. (Please visit <a href="https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#trial-schedulers-tune-schedulers">https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#trial-schedulers-tune-schedulers</a>.)</li>
			</ul>
			<p>Having reviewed the basic concepts and APIs of Ray Tune, in the next section, we will be setting up Ray Tune and MLflow to run HPO experiments.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Setting up Ray Tune and MLflow </h2>
			<p>Now that we understand the basic <a id="_idIndexMarker380"/>concepts and APIs of Ray Tune, let's see how we can set up Ray Tune to perform HPO for the fine-tuning step of our previous NLP sentiment classifier. You might want to download this chapter's code (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/</a>) to follow along with these instructions:</p>
			<ol>
				<li>Install Ray Tune by typing the following command into your conda virtual environment, <strong class="source-inline">dl_model_hpo</strong>:<p class="source-code"><strong class="bold">pip install ray[tune]==1.9.2</strong></p></li>
				<li>This will install Ray Tune in the virtual environment where you will launch the HPO runs for your DL model fine-tuning. Note that we have also provided the complete <strong class="source-inline">requirements.txt</strong> file in this chapter's GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/requirements.txt">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter06/requirements.txt</a>), where you should be able to run the following installation command:<p class="source-code"><strong class="bold">pip install -r requirements.txt</strong></p></li>
				<li>The complete instructions in the <strong class="source-inline">README.md</strong> file, which are in the same folder, should give you more guidance if you need to know how to set up a proper virtual environment. </li>
				<li>For the MLflow setup, assuming<a id="_idIndexMarker381"/> you already have a full-fledged MLflow tracking server set up, the only thing you need to pay attention to is making sure that you have the environment variables set up correctly to access the MLflow tracking server. Run the following in your shell to set them up. Alternatively, you can overwrite your environmental variables by calling <strong class="source-inline">os.environ["environmental_name"]=value</strong> in the Python code. As a reminder, we have shown the following environment variables that can be set in the command lines per Terminal session:<p class="source-code"><strong class="bold">export MLFLOW_TRACKING_URI=http://localhost</strong></p><p class="source-code"><strong class="bold">export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000</strong></p><p class="source-code"><strong class="bold">export AWS_ACCESS_KEY_ID="minio"</strong></p><p class="source-code"><strong class="bold">export AWS_SECRET_ACCESS_KEY="minio123"</strong></p></li>
				<li>Run the step of <strong class="source-inline">download_data</strong> to download the raw data to the local folder under the <strong class="source-inline">chapter06</strong> parent folder:<p class="source-code"><strong class="bold">mlflow run . -P pipeline_steps='download_data' --experiment-name dl_model_chapter06</strong></p></li>
			</ol>
			<p>When the preceding execution is done, you should be able to find the IMDB data under the <strong class="bold">chapter06/data/</strong> folder.</p>
			<p>Now we are ready to create an<a id="_idIndexMarker382"/> HPO step to fine-tune the NLP sentiment model we built earlier. </p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Creating the Ray Tune trainable for the DL model</h2>
			<p>There are multiple changes that <a id="_idIndexMarker383"/>we need to make to allow Ray Tune to<a id="_idIndexMarker384"/> run HPO to fine-tune the DL model that we developed in previous chapters. Let's walk through the steps, as follows:</p>
			<ol>
				<li value="1">First, let's identify the list of possible hyperparameters (both tunable and non-tunable) in our previous fine-tuning code. Recall that our fine-tuning code looks similar to the following (only the key lines of code are shown here; the complete code can be found in <strong class="source-inline">chapter05</strong> in the GitHub repository at <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter05/pipeline/fine_tuning_model.py#L19">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter05/pipeline/fine_tuning_model.py#L19</a>):<p class="source-code">datamodule = TextClassificationData.from_csv(</p><p class="source-code">    input_fields="review",</p><p class="source-code">    target_fields="sentiment",</p><p class="source-code">    train_file=f"{data_path}/imdb/train.csv",</p><p class="source-code">    val_file=f"{data_path}/imdb/valid.csv",</p><p class="source-code">    test_file=f"{data_path}/imdb/test.csv")</p><p class="source-code">classifier_model = TextClassifier(</p><p class="source-code">    backbone= "prajjwal1/bert-tiny",</p><p class="source-code">    num_classes=datamodule.num_classes, </p><p class="source-code">    metrics=torchmetrics.F1(datamodule.num_classes))</p><p class="source-code">trainer = flash.Trainer(max_epochs=3)</p><p class="source-code">trainer.finetune(classifier_model, </p><p class="source-code">    datamodule=datamodule, strategy="freeze") </p></li>
			</ol>
			<p>The preceding code has four major pieces: </p>
			<ul>
				<li>The <strong class="source-inline">datamodule</strong> variable: This defines the data sources for training, validation, and testing. There is a <strong class="source-inline">batch_size</strong> parameter with a default value of <strong class="source-inline">1</strong>, which is not shown here, but it is one of the most important hyperparameters to tune. For<a id="_idIndexMarker385"/> more details, please see the explanation in the <strong class="source-inline">lightning-flash</strong> code documentation (<a href="https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/core/data/data_module.py#L64">https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/core/data/data_module.py#L64</a>).</li>
				<li><strong class="source-inline">classifier_model</strong>: This defines a classifier with the exposed parameters through the <strong class="source-inline">TextClassifier</strong> API of <strong class="source-inline">lightning-flash</strong>. There are multiple hyperparameters in the input arguments that could be tuned, including <strong class="source-inline">learning_rate</strong>, the <strong class="source-inline">backbone</strong> foundation model, <strong class="source-inline">optimizer</strong>, and more. You can see the complete list of input arguments in the <strong class="source-inline">lightning-flash</strong> code documentation for the <strong class="source-inline">TextClassifier</strong> API (<a href="https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/text/classification/model.py#L44">https://github.com/PyTorchLightning/lightning-flash/blob/450902d713980e0edefcfd2d2a2a35eb875072d7/flash/text/classification/model.py#L44</a>).</li>
				<li><strong class="source-inline">trainer</strong>: This defines a trainer <a id="_idIndexMarker386"/>variable that can be used for fine-tuning. Here, there are a few hyperparameters that need to be set, but not necessarily tuned, such as <strong class="source-inline">num_epochs</strong>, as discussed earlier.</li>
				<li><strong class="source-inline">trainer.finetune</strong>: This does the actual finetuning (transfer learning). Note that there is also a possible hyperparameter <strong class="bold">strategy</strong> that could be tuned.</li>
			</ul>
			<p>For learning purposes, we will pick <strong class="source-inline">learning_rate</strong> and <strong class="source-inline">batch_size</strong> as the two hyperparameters to tune, as these two are the most important hyperparameters to optimize for a DL model. Once you finish this chapter, you should be able to easily add additional hyperparameters to the list of candidates for optimization.</p>
			<ol>
				<li value="2">Ray Tune requires a trainable <a id="_idIndexMarker387"/>function to be passed into <strong class="source-inline">tune.run</strong>. This means we need to create a trainable function. By default, a trainable function only takes one required input parameter, <strong class="source-inline">config</strong>, which contains a dictionary of key-value pairs of hyperparameters and other parameters for identifying an execution environment such as an MLflow tracking URL. However, Ray Tune provides a wrapper function, called <strong class="source-inline">tune.with_parameters</strong>, which allows you to pass along additional arbitrary parameters and objects (<a href="https://docs.ray.io/en/latest/tune/tutorials/overview.html#how-can-i-pass-further-parameter-values-to-my-trainable">https://docs.ray.io/en/latest/tune/tutorials/overview.html#how-can-i-pass-further-parameter-values-to-my-trainable</a>). First, let's create a function called <strong class="source-inline">finetuning_dl_model</strong> to encapsulate the logic that we just examined regarding the fine-tuning step, using<a id="_idIndexMarker388"/> a <strong class="source-inline">mlflow_mixin</strong> decorator. This allows MLflow to be initialized automatically when this function is called:<p class="source-code">@mlflow_mixin</p><p class="source-code">def finetuning_dl_model(config, data_dir=None,</p><p class="source-code">                        num_epochs=3, num_gpus=0):</p></li>
			</ol>
			<p>This function takes a <strong class="source-inline">config</strong> dictionary as input where a list of hyperparameters and MLflow configurations can be passed in. Additionally, we add three additional arguments to the function signature: <strong class="source-inline">data_dir</strong> for the location of the directory, <strong class="source-inline">num_epochs</strong> for the maximum number of epochs for each trial to run, and <strong class="source-inline">num_gpus</strong> for the number of GPUs for each trial to use if there is any. </p>
			<ol>
				<li value="3">In this <strong class="source-inline">mlflow_mixin</strong> decorated function, we can use all the MLflow tracking APIs if necessary, but as of MLflow version 1.22.0, since MLflow's autologging support no longer is an experimental feature, but a mature production quality feature (<a href="https://github.com/mlflow/mlflow/releases/tag/v1.22.0">https://github.com/mlflow/mlflow/releases/tag/v1.22.0</a>), we should just use autologging in our code, as follows:<p class="source-code">mlflow.pytorch.autolog()</p></li>
			</ol>
			<p>This is efficient and requires <a id="_idIndexMarker389"/>no change. However, the <strong class="source-inline">batch_size</strong> hyperparameter is not automatically captured by autologging, so we need to add one more logging statement after the fine-tuning is done, as follows:</p>
			<p class="source-code">mlflow.log_param('batch_size',config['batch_size'])</p>
			<ol>
				<li value="4">In the rest of the implementation body of the <strong class="source-inline">finetuning_dl_model </strong>function, the majority of the<a id="_idIndexMarker390"/> code is the same as before. There are a few changes. In the <strong class="source-inline">datamodule</strong> variable assignment statement, we add <strong class="source-inline">batch_size=config['batch_size']</strong> to allow the mini-batch size of the training data to be tunable, as shown here:<p class="source-code">datamodule = TextClassificationData.from_csv(</p><p class="source-code">    input_fields="review",</p><p class="source-code">    target_fields="sentiment",</p><p class="source-code">    train_file=f"{data_dir}/imdb/train.csv",</p><p class="source-code">    val_file=f"{data_dir}/imdb/valid.csv",</p><p class="source-code">    test_file=f"{data_dir}/imdb/test.csv",</p><p class="source-code">    <strong class="bold">batch_size=config['batch_size']</strong>)</p></li>
				<li>When defining the <strong class="source-inline">classifier_model</strong> variable, instead of using the default values of the set of hyperparameters, now we need to pass in the <strong class="source-inline">config</strong> dictionary to assign these values:<p class="source-code">classifier_model = TextClassifier(</p><p class="source-code">    backbone=<strong class="bold">config</strong>['foundation_model'],</p><p class="source-code">    learning_rate=<strong class="bold">config</strong>['lr'],</p><p class="source-code">    optimizer=<strong class="bold">config</strong>['optimizer_type'],</p><p class="source-code">    num_classes=datamodule.num_classes,</p><p class="source-code">    metrics=torchmetrics.F1(datamodule.num_classes))</p></li>
				<li>Next, we need to modify the trainer assignment code. Here, we need to do two things: first, we need to define <a id="_idIndexMarker391"/>a metrics key-value dictionary to pass from PyTorch Lightning to Ray Tune. The key in this metrics dictionary is the name to be referenced in the Ray Tune<a id="_idIndexMarker392"/> trial run, while the value of the key in this dictionary is the corresponding metric name reported by PyTorch Lightning.<p class="callout-heading">Metric Names in the PyTorch Lightning's Validation Step</p><p class="callout">When passing the metrics to Ray Tune, first, we need to know the metric names used in PyTorch Lightning during the validation step since HPO only uses validation data for evaluation, not the hold-out test datasets. It turns out PyTorch Lightning has a hardcoded convention to prefix all metrics with the corresponding training, validation, and testing step names and an underscore. A metric named <strong class="source-inline">f1</strong> will be reported in PyTorch Lightning as <strong class="source-inline">train_f1</strong> during the training step, <strong class="source-inline">val_f1</strong> during the validation step, and <strong class="source-inline">test_f1</strong> during the testing step. (You can view the PyTorch Lightning code logic at <a href="https://github.com/PyTorchLightning/lightning-flash/blob/8b244d785c5569e9aa7d2b878a5f94af976d3f55/flash/core/model.py#L462">https://github.com/PyTorchLightning/lightning-flash/blob/8b244d785c5569e9aa7d2b878a5f94af976d3f55/flash/core/model.py#L462</a>). In our example, we can pick <strong class="source-inline">cross_entropy</strong> and <strong class="source-inline">f1</strong> as the metrics during the validation step, which are named <strong class="source-inline">val_cross_entropy</strong> and <strong class="source-inline">val_f1</strong>, to pass back to Ray Tune as <strong class="source-inline">loss</strong> and <strong class="source-inline">f1</strong>, respectively. That means, in Ray Tune's trial run, we reference these two metrics as simply <strong class="source-inline">loss</strong> and <strong class="source-inline">f1</strong>. </p></li>
			</ol>
			<p>So, here we define two metrics that we want to pass from the PyTorch Lightning validation step, <strong class="source-inline">val_cross_entropy</strong> and <strong class="source-inline">val_f1</strong>, to Ray Tune as <strong class="source-inline">loss</strong> and <strong class="source-inline">f1</strong>, respectively:</p>
			<p class="source-code">metrics = {"loss":"val_cross_entropy", "f1":"val_f1"}</p>
			<p>Now, we can pass this metrics dictionary to the trainer assignment, as follows:</p>
			<p class="source-code">trainer = flash.Trainer(max_epochs=num_epochs,</p>
			<p class="source-code">    gpus=num_gpus,</p>
			<p class="source-code">    progress_bar_refresh_rate=0,</p>
			<p class="source-code">    callbacks=[TuneReportCallback(metrics, </p>
			<p class="source-code">        on='validation_end')])</p>
			<p>Notice that the metrics dictionary is passed through <strong class="source-inline">TuneReportCallBack</strong> when the <strong class="source-inline">validation_end</strong> event happens. This means that when the validation step is done in PyTorch Lightning, it will automatically trigger the Ray Tune report function to<a id="_idIndexMarker393"/> report the list of metrics back to Ray Tune for evaluation. The supported list of <a id="_idIndexMarker394"/>valid events for <strong class="source-inline">TuneReportCallback</strong> to use can be found in Ray Tune's integration with the PyTorch Lightning source code (<a href="https://github.com/ray-project/ray/blob/fb0d6e6b0b48b0a681719433691405b96fbea104/python/ray/tune/integration/pytorch_lightning.py#L170">https://github.com/ray-project/ray/blob/fb0d6e6b0b48b0a681719433691405b96fbea104/python/ray/tune/integration/pytorch_lightning.py#L170</a>). </p>
			<ol>
				<li value="7">Finally, we can call <strong class="source-inline">trainer.finetune</strong> to execute the fine-tuning step. Here, we can pass <strong class="source-inline">finetuning_strategies</strong> as one of the tunable hyperparameters to the argument list:<p class="source-code">trainer.finetune(classifier_model,</p><p class="source-code">    datamodule=datamodule,</p><p class="source-code">    strategy=config['finetuning_strategies'])</p></li>
				<li>This completes the changes to the original function of fine-tuning the DL model. Now we have a new <strong class="source-inline">finetuning_dl_model</strong> function that's ready to be wrapped in <strong class="source-inline">tune.with_parameters</strong> to become a Ray Tune trainable function. It should be called as follows: <p class="source-code">trainable = tune.with_parameters(finetuning_dl_model, data_dir, num_epochs, num_gpus)</p></li>
				<li>Note that there is no need to pass the <strong class="source-inline">config</strong> parameter, as it is implicitly assumed that it's the first parameter of <strong class="source-inline">finetuning_dl_model</strong>. The other three parameters need to be passed to the <strong class="source-inline">tune.with_parameters</strong> wrapper. Also, make sure this statement to create a trainable object for Ray Tune is placed outside <a id="_idIndexMarker395"/>of the <strong class="source-inline">finetuning_dl_model</strong> function. </li>
			</ol>
			<p>In the next section, it will be <a id="_idIndexMarker396"/>placed inside Ray Tune's HPO running function called <strong class="source-inline">run_hpo_dl_model</strong>.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Creating the Ray Tune HPO run function</h2>
			<p>Now, let's create a Ray Tune HPO run<a id="_idIndexMarker397"/> function to do the following five things:</p>
			<ul>
				<li>Define the MLflow runtime configuration parameters including a tracking URI and an experiment name.</li>
				<li>Define the hyperparameter search space using Ray Tune's random distributions API (<a href="https://docs.ray.io/en/latest/tune/api_docs/search_space.html#random-distributions-api">https://docs.ray.io/en/latest/tune/api_docs/search_space.html#random-distributions-api</a>) to sample the list of hyperparameters we identified earlier.</li>
				<li>Define a Ray Tune trainable object using <strong class="source-inline">tune.with_parameters</strong>, as shown toward the end of the previous subsection.</li>
				<li>Call <strong class="source-inline">tune.run</strong>. This will execute the HPO run and return Ray Tune's experiment analysis object when it has been completed.</li>
				<li>Log the best configuration parameters when the entire HPO run is finished.</li>
			</ul>
			<p>Let's walk through the implementation to see how this function can be implemented:</p>
			<ol>
				<li value="1">First, let's define the hyperparameter's <strong class="source-inline">config</strong> dictionary, as follows:<p class="source-code">mlflow.set_tracking_uri(tracking_uri)</p><p class="source-code">mlflow.set_experiment(experiment_name)</p></li>
			</ol>
			<p>This will take <strong class="source-inline">tracking_uri</strong> and <strong class="source-inline">experiment_name</strong> of MLflow as the input parameters and set them<a id="_idIndexMarker398"/> up correctly. If this is the first time you're running this, MLflow will also create the experiment.</p>
			<ol>
				<li value="2">Then, we can define the <strong class="source-inline">config</strong> dictionary, which can include both tunable and non-tunable parameters, and the MLflow configuration parameters. As discussed in the previous section, we will tune <strong class="source-inline">learning_rate</strong> and <strong class="source-inline">batch_size</strong> but will also include other hyperparameters for bookkeeping and future tuning purposes:<p class="source-code">config = {</p><p class="source-code">        "lr": tune.loguniform(1e-4, 1e-1),</p><p class="source-code">        "batch_size": tune.choice([32, 64, 128]),</p><p class="source-code">        "foundation_model": "prajjwal1/bert-tiny",</p><p class="source-code">        "finetuning_strategies": "freeze",</p><p class="source-code">        "optimizer_type": "Adam",</p><p class="source-code">        "mlflow": {</p><p class="source-code">            "experiment_name": experiment_name,</p><p class="source-code">            "tracking_uri": mlflow.get_tracking_uri()</p><p class="source-code">        },</p><p class="source-code">    }</p></li>
			</ol>
			<p>As you can see from the <strong class="source-inline">config</strong> dictionary, we called <strong class="source-inline">tune.loguniform</strong> to sample a log uniform distribution between <strong class="source-inline">1e-4</strong> and <strong class="source-inline">1e-1</strong> to select a learning rate. For the batch size, we called <strong class="source-inline">tune.choice</strong> to select one of three distinct values uniformly. For the rest <a id="_idIndexMarker399"/>of the key-value pairs, they are non-tunable since they do not use any sampling methods but are needed to run the trials.</p>
			<ol>
				<li value="3">Define the trainable object using <strong class="source-inline">tune.with_parameters</strong> with all of the extra parameters except for the <strong class="source-inline">config</strong> parameter:<p class="source-code">trainable = tune.with_parameters(</p><p class="source-code">    finetuning_dl_model,</p><p class="source-code">    data_dir=data_dir,</p><p class="source-code">    num_epochs=num_epochs,</p><p class="source-code">    num_gpus=gpus_per_trial)</p></li>
			</ol>
			<p>In the next statement, this will be called the <strong class="source-inline">tune.run</strong> function.</p>
			<ol>
				<li value="4">Now we are ready to run the HPO by calling <strong class="source-inline">tune.run</strong>, as follows:<p class="source-code">analysis = tune.run(</p><p class="source-code">    trainable,</p><p class="source-code">    resources_per_trial={</p><p class="source-code">        "cpu": 1,</p><p class="source-code">        "gpu": gpus_per_trial</p><p class="source-code">    },</p><p class="source-code">    metric="f1",</p><p class="source-code">    mode="max",</p><p class="source-code">    config=config,</p><p class="source-code">    num_samples=num_samples,</p><p class="source-code">    name="hpo_tuning_dl_model")</p></li>
			</ol>
			<p>Here, the objective is to find the set of hyperparameters that maximizes the F1-score among all of the trials, so the mode is <strong class="source-inline">max</strong> and the metric is <strong class="source-inline">f1</strong>. Note that this metric name, <strong class="source-inline">f1</strong>, is from the <strong class="source-inline">metrics</strong> dictionary that we defined in the previous <strong class="source-inline">finetuning_dl_model</strong> function, where we mapped PyTorch Lightning's <strong class="source-inline">val_f1</strong> to <strong class="source-inline">f1</strong>. This <strong class="source-inline">f1</strong> value is<a id="_idIndexMarker400"/> then passed to Ray Tune at the end of each trial's validation step. The <strong class="source-inline">trainable</strong> object is passed to <strong class="source-inline">tune.run</strong> as the first parameter, which will be executed as many times as the parameter of <strong class="source-inline">num_samples</strong> allows. Following this, <strong class="source-inline">resources_per_trial</strong> defines the CPU and GPU to use. Note that in the preceding example, we haven't specified any search algorithms. This means it will use <strong class="source-inline">tune.suggest.basic_variant</strong> by default, which is a grid search algorithm. There is also no scheduler defined, so, by default, there is no early stopping, and all trials will be run in parallel with the maximum number of CPUs allowed on the execution machine. When the run finishes, an <strong class="source-inline">analysis</strong> variable is returned, which contains the best hyperparameters found, along<a id="_idIndexMarker401"/> with other information. </p>
			<ol>
				<li value="5">Log the best configuration of the hyperparameters found. This can be done by using the returned <strong class="source-inline">analysis</strong> variable from <strong class="source-inline">tune.run</strong>, as follows:<p class="source-code">logger.info("Best hyperparameters found were: %s", analysis.best_config)</p></li>
			</ol>
			<p>That's it. Now we can give it a try. If you download the complete code from this chapter's GitHub repository, you should be able to find the <strong class="source-inline">hpo_finetuning_model.py</strong> file under the <strong class="source-inline">pipeline</strong> folder. </p>
			<p>With the preceding change, now we are ready to run our first HPO experiment.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Running the first Ray Tune HPO experiment with MLflow</h1>
			<p>Now that we have set up<a id="_idIndexMarker402"/> Ray Tune, MLflow, and created the HPO<a id="_idIndexMarker403"/> run function, we can try to run our first Ray Tune HPO experiment, as follows:</p>
			<p class="source-code">python pipeline/hpo_finetuning_model.py</p>
			<p>After a couple of seconds, you will see the following screen, <em class="italic">Figure 6.2</em>, which shows that all 10 trials (that is, the values that we set for <strong class="source-inline">num_samples</strong>) are running concurrently:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="Images/B18120_06_02.jpg" alt="Figure 6.2 – Ray Tune running 10 trials in parallel on a local multi-core laptop&#13;&#10;" width="760" height="404"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Ray Tune running 10 trials in parallel on a local multi-core laptop</p>
			<p>After approximately 12–14 mins, you <a id="_idIndexMarker404"/>will see that all the trials have finished and the best hyperparameters will be printed out on the screen, as shown<a id="_idIndexMarker405"/> in the following (your results might vary due to the stochastic nature, the limited number of samples, and the use of grid search, which does not guarantee a global optimal):</p>
			<p class="source-code">Best hyperparameters found were: {'lr': 0.025639008922511797, 'batch_size': 64, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}</p>
			<p>You can find the results for each trial under the result log directory, which, by default, is in the current user's <strong class="source-inline">ray_results</strong> folder. From <em class="italic">Figure 6.2</em>, we can see that the results are in <strong class="source-inline">/Users/yongliu/ray_results/hpo_tuning_dl_model</strong>. </p>
			<p>You will see the final output of the best hyperparameters on your screen, which means you have completed running your first HPO experiment! You can see that all 10 trials are logged in the MLflow tracking server, and you can visualize and compare all 10 runs using the parallel coordinates plot provided by the MLflow tracking server. You can produce such a plot by going to the MLflow experiment page and selecting the 10 trials you just finished and then clicking on the <strong class="bold">Compare</strong> button near the top of the page (see <em class="italic">Figure 6.3</em>). This will<a id="_idIndexMarker406"/> bring you to the side-by-side comparison page with the plotting options being displayed at the bottom of the page:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="Images/B18120_06_03.jpg" alt="Figure 6.3 – Clicking Compare to compare all 10 trial runs on the MLflow experiment page&#13;&#10;" width="1193" height="453"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Clicking Compare to compare all 10 trial runs on the MLflow experiment page</p>
			<p>You can<a id="_idIndexMarker407"/> click on the <strong class="bold">Parallel Coordinates Plot</strong> menu item, which allows you to select the parameters and metrics to plot. Here, we select <strong class="bold">lr</strong> and <strong class="bold">batch_size</strong> as the parameters and <strong class="bold">val_f1</strong> and <strong class="bold">val_cross_entropy</strong> as the metrics. The plot is shown in <em class="italic">Figure 6.4</em>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="Images/B18120_06_04.jpg" alt="Figure 6.4 –Parallel Coordinates Plot for comparing the HPO trial results&#13;&#10;" width="1153" height="511"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 –Parallel Coordinates Plot for comparing the HPO trial results</p>
			<p>As you can see in <em class="italic">Figure 6.4</em>, it is very easy to see that <strong class="bold">batch_size</strong> of 128 and <strong class="bold">lr</strong> of 0.02874 produce the best <strong class="bold">val_f1</strong> score of 0.6544 and <strong class="bold">val_cross_entropy</strong> (the loss value) of 0.62222. As mentioned earlier, this HPO run did not use any advanced search algorithms and schedulers, so<a id="_idIndexMarker408"/> let's see whether we can do <a id="_idIndexMarker409"/>better with more experiments in the following sections using early stopping and pruning.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Running HPO with Ray Tune using Optuna and HyperBand</h1>
			<p>Now, let's do some experiments with <a id="_idIndexMarker410"/>different search algorithms and schedulers. Given that Optuna is such a great TPE-based search algorithm, and ASHA is a<a id="_idIndexMarker411"/> great scheduler that does asynchronous parallel trials with early termination of the unpromising ones, it would be<a id="_idIndexMarker412"/> interesting to see how many changes <a id="_idIndexMarker413"/>we need to do to make this work. </p>
			<p>It turns out the change is very minimal based on what we have already done in the previous section. Here, we will illustrate the four main changes:</p>
			<ol>
				<li value="1">Install the <strong class="bold">Optuna</strong> package. This can be done by running the following command:<p class="source-code"><strong class="bold">pip install optuna==2.10.0</strong></p></li>
			</ol>
			<p>This will install Optuna in the same virtual environment that we had before. If you have already run <strong class="source-inline">pip install -r requirements.text</strong>, then Optuna has already been installed and you can skip this step.</p>
			<ol>
				<li value="2">Import the relevant <a id="_idIndexMarker414"/>Ray Tune modules that integrate with Optuna and the ASHA scheduler (here, we use the HyperBand implementation<a id="_idIndexMarker415"/> of ASHA) as follows:<p class="source-code">from ray.tune.suggest import ConcurrencyLimiter</p><p class="source-code">from ray.tune.schedulers import AsyncHyperBandScheduler</p><p class="source-code">from ray.tune.suggest.optuna import OptunaSearch</p></li>
				<li>Now we are ready to <a id="_idIndexMarker416"/>add the search algorithm variable<a id="_idIndexMarker417"/> and scheduler variable to the HPO execution function, <strong class="source-inline">run_hpo_dl_model</strong>, as follows:<p class="source-code">searcher = OptunaSearch()</p><p class="source-code">searcher = ConcurrencyLimiter(searcher, max_concurrent=4)</p><p class="source-code">scheduler = AsyncHyperBandScheduler()</p></li>
			</ol>
			<p>Note that the <strong class="source-inline">searcher</strong> variable is now using Optuna, and we set the maximal number of concurrent runs to <strong class="source-inline">4</strong> for this <strong class="source-inline">searcher</strong> variable to try at any given time during the HPO search process. The scheduler is initialized with the HyperBand scheduler.</p>
			<ol>
				<li value="4">Assign the searcher and scheduler to the corresponding parameters of the <strong class="source-inline">tune.run</strong> call, as follows:<p class="source-code">analysis = tune.run(</p><p class="source-code">    trainable,</p><p class="source-code">    resources_per_trial={</p><p class="source-code">        "cpu": 1,</p><p class="source-code">        "gpu": gpus_per_trial</p><p class="source-code">    },</p><p class="source-code">    metric="f1",</p><p class="source-code">    mode="max",</p><p class="source-code">    config=config,</p><p class="source-code">    num_samples=num_samples,</p><p class="source-code">    <strong class="bold">search_alg=searcher</strong>,</p><p class="source-code">    <strong class="bold">scheduler=scheduler</strong>,</p><p class="source-code">    name="hpo_tuning_dl_model")</p></li>
			</ol>
			<p>Note that <strong class="source-inline">searcher</strong> is assigned to the <strong class="source-inline">search_alg</strong> parameter, and <strong class="source-inline">scheduler</strong> is assigned to the <strong class="source-inline">scheduler</strong> parameter. That's it. Now we are ready to run HPO with Optuna <a id="_idIndexMarker418"/>under the unified Ray Tune framework, with all of the MLflow integration that's already been provided by Ray Tune.</p>
			<p>We have provided the complete <a id="_idIndexMarker419"/>Python code in the <strong class="source-inline">hpo_finetuning_model_optuna.py</strong> file under the <strong class="source-inline">pipeline</strong> folder. Let's run this HPO<a id="_idIndexMarker420"/> experiment as<a id="_idIndexMarker421"/> follows:</p>
			<p class="source-code">python pipeline/hpo_finetuning_model_optuna.py</p>
			<p>You will immediately notice the following in the console output:</p>
			<p class="source-code">[I 2022-02-06 21:01:27,609] A new study created in memory with name: optuna</p>
			<p>This means that we are now using Optuna as the search algorithm. Additionally, you will notice that there are four concurrent trials in the status output displayed on the screen. As time goes by, some trials will be terminated after one or two iterations (epochs) before completion. This means ASHA is at work and has eliminated those unpromising trials to save computing resources <a id="_idIndexMarker422"/>and speed up the searching process. <em class="italic">Figure 6.5</em> shows one of the outputs <a id="_idIndexMarker423"/>during the run where three trials were terminated with only one iteration. You can find <strong class="source-inline">num_stopped=3</strong> in the status output (the third line in <em class="italic">Figure 6.5</em>), where it says <strong class="source-inline">Using AsynHyerBand: num_stopped=3</strong>. This means that <strong class="source-inline">AsyncHyperBand</strong> terminated these three trials before they were completed:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="Images/B18120_06_05.jpg" alt="Figure 6.5 – Running HPO with Ray Tune using Optuna and AsyncHyperBand &#13;&#10;" width="1263" height="541"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Running HPO with Ray Tune using Optuna and AsyncHyperBand </p>
			<p>At the end of the run, you will <a id="_idIndexMarker424"/>see the following results:</p>
			<p class="source-code">2022-02-06 21:11:59,695    INFO tune.py:626 -- Total run time: 632.10 seconds (631.91 seconds for the tuning loop).</p>
			<p class="source-code">2022-02-06 21:11:59,728 Best hyperparameters found were: {'lr': 0.0009599443695046438, 'batch_size': 128, 'foundation_model': 'prajjwal1/bert-tiny', 'finetuning_strategies': 'freeze', 'optimizer_type': 'Adam', 'mlflow': {'experiment_name': 'hpo-tuning-chapter06', 'tracking_uri': 'http://localhost'}}</p>
			<p>Notice that the total run time was only 10 minutes. Compared with the previous section that used grid search without<a id="_idIndexMarker425"/> early stopping, this saves 2–4 minutes. Now, this might seem brief, but remember that we are only using a tiny BERT model here with only 3 epochs. In a production HPO run, using a large pretrained foundation model with 20 epochs is not uncommon, and the speed of searching will be significant with<a id="_idIndexMarker426"/> a good search algorithm combined with a scheduler such as the Asynchronous <a id="_idIndexMarker427"/>HyperBand scheduler. The integration of MLflow provided by Ray Tune comes for free, as we can now switch to a different search algorithm and/or a scheduler under a single framework.</p>
			<p>While this section only shows <a id="_idIndexMarker428"/>you how to use Optuna within the Ray Tune and MLflow framework, replacing Optuna with HyperOpt is a simple drop-in change. Instead of initializing a searcher with <strong class="source-inline">OptunaSearch</strong>, we can use <strong class="source-inline">HyperOptSearch</strong> (you can see an example at <a href="https://github.com/ray-project/ray/blob/d6b0b9a209e3f693afa6441eb284e48c02b10a80/python/ray/tune/examples/hyperopt_conditional_search_space_example.py#L80">https://github.com/ray-project/ray/blob/d6b0b9a209e3f693afa6441eb284e48c02b10a80/python/ray/tune/examples/hyperopt_conditional_search_space_example.py#L80</a>), and the rest of the code is the same. We leave <a id="_idIndexMarker429"/>this as an exercise for you to explore.</p>
			<p class="callout-heading">Using Different Search Algorithms and Schedulers with Ray Tune</p>
			<p class="callout">Note that not all search algorithms can work with any scheduler. What search algorithms and schedulers you choose depends on the model complexity and evaluation cost. For a DL model, since the cost of running one epoch is usually high, it is very desirable to use a modern search algorithm such as TPE, Dragonfly, and BlendSearch, coupled with an ASHA type scheduler such as the HyperBand scheduler that we use. For more detailed guidance on which search algorithms and schedulers to use, you should consult the following documentation on the Ray Tune website: <a href="https://docs.ray.io/en/latest/tune/tutorials/overview.html#which-search-algorithm-scheduler-should-i-choose">https://docs.ray.io/en/latest/tune/tutorials/overview.html#which-search-algorithm-scheduler-should-i-choose</a>.</p>
			<p>Now that we understand how to use Ray Tune and MLflow to do highly parallel and efficient HPO for DL models, this builds the foundation for us to do more advanced HPO experiments at scale in the future. </p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Summary</h1>
			<p>In this chapter, we covered the fundamentals and challenges of HPO, why it is important for the DL model pipeline, and what a modern HPO framework should support. We compared three popular frameworks – Ray Tune, Optuna, and HyperOpt – and picked Ray Tune as the winner for running state-of-the-art HPO at scale. We saw how to create HPO-ready DL model code using Ray Tune and MLflow and ran our first HPO experiment with Ray Tune and MLflow. Additionally, we covered how to switch to other search and scheduler algorithms once we have our HPO code framework set up, using the Optuna and HyperBand schedulers as an example. The learnings from this chapter will help you to competently carry out large-scale HPO experiments in real-life production environments, allowing you to produce high-performance DL models in a cost-effective way. We have also provided many references in the <em class="italic">Further reading</em> section at the end of this chapter to encourage you to study further.</p>
			<p>In our next chapter, we will continue learning how to build preprocessing and postprocessing steps for a model inference pipeline using MLflow, which is a typical scenario in a real production environment after having an HPO-tuned DL model that's ready for production.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Further reading</h1>
			<ul>
				<li><em class="italic">Best Tools for Model Tuning and Hyperparameter Optimization</em>: <a href="https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization%20">https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization</a></li>
				<li>Comparison between Optuna and HyperOpt: <a href="https://neptune.ai/blog/optuna-vs-hyperopt">https://neptune.ai/blog/optuna-vs-hyperopt</a></li>
				<li><em class="italic">How (Not) to Tune Your Model with Hyperopt</em>: <a href="https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html%20">https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html</a></li>
				<li><em class="italic">Why Hyper parameter tuning is important for your model?</em>: <a href="https://medium.com/analytics-vidhya/why-hyper-parameter-tuning-is-important-for-your-model-1ff4c8f145d3">https://medium.com/analytics-vidhya/why-hyper-parameter-tuning-is-important-for-your-model-1ff4c8f145d3</a></li>
				<li><em class="italic">The Art of Hyperparameter Tuning in Deep Neural Nets by Example</em>: <a href="https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38">https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38</a></li>
				<li><em class="italic">Automated Hyperparameter tuning</em>: <a href="https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a">https://insaid.medium.com/automated-hyperparameter-tuning-988b5aeb7f2a</a></li>
				<li><em class="italic">Get better at building PyTorch models with Lightning and Ray Tune</em>: <a href="https://towardsdatascience.com/get-better-at-building-pytorch-models-with-lightning-and-ray-tune-9fc39b84e602">https://towardsdatascience.com/get-better-at-building-pytorch-models-with-lightning-and-ray-tune-9fc39b84e602</a></li>
				<li><em class="italic">Ray &amp; MLflow: Taking Distributed Machine Learning Applications to Production</em>: <a href="https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88">https://medium.com/distributed-computing-with-ray/ray-mlflow-taking-distributed-machine-learning-applications-to-production-103f5505cb88</a></li>
				<li><em class="italic">A Novice's Guide to Hyperparameter Optimization at Scale</em>: <a href="https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/">https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/</a></li>
				<li>A Databricks notebook to run Ray Tune and MLflow on a Databricks cluster: <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6762389964551879/1089858099311442/7376217192554178/latest.html">https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6762389964551879/1089858099311442/7376217192554178/latest.html</a></li>
				<li><em class="italic">A Brief Introduction to Ray Distributed Objects, Ray Tune, and a Small Comparison to Parsl</em>: <a href="https://cloud4scieng.org/2021/04/08/a-brief-introduction-to-ray-distributed-objects-ray-tune-and-a-small-comparison-to-parsl/">https://cloud4scieng.org/2021/04/08/a-brief-introduction-to-ray-distributed-objects-ray-tune-and-a-small-comparison-to-parsl/</a></li>
			</ul>
		</div>
	</div></body></html>