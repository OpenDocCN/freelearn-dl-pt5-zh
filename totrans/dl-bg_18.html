<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Final Remarks on the Future of Deep Learning
                </header>
            
            <article>
                
<p class="mce-root">We have been through a journey together, and if you have read this far you deserve to treat yourself with a nice meal. What you have accomplished deserves recognition. Tell your friends, share what you have learned, and remember to <span>always </span>keep on learning. Deep learning is a rapidly changing field; you cannot sit still. This concluding chapter will briefly present to you some of the new exciting topics and opportunities in deep learning. If you want to continue your learning, we will recommend other helpful resources from Packt that can help you move forward in this field. At the end of this chapter, you will k<span>now where to go from here after having learned the basics of deep learning; you will k</span><span>now what other resources Packt offers for you to continue your training in deep learning. </span></p>
<p>This chapter is organized into the following sections:</p>
<ul>
<li>Looking for advanced topics in deep learning</li>
<li>Learning with more resources from Packt</li>
</ul>
<h1 id="uuid-809f68e7-6ae1-4c16-9a20-e78d66948f90">Looking for advanced topics in deep learning</h1>
<p>The future of deep learning is hard to predict at the moment; things are changing rapidly. However, I believe that if you invest your time in the present advanced topics in deep learning, you might see these areas developing prosperously in the near future.</p>
<p>The following sub-sections discuss some of these advanced topics that have the potential of flourishing and being disruptive in our area.</p>
<p class="mce-root"/>
<h2 id="uuid-814ef2a9-25a9-414b-ba54-918353701ede">Deep reinforcement learning</h2>
<p><strong>Deep reinforcement learning</strong> (<strong>DRL</strong>) is an area that has gained a lot of attention recently given that deep convolutional networks, and other types of deep networks, have offered solutions to problems that were difficult to solve in the past. Many of the uses of DRL are in areas where we do not have the luxury<em> </em>of having data on all possible conceivable cases, such as space exploration, playing video games, or self-driving cars.</p>
<p>Let's expand on the latter example. If we were using traditional supervised learning to make a self-driving car that can take us from point A to point B without crashing, we would not only want to have examples of the positive class with events of successful journeys, but we would also need<strong> </strong>examples of the negative class with bad events such as crashes and terrible driving. Think about this: we would need to crash as many cars as we have successful events to keep the dataset balanced. This is not acceptable; however, reinforcement learning comes to the rescue.</p>
<p>DRL aims to <strong>reward</strong> good driving aspects; the models learn that there is a reward to be gained, so we don't need negative examples. In contrast, traditional learning would need to crash cars in order to <strong>penalize</strong> bad outcomes.</p>
<p>When you use DRL to learn using a simulator, you can get AI that can beat pilots on simulated flights (<a href="https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/">https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/</a>), or you can get AI that can win on a video game simulator. The gaming world is a perfect test scenario for DRL. Let's say that you want to make a DRL model to play the famous game <em>Space Invaders</em>, shown in <em>Figure 15.1</em>; you can make a model that rewards destroying space invaders. </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/192d5354-6c0a-4be2-b3c5-248a2160a925.png" style="width:12.08em;height:15.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.1 – Space invaders video game simulator</div>
<p>If you make a traditional model to teach the user to <strong>not die</strong>, for example, then you will still lose because you will eventually be invaded from space. So, the best strategy to prevent invasion is both not dying and destroying space invaders. In other words, you reward the actions that lead to survival, which are to destroy the space invaders quickly while avoiding being killed by their bombs.</p>
<p>In 2018, a new DRL research tool was released, called <strong>Dopamine</strong> (<span>Castro, P. S., </span>et al.<span> 2018)</span>. Dopamine (<a href="https://github.com/google/dopamine">https://github.com/google/dopamine</a>) is meant for fast prototyping of reinforcement learning algorithms. Back in <a href="0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml">Chapter 2</a>, <em>Setup and Introduction to Deep Learning Frameworks</em>, we asked you to install Dopamine for this moment. We simply want to give you an idea of how easy Dopamine is so that you can go ahead and experiment with it if you are interested. In the following lines of code, we will simply load a pre-trained model (agent) and let it play the game.</p>
<p>This will make sure the library is installed, then load the pre-trained agent:</p>
<pre>!pip install -U dopamine-rl<br/><br/>!gsutil -q -m cp -R gs://download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.data-00000-of-00001 ./<br/>!gsutil -q -m cp -R gs://download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.index ./<br/>!gsutil -q -m cp -R gs://download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.meta ./</pre>
<p>The sample trained agent, which in this case is called <kbd>rainbow</kbd>, is provided by the authors of Dopamine, but you can also train your own if you want.</p>
<p>The next step is to make the agent run (that is, decide to take actions based on the rewards) for a number of steps, say <kbd>1024</kbd>:</p>
<pre>from dopamine.utils import example_viz_lib<br/>example_viz_lib.run(agent='rainbow', game='SpaceInvaders', num_steps=1024,<br/>                    root_dir='./agent_viz', restore_ckpt='./tf_ckpt-199',<br/>                    use_legacy_checkpoint=True)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This piece of code may take a while to run. Internally, it connects to PyGame, which is a resource of game simulators for the Python community. It makes several decisions and avoids space invasion (and dying). As shown in <em>Figure 15.2</em>, the model describes the cumulative rewards obtained across time steps and the estimated probability of return for every action taken, such as stop, move left, move right, and shoot:</p>
<div class="CDPAlignCenter CDPAlign packt_figref">   <img src="assets/06b1145f-dc49-487e-8d6a-2f1471fb009c.png" style="width:46.17em;height:17.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.2 – Left: Calculated rewards of the model across time steps. Right: Estimated probability of returns for every action</div>
<p>One of the interesting things about this is that you can visualize the agent at any of the time steps (frames) and see what the agent was doing at that specific time step using the plot in <em>Figure 15.2</em> as a reference to decide which time step to visualize. Let's say that you want to visualize steps 540 or 550; you can do that as follows:</p>
<pre>from IPython.display import Image<br/>frame_number = 540   # or 550<br/>image_file = '/&lt;path to current directory&gt;/agent_viz/SpaceInvaders/rainbow/images/frame_{:06d}.png'.format(frame_number)<br/>Image(image_file)</pre>
<p>You substitute <kbd>&lt;path to current directory&gt;</kbd> with the path to your current working directory. This is because we need an absolute path, otherwise we could have used a relative path with <kbd>./</kbd> instead.</p>
<p>From this, it is self-evident that all frames are saved as images in the <kbd>./agent_viz/SpaceInvaders/rainbow/images/</kbd> <span>directory. Y</span>ou can display them individually or even make a video. The preceding code produces the images shown in <em>Figure 15.3</em>:</p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a661df7f-d0ab-4ab6-a4d6-7af0359e5025.png"/>   </div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.3 – Left: step 540. Right: step 550</div>
<p>Dopamine is as simple as that. We hope that you will be inspired by reinforcement learning and investigate further.</p>
<h2 id="uuid-96425445-639e-4195-aeec-8a44e3047ea5">Self-supervised learning</h2>
<p>Yann LeCun, one of the 2018 ACM Turing Award winners, said at the AAAI conference in 2020: <em>"the future is self-supervised."</em> He was implying that this area is exciting and has a lot of potential.</p>
<p>Self-supervision is a relatively new term used to move away from the term <em>unsupervision</em>. The term "unsupervised learning" might give the impression that there is no supervision when, in fact, unsupervised learning algorithms and models typically use more supervisory data than supervised models. Take, for example, the classification of MNIST data. It uses 10 labels as supervisory signals. However, in an autoencoder whose purpose is perfect reconstruction, every single pixel is a supervisory signal, so there are 784 supervisory signals from a 28 x 28 image, for example.</p>
<p class="mce-root"/>
<p>Self-supervision is also used to mean models that combine some of the stages of unsupervised and supervised learning. For example, if we pipeline a model that learns representations unsupervised, then we can attach a model downstream that will learn to classify something supervised.</p>
<p>Many of the recent advances in deep learning have been in self-supervision. It will be a good investment of your time if you try to learn more about self-supervised learning algorithms and models.</p>
<h2 id="uuid-cc9b5787-d46d-492e-a7ab-eeb446250b91">System 2 algorithms</h2>
<p>The famous economist Daniel Kahneman made popular the theory of dual process with his book <em>Thinking Fast and Slow</em> (Kahneman, D. 2011). The main idea is that there are highly complex tasks that we, as humans, are good at developing relatively fast and often without thinking too much; for example, drinking water, eating food, or looking at an object and recognizing it. These processes are done by <em>System 1</em>.</p>
<p>However, there are tasks that are not quite simple for the human mind, tasks that require our fully devoted attention, such as driving on an unfamiliar road, looking at a strange object that does not belong within the assumed context, or understanding an abstract painting. These processes are done by <em>System 2</em>. Another winner of the 2018 ACM Turing Award, Yoshua Bengio, has made the remark that deep learning has been very good at System 1 tasks, meaning that existing models can recognize objects and perform highly complex tasks relatively easily. However, deep learning has not made much progress on System 2 tasks. That is, the future of deep learning will be in solving those tasks that are very complex for human beings, which will probably involve combining different models across different domains with different learning types. Capsule neural networks might be a good alternative solution to System 2 tasks (Sabour, S., et al. 2017<em>)</em>. </p>
<p>For these reasons, System 2 algorithms will probably be the future of deep learning.</p>
<p>Now, let's look at resources available from Packt that can help in further studying these ideas.</p>
<h1 id="uuid-c4bd55a7-955f-40a1-8cdc-18761f398cc7">Learning with more resources from Packt</h1>
<p>The following lists of books is not meant to be exhaustive, but a starting point for your next endeavor. These titles have come out at a great time when there is a lot of interest in the field. Regardless of your choice, you will not be disappointed.</p>
<h2 id="uuid-d58e74f9-200b-4dd1-8dd6-088cd13184fd">Reinforcement learning</h2>
<ul>
<li><em>Deep Reinforcement Learning Hands-On - Second Edition</em>, by Maxim Lapan, 2020.</li>
<li><em>The Reinforcement Learning Workshop</em>, by Alessandro Palmas <em>et al.</em>, 2020.</li>
<li><em>Hands-On Reinforcement Learning for Games</em>, by Micheal Lanham, 2020.</li>
<li><em>PyTorch 1.x Reinforcement Learning Cookbook</em>, by Yuxi Liu, 2019.</li>
<li><em>Python Reinforcement Learning</em>, by Sudharsan Ravichandiran, 2019.</li>
<li><em>Reinforcement Learning Algorithms with Python</em>, by Andrea Lonza, 2019.</li>
</ul>
<h2 id="uuid-b5950d13-0617-48a0-b8f3-ac9f6f94fa6f">Self-supervised learning</h2>
<ul>
<li><em>The Unsupervised Learning Workshop</em>, by Aaron Jones <em>et. al.</em>, 2020.</li>
<li><em>Applied Unsupervised Learning with Python</em>, by Benjamin Johnston <em>et. al.</em><span>, 2019.</span></li>
<li><em>Hands-On Unsupervised Learning with Python</em>, by Giuseppe Bonaccorso<span>, 2019.</span></li>
</ul>
<h1 id="uuid-59b7e059-4c66-46bd-a114-38b787a564e0">Summary</h1>
<p>This final chapter briefly covered new exciting topics and opportunities in deep learning. We discussed reinforcement learning, self-supervised algorithms, and System 2 algorithms. We also recommended some further resources from Packt, hoping that you will want to continue your learning and move forward in this field. At this point, you should know where to go from here, and be inspired by the future of deep learning. You should be knowledgeable of other recommended books in the area to continue with your learning journey.</p>
<p>You are the future of deep learning, and the future is today. Go ahead and make things happen. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-86aef930-99d6-4413-9160-00d42425cab9">References</h1>
<ul>
<li><span>Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. (2018). Dopamine: A research framework for deep reinforcement learning. </span>arXiv preprint arXiv:1812.06110<span>. </span></li>
<li>Kahneman, D. (2011). <em>Thinking, Fast and Slow</em>. <em>Macmillan</em><span>.</span></li>
<li>Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules. In <em>Advances in neural information processing systems</em> <span>(pp. 3856-3866).</span></li>
</ul>


            </article>

            
        </section>
    </body></html>