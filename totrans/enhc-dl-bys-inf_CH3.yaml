- en: Chapter 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentals of Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, when studying how to apply Bayesian methods and extensions
    to neural networks, we will encounter different neural network architectures and
    applications. This chapter will provide an introduction to common architecture
    types, thus laying the foundation for introducing Bayesian extensions to these
    architectures later on. We will also review some of the limitations of such common
    neural network architectures, in particular their tendency to produce overconfident
    outputs and their susceptibility to adversarial manipulation of inputs. By the
    end of this chapter, you should have a good understanding of deep neural network
    basics and know how to implement the most common neural network architecture types
    in code. This will help you follow the code examples found in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content will be covered in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the multi-layer perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing neural network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the problem with typical neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.1 Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete the practical tasks in this chapter, you will need a Python 3.8
    environment with the `pandas` and `scikit-learn` stack and the following additional
    Python packages installed:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib plotting library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Introducing the multi-layer perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep neural networks are at the core of the deep learning revolution. The aim
    of this section is to introduce basic concepts and building blocks for deep neural
    networks. To get started, we will review the components of the **multi-layer perceptron**
    (**MLP**) and implement it using the `TensorFlow` framework. This will serve as
    the foundation for other code examples in the book. If you are already familiar
    with neural networks and know how to implement them in code, feel free to jump
    ahead to the *Understanding* *the problem with typical NNs* section, where we
    cover the limitations of deep neural networks. This chapter focuses on architectural
    building blocks and principles and does not cover learning rules and gradients.
    If you require additional background information for those topics, we recommend
    Sebastian Raschka’s excellent *Python Machine Learning* book from Packt Publishing
    (in particular, [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian*
    *Inference*](CH2.xhtml#x1-250002)).
  prefs: []
  type: TYPE_NORMAL
- en: The MLP is a feed-forward, fully connected neural network. Feed-forward means
    that the information in an MLP is only passed in one direction, from the input
    to the output layers; there are no backward connections. Fully connected means
    that each neuron is connected to all the neurons in the previous layer. To understand
    these concepts a bit better, let’s have a look at *Figure*  [3.1](#x1-37007r1),
    which gives a diagrammatic overview of an MLP. In this example, the MLP has an
    **input layer** with three neurons (shown in red), two **hidden layers** with
    four neurons each (shown in blue), and one **output layer** with a single output
    node (shown in green). Imagine, for example, that we wanted to build a model that
    predicts housing prices in London. In this example, the three input neurons would
    represent values of three input features of our model, such as distance from the
    city centre, floor area, and the construction year of the house. As indicated
    by the black connections in the figure, these input values are then passed to
    and aggregated by each of the neurons of the first hidden layer. The values of
    these neurons are then, in turn, passed to and aggregated by the neurons in the
    second hidden layer and, finally, the output neuron, which will represent the
    house value predicted by our model.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Diagram of a multi-layer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean exactly for a neuron to aggregate values? To understand this
    a bit better, let’s focus on a single neuron and the operations that it performs
    on the values that are passed to it. In *Figure* [*3.2*](#x1-37013r2), we have
    taken the network shown in *Figure* [*3.1*](#x1-37007r1) (left panel) and zoomed
    in on the first neuron in the first hidden layer and the neurons that pass values
    to it (central panel). In the right panel of the figure, we have slightly rearranged
    the neurons and have named the input neurons *x*[1], *x*[2], and *x*[3]. We have
    also made the connections explicit, by naming the weights associated with them
    *w*[1], *w*[2], and *w*[3], respectively. From the right panel in the figure,
    we can see that an artificial neuron performs two essential operations:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it takes a weighted average over its inputs (indicated by the Σ).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, it takes the output of the first step and applies a non-linearity to
    it (indicated by the *σ*. Note that this does not indicate the standard deviation,
    which is what we’ll use *σ* for throughout most of the book), such as a sigmoid
    function, for example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first operation can be exdivssed more formally as *z* = ∑ [*n*=1]³*x*[*n*]*w*[*n*].
    The second operation can be exdivssed as *a* = *σ*(*z*) = ![ 1 1+e−-z](img/file55.jpg).
    The activation value of the neuron *a* = *σ*(*z*) is then passed to the neurons
    in the second hidden layer, where the same operations are repeated.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Aggregation and transformation performed by an artificial neuron
    in a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have reviewed the different parts of an MLP model, let’s implement
    one in TensorFlow. First, we need to import all the necessary functions. These
    include `Sequential` to build a feed-forward model such as MLP, `Input` to build
    the input layer, and `Dense` to build a fully-connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Equipped with these tools, implementing the MLP is a simple matter of chaining
    `Input` and `Dense` layers in the right order and with the right number of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The aggregation in terms of weighted averaging is automatically handled by TensorFlow
    when using the `Dense` layer object. Furthermore, implementing an activation function
    becomes a simple matter of passing the name of the desired function to the `activation`
    parameter of the Dense layer (`sigmoid` in the preceding example).
  prefs: []
  type: TYPE_NORMAL
- en: Before we turn to other neural network architectures besides the MLP, a side
    note on the word *deep*. A neural network is considered deep if it has more than
    one hidden layer. The MLP shown previously, for example, has two hidden layers
    and can be considered a deep neural network. It is possible to add more and more
    hidden layers, creating very deep neural network architectures. Training such
    deep architectures comes with its own set of challenges and the science (or art)
    of training such deep architectures is called **deep learning** (**DL**).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll learn about some of the common deep neural network
    architectures and in the section thereafter, we will look at the practical challenges
    that come with them.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Reviewing neural network architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw how to implement a fully-connected network in
    the form of an MLP. While such networks were very popular in the early days of
    deep learning, over the years, machine learning researchers have developed more
    sophisticated architectures that work more successfully by including domain-specific
    knowledge (such as computer vision or **Natural Language** **Processing** (**NLP**)).
    In this section, we will review some of the most common of these neural network
    architectures, including **Convolutional Neural** **Networks** (**CNNs**) and
    **Recurrent Neural Networks** (**RNNs**), as well as attention mechanisms and
    transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Exploring CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When looking back at the example of trying to predict London housing prices
    with an MLP model, the input features we used (distance to the city centre, floor
    area, and construction year of the house) were still ”hand-engineered,” meaning
    that a human looked at the problem and decided which inputs might be relevant
    to the model when making price predictions. What might such input features look
    like if we were trying to build a model that takes in images as input and tries
    to predict which object is shown in the image? One breakthrough moment for deep
    learning was the realization that neural networks can directly learn and extract
    the most useful features for a task from the raw data – in the case of visual
    object classification, these features are learned directly from the pixels in
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: What would a neural network architecture need to look like if we wanted to extract
    the most relevant input features from an image for an object classification task?
    When trying to answer this question, early machine learning researchers turned
    to mammalian brains. Object classification is a task that our visual system performs
    relatively effortlessly. One observation that inspired the development of CNNs
    was that the visual cortex responsible for object recognition in mammals implements
    a hierarchy of feature extractors that work with increasingly large receptive
    fields. A **receptive** **field** is the area in the image that a biological neuron
    responds to. The neurons at the early layers of the visual cortex respond to relatively
    small regions of an image only, while neurons in layers higher up the hierarchy
    respond to areas that cover large parts (or even the entirety) of an input image.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the cortical hierarchy in the brain, CNNs implement a hierarchy
    of feature extractors with artificial neurons higher up in the hierarchy having
    larger receptive fields. To understand how that works, let’s look at how CNNs
    build features based on input images. *Figure* [*3.3*](#x1-39005r3) shows an early
    convolutional layer in a CNN operating on the input image (shown on the left)
    to extract features into a feature map (shown on the right). You can imagine the
    feature map as a matrix with *n* rows and *m* columns and every feature in the
    feature map as a scalar value. The example highlights two instances where the
    convolutional layer operates on different local regions of the image. In the first
    instance, the feature in the feature map receives input from the face of the kitten.
    In the second instance, the feature receives inputs from the kitten’s right paw.
    The final feature map will be the result of repeating this same operation over
    all regions of the input image, sliding a kernel from left to right and from top
    to bottom to fill all the values in the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Building a feature map from the input image'
  prefs: []
  type: TYPE_NORMAL
- en: What does such a single operation look like numerically? This is illustrated
    in *Figure* [*3.4*](#x1-39007r4).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Numerical operations performed by convolutional layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have zoomed in on one part of the input image and made its pixel values
    explicit (left side). You can imagine that the kernel (shown in the middle) slides
    over the input image step by step. In the step that is shown in the figure, the
    kernel is operating on the upper-left corner of the input image (highlighted in
    red). Given the values in the input image and the kernel values, the final value
    in the feature map (**28** in the example) is obtained via weighted averaging:
    each value in the input image is weighted by the corresponding value in the kernel,
    which yields 9 ∗ 0 + 3 ∗ 1 + 1 ∗ 0 + 4 ∗ 0 + 8 ∗ 2 + 5 ∗ 0 + 5 ∗ 1 + 2 ∗ 1 + 2
    ∗ 1 = 28.'
  prefs: []
  type: TYPE_NORMAL
- en: Slightly more formally, let us denote the input image by *x* and the kernel
    by *w*. Convolution in a CNN can then be exdivssed as *z* = ∑ [*i*=1]^(*n*) ∑
    [*j*=1]^(*m*)*x*[*i,j*]*w*[*i,j*]. This is usually followed by a non-linearity,
    *a* = *σ*(*z*), just like for the MLP. *σ* could be the sigmoid function introduced
    previously, but a more popular choice for CNNs is the **Rectified Linear Unit**
    (**ReLU**), which is defined as *ReLU*(*z*) = *max*(0*,z*).
  prefs: []
  type: TYPE_NORMAL
- en: In modern CNNs, many of these convolutional layers will be stacked on top of
    each other, such that the feature map that forms the output of one convolutional
    layer will serve as the input (image) for the next convolutional layer, and so
    forth. Putting convolutional layers in sequence like this allows the CNN to build
    more and more abstract feature representations. When studying feature maps at
    different positions of the hierarchy, it was shown by Matthew Zeiler et al. (see
    *Further reading*) that feature maps at early convolutional layers often show
    edges and simple textures, while feature maps at later convolutional layers show
    more complex patterns and parts of objects. Similar to the visual cortical hierarchy,
    neurons in later convolutional layers will tend to have larger receptive fields
    because they accumulate input from several earlier neurons, which in turn receive
    inputs from different local regions of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of convolutional layers that are stacked on top of each other will
    determine the depth of a CNN: the more layers, the deeper the network. Another
    important dimension for a CNN is its width, which is determined by the number
    of convolutional kernels per layer. You can imagine that we can apply more than
    one kernel at a given convolutional layer, which will result in additional feature
    maps – one for every additional kernel. In this case, the kernels in the subsequent
    convolutional layer will need to be three-dimensional in order to handle the multitude
    of feature maps in the input, where the third dimension of the kernel will be
    determined by the number of incoming feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: Along with convolutional layers, another common building block for CNNs is **pooling
    layers**, in particular **mean-pooling** and **max-pooling** layers. The function
    of these layers is to sub-sample the input, which reduces the input size of the
    image and thus the subsequent number of parameters needed in the network (and
    thus reduces the computational load and memory footprint).
  prefs: []
  type: TYPE_NORMAL
- en: How do pooling layers operate? In *Figure* [*3.5*](#x1-39017r5), we see both
    a mean-pooling (left) and max-pooling (right) layer in operation. We see that,
    like convolutional layers, they operate on local regions of the input. The operations
    they perform are straightforward – either they take the mean or the maximum of
    the pixel values in their receptive field.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Numerical operations performed by pooling layers'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to computational and memory considerations, another advantage of
    pooling layers is that they can make the network more robust to small variations
    in the input. Imagine, for example, one of the input pixel values in the example
    changed to 0\. This will either affect the output very little (mean-pooling layer)
    or not at all (max-pooling layer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have reviewed the essential operations, let’s implement a CNN in
    TensorFlow. Importing all the necessary functions includes the already familiar
    `Sequential` function to build a feed-forward model as well as the `Dense` layer.
    In addition, this time, we also import `Conv2D` for convolution and `MaxPooling2D`
    for max-pooling. With these tools, we can implement a CNN by chaining these layer
    functions in the right order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have built a CNN by chaining a convolutional layer with 32 kernels, followed
    by a max-pooling operation, followed by a convolutional layer with 64 kernels,
    and another max-pooling operation. In the end, we add two `Dense` layers. The
    final `Dense` layer will serve to match the number of output neurons to the number
    of classes in a classification problem. In the preceding example, that number
    is `10`. Our network is now ready for us to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs have become crucial for a broad variety of problems, forming a key component
    in systems designed for a whole range of problems, from self-driving cars through
    to medical imaging. They also provided a foundation for other important neural
    network architectures, such as **graph convolutional** **networks** (**GCNs**).
    But the field of deep learning wasn’t able to dominate the world of machine learning
    with CNNs alone. In the next section, we’ll learn about another important architecture:
    the recurrent neural network, an invaluable method for processing sequential data.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Exploring RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The neural networks that we have seen so far are what we call feedforward networks:
    each layer of the network feeds into the next layer of the network; there is no
    cycle. Moreover, the convolutional neural networks we looked at receive a single
    input (an image) and output a single output: a label or a score for that label.
    But there are many cases in which we are working with something more complex than
    a single input, single output task. In this section, we will focus on a family
    of models called **recurrent neural networks** (**RNNs**), which focus on processing
    sequences of inputs, with some also producing sequential outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical example of an RNN task is machine translation. For example, translating
    the English sentence, ”the apple is green,” to French. For such a task to work,
    a network needs to consider the relationship between the inputs we feed it. Another
    task could be video classification, where we need to look at different frames
    of a video to classify the content of the video. An RNN processes an input one
    step at a time, where every time step can be denoted as [*t*]. At every time step,
    the model computes a hidden state *h*[*t*] and an output *y*[*t*]. But to compute
    *h*[*t*], the model does not only receive the input *x*[*t*] but also the hidden
    state at the previous time step *h*[*t*−1]. For a single time step, a vanilla
    RNN thus computes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ht = f(Wxxt + Whht− 1 + b) ](img/file60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*W*[*x*] are the weights of the RNN for the input *x*[*t*]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W*[*h*] are the weights for the hidden layer output from the previous time
    step *h*[*t*−1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the bias term'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f* is an activation function – in a vanilla RNN a *tanh* activation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, at every time step, the model also has awareness of what happened
    at previous time steps because of the additional input *h*[*t*−1].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the flow of an RNN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Example of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we need an initial hidden state as well at time step zero. This
    is usually just a vector of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important variant of a vanilla neural network is a **sequence-to-sequence**
    (**seq2seq**) neural network, a popular paradigm in machine translation. The idea
    of this network is, as the name suggests, to take a sequence as input and output
    another sequence. Importantly, both sequences do not have to be of the same length.
    This enables the architecture to translate sentences in a more flexible way, which
    is crucial as different languages do not use the same number of words in sentences
    with the same meaning. This flexibility is achieved with an encoder-decoder architecture.
    This means that we will have two parts of our NN: an initial part that encodes
    the input up until a single weight (many inputs encoded to one hidden vector)
    matrix that is then used as input for the decoder of the network to produce multiple
    outputs (one input to many outputs). The encoder and the decoder have separate
    weight matrices. For a model with two inputs and two outputs, this can be visualized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Example of a sequence-to-sequence network'
  prefs: []
  type: TYPE_NORMAL
- en: In this figure *w*[*e*] are the weights of the encoder and *w*[*d*] are the
    weights of the decoder. We can see that compared to our RNN, we now have a new
    hidden state of the decoder *s*[0] and we can also observe *c*, which is a context
    vector. In standard sequence-to-sequence models, *c* is equal to the hidden state
    at the end of the encoder, whereas *s*[0], the initial hidden state of the encoder,
    is typically computed with one or more feedforward layers. The context vector
    is an additional input to each part of the decoder; it allows each part to use
    the information of the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Attention mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although recurrent neural network models can be powerful, they have an important
    disadvantage: all information that the encoder can give to the decoder has to
    be in the hidden bottleneck layer – the hidden input state that the decoder receives
    at the start. That is fine for short sentences, but you can imagine that this
    becomes more difficult when we want to translate an entire paragraph or a very
    long sentence. We simply cannot expect that a single vector contains all information
    that is required to translate a long sentence. This downside is solved by a mechanism
    called attention. Later on, we will generalize the concept of attention, but let
    us first see how attention can be applied in the context of a seq2seq model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention allows the decoder of a seq2seq model to ”attend” to hidden states
    of the encoder according to some attention weights. This means that instead of
    having to rely on the bottleneck layer to translate the input, the decoder can
    go back to each hidden state of the encoder and decide how much information it
    wants to use. This is done via a context vector at every time step of the decoder
    that now functions as a probability vector, determining how much weight to give
    to each of the hidden states of the encoder. We can think of attention in this
    context as the following sequence for every time step of the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '*e*[*t,i*] = *f*(*s*[*t*−1]*,h*[*i*]) computes *alignment scores* for every
    hidden state of the encoder. This computation can be an MLP for every hidden state
    of the encoder, taking as input the current hidden state of the decoder *s*[*t*−1],
    and *h*[*i*] the hidden states of the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*e*[*t,i*] gives us alignment scores; they tell us something about the relation
    of every hidden state in the encoder and a single hidden state of the decoder.
    But the output of *f* is a scalar, which makes it impossible to compare different
    alignment scores. That is why we then take the softmax over all alignment scores
    to get a probability vector; attention weights: *a*[*t,i*] = *softmax*(*e*[*t,i*]).
    These weights are now values between 0 and 1 and tell us, for a single hidden
    state in the decoder, how much weight to give to every hidden state of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With our attention weights, we now take a weighted average of the hidden states
    of the encoder. This produces the context vector *c*[1], which can be used for
    the first time step of the decoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because this mechanism computes attention weights for every time step of the
    decoder, the model is now much more flexible: at every time step, it knows how
    much weight to give to each part of the encoder. Moreover, because we are using
    an MLP here to compute the attention weights, this mechanism can be trained end
    to end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This tells you a way to use attention in a sequence-to-sequence model. But
    the attention mechanism can be generalized to make it even more powerful. This
    generalization is used as a building block in the most powerful neural network
    applications you see today. It uses three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: Queries, denoted as *Q*. You can think of these as the hidden states of the
    decoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keys, denoted as *K*. You can think of the keys as the hidden state of the inputs
    *h*[*i*].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values, denoted as *V* . In the standard attention mechanism, these are the
    same as the keys, but just separated as a separate value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, the queries, keys, and values form the attention mechanism in the
    form of
  prefs: []
  type: TYPE_NORMAL
- en: '![ QKT Attention (Q, K, V) = softmax (-√---)V dk ](img/file63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can distinguish three generalizations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using an MLP to compute the attention weights is a relatively heavy operation
    for every time step. Instead, we can use something more lightweight that allows
    us to compute the attention weights for every hidden state of the decoder much
    faster: we use a scaled dot product of the hidden state of the decoder and the
    hidden states of the encoder. We scale the dot product by the square root of the
    dimension of *K*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![ T Q√K--- dk ](img/file64.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'This is because of two reasons:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The softmax operation can lead to extreme values – values very close to zero
    and very close to one. This makes the optimization process more difficult. By
    scaling the dot product, we avoid this issue.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism takes the dot product of vectors with a high dimension.
    This causes the dot product to be very large. By scaling the dot product, we counteract
    this tendency.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the input vectors separately – we separate them as keys and values in
    different input streams. This gives the model more flexibility to handle them
    in different ways. Both are learnable matrices, so the model can optimize both
    in different ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention takes a set of inputs as the query vector. This is more computationally
    efficient; instead of computing the dot product for every single query vector,
    we can do this for all of them at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three generalizations make attention a very widely applicable algorithm.
    You see it in most of today’s most performant models, some of the best image classification
    models – large language models that generate very realistic text or text-to-image
    models that can create the most beautiful and creative images. Because of the
    wide use of the attention mechanism, it is easily available in TensorFlow and
    other deep learning libraries. In TensorFlow, you can use attention like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And it can be called with our query, key, and value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding sections, we discussed some important building blocks of neural
    networks; we discussed the basic MLP, the concept of convolution, recurrent neural
    networks, and attention. There are other components we did not discuss here, and
    even more variants and combinations of the components we discussed. If you want
    to know more about these building blocks, refer to the reading list at the end
    of this chapter. There are excellent resources available in case you want to dive
    deeper into neural network architectures and components.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Understanding the problem with typical neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deep neural networks we discussed in previous sections are extremely powerful
    and, paired with appropriate training data, have enabled big strides in machine
    perception. In machine vision, convolutional neural networks enable us to classify
    images, locate objects in images, segment images into different segments or instances,
    and even to generate entirely novel images. In natural language processing, recurrent
    neural networks and transformers have allowed us to classify text, to recognize
    speech, to generate novel text or, as reviewed previously, to translate between
    two different languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, these standard types of neural network models also have several limitations.
    In this section, we will explore some of these limitations. We will look at the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: How the prediction scores of such neural network models can be overconfident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How such models can produce very confident predictions on OOD data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How tiny, imperceptible changes in an input image can cause a model to make
    completely wrong predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.4.1 Uncalibrated and overconfident predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One problem with modern vanilla neural networks is that they often produce outputs
    that are not well-calibrated. This means that the confidence scores produced by
    these networks no longer represent their empirical correctness. To understand
    better what that means, let’s look at the reliability plot for the ideally-calibrated
    network that is shown in *Figure* [*3.8*](#x1-43002r8).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Reliability plot for a well-calibrated neural network. The empirically
    determined (”actual”) accuracy is consistent with the prediction values output
    by the network'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the reliability plot shows accuracy (on the *y* axis) as a function
    of confidence (on the *x* axis). The basic idea is that, for a well-calibrated
    network, the output (or confidence) score associated with a prediction should
    match its empirical correctness. Here, empirical correctness is defined as the
    accuracy of a group of samples that all share similar output values and, for that
    reason, are grouped into the same bin in the reliability plot. So, for example,
    for the group of samples that all were assigned an output score between 0.7 and
    0.8, the expectation, for a well-calibrated network, would be that 75% of these
    predictions should be correct.
  prefs: []
  type: TYPE_NORMAL
- en: To make this idea a bit more formal, let’s imagine that we have a dataset **X**
    with **N** data samples. Each data sample **x** has a corresponding target label
    **y**. In a classification setting, we would have *y*, which represents the membership
    to one in *K* classes. To obtain a reliability plot, we would use a neural network
    model to run inference over the entire dataset **X** to obtain an output score
    *ŷ* per sample **x**. We would then use the output scores to assign each data
    sample to a bin *m* in the reliability plot. In the preceding figure, we have
    opted for *M* = 10 bins. *B*[*m*] would be the set of indices of samples that
    fall into bin *m*. Finally, we would measure and plot the average accuracy of
    the predictions for all the samples in a given bin, which is defined as *acc*(*B*[*m*])
    = ![-1-- |Bm |](img/file66.jpg)∑ [*i*∈*B*[m]]1(*ŷ*[*i*] = *y*[*i*]).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a well-calibrated network, the average accuracy of the samples
    in a given bin should match the confidence values of that bin. In the preceding
    figure, we can see, for example, that for samples that fell in the bin for output
    scores between 0.2 and 0.3, we observed a matching average accuracy of 0.25\.
    Now let’s see what happens in the case of an uncalibrated network that is over-confident
    in its predictions. Figure  [3.9](#x1-43004r9) illustrates this scenario, which
    is representative of the behavior shown by many modern vanilla neural network
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Reliability plot for an overconfident neural network. The ”actual”
    empirically determined accuracy (purple bars) is consistently below the accuracy
    suggested by the prediction values of the network (pink bars and gray dashed line)'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that for all the bins, the observed (”actual”) accuracy for the
    samples in the bin is below the accuracy that is expected based on the output
    score of the samples. This is the exdivssion of over-confident predictions. The
    network’s output makes us believe that it has a high degree of confidence in its
    predictions, while in reality the actual performance does not match up.
  prefs: []
  type: TYPE_NORMAL
- en: Over-confident predictions can be very problematic in safety- and mission-critical
    applications, such as medical decision making, self-driving cars, and legal or
    financial decisions. Networks with overconfident predictions will lack the ability
    to indicate to us humans (or other networks) when they are likely to be wrong.
    This lack of awareness can become dangerous, for example, when a network is used
    to help decide whether a defendant should be granted bail or not. Assume a network
    is presented with a defendant’s data (such as past convictions, age, education
    level) and predicts with 95% confidence score that bail should not be granted.
    presented with this output, a judge might falsely think that the model can be
    trusted and base their verdict largely on the model’s output. By contrast, calibrated
    confidence outputs can indicate to what degree we can trust the model’s output.
    If the model is uncertain, this indicates that there is something about the input
    data that isn’t well represented in the model’s training data – indicating that
    the model is more likely to make a mistake. Thus, well-calibrated uncertainties
    allow us to decide whether to incorporate the model’s predictions in our decision
    making, or whether to ignore the model entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drawing and inspecting reliability plots is useful for visualizing calibration
    in a few neural networks. However, sometimes we want to compare the calibration
    performance across several neural networks, possibly each network using more than
    one configuration. In such cases where we need to compare many networks and settings,
    it is useful to summarize the calibration of a neural network in a scalar statistic.
    The **Expected Calibration Error** (**ECE**) represents such a summary statistic.
    For every bin in the reliability plot, it measures the difference between the
    observed accuracy, *acc*(*B*[*m*]), and the accuracy we would have expected based
    on the output score of the samples, *conf*(*B*[*m*]), which is defined as ![-1--
    |Bm |](img/file68.jpg)∑ [*i*∈*B*[m]]*ŷ*. Then, it takes a weighted average across
    all bins, where the weight for each bin is determined by the number of samples
    in the bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ M∑ ECE = |Bm-||acc(Bm ) − conf(Bm )| m=1 n ](img/file69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This provides a first introduction to ECE and how it is measured. We will revisit
    ECE in more detail in [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian
    Deep Learning*](CH8.xhtml#x1-1320008) by providing a code implementation as part
    of the *Revealing dataset shift with Bayesian* *methods* case study.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a perfectly calibrated neural network output, the *ECE* would
    be zero. The more uncalibrated a neural network is, the larger *ECE* would become.
    Let us look at some of the reasons for which neural networks are poorly calibrated
    and overconfident.
  prefs: []
  type: TYPE_NORMAL
- en: 'One reason is that the softmax function, which is usually the last operation
    of a classification network, uses the exponential function to make sure that all
    values are positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ zi σ(⃗z)i = ∑--e----- Kj=1ezj ](img/file70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result of this is that small changes of the input to the softmax function
    can lead to substantial changes in its output.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for overconfidence is the increased model capacity of modern
    deep neural networks (Chuan Guo, et al. (2017)). Neural network architectures
    have become deeper (more layers) and wider (more neurons per layer) over the years.
    Such deep and wide neural networks have high variance and can very flexibly fit
    large amounts of input data. When experimenting with either the number of layers
    for a neural network or the number of filters per layer, Chuan Guo et al. observed
    that mis-calibration (and thus overconfidence) becomes worse with deeper and wider
    architectures. They also found that using batch normalization or training with
    less weight decay had a negative impact on calibration. These observations point
    to the conclusion that increased model capacity for modern deep neural network
    contributes to their overconfidence.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, overconfident estimates can result from choosing particular neural
    network components. It has been shown, for example, that fully-connected networks
    that use ReLU functions lead to continuous piecewise affine classifiers ([**?**]).
    This, in turn, implies that it is always possible to find input samples for which
    the ReLU network will produce high-confidence outputs. This holds even for input
    samples that are unlike the training data, for which generalization performance
    might be poor and we would thus expect lower confidence outputs. Such arbitrarily
    high confidence predictions also apply to convolutional networks that use either
    max- or mean-pooling following the convolutional layers, or any other network
    that results in a piecewise affine classifier function.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is inherent to such neural network architectures and can only be
    addressed by changing the architecture itself ([**?**]).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Predictions on out-of-distribution data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have seen that models can be overconfident and therefore uncalibrated,
    let’s look at another problem of neural networks. Neural networks are typically
    trained under the assumption that our test and train data are drawn from the same
    distribution. In practice, however, that is not always the case. The data that
    a model sees when it is deployed in the real world can change. We call these changes
    dataset shifts and they are typically divided into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariate shift**: The feature distribution *p*(*x*) changes while *p*(*y*|*x*)
    is fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-set recognition**: New labels appear at test time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label shift**: The distribution of labels *p*(*y*) changes while *p*(*x*|*y*)
    is fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of the preceding items include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariate shift: A model is trained to recognize faces. The training data consists
    of faces of mostly young people. The model at test time sees faces of all ages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open-set recognition: A model is trained to classify a limited number of dog
    breeds. At test time, the model sees more dog breeds than present in the training
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label shift: A model is trained to predict different diseases, of which some
    are very rare at the time of training. However, over time, the frequency of a
    rare disease changes and it becomes one of the most frequently seen diseases at
    test time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![PIC](img/file71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: The training data distribution and data in the real world mostly
    overlap, but we cannot expect our model to perform well on the out-of-distribution
    points in the top right of the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of these changes, a model might be less performant when deployed
    in the real world if it is confronted with data that was not drawn from the same
    distribution as the training data. How likely a model is to be confronted with
    out-of-distribution data depends very much on the environment in which the model
    is deployed: some environments are more static (lower chance of out-of-distribution
    data), while others are more dynamic (higher chance of out-of-distribution data).'
  prefs: []
  type: TYPE_NORMAL
- en: One reason for the problems of deep learning models with out-of-distribution
    data is that models often have a large number of parameters and can therefore
    memorize specific patterns and features of the training data instead of robust
    and meaningful representations of the data that reflect the underlying data distribution.
    When new data that looks slightly different from the training data presents itself
    at test time, the model does not actually have the ability to generalize and make
    the right prediction. One such example is an image of a cow (*Figure* [*3.11*](#x1-44008r11))
    on a beach, when cows in the training dataset happened to be on green grasslands.
    Models often use the context present in the data to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: An object in a different environment (here, a cow on a beach)
    can make it difficult for a model to recognize that the image contains the object'
  prefs: []
  type: TYPE_NORMAL
- en: Before we go over a practical example of how a simple model handles out-of-distribution
    data, let’s examine a few approaches that can highlight the problem of out-of-distribution
    data in typical neural networks. Ideally, we would like our model to exdivss high
    uncertainty when it encounters data that is different from the distribution it
    was trained on. If that is the case, out-of-distribution data will not be a big
    problem when a model is deployed in the real world. For example, in mission-critical
    systems where errors of a model are costly, there is often a certain confidence
    threshold that should be met before the prediction is trusted. If a model is well-calibrated
    and it assigns a low confidence score to out-of-distribution inputs, the business
    logic around the model can throw an exception and not use the model’s output.
    For example, a self-driving car can alert the driver that it should take over
    control or it can slow down to avoid an accident.
  prefs: []
  type: TYPE_NORMAL
- en: However, common *neural networks do not know when they do not know*; they typically
    do not assign a low confidence score to out-of-distribution data.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this is given in a Google paper titled *Can You Trust Your Model’s*
    *Uncertainty? Evaluating predictive Uncertainty Under Dataset Shift*. The paper
    shows that if you apply perturbations to a test dataset such as blur or noise,
    such that the images become more and more out-of-distribution, the accuracy of
    the model goes down. However, the confidence calibration of the model also decreases.
    This means that the scores of the model are not trustworthy anymore on out-of-distribution
    data: they do not accurately indicate the model’s confidence in its predictions.
    We will explore this behaviour ourselves in the *Revealing dataset shift with
    Bayesian methods* case study in [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying*
    *Bayesian Deep Learning*](CH8.xhtml#x1-1320008).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to determine how a model handles out-of-distribution data is by
    feeding it data that is not just perturbed, but completely different from the
    dataset it was trained on. The procedure to measure the model’s out-of-distribution
    detection performance is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a model on an in-distribution dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the confidence scores of the model on the in-distribution test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed a completely different, out-of-distribution dataset to the model and save
    the corresponding confidence scores of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, treat the scores from both datasets as scores from a binary problem: in-distribution
    or out-of-distribution. Compute binary metrics, such as the **area under the Receiver**
    **Operating Characteristic** (**AUROC**) curve or the area under the precision-recall
    curve.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This strategy tells you how well your model can separate in-distribution from
    out-of-distribution data. The assumption is that in-distribution data should always
    receive a higher confidence score than out-of-distribution data; in the ideal
    scenario, there is no overlap between the two distributions. In practice, however,
    this is not the case. Models do often give high confidence scores to out-of-distribution
    data. We will explore an example of this in the next section and a few solutions
    to this problem in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Example of confident, out-of-distribution predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see how a vanilla neural net can produce confident predictions on out-of-distribution
    data. In this example, we will first train a model and then feed it out-of-distribution
    data. To keep things simple, we will use a dataset with different types of dogs
    and cats and build a binary classifier that predicts whether the image contains
    a dog or a cat.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first download our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load our data into a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use scikit-learn’s `train_test_val()` function to create a training
    and validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create our train and validation data. Our `divprocess()` function loads
    our download image into memory and formats our label such that our model can process
    it. We use a batch size of 256 and an image size of 160x160 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our model. To speed up learning, we can use transfer learning
    and start with a model that was pre-trained on ImageNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can train the model, we first need to compile it. Compiling simply
    means that we specify a loss function and an optimizer for the model and, optionally,
    add some metrics for monitoring during training. In the following code, we specify
    that the model should be trained using the binary cross-entropy loss and the Adam
    optimizer and that, during training, we want to monitor the model’s accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Because of transfer learning, just fitting our model for three epochs leads
    to a validation accuracy of about 99 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also test our model on the test set of this dataset. We first prepare
    our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then feed the dataset to our trained model. We obtain a test set accuracy
    of about 98.3 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a model that is pretty good at classifying cats and dogs. But what
    will happen if we give this model an image that is neither a cat or a dog? Ideally,
    the model should recognize that this image is not part of the data distribution
    and should output close to a uniform distribution. Let’s see if this actually
    happens in practice. We feed our model some images from the ImageNet dataset –
    the actual dataset that was used to pre-train our model. The ImageNet dataset
    is large. That is why we download a subset of the dataset: a dataset called Imagenette.
    This dataset contains just 10 out of the 1,000 classes of the original ImageNet
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then take an image from the `parachute` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The image clearly does not contain a dog or a cat; it is obviously out-of-distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: An image of a parachute from the ImageNet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We run the image through our model and print the score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the model classifies the image of a parachute as a dog with
    more than 99% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: We can test the model’s performance on the ImageNet `parachute` class more systematically
    as well. Let’s run all the parachute images from the train split through the model
    and plot a histogram of the scores of the `dog` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a small function to create special dataset with all the parachute
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then feed the dataset to our model and create a list of all the softmax
    scores related to the `dog` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plot a histogram with these scores – this shows us the distribution
    of softmax scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Ideally, we want these scores to be distributed close to 0.5 as the images
    in this dataset are neither dogs nor cats; the model should be very uncertain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Distribution of softmax scores on the parachute dataset'
  prefs: []
  type: TYPE_NORMAL
- en: However, we see something very different. More than 800 images are classified
    as a dog with at least 90% confidence. Our model clearly does not know how to
    handle out-of-distribution data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Susceptibility to adversarial manipulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One other vulnerability of most neural networks is that they are susceptible
    to adversarial attacks. Adversarial attacks, simply put, are ways to fool a deep
    learning system, most often in ways that would not fool humans. These attacks
    can be harmless or harmful. Here are some examples of adversarial attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: A classifier can detect different types of animals. It classifies an image of
    a panda as a panda with 57.7% confidence. By slightly perturbing the image in
    a way that is invisible to humans, the image is now classified as a gibbon with
    93.3% confidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model can detect whether a movie recommendation is positive or negative. It
    classifies a given movie as negative. By changing a word that does not change
    the overall tone of the review, for example, from ”surprising” to ”astonishing,”
    the prediction of the model can change from a negative to a positive recommendation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A stop sign detector can detect stop signs. However, by putting a relatively
    small sticker on top of the stop sign, the model no longer recognises the stop
    sign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These examples show that there are different kinds of adversarial attacks.
    A useful way to categorize adversarial attacks, is by trying to determine how
    much information about the model is available to the human (or machine) attacking
    the model. An attacker can always feed an input to the model, but what the model
    returns or how the attacker can inspect the model varies. With this lens, we can
    see the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hard-label black box*: The attacker only has access to the labels resulting
    from feeding the model an input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Soft-label black box*: The attacker has access to the scores and the labels
    of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*White box* setting: The attacker has full access to the model. They can access
    the weights and can see the scores, the structure of the model, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine that these different settings make it more or less difficult
    to attack a model. If people who want to fool a model can only see the label resulting
    from an input, they cannot be sure that a small change of the input will lead
    to a difference in the model’s behavior as long as the label stays the same. This
    becomes easier when they have access to the model’s label and scores. They can
    then see if a change in the input increases or decreases the confidence of the
    model. As a result, they can more systematically try to change our input, in ways
    that decrease the model’s confidence in the label. This might make it possible
    to find a vulnerability of the model, given that there is enough time to change
    the input in an iterative fashion. Now, when someone has full access to the model
    (white box setting), finding a vulnerability might become even easier. They can
    now use more information to guide the change of the image, such as the gradient
    of the loss with respect to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of information available to an attacker is not the only way to distinguish
    between different kinds of adversarial attacks; there are many types of attacks.
    For example, in the context of attacks on vision models, some attacks are based
    on single-patch adjustments of an image (or even single pixels!) while other attacks
    will change an entire image. Some attacks are specific to a single model, some
    attacks can be applied to multiple models. We can also distinguish between attacks
    that digitally manipulate an image and attacks that can be applied in the real
    world to fool a model, or attacks that are visible by the human eye and attacks
    that are not. As a result of the wide variety of attacks, the research literature
    about this topic is still very active – there are always new ways to attack a
    model, and subsequently a need to find defenses for these attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the section about out-of-distribution data, we trained a model to determine
    whether a given image is a cat or a dog. We saw that the classifier worked well:
    it achieved a test accuracy of about 98.3 percent. Is this model robust to adversarial
    attacks? Let’s create an attack to find out. We will use the **fast-gradient sign
    method** (**FGSM**) to slightly perturb an image of a dog and make the model think
    it is actually an image of a cat. The fast-gradient sign method was introduced
    in 2014 by Ian Goodfellow et al. and remains one of the most famous examples of
    an adversarial attack. This is probably because of its simplicity; we will see
    that we will only need a few lines of code to create such an attack. Moreover,
    the results of this attack are astounding – Goodfellow himself mentioned that
    he could not really believe the results when he first tested this attack and he
    had to verify that the perturbed, adversarial image he fed to the model was actually
    different from the original input image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an effective adversarial image, we have to make sure that the pixels
    in the image change – but only by so much that the change does not become apparent
    to the human eye. If we perturb the pixels in our image of a dog in such a way
    that the image now actually looks like a cat, then the model is not mistaken if
    it classifies that image as a cat. We make sure that we do not perturb the image
    too much by constraining the perturbation by the max norm – this essentially tells
    us that no pixel in the image can change by more than some amount *𝜖*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![∥˜x − x∥∞ ≤ 𝜖 ](img/file75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *x* is our perturbed image and *x* our original input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to create our adversarial example in the fast-gradient sign method, we
    use the gradient of the loss with respect to our input image to create a new image.
    Instead of minimizing the loss as we want to do in gradient descent, we now want
    to maximize the loss. Given our network weights *𝜃*, input *x*, label *y*, and
    *J* as a function to compute our loss, we can create an adversarial image by perturbing
    the image in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![η = 𝜖sgn(∇xJ (𝜃,x,y)) ](img/file76.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, we compute the sign of the gradient of the loss with respect
    to the input, i.e. determine whether the gradient is positive (1), negative (-1)
    or 0\. The sign enforces the max norm constraint, and by multiplying it by epsilon,
    we make sure that our perturbations are small – computing the sign just tells
    us that if we want to add or subtract epsilon in order to perturb the image in
    a way that hurts the model’s performance on the image. *η* is now the perturbation
    that we want to add to our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![˜x = x+ η ](img/file77.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see what this looks like in Python. Given our trained network that classifies
    images as either dogs or cats, we can create a function that creates a perturbation
    that, when multiplied by epsilon and added to our image, creates an adversarial
    attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a small function that runs an input image through our model
    and returns the confidence of the model that the image contains a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We download an image of a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we pre-process it so we can feed it to our model. We set the label to
    0, which corresponds to the `cat` label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can perturb our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now get the confidence of the model that the original image is a cat,
    and the confidence of the model that the perturbed image is a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, we can create the following plot, showing the original
    image, the perturbation applied to the image, and the perturbed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 3.14* shows both the original image and the perturbed image along with
    the model prediction for each image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Example of an adversarial attack'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure* [*3.14*](#x1-46175r14), we can see that our model initially classifies
    the image as a cat, with a confidence of 100 percent. After we applied the perturbation
    (shown in the middle) to our initial cat image (shown on the left), our image
    (on the right) is now classified as a dog with 98.73 percent confidence, although
    the image visually looks the same as the original input image. We successfully
    created an adversarial attack that fools our model!
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we have seen different types of common neural networks. First,
    we discussed the key building blocks of neural networks with a special focus on
    the multi-layer perceptron. Then we reviewed common neural network architectures:
    convolutional neural networks, recurrent neural networks, and the attention mechanism.
    All these components allow us to build very powerful deep learning models that
    can sometimes achieve super-human performance. However, in the second part of
    the chapter, we reviewed a few problems of neural networks. We discussed how they
    can be overconfident, and do not handle out-of-distribution data very well. We
    also saw how small, imperceptible changes to a neural network’s input can cause
    the model to make an incorrect prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will combine the concepts learned in this chapter and
    in [*Chapter 3*](#x1-350003), [*Fundamentals of Deep Learning*](#x1-350003), and
    discuss Bayesian deep learning, which has the potential to overcome some of the
    challenges of standard neural networks we have seen in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a lot of great resources to learn more about the essential building
    blocks of deep learning. Here are just a few popular resources that are a great
    start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nielsen, M.A., 2015\. *Neural networks* *and deep learning* (Vol. 25). San
    Francisco, CA, USA: Determination press., [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet, F., 2021\. *Deep learning with Python*. Simon and Schuster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raschka, S., 2015\. *Python Machine Learning*. Packt Publishing Ltd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ng, Andrew, 2022, *Deep Learning Specialization*. Coursera.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson, Justin, 2019\. EECS 498-007 / 598-005, *Deep Learning for* *Computer
    Vision*. University of Michigan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about the problems of deep learning models, you can read some
    of the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overconfidence and calibration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo, C., Pleiss, G., Sun, Y. and Weinberger, K.Q., 2017, July. *On calibration
    of modern neural networks*. In International conference on machine learning (pp.
    1321-1330). PMLR.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
    J., Lakshminarayanan, B. and Snoek, J., 2019\. *Can you* *trust your model’s uncertainty?
    evaluating predictive uncertainty* *under dataset shift.* Advances in neural information
    processing systems, 32.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Out-of-distribution detection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks, D. and Gimpel, K., 2016\. *A baseline for detecting* *misclassified
    and out-of-distribution examples in neural networks.* arXiv preprint arXiv:1610.02136.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang, S., Li, Y. and Srikant, R., 2017\. *Enhancing the reliability* *of out-of-distribution
    image detection in neural networks.* arXiv preprint arXiv:1706.02690.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee, K., Lee, K., Lee, H. and Shin, J., 2018\. *A simple* *unified framework
    for detecting out-of-distribution samples and* *adversarial attacks.* Advances
    in neural information processing systems, 31.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fort, S., Ren, J. and Lakshminarayanan, B., 2021\. *Exploring* *the limits of
    out-of-distribution detection.* Advances in Neural Information Processing Systems,
    34, pp.7068-7081.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversarial attacks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.
    and Fergus, R., 2013\. *Intriguing properties of* *neural networks*. arXiv preprint
    arXiv:1312.6199.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014\. *Explaining and* *harnessing
    adversarial examples*. arXiv preprint arXiv:1412.6572.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nicholas Carlini, 2019\. *Adversarial Machine Learning Reading* *List* [https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can have a look at the following resources to dive deeper into the topics
    and experiments covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jasper Snoek, MIT 6.S191: *Uncertainty in Deep Learning*, January 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Core Tutorial, *Adversarial example using FGSM*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014\. *Explaining and* *harnessing
    adversarial examples*. arXiv preprint arXiv:1412.6572.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of
    modern neural networks. In *International Conference on* *Machine Learning*, pages
    1321–1330\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanford University School of Engineering, CS231N, *Lecture 16 —* *Adversarial
    Examples and Adversarial Training*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Danilenka, Anastasiya, Maria Ganzha, Marcin Paprzycki, and Jacek Mańdziuk, 2022\.
    *Using adversarial images to improve outcomes of* *federated learning for non-IID
    data.* arXiv preprint arXiv:2206.08124.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.
    and Fergus, R., 2013\. *Intriguing properties of neural* *networks*. arXiv preprint
    arXiv:1312.6199.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma, A., Bian, Y., Munz, P. and Narayan, A., 2022\. *Adversarial* *Patch
    Attacks and Defences in Vision-Based Tasks: A Survey*. arXiv preprint arXiv:2206.08304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nicholas Carlini, 2019\. *Adversarial Machine Learning Reading List* [https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parkhi, O.M., Vedaldi, A., Zisserman, A. and Jawahar, C.V., 2012, June. *Cats
    and dogs*. In 2012 IEEE conference on computer vision and pattern recognition
    (pp. 3498-3505). IEEE. (Dataset cat vs dog).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June.
    *Imagenet: A large-scale hierarchical image database*. In 2009 IEEE conference
    on computer vision and pattern recognition (pp. 248-255). Ieee. (ImageNet dataset).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthew D Zeiler and Rob Fergus. *Visualizing and understanding* *convolutional
    networks*. In European conference on computer vision, pages 818–833\. Springer,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
