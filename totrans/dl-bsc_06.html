<html><head></head><body>
		<div>
			<div id="_idContainer156" class="Content">
			</div>
		</div>
		<div id="_idContainer157" class="Content">
			<h1 id="_idParaDest-114"><a id="_idTextAnchor120"/>5. Backpropagation</h1>
		</div>
		<div id="_idContainer246" class="Content">
			<p>The previous chapter described neural network training. There, the gradient of a weight parameter in the neural network (i.e., the gradient of the loss function for a weight parameter) was obtained by using numerical differentiation. Numerical differentiation is simple, and its implementation is easy, but it has the disadvantage that calculation takes time. This chapter covers backpropagation, which is a more efficient way to calculate the gradients of weight parameters.</p>
			<p>There are two ways to understand backpropagation correctly. One of them uses "equations," while the other uses <strong class="bold">computational graphs</strong>. The former is a common way, and many books about machine learning expand on this by focusing on formulas. This is good because it is strict and simple, but it may hide essential details or end in a meaningless list of equations. </p>
			<p>Therefore, this chapter will use computational graphs so that you can understand backpropagation "visually." Writing code will deepen your understanding further and convince you of this. The idea of using computational graphs to explain backpropagation is based on Andrej Karpathy's blog (<em class="italics">Hacker's guide to Neural Networks</em>, (<a href="http://karpathy.github.io/neuralnets/">http://karpathy.github.io/neuralnets/</a>)) and the deep learning course (<em class="italics">CS231n:</em> <em class="italics">Convolutional Neural Networks for Visual Recognition</em> (<a href="http://cs231n.github.io/">http://cs231n.github.io/</a>)) provided by him and Professor Fei-Fei Li at Stanford University.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor121"/>Computational Graphs</h2>
			<p>A computational graph shows the process of calculation. This graph is used as a graph of data structure and is represented by multiple nodes and edges (meaning, straight lines that connect nodes). In this section, we will solve easy problems to familiarize ourselves with computational graphs before advancing step by step into more complex backpropagation.</p>
			<h3 id="_idParaDest-116"><a id="_idTextAnchor122"/>Using Computational Graphs to Solve Problems</h3>
			<p>The problems in this section are simple enough that you can solve them with mental arithmetic, but the purpose here is to get familiar with computational graphs. Learning to use computational graphs will be helpful for the complicated calculations we will cover later, so it's important to first master how to use them here.</p>
			<p><strong class="bold">Question 1</strong>: Taro bought 2 apples that were 100 yen apiece. Calculate the amount of money he paid if a 10% consumption tax was applied.</p>
			<p>A computational graph shows the process of calculation with nodes and arrows. A node is represented by a circle, and an operation is described in it. The intermediate result of the calculation above an arrow shows the result of each node that flows from left to right. The following diagram shows the computational graph that solves Question 1:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/fig05_1.jpg" alt="Figure 5.1: Answer to Question 1 using a computational graph&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.1: Answer to Question 1 using a computational graph</h6>
			<p>As shown in the preceding diagram, 100 yen for an apple that flows to the "x2" node becomes 200 yen, which is passed to the next node. Then, 200 yen is passed to the "× 1.1" node and becomes 220 yen. The result of this computational graph shows that the answer is 220 yen.</p>
			<p>In the preceding diagram, each circle contains "× 2" or "× 1.1" as one operation. You can also place only "x" in a circle to show the operation. In that case, as shown in the following diagram, you can place "<em class="italics">2</em>" and "<em class="italics">1.1</em>" outside the circles as the "Number of apples" and "Consumption tax" variables.</p>
			<p>The solution to this problem can be observed in the figure below:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/Figure_5.2.jpg" alt="Figure 5.2: Answer to Question 1 using a computational graph: the &quot;Number of apples&quot; and &quot;Consumption tax&quot; variables are placed outside circles&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.2: Answer to Question 1 using a computational graph: the "Number of apples" and "Consumption tax" variables are placed outside circles</h6>
			<p><strong class="bold">Question 2</strong>: Taro bought 2 apples and 3 oranges. An apple was 100 yen, and the orange was 150. A 10% consumption tax was applied. Calculate the amount of money he paid.</p>
			<p>As in Question 1, we will use a computational graph to solve Question 2. The following diagram shows the computational graph for this:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/Figure_5.3.jpg" alt="Figure 5.3: Answer to Question 2 using a computational graph&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.3: Answer to Question 2 using a computational graph</h6>
			<p>In this question, an addition node, "+", was added to sum the amounts of apples and oranges. After creating a computational graph, we advance the calculation from left to right. The calculation result moves from left to right, just like an electric current flows in a circuit, and the calculation ends when the result reaches the rightmost side. The preceding diagram shows that the answer is 715 yen.</p>
			<p>To solve a problem using a computational graph, then, you must perform the following:</p>
			<ol>
				<li>Create a computational graph.</li>
				<li>Advance the calculation from left to right on the computational graph.</li>
			</ol>
			<p>Step 2 is known as propagating in the forward direction or <strong class="bold">forward propagation</strong>. In forward propagation, calculation propagates from start to finish in a computational graph. If forward propagation exists, we can also consider propagation in the backward direction—from right to left. This is called <strong class="bold">backward</strong> <strong class="bold">propagation</strong> and is known as backpropagation. It will play an important role when we calculate derivatives later.</p>
			<h3 id="_idParaDest-117"><a id="_idTextAnchor123"/>Local Calculation</h3>
			<p>The main characteristic of a computational graph is that it can obtain the final result by propagating "local calculation." The word "local" means "a small range related to the node." A local calculation can return the next result (subsequent result) from information related to the node, no matter what is happening on the whole.</p>
			<p>We can break down local calculations using a specific example. For example, let's assume that we bought two apples and many other things at a supermarket. To visualize this, you can create a computational graph like so:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/Figure_5.4.jpg" alt="Figure 5.4: Example of buying two apples and many other things&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.4: Example of buying two apples and many other things</h6>
			<p>Let's assume that we bought many things and that the total amount was 4,000 yen (after a complicated calculation), as shown in the preceding computational graph. What is important here is that the calculation in each node is a local calculation. To sum the amounts of the apples and the other purchases (4,000 + 200 -&gt; 4,200), you can add the two figures without thinking about how 4,000 was obtained. In other words, what to calculate in each node is only the calculation related to the node—in this example, the addition of the two numbers provided. We do not need to think about the whole graph.</p>
			<p>Thus, you can focus on local calculation in a computational graph. However complicated the whole calculation is, what is done at each step is "local calculation" for the target node. By passing the results of simple local calculations, you can obtain the result of the complicated calculations that constitute the whole graph.</p>
			<h4>Note</h4>
			<p class="callout">For example, the assembly of a car is complicated, but it is usually conducted based on the division of labor on an "assembly line." Each worker (machine) conducts simple work. The outcome of the workflows to the next worker, and finally, a car is built. A computational graph also divides complicated calculations into "simple and local calculations" and passes the calculation result to the next node, just like a car is passed down an assembly line. Like the assembly of a car, complicated calculations can be divided into simple calculations.</p>
			<h3 id="_idParaDest-118"><a id="_idTextAnchor124"/>Why Do We Use Computational Graphs?</h3>
			<p>We have solved two problems by using computational graphs and may now consider the advantages.  One of them is "local calculation," as described earlier. However complicated the whole calculation is, local calculation enables you to focus on the simple calculations in each node in order to simplify the problem as a whole. </p>
			<p>Another advantage is that you can keep all the results of intermediate calculations in a computational graph (for example, 200 yen after 2 apples are calculated and 650 yen before consumption tax is added). However, the largest reason for using computational graphs is that you can calculate "derivatives" efficiently by propagating in the backward direction.</p>
			<p>To describe backward propagation in a computational graph, consider Question 1 again. In this problem, you calculated the final amount paid regarding two apples and the consumption tax. Now say that you need to know how the final amount paid will be affected when the price of an apple goes up. This corresponds to obtaining the "derivative of the amount paid with respect to the price of an apple." It corresponds to obtaining <img src="image/Figure_5.4_a.png" alt="61"/> when the price of an apple is x and the amount paid is L. The value of this derivative indicates how much the amount paid increases when the price of an apple goes up "slightly."</p>
			<p>As we mentioned earlier, you can use backward propagation in a computational graph to obtain a value, such as the "derivative of the amount paid with respect to the price of an apple." First, we will only look at the result. As shown in the following diagram, you can obtain derivatives by using backpropagation in a computational graph:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/Figure_5.5.jpg" alt="Figure 5.5: Propagating differential values using backward propagation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.5: Propagating differential values using backward propagation</h6>
			<p>As shown in the preceding diagram, backward propagation is shown graphically with arrows (thick lines) in the opposite direction to the forward direction. Backward propagation passes "local differentials," and the values are placed below the arrows. In this example, derivative values are passed from right to left, as in 1 -&gt; 1.1 -&gt; 2.2. The result shows that the value of the "derivative of the amount paid with respect to the price of an apple" is 2.2. </p>
			<p>This indicates that the final amount paid increases by 2.2 yen when the price of an apple goes up 1 yen. This means that when the price of an apple goes up by a small amount, the final amount increases by 2.2 times that of the small value. Here, only the derivative with respect to the price of an apple was obtained, but you can also obtain the "derivative of the amount paid with respect to consumption tax" and the "derivative of the amount paid with respect to the number of apples" by using similar steps. </p>
			<p>During these steps, you can share the intermediate results of the derivatives (the derivatives that are passed halfway) so that you can calculate multiple derivatives efficiently. Thus, the advantage of a computational graph is that forward, and backward propagations enable you to obtain the derivative value of each variable efficiently. </p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor125"/>Chain Rule</h2>
			<p>Forward propagation in a computational graph propagates the calculation result in the forward direction from left to right. These calculations seem natural because they are usually conducted. On the other hand, in backward propagation, a "local derivative" is propagated in the backward direction from right to left. The principle that propagates the "local derivative" is based on the <strong class="bold">chain rule</strong>. Let's look at the chain rule and clarify how it corresponds to backward propagation in a computational graph. </p>
			<h3 id="_idParaDest-120"><a id="_idTextAnchor126"/>Backward Propagation in a Computational Graph</h3>
			<p>We will now look at an example of backward propagation using a computational graph. Let's assume that a calculation, <em class="italics">y = f (x)</em>, exists. The following diagram  shows the backward propagation of this calculation: </p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/Figure5.6.jpg" alt="Figure 5.6: Backward propagation in a computational graph – the local derivative &#13;&#10;is multiplied in the backward direction&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.6: Backward propagation in a computational graph – the local derivative is multiplied in the backward direction</h6>
			<p>As shown in the preceding diagram, backward propagation multiplies the signal E by the local derivative of the node, <img src="image/Figure_5.6_a.png" alt="60"/>, and propagates it to the next node. The local derivative here means obtaining the derivative of the calculation, y = f (x), in forward propagation and indicates obtaining the derivative, <strong class="inline">y</strong>, with respect to x <img src="image/Figure_5.6_b.png" alt="59"/>; for example, y = f (x) = x<span class="superscript">2</span>, <img src="image/Figure_5.6_c.png" alt="58"/>. The local derivative is multiplied by the value propagated from the upper stream (E, in this example) and passed to the previous node.</p>
			<p>This is the procedure of backward propagation. It can obtain the target derivative values efficiently. The reason why this is possible can be explained by the principle of the chain rule, defined in the next section. </p>
			<h3 id="_idParaDest-121"><a id="_idTextAnchor127"/>What Is the Chain Rule?</h3>
			<p>Before explaining the chain rule, we need to talk about <strong class="bold">composite functions</strong>. A composite function is a function that consists of multiple functions. For example, the equation <strong class="inline">z = (x + y)</strong><span class="superscript">2</span> consists of two equations, as shown in equation (5.1):</p>
			<table id="table001-3" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.6_d.png" alt="57"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>The chain rule is the characteristic related to the derivative of a composite function and is defined as follows. </p>
			<p>When a function is expressed by a composite function, the derivative of the composite function can be expressed by the product of the derivative of each function that constitutes the composite function. </p>
			<p>This is called the principle of the chain rule. Although it may seem difficult, it is actually quite simple. In the example given in equation (5.1), <img src="image/Figure_5.6_g.png" alt="2"/> (a derivative of z with respect to x) is the product of <img src="image/Figure_5.6_f.png" alt="1"/> (a derivative of <em class="italics">z</em> with respect to <em class="italics">t</em>) and <img src="image/Figure_5.6_h.png" alt="3"/> (a derivative of <em class="italics">t</em> with respect to x). You can express this with the following equation (5.2): </p>
			<table id="table002-3" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.6_j.png" alt="4"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.2)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>You can remember equation (5.2) easily because ∂t's cancel each other out, as shown here: <img src="image/Figure_5.6_k.png" alt="5"/></p>
			<p>Now, let's use the chain rule to obtain the derivative of equation (5.2), <img src="image/Figure_5.6_L.png" alt="6"/>. First, obtain the local differential (partial differential) of equation (5.1):</p>
			<table id="table003-3" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.6_m.png" alt="7"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.3)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>As shown in equation (5.3), <img src="image/Figure_5.6_o.png" alt="9"/> is 2t and <img src="image/Figure_5.6_p.png" alt="10"/> is 1. This result is analytically obtained from the differentiation formula. The final result, <img src="image/Figure_5.6_q.png" alt="11"/>, can be calculated by the product of the derivatives obtained in equation (5.3):</p>
			<table id="table004-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.6_r.png" alt="12"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.4)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<h3 id="_idParaDest-122"><a id="_idTextAnchor128"/>The Chain Rule and Computational Graphs</h3>
			<p>Now, let's use a computational graph to express the calculation of the chain rule in equation (5.4). When we represent a square with a node "**2", we can write a graph for it, as follows:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/Figure_5.7.jpg" alt="Figure 5.7: Computational graph of equation (5.4) – local derivatives are multiplied &#13;&#10;and passed in the backward direction&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.7: Computational graph of equation (5.4) – local derivatives are multiplied and passed in the backward direction</h6>
			<p>As shown in the preceding diagram, backward propagation in a computational graph propagates signals from right to left. Backward propagation multiplies the signal provided to a node by the local derivative (partial derivative) of the node and passes it to the next node. For example, the input to "**2" in backward propagation is <img src="image/Figure_5.7_a.png" alt="55"/>. It is multiplied by the local derivative, <img src="image/Figure_5.7_b.png" alt="54"/> (in forward propagation, the input is t and the output is z, so the (local) derivative at this node is <img src="image/Figure_5.7_c.png" alt="53"/>) and then multiplied and passed to the next node. In the preceding diagram, the first signal in backward propagation <img src="image/Figure_5.7_g.png" alt="51"/> did not appear in the previous equation. It was omitted there because <img src="image/Figure_5.7_g1.png" alt="51"/> = 1.</p>
			<p>What we should note from the preceding diagram is the result of backward propagation at the leftmost position. It corresponds to the "derivative of z with respect to x" because <img src="image/Figure_5.7_e.png" alt="5k"/> due to the chain rule. What backward propagation performs is based on the principle of the chain rule.</p>
			<p>When you assign the result of equation (5.3), as shown in the preceding diagram, the result is as follows. Thus, <img src="image/Figure_5.7_h.png" alt="50"/> is 2(x + y):</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/Figure_5.8.jpg" alt="Figure 5.8: Based on the result of backward propagation in the computational graph,  is 2(x + y) &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.8: Based on the result of backward propagation in the computational graph,  is 2(x + y) </h6>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor129"/>Backward Propagation</h2>
			<p>The previous section described how backward propagation in a computational graph is based on the chain rule. We will now cover how backward propagation works by taking operations, such as "+" and "x", as examples. </p>
			<h3 id="_idParaDest-124"><a id="_idTextAnchor130"/>Backward Propagation in an Addition Node</h3>
			<p>First, let's consider backward propagation in an additional node. Here, we will look at backward propagation for the equation <em class="italics">z = x + y</em>. We can obtain the derivatives of <em class="italics">z = x + y</em> (analytically) as follows:</p>
			<table id="table005-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.8_b.png" alt="13"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.5)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>As equation (5.5) shows, both <img src="image/Figure_5.8_d.png" alt="15"/> and <img src="image/Figure_5.8_f.png" alt="16"/> are 1. Therefore, we can represent them in a computational graph, as shown in the following diagram. In backward propagation, the derivative from the upper stream—<img src="image/Figure_5.8_g.png" alt="17"/>, in this example—is multiplied by 1 and passed downstream. In short, backward propagation in an addition node multiplies 1, so it only passes the input value to the next node.</p>
			<p>In this example, the differential value from the upper stream is expressed as <img src="image/Figure_5.8_g.png" alt="18"/>. This is because we assume a large computational graph that finally outputs L, as shown in <em class="italics">Figure 5.10</em>. The calculation, <em class="italics">z = x + y</em>, exists somewhere in the large computational graph, and the value of <img src="image/Figure_5.8_g.png" alt="19"/> is passed from the upper stream. The values of <img src="image/Figure_5.8_i.png" alt="20"/> and <img src="image/Figure_5.8_j.png" alt="21"/> are propagated downstream:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/Figure_5.9.jpg" alt="Figure 5.9: Backward propagation in an addition node – forward propagation on the left and backward propagation on the right. &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.9: Backward propagation in an addition node – forward propagation on the left and backward propagation on the right. </h6>
			<p>As shown on the right, backward propagation in an addition node passes a value from the upper stream to the lower stream without changing it.</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/Figure_5.10.jpg" alt="Figure 5.10: This addition node exists somewhere in the final output calculation. &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.10: This addition node exists somewhere in the final output calculation. </h6>
			<p>In backward propagation, starting from the rightmost output, local derivatives are propagated from node to node in the backward direction</p>
			<p>Now, let's look at an example of backward propagation. For example, say that a calculation, "10 + 5 = 15", exists and that a value of 1.3 flows from the upper stream in backward propagation. The following diagram shows this in terms of a computational graph: </p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/fig05_11.jpg" alt="Figure 5.11: Example of backward propagation in an addition node&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.11: Example of backward propagation in an addition node</h6>
			<p>Because backward propagation in an addition node only outputs the input signal to the next node, it passes 1.3 to the next node.</p>
			<h3 id="_idParaDest-125"><a id="_idTextAnchor131"/>Backward Propagation in a Multiplication Node</h3>
			<p>Let's take a look at backward propagation in a multiplication node by taking an equation, <em class="italics">z = xy</em>, as an example. The differential of this equation is expressed by the following equation (5.6):</p>
			<table id="table006-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.11_a.png" alt="22"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.6)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Based on the preceding equation (5.6), you can write a computational graph as follows.</p>
			<p>For the backward propagation of multiplication, the upstream value multiplied by the "reversed value" of the input signal for forward propagation is passed downstream. A reversed value means that if the signal is x in forward propagation, the value to multiply is y in backward propagation; and that if the signal is y in forward propagation, the value to multiply is x in backward propagation, as shown in the following diagram.</p>
			<p>Let's look at an example. Assume that a calculation 10 x 5 = 50 exists and that the value of 1.3 flows from the upper stream in backward propagation. <em class="italics">Figure 5.13</em> shows this in the form of a computational graph.</p>
			<p>In the backward propagation of multiplication, the reversed input signals are multiplied, so 1.3 x 5 = 6.5 and 1.3 x 10 = 13 are obtained. In the backward propagation of addition, the upstream value was only passed downstream. Therefore, the value of the input signal in forward propagation is not required. On the other hand, for the backward propagation of multiplication, the value of the input signal in forward propagation is required. Therefore, to implement a multiplication node, the input signal of forward propagation is retained:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/fig05_12.jpg" alt="Figure 5.12: Backward propagation in a multiplication node – forward propagation on the left and backward propagation on the right&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.12: Backward propagation in a multiplication node – forward propagation on the left and backward propagation on the right</h6>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/fig05_13.jpg" alt="Figure 5.13: Example of backward propagation in a multiplication node&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.13: Example of backward propagation in a multiplication node</h6>
			<h3 id="_idParaDest-126"><a id="_idTextAnchor132"/>Apples Example</h3>
			<p>Let's think about the example of buying apples—two apples and consumption tax—from the beginning of this chapter again. The problem to solve here is how each of the three variables (the price of an apple, the number of apples, and consumption tax) affect the final amount paid. This corresponds to obtaining the "derivative of the amount paid with respect to the price of an apple," the "derivative of the amount paid with respect to the number of apples," and the "derivative of the amount paid with respect to consumption tax." We can solve this by using backward propagation in a computational graph, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/Figure_5.14.jpg" alt="Figure 5.14: Example of backward propagation for purchasing apples&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.14: Example of backward propagation for purchasing apples</h6>
			<p>As mentioned previously, input signals are reversed and passed downstream in the backward propagation of a multiplication node. According to the result shown in the preceding diagram, the differential of the price of an apple is 2.2, there are 110 apples, and the consumption tax is 200. They indicate that when consumption tax and the price of an apple increase by the same quantity, the consumption tax affects the final amount paid in the size of 200 and that the price of an apple affects it in the size of 2.2. However, this result is brought about because the consumption tax and the price of an apple in this example are different in terms of units (1 for the consumption tax is 100%, while 1 for the price of an apple is 1 yen). </p>
			<p>Lastly, let's solve the backward propagation of "buying apples and oranges" as an exercise. Please obtain the derivatives of individual variables and put the numbers in the squares provided in the following diagram (you can find the answer in the next section):</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/Figure_5.15.jpg" alt="Figure 5.15: Example of backward propagation for purchasing apples and oranges – complete this calculation by putting figures in the squares&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.15: Example of backward propagation for purchasing apples and oranges – complete this calculation by putting figures in the squares</h6>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor133"/>Implementing a Simple Layer</h2>
			<p>In this section, we will implement the apple example we've described in Python using the multiplication node in a computational graph as the <strong class="bold">multiplication layer </strong>(<strong class="bold">MulLayer</strong>) and the addition node as the <strong class="bold">addition layer</strong> (<strong class="bold">AddLayer</strong>).</p>
			<h4>Note</h4>
			<p class="callout">In the next section, we will implement the "layers" that constitute a neural network in one class. The "layer" here is a functional unit in a neural network—the Sigmoid layer for a sigmoid function, and the Affine layer for matrix multiplication. Therefore, we will also implement multiplication and addition nodes here on a "layer" basis. </p>
			<h3 id="_idParaDest-128"><a id="_idTextAnchor134"/>Implementing a Multiplication Layer</h3>
			<p>We will implement a layer so that it has two common methods (interfaces): <strong class="inline">forward()</strong> and <strong class="inline">backward()</strong>, which correspond to forward propagation and backward propagation, respectively. Now, you can implement a multiplication layer as a class called <strong class="inline">MulLayer</strong>, as follows (the source code is located at <strong class="inline">ch05/layer_naive.py</strong>):</p>
			<p class="source-code">class MulLayer:</p>
			<p class="source-code">   def __init__ (self): </p>
			<p class="source-code">      self.x = None</p>
			<p class="source-code">      self.y = None</p>
			<p class="source-code">    def forward(self, x, y):</p>
			<p class="source-code">       self.x = x</p>
			<p class="source-code">       self.y =y</p>
			<p class="source-code">       out = x * y</p>
			<p class="source-code">       return out</p>
			<p class="source-code">    def backward(self, dout):</p>
			<p class="source-code">       dx = dout * self.y # Reverse x and y</p>
			<p class="source-code">       dy = dout * self.x</p>
			<p class="source-code">       return dx, dy</p>
			<p><strong class="inline">__init__()</strong> initializes the instance variables, <strong class="inline">x,</strong> and <strong class="inline">y</strong>, which are used to retain the input values in forward propagation. <strong class="inline">forward()</strong> takes two variables, <strong class="inline">x</strong> and <strong class="inline">y</strong>, and multiplies and outputs their product. On the other hand, <strong class="inline">backward()</strong> multiplies the derivative from the upper stream (<strong class="inline">dout</strong>) by the "reversed value" of forward propagation and passes the result downstream.</p>
			<p>Now, use <strong class="inline">MulLayer</strong> to implement the "purchase of apples"—two apples and consumption tax. In the previous section, we used forward and backward propagations in a computational graph for this calculation, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/Figure_5.16.jpg" alt="Figure 5.16: Purchasing two apples&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.16: Purchasing two apples</h6>
			<p>By using the multiplication layer, we can implement forward propagation for this as follows (the source code is located at <strong class="inline">ch05/buy_apple.py</strong>):</p>
			<p class="source-code">apple = 100</p>
			<p class="source-code">apple_num = 2</p>
			<p class="source-code">tax = 1.1</p>
			<p class="source-code"># layer</p>
			<p class="source-code">mul_apple_layer = MulLayer()</p>
			<p class="source-code">mul_tax_layer = MulLayer()</p>
			<p class="source-code"># forward</p>
			<p class="source-code">apple_price = mul_apple_layer.forward(apple, apple_num)</p>
			<p class="source-code">price = mul_tax_layer.forward(apple_price, tax)</p>
			<p class="source-code">print(price) # 220</p>
			<p>You can use <strong class="inline">backward()</strong> to obtain the differential of each variable.</p>
			<p class="source-code"># backward</p>
			<p class="source-code">dprice = 1</p>
			<p class="source-code">dapple_price, dtax = mul_tax_layer.backward(dprice)</p>
			<p class="source-code">dapple, dapple_num = mul_apple_layer.backward(dapple_price)</p>
			<p class="source-code">print(dapple, dapple_num, dtax) # 2.2 110 200</p>
			<p>Here, the order of calling <strong class="inline">backward()</strong> is the opposite of that of calling <strong class="inline">forward()</strong>. Note that the argument of <strong class="inline">backward()</strong> is the "derivative with respect to the output variable in forward propagation." For example, the multiplication layer, <strong class="inline">mul_apple_layer</strong>, returns <strong class="inline">apple_price</strong> in forward propagation, while it takes the derivative value of <strong class="inline">apple_price (dapple_price)</strong> as an argument in backward propagation. The execution result of this program matches the result shown in the preceding diagram.</p>
			<h3 id="_idParaDest-129"><a id="_idTextAnchor135"/>Implementing an Addition Layer</h3>
			<p>Now, we will implement an addition layer, which is an addition node, as follows:</p>
			<p class="source-code">class AddLayer:</p>
			<p class="source-code">   def __init__ (self):</p>
			<p class="source-code">      pass</p>
			<p class="source-code">   def forward(self, x, y):</p>
			<p class="source-code">      out = x + y</p>
			<p class="source-code">      return out</p>
			<p class="source-code">   def backward(self, dout): </p>
			<p class="source-code">      dx = dout * 1</p>
			<p class="source-code">      dy = dout * 1</p>
			<p class="source-code">      return dx, dy</p>
			<p>An addition layer requires no initialization, so <strong class="inline">__init__ ()</strong> does nothing (the pass statement is "does nothing"). <strong class="inline">forward()</strong> in the addition layer takes two arguments, <strong class="inline">x</strong> and <strong class="inline">y</strong>, and adds them for output. <strong class="inline">backward()</strong> passes the differential (<strong class="inline">dout</strong>) from the upper stream to the lower stream.</p>
			<p>Now, let's use the addition and multiplication layers to implement the purchase of two apples and three oranges, as shown in the following diagram. </p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/Figure_5.17.jpg" alt="Figure 5.17: Purchasing two apples and three oranges&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.17: Purchasing two apples and three oranges</h6>
			<p>You can implement this computational graph in Python as follows (the source code is located at <strong class="inline">ch05/buy_apple_orange.py</strong>):</p>
			<p class="source-code">apple = 100</p>
			<p class="source-code">apple_num = 2</p>
			<p class="source-code">orange = 150</p>
			<p class="source-code">orange_num = 3</p>
			<p class="source-code">tax = 1.1</p>
			<p class="source-code"># layer</p>
			<p class="source-code">mul_apple_layer = MulLayer()</p>
			<p class="source-code">mul_orange_layer = MulLayer()</p>
			<p class="source-code">add_apple_orange_layer = AddLayer()</p>
			<p class="source-code">mul_tax_layer = MulLayer()</p>
			<p class="source-code"># forward</p>
			<p class="source-code">apple_price = mul_apple_layer.forward(apple, apple_num) #(1)</p>
			<p class="source-code">orange_price = mul_orange_layer.forward(orange, orange_num) #(2)</p>
			<p class="source-code">all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)</p>
			<p class="source-code">price = mul_tax_layer.forward(all_price, tax) #(4)</p>
			<p class="source-code"># backward</p>
			<p class="source-code">dprice = 1</p>
			<p class="source-code">dall_price, dtax = mul_tax_layer.backward(dprice) #(4)</p>
			<p class="source-code">dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)</p>
			<p class="source-code">dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)</p>
			<p class="source-code">dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)</p>
			<p class="source-code">print(price) # 715</p>
			<p class="source-code">print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650</p>
			<p>This implementation is a little long, but each statement is simple. The required layers are created and the forward propagation method, <strong class="inline">forward()</strong>, is called in an appropriate order. Then, the backward propagation method, <strong class="inline">backward()</strong>, is called in the opposite order to forward propagation to obtain the desired derivatives.</p>
			<p>In this way, implementing layers (here, the addition and multiplication layers) in a computational graph are easy, and you can use them to obtain complicated derivatives. Next, we will implement the layers that are used in a neural network. </p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor136"/>Implementing the Activation Function Layer</h2>
			<p>Now, we will apply the idea of a computational graph to a neural network. Here, we will implement the "layers" that constitute a neural network in one class using the ReLU and Sigmoid layers, which are activation functions.</p>
			<h3 id="_idParaDest-131"><a id="_idTextAnchor137"/>ReLU Layer</h3>
			<p>A <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>) is used as an activation function and is expressed by the following equation (5.7):</p>
			<table id="table007-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.17_a.png" alt="49"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.7)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>From the preceding equation (5.7), you can obtain the derivative of y with respect to x with equation (5.8):</p>
			<table id="table008-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.17_b.png" alt="48"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.8)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>As equation (5.8) shows, if the input in forward propagation, x, is larger than 0, backward propagation passes the upstream value downstream without changing it. Meanwhile, if x is 0 or smaller in forward propagation, the signal stops there in backward propagation. You can express this in a computational graph, as shown in the following diagram: </p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/Figure_5.18.jpg" alt="Figure 5.18: Computational graph of the ReLU layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.18: Computational graph of the ReLU layer</h6>
			<p>Next, let's implement the ReLU layer. When implementing a layer in a neural network, we assume that <strong class="inline">forward()</strong> and <strong class="inline">backward()</strong> take NumPy arrays as arguments. The implementation of the ReLU layer is located at <strong class="inline">common/layers.py</strong>:</p>
			<p class="source-code">class Relu:</p>
			<p class="source-code">   def __init__ (self): </p>
			<p class="source-code">      self.mask = None</p>
			<p class="source-code">   def forward(self, x):</p>
			<p class="source-code">      self.mask = (x &lt;= 0)</p>
			<p class="source-code">      out = x.copy()</p>
			<p class="source-code">      out[self.mask] = 0</p>
			<p class="source-code">      return out</p>
			<p class="source-code">   def backward(self, dout):</p>
			<p class="source-code">      dout[self.mask] = 0</p>
			<p class="source-code">      dx = dout</p>
			<p class="source-code">      return dx</p>
			<p>The <strong class="inline">Relu</strong> class has an instance variable, <strong class="inline">mask</strong>. The <strong class="inline">mask</strong> variable is a NumPy array that consists of <strong class="inline">True</strong>/<strong class="inline">False</strong> values. If an element of the input, <strong class="inline">x</strong>, in forward propagation is <strong class="inline">0</strong> or smaller, the mask's corresponding element is <strong class="inline">True</strong>. Otherwise (if it is larger than 0), the element is <strong class="inline">False</strong>. For example, the <strong class="inline">mask</strong> variable contains a NumPy array that consists of <strong class="inline">True</strong> and <strong class="inline">False</strong>, as shown in the following code: </p>
			<p class="source-code">&gt;&gt;&gt; x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )</p>
			<p class="source-code">&gt;&gt;&gt; print(x)</p>
			<p class="source-code">[[  1. <a id="_idTextAnchor138"/>   -0.5]</p>
			<p class="source-code">[-2.    3. ]]</p>
			<p class="source-code">&gt;&gt;&gt; mask = (x &lt;= 0)</p>
			<p class="source-code">&gt;&gt;&gt; print(mask)</p>
			<p class="source-code">[[False True]</p>
			<p class="source-code">[ True False]] </p>
			<p>As shown in the preceding diagram, the value of backward propagation is 0 when the input value in forward propagation is 0 or smaller. Therefore, in backward propagation, the mask variable stored in forward propagation is used to set <strong class="inline">dout</strong> from the upper stream. If an element of the mask is <strong class="inline">True</strong>, the corresponding element in <strong class="inline">dout</strong> is set to 0. </p>
			<h4>Note</h4>
			<p class="callout">The ReLU layer works as a "switch" in a circuit. In forward propagation, it turns on the switch if an electric current flows through it, and turns off the switch if an electric current does not flow through it. In backward propagation, the electric current keeps on flowing if the switch is ON and does not flow any longer if the switch is OFF. </p>
			<h3 id="_idParaDest-132"><a id="_idTextAnchor139"/>Sigmoid Layer</h3>
			<p>Next, let's implement a sigmoid function. This is expressed by equation (5.9):</p>
			<table id="table009-1" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer210">
									<img src="image/Figure_5.18_a.png" alt="24"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(5.9)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>The following diagram shows the computational graph that represents equation (5.9): </p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/Figure_5.19.jpg" alt="Figure 5.19: Computational graph of the Sigmoid layer (forward propagation only) &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.19: Computational graph of the Sigmoid layer (forward propagation only) </h6>
			<p>Here, the <em class="italics">exp</em> and <em class="italics">/</em> nodes appear in addition to the <em class="italics">X</em> and <em class="italics">+</em> nodes. The <em class="italics">exp</em> node calculates <em class="italics">y = exp(x)</em>, while the <em class="italics">/</em> node calculates <img src="image/Figure_5.19_a.png" alt="47"/>.</p>
			<p>The calculation of equation (5.9) consists of the propagation of local calculations. Next, let's consider the backward propagation shown in the preceding computational graph, looking at the flow of backward propagation step by step to summarize what we have described so far. </p>
			<p><strong class="bold">Step 1:</strong></p>
			<p>The <em class="italics">/</em> node represents <img src="image/Figure_5.19_a.png" alt="46"/>. Its derivative is analytically expressed by the following equation:</p>
			<table id="table010" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="Basic-Paragraph"><strong class="inline"><img src="image/Figure_5.19_b.png" alt="47"/></strong></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.10)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Based on equation (5.10), in backward propagation, the node multiplies the upstream value by <em class="italics">−y</em><span class="superscript">2</span> (the additive inverse of the square of the output in forward propagation) and passes the value to the lower stream. The following computational graph shows this:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/Figure_5.20.jpg" alt="Figure 5.20: Computational graph of the Sigmoid layer (with the additive inverse of the square)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.20: Computational graph of the Sigmoid layer (with the additive inverse of the square)</h6>
			<p><strong class="bold">Step 2:</strong></p>
			<p>The <em class="italics">+</em> node only passes the upstream value to the lower stream. The following computational graph shows this:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/Figure_5.21.jpg" alt="Figure 5.21: Computational graph of the Sigmoid layer (with passing upstream value)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.21: Computational graph of the Sigmoid layer (with passing upstream value)</h6>
			<p><strong class="bold">Step 3:</strong></p>
			<p>The "exp" node represents <em class="italics">y = exp(x)</em>, and its derivative is expressed by the following equation: </p>
			<table id="table011" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.21_a.png" alt="48"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.11)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>In the following computational graph, the node multiplies the upstream value by the output in forward propagation (<em class="italics">exp(-x)</em>, in this example) and passes the value to the lower stream:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/Figure_5.22.jpg" alt="Figure 5.22: Computational graph of the Sigmoid layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.22: Computational graph of the Sigmoid layer</h6>
			<p><strong class="bold">Step 4:</strong></p>
			<p>The <em class="italics">X</em> node reverses the values in forward propagation for multiplication. Therefore, <em class="italics">−1</em> is multiplied here:</p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/Figure_5.23.jpg" alt="Figure 5.23: Computational graph of the Sigmoid layer (reversed values)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.23: Computational graph of the Sigmoid layer (reversed values)</h6>
			<p>Thus, we can show the backward propagation of the Sigmoid layer in the computational graph shown in the preceding diagram. According to the result of the preceding computational graph, the output of backward propagation is <img src="image/Figure_5.23_a.png" alt="45"/> and it is propagated to the downstream nodes. Note here that <img src="image/Figure_5.23_b.png" alt="44"/> can be calculated from the input, <em class="italics">x</em>, and output, <em class="italics">y</em>, of forward propagation. Therefore, we can draw the computational graph shown in the preceding diagram as a grouped "sigmoid" node, as follows:</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/Figure_5.24.jpg" alt="Figure 5.24: Computational graph of the Sigmoid layer (simple version) &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.24: Computational graph of the Sigmoid layer (simple version) </h6>
			<p>The computational graph in <em class="italics">Figure 5.23</em> and the simplified computational graph in <em class="italics">Figure 5.24</em> provide the same calculation result. The simple version is more efficient because it can omit the intermediate calculation in backward propagation. It is also important to note that you can only concentrate on the input and output, without caring about the details of the Sigmoid layer, by grouping the nodes.</p>
			<p>You can also organize <img src="image/Figure_5.24_a.png" alt="26"/> as follows: </p>
			<table id="table012" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.24_b.png" alt="53"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.12)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Therefore, you can only calculate the backward propagation in the Sigmoid layer shown in the preceding diagram from the output of forward propagation:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/Figure_5.25.jpg" alt="Figure 5.25: Computational graph of the Sigmoid layer – you can use the output, y, of forward propagation to calculate the backward propagation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.25: Computational graph of the Sigmoid layer – you can use the output, y, of forward propagation to calculate the backward propagation</h6>
			<p>Now, let's implement the Sigmoid layer in Python. Based on the preceding diagram, you can implement it as follows (this implementation is located at <strong class="inline">common/layers.py</strong>):</p>
			<p class="source-code">class Sigmoid:</p>
			<p class="source-code">   def __init__ (self): </p>
			<p class="source-code">      self.out = None</p>
			<p class="source-code">   def forward(self, x):</p>
			<p class="source-code">      out = 1 / (1 + np.exp(-x)) </p>
			<p class="source-code">      self.out = out</p>
			<p class="source-code">   return out</p>
			<p class="source-code">def backward(self, dout):</p>
			<p class="source-code">   dx = dout * (1.0 - self.out) * self.out</p>
			<p class="source-code">   return dx</p>
			<p>This implementation retains the output of forward propagation in the <strong class="inline">out</strong> instance variable, then uses the <strong class="inline">out</strong> variable for calculation purposes in backward propagation. </p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor140"/>Implementing the Affine and Softmax Layers</h2>
			<h3 id="_idParaDest-134"><a id="_idTextAnchor141"/>Affine Layer</h3>
			<p>In forward propagation in a neural network, the product of matrices (<strong class="inline">np.dot()</strong>, in NumPy) was used to sum the weighted signals (for details, refer to the <em class="italics">Calculating Multidimensional Arrays</em> section in <em class="italics">Chapter 3</em>, <em class="italics">Neural Networks</em>). For example, do you remember the following implementation in Python? </p>
			<p class="source-code">&gt;&gt;&gt; X = np.random.rand(2) # Input values</p>
			<p class="source-code">&gt;&gt;&gt; W = np.random.rand(2,3) # Weights</p>
			<p class="source-code">&gt;&gt;&gt; B = np.random.rand(3) # Biases</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; X.shape # (2,)</p>
			<p class="source-code">&gt;&gt;&gt; W.shape # (2, 3)</p>
			<p class="source-code">&gt;&gt;&gt; B.shape # (3,)</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; Y = np.dot(X, W) + B</p>
			<p>Here, assume that <strong class="inline">X</strong>, <strong class="inline">W</strong>, and <strong class="inline">B</strong> are multidimensional arrays of the shape <strong class="inline">(2,)</strong>, <strong class="inline">(2, 3)</strong>, and <strong class="inline">(3,)</strong>, respectively. With this, you can calculate the weighted sum of neurons as <strong class="inline">Y = np.dot(X, W) + B</strong>. <strong class="inline">Y</strong> is converted by the activation function and propagated to the next layer, which is the flow of forward propagation in a neural network. Note that the number of elements in corresponding dimensions must be the same for matrix multiplication. This means that in the product of <strong class="inline">X</strong> and <strong class="inline">W</strong>, the number of elements in corresponding dimensions must be the same, as shown in the following image. Here, the shape of a matrix is represented in parentheses as <em class="italics">(2, 3)</em> (this is for consistency with the output of NumPy's shape):</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/Figure_5.26.jpg" alt="Figure 5.26: The number of elements in corresponding dimensions &#13;&#10;must be the same for matrix multiplication&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.26: The number of elements in corresponding dimensions must be the same for matrix multiplication</h6>
			<h4>Note</h4>
			<p class="callout">The product of matrices in forward propagation in a neural network is called an "affine transformation" in the field of geometry. Therefore, we will implement the process that performs an affine transformation as an "Affine layer." </p>
			<p>Now, let's take a look at the calculation—the product of matrices and the sum of biases—in a computational graph. When we represent the node that calculates the product of matrices as "dot," the following computational graph can show the calculation <strong class="inline">np.dot(X, W) + B</strong>. Above each variable, the shape of the variable is indicated (for example, the shape of <em class="italics">X</em> is <em class="italics">(2,)</em> and that of <em class="italics">X – W</em> is <em class="italics">(3,)</em> are shown here):</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/Figure_5.27.jpg" alt="Figure 5.27: Computational graph of the Affine layer. Note that the variables are matrices. Above each variable, the shape of the variable is shown&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.27: Computational graph of the Affine layer. Note that the variables are matrices. Above each variable, the shape of the variable is shown</h6>
			<p>The preceding is a relatively simple computational graph. However, note that <em class="italics">X</em>, <em class="italics">W</em>, and <em class="italics">B</em> are multidimensional arrays. In the computational graphs we've looked at so far, "scalar values" flow between nodes, while in this example, "multidimensional arrays" propagate between nodes.</p>
			<p>Let's think about the backward propagation of the preceding computational graph. To obtain the backward propagation for multidimensional arrays, you can use the same procedure as the previous computational graphs used for scalar values by writing each element of the multidimensional arrays. Doing this, we can obtain the following equation (how we can obtain equation (5.13) is omitted here):</p>
			<table id="table013" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.27_a.png" alt="30"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.13)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>In equation (5.13), T in W<span class="superscript">T</span> indicates transpose. Transpose switches the (i, j) elements of W to the (j, i) elements, shown in the following equation:</p>
			<table id="table014" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.27_d.png" alt="43"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.14)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>As shown in equation (5.14), when the shape of W is (2, 3), the shape of W<span class="superscript">T</span> becomes (3, 2).</p>
			<p>Based on equation (5.13), let's write backward propagation in the computational graph. The following diagram shows the result:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/Figure_5.28.jpg" alt="Figure 5.28: Backward propagation of the Affine layer. Note that the variables are matrices. Below each variable, the shape of the variable is shown&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.28: Backward propagation of the Affine layer. Note that the variables are matrices. Below each variable, the shape of the variable is shown</h6>
			<p>Let's consider the shape of each variable carefully. Please note that X and <img src="image/Figure_5.28_a.png" alt="41"/> are the same shape, and that W and <img src="image/Figure_5.28_b.png" alt="40"/> are the same in terms of shape because of the following equation:</p>
			<table id="table015" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_5.28_d.png" alt="60"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(5.15)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>We pay attention to the shapes of matrices because the number of elements in the corresponding dimensions must be the same for matrix multiplication, and checking that they're the same can lead to equation (5.13). For example, consider the product of <img src="image/Figure_5.28_f.png" alt="33"/> and W so that the shape of <img src="image/Figure_5.28_g.png" alt="34"/> becomes (2,) when the shape of <img src="image/Figure_5.28_i.png" alt="35"/> is (3,) and the shape of W is (2,3). Then, equation (5.13) follows. This can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/Figure_5.29.jpg" alt="Figure 5.29: Product of matrices (you can create backward propagation of the &quot;dot&quot; node by configuring a product so that the number of elements in the corresponding dimensions is the same in the matrices)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.29: Product of matrices (you can create backward propagation of the "dot" node by configuring a product so that the number of elements in the corresponding dimensions is the same in the matrices)</h6>
			<h3 id="_idParaDest-135"><a id="_idTextAnchor142"/>Batch-Based Affine Layer</h3>
			<p>The Affine layer takes one piece of data as input, X. In this section, we will consider a batch-based Affine layer, which propagates N pieces of data collectively (a group of data is called a "batch"). Let's start by looking at the computational graph of the batch-based Affine layer (<em class="italics">Figure 5.30</em>).</p>
			<p>The only difference from the previous explanation is that the shape of the input, X, is now (N, 2). All we have to do is to calculate the matrices in the computational graph in the same way as we did previously. For backward propagation, we must be careful regarding the shapes of the matrices. Only after that can we obtain <img src="image/Figure_5.29_a.png" alt="36"/> and <img src="image/Figure_5.29_b.png" alt="37"/> in the same way.</p>
			<p>You must be careful when adding bias. When adding biases in forward propagation, a bias is added to each piece of data for X · W. For example, when N = 2 (two pieces of data), biases are added to each of the two pieces of data (to each calculation result). The following diagram shows a specific example of this:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/Figure_5.30.jpg" alt="Figure 5.30: Computational graph of the batch-based Affine layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.30: Computational graph of the batch-based Affine layer</h6>
			<p class="source-code">&gt;&gt;&gt; X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])</p>
			<p class="source-code">&gt;&gt;&gt; B = np.array([1, 2, 3])</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; X_dot_W</p>
			<p class="source-code">array([[  0,    0,    0],</p>
			<p class="source-code">    [ 10,  10, 10]])</p>
			<p class="source-code">&gt;&gt;&gt; X_dot_W + B</p>
			<p class="source-code">array([[  1,    2,    3],</p>
			<p class="source-code">    [11,  12, 13]])</p>
			<p>In forward propagation, the biases are added to each piece of data (the first, the second, and so on). Therefore, in backward propagation, the values of each piece of data in backward propagation must be integrated into the elements of biases. The following code shows this: </p>
			<p class="source-code">&gt;&gt;&gt; dY = np.array([[1, 2, 3,], [4, 5, 6]])</p>
			<p class="source-code">&gt;&gt;&gt; dY</p>
			<p class="source-code">array([[1,  2,  3],</p>
			<p class="source-code">[4,  5,  6]])</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; dB = np.sum(dY, axis=0)</p>
			<p class="source-code">&gt;&gt;&gt; dB</p>
			<p class="source-code">array([5,  7,  9])</p>
			<p>In this example, we're assuming that there are two pieces of data (N = 2). In backward propagation of biases, the derivatives with respect to the two pieces of data are summed for each piece of data. To do that, <strong class="inline">np.sum()</strong> sums the elements of axis 0.</p>
			<p>Thus, the implementation of the Affine layer is as follows. The Affine implementation in <strong class="inline">common/layers.py</strong> is slightly different from the implementation described here because it considers the case when input data is a tensor (four-dimensional array):</p>
			<p class="source-code">class Affine:</p>
			<p class="source-code">    def __init__ (self, W, b): </p>
			<p class="source-code">       self.W = W</p>
			<p class="source-code">       self.b = b</p>
			<p class="source-code">       self.x = None</p>
			<p class="source-code">       self.dW = None</p>
			<p class="source-code">       self.db = None</p>
			<p class="source-code">    def forward(self, x): </p>
			<p class="source-code">       self.x = x</p>
			<p class="source-code">       out = np.dot(x, self.W) + self.b</p>
			<p class="source-code">       return out</p>
			<p class="source-code">    def backward(self, dout):</p>
			<p class="source-code">        dx = np.dot(dout, self.W.T)</p>
			<p class="source-code">       self.dW = np.dot(self.x.T, dout)</p>
			<p class="source-code">       self.db = np.sum(dout, axis=0)</p>
			<p class="source-code">       return dx</p>
			<h3 id="_idParaDest-136"><a id="_idTextAnchor143"/>Softmax-with-Loss Layer</h3>
			<p>Finally, we should consider a softmax function, which is the output layer. The softmax function normalizes entered values and outputs them (as described earlier). For example, the following diagram shows the output of the Softmax layer for handwritten digit recognition.</p>
			<p>As we can see, the Softmax layer normalizes the entered values (meaning it converts them so that the total of the output values is 1) and outputs them. The Softmax layer has 10 inputs because handwritten digit recognition classifies data into 10 classes. </p>
			<h4>Note</h4>
			<p class="callout">Neural network processing consists of two phases: <strong class="bold">inference</strong> and <strong class="bold">training</strong>. Usually, inference in a neural network does not use the Softmax layer. For example, for inference in the network shown in the following diagram, the output of the final Affine layer is used as the inference result. The unnormalized output result from a neural network (the output of the Affine layer before the Softmax layer in the following diagram) is sometimes called a "score." To obtain only one answer in neural network inference, you only need to calculate the maximum score, so you do not need a Softmax layer. However, you do need a Softmax layer in neural network training.</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/Figure_5.31.jpg" alt="Figure 5.31: The images are converted by the Affine and ReLU layers and 10 input values &#13;&#10;are normalized by the Softmax layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.31: The images are converted by the Affine and ReLU layers and 10 input values are normalized by the Softmax layer</h6>
			<p>In this example, the score for "0" is 5.3, which is converted into 0.008 (0.8%) by the Softmax layer. The score for "2" is 10.1, which is converted into 0.991 (99.1%).</p>
			<p>Now, we will implement the "Softmax-with-Loss layer," which also contains a cross-entropy error, which is a loss function. The following diagram shows the computational graph of the Softmax-with-Loss layer (the softmax function and cross-entropy error):</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/Figure_5.32.jpg" alt="Figure 5.32: Computational graph of the Softmax-with-Loss layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.32: Computational graph of the Softmax-with-Loss layer</h6>
			<p>As you can see, the Softmax-with-Loss layer is slightly complicated. Only the result is shown here. If you are interested in how the Softmax-with-Loss layer is created, refer to the <em class="italics">Computational Graph of the Softmax-with-Loss Layer</em> section in the<em class="italics"> Appendix</em>.</p>
			<p>We can simplify the computational graph shown in the preceding diagram as follows:</p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/Figure_5.33.jpg" alt="Figure 5.33: &quot;Simplified&quot; computational graph of the Softmax-with-Loss layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.33: "Simplified" computational graph of the Softmax-with-Loss layer</h6>
			<p>In the preceding computational graph, the Softmax layer indicates the Softmax function, while the Cross-Entropy Error layer indicates the cross-entropy error. Here, we're assuming that the data, which is classified into three classes and three inputs (scores), is received from the previous layer. As we can see, the Softmax layer normalizes the input (a1, a2, a3) and outputs (y1, y2, and y3). The Cross-Entropy Error layer receives the output from Softmax, (y1, y2, y3), and the label, (t1, t2, t3), and outputs the loss, L, based on this data.</p>
			<p>Backward propagation from the Softmax layer returns (y1 t1, y2 t2, y3 t3), which is a "clean" result. Because (y1, y2, y3) is the output of the Softmax layer and (t1, t2, t3) is the label, (y1 − t1, y2 − t2, y3 − t3) is the difference between the output of the Softmax layer and the label. When backward propagating a neural network, an error, which is the difference, is passed to the previous layer. This characteristic is important when training a neural network.</p>
			<p>Note that the purpose of neural network training is to adjust weight parameters so that the neural network's output (output of Softmax) approaches the label. To do that, we need to pass the error between the neural network's output and the label to the previous layer in an efficient manner. The previous result, (y1 − t1, y2 − t2, y3 − t3), is exactly the difference between the output of the Softmax layer and the label, and clearly shows the current error between the neural network's output and the label.</p>
			<p>When we use a "cross-entropy error" as the loss function for the "softmax function," backward propagation returns a "pretty" result, (y1 − t1, y2 − t2, y3 − t3). This "pretty" result is not accidental. A function that calls a cross-entropy error is designed to do this. In a regression problem, an "identity function" is used for the output layer, and the "sum of squared errors" is used as the loss function (refer to the <em class="italics">Designing the Output Layer</em> section of <em class="italics">Chapter 3</em>, <em class="italics">Neural Networks</em>) for the same reason. When we use the sum of squared errors as the loss function of an "identity function", backward propagation provides a "pretty" result, (y1 − t1, y2 − t2, y3 − t3).</p>
			<p>Let's consider a specific example here. Say that, for one piece of data, the label is (0, 1, 0) and the output of the Softmax layer is (0.3, 0.2, 0.5). At this time, the neural network does not recognize it correctly because the probability of it being the correct label is 0.2 (20%). Here, backward propagating from the Softmax layer propagates a large error, (0.3, −0.8, 0.5). Because this large error propagates to the previous layers, the layers before the Softmax layer learn a lot from the large error.</p>
			<p>As another example, let's assume that, for one piece of data, the label is (0, 1, 0) and the output of the Softmax layer is (0.01, 0.99, 0) (this neural network recognizes quite accurately). In this case, backward propagating from the Softmax layer propagates a small error, (0.01, −0.01, 0). This small error propagates to the previous layers. The layers before the Softmax layer learn only small pieces of information because the error is small.</p>
			<p>Now, let's implement the Softmax-with-Loss layer. You can implement the Softmax-with-Loss layer as follows:</p>
			<p class="source-code">class SoftmaxWithLoss:</p>
			<p class="source-code">    def __init__ (self):</p>
			<p class="source-code">        self.loss = None # Loss</p>
			<p class="source-code">        self.y = None	# Output of softmax</p>
			<p class="source-code">        self.t = None	# Label data (one-hot vector)</p>
			<p class="source-code">    def forward(self, x, t):</p>
			<p class="source-code">        self.t = t</p>
			<p class="source-code">        self.y = softmax(x)</p>
			<p class="source-code">        self.loss = cross_entropy_error(self.y, self.t)</p>
			<p class="source-code">        return self.loss</p>
			<p class="source-code">    def backward(self, dout=1): </p>
			<p class="source-code">        batch_size = self.t.shape[0]</p>
			<p class="source-code">        dx = (self.y - self.t) / batch_size</p>
			<p class="source-code">        return dx</p>
			<p>This implementation uses the <strong class="inline">softmax()</strong> and <strong class="inline">cross_entropy_error()</strong> functions. They were implemented in the sub-section, <em class="italics">Issues when Implementing the Softmax Function</em>, of <em class="italics">Chapter 3</em>, <em class="italics">Neural networks</em> and sub-section, <em class="italics">Implementing Cross-Entropy Error (Using Batches)</em>, of <em class="italics">Chapter 4</em>, <em class="italics">Neural Network Training</em>. Therefore, the implementation here is very easy. Note that the error per data propagates to the previous layers in backward propagation because the value of propagation is divided by the number of batches (<strong class="inline">batch_size</strong>).</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor144"/>Implementing Backpropagation</h2>
			<p>You can build a neural network by combining the layers implemented in the previous sections as if you were assembling Lego blocks. Here, we will build a neural network by combining the layers we've implemented so far.</p>
			<h3 id="_idParaDest-138"><a id="_idTextAnchor145"/>Overall View of Neural Network Training</h3>
			<p>Because my description was a little long, let's check the overall view of neural network training again before proceeding with its implementation. Now we will take a look at the procedure for neural network training.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor146"/>Presupposition</h3>
			<p>A neural network has adaptable weights and biases. Adjusting them so that they fit the training data is called "training." Neural network training consists of the following four steps:</p>
			<p><strong class="bold">Step 1 (mini-batch):</strong></p>
			<p>Select some data at random from the training data.</p>
			<p><strong class="bold">Step 2 (calculating the gradients):</strong></p>
			<p>Obtain the gradient of the loss function for each weight parameter.</p>
			<p><strong class="bold">Step 3 (updating the parameters):</strong></p>
			<p>Update the parameters slightly in the gradient's direction.</p>
			<p><strong class="bold">Step 4 (repeating):</strong></p>
			<p>Repeat steps 1, 2, and 3.</p>
			<p>Backpropagation occurs in <em class="italics">Step 2,</em> <em class="italics">Calculating the gradients</em>. In the previous chapter, we used numerical differentiation to obtain a gradient. Numerical differentiation is easy to implement, but calculation takes a lot of time. If we use backpropagation, we can obtain a gradient much more quickly and efficiently.</p>
			<h3 id="_idParaDest-140"><a id="_idTextAnchor147"/>Implementing a Neural Network That Supports Backpropagation</h3>
			<p>In this section, we will implement a two-layer neural network called <strong class="inline">TwoLayerNet</strong>. First, we will look at the instance variables and methods of this class in <em class="italics">Tables 5.1</em> and <em class="italics">5.2</em>.</p>
			<p>The implementation of this class is a little long but has many sections in common with the implementation as in the <em class="italics">Implementing a Training Algorithm</em> section, of <em class="italics">Chapter 4</em>, <em class="italics">Neural Network Training</em>. A large change from the previous chapter is that layers are being used here. If you use layers, you can obtain recognition results (<strong class="inline">predict( )</strong>) and gradients (<strong class="inline">gradient( )</strong>) by propagating between the layers:</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/Table_5.1.jpg" alt="Table 5.1: Instance variables in the TwoLayerNet class&#13;&#10;"/>
				</div>
			</div>
			<h6>Table 5.1: Instance variables in the TwoLayerNet class</h6>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/Table_5.2.jpg" alt="Table 5.2: Methods in the TwoLayerNet class&#13;&#10;"/>
				</div>
			</div>
			<h6>Table 5.2: Methods in the TwoLayerNet class</h6>
			<p>Now, let's implement <strong class="inline">TwoLayerNet</strong>:</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir)</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from common.layers import *</p>
			<p class="source-code">from common.gradient import numerical_gradient</p>
			<p class="source-code">from collections import OrderedDict</p>
			<p class="source-code">class TwoLayerNet:</p>
			<p class="source-code">    def __init__ (self, input_size, hidden_size, output_size,</p>
			<p class="source-code">            weight_init_std=0.01):</p>
			<p class="source-code">    # Initialize weights</p>
			<p class="source-code">    self.params = {}</p>
			<p class="source-code">    self.params['W1'] = weight_init_std * \</p>
			<p class="source-code">                    np.random.randn(input_size, hidden_size)</p>
			<p class="source-code">    self.params['b1'] = np.zeros(hidden_size)</p>
			<p class="source-code">    self.params['W2'] = weight_init_std * \</p>
			<p class="source-code">                    np.random.randn(hidden_size, output_size)</p>
			<p class="source-code">    self.params['b2'] = p.zeros(output_size)</p>
			<p class="source-code">    # Create layers</p>
			<p class="source-code"><strong class="inline">    </strong><strong class="inline">self.layers = OrderedDict( )</strong></p>
			<p class="source-code"><strong class="inline">    self.layers['Affine1'] = \</strong></p>
			<p class="source-code"><strong class="inline">        Affine(self.params['W1'], self.params['b1'])</strong></p>
			<p class="source-code"><strong class="inline">    self.layers['Relu1'] = Relu( ) </strong></p>
			<p class="source-code"><strong class="inline">    self.layers['Affine2'] = \</strong></p>
			<p class="source-code"><strong class="inline">    Affine(self.params['W2'], self.params['b2'])</strong></p>
			<p class="source-code"><strong class="inline">    self.lastLayer = SoftmaxWithLoss( )</strong></p>
			<p class="source-code">    def predict(self, x):</p>
			<p class="source-code"><strong class="inline">        for layer in self.layers.values( ):</strong></p>
			<p class="source-code"><strong class="inline">            </strong><strong class="inline">x = layer.forward(x)</strong></p>
			<p class="source-code">        return x</p>
			<p class="source-code">    # x: input data, t: label data</p>
			<p class="source-code">    def loss(self, x, t):</p>
			<p class="source-code">        y = self.predict(x)</p>
			<p class="source-code">        return self.lastLayer.forward(y, t)</p>
			<p class="source-code">    def accuracy (self, x, t): </p>
			<p class="source-code">        y = self.predict(x)</p>
			<p class="source-code">        y = np.argmax(y, axis=1)</p>
			<p class="source-code">        if t.ndim != 1 : t = np.argmax(t, axis=1)</p>
			<p class="source-code">        accuracy = np.sum(y == t) / float(x.shape[0])</p>
			<p class="source-code">        return accuracy</p>
			<p class="source-code">    # x: input data, t: teacher data</p>
			<p class="source-code">    def numerical_gradient(self, x, t):</p>
			<p class="source-code">        loss_W = lambda W: self.loss(x, t)</p>
			<p class="source-code">        grads = {}</p>
			<p class="source-code">        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])</p>
			<p class="source-code">        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])</p>
			<p class="source-code">        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])</p>
			<p class="source-code">        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])</p>
			<p class="source-code">        return grads</p>
			<p class="source-code">    def gradient(self, x, t): </p>
			<p class="source-code"><strong class="inline">        # forward</strong></p>
			<p class="source-code"><strong class="inline">        self.loss(x, t)</strong></p>
			<p class="source-code"><strong class="inline">        # backward</strong></p>
			<p class="source-code"><strong class="inline">        dout = 1</strong></p>
			<p class="source-code"><strong class="inline">        dout = self.lastLayer.backward(dout)</strong></p>
			<p class="source-code"><strong class="inline">        layers = list(self.layers.values( ))</strong></p>
			<p class="source-code"><strong class="inline">        layers.reverse( )</strong></p>
			<p class="source-code"><strong class="inline">        </strong><strong class="inline">for layer in layers:</strong></p>
			<p class="source-code"><strong class="inline">            dout = layer.backward(dout)</strong></p>
			<p class="source-code">        # Settings</p>
			<p class="source-code">        grads = {}</p>
			<p class="source-code">        grads['W1'] = self.layers['Affine1'].dW</p>
			<p class="source-code">        grads['b1'] = self.layers['Affine1'].db</p>
			<p class="source-code">        grads['W2'] = self.layers['Affine2'].dW</p>
			<p class="source-code">        grads['b2'] = self.layers['Affine2'].db</p>
			<p class="source-code">        return grads</p>
			<p>Note the code in bold here. Retaining a neural network layer as <strong class="inline">OrderedDict</strong> (i,e, an ordered dictionary) is especially important, as it means that the dictionary can remember the order of the elements added to it. Therefore, in forward propagation in the neural network, you can complete processing by calling the <strong class="inline">forward()</strong> method of the layer in the order of addition. In backward propagation, you only have to call the layers in reverse order. The Affine and ReLU layers internally process forward propagation and backward propagation properly. So, all you have to do is combine the layers in the correct order and call them in order (or in reverse order).</p>
			<p>Thus, by implementing the components of the neural network as "layers," you can build the neural network easily. The advantage of modular implementation using "layers" is enormous. If you want to create a large network containing five, 10, or 20 layers, you can create it by adding the required layers (as if you were assembling Lego blocks). In this way, the gradients required for recognition and learning are obtained properly by forward propagation and backward propagation being implemented in each layer.</p>
			<h3 id="_idParaDest-141"><a id="_idTextAnchor148"/>Gradient Check</h3>
			<p>So far, we have seen two methods for calculating a gradient. One of them uses numerical differentiation, while the other solves the equation analytically. The latter method enables efficient calculation by using backpropagation, even if many parameters exist. Therefore, we'll be using backpropagation instead of slow numerical differentiation to calculate a gradient from now on.</p>
			<p>Numerical differentiation takes time to calculate. If the correct implementation of backpropagation exists, we do not need the implementation of numerical differentiation. So, what is numerical differentiation useful for? The fact is that numerical differentiation is required to check whether the implementation of backpropagation is correct.</p>
			<p>The advantage of numerical differentiation is that it is easy to implement, making mistakes infrequent compared to the far more complicated backpropagation. So, the result of numerical differentiation is often compared with that of backpropagation to check whether the implementation of backpropagation is correct. The process of this verification is called a <strong class="bold">gradient check</strong>. The following code shows the implementation of a gradient check (the source code is located at <strong class="inline">ch05/gradient_check.py</strong>):</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir)</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from dataset.mnist import load_mnist</p>
			<p class="source-code">from two_layer_net import TwoLayerNet</p>
			<p class="source-code"># Load data</p>
			<p class="source-code">(x_train, t_train), (x_test, t_test) = \</p>
			<p class="source-code">   load_mnist(normalize=True, one_hot_label=True)</p>
			<p class="source-code">network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)</p>
			<p class="source-code">x_batch = x_train[:3]</p>
			<p class="source-code">t_batch = t_train[:3]</p>
			<p class="source-code">grad_numerical = network.numerical_gradient(x_batch, t_batch) </p>
			<p class="source-code">grad_backprop = network.gradient(x_batch, t_batch)</p>
			<p class="source-code"># Calculate the average of the absolute errors of individual weights</p>
			<p class="source-code">for key in grad_numerical.keys( ):</p>
			<p class="source-code">   diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )</p>
			<p class="source-code">   print(key + ":" + str(diff))</p>
			<p>Here, the MNIST dataset is loaded as usual. Next, part of the training data is used to check the error between the gradient by numerical differentiation and that of backpropagation. As the error, the absolute values of the differences between the elements in the individual weight parameters are averaged. When the preceding code is executed, the following result is returned:</p>
			<p class="source-code">b1:9.70418809871e-13</p>
			<p class="source-code">W2:8.41139039497e-13</p>
			<p class="source-code">b2:1.1945999745e-10</p>
			<p class="source-code">W1:2.2232446644e-13</p>
			<p>The result shows that the differences between the gradients by numerical differentiation and those by backpropagation are quite small. Case in point, the error of the biases for layer 1 is <strong class="inline">9.7e-13 (0.00000000000097)</strong>. This indicates that the gradient by backpropagation is also correct and improves the reliability of its accuracy.</p>
			<p>The error between the calculation result of numerical differentiation and that of backpropagation rarely becomes 0. This is because the accuracy of the calculations that are performed by a computer is finite (for example, 32-bit floating-point numbers are used). Because the numerical precision is limited, the error is not usually 0. However, if the implementation is correct, the error is expected to be a small value near 0. If the value is large, the implementation of backpropagation is incorrect.</p>
			<h3 id="_idParaDest-142"><a id="_idTextAnchor149"/>Training Using Backpropagation</h3>
			<p>Lastly, we will see how we can implement neural network training using backpropagation. The only difference is that gradients are calculated via backpropagation. We will only see the code and omit the description (the source code is located at <strong class="inline">ch05/train_neuralnet.py</strong>):</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir)</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from dataset.mnist import load_mnist</p>
			<p class="source-code">from two_layer_net import TwoLayerNet</p>
			<p class="source-code"># Load data</p>
			<p class="source-code">(x_train, t_train), (x_test, t_test) = \</p>
			<p class="source-code">    load_mnist(normalize=True, one_hot_label=True)</p>
			<p class="source-code">network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)</p>
			<p class="source-code">iters_num = 10000</p>
			<p class="source-code">train_size = x_train.shape[0]</p>
			<p class="source-code">batch_size = 100</p>
			<p class="source-code">learning_rate = 0.1</p>
			<p class="source-code">train_loss_list = [ ]</p>
			<p class="source-code">train_acc_list = [ ]</p>
			<p class="source-code">test_acc_list = [ ]</p>
			<p class="source-code">iter_per_epoch = max(train_size / batch_size, 1)</p>
			<p class="source-code">for i in range(iters_num):</p>
			<p class="source-code">   batch_mask = np.random.choice(train_size, batch_size)</p>
			<p class="source-code">   x_batch = x_train[batch_mask]</p>
			<p class="source-code">   t_batch = t_train[batch_mask]</p>
			<p class="source-code"><strong class="inline">   # Use backpropagation to obtain a gradient</strong></p>
			<p class="source-code"><strong class="inline">   grad = network.gradient(x_batch, t_batch)</strong></p>
			<p class="source-code">   # Update</p>
			<p class="source-code">   for key in ('W1', 'b1', 'W2', 'b2'): </p>
			<p class="source-code">      network.params[key] -= learning_rate * grad[key]</p>
			<p class="source-code">   loss = network.loss(x_batch, t_batch)</p>
			<p class="source-code">   train_loss_list.append(loss)</p>
			<p class="source-code">if i % iter_per_epoch == 0:</p>
			<p class="source-code">   train_acc = network.accuracy(x_train, t_train)</p>
			<p class="source-code">   test_acc = network.accuracy(x_test, t_test)</p>
			<p class="source-code">   train_acc_list.append(train_acc)</p>
			<p class="source-code">   test_acc_list.append(test_acc)</p>
			<p class="source-code">   print(train_acc, test_acc)</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor150"/>Summary</h2>
			<p>In this chapter, we learned about computational graphs, which show calculation processes visually. We looked at a computational graph that described backpropagation in a neural network and implemented processing in a neural network with layers, including the ReLU layer, Softmax-with-Loss layer, Affine layer, and Softmax layer. These layers have forward and backward methods and can calculate the gradients of weight parameters efficiently by propagating data both forward and backward in direction. By using layers as modules, you can combine them freely in a neural network so that you can build the desired network easily. The following points were covered in this chapter:</p>
			<ul>
				<li>We can use computational graphs to show calculation processes visually.</li>
				<li>A node in a computational graph consists of local calculations. Local calculations constitute the whole calculation.</li>
				<li>Performing forward propagation in a computational graph leads to a regular calculation. Meanwhile, performing backward propagation in a computational graph can calculate the differential of each node.</li>
				<li>By implementing components in a neural network as layers, you can calculate gradients efficiently (backpropagation).</li>
				<li>By comparing the results of numerical differentiation and backpropagation, you can check that the implementation of backpropagation is correct (gradient check).</li>
			</ul>
		</div>
		<div>
			<div id="_idContainer247" class="Content">
			</div>
		</div>
	</body></html>