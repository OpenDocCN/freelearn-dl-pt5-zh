- en: Chapter¬†2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentals of Bayesian Inference
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get into Bayesian inference with **Deep Neural Networks** (**DNNs**),
    we should take some time to understand the fundamentals. In this chapter, we‚Äôll
    do just that: exploring the core concepts of Bayesian modeling, and taking a look
    at some of the popular methods used for Bayesian inference. By the end of this
    chapter, you should have a good understanding of why we use probabilistic modeling,
    and what kinds of properties we look for in well principled ‚Äì or well conditioned
    ‚Äì methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This content will be covered in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Refreshing our knowledge of Bayesian modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian inference via sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Gaussian processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.1 Refreshing our knowledge of Bayesian modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bayesian modeling is concerned with understanding the probability of an event
    occurring given some prior assumptions and some observations. The prior assumptions
    describe our initial beliefs, or hypothesis, about the event. For example, let‚Äôs
    say we have two six-sided dice, and we want to predict the probability that the
    sum of the two dice is 5\. First, we need to understand how many possible outcomes
    there are. Because each die has 6 sides, the number of possible outcomes is 6
    √ó 6 = 36\. To work out the possibility of rolling a 5, we need to work out how
    many combinations of values will sum to 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.1: Illustration of all values summing to five when rolling two six-sided
    dice'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, there are 4 combinations that add up to 5, thus the probability
    of having two dice produce a sum of 5 is ![-4 36](img/file8.jpg), or ![1 9](img/file9.jpg).
    We call this initial belief the **prior**. Now, what happens if we incorporate
    information from an observation? Let‚Äôs say we know what the value for one of the
    dice will be ‚Äì let‚Äôs say 3\. This shrinks our number of possible values down to
    6, as we only have the remaining die to roll, and for the result to be 5, we‚Äôd
    need this value to be 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.2: Illustration of remaining value, which sums to five after rolling
    the first die'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we assume our die is fair, the probability of the sum of the dice being
    5 is now ![1 6](img/file11.jpg). This probability, called the **posterior**, is
    obtained using information from our observation. At the core of Bayesian statistics
    is Bayes‚Äô rule (hence ‚ÄùBayesian‚Äù), which we use to determine the posterior probability
    given some prior knowledge. Bayes‚Äô rule is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![P(A |B ) = P(B-|A)√ó-P-(A)- P(B ) ](img/file12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where we can define *P*(*A*|*B*) as *P*(*d*[1] + *d*[2] = 5|**d**[1] = **3**),
    where *d*[1] and *d*[2] represent dice 1 and 2 respectively. We can see this in
    action using our previous example. Starting with the **likelihood**, that is,
    the term on the left of our numerator, we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![1- P (B |A) = P (d1 = 3|d1 + d2 = 5) = 4 ](img/file13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can verify this by looking at our grid. Moving to the second part of the
    numerator ‚Äì the prior ‚Äì we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 4 1 P(A ) = P (d1 + d2 = 5) =--= -- 36 9 ](img/file14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the denominator, we have our **normalization constant** (also referred to
    as the **marginal likelihood**), which is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '![P(B ) = P (d1 = 3) = 1 6 ](img/file15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting this all together using Bayes‚Äô theorem, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 14 √ó 19 1 P(d1 + d2 = 5|d1 = 3) = --1---= 6- 6 ](img/file16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What we have here is the *probability* of the outcome being 5 if we know one
    die‚Äôs value. However, in this book, we‚Äôll often be referring to **uncertainties**
    rather than probabilities ‚Äì and learning methods to obtain uncertainty estimates
    with DNNs. These methods belong to a broader class of **uncertainty** **quantification**,
    and aim to quantify the uncertainty in the predictions from an ML model. That
    is, we want to predict *P*(*≈∑*|*ùúÉ*), where *≈∑* is a prediction from a model, and
    *ùúÉ* represents the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: As we know from fundamental probability theory, probabilities are bound between
    0 and 1\. The closer we are to 1, the more likely ‚Äì or probable ‚Äì the event is.
    We can view our uncertainty as subtracting our probability from 1\. In the context
    of the example here, the probability of the sum being 5 is *P*(*d*[1] + *d*[2]
    = 5|*d*[1] = 3) = ![1 6](img/file17.jpg) = 0*.*166\. So, our uncertainty is simply
    1 ‚àí![16](img/file18.jpg) = ![56](img/file19.jpg) = 0*.*833, meaning that there‚Äôs
    a *>* 80% chance that the outcome *will* *not* be 5\. As we proceed through the
    book, we‚Äôll learn about different sources of uncertainty, and how uncertainties
    can help us to develop more robust deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs continue using our dice example to build a better understanding of for
    model uncertainty estimates. Many common machine learning models work on the basis
    of **maximum likelihood estimation** or **MLE**. That is, they look to predict
    the value that is *most likely*: tuning their parameters during training to produce
    the most likely outcome *≈∑* given some input *x*. As a simple illustration, let‚Äôs
    say we want to predict the value of *d*[1] + *d*[2] given a value of *d*[1]. We
    can simply define this as the **expectation** of *d*[1] + *d*[2] conditioned on
    *d*[1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ÀÜy = ùîº [d + d |d ] 1 2 1 ](img/file20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, the *mean* of the possible values of *d*[1] + *d*[2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting *d*[1] = 3, our possible values for *d*[1] + *d*[2] are {4*,*5*,*6*,*7*,*8*,*9}
    (as illustrated in *Figure 2.2*), making our mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ 1 ‚àë6 4+ 5 + 6+ 7+ 8 + 9 Œº = -- ai = --------------------= 6.5 6 i=1 6 ](img/file21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the value we‚Äôd get from a simple linear model, such as a linear regression
    defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ÀÜy = Œ≤x + Œæ ](img/file22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the values of our intersection and bias are *Œ≤* = 1, *Œæ* = 3*.*5\.
    If we change our value of *d*[1] to 1, we see that this mean changes to 4*.*5
    ‚Äì the mean of the set of possible values of *d*[1] + *d*[2]|*d*[1] = 1, in other
    words {2*,*3*,*4*,*5*,*6*,*7}. This perspective on our model predictions is important:
    while this example is very straightforward, the same principle applies to far
    more sophisticated models and data. The value we typically see with ML models
    is the *expectation*, otherwise known as the mean. As you are likely aware, the
    mean is often referred to as the **first statistical moment** ‚Äì with the **second
    statistical** **moment** being the **variance**, and the variance allows us to
    quantify uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance for our simple example is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ‚àë6 2 œÉ2 = --i=1(ai ‚àí-Œº) n ‚àí 1 ](img/file23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These statistical moments should be familiar to you, as should the fact that
    the variance here is represented as the square of the **standard deviation**,
    *œÉ*. For our example here, for which we assume *d*[2] is a fair die, the variance
    will always be constant: *œÉ*¬≤ = 2*.*917\. That is to say, given any value of *d*[1],
    we know that values of *d*[2] are all equally likely, so the uncertainty does
    not change. But what if we have an unfair die *d*[2], which has a 50% chance of
    landing on a 6, and a 10% chance of landing on each other number? This changes
    both our mean and our variance. We can see this by looking at how we would represent
    this as a set of possible values (in other words, a perfect sample of the die)
    ‚Äì the set of possible values for *d*[1] + *d*[2]|*d*[1] = 1 now becomes {2*,*3*,*4*,*5*,*6*,*7*,*7*,*7*,*7*,*7}.
    Our new model will now have a bias of *Œæ* = 4*.*5, making our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ÀÜy = 1 √ó 1 + 4.5 = 5.5 ](img/file24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that the expectation has increased due to the change in the underlying
    probability of the values of die *d*[1]. However, the important difference here
    is in the change in the variance value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ‚àë10 (a ‚àí Œº)2 œÉ2 = --i=1--i----- = 3.25 n ‚àí 1 ](img/file25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our variance has *increased*. As variance essentially gives us the average
    of the distance of each possible value from the mean, this shouldn‚Äôt be surprising:
    given the weighted die, it‚Äôs more likely that the outcome will be distant from
    the mean than with an unweighted die, and thus our variance increases. To summarize,
    in terms of uncertainty: the greater the likelihood that the outcome will be further
    from the mean, the greater the uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: This has important implications for how we interpret predictions from machine
    learning models (and statistical models more generally). If our predictions are
    an approximation of the mean, and our uncertainty quantifies how likely it is
    for an outcome to be distant from the mean, then our uncertainty tells us **how
    likely it** **is that our model prediction is incorrect**. Thus, model uncertainties
    allow us to decide when to trust the predictions, and when we should be more cautious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples given here are very basic, but should help to give you an idea
    of what we‚Äôre looking to achieve with model uncertainty quantification. We will
    continue to explore these concepts as we learn about some of the benchmark methods
    for Bayesian inference, learning how these concepts apply to more complex, real-world
    problems. We‚Äôll start with perhaps the most fundamental method of Bayesian inference:
    sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Bayesian inference via sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practical applications, it‚Äôs not possible to know exactly what a given outcome
    would be, and, similarly, it‚Äôs not possible to observe all possible outcomes.
    In these cases, we need to make a best estimate based on the evidence we have.
    The evidence is formed of **samples** ‚Äì observations of possible outcomes. The
    aim of ML, broadly speaking, is to learn models that generalize well from a subset
    of data. The aim of Bayesian ML is to do so while also providing an estimate of
    the uncertainty associated with the model‚Äôs predictions. In this section, we‚Äôll
    learn about how we can use sampling to do this, and will also learn why sampling
    may not be the most sensible approach.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Approximating distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the most fundamental level, sampling is about approximating distributions.
    Say we want to know the distribution of the height of people in New York. We could
    go out and measure everyone, but that would involve measuring the height of 8.4
    million people! While this would give us our most accurate answer, it‚Äôs also a
    deeply impractical approach.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can sample from the population. This gives us a basic example of
    **Monte Carlo sampling**, where we use random sampling to provide data from which
    we can approximate a distribution. For example, given a database of New York residents,
    we could select ‚Äì at random ‚Äì a sub-population of residents, and use this to approximate
    the height distribution of all residents. With random sampling ‚Äì and any sampling,
    for that matter ‚Äì the accuracy of the approximation is dependent on the size of
    the sub-population. What we‚Äôre looking to achieve is a **statistically significant**
    sub-sample, such that we can be confident in our approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better imdivssion of this, we‚Äôll simulate the problem by generating
    100,000 data points from a truncated normal distribution, to approximate the kind
    of height distribution we may see for a population of 100,000 people. Say we draw
    10 samples, at random, from our population. Here‚Äôs what our distribution would
    look like (on the right) compared with the true distribution (on the left):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.3: Plot of true distribution (left) versus sample distribution (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, this isn‚Äôt a great representation of the true distribution:
    what we see here is closer to a triangular distribution than a truncated normal.
    If we were to infer something about the population‚Äôs height based on this distribution
    alone, we‚Äôd arrive at a number of inaccurate conclusions, such as missing the
    truncation above 200 cm, and the tail on the left of the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a better imdivssion by increasing our sample size ‚Äì let‚Äôs try drawing
    100 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.4: Plot of true distribution (left) versus sample distribution (right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Things are starting to look better: we‚Äôre starting to see some of the tail
    on the left as well as the truncation toward 200 cm. However, this sample has
    sampled more from some regions than others, leading to misrepresentation: our
    mean has been pulled down, and we‚Äôre seeing two distinct peaks, rather than the
    single peak we see in the true distribution. Let‚Äôs increase our sample size by
    a further order of magnitude, scaling up to 1,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.5: Plot of true distribution (left) versus sample distribution (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is looking much better ‚Äì with a sample set of only one hundredth the size
    of our true population, we now see a distribution that closely matches our true
    distribution. This example demonstrates how, through random sampling, we can approximate
    the true distribution using a significantly smaller pool of observations. But
    that pool still has to have enough information to allow us to arrive at a good
    approximation of the true distribution: too few samples and our subset will be
    statistically *insufficient*, leading to poor approximation of the underlying
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But simple random sampling isn‚Äôt the most practical method for approximating
    distributions. To achieve this, we turn to **probabilistic inference**. Given
    a model, probabilistic inference provides a way to find the model parameters that
    best describe our data. To do so, we need to first define the type of model ‚Äì
    this is our prior. For our example, we‚Äôll use a truncated Gaussian: the idea here
    being, using our intuition, it‚Äôs reasonable to assume people‚Äôs height follows
    a normal distribution, but that very few people are above, say, 6‚Äô5.‚Äù So, we‚Äôll
    specify a truncated Gaussian distribution with an upper limit of 205 cm, or just
    over 6‚Äô5.‚Äù As it‚Äôs a Gaussian distribution, in other words, ùí©(*Œº,œÉ*), our model
    parameters are *ùúÉ* = {*Œº,œÉ*} ‚Äì with the additional constraint that our distribution
    has an upper limit of *b* = 205.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to a fundamental class of algorithms: **Markov Chain Monte**
    **Carlo**, or **MCMC** methods. Like simple random sampling, these allow us to
    build a picture of the true underlying distribution, but they do so sequentially,
    whereby each sample is dependent on the sample before it. This sequential dependence
    is known as the **Markov property**, thus the *Markov chain* component of the
    name. This sequential approach accounts for the probabilistic dependence between
    samples and allows us to better approximate the probability density.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MCMC achieves this through sequential random sampling. Just as with the random
    sampling we‚Äôre familiar with, MCMC randomly samples from our distribution. But,
    unlike simple random sampling, MCMC considers pairs of samples: some previous
    sample *x*[*t*‚àí1] and some current sample *x*[*t*]. For each pair of samples,
    we have some criteria that specifies whether or not we keep the sample (this varies
    depending on the particular flavor of MCMC). If the new value meets this criteria,
    say if *x*[*t*] is ‚Äùpreferential to‚Äù our previous value *x*[*t*‚àí1], then the sample
    is added to the chain and becomes *x*[*t*] for the next round. If the sample doesn‚Äôt
    meet the criteria, we stick with the current *x*[*t*] for the next round. We repeat
    this over a (usually large) number of iterations, and in the end we should arrive
    at a good approximation of our distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is an efficient sampling method that is able to closely approximate
    the true parameters of our distribution. Let‚Äôs see how this applies to our height
    distribution example. Using MCMC with just 10 samples, we arrive at the following
    approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.6: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not bad for ten samples ‚Äì certainly far better than the triangular distribution
    we arrived at with simple random sampling. Let‚Äôs see how we do with 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.7: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is looking pretty excellent ‚Äì in fact, we‚Äôre able to obtain a better approximation
    of our distribution with 100 MCMC samples than we are with 1,000 simple random
    samples. If we continue to larger numbers of samples, we‚Äôll arrive at closer and
    closer approximations of our true distribution. But our simple example doesn‚Äôt
    fully capture the power of MCMC: MCMC‚Äôs true advantage comes from being able to
    approximate high-dimensional distributions, and has made it an invaluable technique
    for approximating intractable high-dimensional integrals in a variety of domains.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we‚Äôre interested in how we can estimate the probability distribution
    of the parameters of machine learning models ‚Äì this allows us to estimate the
    uncertainty associated with our predictions. In the next section, we‚Äôll take a
    look at how we do this practically by applying sampling to Bayesian linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Implementing probabilistic inference with Bayesian linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In typical linear regression, we want to predict some output *≈∑* from some
    input *x* using a linear function *f*(*x*), such that *≈∑* = *Œ≤x* + *Œæ*. With Bayesian
    linear regression, we do this probabilistically, introducing another parameter,
    *œÉ*¬≤, such that our regression equation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ÀÜy = ùí© (x Œ≤ + Œæ,œÉ2 ) ](img/file31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, *≈∑* follows a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see our familiar bias term *Œæ* and intercept *Œ≤*, and introduce a
    variance parameter *œÉ*¬≤. To fit our model, we need to define a prior over these
    parameters ‚Äì just as we did for our MCMC example in the last section. We‚Äôll define
    these priors as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Œæ ‚âà ùí© (0,1 ) ](img/file32.jpg)![Œ≤ ‚âà ùí© (0,1) ](img/file33.jpg)![œÉ2 ‚âà |ùí© (0,1)|
    ](img/file34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that equation 2.15 denotes the half-normal of a Gaussian distribution
    (the positive half of a zero-mean Gaussian, as standard deviation cannot be negative).
    We‚Äôll refer to our model parameters as *ùúÉ* = *Œ≤,Œæ,œÉ*¬≤, and we‚Äôll use sampling
    to find the parameters that maximise the likelihood of these given our data, in
    other words, the conditional probability of our parameters given our data *D*:
    *P*(*ùúÉ*|*D*).'
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of MCMC sampling approaches we could use to find our model
    parameters. A common approach is to use the **Metropolis-Hastings** algorithm.
    Metropolis-Hastings is particularly useful for sampling from intractable distributions.
    It does so through the use of a proposal distribution, *Q*(*ùúÉ*‚Ä≤|*ùúÉ*), which is
    proportional to, but not exactly equal to, our true distribution. This means that,
    for example, if some value *x*[1] is twice as likely as some other value *x*[2]
    in our true distribution, this will be true of our proposal distribution too.
    Because we‚Äôre interested in the probability of observations, we don‚Äôt need to
    know what the *exact* value would be in our true distribution ‚Äì we just need to
    know that, proportionally, our proposal distribution is equivalent to our true
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the key steps of Metropolis-Hastings for our Bayesian linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize with an arbitrary point *ùúÉ* sampled from our parameter
    space, according to the priors for each of our parameters. Using a Gaussian distribution
    centered on our first set of parameters *ùúÉ*, select a new point *ùúÉ*‚Ä≤. Then, for
    each iteration *t* ‚àà *T*, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the acceptance criteria, defined as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ P(ùúÉ‚Ä≤|D ) Œ± = -------- P(ùúÉ|D ) ](img/file35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Generate a random number from a uniform distribution *ùúñ* ‚àà [0*,*1]. If *ùúñ <*=
    *Œ±*, accept the new candidate parameters ‚Äì adding these to the chain, assigning
    *ùúÉ* = *ùúÉ*‚Ä≤. If *ùúñ > Œ±*, keep the current *ùúÉ* and draw a new value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This acceptance criteria means that, if our new set of parameters have a higher
    likelihood than our last set of parameters, we‚Äôll see *Œ± >* 1, in which case *Œ±
    < ùúñ*. This means that, when we sample parameters that are *more likely* given
    our data, we‚Äôll always accept these parameters. If, on the other hand, *Œ± <* 1,
    there‚Äôs a chance we‚Äôll reject the parameters, but we may also accept them ‚Äì allowing
    us to explore regions of lower likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: These mechanics of Metropolis-Hastings result in samples that can be used to
    compute high-quality approximations of our posterior distribution. Practically,
    Metropolis-Hastings (and MCMC methods more generally) requires a burn-in phase
    ‚Äì an initial phase of sampling used to escape regions of low density, which are
    typically encountered given the arbitrary initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs apply this to a simple problem: we‚Äôll generate some data for the function
    *y* = *x*¬≤ + 5 + *Œ∑*, where *Œ∑* is a noise parameter distributed according to
    *Œ∑* ‚âàùí©(0*,*5). Using Metropolis-Hastings to fit our Bayesian linear regressor,
    we get the following fit using the points sampled from our function (represented
    by the crosses):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.8: Bayesian linear regression on generated data with low variance'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that our model fits the data in the same way we would expect for standard
    linear regression. However, unlike standard linear regression, our model produces
    predictive uncertainty: this is represented by the shaded region. This predictive
    uncertainty gives an imdivssion of how much our underlying data varies; this makes
    this model much more useful than a standard linear regression, as now we can get
    an imdivssion of the sdivad of our data, as well as the general trend. We can
    see how this varies if we generate new data and fit again, this time increasing
    the sdivad of the data by modifying our noise distribution to *Œ∑* ‚âàùí©(0*,*20):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.9: Bayesian linear regression on generated data with high variance'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that our predictive uncertainty has increased proportionally to the
    sdivad of the data. This is an important property in uncertainty-aware methods:
    when we have small uncertainty, we know our prediction fits the data well, whereas
    when we have large uncertainty, we know to treat our prediction with caution,
    as it indicates the model isn‚Äôt fitting this region particularly well. We‚Äôll see
    a better example of this in the next section, which will go on to demonstrate
    how regions of more or less data contribute to our model uncertainty estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see that our predictions fit our data pretty well. In addition, we
    see that *œÉ*¬≤ varies according to the availability of data in different regions.
    What we‚Äôre seeing here is a great example of a very important concept, **well
    calibrated uncertainty** ‚Äì also termed **high-quality** **uncertainty**. This
    refers to the fact that, in regions where our Predictions are inaccurate, our
    uncertainty is also high. Our uncertainty estimates are **poorly calibrated**
    if we‚Äôre very confident in regions with inaccurate predictions, or very uncertain
    in regions with accurate predictions. As it‚Äôs well-calibrated, sampling is often
    used as a benchmark for uncertainty quantification.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, while sampling is effective for many applications, the need to
    obtain many samples for each parameter means that it quickly becomes computationally
    prohibitive for high dimensions of parameters. For example, if we wanted to start
    sampling parameters for complex, non-linear relationships (such as sampling the
    weights of a neural network), sampling would no longer be practical. Despite this,
    it‚Äôs still useful in some cases, and later we‚Äôll see how various BDL methods make
    use of sampling.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we‚Äôll explore the Gaussian process ‚Äì another fundamental
    method for Bayesian inference, and a method that does not suffer from the same
    computational overheads as sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Exploring the Gaussian process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we‚Äôve seen in the previous section, sampling quickly becomes prohibitively
    expensive. To address this, we can use ML models specifically designed to produce
    uncertainty estimates ‚Äì the gold standard of which is the **Gaussian** **process**.
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian process, or **GP**, has become a staple probabilistic ML model,
    seeing use in a broad variety of applications from pharmacology through to robotics.
    Its success is largely down to its ability to produce high-quality uncertainty
    estimates over its predictions in a well-principled fashion. So, what do we mean
    by a Gaussian process?
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, a GP is a distribution over functions. To understand what we mean
    by this, let‚Äôs take a typical ML use case. We want to learn some function *f*(**x**),
    which maps a series of inputs **x** onto a series of outputs **y**, such that
    we can approximate our output via **y** = *f*(**x**). Before we see any data,
    we know nothing about our underlying function; there is an infinite number of
    possible functions this could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.10: Illustration of space of possible functions before seeing data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the black line is the true function we wish to learn, while the dotted
    lines are the possible functions given the data (in this case, no data). Once
    we observe some data, we see that the number of possible functions becomes more
    constrained, as we see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.11: Illustration of space of possible functions after seeing some
    data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see that our possible functions all pass through our observed data
    points, but outside of those data points, our functions take on a range of very
    different values. In a simple linear model, we don‚Äôt care about these deviations
    in possible values: we‚Äôre happy to interpolate from one data point to another,
    as we see in *Figure* [2.12](#x1-30007r12):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.12: Illustration of linearly interpolating through our observations'
  prefs: []
  type: TYPE_NORMAL
- en: But this interpolation can lead to wildly inaccurate predictions, and has no
    way of accounting for the degree of uncertainty associated with our model predictions.
    The deviations that we see here in the regions without data points are exactly
    what we want to capture with our GP. When there are a variety of possible values
    our function can take, then there is uncertainty ‚Äì and through capturing the degree
    of uncertainty, we are able to estimate what the possible variation in these regions
    may be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, a GP can be defined as a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![f(x) ‚âà GP (m (x),k(x,x‚Ä≤)) ](img/file41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *m*(**x**) is simply the mean of our possible function values for a given
    point **x**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![m (x) = ùîº[f (x)] ](img/file42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next term, *k*(**x***,***x**‚Ä≤) is a covariance function, or kernel. This
    is a fundamental component of the GP as it defines the way we model the relationship
    between different points in our data. GPs use the mean and covariance functions
    to model the space of possible functions, and thus to produce predictions as well
    as their associated uncertainties. Now that we‚Äôve introduced some of the high-level
    concepts, let‚Äôs dig a little deeper and understand exactly how it is they model
    the space of possible functions, and thus estimate uncertainty. To do this, we
    need to understand GP priors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Defining our prior beliefs with kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GP kernels describe the prior beliefs we have about our data, and so you‚Äôll
    often see them referred to as GP priors. In the same way that the prior in equation
    2.3 tells us something about the probability of the outcome of our two dice rolls,
    the GP prior tells us something important about the relationship we expect from
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: While there are advanced methods for inferring a prior from our data, they are
    beyond the scope of this book. We will instead focus on more traditional uses
    of GPs, for which we select a prior using our knowledge of the data we‚Äôre working
    with.
  prefs: []
  type: TYPE_NORMAL
- en: In the literature and any implementations you encounter, you‚Äôll see that the
    GP prior is often referred to as the **kernel** or **covariance function** (just
    as we have here). These three terms are all interchangeable, but for consistency
    with other work, we will henceforth refer to this as the kernel. Kernels simply
    provide a means of calculating a distance between two data points, and are exdivssed
    as *k*(*x,x*‚Ä≤), where *x* and *x*‚Ä≤ are data points, and *k*() represents the function
    of the kernel. While the kernel can take on many forms, there are a small number
    of fundamental kernels that are used in a large proportion of GP applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most commonly encountered kernel is the **squared exponential**
    or **radial basis function** (**RBF**) kernel. This kernel takes the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ (x ‚àí x ‚Ä≤)2 k(x,x ‚Ä≤) = œÉ2exp ‚àí ----2---- 2l ](img/file43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This introduces us to a couple of common kernel parameters: *l* and *œÉ*¬≤. The
    output variance parameter *œÉ*¬≤ is simply a scaling factor, used to control the
    distance of the function from its mean. The length scale parameter *l* controls
    the smoothness of the function ‚Äì in other words, how much your function is expected
    to vary across particular dimensions. This parameter can either be a scalar that
    is applied to all input dimensions, or a vector with a different scalar value
    for each input dimension. The latter is often achieved using **Automatic Relevance**
    **Determination**, or **ARD**, which identifies the relevant values in the input
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: GPs make predictions via a covariance matrix based on the kernel ‚Äì essentially
    comparing a new data point to previously observed data points. However, just as
    with all ML models, GPs need to be trained, and this is where the length scale
    comes in. The length scale forms the parameters of our GP, and through the training
    process it learns the optimal value(s) for the length scale(s). This is typically
    done using a nonlinear optimizer, such as the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) optimizer. Many optimizers can be used, including optimizers you may
    be familiar with for deep learning, such as stochastic gradient descent and its
    variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs take a look at how different kernels affect GP predictions. We‚Äôll start
    with a straightforward example ‚Äì a simple sine wave:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.13: Plot of sine wave with four sampled points'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the function illustrated here, as well as some points sampled from
    this function. Now, let‚Äôs fit a GP with a periodic kernel to the data. The periodic
    kernel is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ‚Ä≤ 2 ( 2sin2(œÄ |x ‚àí x‚Ä≤|‚àïp)) kper(x, x) = œÉ exp -------l2-------- ](img/file45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we see a new parameter: *p*. This is simply the period of the periodic
    function. Setting *p* = 1 and applying a GP with a periodic kernel to the preceding
    example, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.14: Plot of posterior predictions from a periodic kernel with *p*
    = 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'This looks pretty noisy, but you should be able to see that there is clear
    periodicity in the functions produced by the posterior. It‚Äôs noisy for a couple
    of reasons: a lack of data, and a poor prior. If we‚Äôre limited on data, we can
    try to fix the problem by improving our prior. In this case, we can use our knowledge
    of the periodicity of the function to improve our prior by setting *p* = 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.15: Plot of posterior predictions from a periodic kernel with *p*
    = 6'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that this fits the data pretty well: we‚Äôre still uncertain in regions
    for which we have little data, but the periodicity of our posterior now looks
    sensible. This is possible because we‚Äôre using an informative prior; that is,
    a prior that incorporates information that describes the data well. This prior
    is composed of two key components:'
  prefs: []
  type: TYPE_NORMAL
- en: Our periodic kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our knowledge about the periodicity of the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see how important this is if we modify our GP to use an RBF kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.16: Plot of posterior predictions from an RBF kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 'With an RBF kernel, we see that things are looking pretty chaotic again: because
    we have limited data and a poor prior, we‚Äôre unable to appropriately constrain
    the space of possible functions to fit our true function. In the ideal case, we‚Äôd
    fix this by using a more appropriate prior, as we saw in *Figure* [*2.15*](#x1-31013r15)
    ‚Äì but this isn‚Äôt always possible. Another solution is to sample more data. Sticking
    with our RBF kernel, we sample 10 data points from our function and re-train our
    GP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.17: Plot of posterior predictions from an RBF kernel, trained on 10
    observations'
  prefs: []
  type: TYPE_NORMAL
- en: This is looking much better ‚Äì but what if we have more data *and* an informative
    prior?
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.18: Plot of posterior predictions from a periodic kernel with *p*
    = 6, trained on 10 observations'
  prefs: []
  type: TYPE_NORMAL
- en: The posterior now fits our true function very closely. Because we don‚Äôt have
    infinite data, there are still some areas of uncertainty, but the uncertainty
    is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we‚Äôve seen some of the core principles in action, let‚Äôs return to
    our example from *Figures* [*2.10*](#x1-30002r10)*-*[*2.12*](#x1-30007r12). Here‚Äôs
    a quick reminder of our target function, our posterior samples, and the linear
    interpolation we saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.19: Plot illustrating the difference between linear interpolation
    and the true function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we‚Äôve got some idea of how a GP will affect our predictive posterior,
    it‚Äôs easy to see that linear interpolation falls very short of what we achieve
    with a GP. To illustrate this more clearly, let‚Äôs take a look at what the GP prediction
    would be for this function given the three samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.20: Plot illustrating the difference between GP predictions and the
    true function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the dotted lines are our mean (*Œº*) predictions from the GP, and the
    shaded area is the uncertainty associated with those predictions ‚Äì the standard
    deviation (*œÉ*) around the mean. Let‚Äôs contrast what we see in *Figure* [*2.20*](#x1-31026r20)
    with *Figure* [*2.19*](#x1-31024r19). The differences may seem subtle at first,
    but we can clearly see that this is no longer a straightforward linear interpolation:
    the predicted values from the GP are being ‚Äùpulled‚Äù toward our actual function
    values. As with our earlier sine wave examples, the behavior of the GP predictions
    are affected by two key factors: the prior (or kernel) and the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But there‚Äôs another crucial detail illustrated in *Figure* [*2.20*](#x1-31026r20):
    the predictive uncertainties from our GP. We see that, unlike many typical ML
    models, a GP gives us uncertainties associated with its predictions. This means
    we can make better decisions about what we do with the model‚Äôs predictions ‚Äì having
    this information will help us to ensure that our systems are more robust. For
    example, if the uncertainty is too great, we can fall back to a manual system.
    We can even keep track of data points with high predictive uncertainty so that
    we can continuously refine our models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this refinement affects our predictions by adding a few more
    observations ‚Äì just as we did in the earlier examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure¬†2.21: Plot illustrating the difference between GP predictions and the
    true function, trained on 5 observations'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure* [*2.21*](#x1-31029r21) illustrates how our uncertainty changes over
    regions with different numbers of observations. We see here that between *x* =
    3 and *x* = 4 our uncertainty is quite high. This makes a lot of sense, as we
    can also see that our GP‚Äôs mean predictions deviate significantly from the true
    function values. Conversely, if we look at the region between *x* = 0*.*5 and
    *x* = 2, we can see that our GP‚Äôs predictions follow the true function fairly
    closely, and our model is also more confident about these predictions, as we can
    see from the smaller interval of uncertainty in this region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we‚Äôre seeing here is a great example of a very important concept: **well**
    **calibrated uncertainty** ‚Äì also termed **high-quality uncertainty**. This refers
    to the fact that, in regions where our predictions are inaccurate, our uncertainty
    is also high. Our uncertainty estimates are **poorly calibrated** if we‚Äôre very
    confident in regions with inaccurate predictions, or very uncertain in regions
    with accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPs are what we can term a **well principled** method ‚Äì this means that they
    have solid mathematical foundations, and thus come with strong theoretical guarantees.
    One of these guarantees is that they are well calibrated, and this is what makes
    GPs so popular: if we use GPs, we know we can rely on their uncertainty estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, however, GPs are not without their shortcomings ‚Äì we‚Äôll learn
    more about these in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Limitations of Gaussian processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the fact that GPs are well-principled and capable of producing high-quality
    uncertainty estimates, you‚Äôd be forgiven for thinking they‚Äôre the perfect uncertainty-aware
    ML model. GPs struggle in a few key situations:'
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large amounts of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly complex data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two points here are largely down to the inability of GPs to scale
    well. To understand this, we just need to look at the training and inference procedures
    for GPs. While it‚Äôs beyond the scope of this book to cover this in detail, the
    key point here is in the matrix operations required for GP training.
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, it is necessary to invert a *D* √ó *D* matrix, where *D* is
    the dimensionality of our data. Because of this, GP training quickly becomes computationally
    prohibitive. This can be somewhat alleviated through the use of Cholesky deomposition,
    rather than direct matrix inversion. As well as being more computationally efficient,
    Cholesky decomposition is also more numerically stable. Unfortunately, Cholesky
    decomposition also has its weaknesses: computationally, its complexity is *O*(*n*¬≥).
    This means that, as the size of our dataset increases, GP training becomes more
    and more expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But it‚Äôs not only training that‚Äôs affected: because we need to compute the
    covariance between a new data point and all observed data points at inference,
    GPs have a *O*(*n*¬≤) computational complexity at inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As well as the computational cost, GPs aren‚Äôt light in memory: because we need
    to store our covariance matrix **K**, GPs have a *O*(*n*¬≤) memory complexity.
    Thus, in the case of large datasets, even if we have the compute resources necessary
    to train them, it may not be practical to use them in real-world applications
    due to their memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The last point in our list concerns the complexity of data. As you are probably
    aware ‚Äì and as we‚Äôll touch on in *Chapter 3, Fundamentals of Deep Learning* ‚Äì
    one of the major advantages of DNNs is their ability to process complex, high-dimensional
    data through layers of non-linear transformations. While GPs are powerful, they‚Äôre
    also relatively simple models, and they‚Äôre not able to learn the kinds of powerful
    feature representations that are possible with DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these factors mean that, while GPs are an excellent choice for relatively
    low-dimensional data and reasonably small datasets, they aren‚Äôt practical for
    many of the complex problems we face in ML. And so, we turn to BDL methods: methods
    that have the flexibility and scalability of deep learning, while also producing
    model uncertainty estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we‚Äôve covered some of the fundamental concepts and methods
    related to Bayesian inference. First, we reviewed Bayes‚Äô theorem and the fundamentals
    of probability theory ‚Äì allowing us to understand the concept of uncertainty,
    as well as how we apply it to the predictions of ML models. Next, we introduced
    sampling, and an important class of algorithms: Markov Chain Monte Carlo, or MCMC,
    methods. Lastly, we covered Gaussian processes, and illustrated the crucial concept
    of well calibrated uncertainty. These key topics will provide you with the necessary
    foundation for the content that will follow, however, we encourage you to explore
    the recommended reading materials for a more comprehensive treatment of the topics
    introduced in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how DNNs have changed the landscape of machine
    learning over the last decade, exploring the tremendous advantages offered by
    deep learning, and the motivation behind the development of BDL methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a variety of techniques being explored to improve the flexibility
    and scalability of GPs ‚Äì such as Deep GPs or Sparse GPs. The following resources
    explore some of these topics, and also provide a more thorough treatment of the
    content covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bayesian Analysis with Python*, Martin: this book comprehensively covers core
    topics in statistical modeling and probabilistic programming, and includes practical
    walk-throughs of various sampling methods, as well as a good overview of Gaussian
    processes and a variety of other techniques core to Bayesian analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: this is
    often considered the definitive text on Gaussian processes, and provides highly
    detailed explanations of the theory underlying Gaussian processes. A key text
    for anyone serious about Bayesian inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
