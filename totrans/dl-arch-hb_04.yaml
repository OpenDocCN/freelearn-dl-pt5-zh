- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **recurrent neural network** (**RNN**) is a neural network that is made to
    process sequential data while being aware of the sequence of the data. Sequential
    data can involve time series based data and data that has a sequence but does
    not have a time component, such as text data. The applications of such a neural
    network are built upon the nature of the data itself. For time-series data, this
    can be either for nowcasting (predictions made for the current time with both
    past and present data) or forecasting targets. For text data, applications such
    as speech recognition and machine translation can utilize these neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Research in recurrent neural networks has slowed in the past few years with
    the advent of neural networks that can capture sequential data while removing
    recursive connections completely and achieving better performance, such as transformers.
    However, RNNs are still used extensively in the real world today to serve as a
    good baseline or just an alternative model for faster computations due to their
    lower number of computations and low memory requirements with reasonable metric
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The two most prominent RNN layers are **Long Short-Term Memory** (**LSTM**)
    and **Gated Recurrent Units** (**GRU**). We will not be going through the vanilla
    and the original recurrent neural networks in this book, but we will show LSTM
    and GRU as a refresher. The main operations of LSTM and GRU provide a mechanism
    to keep only relevant memory and ignore data that is not useful, which is a key
    inductive bias crafted for time series or sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will dive deeper into these two RNN networks more extensively.
    Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding advancements over the standard GRU and LSTM layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is short and sweet but still covers some practical implementations
    in the `Pytorch` library installed.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTM was invented in 1997 but remains a widely adopted neural network. LSTM
    uses the `tanh` activation function as it provides nonlinearities while providing
    second derivatives that can be preserved for a longer sequence. The `tanh` function
    helps to prevent exploding and vanishing gradients. An LSTM layer uses a sequence
    of LSTM cells sequentially connected. Let’s take an in-depth look at what the
    LSTM cell looks like in *Figure 4**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM
    cells that forms an LSTM layer](img/B18187_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM cells
    that forms an LSTM layer
  prefs: []
  type: TYPE_NORMAL
- en: 'The first LSTM cell on the left depicts the high-level structure of an LSTM
    cell and the second LSTM cell on the left depicts the medium-level operations,
    connections, and structure of an LSTM cell, while the third cell on the right
    is just another LSTM cell to emphasize that LSTM layers are made of multiple LSTM
    cells sequentially connected to each other. Think of an LSTM cell as containing
    four gating mechanisms that provide a way to **forget**, **learn**, **remember**,
    and **use** the sequential data. One notable thing you might wonder about is why
    the sigmoid is shown as a separate process in three paths from the input and past
    the hidden state to the forget gate, the remember gate, and the use gate. The
    structure shown in *Figure 4**.1* is a famous depiction of the LSTM cell but does
    not contain information about the weights of the connections. This is due to the
    fact that the inputs go through a weighted addition process to combine the previous
    cell’s hidden state and the current sequence data with different sets of weights
    for each of the four connections. *Figure 4**.2* shows the final low-level structure
    of a single LSTM cell with weights considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Low-level structure of the LSTM cell](img/B18187_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Low-level structure of the LSTM cell
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.2*, W and B represent the weights and biases, respectively. The
    two small letters represent the data type and gate mechanisms, respectively. The
    data type is split into two – the hidden state represented by h and the input
    data represented by i. The gate mechanisms involved are the forget mechanism represented
    by F, the learning mechanism represented by L, where there are two weights and
    biases associated with learning, and the use mechanism represented by U. To properly
    perceive how many parameters an LSTM cell has we still have to decode the dimensions
    of the hidden state and input state weight vectors. For ease of reference, let’s
    take the input vector size as n and the hidden size as m.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state weights have a dimension of:'
  prefs: []
  type: TYPE_NORMAL
- en: nm
  prefs: []
  type: TYPE_NORMAL
- en: 'While the input state weights have a dimension of:'
  prefs: []
  type: TYPE_NORMAL
- en: n 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias, on the other hand, is the size of the input vector size. For Tensorflow
    and Keras with Tensorflow, bias is only added once per mechanism. For PyTorch,
    the bias is added for each hidden state and input state weight. For PyTorch, the
    number of parameters for bias can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: 2n
  prefs: []
  type: TYPE_NORMAL
- en: 'As there are four mechanisms, as depicted in *Figure 4**.2*, this means that
    the number of parameters for an LSTM in the PyTorch implementation can then be
    computed according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: number of parameters = 4(nm + n 2 + 2n)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand where the actual parameters live in the cell, let’s dive
    into each of these gating mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the forget mechanism of LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The forget mechanism is accomplished by using a sigmoid activation function
    multiplied against the previous cell state. The name of the gating mechanism implies
    that it determines the information to be removed based on a combination of the
    current input sequence and the previous cell output. A way to think of it is,
    on a scale of `0` to `1`, how relevant is the information from the past? The sigmoid
    mechanism forces the scale to be between `0` and `1`. Values closer to `0` forget
    more of the previous cell state (long-term memory) and values closer to `1` forget
    less of the previous cell state memory.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the learn mechanism of LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The learn mechanism employs a combination of sigmoid activation of previous
    cell output and `tanh` activation of previous cell output added on the outputs
    of the forget gate and multiplied by the outputs of the use gate. This mechanism
    is also known as the input gate. This mechanism allows information learning from
    the current input sequence. The information learned then gets passed into the
    remembering mechanism. Additionally, the information learned will also get passed
    into the mechanism that allows information usage for the next LSTM cell. Both
    of these mechanisms will be introduced sequentially next.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the remember mechanism of LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remember mechanism is achieved by simply adding up information from what’s
    left of the forget process and what has been learned, which is the output of the
    learning gate. The output of this gate will then be considered the current cell
    state of the LSTM cell. The cell state contains what is known as the long-term
    memory of the LSTM sequence. Take this mechanism simply as an operation that allows
    the network to selectively choose which part of the input to maintain and remember.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the “information-using” mechanism of LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The information-using mechanism is achieved by applying nonlinearities using
    the tanh activation function on the current cell state and again using the current
    input sequence and previous cell output as a weighting mechanism to determine
    how much relevant information from the past and present should be used. The output
    of applying the use gate gives us the hidden state that will also be used as the
    previous cell output for the next LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: Building a full LSTM network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Usually, to create a full LSTM network, multiple LSTM layers are concatenated
    to each other using the sequence of hidden states from multiple LSTM cells as
    the subsequent sequence data to apply to the next LSTM layer. After a couple of
    LSTM layers, the hidden state sequence of the previous layer will usually then
    be passed into a fully connected layer to form the basis of supervised learning
    based simple LSTM architecture. *Figure 4**.3* shows a visual structure of how
    this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully
    connected layer](img/B18187_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully connected
    layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the network depicted in *Figure 4**.3*, the implementation in PyTorch
    will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first import the PyTorch library’s handy `nn` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define the network architecture based on *Figure 4**.3*, using
    the sequential API this time instead of the class method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The input size, hidden sizes, and layer number of the LSTM, along with the output
    feature size of the linear layer, can be configured according to your input dataset
    and desire. Note that each timestep or sequential step of the input data can have
    a size greater than one. This allows us to easily map original features into more
    representative feature embeddings and leverage their descriptive power. Additionally,
    the dropout regularizer can be added easily by setting the `dropout` parameter
    to a value between 0 and 1, which will introduce the dropout layer at each layer
    except the last layer at the specified probability. The RNN defined in `pytorch`
    in *step 2* can now be trained as usual, like any PyTorch models defined in a
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As usual, PyTorch has made building RNNs so much simpler and faster. Next, we
    will step into the next type of RNN, called **Gated** **Recurrent Units**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gated recurrent** **units** (**GRU**) was invented in 2014 and based on the
    ideas implemented in LSTM. GRU was made to simplify LSTM and provide a faster
    and more efficient way of achieving the same goals as LSTM to adaptively remember
    and forget based on past and present data. In terms of the learning capacity and
    metric performance achievable, there isn’t a clear silver-bullet winner among
    the two and often in the industry, the two RNN units are benchmarked against each
    other to figure out which method provides a better performance level. *Figure
    4**.4* shows the structure of GRU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – A low-level depiction of GRU](img/B18187_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – A low-level depiction of GRU
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.4* adopts the same weights and bias notations as the LSTM depicted
    in *Figure 4**.2*. There are three different names here for the final small letter
    notation. R being the reset gate, z representing the update gate, and h representing
    weights used to obtain the next hidden states. This means a GRU cell has fewer
    parameters than an LSTM cell, with three sets of weights and biases instead of
    four. This allows GRU networks to be slightly faster than LSTM networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although depicted as a single cell, the same theory that required multiple
    LSTM cells to be sequentially connected also applies to GRU: a GRU network layer
    will have multiple GRU cells connected sequentially together. GRU contains only
    two mechanisms, called the **reset gate** and the **update gate**, and only has
    one input from the previous GRU cell, and one output to the next GRU cell. The
    one input-output itself is obviously more efficient than LSTM, as we require fewer
    operations to be carried out. Now, let’s dive into these two mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the reset gate of GRU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reset gate of GRU serves as a mechanism to forget the long-term information,
    also called the hidden state, of the previous cell. The goal of this mechanism
    is similar to the forget gate in the LSTM cell. Similarly, this will exist on
    a scale of 0 to 1, based on the current input sequence and the previous cell state,
    which decides how much we should reduce and remove the previously gained long-term
    information.
  prefs: []
  type: TYPE_NORMAL
- en: However, the reset gates of GRU are functionally different from the forget gate
    of LSTM. While the forget gate of LSTM decides what information to forget from
    the long-term memory, the reset gate of GRU decides how much of the previous hidden
    state to forget.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the update gate of GRU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The update gate of GRU controls the amount of information from the long-term
    memory to be transferred to the currently maintained memory. This is similar to
    the remember gate in LSTMs and helps the network to remember long-term information.
    Each weight associated with the previous cell’s hidden unit will learn to capture
    both short-term dependencies and long-term dependencies. The short-term dependencies
    usually have reset gates output values that are closer to `0` to forget former
    information more frequently, and vice versa with weights and hidden state positions
    that learn long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the difference from the LSTM remember gate, while the remember gate
    of LSTM decides what information to remember from the current input and previous
    hidden state, the update gate of GRU decides how much of the previous hidden state
    to remember.
  prefs: []
  type: TYPE_NORMAL
- en: GRU is a simple RNN to consider with more efficient operations compared to LSTMs.
    Now that we have decoded both LSTMs and GRU, instead of repeating another GRU-only
    full network similar to LSTM, let’s discover improvements that can be made using
    these two methods as a base.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding advancements over the standard GRU and LSTM layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GRU and LSTM are the most widely used RNN methods today, but one might wonder
    how to push the boundaries achievable by a standard GRU or a standard LSTM. One
    good start to building this intuition is to understand that both of the layer
    types are capable of accepting sequential data, and to build a network you need
    multiple RNN layers. This means that it is entirely possible to combine GRU and
    LSTM layers in the same network. This, however, is not credible enough to be considered
    an advancement as a fully LSTM network or a fully GRU network can exceed the performance
    of a combined LSTM and GRU network at any time. Let’s dive into another simple
    improvement you can make on top of these standard RNN layers, called **bidirectional
    RNN**.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding bidirectional RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both GRU and LSTM rely on the sequential nature of the data. This order of the
    sequence can be forward in increasing time steps and also can be backward in decreasing
    time steps. Which direction to use usually comes down to an act of trial and error,
    and more often than not, the natural direction to use will be the forward time
    order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the year 1997, an improvement was made called bidirectional RNNs, which
    combine both forward-ordered RNNs with backward-ordered RNNs in an effort to maximize
    the input data that can be processed by an RNN model. The original idea was to
    estimate the value at a current timestep using both future information and historical
    information, given that both future and historical information was available with
    two of the RNNs taking in different sets of data. This naturally allowed for the
    capacity to achieve better prediction performance on such data setups. Today,
    this idea has extended to be a general layer applied to the same sequential data
    estimating and is also proven to provide prediction performance improvements.
    *Figure 4**.5* shows an example of bidirectional RNNs using GRU where the hidden
    states from the forward and backward GRU are concatenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Bidirectional GRU](img/B18187_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Bidirectional GRU
  prefs: []
  type: TYPE_NORMAL
- en: 'The concatenated hidden states can then be passed into fully connected layers
    for standard supervised learning objectives. An example implementation in PyTorch
    of a bidirectional GRU is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s discover an improvement that is made based on LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Adding peepholes to LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced in 2000, peepholes enable the cell states (from the previous and
    current cell), which hold the long-term memory of LSTMs, to influence the sigmoid
    gating mechanisms in the LSTM cell. The intuition is that the long-term memory
    has information about the past time steps that are not available in the short-term
    memory held in the hidden state of the previous cell. This allowed for improved
    predictive performance when compared to a vanilla LSTM. *Figure 4**.6* shows the
    extra peephole connections from the cell state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – LSTM peephole connections](img/B18187_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – LSTM peephole connections
  prefs: []
  type: TYPE_NORMAL
- en: However, one pitfall of this method is that the cell states can grow to large
    values over time due to the long-term memory nature of the states, as it is unbounded.
    This may saturate the gates to always be in an open state and render the gate
    useless sometimes. This brings us to the last improvement that we will discuss
    in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Adding working memory to exceed the peephole connection limitations for LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2021, an improvement was made on top of peephole connections for LSTMs to
    enforce a bound to the cell states by using the `tanh` activation. The simple
    addition proved itself to be better in performance than the LSTM with peepholes
    unbounded version in experimental benchmarks and was called **Working Memory Connections**
    **for LSTM**. *Figure 4**.7* shows the Working Memory Connection for LSTM structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Working Memory Connection for LSTM structure](img/B18187_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Working Memory Connection for LSTM structure
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are similar to the standard MLP in the sense that there isn’t a single
    generic dataset that is used as a reference across different research initiatives.
    Even if the same dataset is used, the results might not be a definitive source
    of truth as, again, the dataset is not extensive enough to generalize across other
    datasets. In other words, there is no `ImageNet` equivalent in sequence data.
    Text data, video data, and other time-series data are all wildly different from
    each other but essentially are all considered sequence data and can be fed into
    RNNs. Take benchmark results from anywhere on RNNs and MLPs with a pinch of salt
    as results can vary widely from dataset to dataset. *Figure 4**.8*, however, shows
    one version of benchmarks done with GRU, LSTM, `COCO` dataset using a metric that
    considers the naturalism of the produced text – the higher the better.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – RNN benchmark on image captioning task on COCO dataset](img/B18187_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – RNN benchmark on image captioning task on COCO dataset
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows that LSTM-WM dominates over the other methods in a single experiment
    across different evaluation scores. Again, take the results with a pinch of salt
    as the COCO dataset is by no means a representative dataset of sequential or time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Try it out on your own dataset to know for sure! With that, we have gone through
    important concepts of RNN, from basic to advanced levels. Let’s summarize the
    chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are a type of neural network that explicitly includes
    inductive biases of sequential data in its structure.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of variations of RNNs exist but all of them maintain the same high-level
    concept for their overall structure. Mainly, they provide varying ways to decide
    which data to learn from and remember along with which data to forget from the
    memory from the remembering stage.
  prefs: []
  type: TYPE_NORMAL
- en: However, do note that a more recent architecture called transformers, which
    will be introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding
    Neural Network Transformers*, demonstrated that recurrence is not needed to achieve
    a good performance on sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are done with RNNs and will dive briefly into the world of autoencoders
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
