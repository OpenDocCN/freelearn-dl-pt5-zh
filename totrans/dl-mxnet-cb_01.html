<html><head></head><body>
<div id="_idContainer012">
<h1 class="c apter-number" id="_idParaDest-15"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.2.1">Up and Running with MXNet</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">MXNet</span></strong><span class="koboSpan" id="kobo.4.1"> is</span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.5.1"> one of the most used deep learning frameworks and is an Apache open source project. </span><span class="koboSpan" id="kobo.5.2">Before 2016, </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">AWS</span></strong><span class="koboSpan" id="kobo.9.1">)’s research efforts did not use a preferred deep learning framework, allowing each team to research and develop according to their choices. </span><span class="koboSpan" id="kobo.9.2">Although some deep learning frameworks have thriving communities, sometimes AWS was not able to fix code bugs at the required speed (among other issues). </span><span class="koboSpan" id="kobo.9.3">To solve these issues, at the end of 2016, AWS announced MXNet as its deep learning framework of choice, investing in internal teams to develop it further. </span><span class="koboSpan" id="kobo.9.4">Research institutions that support MXNet are Intel, Baidu, Microsoft, Carnegie Mellon University, and MIT, among others. </span><span class="koboSpan" id="kobo.9.5">It was co-developed by Carlos Guestrin at Carnegie Mellon University and the University of Washington (along </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">with GraphLab).</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Some of its advantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Imperative/symbolic programming and hybridization (which will be covered in </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapters 1</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.15.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.16.1">9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Support for multiple GPUs and distributed training (which will be covered in </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Chapters 7</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.20.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.21.1">8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Highly optimized for inference production systems (which will be covered in </span><em class="italic"><span class="koboSpan" id="kobo.24.1">Chapters 7</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.25.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.26.1">9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">A large number of pre-trained models on its Model Zoos in the fields of computer vision and natural language processing, among others (covered in </span><em class="italic"><span class="koboSpan" id="kobo.29.1">Chapters 6</span></em><span class="koboSpan" id="kobo.30.1">, </span><em class="italic"><span class="koboSpan" id="kobo.31.1">7</span></em><span class="koboSpan" id="kobo.32.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.34.1">8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.36.1">To start working with MXNet, we need to install the library. </span><span class="koboSpan" id="kobo.36.2">There are several different versions of MXNet available to be installed, and in this chapter, we will cover how to choose the right version. </span><span class="koboSpan" id="kobo.36.3">The most important parameter will be the available hardware we have. </span><span class="koboSpan" id="kobo.36.4">In order to optimize performance, it is always best to maximize the use of our available hardware. </span><span class="koboSpan" id="kobo.36.5">We will compare the usage of a well-known linear algebra library, NumPy, with similar operations in MXNet. </span><span class="koboSpan" id="kobo.36.6">We will then compare the performance of the different MXNet versions </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">versus NumPy.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">MXNet includes its own API for deep learning, Gluon, and moreover, Gluon provides different libraries for computer vision and natural language processing that include pre-trained models and utilities. </span><span class="koboSpan" id="kobo.38.2">These libraries are known as GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">and GluonNLP.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.42.1">Installing MXNet, Gluon, GluonCV, </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">and GluonNLP</span></span></li>
<li><span class="koboSpan" id="kobo.44.1">NumPy and MXNet ND arrays – comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">their performance</span></span></li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.46.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.47.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.48.1">Preface</span></em><span class="koboSpan" id="kobo.49.1">, no other requirements apply to </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01"><span class="No-Break"><span class="koboSpan" id="kobo.53.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.54.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">Furthermore, you can access directly each recipe from Google Colab – for example, use the following for the first recipe of this </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">chapter: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.57.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.58.1">.</span></span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.59.1">Installing MXNet, Gluon, GluonCV, and GluonNLP</span></h1>
<p><span class="koboSpan" id="kobo.60.1">In order to get the maximum performance out of the available software (programming languages) and hardware (CPU and GPU), there are different MXNet library versions available to install. </span><span class="koboSpan" id="kobo.60.2">We shall learn how to install them in </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">this recipe.</span></span></p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.62.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.63.1">Before getting started with the MXNet installation, let us review the different versions available of the software packages that we will use, including MXNet. </span><span class="koboSpan" id="kobo.63.2">The reason we do that is that our hardware configuration must map to the chosen versions of our software packages in order to </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">maximize performance:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Python</span></strong><span class="koboSpan" id="kobo.66.1">: MXNet </span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.67.1">is available for different programming languages – Python, Java, R, and C++, among others. </span><span class="koboSpan" id="kobo.67.2">We will use MXNet for Python, and Python 3.7+ </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">is recommended.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">Jupyter</span></strong><span class="koboSpan" id="kobo.70.1">: Jupyter is</span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.71.1"> an open source web application that provides an easy-to-use interface to show Markdown text, working code, and data visualizations. </span><span class="koboSpan" id="kobo.71.2">It is very useful for understanding deep learning, as we can describe concepts, write the code to run through those concepts, and visualize the results (typically comparing them with the input data). </span><span class="koboSpan" id="kobo.71.3">Jupyter Core 4.5+ </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">is recommended.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.73.1">CPUs and GPUs</span></strong><span class="koboSpan" id="kobo.74.1">: MXNet </span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.75.1">can work with any hardware configuration – that is, any single </span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.76.1">CPU can run MXNet. </span><span class="koboSpan" id="kobo.76.2">However, there are several hardware components that MXNet can leverage to </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">improve performance:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Intel CPUs</span></strong><span class="koboSpan" id="kobo.79.1">: Intel </span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.80.1">developed a library known as </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">Math Kernel Library</span></strong><span class="koboSpan" id="kobo.82.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.83.1">MKL</span></strong><span class="koboSpan" id="kobo.84.1">) for</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.85.1"> optimized math operations. </span><span class="koboSpan" id="kobo.85.2">MXNet has support for this library, and using the optimized version can improve certain operations. </span><span class="koboSpan" id="kobo.85.3">Any modern version of Intel MKL </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">is sufficient.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.87.1">NVIDIA GPUs</span></strong><span class="koboSpan" id="kobo.88.1">: NVIDIA </span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.89.1">developed a library known as </span><strong class="bold"><span class="koboSpan" id="kobo.90.1">Compute Unified Device Architecture (CUDA)</span></strong><span class="koboSpan" id="kobo.91.1"> for </span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.92.1">optimized parallel operations (such as matrix operations, which are very common in deep learning). </span><span class="koboSpan" id="kobo.92.2">MXNet has support for this library, and using the optimized version can dramatically improve large deep learning workloads, such as model training. </span><span class="koboSpan" id="kobo.92.3">CUDA 11.0+ </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">is recommended.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">MXNet version</span></strong><span class="koboSpan" id="kobo.95.1">: At the</span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.96.1"> time of writing, MXNet 1.9.1 is the most up-to-date stable version that has been released. </span><span class="koboSpan" id="kobo.96.2">All the code throughout the book has been verified with this version. </span><span class="koboSpan" id="kobo.96.3">MXNet, and deep learning in general, can be considered a live ongoing project, and therefore, new versions will be released periodically. </span><span class="koboSpan" id="kobo.96.4">These new versions will have improved functionality and new features, but they might also contain breaking changes from previous APIs. </span><span class="koboSpan" id="kobo.96.5">If you are revisiting this book in a few months and a new version has been released with breaking changes, how to install MXNet version 1.8.0 specifically is also </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">described here.</span></span></li>
</ul>
<p class="callout- eading"><span class="koboSpan" id="kobo.98.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.99.1">I have used Google Colab as the platform to run the code described in this book. </span><span class="koboSpan" id="kobo.99.2">At the time of writing, it provides Python 3.10.12, up-to-date Jupyter libraries, Intel CPUs (Xeon @ 2.3 GHz), and NVIDIA GPUs (which can vary: K80s, T4s, P4s, and P100s) with CUDA 11.8 pre-installed. </span><span class="koboSpan" id="kobo.99.3">Therefore, minimal steps are required to install MXNet and get </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">it running.</span></span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.101.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.102.1">Throughout the book, we </span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.103.1">will not only use code extensively but also clarify comments and headings in that code to provide structure, as well as several types of visual information such as images or generated graphs. </span><span class="koboSpan" id="kobo.103.2">For these reasons, we will use Jupyter as the supporting development environment. </span><span class="koboSpan" id="kobo.103.3">Moreover, in order to facilitate setup, installation, and experimentation, we will use </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">Google Colab.</span></span></p>
<p><span class="koboSpan" id="kobo.105.1">Google Colab is a </span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.106.1">hosted Jupyter Notebook service that requires no setup to use, while providing free access to computing resources, including GPUs. </span><span class="koboSpan" id="kobo.106.2">In order to set up Google Colab properly, this section is divided into two </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">main points:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.108.1">Setting up </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">the notebook</span></span></li>
<li><span class="koboSpan" id="kobo.110.1">Verifying and </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">installing libraries</span></span></li>
</ul>
<p class="callout- eading"><span class="koboSpan" id="kobo.112.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.113.1">If you prefer, you can use any local environment that supports Python 3.7+, such as Anaconda, or any other Python distribution. </span><span class="koboSpan" id="kobo.113.2">This is highly encouraged if your hardware specifications are better than Google Colab’s offering, as better hardware will reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">computation time.</span></span></p>
<h3><span class="koboSpan" id="kobo.115.1">Setting up the notebook</span></h3>
<p><span class="koboSpan" id="kobo.116.1">In this section, we</span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.117.1"> will learn how to work with Google Colab and set up a new notebook, which we will use to verify our </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">MXNet installation:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.119.1">Open your favorite web browser. </span><span class="koboSpan" id="kobo.119.2">In my case, I have used Google Chrome as the web browser throughout the book. </span><span class="koboSpan" id="kobo.119.3">Visit </span><a href="https://colab.research.google.com/"><span class="koboSpan" id="kobo.120.1">https://colab.research.google.com/</span></a><span class="koboSpan" id="kobo.121.1"> and click on </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.122.1">NEW NOTEBOOK</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer007">
<span class="koboSpan" id="kobo.124.1"><img alt="Figure 1.1 – The Google Colab start screen" src="image/B16591_01_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.125.1">Figure 1.1 – The Google Colab start screen</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.126.1">Change the title of the notebook – for example, as you can see in the following screenshot, I have changed the title to </span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">DL with MXNet Cookbook 1.1 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">Installing MXNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer008">
<span class="koboSpan" id="kobo.130.1"><img alt="Figure 1.2 – A Google Colab notebook" src="image/B16591_01_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.131.1">Figure 1.2 – A Google Colab notebook</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.132.1">Change </span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.133.1">your Google Colab runtime type to use </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">a GPU:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.135.1">Select </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">Change runtime type</span></strong><span class="koboSpan" id="kobo.137.1"> from the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.138.1">Runtime</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.139.1"> menu.</span></span></li></ol></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer009">
<span class="koboSpan" id="kobo.140.1"><img alt="Figure 1.3 – Change runtime type" src="image/B16591_01_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.141.1">Figure 1.3 – Change runtime type</span></p>
<ol>
<li class="upper-roman" value="2"><span class="koboSpan" id="kobo.142.1">In </span><strong class="bold"><span class="koboSpan" id="kobo.143.1">Notebook settings</span></strong><span class="koboSpan" id="kobo.144.1">, select </span><strong class="bold"><span class="koboSpan" id="kobo.145.1">GPU</span></strong><span class="koboSpan" id="kobo.146.1"> as the </span><strong class="bold"><span class="koboSpan" id="kobo.147.1">Hardware </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.148.1">accelerator</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1"> option.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer010">
<span class="koboSpan" id="kobo.150.1"><img alt="Figure 1.4 – Hardware accelerator | GPU" src="image/B16591_01_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.151.1">Figure 1.4 – Hardware accelerator | GPU</span></p>
<h3><span class="koboSpan" id="kobo.152.1">Verifying and installing libraries</span></h3>
<p><span class="koboSpan" id="kobo.153.1">In this</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.154.1"> section, go to the first cell (make sure it is a code cell) and type the </span><a id="_idIndexMarker015"/><span class="No-Break"><span class="koboSpan" id="kobo.155.1">following commands:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.156.1">Verify the Python version by typing </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.158.1">
import platform
platform.python_version()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.159.1">This will yield an output </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">as follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.161.1">3.7.10</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.162.1">Check the version, and make sure that it </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">is 3.7+.</span></span></p></li> </ol>
<p class="callout- eading"><span class="koboSpan" id="kobo.164.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.165.1">In Google Colab, you can directly run commands as if you were in the Linux Terminal by adding the </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">!</span></strong><span class="koboSpan" id="kobo.167.1"> character to the command. </span><span class="koboSpan" id="kobo.167.2">Feel free to try other commands such </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">!ls</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.171.1">We now need to verify the Jupyter version (Jupyter Core 4.5.0 or above </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">will suffice):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.173.1">
!jupyter --version</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.174.1">This is one </span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.175.1">potential output from the </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">previous command:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.177.1">jupyter core : 4.5.0
jupyter-notebook : 5.2.2
qtconsole : 4.5.2
ipython : 5.5.0
ipykernel : 4.10.1
jupyter client : 5.3.1
jupyter lab : not installed
nbconvert : 5.5.0
ipywidgets : 7.5.0
nbformat : 4.4.0
traitlets : 4.3.2</span></pre></li> </ol>
<p class="callout- eading"><span class="koboSpan" id="kobo.178.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.179.1">Jupyter, an open source notebook application, is assumed to be installed, as is the case for Google Colab. </span><span class="koboSpan" id="kobo.179.2">For further instructions on how to install it, </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">visit </span></span><a href="https://jupyter.org/install"><span class="No-Break"><span class="koboSpan" id="kobo.181.1">https://jupyter.org/install</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.182.1">.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.183.1">Verify </span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.184.1">whether an Intel CPU is present in </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">the hardware:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.186.1">
!lscpu | grep 'Model name'</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.187.1">This will yield a similar output to </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">the following:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.189.1">Model name: Intel(R) Xeon(R) CPU @ 2.20GHz</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.190.1">The more up to date the processor the better, but for the purposes of this book, the dependency is larger with the GPU than with </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">the CPU.</span></span></p></li> <li><span class="koboSpan" id="kobo.192.1">Verify the</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.193.1"> NVIDIA GPU is present in the hardware (there are devices listed below) and that NVIDIA CUDA </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">is installed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.195.1">
!nvidia-smi</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.196.1">This will </span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.197.1">yield a similar output to </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">the following:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.199.1">+-----------------------------------------------------------------+
| NVIDIA-SMI 460.67 Driver Version: 460.32.03 CUDA Version: 11.2  |
|---------------------------+--------------+----------------------+
|GPU Name      Persistence-M|Bus-Id  Disp.A| Volatile Uncorr. </span><span class="koboSpan" id="kobo.199.2">ECC |
|Fan Temp Perf Pwr:Usage/Cap|  Memory-Usage|  GPU-Util Compute M. </span><span class="koboSpan" id="kobo.199.3">|
|                           |              |               MIG M. </span><span class="koboSpan" id="kobo.199.4">| |===========================+==============+======================|
|   0 Tesla T4          Off |0:00:04.0 Off |                    0 |
| N/A  37C  P8     9W / 70W |0MiB/15109MiB |      0%      Default |
                |              |                  N/A |
+---------------------------+--------------+----------------------+
+-----------------------------------------------------------------+
| Processes:                                                      |
|  GPU    GI  CI      PID  Type  Process  name         GPU Memory |
      ID  ID                                       Usage      | |=================================================================|
| No running processes found                                      |
+-----------------------------------------------------------------+</span></pre></li> </ol>
<p class="callout- eading"><span class="koboSpan" id="kobo.200.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.201.1">CUDA 11.0 has known issues with the NVIDIA K80. </span><span class="koboSpan" id="kobo.201.2">If you have an NVIDIA K80 and are having issues with the examples described, uninstall CUDA 11.0 and install CUDA 10.2. </span><span class="koboSpan" id="kobo.201.3">Afterward, install MXNet for CUDA 10.2 following the steps </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">described here.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.203.1">Verify </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.204.1">that the CUDA version is 11.0 </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">or above:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.206.1">
!nvcc --version</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.207.1">This </span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.208.1">will yield a similar output to </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">the following:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.210.1">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Wed_Jul_22_19:09:09_PDT_2020
Cuda compilation tools, release 11.0, V11.0.221
Build cuda_11.0_bu.TC445_37.28845127_0</span></pre></li> <li><span class="koboSpan" id="kobo.211.1">Install MXNet, depending on your hardware configuration. </span><span class="koboSpan" id="kobo.211.2">The following are the different MXNet versions that you </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">can install:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Recommended/Google Colab</span></strong><span class="koboSpan" id="kobo.214.1">: The latest MXNet version (1.9.1) with </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">GPU support:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.216.1">
!python3 -m pip install mxnet-cu117</span></pre></li><li><strong class="bold"><span class="koboSpan" id="kobo.217.1">No Intel CPU nor NVIDIA GPU</span></strong><span class="koboSpan" id="kobo.218.1">: Install MXNet with the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.220.1">!python3 -m pip install mxnet</span></pre></li><li><strong class="bold"><span class="koboSpan" id="kobo.221.1">Intel CPU without NVIDIA GPU</span></strong><span class="koboSpan" id="kobo.222.1">: Install MXNet with Intel MKL, with the </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.224.1">!python3 -m pip install mxnet-mkl</span></pre></li><li><strong class="bold"><span class="koboSpan" id="kobo.225.1">No Intel CPU with NVIDIA GPU</span></strong><span class="koboSpan" id="kobo.226.1">: Install MXNet with NVIDIA CUDA 10.2, with the </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.228.1">!python3 -m pip install mxnet-cu102</span></pre></li><li><strong class="bold"><span class="koboSpan" id="kobo.229.1">Intel CPU and NVIDIA GPU</span></strong><span class="koboSpan" id="kobo.230.1">: Install MXNet with Intel MKL and NVIDIA CUDA 11.0, with </span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.231.1">the</span><a id="_idIndexMarker023"/> <span class="No-Break"><span class="koboSpan" id="kobo.232.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.233.1">!python3 -m pip install mxnet-cu110</span></pre></li></ul></li> </ol>
<p class="callout- eading"><span class="koboSpan" id="kobo.234.1">Tip</span></p>
<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">pip3</span></strong><span class="koboSpan" id="kobo.236.1">, a Python 3 package manager, is assumed to be installed, as is the case for Google Colab. </span><span class="koboSpan" id="kobo.236.2">If a different installation method for MXNet is preferred, visit </span><a href="https://mxnet.apache.org/versions/master/get_started"><span class="koboSpan" id="kobo.237.1">https://mxnet.apache.org/versions/master/get_started</span></a> <span class="No-Break"><span class="koboSpan" id="kobo.238.1">for instructions.</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.239.1">After version 1.6.0, MXNet is released by default with the Intel MKL library extension; therefore, there is no need to add the </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">mkl</span></strong><span class="koboSpan" id="kobo.241.1"> suffix anymore when installing the most recent versions, as seen previously in the </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">recommended installation.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.243.1">Verify </span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.244.1">that the MXNet installation has been successful with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">two steps:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.246.1">The following commands must not return any error and must successfully display MXNet </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">version 1.9.1:</span></span></li></ol><pre class="source-code"><span class="koboSpan" id="kobo.248.1">
import mxnet
mxnet.__version__</span></pre><ol><li class="upper-roman" value="2"><span class="koboSpan" id="kobo.249.1">The</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.250.1"> list of features that appear in the following contain the </span><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">CUDA</span></strong><span class="koboSpan" id="kobo.252.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">CUDNN</span></strong><span class="koboSpan" id="kobo.254.1">, and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">MKLDNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.256.1"> features:</span></span></li></ol><pre class="source-code"><span class="koboSpan" id="kobo.257.1">features = mxnet.runtime.Features()
print(features)
 print(features.is_enabled('CUDA'))
 print(features.is_enabled('CUDNN'))
 print(features.is_enabled('MKLDNN'))</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.258.1">The output will list all the features and </span><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">True</span></strong><span class="koboSpan" id="kobo.260.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">each one.</span></span></p></li> <li><span class="koboSpan" id="kobo.262.1">Install GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">and GluonNLP:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.264.1">
!python3 -m pip install gluoncv gluonnlp</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.265.1">This command will install the latest versions of GluonCV and GluonNLP, which at the time of writing were, respectively, 0.10 </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">and 0.10.</span></span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.267.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.268.1">The training, inference, and evaluation of deep learning networks are highly complex operations, involving hardware and several layers of software, including drivers, low-level performance libraries such as MKL and CUDA, and high-level programming languages and libraries such as Python </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">and MXNet.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.270.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.271.1">MXNet is an actively developed project, part of the Apache Incubator program. </span><span class="koboSpan" id="kobo.271.2">Therefore, new versions are expected to be released, and they might contain breaking changes. </span><span class="koboSpan" id="kobo.271.3">The preceding command will install the latest stable version available. </span><span class="koboSpan" id="kobo.271.4">Throughout this book, the version of MXNet used is 1.9.1. </span><span class="koboSpan" id="kobo.271.5">If your code fails and it uses a different MXNet version, try installing MXNet version 1.9.1 by running </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">the following:</span></span></p>
<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">!python3 -m pip </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">install mxnet-cu117==1.9.1</span></strong></span></p>
<p><span class="koboSpan" id="kobo.275.1">By checking </span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.276.1">all the hardware and software components, we can install the most optimized version of MXNet. </span><span class="koboSpan" id="kobo.276.2">We can use Google Colab, which easily transfers to other local configurations such as the </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">Anaconda distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Moreover, we can identify the right combination of CUDA drivers and MXNet versions that will maximize performance and verify a </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">successful installation.</span></span></p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.280.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.281.1">It is highly recommended to always use the latest versions of all the software components discussed. </span><span class="koboSpan" id="kobo.281.2">Deep learning is an evolving field and there are always improvements such as new functionalities added, changes in the APIs, and updates in the internal functions to increase performance, among </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">other changes.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">However, it is very important that all components (CPU, GPU, CUDA, and the MXNet version) are compatible. </span><span class="koboSpan" id="kobo.283.2">To match these components, it is highly recommended to visit </span><a href="https://mxnet.apache.org/versions/master/get_started"><span class="koboSpan" id="kobo.284.1">https://mxnet.apache.org/versions/master/get_started</span></a><span class="koboSpan" id="kobo.285.1"> and check for the latest CUDA and MXNet versions you can install to maximize your </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">hardware performance.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">As an example, for a Python 3-based Linux distribution, installed using </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">pip3</span></strong><span class="koboSpan" id="kobo.289.1">, these are the MXNet versions available (note with/without CPU acceleration and/or with </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">GPU acceleration).</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">If you are interested in knowing more </span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.292.1">about Intel’s </span><strong class="bold"><span class="koboSpan" id="kobo.293.1">MKL</span></strong><span class="koboSpan" id="kobo.294.1">, the following link is a very good starting </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">point: </span></span><a href="https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html"><span class="No-Break"><span class="koboSpan" id="kobo.296.1">https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.297.1">.</span></span></p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.298.1">NumPy and MXNet ND arrays</span></h1>
<p><span class="koboSpan" id="kobo.299.1">If you have worked </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.300.1">with data previously in Python, chances are you have found yourself working with NumPy and its </span><strong class="bold"><span class="koboSpan" id="kobo.301.1">N-dimensional arrays</span></strong><span class="koboSpan" id="kobo.302.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.303.1">ND arrays</span></strong><span class="koboSpan" id="kobo.304.1">). </span><span class="koboSpan" id="kobo.304.2">These </span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.305.1">are also known as tensors, and the 0D variants are</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.306.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.307.1">scalars</span></strong><span class="koboSpan" id="kobo.308.1">, the 1D </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.309.1">variants are called </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">vectors</span></strong><span class="koboSpan" id="kobo.311.1">, and the 2D variants </span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.312.1">are </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.314.1">matrixes</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">MXNet provides</span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.317.1"> its own ND array type, and there are two different ways to work with them. </span><span class="koboSpan" id="kobo.317.2">On one hand, there is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.318.1">nd</span></strong><span class="koboSpan" id="kobo.319.1"> module, MXNet’s native and optimized way to work with MXNet ND arrays. </span><span class="koboSpan" id="kobo.319.2">On the other hand, there is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">np</span></strong><span class="koboSpan" id="kobo.321.1"> module, which has the same interfaces and syntax as the NumPy ND array type and has also been optimized, but it’s limited due to the interface constraints. </span><span class="koboSpan" id="kobo.321.2">With MXNet ND arrays, we can leverage its underlying engine, with compute optimizations such as Intel MKL and/or NVIDIA CUDA, if our hardware configuration is compatible. </span><span class="koboSpan" id="kobo.321.3">This means we will be able to use almost the same syntax as when working with NumPy, but accelerated with the MXNet engine and our GPUs, not supported </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">by NumPy.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">Moreover, as we will see in the next chapters, a very common operation that we will execute on MXNet is automatic differentiation on these ND arrays. </span><span class="koboSpan" id="kobo.323.2">By using MXNet ND array libraries, this operation will also leverage our hardware for optimum performance. </span><span class="koboSpan" id="kobo.323.3">NumPy does not provide automatic differentiation out of </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">the box.</span></span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.325.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.326.1">If you have already installed MXNet, as described in the previous recipe, in terms of executing accelerated code, the only remaining steps before using MXNet ND arrays is importing </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">their libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.328.1">
import numpy as np
import mxnet as mx</span></pre> <p><span class="koboSpan" id="kobo.329.1">However, it is worth noting here an important underlying difference between NumPy ND array operations and MXNet ND array operations. </span><span class="koboSpan" id="kobo.329.2">NumPy follows an eager evaluation strategy – that is, all operations are evaluated at the moment of execution. </span><span class="koboSpan" id="kobo.329.3">Conversely, MXNet uses a lazy evaluation strategy, more optimal for large compute loads, where the actual calculation is deferred until the values are </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">actually needed.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">Therefore, when comparing performances, we will need to force MXNet to finalize all calculations before computing the time needed for them. </span><span class="koboSpan" id="kobo.331.2">As we will see in the examples, this is achieved by calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.332.1">wait_to_read()</span></strong><span class="koboSpan" id="kobo.333.1"> function, Furthermore, when accessing the data with functions such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.334.1">print()</span></strong><span class="koboSpan" id="kobo.335.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">.asnumpy()</span></strong><span class="koboSpan" id="kobo.337.1">, execution is then completed before calling these functions, yielding the wrong impression that these functions</span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.338.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">actually time-consuming:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.340.1">Let’s check a specific example and start by running it on </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">the CPU:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.342.1">
import time
x_mx_cpu = mx.np.random.rand(1000, 1000, ctx = mx.cpu())
start_time = time.time()
mx.np.dot(x_mx_cpu, x_mx_cpu).wait_to_read()
print("Time of the operation: ", time.time() - start_time)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.343.1">This will yield a similar output to </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">the following:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.345.1">Time of the operation: 0.04673886299133301</span></pre></li> <li><span class="koboSpan" id="kobo.346.1">However, let’s see what happens if we measure the time without the call </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">wait_to_read()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.350.1">
x_mx_cpu = mx.np.random.rand(1000, 1000, ctx = mx.cpu())
start_time = time.time()
x_2 = mx.np.dot(x_mx_cpu, x_mx_cpu)
 print("(FAKE, MXNet has lazy evaluation)")
 print("Time of the operation : ", time.time() - start_time)
 start_time = time.time()
print(x_2)
 print("(FAKE, MXNet has lazy evaluation)")
 print("Time to display: ", time.time() - start_time)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.351.1">The following will be </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">the output:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.353.1">(FAKE, MXNet has lazy evaluation)
 Time of the operation : 0.00118255615234375
 [[256.59583 249.70404 249.48639 ... </span><span class="koboSpan" id="kobo.353.2">251.97151 255.06744 255.60669]
 [255.22629 251.69475 245.7591 ... </span><span class="koboSpan" id="kobo.353.3">252.78784 253.18878 247.78052]
 [257.54187 254.29262 251.76346 ... </span><span class="koboSpan" id="kobo.353.4">261.0468 268.49127 258.2312 ]
 ...
 </span><span class="koboSpan" id="kobo.353.5">[256.9957 253.9823 249.59073 ... </span><span class="koboSpan" id="kobo.353.6">256.7088 261.14255 253.37457]
 [255.94278 248.73282 248.16641 ... </span><span class="koboSpan" id="kobo.353.7">254.39209 252.4108 249.02774]
 [253.3464 254.55524 250.00716 ... </span><span class="koboSpan" id="kobo.353.8">253.15712 258.53894 255.18658]]
 (FAKE, MXNet has lazy evaluation)
 Time to display: 0.042133331298828125</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.354.1">As we can see, the </span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.355.1">first experiment indicated that the computation took ~50 ms to complete; however, the second experiment indicated that the computation took ~1 ms (50 times less!), and the visualization was more than 40 ms. </span><span class="koboSpan" id="kobo.355.2">This is an incorrect result. </span><span class="koboSpan" id="kobo.355.3">This is because we measured our performance incorrectly in the second experiment. </span><span class="koboSpan" id="kobo.355.4">Refer to the first experiment and the call to </span><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">wait_to_read()</span></strong><span class="koboSpan" id="kobo.357.1"> for a proper </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">performance measurement.</span></span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.359.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.360.1">In this section, we will</span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.361.1"> compare performance in terms of computation time for two </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">compute-intensive operations:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.363.1">Matrix creation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.364.1">Matrix multiplication</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.365.1">We will compare five different compute profiles for </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">each operation:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.367.1">Using the NumPy library (no CPU or </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">GPU acceleration)</span></span></li>
<li><span class="koboSpan" id="kobo.369.1">Using the MXNet </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">np</span></strong><span class="koboSpan" id="kobo.371.1"> module with CPU acceleration but </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">no GPU</span></span></li>
<li><span class="koboSpan" id="kobo.373.1">Using the MXNet </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">np</span></strong><span class="koboSpan" id="kobo.375.1"> module with CPU acceleration and </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">GPU acceleration</span></span></li>
<li><span class="koboSpan" id="kobo.377.1">Using the MXNet </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">nd</span></strong><span class="koboSpan" id="kobo.379.1"> module with CPU acceleration but </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">no GPU</span></span></li>
<li><span class="koboSpan" id="kobo.381.1">Using the MXNet </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">nd</span></strong><span class="koboSpan" id="kobo.383.1"> module with CPU acceleration and </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">GPU acceleration</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.385.1">To finalize, we </span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.386.1">will plot the results and draw </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">some conclusions.</span></span></p>
<h3><span class="koboSpan" id="kobo.388.1">Timing data structures</span></h3>
<p><span class="koboSpan" id="kobo.389.1">We will store the </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.390.1">computation time in five dictionaries, one for each compute profile (</span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">timings_np</span></strong><span class="koboSpan" id="kobo.392.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">timings_mx_cpu</span></strong><span class="koboSpan" id="kobo.394.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">timings_mx_gpu</span></strong><span class="koboSpan" id="kobo.396.1">). </span><span class="koboSpan" id="kobo.396.2">The initialization of the data structures is </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.398.1">
timings_np = {}
timings_mx_np_cpu = {}
timings_mx_np_gpu = {}
timings_mx_nd_cpu = {}
timings_mx_nd_gpu = {}</span></pre> <p><span class="koboSpan" id="kobo.399.1">We will run each operation (matrix generation and matrix multiplication) with matrixes in a different order, namely </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.401.1">
matrix_orders = [1, 5, 10, 50, 100, 500, 1000, 5000, 10000]</span></pre> <h3><span class="koboSpan" id="kobo.402.1">Matrix creation</span></h3>
<p><span class="koboSpan" id="kobo.403.1">We define three functions to generate matrixes; the first function will use the NumPy library to generate a matrix, and it will receive as an input parameter the matrix order. </span><span class="koboSpan" id="kobo.403.2">The second function will use the MXNet np module, and the third function will use the MXNet and module. </span><span class="koboSpan" id="kobo.403.3">For the second and third functions, as input parameters we will provide the context where the matrix needs to be created, apart from the matrix order. </span><span class="koboSpan" id="kobo.403.4">This context specifies whether the result (the created matrix in this case) must be computed in the </span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.404.1">CPU or the GPU (and which GPU if there are multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">devices available):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.406.1">
def create_matrix_np(n):
    """
    Given n, creates a squared n x n matrix,
    with each matrix value taken from a random
    uniform distribution between [0, 1].
</span><span class="koboSpan" id="kobo.406.2">    Returns the created matrix a.
</span><span class="koboSpan" id="kobo.406.3">    Uses NumPy.
</span><span class="koboSpan" id="kobo.406.4">    """
    a = np.random.rand(n, n)
    return a
def create_matrix_mx(n, ctx=mx.cpu()):
    """
    Given n, creates a squared n x n matrix,
    with each matrix value taken from a random
    uniform distribution between [0, 1].
</span><span class="koboSpan" id="kobo.406.5">    Returns the created matrix a.
</span><span class="koboSpan" id="kobo.406.6">    Uses MXNet NumPy syntax and context ctx
    """
    a = mx.np.random.rand(n, n, ctx=ctx)
    a.wait_to_read()
    return a
def create_matrix_mx_nd(n, ctx=mx.cpu()):
    """
    Given n, creates a squared n x n matrix,
    with each matrix value taken from a random
    uniform distribution between [0, 1].
</span><span class="koboSpan" id="kobo.406.7">    Returns the created matrix a.
</span><span class="koboSpan" id="kobo.406.8">    Uses MXNet ND native syntax and context ctx
    """
    a = mx.nd.random.uniform(shape=(n, n), ctx=ctx)
    a.wait_to_read()
    return a</span></pre> <p><span class="koboSpan" id="kobo.407.1">To store</span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.408.1"> necessary data for our performance comparison later, we use the structures created previously, with the </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.410.1">
timings_np["create"] = []
for n in matrix_orders:
    result = %timeit -o create_matrix_np(n)
    timings_np["create"].append(result.best)
timings_mx_np_cpu["create"] = []
for n in matrix_orders:
    result = %timeit -o create_matrix_mx_np(n)
    timings_mx_np_cpu["create"].append(result.best)
timings_mx_np_gpu["create"] = []
ctx = mx.gpu()
for n in matrix_orders:
    result = %timeit -o create_matrix_mx_np(n, ctx)
    timings_mx_np_gpu["create"].append(result.best)
timings_mx_nd_cpu["create"] = []
for n in matrix_orders:
    result = %timeit -o create_matrix_mx_nd(n)
    timings_mx_nd_cpu["create"].append(result.best)
timings_mx_nd_gpu["create"] = []
ctx = mx.gpu()
for n in matrix_orders:
    result = %timeit -o create_matrix_mx_nd(n, ctx)
    timings_mx_nd_gpu["create"].append(result.best)</span></pre> <h3><span class="koboSpan" id="kobo.411.1">Matrix multiplication</span></h3>
<p><span class="koboSpan" id="kobo.412.1">We define </span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.413.1">three functions to compute the matrixes multiplication; the first function will use the NumPy library and will receive as input parameters the matrixes to multiply. </span><span class="koboSpan" id="kobo.413.2">The second function will use the MXNet np module, and the third function will use the MXNet nd module. </span><span class="koboSpan" id="kobo.413.3">For the second and third functions, the same parameters are used. </span><span class="koboSpan" id="kobo.413.4">The context where the multiplication will happen is given by the context where the matrixes were created; no parameter needs to be added. </span><span class="koboSpan" id="kobo.413.5">Both matrixes need to have been created in the same context, or an error will </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">be triggered:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.415.1">
def multiply_matrix_np(a, b):
    """
    Multiplies 2 squared matrixes a and b
    and returns the result c.
</span><span class="koboSpan" id="kobo.415.2">    Uses NumPy.
</span><span class="koboSpan" id="kobo.415.3">    """
    #c = np.matmul(a, b)
    c = np.dot(a, b)
    return c
def multiply_matrix_mx_np(a, b):
    """
    Multiplies 2 squared matrixes a and b
    and returns the result c.
</span><span class="koboSpan" id="kobo.415.4">    Uses MXNet NumPy syntax.
</span><span class="koboSpan" id="kobo.415.5">    """
    c = mx.np.dot(a, b)
    c.wait_to_read()
    return c
def multiply_matrix_mx_nd(a, b):
    """
    Multiplies 2 squared matrixes a and b
    and returns the result c.
</span><span class="koboSpan" id="kobo.415.6">    Uses MXNet ND native syntax.
</span><span class="koboSpan" id="kobo.415.7">    """
    c = mx.nd.dot(a, b)
    c.wait_to_read()
    return c</span></pre> <p><span class="koboSpan" id="kobo.416.1">To store</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.417.1"> the necessary data for our performance comparison later, we will use the structures created previously, with the </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.419.1">
timings_np["multiply"] = []
for n in matrix_orders:
    a = create_matrix_np(n)
    b = create_matrix_np(n)
    result = %timeit -o multiply_matrix_np(a, b)
    timings_np["multiply"].append(result.best)
timings_mx_np_cpu["multiply"] = []
for n in matrix_orders:
    a = create_matrix_mx_np(n)
    b = create_matrix_mx_np(n)
    result = %timeit -o multiply_matrix_mx_np(a, b)
    timings_mx_np_cpu["multiply"].append(result.best)
timings_mx_np_gpu["multiply"] = []
ctx = mx.gpu()
for n in matrix_orders:
    a = create_matrix_mx_np(n, ctx)
    b = create_matrix_mx_np(n, ctx)
    result = %timeit -o multiply_matrix_mx_np(a, b)
    timings_mx_gpu["multiply"].append(result.best)
timings_mx_nd_cpu["multiply"] = []
for n in matrix_orders:
    a = create_matrix_mx_nd(n)
    b = create_matrix_mx_nd(n)
    result = %timeit -o multiply_matrix_mx_nd(a, b)
    timings_mx_nd_cpu["multiply"].append(result.best)
timings_mx_nd_gpu["multiply"] = []
ctx = mx.gpu()
for n in matrix_orders:
    a = create_matrix_mx_nd(n, ctx)
    b = create_matrix_mx_nd(n, ctx)
    result = %timeit -o multiply_matrix_mx_nd(a, b)
    timings_mx_nd_gpu["multiply"].append(result.best)</span></pre> <h3><span class="koboSpan" id="kobo.420.1">Drawing conclusions</span></h3>
<p><span class="koboSpan" id="kobo.421.1">The first step</span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.422.1"> before making any assessments is to plot the data we have captured in the previous steps. </span><span class="koboSpan" id="kobo.422.2">For this step, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">pyplot</span></strong><span class="koboSpan" id="kobo.424.1"> module from a library called Matplotlib, which will allow us to create charts easily. </span><span class="koboSpan" id="kobo.424.2">The following code plots the runtime (in seconds) for the matrix generation and all the matrix </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">orders computed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.426.1">
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(matrix_orders, timings_np["create"], color='red', marker='s')
plt.plot(matrix_orders, timings_mx_np_cpu["create"], color='blue', marker='o')
plt.plot(matrix_orders, timings_mx_np_gpu["create"], color='green', marker='^')
plt.plot(matrix_orders, timings_mx_nd_cpu["create"], color='yellow', marker='p')
plt.plot(matrix_orders, timings_mx_nd_gpu["create"], color='orange', marker='*')
plt.title("Matrix Creation Runtime", fontsize=14)
plt.xlabel("Matrix Order", fontsize=14)
plt.ylabel("Runtime (s)", fontsize=14)
plt.grid(True)
ax = fig.gca()
ax.set_xscale("log")
ax.set_yscale("log")
plt.legend(["NumPy", "MXNet NumPy (CPU)", "MXNet NumPy (GPU)", "MXNet ND (CPU)", "MXNet ND (GPU)"])
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.427.1">Quite similarly as shown in the previous code block, the</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.428.1"> following code plots the runtime (in seconds) for the matrix multiplication and all the matrix </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">orders computed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.430.1">
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(matrix_orders, timings_np["multiply"], color='red', marker='s')
 plt.plot(matrix_orders, timings_mx_np_cpu["multiply"], color='blue', marker='o')
 plt.plot(matrix_orders, timings_mx_np_gpu["multiply"], color='green', marker='^')
 plt.plot(matrix_orders, timings_mx_nd_cpu["multiply"], color='yellow', marker='p')
 plt.plot(matrix_orders, timings_mx_nd_gpu["multiply"], color='orange', marker='*')
 plt.title("Matrix Multiplication Runtime", fontsize=14)
 plt.xlabel("Matrix Order", fontsize=14)
 plt.ylabel("Runtime (s)", fontsize=14)
 plt.grid(True)
 ax = fig.gca()
ax.set_xscale("log")
ax.set_yscale("log")
plt.legend(["NumPy", "MXNet NumPy (CPU)", "MXNet NumPy (GPU)", "MXNet ND (CPU)", "MXNet ND (GPU)"])
 plt.show()</span></pre> <p><span class="koboSpan" id="kobo.431.1">These are the</span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.432.1"> plots displayed (the results will vary according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">hardware configuration):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<span class="koboSpan" id="kobo.434.1"><img alt="Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication" src="image/B16591_01_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.435.1">Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.436.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.437.1">Note that the charts use a logarithmic scale for both axes, horizontal and vertical (the differences are larger than they seem). </span><span class="koboSpan" id="kobo.437.2">Furthermore, the actual values depend on the hardware architecture that the computations are run on; therefore, your specific results </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">will vary.</span></span></p>
<p><span class="koboSpan" id="kobo.439.1">There are several conclusions that can be drawn, both from each individual operation </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">and collectively:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.441.1">For smaller matrix orders, using NumPy is much faster in both operations. </span><span class="koboSpan" id="kobo.441.2">This is because MXNet works in a different memory space, and the amount of time to move the data to this memory space is longer than the actual </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">compute time.</span></span></li>
<li><span class="koboSpan" id="kobo.443.1">In matrix creation, for larger matrix orders, the difference between NumPy (remember, it’s CPU only) and MXNet with the np module and CPU acceleration is negligible, but with the nd module and CPU, acceleration is ~2x faster. </span><span class="koboSpan" id="kobo.443.2">For matrix multiplication, and depending on your hardware, MXNet with CPU acceleration can be ~2x faster (regardless of the module). </span><span class="koboSpan" id="kobo.443.3">This is because MXNet uses Intel MKL to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">CPU computations.</span></span></li>
<li><span class="koboSpan" id="kobo.445.1">In the ranges that are interesting for deep learning – that is, large computational loads involving matrix orders &gt; 1,000 (which can represent data such as images composed of several megapixels or large language dictionaries), GPUs deliver typical gains of several orders of magnitude (~200x for creation, and ~40x for multiplication, exponentially growing with every increase of matrix order). </span><span class="koboSpan" id="kobo.445.2">This is by far the most compelling reason to work with GPUs when running deep </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">learning experiments.</span></span></li>
<li><span class="koboSpan" id="kobo.447.1">When</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.448.1"> using the GPU, the MXNet np module is faster than the MXNet nd module in creation (~7x), but the difference is negligible in multiplication. </span><span class="koboSpan" id="kobo.448.2">Typically, deep learning algorithms are more similar to multiplications to terms of computational loads, and therefore, a priori, there is no significant advantage in using the np module or the nd module. </span><span class="koboSpan" id="kobo.448.3">However, </span><em class="italic"><span class="koboSpan" id="kobo.449.1">MXNet recommends using the native MXNet nd module</span></em><span class="koboSpan" id="kobo.450.1"> (and the author subscribes to this recommendation) because some operations on the np module are not supported by </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">autograd</span></strong><span class="koboSpan" id="kobo.452.1"> (MXNet’s auto-differentiation module). </span><span class="koboSpan" id="kobo.452.2">We will see in the upcoming chapters, when we train neural networks, how the </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">autograd</span></strong><span class="koboSpan" id="kobo.454.1"> module is used and why it </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">is critical.</span></span></li>
</ul>
<h2 id="_idParaDest-26"><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.456.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.457.1">MXNet provides</span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.458.1"> two optimized modules to work with ND arrays, including one that is an in-place substitute for NumPy. </span><span class="koboSpan" id="kobo.458.2">The advantages of operating with MXNet ND arrays </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">are twofold:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.460.1">MXNet ND array operations support automatic differentiation. </span><span class="koboSpan" id="kobo.460.2">As we will see in the following chapters, automatic differentiation is a key feature that allows developers to concentrate on the forward pass of the models, letting the backward pass be </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">automatically derived.</span></span></li>
<li><span class="koboSpan" id="kobo.462.1">Conversely, operations with MXNet ND arrays are optimized for the underlying hardware, yielding impressive results with GPU acceleration. </span><span class="koboSpan" id="kobo.462.2">We computed results for matrix creation and matrix multiplication to validate this </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">conclusion experimentally.</span></span></li>
</ul>
<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.464.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.465.1">In this recipe, we have barely scratched the surface of MXNet operations with ND arrays. </span><span class="koboSpan" id="kobo.465.2">If you want to read more about MXNet and ND arrays, this is the link to the official MXNet API </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">reference: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">https://mxnet.apache.org/versions/1.0.0/api/python/ndarray/ndarray.html</span></span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.469.1">Furthermore, a very interesting tutorial can be found in the official MXNet </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">documentation: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">https://gluon.mxnet.io/chapter01_crashcourse/ndarray.html</span></span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.473.1">Moreover, we have taken a glimpse at how to measure performance on MXNet. </span><span class="koboSpan" id="kobo.473.2">We will revisit this topic in the following chapters; however, a good deep-dive into the topic is given in the official MXNet </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">documentation: </span></span><a href="https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html"><span class="No-Break"><span class="koboSpan" id="kobo.475.1">https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.476.1">.</span></span></p>
</div>
</body></html>