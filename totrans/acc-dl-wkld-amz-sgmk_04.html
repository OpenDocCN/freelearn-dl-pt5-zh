<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer023">
<h1 class="chapter-number" id="_idParaDest-70"><a id="_idTextAnchor067"/>4</h1>
<h1 id="_idParaDest-71"><a id="_idTextAnchor068"/>Managing Deep Learning Datasets</h1>
<p><strong class="bold">Deep learning</strong> models usually require a considerable amount of training data to learn useful patterns. In many real-life applications, new data is continuously collected, processed, and added to the training dataset, so your models can be periodically retrained so that they can adjust to changing real-world conditions. In this chapter, we will look into SageMaker capabilities and other AWS services to help you manage your training data.</p>
<p>SageMaker provides a wide integration capability where you can use AWS general-purpose data storage services such as Amazon S3, Amazon EFS, and Amazon FSx for Lustre. Additionally, SageMaker has purpose-built storage for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) called SageMaker Feature Store. We will discuss when to choose one or another storage solution, depending on the type of data, consumption, and ingestion patterns.</p>
<p>In many cases, before you can use training data, you need to pre-process it. For instance, data needs to be converted into a specific format or datasets need to be augmented with modified versions of samples. In this chapter, we will review SageMaker Processing and how it can be used to process large-scale ML datasets.</p>
<p>We will close this chapter by looking at advanced techniques on how to optimize the data retrieval process for TensorFlow and PyTorch models using AWS data streaming utilities.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Selecting storage solutions for ML datasets</li>
<li>Processing data at scale</li>
<li>Optimizing data storage and retrieval</li>
</ul>
<p>After reading this chapter, you will know how to organize your DL dataset’s life cycle for training and inference on SageMaker. We will also run through some hands-on examples for data processing and data retrieval to help you gain practical skills in those areas.</p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor069"/>Technical requirements</h1>
<p>In this chapter, we will provide code samples so that you can develop your practical skills. The full code examples are available at <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/</a>.</p>
<p>To follow this code, you will need the following:</p>
<ul>
<li>An AWS account and IAM user with permission to manage Amazon SageMaker resources</li>
<li>A SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible environment established</li>
</ul>
<h1 id="_idParaDest-73"><a id="_idTextAnchor070"/>Selecting storage solutions for ML datasets</h1>
<p>AWS Cloud <a id="_idIndexMarker239"/>provides a wide range of storage<a id="_idIndexMarker240"/> solutions that can be used to store inference and training data. When choosing an optimal storage solution, you may consider the following factors:</p>
<ul>
<li>Data volume and velocity</li>
<li>Data types and associated metadata</li>
<li>Consumption patterns</li>
<li>Backup and retention requirements</li>
<li>Security and audit requirements</li>
<li>Integration capabilities</li>
<li>Price to store, write, and read data</li>
</ul>
<p>Carefully analyzing your specific requirements may suggest the right solution for your use case. It’s also typical to combine several storage solutions for different stages of your data life cycle. For instance, you could store data used for inference consumption with lower latency requirements in faster but more expensive storage; then, you could move the data to cheaper and slower storage solutions for training purposes and long-term retention.</p>
<p>There are several types of common storage types with different characteristics: filesystems, object storage and block storage solutions. Amazon provides managed services for each type of storage. We will review their characteristics and how to use them in your SageMaker workloads in the following subsections. Then, we will focus on Amazon SageMaker <a id="_idIndexMarker241"/>Feature<a id="_idIndexMarker242"/> Store as it provides several unique features specific to ML workloads and datasets.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor071"/>Amazon EBS – high-performance block storage</h2>
<p>Block storage <a id="_idIndexMarker243"/>solutions are designed for quick data retrieval and manipulation. The data is broken into blocks on the physical device for efficient utilization. Block storage allows you to abstract and decouple data from the runtime environment. At data retrieval time, blocks are reassembled by the storage solution and returned to users. </p>
<p>Amazon EBS is a fully managed block storage solution that supports a wide range of use cases for different read-write patterns, throughput, and latency requirements. A primary use case of Amazon EBS is to serve as data volumes attached to Amazon EC2 compute nodes.</p>
<p>Amazon SageMaker provides seamless integration with EBS. In the following example, we are provisioning a training job with four nodes; each node will have an EBS volume with 100 GB attached to it. The training data will be downloaded and stored on EBS volumes by SageMaker:</p>
<pre class="source-code">
from sagemaker.pytorch import PyTorch
pytorch_estimator = PyTorch(
                        session=session,
                        entry_point=f'path/to/train.py',
                        role=role,
                        instance_type="ml.m4.xlarge",
  volume_size=100,
                        instance_count=4,
                        framework_version="1.9.0",
                        )</pre>
<p>Note that you cannot customize the type of EBS volume that’s used. Only <strong class="bold">general-purpose SSD</strong> volumes<a id="_idIndexMarker244"/> are supported. Once the training job is completed, all instances and attached<a id="_idIndexMarker245"/> EBS volumes will be purged.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor072"/>Amazon S3 – industry-standard object storage</h2>
<p>Object storage <a id="_idIndexMarker246"/>implements a flat structure, where each file object has a unique identifier (expressed as a path) and associated data object. A flat structure allows you to scale object storage solutions linearly and sustain high-throughput data reads and writes while keeping costs low. </p>
<p>Object storage can handle objects of different types and sizes. Object storage also allows you to store metadata associated with each object. Data reads and writes are typically done via HTTP APIs, which allows for ease of integration. However, note that object storage solutions are generally slower than filesystems or block storage solutions.</p>
<p>Amazon S3 was the first petabyte-scale cloud object storage service. It offers durability, availability, performance, security, and virtually unlimited scalability at very low costs. Many object storage solutions follow the Amazon S3 API. Amazon S3 is used to store customer data, but it’s also used for many internal AWS features and services where data needs to be persisted.</p>
<p>SageMaker provides seamless integration with Amazon S3 for storing input and output objects, such as datasets, log streams, job outputs, and model artifacts. Let’s look at an example of a training job and how to define where we will store our inputs and outputs:</p>
<ul>
<li>The <strong class="source-inline">model_uri</strong> parameter specifies an S3 location of model artifacts, such as pre-trained weights, tokenizers, and others. SageMaker automatically downloads these artifacts to each training node.</li>
<li><strong class="source-inline">checkpoint_s3_uri</strong> defines the S3 location where training checkpoints will be uploaded during training. Note that it is the developer’s responsibility to implement checkpointing functionality in the training script.</li>
<li><strong class="source-inline">output_path</strong> specifies the S3 destination of all output artifacts that SageMaker will upload from training nodes after training job completion. </li>
<li><strong class="source-inline">tensorboard_output_config</strong> defines where to store TensorBoard logs on S3. Note that SageMaker continuously uploads these logs during training job execution, so you<a id="_idIndexMarker247"/> can monitor your training progress in near real time in TensorBoard:<p class="source-code">from sagemaker.huggingface.estimator import HuggingFace</p><p class="source-code">from sagemaker.debugger import TensorBoardOutputConfig</p><p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">role=get_execution_role()</p><p class="source-code">estimator = HuggingFace(</p><p class="source-code">    py_version="py36",</p><p class="source-code">    entry_point="train.py",</p><p class="source-code">    pytorch_version="1.7.1",</p><p class="source-code">    transformers_version="4.6.1",</p><p class="source-code">    instance_type="ml.p2.xlarge",</p><p class="source-code">    instance_count=2,</p><p class="source-code">    role=role,</p><p class="source-code">    model_uri="s3://unique/path/models/pretrained-bert/",</p><p class="source-code">    checkpoint_s3_uri="s3://unique/path/training/checkpoints",</p><p class="source-code">    output_path="s3://unique/path/training/output",</p><p class="source-code">    tensorboard_output_config = TensorBoardOutputConfig(</p><p class="source-code">        s3_output_path='s3://unique/path/tensorboard/ ',</p><p class="source-code">        container_local_output_path='/local/path/'</p><p class="source-code">        ),</p><p class="source-code">)</p></li>
</ul>
<p>We also use S3 to store our training dataset. Take a look at the following <strong class="source-inline">estimator.fit()</strong> method, which defines the location of our training data:</p>
<p class="source-code">estimator.fit({</p>
<p class="source-code">    "train":"s3://unique/path/train_files/",</p>
<p class="source-code">    "test":"s3://unique/path/test_files"}</p>
<p class="source-code">    )</p>
<p>Here, the <strong class="source-inline">"train"</strong> and <strong class="source-inline">"test"</strong> parameters are<a id="_idIndexMarker248"/> called <strong class="bold">data channels</strong> in SageMaker terminology. Data channel parameters are a way to tell SageMaker to automatically download any required data, such as training and validation datasets, embeddings, lookup tables, and more. SageMaker allows you to use arbitrary names for your data channels. Before you start the training job, the SageMaker training toolkit will download model artifacts to a specific location under the <strong class="source-inline">/opm/ml/input/data/{channel_name}</strong> directory. Additionally, the training toolkit will create <strong class="source-inline">SM_CHANNEL_{channel_name}</strong> environment variables, which you can use in <a id="_idIndexMarker249"/>your training script to access model artifacts locally.</p>
<p>As shown in the preceding code block, Amazon S3 can be used to store the input and output artifacts of SageMaker training jobs.</p>
<h3>File, FastFile, and Pipe modes</h3>
<p>S3 storage is <a id="_idIndexMarker250"/>a common <a id="_idIndexMarker251"/>place<a id="_idIndexMarker252"/> to store your training <a id="_idIndexMarker253"/>datasets. By <a id="_idIndexMarker254"/>default, when<a id="_idIndexMarker255"/> working with data stored on S3, all objects matching the path will be downloaded to each compute node and stored on its EBS volumes. This is known as <strong class="bold">File</strong> mode.</p>
<p>However, in many scenarios, training datasets can be hundreds of gigabytes or larger. Downloading such large files will take a considerable amount of time, even before your training begins. To reduce the time needed to start a training job, SageMaker supports <strong class="bold">Pipe</strong> mode, which allows you to stream data from the S3 location without fully downloading it. This allows you to start training jobs immediately and fetch data batches as needed during the training cycle.</p>
<p>One of the drawbacks of Pipe mode is that it requires using framework-specific implementations of data utilities to stream data. The recently introduced <strong class="bold">FastFile</strong> mode addresses this challenge. FastFile mode allows you to stream data directly from S3 without the need to implement any specific data loaders. In your training or processing scripts, you can treat <strong class="bold">FastFiles</strong> as if they are regular files stored on disk; Amazon SageMaker <a id="_idIndexMarker256"/>will<a id="_idIndexMarker257"/> take <a id="_idIndexMarker258"/>care<a id="_idIndexMarker259"/> of <a id="_idIndexMarker260"/>the <a id="_idIndexMarker261"/>read and write operations for you.</p>
<p>We will develop practical skills on how to organize training code for S3 streaming using <strong class="bold">FastFile</strong> and <strong class="bold">Pipe</strong> modes in the <em class="italic">Optimizing data storage and retrieval</em> section.</p>
<h3>FullyReplicated and ShardedByKey</h3>
<p>In many <a id="_idIndexMarker262"/>training<a id="_idIndexMarker263"/> and data <a id="_idIndexMarker264"/>processing tasks, we want to parallelize our <a id="_idIndexMarker265"/>job across multiple compute nodes. In scenarios where we have many data objects, we can split our tasks by splitting our full set of objects into unique subsets.</p>
<p>To implement such a scenario, SageMaker supports <strong class="bold">ShardedByKey</strong> mode, which attempts to evenly split all matching objects and deliver a unique subset of objects to each node. For instance, if you have <em class="italic">n</em> objects in your dataset and <em class="italic">k</em> compute nodes in your job, then each compute node will get a unique set of <em class="italic">n/k</em> objects.</p>
<p>Unless otherwise specified, the default mode, <strong class="bold">FullyReplicated</strong>, is used when SageMaker downloads all matching objects to all nodes. </p>
<p>We will acquire practical skills on how to distribute data processing tasks in the <em class="italic">Distributed data processing</em> section.</p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor073"/>Amazon EFS – general-purpose shared filesystem</h2>
<p>Amazon EFS is<a id="_idIndexMarker266"/> a managed file storage service that is easy to set up and automatically scales up to petabytes. It provides a filesystem interface and file semantics such as file locking and strong consistency. Unlike Amazon EBS, which allows you to attach storage to a single compute node, EFS can be simultaneously used by hundreds or thousands of compute nodes. This allows you to organize efficient data sharing between nodes without the need to duplicate and distribute data.</p>
<p>Amazon SageMaker allows you to use EFS to store training datasets. The following code shows an example of how to use Amazon EFS in the training job configuration using the <strong class="source-inline">FileSystemInput</strong> class. Note that in this case, we have configured read-only access to the data (the <strong class="source-inline">ro</strong> flag of the <strong class="source-inline">file_system_access_mode</strong> parameter), which is typically the case for the training job. However, you can also specify read-write permissions by setting <strong class="source-inline">file_system_access_mode</strong> to <strong class="source-inline">rw</strong>:</p>
<pre class="source-code">
from sagemaker.tensorflow.estimator import TensorFlow
from sagemaker.inputs import FileSystemInput
from sagemaker import get_execution_role
role=get_execution_role()
estimator = TensorFlow(entry_point='train.py',
                       role=role,
                       image_uri="image/uri",
                       instance_count=4,
                       instance_type='ml.c4.xlarge')
file_system_input = FileSystemInput(file_system_id='fs-1',
                                    file_system_type='EFS',
                                    directory_path='/tensorflow',
                                    file_system_access_mode='ro')
estimator.fit(file_system_input)</pre>
<p>Here, you can control other EFS resources. Depending on the latency requirements for data reads and writes, you can choose from several modes that define the latency and concurrency characteristics of your filesystem. At the time of writing this book, EFS can sustain 10+ GB <a id="_idIndexMarker267"/>per second throughput and scale up to thousands of connected compute nodes.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor074"/>Amazon FSx for Lustre – high-performance filesystem</h2>
<p>Amazon FSx for <a id="_idIndexMarker268"/>Lustre is a file storage service optimized for ML and <strong class="bold">high-performance computing</strong> (<strong class="bold">HPC</strong>) workloads. It is<a id="_idIndexMarker269"/> designed for sub-millisecond latency for read and write operations and can provide hundreds of GB/s throughput. You can also choose to store data in S3 and synchronize it with the Amazon FSx for Lustre filesystem. In this case, an FSx system presents S3 objects as files and allows you to update data back to the S3 origin.</p>
<p>Amazon SageMaker supports storing training data in the FSx for Lustre filesystem. Training job configuration is similar to using the EFS filesystem; the only difference is that the <strong class="source-inline">file_system_type</strong> parameter is set to <strong class="source-inline">FSxLustre</strong>. The following code shows a sample training job:</p>
<pre class="source-code">
from sagemaker.inputs import FileSystemInput
from sagemaker import get_execution_role
role=get_execution_role()
estimator = TensorFlow(entry_point='train.py',
                       role=role,
                       image_uri="image/uri",
                       instance_count=4,
                       instance_type='ml.c4.xlarge')
file_system_input = FileSystemInput(
                       file_system_id='fs-XYZ', 
                       file_system_type='FSxLustre',
                       directory_path='/tensorflow',
                       file_system_access_mode='ro')
estimator.fit(file_system_input)</pre>
<p>Note that when provisioning your Lustre filesystem, you may choose either SSD or HDD storage. You should choose SSD for latency-sensitive workloads; HDD is a better fit for workloads with<a id="_idIndexMarker270"/> high-throughput requirements.</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor075"/>SageMaker Feature Store – purpose-built ML storage</h2>
<p>So far, we’ve <a id="_idIndexMarker271"/>discussed general-purpose file and object storage services that can be used to store data in your SageMaker workloads. However, real-life ML workflows may present certain challenges when it comes to feature engineering and data management, such as the following:</p>
<ul>
<li>Managing the data ingestion pipeline to keep data up to date</li>
<li>Organizing data usage between different teams in your organization and eliminating duplicative efforts</li>
<li>Sharing data between inference and training workloads when needed</li>
<li>Managing dataset consistency, its metadata and versioning</li>
<li>Ad hoc analysis of data</li>
</ul>
<p>To address these challenges, SageMaker provides an ML-specific data storage solution called <strong class="bold">Feature Store</strong>. It allows <a id="_idIndexMarker272"/>you to accelerate data processing and curation by reducing repetitive steps and providing a set of APIs to ingest, transform, and consume data for inference and model training. </p>
<p>Its central concept is a <strong class="bold">feature</strong> – a single attribute of a data record. Each data record consists of one or many features. Additionally, data records contain metadata such as record update time, unique record ID, and status (deleted or not). The feature can be of the string, integer, or fractional type. Data records and their associated features can be organized into logical units<a id="_idIndexMarker273"/> called <strong class="bold">feature groups</strong>. </p>
<p>Now, let’s review <a id="_idIndexMarker274"/>the key features of SageMaker Feature Store.</p>
<h3>Online and offline storage</h3>
<p>Feature<a id="_idIndexMarker275"/> Store supports<a id="_idIndexMarker276"/> several storage options for different use cases:</p>
<ul>
<li><strong class="bold">Offline storage</strong> is designed<a id="_idIndexMarker277"/> to store your data in scenarios where data retrieval latency is not critical, such as storing data for training or batch inference. Your dataset resides in S3 storage and can be queried using the Amazon Athena SQL engine.</li>
<li><strong class="bold">Online storage</strong> allows<a id="_idIndexMarker278"/> you to retrieve a single or batch of records with millisecond latency for real-time inference use cases.</li>
<li><strong class="bold">Offline and online storage</strong> allows<a id="_idIndexMarker279"/> you to store the same data in both forms of storage and use it in both inference and training scenarios.</li>
</ul>
<p>Ingestion Interfaces</p>
<p>There are several ways<a id="_idIndexMarker280"/><a id="_idIndexMarker281"/> to get your data in Feature Store. One way is using Feature Store’s <strong class="source-inline">PutRecord</strong> API, which allows you to write either a single or a batch of records. This will write the records in both offline and online storage. </p>
<p>Another option is to use a Spark connector. This is a convenient way to ingest data if you already have your Spark-based data processing pipeline. </p>
<h3>Analytical queries</h3>
<p>When data<a id="_idIndexMarker282"/> is stored in offline storage, you can use Athena SQL to query the dataset using SQL syntax. This is helpful when you have a diverse team that has different levels of coding skills. As Feature Store contains useful metadata fields, such <a id="_idIndexMarker283"/>as <strong class="bold">Event Time</strong> and <strong class="bold">Status</strong>, you can use these<a id="_idIndexMarker284"/> times to run <em class="italic">time travel</em> queries, for instance, to get a historical snapshot of your dataset at a given point in time.</p>
<h3>Feature discovery</h3>
<p>Once data has<a id="_idIndexMarker285"/> been ingested into Feature Store, you can use SageMaker Studio to review and analyze datasets via an intuitive UI component without the need to write any code:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI " height="463" src="image/B17519_04_01.jpg" width="1097"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI</p>
<p>Now that we understand the value proposition of Feature Store compared to more general-purpose <a id="_idIndexMarker286"/>storage solutions, let’s see how it can be used for a typical DL scenario when we want to have tokenized text available next to its original form. </p>
<h3>Using Feature Store for inference and training</h3>
<p>In this <a id="_idIndexMarker287"/>practical example, we will develop skills on how to use SageMaker Feature Store to ingest, process, and consume datasets that contain IMDb reviews. We will take the original dataset that contains the reviews and run a custom BERT tokenizer to convert unstructured text into a set of integer tokens. Then, we will ingest the dataset with the tokenized text feature into Feature Store so that you don’t have to tokenize the dataset the next time we want to use it. After that, we will train our model to categorize positive and negative reviews.</p>
<p>We will use SageMaker Feature Store SDK to interact with Feature Store APIs. We will use the <a id="_idIndexMarker288"/>HuggingFace Datasets (<a href="https://huggingface.co/docs/datasets/">https://huggingface.co/docs/datasets/</a>) and Transformers (<a href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a>) libraries to tokenize the text and run training and inference. Please make sure that these libraries are installed.</p>
<h4>Preparing the data</h4>
<p>Follow<a id="_idIndexMarker289"/> these steps to prepare the data:</p>
<ol>
<li>The first step is to acquire the initial dataset that contains the IMDb reviews:<p class="source-code">from datasets import load_dataset</p><p class="source-code">dataset = load_dataset("imdb")</p></li>
<li>Then, we must convert the dataset into a pandas DataFrame that is compatible with <strong class="bold">Feature Store</strong>. Next, we must cast data types into supported ones using Feature Store. Note that we must also add metadata fields – <strong class="source-inline">EventTime</strong> and <strong class="source-inline">ID</strong>. Both are required by Feature Store to support fast retrieval and feature versioning:<p class="source-code">import pandas as pd</p><p class="source-code">import time</p><p class="source-code">dataset_df = dataset['train'].to_pandas()</p><p class="source-code">current_time_sec = int(round(time.time()))</p><p class="source-code">dataset_df["EventTime"] = pd.Series([current_time_sec]*len(dataset_df), dtype="float64")</p><p class="source-code">dataset_df["ID"] = dataset_df.index</p><p class="source-code">dataset_df["text"] = dataset_df["text"].astype('string')</p><p class="source-code">dataset_df["text"] = dataset_df["text"].str.encode("utf8")</p><p class="source-code">dataset_df["text"] = dataset_df["text"].astype('string')</p></li>
<li>Now, let’s<a id="_idIndexMarker290"/> run the downloaded pre-trained tokenizer for the <strong class="source-inline">Distilbert</strong> model and add a new attribute, <strong class="source-inline">tokenized-text</strong>, to our dataset. Note that we cast <strong class="source-inline">tokenized-text</strong> to a string as SageMaker Feature Store doesn’t support collection data types such as arrays or maps:<p class="source-code">from transformers import DistilBertTokenizerFast</p><p class="source-code">tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')</p><p class="source-code">dataset_df["tokenized-text"] = tokenizer(dataset_df["text"].tolist(), truncation=True, padding=True)["input_ids"]</p><p class="source-code">dataset_df["tokenized-text"] = dataset_df["tokenized-text"].astype('string')</p></li>
</ol>
<p>As a result, we have a pandas DataFrame object that contains the features we are looking to<a id="_idIndexMarker291"/> ingest into Feature Store.</p>
<h4>Ingesting the data</h4>
<p>The next step is<a id="_idIndexMarker292"/> to provision Feature Store resources and prepare them for ingestion. Follow these steps:</p>
<ol>
<li value="1">We will start by configuring the feature group and preparing feature definitions. Note that since we stored our dataset in a pandas DataFrame, Feature Store can use this DataFrame to infer feature types:<p class="source-code">from sagemaker.feature_store.feature_group import FeatureGroup</p><p class="source-code">imdb_feature_group_name = "imdb-reviews-tokenized"</p><p class="source-code">imdb_feature_group = FeatureGroup(name=imdb_feature_group_name, sagemaker_session=sagemaker_session)</p><p class="source-code">imdb_feature_group.load_feature_definitions(data_frame=dataset_df)</p></li>
<li>Now that we have prepared the feature group configuration, we are ready to create it. This may take several minutes, so let’s add a <strong class="source-inline">Waiter</strong>. Since we are planning to use<a id="_idIndexMarker293"/> both online and offline storage, we will set the <strong class="source-inline">enable_online_store</strong> flag to <strong class="source-inline">True</strong>: <p class="source-code">imdb_feature_group.create(    </p><p class="source-code">s3_uri=f"s3://{s3_bucket_name}/{imdb_feature_group_name}",</p><p class="source-code">    record_identifier_name="ID",</p><p class="source-code">    event_time_feature_name="EventTime",</p><p class="source-code">    role_arn=role,</p><p class="source-code">    enable_online_store=True</p><p class="source-code">)</p><p class="source-code"># Waiter for FeatureGroup creation</p><p class="source-code">def wait_for_feature_group_creation_complete(feature_group):</p><p class="source-code">    status = feature_group.describe().get('FeatureGroupStatus')</p><p class="source-code">    print(f'Initial status: {status}')</p><p class="source-code">    while status == 'Creating':</p><p class="source-code">        print(f'Waiting for feature group: {feature_group.name} to be created ...')</p><p class="source-code">        time.sleep(5)</p><p class="source-code">        status = feature_group.describe().get('FeatureGroupStatus')</p><p class="source-code">    if status != 'Created':</p><p class="source-code">        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')</p><p class="source-code">    print(f'FeatureGroup {feature_group.name} was successfully created.')</p><p class="source-code">wait_for_feature_group_creation_complete(imdb_feature_group)</p></li>
<li>Once the group <a id="_idIndexMarker294"/>is available, we are ready to ingest data. Since we have a full dataset available, we will use a batch ingest API, as shown here: <p class="source-code">imdb_feature_group.ingest(data_frame=dataset_df, max_processes=16, wait=True)</p></li>
<li>Once the data has been ingested, we can run some analytical queries. For example, we can check if we are dealing with a balanced or imbalanced dataset. As mentioned previously, Feature Store supports querying data using the Amazon Athena SQL engine:<p class="source-code">athena_query = imdb_feature_group.athena_query()</p><p class="source-code">imdb_table_name = athena_query.table_name</p><p class="source-code">result = athena_query.run(f'SELECT "label", COUNT("label") as "Count" FROM "sagemaker_featurestore"."{imdb_table_name}" group by "label";', output_location=f"s3://{s3_bucket_name}/athena_output")</p><p class="source-code">athena_query.wait()</p><p class="source-code">print(f"Counting labels in dataset: \n {athena_query.as_dataframe()}")</p></li>
</ol>
<p>It will take a <a id="_idIndexMarker295"/>moment to run, but in the end, you should get a count of the labels in our dataset.</p>
<h3>Using Feature Store for training</h3>
<p>Now that we<a id="_idIndexMarker296"/> have data available, let’s train our binary classification model. Since data in Feature Store is stored in Parquet<a id="_idIndexMarker297"/> format (<a href="https://parquet.apache.org/">https://parquet.apache.org/</a>) in a designated S3 location, we can directly use Parquet files for training.</p>
<p>To handle Parquet files, we need to make sure that our data reader is aware of the format. For this, we can use the pandas <strong class="source-inline">.read_parquet()</strong> method. Then, we can convert the pandas DataFrame object into the HuggingFace dataset and select the attributes of interest – <strong class="source-inline">tokenized-text</strong> and <strong class="source-inline">label</strong>:</p>
<pre class="source-code">
    df = pd.read_parquet(args.training_dir)
    df["input_ids"] = df["tokenized-text"].astype("string")
    train_dataset = Dataset.from_pandas(df[["input_ids", "label"]])</pre>
<p>Now, we need to convert <strong class="source-inline">tokenized-text</strong> from a string into a list of integers:</p>
<pre class="source-code">
    def string_to_list(example):
        list_of_str = example["input_ids"].strip("][").split(", ")
        example["input_ids"] = [int(el) for el in list_of_str]
        return example
    train_dataset = train_dataset.map(string_to_list)</pre>
<p>The rest of the training script is the same. You can find the full code at <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py</a>.</p>
<p>Now that we’ve<a id="_idIndexMarker298"/> modified the training script, we are ready to run our training job: </p>
<ol>
<li value="1">First, we must get the location of the dataset:<p class="source-code">train_dataset_uri = imdb_feature_group.describe()['OfflineStoreConfig']["S3StorageConfig"]["ResolvedOutputS3Uri"]</p></li>
<li>Now, we must pass it to our <strong class="source-inline">Estimator</strong> object:<p class="source-code">from sagemaker.huggingface.estimator import HuggingFace</p><p class="source-code">estimator = HuggingFace(</p><p class="source-code">    py_version="py36",</p><p class="source-code">    entry_point="train.py",</p><p class="source-code">    source_dir="1_sources",</p><p class="source-code">    pytorch_version="1.7.1",</p><p class="source-code">    transformers_version="4.6.1",</p><p class="source-code">    hyperparameters={</p><p class="source-code">        "model_name":"distilbert-base-uncased",</p><p class="source-code">        "train_batch_size": 16,</p><p class="source-code">        "epochs": 3</p><p class="source-code">        # "max_steps": 100 # to shorten training cycle, remove in real scenario</p><p class="source-code">    },</p><p class="source-code">    instance_type="ml.p2.xlarge",</p><p class="source-code">    debugger_hook_config=False,</p><p class="source-code">    disable_profiler=True,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    role=role</p><p class="source-code">)</p><p class="source-code">estimator.fit(train_dataset_uri)</p></li>
</ol>
<p>After some time (depending on how many epochs or steps you use), the model should be trained<a id="_idIndexMarker299"/> to classify reviews based on the input text.</p>
<h3>Using Feature Store for inference</h3>
<p>For inference, we can use the Feature Store runtime client from the Boto3 library to fetch a single record <a id="_idIndexMarker300"/>or a batch:</p>
<pre class="source-code">
import boto3
client = boto3.client('sagemaker-featurestore-runtime')</pre>
<p>Note that you need to know the unique IDs of the records to retrieve them: </p>
<pre class="source-code">
response = client.batch_get_record(
    Identifiers=[
        {
            'FeatureGroupName':imdb_feature_group.name,
            'RecordIdentifiersValueAsString': ["0", "1", "2"], # picking several records to run inference.
            'FeatureNames': [
                'tokenized-text', "label", 'text'
            ]
        },
    ]
)
# preparing the inference payload
labels = []
input_ids = []
texts = []
for record in response["Records"]:
    for feature in record["Record"]:
        if feature["FeatureName"]=="label":
            labels.append(feature["ValueAsString"])
        if feature["FeatureName"]=="tokenized-text":
            list_of_str = feature["ValueAsString"].strip("][").split(", ")
            input_ids.append([int(el) for el in list_of_str])
        if feature["FeatureName"]=="text":
            texts.append(feature["ValueAsString"])    </pre>
<p>Now, you can send this request for inference to your deployed model. Refer to the following notebook for an end-to-end example: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb</a>.</p>
<p>In this section, we reviewed the options you can use to store your ML data for inference and training needs. But it’s rarely the case that data can be used “as-is.” In many scenarios, you <a id="_idIndexMarker301"/>need to continuously process data at scale before using it in your ML workloads. SageMaker Processing provides a scalable and flexible mechanism to process your data at scale. Let’s take a look.</p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor076"/>Processing data at scale</h1>
<p>SageMaker <a id="_idIndexMarker302"/>Processing allows you to run containerized code in the cloud. This is useful for scenarios such as data pre and post-processing, feature engineering, and model evaluation. SageMaker Processing can be useful for ad hoc workloads as well as recurrent jobs. </p>
<p>As in the case of a training job, Amazon SageMaker provides a managed experience for underlying compute and data infrastructure. You will need to provide a processing job configuration, code, and the container you want to use, but SageMaker will take care of provisioning the instances and deploying the containerized code, as well as running and monitoring the job and its progress. Once your job reaches the terminal state (success or failure), SageMaker will upload the resulting artifacts to the S3 storage and deprovision the cluster. </p>
<p>SageMaker Processing provides two pre-built containers:</p>
<ul>
<li>A PySpark container with dependencies to run Spark computations</li>
<li>A scikit-learn container</li>
</ul>
<p>When selecting which built-in processing container, note that the PySpark container supports distributed Spark jobs. It allows you to coordinate distributed data processing in a Spark cluster, maintain it globally across the dataset, and visualize processing jobs via the Spark UI. At the same time, the scikit-learn container doesn’t support a shared global state, so each processing node runs independently. Limited task coordination can be done by sharding datasets into sub-datasets and processing each sub-dataset independently.</p>
<p>You can also provide a <strong class="bold">Bring-Your-Own</strong> (<strong class="bold">BYO</strong>) processing container with virtually any runtime configuration to run SageMaker Processing. This flexibility allows you to easily move your existing processing code so that it can run on SageMaker Processing with minimal effort:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 4.2 – SageMaker Processing node " height="705" src="image/B17519_04_02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – SageMaker Processing node</p>
<p>Let’s try to<a id="_idIndexMarker303"/> build a container for processing and run a multi-node processing job to augment the image dataset for further training.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor077"/>Augmenting image data using SageMaker Processing</h2>
<p>In this <a id="_idIndexMarker304"/>example, we<a id="_idIndexMarker305"/> will download the 325 Bird Species dataset from Kaggle (<a href="https://www.kaggle.com/gpiosenka/100-bird-species/">https://www.kaggle.com/gpiosenka/100-bird-species/</a>). Then, we will augment this dataset with modified versions of the images (rotated, cropped, resized) to improve the performance of downstream image classification tasks. For image transformation, we will use the Keras library. Then, we will run our processing job on multiple nodes to speed up our job. Follow these steps:</p>
<ol>
<li value="1">We will start by building a custom processing container. Note that SageMaker runs processing containers using the <strong class="source-inline">docker run image_uri</strong> command, so we need to specify the entry point in our Dockerfile. We are using the official Python 3.7 container with a basic Debian version:<p class="source-code">FROM python:3.7-slim-buster</p><p class="source-code">########### Installing packages ##########</p><p class="source-code">RUN pip3 install pandas numpy tensorflow numpy scipy</p><p class="source-code">RUN pip install Pillow</p><p class="source-code">ENV PYTHONUNBUFFERED=TRUE</p><p class="source-code">########### Configure processing scripts ##########</p><p class="source-code">ARG code_dir=/opt/ml/code</p><p class="source-code">RUN mkdir -p $code_dir</p><p class="source-code">COPY 2_sources $code_dir</p><p class="source-code">WORKDIR $code_dir</p><p class="source-code">ENTRYPOINT ["python3","processing.py"]</p></li>
</ol>
<p>We will<a id="_idIndexMarker306"/> start <a id="_idIndexMarker307"/>by building a custom processing container. </p>
<ol>
<li value="2">Now, we need to provide our processing code. We will use <strong class="source-inline">keras.utils</strong> to load the original dataset into memory and specify the necessary transformations:<p class="source-code">    dataset = keras.utils.image_dataset_from_directory(</p><p class="source-code">        args.data_location,</p><p class="source-code">        labels="inferred",</p><p class="source-code">        label_mode="int",</p><p class="source-code">        class_names=None,</p><p class="source-code">        color_mode="rgb",</p><p class="source-code">        batch_size=args.batch_size,</p><p class="source-code">        image_size=(WIDTH, HEIGHT),</p><p class="source-code">        shuffle=True,</p><p class="source-code">        seed=None,</p><p class="source-code">        validation_split=None,</p><p class="source-code">        subset=None,</p><p class="source-code">        interpolation="bilinear",</p><p class="source-code">        follow_links=False,</p><p class="source-code">        crop_to_aspect_ratio=False,</p><p class="source-code">    )</p><p class="source-code">    datagen = ImageDataGenerator(</p><p class="source-code">        rotation_range=40,</p><p class="source-code">        width_shift_range=0.2,</p><p class="source-code">        height_shift_range=0.2,</p><p class="source-code">        shear_range=0.2,</p><p class="source-code">        zoom_range=0.2,</p><p class="source-code">        horizontal_flip=True,</p><p class="source-code">        fill_mode="nearest",</p><p class="source-code">    )</p></li>
<li>Since the Keras<a id="_idIndexMarker308"/> generator<a id="_idIndexMarker309"/> operates in memory, we need to save the generated images to disk:<p class="source-code">    for batch_data, batch_labels in dataset.as_numpy_iterator():</p><p class="source-code">        print(f"Processing batch with index {i} out from {len(dataset)}")</p><p class="source-code">        for image, label in zip(batch_data, batch_labels):</p><p class="source-code">            label_name = class_lookup.iloc[label]["class"]</p><p class="source-code">            image_save_dir = os.path.join(augmented_root_dir, label_name)</p><p class="source-code">            os.makedirs(image_save_dir, exist_ok=True)</p><p class="source-code">            j = 0</p><p class="source-code">            image = np.expand_dims(image, axis=0)</p><p class="source-code">            # generate 5 new augmented images</p><p class="source-code">            for batch in datagen.flow(</p><p class="source-code">                image,</p><p class="source-code">                batch_size=1,</p><p class="source-code">                save_to_dir=image_save_dir,</p><p class="source-code">                save_prefix="augmented",</p><p class="source-code">                save_format="jpeg",</p><p class="source-code">            ):</p><p class="source-code">                j += 1</p><p class="source-code">                if j &gt; max_augmentations:</p><p class="source-code">                    break</p><p class="source-code">        i += 1</p><p class="source-code">        if args.max_samples is not None:</p><p class="source-code">            if i &gt; args.max_samples:</p><p class="source-code">                break</p></li>
</ol>
<p>We save<a id="_idIndexMarker310"/> the <a id="_idIndexMarker311"/>augmented images in a similar directory hierarchy, where labels are defined by directory name. </p>
<ol>
<li value="4">Once we have the BYO container and processing code, we are ready to schedule the processing job. First, we need to instantiate the <strong class="source-inline">Processor</strong> object with basic job configurations, such as the number and type of instances and container images:<p class="source-code">from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput</p><p class="source-code">lookup_location = "/opt/ml/processing/lookup"</p><p class="source-code">data_location = "/opt/ml/processing/input"</p><p class="source-code">output_location = '/opt/ml/processing/output'</p><p class="source-code">sklearn_processor = Processor(</p><p class="source-code">                      image_uri=image_uri,</p><p class="source-code">                      role=role,</p><p class="source-code">                      instance_count=2,</p><p class="source-code">                      base_job_name="augmentation",</p><p class="source-code">                      sagemaker_session=sess, </p><p class="source-code">                      instance_type="ml.m5.xlarge")</p></li>
</ol>
<p>To start the <a id="_idIndexMarker312"/>job, we <a id="_idIndexMarker313"/>must execute the <strong class="source-inline">.run()</strong> method. This method allows us to provide additional configuration parameters. For instance, to distribute tasks evenly, we need to split datasets into chunks. This is easy to do using the <strong class="source-inline">ShardedByKey</strong> distribution type. In this case, SageMaker will attempt to evenly distribute objects between our processing nodes. SageMaker Processing allows you to pass your custom script configuration via the <strong class="source-inline">arguments</strong> collection. You will need to make sure that your processing script can parse these command-line arguments properly:</p>
<p class="source-code">sklearn_processor.run(</p>
<p class="source-code">    inputs=[</p>
<p class="source-code">      ProcessingInput(</p>
<p class="source-code">          source=dataset_uri,</p>
<p class="source-code">          destination=data_location,</p>
<p class="source-code">          s3_data_distribution_type="ShardedByS3Key"),</p>
<p class="source-code">      ProcessingInput(</p>
<p class="source-code">          source=class_dict_uri,</p>
<p class="source-code">          destination=lookup_location),],</p>
<p class="source-code">    outputs=[</p>
<p class="source-code">      ProcessingOutput(</p>
<p class="source-code">          source=output_location)],</p>
<p class="source-code">          arguments = [</p>
<p class="source-code">               "--data_location", data_location, </p>
<p class="source-code">               "--lookup_location", lookup_location,</p>
<p class="source-code">               "--output_location", output_location,</p>
<p class="source-code">               "--batch_size", "32",</p>
<p class="source-code">               "--max_samples", "10",</p>
<p class="source-code">               "--max_augmentations", "5"</p>
<p class="source-code">               ])                     </p>
<p>For the full processing code, please refer to <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py</a>.</p>
<p>This example <a id="_idIndexMarker314"/>should <a id="_idIndexMarker315"/>give you an intuition of how SageMaker Processing can be used for your data processing needs. At the same time, SageMaker Processing is flexible enough to run any arbitrary tasks, such as batch inference, data aggregation and analytics, and others. </p>
<p>In the next section, we will discuss how to optimize data storage and retrieval for large DL datasets.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor078"/>Optimizing data storage and retrieval</h1>
<p>When <a id="_idIndexMarker316"/>training <strong class="bold">SOTA DL</strong> models, you typically<a id="_idIndexMarker317"/> need <a id="_idIndexMarker318"/>a large dataset for a model to train. It can be expensive to store and retrieve such large datasets. For instance, the popular computer vision <a id="_idIndexMarker319"/>dataset <strong class="bold">COCO2017</strong> is approximately 30 GB, while the <strong class="bold">Common Crawl</strong> dataset <a id="_idIndexMarker320"/>for NLP tasks has a size of hundreds of TB. Dealing with such large datasets requires careful consideration of where to store the dataset and how to retrieve it at inference or training time. In this section, we will discuss some of the optimization strategies you can use when choosing storage and retrieval strategies.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor079"/>Choosing a storage solution</h2>
<p>When choosing <a id="_idIndexMarker321"/>an optimal storage solution, you may<a id="_idIndexMarker322"/> consider the following factors, among others:</p>
<ul>
<li>The cost of storage and data retrieval</li>
<li>The latency and throughput requirements for data retrieval</li>
<li>Data partitioning</li>
<li>How frequently data is refreshed</li>
</ul>
<p>Let’s take a look at the <a id="_idIndexMarker323"/>pros and cons of various storage solutions:</p>
<ul>
<li><strong class="bold">Amazon S3</strong> provides <a id="_idIndexMarker324"/>the cheapest storage solution among those considered. However, you should be aware that Amazon S3 also charges for data transfer and data requests. In scenarios where your dataset consists of a large number of small files, you may incur considerable costs associated with <strong class="bold">PUT</strong> and <strong class="bold">GET</strong> records. You may consider batching small objects into large objects to reduce this cost. Note that there are additional charges involved when retrieving data stored in another AWS region. It could be reasonable to collocate your workload and data in the same AWS region to avoid these costs. S3 is also generally the slowest storage solution. By default, Amazon SageMaker downloads all objects from S3 before the training begins. This initial download time can take minutes and will add to the general training time. For instance, in the case of the <strong class="bold">COCO2017</strong> dataset, it <a id="_idIndexMarker325"/>takes ~20 minutes to download it on training nodes from S3. Amazon provides several mechanisms to stream data from S3 and eliminates download time. We will discuss these in this section.</li>
<li><strong class="bold">Amazon EFS</strong> storage is <a id="_idIndexMarker326"/>generally more expensive than Amazon S3. However, unlike Amazon S3, Amazon EFS doesn’t have any costs associated with read and write operations. Since EFS provides a filesystem interface, compute nodes can directly mount to the EFS directory that contains the dataset and use it immediately without the need to download the dataset. Amazon EFS provides an easy mechanism to share reusable datasets between workloads or teams. </li>
<li><strong class="bold">Amazon FSx for Lustre</strong> provides<a id="_idIndexMarker327"/> the lowest latency for data retrieval but also the most expensive storage price. Like Amazon EFS, it doesn’t require any download time. One of the common use cases for scenarios is to store your data in S3. When you need to run your set of experiments, you can provision FSx for Lustre with synchronization from S3, which seamlessly copies data from S3 to your filesystem. After that, you can run your experiments and use FSx for Lustre as a data source, leveraging the lowest latency for data retrieval. Once experimentation is done, you can de-provision the Lustre filesystem to avoid any additional costs and keep the original data in S3.</li>
<li><strong class="bold">SageMaker Feature Store</strong> has the <a id="_idIndexMarker328"/>most out-of-the-box ML-specific features; however, it <a id="_idIndexMarker329"/>has its shortcomings and strong assumptions. Since its offline storage is backed by S3, it has a similar cost structure and latency considerations. Online storage adds additional storage, read, and write costs. SageMaker Feature Store fits well into scenarios when you need to reuse the same dataset for inference and training workloads. Another popular use case for Feature Store is when you need to have audit requirements or run analytical queries against your datasets. Note that since Feature Store supports only a limited amount of data types (for example, it doesn’t support any collections), you may need to do type casting when consuming data from Feature Store.</li>
</ul>
<p>AWS provides a <a id="_idIndexMarker330"/>wide range of storage solutions and at times, it may not be obvious which solution to choose. As always, it’s important to start by understanding your use case requirements and success criteria (for instance, lowest possible latency, highest throughput, or most cost-optimal solution).</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor080"/>Streaming datasets </h2>
<p>Amazon S3 is a<a id="_idIndexMarker331"/> popular storage solution for large ML datasets, given its low cost, high durability, convenient API, and integration with other services, such as SageMaker. As we discussed in the previous section, one of the downsides of using S3 to store training datasets is that you need to download the dataset to your training nodes before training can start.</p>
<p>You can choose to <a id="_idIndexMarker332"/>use the <strong class="bold">ShardedByKey</strong> distribution strategy, which will reduce the amount of data downloaded to each training node. However, that approach only reduces the amount of data that needs to be downloaded to your training nodes. For large datasets (100s+ GB), it solves the problem only partially. You will also need to ensure that your training nodes have enough EBS volume capacity to store data.</p>
<p>An alternative approach to reduce training time is to stream data from Amazon S3 without downloading it upfront. Several implementations of S3 data streaming are provided by Amazon SageMaker:</p>
<ul>
<li>Framework-specific streaming implementations, such as <strong class="source-inline">PipeModeDataset</strong> for TensorFlow and Amazon S3 Plugin for PyTorch</li>
<li>Framework-agnostic FastFile mode </li>
</ul>
<p>Let’s review the <a id="_idIndexMarker333"/>benefits of these approaches.</p>
<h3>PipeModeDataset for TensorFlow</h3>
<p><strong class="bold">PipeModeDataset</strong> is an <a id="_idIndexMarker334"/>open source <a id="_idIndexMarker335"/>implementation of the TensorFlow<a id="_idIndexMarker336"/> Dataset API that allows to you read SageMaker Pipe mode channels. SageMaker Pipe mode is a mechanism that you can use to access stored on S3 using named pipes. Using <strong class="source-inline">PipeModeDataset</strong>, your training program can read from S3 without managing access to S3 objects. When using <strong class="source-inline">PipeModeDataset</strong>, you need to ensure that you are using a matching version of TensorFlow.</p>
<p>SageMaker Pipe mode is enabled when configuring a SageMaker training job. You can map multiple datasets to a single pipe if you’re storing multiple datasets under the same S3 path. Note that SageMaker supports up to 20 pipes. If you need more than 20 pipes, you may consider using Augmented Manifest files, which allow you to explicitly list a set of S3 objects to be streamed. During training, SageMaker will read objects from the manifest file and stream them into the pipe.</p>
<p><strong class="source-inline">PipeModeDataset</strong> supports the following dataset formats: text line, RecordIO, and TFRecord. If you have a dataset in a different format (for instance, as separate image files) you will have to convert your dataset. Note that the performance of <strong class="source-inline">PipeModeDataset</strong> performance on the number and size of the files. It’s generally recommended to <a id="_idIndexMarker337"/>keep <a id="_idIndexMarker338"/>the file size around 100 to 200 MB for optimal performance.</p>
<p class="callout-heading">Note</p>
<p class="callout">Since <strong class="source-inline">PipeModeDataset</strong> implements the TensorFlow Dataset API, you can use familiar methods to manipulate your datasets, such as <strong class="source-inline">.apply(), .map()</strong>. <strong class="source-inline">PipeModeDataset</strong> also can be passed to TensorFlow Estimator directly.</p>
<p>There are several <a id="_idIndexMarker339"/>differences between <strong class="source-inline">PipeModeDataset</strong> and TensorFlow Dataset that you should consider:</p>
<ul>
<li><strong class="source-inline">PipeModeDataset</strong> reads data <a id="_idIndexMarker340"/>sequentially in files. SageMaker supports the <strong class="source-inline">ShuffleConfig</strong> (<a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml</a>) parameter, which shuffles the order of the files to read. You can also call the <strong class="source-inline">.shuffle()</strong> method to further shuffle records. </li>
<li><strong class="source-inline">PipeModeDataset</strong> supports only three data types, all of which require data to be converted into one of the supported formats. </li>
<li><strong class="source-inline">PipeModeDataset</strong> has limited controls when it comes to manipulating data at training time. For instance, if you need to boost the underrepresented class in your classification dataset, you will need to use a separate pipe to stream samples of the underrepresented file and handle the boosting procedure in your training script.</li>
<li><strong class="source-inline">PipeModeDataset</strong> doesn’t support SageMaker Local mode, so it can be tricky to debug your training program. When using SageMaker Pipe mode, you don’t have access to the internals of how SageMaker streams your data objects into pipes.</li>
</ul>
<p>Let’s look at how <strong class="source-inline">PipeModeDataset</strong> can be used. In this example, for training purposes, we will convert the CIFAR-100 dataset into TFRecords and then stream this dataset at training time using <strong class="source-inline">PipeModeDataset</strong>. We will provide a redacted version for brevity instead of listing the entire example. The full source is available at <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb</a>. Follow these steps:</p>
<ol>
<li value="1">Let’s start <a id="_idIndexMarker341"/>by converting <a id="_idIndexMarker342"/>our <a id="_idIndexMarker343"/>dataset into TFRecord format. In the following code block, there is a method that iterates over a batch of files, converts a pair of images and labels into a TensorFlow <strong class="source-inline">Example</strong> class, and writes a batch of <strong class="source-inline">Example</strong> objects into a single <strong class="source-inline">TFRecord</strong> file:<p class="source-code">def convert_to_tfrecord(input_files, output_file):</p><p class="source-code">    """Converts a file to TFRecords."""</p><p class="source-code">    print("Generating %s" % output_file)</p><p class="source-code">    with tf.io.TFRecordWriter(output_file) as record_writer:</p><p class="source-code">        for input_file in input_files:</p><p class="source-code">            data_dict = read_pickle_from_file(input_file)</p><p class="source-code">            data = data_dict[b"data"]</p><p class="source-code">            labels = data_dict[b"fine_labels"]</p><p class="source-code">            num_entries_in_batch = len(labels)</p><p class="source-code">            for i in range(num_entries_in_batch):</p><p class="source-code">                example = tf.train.Example(</p><p class="source-code">                    features=tf.train.Features(</p><p class="source-code">                        feature={</p><p class="source-code">                            "image": _bytes_feature(data[i].tobytes()),</p><p class="source-code">                            "label": _int64_feature(labels[i]),</p><p class="source-code">                        }</p><p class="source-code">                    )</p><p class="source-code">                )</p><p class="source-code">                record_writer.write(example.SerializeToString())</p></li>
<li>Once we have our datasets in TFRecord format, we need to create our training script. It will largely follow a typical TensorFlow training script, with the only difference being that we will use <strong class="source-inline">PipeModeDataset</strong> instead of <strong class="source-inline">TFRecordDataset</strong>. You<a id="_idIndexMarker344"/> can <a id="_idIndexMarker345"/>use the following code to <a id="_idIndexMarker346"/>configure <strong class="source-inline">PipeModeDataset</strong>:<p class="source-code">def _input(epochs, batch_size, channel, channel_name):</p><p class="source-code">    mode = args.data_config[channel_name]["TrainingInputMode"]</p><p class="source-code">    dataset = PipeModeDataset(channel=channel_name, record_format="TFRecord")</p><p class="source-code">    dataset = dataset.repeat()</p><p class="source-code">    dataset = dataset.prefetch(10)</p><p class="source-code">    dataset = dataset.map(_dataset_parser, num_parallel_calls=10)</p><p class="source-code">    if channel_name == "train":</p><p class="source-code">        buffer_size = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * 0.4) + 3 * batch_size</p><p class="source-code">        dataset = dataset.shuffle(buffer_size=buffer_size)</p><p class="source-code">    dataset = dataset.batch(batch_size, drop_remainder=True)</p><p class="source-code">    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)</p><p class="source-code">    image_batch, label_batch = iterator.get_next()</p><p class="source-code">    return {INPUT_TENSOR_NAME: image_batch}, label_batch</p></li>
<li>When configuring the SageMaker training job, we need to explicitly specify that we want to<a id="_idIndexMarker347"/> use <a id="_idIndexMarker348"/>Pipe <a id="_idIndexMarker349"/>mode:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">hyperparameters = {"epochs": 10, "batch-size": 256}</p><p class="source-code">estimator = TensorFlow(</p><p class="source-code">    entry_point="train.py",</p><p class="source-code">    source_dir="3_sources",</p><p class="source-code">    metric_definitions=metric_definitions,</p><p class="source-code">    hyperparameters=hyperparameters,</p><p class="source-code">    role=role,</p><p class="source-code">    framework_version="1.15.2",</p><p class="source-code">    py_version="py3",</p><p class="source-code">    train_instance_count=1,</p><p class="source-code">    input_mode="Pipe",</p><p class="source-code">    train_instance_type="ml.p2.xlarge",</p><p class="source-code">    base_job_name="cifar100-tf",</p><p class="source-code">)</p></li>
</ol>
<p>Note that since the CIFAF100 dataset is relatively small, you may be not able to see any considerable <a id="_idIndexMarker350"/>decrease in the <a id="_idIndexMarker351"/>training <a id="_idIndexMarker352"/>start time. However, with bigger datasets such as COCO2017, you can expect the training time to reduce by at least several minutes. </p>
<h3>Amazon S3 Plugin for PyTorch</h3>
<p>Amazon S3 Plugin<a id="_idIndexMarker353"/> for PyTorch allows <a id="_idIndexMarker354"/>you to stream data directly from S3 objects with minimal changes to your existing PyTorch training script. Under the hood, S3 Plugin uses <strong class="source-inline">TransferManager</strong> from the AWS SDK for C++ to fetch files from S3 and utilizes S3 multipart download for optimal data throughput and reliability.</p>
<p>S3 Plugin provides two implementations of PyTorch dataset APIs: a map-style <strong class="source-inline">S3Dataset</strong> and an iterable-style <strong class="source-inline">S3IterableDataset</strong>. In the following section, we will discuss when to use one or another.</p>
<h4>Map-style S3Dataset</h4>
<p><strong class="source-inline">S3Dataset</strong> represents <a id="_idIndexMarker355"/>a mapping of indexes <a id="_idIndexMarker356"/>and data records and implements the <strong class="source-inline">__getitem__()</strong> method. It allows you to randomly access data records based on their indices. A map-style dataset works best when each file has a single data record. You can use PyTorch’s distributed sampler to further partition the dataset between training nodes.</p>
<p>Here is an example of using <strong class="source-inline">S3Dataset</strong> for images stored on S3:</p>
<ol>
<li value="1">First, we will define the dataset class that inherits from the parent <strong class="source-inline">S3Dataset</strong>. Then, we will define the data processing pipeline using PyTorch functions:<p class="source-code">from awsio.python.lib.io.s3.s3dataset import S3Dataset</p><p class="source-code">from torch.utils.data import DataLoader</p><p class="source-code">from torchvision import transforms</p><p class="source-code">from PIL import Image</p><p class="source-code">import io</p><p class="source-code">class S3ImageSet(S3Dataset):</p><p class="source-code">    def __init__(self, urls, transform=None):</p><p class="source-code">        super().__init__(urls)</p><p class="source-code">        self.transform = transform</p><p class="source-code">    def __getitem__(self, idx):</p><p class="source-code">        img_name, img = super(S3ImageSet, self).__getitem__(idx)</p><p class="source-code">        # Convert bytes object to image</p><p class="source-code">        img = Image.open(io.BytesIO(img)).convert('RGB')</p><p class="source-code">        </p><p class="source-code">        # Apply preprocessing functions on data</p><p class="source-code">        if self.transform is not None:</p><p class="source-code">            img = self.transform(img)</p><p class="source-code">        return img</p><p class="source-code">batch_size = 32</p><p class="source-code">preproc = transforms.Compose([</p><p class="source-code">    transforms.ToTensor(),</p><p class="source-code">    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),</p><p class="source-code">    transforms.Resize((100, 100))</p><p class="source-code">])</p></li>
<li>Next, we will <a id="_idIndexMarker357"/>create a PyTorch-native <strong class="source-inline">Dataloader</strong> object<a id="_idIndexMarker358"/> that can be passed to any training script:<p class="source-code"># urls can be S3 prefix containing images or list of all individual S3 images</p><p class="source-code">urls = 's3://path/to/s3_prefix/'</p><p class="source-code">dataset = S3ImageSet(urls, transform=preproc)</p><p class="source-code">dataloader = DataLoader(dataset,</p><p class="source-code">        batch_size=batch_size,</p><p class="source-code">        num_workers=64)</p></li>
</ol>
<h4>Iterable-style S3IterableDataset</h4>
<p><strong class="source-inline">S3IterableDataset</strong> represents <a id="_idIndexMarker359"/>iterable <a id="_idIndexMarker360"/>objects and implements Python’s <strong class="source-inline">__iter__()</strong> method. Generally, you use an iterable-style dataset when random reads (such as in a map-style dataset) are expensive or impossible. You should use an iterable-style dataset when you have a batch of data records stored in a single file object. </p>
<p>When using <strong class="source-inline">S3IterableDataset</strong>, it’s important to control your file sizes. If your dataset is represented by a large number of files, accessing each file will come with overhead. In such scenarios, it’s recommended to merge data records into larger file objects. </p>
<p><strong class="source-inline">S3IterableDataset</strong> doesn’t restrict what file types can be used. A full binary blob of the file object is returned, and you are responsible to provide parsing logic. You can shuffle the URLs of file objects by setting the <strong class="source-inline">shuffle_urls</strong> flag to true. Note that if you need to shuffle records within the same data objects, you can use <strong class="bold">ShuffleDataset</strong>, which is provided as part of the S3 PyTorch plugin. <strong class="source-inline">ShuffleDataset</strong> accumulates data records across multiple file objects and returns a random sample from it.</p>
<p><strong class="source-inline">S3IterableDataset</strong> takes care of sharding data between training nodes when running distributed training. You can wrap <strong class="source-inline">S3IterableDataset</strong> with PyTorch’s <strong class="source-inline">DataLoader</strong> for parallel data loading and pre-processing.</p>
<p>Let’s look at an example of how to construct an iterable-style dataset from several TAR archives stored on S3 and apply data transformations: </p>
<ol>
<li value="1">We will start by defining a custom dataset class using PyTorch’s native <strong class="source-inline">IterableDataset</strong>. As part of the class definition, we use  <strong class="source-inline">S3IterableDataset</strong> to fetch data from S3 and data transformations that will be applied to<a id="_idIndexMarker361"/> individual data records:<p class="source-code">from torch.utils.data import IterableDataset</p><p class="source-code">from awsio.python.lib.io.s3.s3dataset import S3IterableDataset</p><p class="source-code">from PIL import Image</p><p class="source-code">import io</p><p class="source-code">import numpy as np</p><p class="source-code">from torchvision import transforms</p><p class="source-code">class ImageS3(IterableDataset):</p><p class="source-code">    def __init__(self, urls, shuffle_urls=False, transform=None):</p><p class="source-code">        self.s3_iter_dataset = S3IterableDataset(urls,</p><p class="source-code">                                   shuffle_urls)</p><p class="source-code">        self.transform = transform</p><p class="source-code">    def data_generator(self):</p><p class="source-code">        try:</p><p class="source-code">            while True:</p><p class="source-code">                label_fname, label_fobj =       next(self.s3_iter_dataset_iterator)</p><p class="source-code">                image_fname, image_fobj = next(self.s3_iter_dataset_iterator)</p><p class="source-code">                label = int(label_fobj)</p><p class="source-code">                image_np = Image.open(io.BytesIO(image_fobj)).convert('RGB')                </p><p class="source-code">                # Apply torch vision transforms if provided</p><p class="source-code">                if self.transform is not None:</p><p class="source-code">                    image_np = self.transform(image_np)</p><p class="source-code">                yield image_np, label</p><p class="source-code">        except StopIteration:</p><p class="source-code">            return</p><p class="source-code">    def __iter__(self):</p><p class="source-code">        self.s3_iter_dataset_iterator = iter(self.s3_iter_dataset)</p><p class="source-code">        return self.data_generator()        </p><p class="source-code">    def set_epoch(self, epoch):</p><p class="source-code">        self.s3_iter_dataset.set_epoch(epoch)</p></li>
<li>Next, we define a transformation to normalize images and then instantiate a dataset instance with the ability to stream images from S3:<p class="source-code"># urls can be a S3 prefix containing all the shards or a list of S3 paths for all the shards </p><p class="source-code"> urls = ["s3://path/to/file1.tar", "s3://path/to/file2.tar"]</p><p class="source-code"># Example Torchvision transforms to apply on data    </p><p class="source-code">preproc = transforms.Compose([</p><p class="source-code">    transforms.ToTensor(),</p><p class="source-code">    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),</p><p class="source-code">    transforms.Resize((100, 100))</p><p class="source-code">])</p><p class="source-code">dataset = ImageS3(urls, transform=preproc)</p></li>
</ol>
<p>Now, let’s<a id="_idIndexMarker362"/> look at FastFile<a id="_idIndexMarker363"/> mode.</p>
<h3>FastFile mode</h3>
<p>In late 2021, Amazon <a id="_idIndexMarker364"/>announced a new approach for <a id="_idIndexMarker365"/>streaming data directly from S3 called FastFile mode. It combines the benefits of streaming data from S3 with the convenience of working with local files. In <strong class="bold">FastFile</strong> mode, each file will appear to your training program as a POSIX filesystem mount. Hence, it will be indistinguishable from any other local files, such as the ones stored on the mounted EBS volume. </p>
<p>When reading file objects in FastFile mode, SageMaker retrieves chunks of the file if the file format supports chunking; otherwise, a full file is retrieved. FastFile mode performs optimally if data is read sequentially. Please note that there is an additional overhead on retrieving each file object. So, fewer files will usually result in a lower startup time for your training job.</p>
<p>Compared to the previously discussed framework-specific streaming plugins, FastFile mode has several benefits:</p>
<ul>
<li>Avoids any framework-specific implementations for data streaming. You can use your PyTorch or TensorFlow native data utilities and share datasets between frameworks. </li>
<li>As a result, you have more granular control over data inputs using your framework utilities to perform operations such as shuffling, dynamic boosting, and data processing <em class="italic">on the fly</em>.</li>
<li>There are no restrictions on the file format.</li>
<li>It is easier to debug your training program as you can use SageMaker Local mode to test and debug your program locally first.</li>
</ul>
<p>To use FastFile mode, you need to supply an appropriate <strong class="source-inline">input_mode</strong> value when configuring your SageMaker <strong class="source-inline">Estimator</strong> object. The following code shows an example of a TensorFlow training job:</p>
<pre class="source-code">
from sagemaker.tensorflow import TensorFlow
estimator = TensorFlow(
    entry_point="train.py",
    source_dir="3_sources",
    metric_definitions=metric_definitions,
    hyperparameters=hyperparameters,
    role=role,
    framework_version="1.15.2",
    py_version="py3",
    train_instance_count=1,
    input_mode="FastFile",
    train_instance_type="ml.p2.xlarge",
)</pre>
<p>FastFile mode can be a good starting choice, given its ease of use and versatility. If, for some reason, you are not happy with its performance, you can always consider tuning the configuration of your dataset (file format, file size, data processing pipeline, parallelism, and so on) or reimplement the use of one of the framework-specific implementations. It<a id="_idIndexMarker366"/> may also be a good idea to compare<a id="_idIndexMarker367"/> FastFile mode’s performance of streaming data from S3 using other methods such as Pipe mode and S3 Plugin for PyTorch.</p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor081"/>Summary</h1>
<p>In this chapter, we reviewed the available storage solutions for storing and managing DL datasets and discussed their pros and cons in detail, along with their usage scenarios. We walked through several examples of how to integrate your SageMaker training scripts with different storage services. Later, we learned about various optimization strategies for storing data and discussed advanced mechanisms for optimizing data retrieval for training tasks. We also looked into SageMaker Processing and how it can be used to scale your data processing efficiently.</p>
<p>This chapter closes the first part of this book, which served as an introduction to using DL models on SageMaker. Now, we will move on to advanced topics. In the next chapter, we will discuss the advanced training capabilities that SageMaker offers.</p>
</div>
</div>

<div id="sbo-rt-content"><div>
<div id="_idContainer024">
</div>
</div>
<div class="Content" id="_idContainer025">
<h1 id="_idParaDest-85"><a id="_idTextAnchor082"/>Part 2: Building and Training Deep Learning Models</h1>
<p>In this part, we will learn how to train DL models using SageMaker-managed capabilities, outlining available software frameworks to distribute training processes across many nodes, optimizing hardware utilization, and monitoring your training jobs.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B17519_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Considering Hardware for Deep Learning Training</em></li>
<li><a href="B17519_06.xhtml#_idTextAnchor097"><em class="italic">Chapter 6</em></a>, <em class="italic">Engineering Distributed Training</em></li>
<li><a href="B17519_07.xhtml#_idTextAnchor110"><em class="italic">Chapter 7</em></a>, <em class="italic">Operationalizing Deep Learning Training</em></li>
</ul>
</div>
<div>
<div id="_idContainer026">
</div>
</div>
</div></body></html>