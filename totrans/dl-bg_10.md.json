["```py\nfrom tensorflow.keras.layers import BatchNormalization\n...\nbn_layer = BatchNormalization()(prev_layer)\n...\n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Input\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nimport numpy as np\n\ninpt_dim = 512    #input dimensions\nltnt_dim = 256    #latent dimensions\n\n# -- the explanation for this will come later --\n(x_train, y_train), (x_test, y_test) = imdb.load_data()\nx_train = sequence.pad_sequences(x_train, maxlen=inpt_dim)\nx_test = sequence.pad_sequences(x_test, maxlen=inpt_dim)\n# ----------------------------------------------\n```", "```py\nx_train = x_train.astype('float32') \nx_test = x_test.astype('float32')\n\n# model with batch norm\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(ltnt_dim)(inpt_vec)        #dense layer followed by\nel2 = BatchNormalization()(el1)        #batch norm\nencoder = Activation('sigmoid')(el2)\ndecoder = Dense(inpt_dim, activation='sigmoid') (encoder)\nautoencoder = Model(inpt_vec, decoder)\n\n# compile and train model with bn\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\nautoencoder.fit(x_train, x_train, epochs=20, batch_size=64, \n                shuffle=True, validation_data=(x_test, x_test))\n```", "```py\n# model without batch normalization\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(ltnt_dim)(inpt_vec) #no batch norm after this\nencoder = Activation('sigmoid')(el1)\nlatent_ncdr = Model(inpt_vec, encoder)\ndecoder = Dense(inpt_dim, activation='sigmoid') (encoder)\nautoencoder = Model(inpt_vec, decoder)\n\n# compile and train model with bn\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\nautoencoder.fit(x_train, x_train, epochs=20, batch_size=64, \n                shuffle=True, validation_data=(x_test, x_test))\n```", "```py\nfrom tensorflow.keras.layers import Dropout\n...\n# encoder with dropout\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(inpt_dim/2)(inpt_vec)\n```", "```py\n\nel2 = Activation('relu')(el1)\nel3 = Dropout(0.1)(el2)\nel4 = Dense(ltnt_dim)(el3)\nencoder = Activation('relu')(el4)\n```", "```py\n# without dropout\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(inpt_dim/2)(inpt_vec)\nel2 = Activation('relu')(el1)\nel3 = Dense(ltnt_dim)(el2)\nencoder = Activation('relu')(el3)\n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Input\nfrom tensorflow.keras.layers import Dropout\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ninpt_dim = 512\nltnt_dim = 128\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data()\nx_train = sequence.pad_sequences(x_train, maxlen=inpt_dim)\nx_test = sequence.pad_sequences(x_test, maxlen=inpt_dim)\n\nx_train = x_train.astype('float32') \nx_test = x_test.astype('float32')\n```", "```py\n# with dropout\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(inpt_dim/2)(inpt_vec)\nel2 = Activation('relu')(el1)\nel3 = Dropout(0.1)(el2)\nel4 = Dense(ltnt_dim)(el3)\nencoder = Activation('relu')(el4)\n\n# model that takes input and encodes it into the latent space\nlatent_ncdr = Model(inpt_vec, encoder)\n\ndecoder = Dense(inpt_dim, activation='relu') (encoder)\n\n# model that takes input, encodes it, and decodes it\nautoencoder = Model(inpt_vec, decoder)\n```", "```py\nautoencoder.compile(loss='binary_crossentropy', optimizer='adagrad')\n\nhist = autoencoder.fit(x_train, x_train, epochs=100, batch_size=64, \n                       shuffle=True, validation_data=(x_test, x_test))\n\nbn_loss = hist.history['loss']\nbn_val_loss = hist.history['val_loss']\n\ndel autoencoder\ndel hist\n```", "```py\n# now without dropout\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(inpt_dim/2)(inpt_vec)\nel2 = Activation('relu')(el1)\nel3 = Dense(ltnt_dim)(el2)\nencoder = Activation('relu')(el3)\n\n# model that takes input and encodes it into the latent space\nlatent_ncdr = Model(inpt_vec, encoder)\n\ndecoder = Dense(inpt_dim, activation='relu') (encoder)\n\n# model that takes input, encodes it, and decodes it\nautoencoder = Model(inpt_vec, decoder)\n\nautoencoder.compile(loss='binary_crossentropy', optimizer='adagrad')\n\nhist = autoencoder.fit(x_train, x_train, epochs=100, batch_size=64, \n                       shuffle=True, validation_data=(x_test, x_test))\n```", "```py\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(bn_loss, color='#785ef0')\nplt.plot(bn_val_loss, color='#dc267f')\nplt.plot(loss, '--', color='#648fff')\nplt.plot(val_loss, '--', color='#fe6100')\nplt.title('Model reconstruction loss')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['With Drop Out - Training', \n            'With Drop Out - Validation', \n            'Without Drop Out - Training', \n            'Without Drop Out - Validation'], loc='upper right')\nplt.show()\n```", "```py\nimport numpy as np\nfrom tensorflow.keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\nprint('x_train shape is:', x_train.shape)\nprint('x_test shape is:', x_test.shape)\n```", "```py\nx_train shape is: (50000, 3072)\nx_test shape is: (10000, 3072)\n```", "```py\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.layers import BatchNormalization import matplotlib.pyplot as plt\nimport numpy as np\n\ninpt_dim = 32*32*3\nltnt_dim = 64\n\n# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n```", "```py\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(2048)(inpt_vec)\nel2 = Activation('relu')(el1)\nel3 = Dense(1024)(el2)\nel4 = BatchNormalization()(el3)\nel5 = Activation('relu')(el4)\nel6 = Dropout(0.2)(el5)\n\nel7 = Dense(512)(el6)\nel8 = Activation('relu')(el7)\nel9 = Dense(256)(el8)\nel10 = BatchNormalization()(el9)\nel11 = Activation('relu')(el10)\nel12 = Dropout(0.2)(el11)\n\nel13 = Dense(128)(el12)\nel14 = Activation('relu')(el13)\nel15 = Dropout(0.2)(el14)\nel16 = Dense(ltnt_dim)(el15)\nel17 = BatchNormalization()(el16)\nencoder = Activation('tanh')(el17)\n\n# model that takes input and encodes it into the latent space\nlatent_ncdr = Model(inpt_vec, encoder)\n```", "```py\ndl1 = Dense(128)(encoder)\ndl2 = BatchNormalization()(dl1)\ndl3 = Activation('relu')(dl2)\n\ndl4 = Dropout(0.2)(dl3)\ndl5 = Dense(256)(dl4)\ndl6 = Activation('relu')(dl5)\ndl7 = Dense(512)(dl6)\ndl8 = BatchNormalization()(dl7)\ndl9 = Activation('relu')(dl8)\n\ndl10 = Dropout(0.2)(dl9)\ndl11 = Dense(1024)(dl10)\ndl12 = Activation('relu')(dl11)\ndl13 = Dense(2048)(dl12)\ndl14 = BatchNormalization()(dl13)\ndl15 = Activation('relu')(dl14)\ndecoder = Dense(inpt_dim, activation='sigmoid') (dl15)\n```", "```py\n# model that takes input, encodes it, and decodes it\nautoencoder = Model(inpt_vec, decoder)\n\n# setup RMSprop optimizer\nopt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6, )\n\nautoencoder.compile(loss='binary_crossentropy', optimizer=opt)\n\nhist = autoencoder.fit(x_train, x_train, epochs=200, batch_size=10000, \n                       shuffle=True, validation_data=(x_test, x_test))\n\n# and now se visualize the results\nfig = plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], color='#dc267f')\nplt.title('Model reconstruction loss')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Test Set'], loc='upper right')\nplt.show()\n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input\nfrom tensorflow.keras.layers import BatchNormalization, MaxPooling1D\nimport numpy as np\n\ninpt_dim = 32*32*3\nltnt_dim = 64\n\ninpt_vec = Input(shape=(inpt_dim,))\nel1 = Dense(2048)(inpt_vec)\nel2 = Activation('relu')(el1)\nel3 = Dense(1024)(el2)\nel4 = BatchNormalization()(el3)\nel5 = Activation('relu')(el4)\nel6 = Dropout(0.2)(el5)\n\nel7 = Dense(512)(el6)\nel8 = Activation('relu')(el7)\nel9 = Dense(256)(el8)\nel10 = BatchNormalization()(el9)\nel11 = Activation('relu')(el10)\nel12 = Dropout(0.2)(el11)\n\nel13 = Dense(128)(el12)\nel14 = Activation('relu')(el13)\nel15 = Dropout(0.2)(el14)\nel16 = Dense(ltnt_dim)(el15)\nel17 = BatchNormalization()(el16)\nencoder = Activation('tanh')(el17)\n\n# model that takes input and encodes it into the latent space\nlatent_ncdr = Model(inpt_vec, encoder)\n```", "```py\ndl1 = Dense(128)(encoder)\ndl2 = BatchNormalization()(dl1)\ndl3 = Activation('relu')(dl2)\n\ndl4 = Dropout(0.2)(dl3)\ndl5 = Dense(256)(dl4)\ndl6 = Activation('relu')(dl5)\ndl7 = Dense(512)(dl6)\ndl8 = BatchNormalization()(dl7)\ndl9 = Activation('relu')(dl8)\n\ndl10 = Dropout(0.2)(dl9)\ndl11 = Dense(1024)(dl10)\ndl12 = Activation('relu')(dl11)\ndl13 = Dense(2048)(dl12)\ndl14 = BatchNormalization()(dl13)\ndl15 = Activation('relu')(dl14)\ndecoder = Dense(inpt_dim, activation='sigmoid') (dl15)\n\n# model that takes input, encodes it, and decodes it\nautoencoder = Model(inpt_vec, decoder)\n```", "```py\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n\nhist = autoencoder.fit(x_train, x_train, epochs=200, batch_size=5000, \n                       shuffle=True, validation_data=(x_test, x_test))\n```"]