<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Latent Space Interpolation with MusicVAE</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we'll learn about the importance of continuous latent space of <strong>Variational</strong> <strong>Autoencoders</strong> (<strong>VAEs</strong>) and its importance in music generation compared to standard <strong>Autoencoders</strong> (<strong>AEs</strong>). We'll use the MusicVAE model, a hierarchical recurrent VAE, from Magenta to sample sequences and then interpolate between them, effectively morphing smoothly from one to another. We'll then see how to add groove, or humanization, to an existing sequence using the GrooVAE model. We'll finish by looking at the TensorFlow code used to build the VAE model.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li class="mce-root">Continuous latent space in VAEs</li>
<li class="mce-root">Score transformation with MusicVAE and GrooVAE</li>
<li class="mce-root">Understanding TensorFlow code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li>A <strong>command line</strong> or <strong>bash</strong> to launch Magenta from the Terminal</li>
<li><strong>Python</strong> and its libraries to write music generation code using Magenta</li>
<li><strong>Magenta</strong> to generate music in MIDI</li>
<li><strong>MuseScore</strong> or <strong>FluidSynth</strong> to listen to the generated MIDI</li>
</ul>
<p class="mce-root"/>
<p>In Magenta, we'll make the use of the <strong>MusicVAE</strong> and <strong>GrooVAE</strong> models. We'll be explaining these models in depth, but if you feel like you need more information, the model's README in Magenta's source code (<a href="https://github.com/tensorflow/magenta/tree/master/magenta/models">github.com/tensorflow/magenta/tree/master/magenta/models</a>) is a good place to start. You can also take a look at Magenta's code, which is well documented. We also provide additional content in the last section, <em>Further reading</em>.</p>
<p>The code for this chapter is in this book's code GitHub in the <span><kbd>Chapter04</kbd> </span>folder, located at <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter04">github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter04</a>. The examples and code snippets will suppose you are located in this chapter's folder. For this chapter, you should go to <kbd>cd Chapter04</kbd> before you start.</p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/3176ylN">http://bit.ly/3176ylN</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous latent space in VAEs</h1>
                </header>
            
            <article>
                
<p>In <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with the Drums RNN</em>, we saw how we can use an RNN (LSTM) and a beam search to iteratively generate a sequence, by taking an input and then predicting, note by note, which next note is the most probable. That enabled us to use a primer as a basis for the generation, using it to set a starting melody or a certain key.</p>
<p>Using that technique is useful, but it has its limitations. What if we wanted to start with a primer and explore variations around it, and not just in a random way, but in a desired <strong>specific direction</strong>? For example, we could have a two-bars melody for a bass line, and we would like to hear how it sounds when played more as an arpeggio. Another example would be transitioning smoothly between two melodies. This is where the RNN models we previously saw fall short and where VAEs comes into play.</p>
<p>Before getting into the specifics of VAEs and how they are implemented in MusicVAE, let's first introduce standard AEs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The latent space in standard AEs</h1>
                </header>
            
            <article>
                
<p>An AE network is a pair of two connected networks, an <strong>encoder</strong> and a <strong>decoder</strong>, where the encoder produces an <strong>embedding</strong> from an input that the decoder will try to replicate. The embedding is a dense representation of the input, where useless features have been dropped, but is still representative enough so that the decoder can try and reproduce the input.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>What's the use of the encoder and decoder pair if the decoder merely tries to reproduce the input? Its main use is <strong>dimensionality reduction</strong>, where the input can be represented in a lower spatial resolution (with fewer dimensions) while still keeping its meaning. This forces the network to discover significant features to be encoded in the hidden layer nodes.</p>
<p>In the following diagram, we illustrate a VAE network, which is separated into three main parts—the hidden layer nodes (latent space or latent variables) in the middle, the encoder on the left, and the decoder on the right:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/043aa5d4-f3a3-414a-a0d3-edec46f86a10.png"/></p>
<p>Regarding the network training, the loss function, called <strong>reconstruction loss</strong>, is defined such as the network is penalized for creating outputs different from the input.</p>
<p>Generation is possible by instantiating the latent variables, which produces the embeddings, and then decoding that to produce a new output. Unfortunately, the learned latent space of the AE might not be continuous, which is a major shortcoming of that architecture, making its real-world usages limited. A latent space that is not continuous means that sampling a point at random might result in a vector that the decoder cannot make sense of. This is because the encoder hasn't learned how to handle that specific point and cannot generalize from its other learning.</p>
<p class="mce-root"/>
<p>In the following diagram, the black point marked by <span class="packt_screen">?</span> falls in such a space, meaning the encoder won't be able to reconstruct the input from it. This is an example visualization of samples of the latent space (for three classes), with the axis representing the first two dimensions of the latent space and the colors representing three classes, which shows the formation of distinct clusters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7aa4d654-12d4-4087-84a9-5d30b6b349c5.png" style="width:35.50em;height:31.33em;"/></p>
<p>This is fine if you are just replicating an input, but what if you want to sample from the latent space or interpolate between two inputs? In the diagram, you can see that the black data point (denoted with a question mark) falls in a region the decoder won't be able to make sense of. This is why the discontinuous latent space from AEs is a problem for our use case.</p>
<p>Now, let's see how a VAE solves that problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using VAEs in generating music</h1>
                </header>
            
            <article>
                
<p>There is one property in VAEs that makes them useful for generating music (or any generation) is that their latent space is <strong>continuous</strong>. To achieve that, the encoder doesn't output a vector, but rather two vectors: a vector of means called <strong>µ</strong> (mu) and a vector of standard deviations called <strong>σ</strong> (sigma). Therefore, latent variables, often called <strong>z</strong> by convention, follow a probability distribution of <em>P(z)</em>, often a Gaussian distribution.</p>
<p>In other words, the mean of the vector controls where the encoding of the input should be located and the standard deviation controls the size of the area around it, making the latent space continuous. Reusing the previous example, an example plot of the latent space, with the <em>x</em> and <em>y</em> axes representing its first two dimensions, for three classes represented by three colors, you can see the clusters now cover an area instead of being discrete:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5a6f6e91-8f39-4257-882a-209640b9c800.png" style="width:35.50em;height:31.08em;"/></p>
<p class="mce-root"/>
<p>Here is the VAE network, where you can see the change in the hidden layer with µ and σ:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c5991fc-730a-4dfa-a1fd-cc494dc6350c.png"/></p>
<p>This network architecture is very powerful for generating music and is often considered in a class of model called generative models. One property of that type of model is that the generation is stochastic, meaning that for a given input (and the same values of mean and standard deviation), the sampling will make the encoding vary a little for each pass.</p>
<p>This model has multiple properties that are really interesting for music generation, such as the following:</p>
<ul>
<li><strong>Expression</strong>: A musical sequence can be mapped to the latent space and reconstructed from it.</li>
<li><strong>Realism</strong>: Any point of the latent space represents a realistic example.</li>
<li><strong>Smoothness</strong>: Samples from nearby points are similar.</li>
</ul>
<p>We'll be explaining more on VAE in this chapter, but this minimal introduction is important to understand the code we're about to write.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Score transformation with MusicVAE and GrooVAE</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we've learned to generate various parts of a score. We've generated percussion and monophonic and polyphonic melodies and learned about expressive timing. This section builds on that foundation and shows how to manipulate the generated scores and transform them. In our example, we'll sample two small scores from the latent space, we'll then interpolate<span> </span><span>between the two samples</span><span> (progressively going from the first sample to the second sample), and finally, we'll add some groove (or</span> <strong>humanization</strong><span>, see the following information box for more information) on the resulting score.</span></p>
<p>For our example, we'll work on percussion since adding groove in MusicVAE only works on drums. We'll be using different configurations and pre-trained models in MusicVAE to perform the following steps. Remember, there are more pre-trained models in Magenta than we can present here (see the first section, <em>Technical requirements</em>, for a link to the README that contains all of them):</p>
<ul>
<li><strong>Sample:</strong> By using the <kbd>cat-drums_2bar_small</kbd> configuration and pre-trained model, we sample two different scores of two bars each. We could do the same thing for the melody by using the <kbd>cat-mel_2bar_big</kbd> configuration.</li>
<li><strong>Interpolate</strong>: By using the same configuration, we can interpolate between the two generated scores. What interpolation means is that it will progressively change the score, going from the first sample to the second. By asking a different number of outputs, we can decide how gradually we go between the two samples.</li>
<li><strong>Groove:</strong> By using the <kbd>groovae_2bar_humanize</kbd> configuration, we can then humanize the previous 16-bars sequence by adding groove.</li>
</ul>
<p class="mce-root"/>
<p>Here is a diagram explaining the different steps of our example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b4ce57e4-ba59-4e18-b043-070c5664a709.png"/></p>
<p>First, we'll be sampling <kbd>sample1</kbd> and <kbd>sample2</kbd> (2 bars each). Then, we'll ask the interpolation for 4 output sequences ("i1", "i2", "i3", and "i4") of 2 bars each. The resulting 6 output sequences of 12 bars will contain the 2 input sequences in both ends, plus the score progression of 6 sequences in between. Finally, we'll add groove to the whole sequence.</p>
<div class="packt_infobox">If you remember from the last chapter, in the <em>Performance music with the Performance RNN</em> section<span>, we introduced what</span> <strong>groove</strong><span> or</span> <strong>humanization</strong><span> is and how to generate sequences that feel less robotic. </span>This boils down to two things: expressive timing and dynamics. The former changes the timing of the notes so that they don't fall exactly on step boundaries, while the latter changes the force at which each note is played (its velocity) to emulate a human playing an instrument.</div>
<p>We'll be explaining more on these configurations as we go along. If you want to try out the examples for the melody instead of the percussion, follow the example by changing the mentions of <kbd>cat-drums_2bar_small</kbd> to <kbd>cat-mel_2bar_big</kbd>. We'll also be looking at other models, including the melody model, later in this chapter.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing the model</h1>
                </header>
            
            <article>
                
<p>Before sampling, interpolating, and grooving, we need to initialize the model that we're going to use. The first thing you'll notice is that MusicVAE doesn't have a similar interface to the previous chapters; it has its own interface and model definition. This means the code we've written up to now cannot be reused, except for some things such as MIDI and plot files handling.</p>
<div class="packt_tip">You can follow this example in the <kbd>chapter_04_example_01.py</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</div>
<p>The pre-trained <span>MusicVAE</span><span> </span><span>models are not packaged in bundles (the</span> <kbd>.mag</kbd><span> files) unlike in the previous chapters. A model and a configuration now correspond to a</span> <strong>checkpoint</strong><span>, which is slightly less expressive than bundles. We've already briefly explained what a checkpoint </span><span>is</span><span> </span><span>and we'll be looking into this in detail in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>,</span> <em>Training Magenta Models</em><span>. Just remember for now that checkpoints are used in TensorFlow to save the model state that occurs during training, making it easy to reload the model's state at a later time:</span></p>
<ol>
<li>Let's first make a <kbd>download_checkpoint</kbd> method that downloads a checkpoint corresponding to a model:</li>
</ol>
<pre style="padding-left: 60px"><span>import os<br/>import tensorflow as tf<br/>from six.moves import urllib<br/><br/>def </span>download_checkpoint(model_name: <span>str</span>,<br/>                        checkpoint_name: <span>str</span>,<br/>                        target_dir: <span>str</span>):<span><br/></span><span>  </span>tf.gfile.MakeDirs(target_dir)<br/>  checkpoint_target = os.path.join(target_dir, checkpoint_name)<br/>  <span>if not </span>os.path.exists(checkpoint_target):<br/>    response = urllib.request.urlopen(<br/>      f"https://storage.googleapis.com/magentadata/models/"<br/>      f"{model_name}/checkpoints/{checkpoint_name}")<br/>    data = response.read()<br/>    local_file = <span>open</span>(checkpoint_target, <span>'wb'</span>)<br/>    local_file.write(data)<br/>    local_file.close()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">You don't have to worry too much about the details of this method; basically, it downloads the checkpoint from online storage. It is analogous to the <kbd>download_bundle</kbd> <span>method</span><span> </span><span>from</span> <kbd>magenta.music.notebook_utils</kbd><span>, which we've been using in the previous chapters.</span></p>
<ol start="2">
<li>We can now write a <kbd>get_model</kbd> method that instantiates the MusicVAE model using the checkpoint:</li>
</ol>
<pre style="padding-left: 60px"><span>from magenta.models.music_vae import TrainedModel, configs<br/><br/>def </span>get_model(name: <span>str</span>):<span><br/></span><span>  </span>checkpoint = name + <span>".tar"<br/></span><span>  </span>download_checkpoint(<span>"music_vae"</span>, checkpoint, <span>"bundles"</span>)<br/>  <span>return </span><strong>TrainedModel</strong>(<br/>    <span># Removes the .lohl in some training checkpoints<br/>    # which shares the same config<br/></span><span>    </span><strong>configs.CONFIG_MAP</strong>[name.split(<span>"."</span>)[<span>0</span>] <span>if </span><span>"." </span><span>in </span>name <span>else </span>name]<br/>    <span># The batch size changes the number of sequences <br/>    # to be run together<br/></span><span>    </span><strong><span>batch_size</span></strong>=<span>8</span>,<br/>    <span>checkpoint_dir_or_path</span>=os.path.join(<span>"bundles"</span>, checkpoint))</pre>
<p>In this method, we first download the checkpoint for the given model name with our <kbd>download_checkpoint</kbd> method. Then, we instantiate the <kbd>TrainedModel</kbd> class from <kbd>magenta.models.music_vae</kbd> with the checkpoint, <kbd>batch_size=8</kbd>. This value defines how many sequences the model will process at the same time.</p>
<p>Having a batch size that's too big will result in wasted overhead; a batch size too small will result in multiple passes, probably making the whole code run slower. Unlike during training, the batch size doesn't need to be big. In this example, the sample uses two sequences, the interpolation two sequences, and the humanizing code six sequences, so if we wanted to nitpick, we could change <kbd>batch_size</kbd> to match each call.</p>
<p>For the first argument of <kbd>TrainedModel</kbd>, we pass an instance of <kbd>Config</kbd>. Each model corresponds to a configuration in the <kbd>models/music_vae/configs.py</kbd> file. If you look at the content of that file, you'll probably recognize some content we already saw. For example, let's take the configuration named <kbd>cat-drums_2bar_small</kbd> from <kbd>CONFIG_MAP</kbd>, which is the configuration we'll be using for sampling.</p>
<p class="mce-root"/>
<p>Now, follow the reference of the <kbd>data_converter</kbd><span> </span><span>attribute</span><span>, you'll end up in a class named </span><kbd>DrumsConverter</kbd> <span>in</span> <kbd>models.music_vae.data</kbd><span>. In the</span> <kbd>__init__</kbd> <span>method, you can see classes and methods we've already covered in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with the Drums RNN</em> for the DrumsRNN models, such as the</span> <kbd>MultiDrumOneHotEncoding</kbd> <span>class that we explained in the </span><span>section, </span><em>Encoding percussion events<span> </span><span>as classes</span></em><span>.</span></p>
<p>The MusicVAE code builds on the content we previously saw, adding a new layer that enables the conversion of note sequences to Tensforflow tensors. We'll be looking into the TensorFlow code in more detail in the <em>Understanding TensorFlow 2.0 code</em><strong> </strong>section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling the latent space</h1>
                </header>
            
            <article>
                
<p>Now that we can download and initialize our MusicVAE models, we can sample (analogous to generate) sequences. Remembering what we've learned from the previous section on VAEs, we know that we can sample any point in the latent space, by instantiating the latent variables corresponding to our probability distribution and then decoding the embeddings.</p>
<div class="packt_infobox">Until now, we've been using the term <strong>generate</strong> when speaking of creating a new sequence. That term refers to the generation algorithm we described in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with the Drums RNN</em>, and that was also used in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>.<br/>
<br/>
We're now using the term <strong>sample</strong> when speaking of creating a new sequence. This refers to the act of sampling (because we're effectively sampling a probability distribution) the latent space and differs from the generation algorithm we previously described.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the sampling code</h1>
                </header>
            
            <article>
                
<p>Let's now write the first method for our example, the <kbd>sample</kbd> method:</p>
<ol>
<li>First, let's define the method, which takes a model name as an input and returns a list of two generated <kbd>NoteSequence</kbd> objects:</li>
</ol>
<pre style="padding-left: 60px"><span>from typing import List<br/>from magenta.protobuf.music_pb2 import NoteSequence<br/><br/>from utils import save_midi, save_plot<br/><br/>def </span>sample(model_name: str,<br/>           num_steps_per_sample: int) -&gt; List[NoteSequence]:<br/><span>  </span>model = <strong>get_model</strong>(model_name)<br/><br/>  <span># Uses the model to sample 2 sequences</span><span><br/></span><span>  </span>sample_sequences = model.<strong>sample</strong>(n=2, length=num_steps_per_sample)<br/><br/>  <span># Saves the midi and the plot in the sample folder<br/></span><span>  </span><strong>save_midi</strong>(sample_sequences, <span>"sample"</span>, model_name)<br/>  <strong>save_plot</strong>(sample_sequences, <span>"sample"</span>, model_name)<br/><br/>  <span>return </span>sample_sequences</pre>
<p style="padding-left: 60px">In this method, we first instantiate the model using our previous <kbd>get_model</kbd> method. We then call the <kbd>sample</kbd> method, asking for <kbd>n=2</kbd> sequences that the method will return. We are keeping the default temperature (which is 1.0, for all models), but we can change it using the <kbd>temperature</kbd> parameter. Finally, we save the MIDI files and the plot files using the <kbd>save_midi</kbd> and <kbd>save_plot</kbd> <span>methods</span><span> </span><span>respectively, from the previous chapter, present in the <kbd>utils.py</kbd> file.</span></p>
<ol start="2">
<li>Let's call the sample method we created:</li>
</ol>
<pre style="padding-left: 60px">num_bar_per_sample = <span>2</span><span><br/></span>num_steps_per_sample = num_bar_per_sample * DEFAULT_STEPS_PER_BAR<br/>generated_sample_sequences = <strong>sample</strong>("cat-drums_2bar_small.lokl",<br/>                                    num_steps_per_sample)</pre>
<p style="padding-left: 60px">You might have noticed that the pre-trained model, <kbd>cat-drums_2bar_small.lokl</kbd>, has <kbd>.lokl</kbd> suffixed. There's also a <kbd>.hikl</kbd> model, which refers to the KL divergence during training. We'll be explaining that in the next section, <em>Refining the loss function with KL divergence</em>.</p>
<p style="padding-left: 60px">In the previous snippet, <kbd>num_bar_per_sample</kbd> and <kbd>num_steps_per_sample</kbd> define the number of bars and steps respectively for each sample. The configuration we are using, <kbd>cat-drums_2bar_small</kbd>, is a small 9 classes drum kit configuration, similar to the one we saw in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml"/><a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>. For our example, we'll use 32 steps (2 bars).</p>
<ol start="3">
<li>Let's open the <kbd>output/sample/music_vae_00_TIMESTAMP.html</kbd> file, changing <kbd>TIMESTAMP</kbd> for the printed value in the console. Here is the first generated sample we are going to work with:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/747b12cd-3237-4133-9049-a0412f20a9b7.png"/></p>
<p style="padding-left: 60px">Notice we've activated the velocity output in Visual MIDI, meaning the notes don't quite fill the whole vertical space because the default velocity in Magenta is 100 (remember MIDI values go from 0 to 127). Because we'll be adding groove later, we need to see the notes' velocity.</p>
<ol start="4">
<li>Let's open the <kbd>output/sample/music_vae_01_TIMESTAMP.html</kbd> file, changing <kbd>TIMESTAMP</kbd> for the printed value in the console. Here is the second sample:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9995a10-42e9-436f-85fd-59e2eb32a1b2.png"/></p>
<p class="mce-root"/>
<ol start="5">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synthesizer, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>
<p>We now have two 2 bar samples to work with; we'll be interpolating between them in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refining the loss function with KL divergence</h1>
                </header>
            
            <article>
                
<p>You might have noticed in the previous code snippet that the <kbd>cat-drums_2bar_small.lokl</kbd> <span>checkpoint</span><span> </span><span>we are using is suffixed with</span> <kbd>lokl.</kbd> <span>This is because this configuration has two different trained checkpoints:</span> <kbd>lokl</kbd><span> and</span> <kbd>hikl</kbd><span>. The first one has been trained for more realistic sampling, while the second one has been trained for better reconstruction and interpolation. We've used the first one in the previous code for sampling, and we'll use the second one in the next section for interpolation.</span></p>
<p>But what do <kbd>lokl</kbd> and <kbd>hikl</kbd> mean exactly? These refer to <strong>low</strong> or <strong>high</strong> <strong>Kulback</strong>-<strong>Leibler</strong> (<strong>KL</strong>) divergence. The KL divergence measures how much two probability distributions diverge from each other. Reusing our previous example, we can show that we want to minimize the KL divergence to achieve smoothness during interpolation:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f24e53ff-0e1e-482e-ba66-157e1c7e88ab.png" style="width:50.58em;height:24.25em;"/></p>
<p>This is an example visualization of samples of the latent space (for 3 classes), with the axis representing the first 2 dimensions of the latent space, and the colors representing 3 classes. On the left, we have encodings that are fairly close to one another, enabling smooth interpolation. On the right, we have clusters that are further apart, which means the interpolation will be harder but might result in a better sampling because the clusters are more distinct.<span class="underline"><br/></span></p>
<p>The KL loss function sums all the KL divergences with the standard normal. Alone, the KL loss results in a random cluster centered around the prior (a round blob around 0), which is not really useful by itself. By <strong>combining</strong> the reconstruction loss function and the KL loss function, we achieve clusters of similar encodings that are densely packed around the latent space origin.</p>
<p>You can look at the implementation of the model loss function in Magenta's code in the <kbd>MusicVAE</kbd> class, in the <kbd>magenta.models.music_vae</kbd> package, in the <kbd>_compute_model_loss</kbd> function.</p>
<p>During training, the KL divergence is tuned using the hyperparameters, <kbd>free_bits</kbd> and <kbd>max_beta</kbd>. By increasing the effect of the KL loss (which means decreasing <kbd>free_bits</kbd> or increasing <kbd>max_beta</kbd>), you'll have a model that produces better random samples but is worse at reconstruction.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling from the same area of the latent space</h1>
                </header>
            
            <article>
                
<p>What is interesting for sampling is that we can reuse the same <kbd>z</kbd> variable for each of the generated sequences in the same batch. That is useful for generating sequences from the same area of the latent space. For example, to generate 2 sequences of 64 steps (4 bars) using the same <kbd>z</kbd> variable for both, we would be using the following code:</p>
<pre>sample_sequences = model.sample(<span>n</span>=<span>2</span>, <span>length</span>=<span>64</span>, <strong><span>same_z</span></strong>=<span>True</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling from the command line</h1>
                </header>
            
            <article>
                
<p>You can also call the model sampling from the command line. The example from this section can be called using the following command line:</p>
<pre>&gt; <strong>curl</strong> <strong>--output "checkpoints/cat-drums_2bar_small.lokl.tar"</strong> "https://storage.googleapis.com/magentadata/models/music_vae/checkpoints/cat-drums_2bar_small.lokl.tar"<br/>&gt; <strong>music_vae_generate</strong> <strong>--config="cat-drums_2bar_small" --checkpoint_file="checkpoints/cat-drums_2bar_small.lokl.tar" --mode="sample" --num_outputs="2" --output_dir="output/sample"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpolating between two samples</h1>
                </header>
            
            <article>
                
<p>We now have 2 generated samples and we want to interpolate between the two of them, with 4 intermediate sequences in between, resulting in a continuous 6 sequences of 2 bars each, for a 12 bars total sequence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the sequence length right</h1>
                </header>
            
            <article>
                
<p>For our example, we used <kbd>length=32</kbd> when calling the <kbd>sample</kbd> method on the model, so the return of the method are sequences of 2 bars each. You should know that the sequence length is important in MusicVAE since each model works on different sequence lengths—<kbd>cat-drums_2bar_small</kbd> works on 2 bar sequences, while <kbd>hierdec-mel_16bar</kbd> works on 16 bar sequences.</p>
<p class="mce-root"/>
<p>When sampling, Magenta won't complain, because it can generate a longer sequence and then truncate it. But during interpolation, you'll end up with an exception like this, meaning that you haven't asked for the proper number of steps:</p>
<pre>Traceback (most recent call last):<br/>...<br/>  File "/home/Packt/miniconda3/envs/magenta/lib/python3.5/site-packages/magenta/models/music_vae/trained_model.py", line 224, in encode<br/>    (len(extracted_tensors.inputs), note_sequence))<br/>magenta.models.music_vae.trained_model.<strong>MultipleExtractedExamplesError: Multiple (2) examples extracted from NoteSequence</strong>: ticks_per_quarter: 220</pre>
<p>Exceptions in MusicVAE are especially cryptic and the encoder is quite finicky, so we'll try listing the common mistakes and their associated exception.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the interpolation code</h1>
                </header>
            
            <article>
                
<p>Let's now write the second method for our example, the <kbd>interpolate</kbd> method:</p>
<ol>
<li>First, let's define the method, which takes a list of two <kbd>NoteSequence</kbd> objects as an input and returns a 16 bar interpolated sequence:</li>
</ol>
<pre style="padding-left: 60px">import magenta.music as mm<br/><br/>def interpolate(model_name: str,<br/>                sample_sequences: List[NoteSequence],<br/>                num_steps_per_sample: int,<br/>                num_output: int,<br/>                total_bars: int) -&gt; NoteSequence:<br/>  model = <strong>get_model</strong>(model_name)<br/><br/>  <span># Use the model to interpolate between the 2 input sequences</span><span><br/></span>  interpolate_sequences = model.<strong>interpolate</strong>(<br/>      start_sequence=sample_sequences[0],<br/>      end_sequence=sample_sequences[1],<br/>      num_steps=num_output,<br/>      length=num_steps_per_sample)<br/><span><br/></span><span>  </span>save_midi(interpolate_sequences, <span>"interpolate"</span>, model_name)<br/>  save_plot(interpolate_sequences, <span>"interpolate"</span>, model_name)<br/><br/>  <span># Concatenates the resulting sequences into one</span><span> single sequence</span><span><br/></span><span>  </span>interpolate_sequence = mm.sequences_lib.<strong>concatenate_sequences</strong>(<br/>      interpolate_sequences, [<span>4</span>] * num_output)<br/><span><br/></span><span>  </span>save_midi(interpolate_sequence, <span>"merge"</span>, model_name)<br/>  save_plot(interpolate_sequence, <span>"merge"</span>, model_name,               <br/>            plot_max_length_bar=total_bars,<br/>            bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])<br/><br/>  <span>return </span>interpolate_sequence</pre>
<p style="padding-left: 60px">We first instantiate the model, then we call the <kbd>interpolate</kbd> method with the first and last sample using the parameters, <kbd>start_sequence</kbd> and <kbd>end_sequence</kbd> respectively, the number of output sequences of 6 using the <kbd>num_steps</kbd> <span>parameter</span><span> </span><span>(be careful it has nothing to do with the sequence length in steps) and the</span> <kbd>length</kbd> <span>parameter of 2 bars (in steps). The interpolation result is a list of six </span><kbd>NoteSequence</kbd> objects<span>, each of 2 bars.</span></p>
<p style="padding-left: 60px">We then concatenate the elements of the list to form a single <kbd>NoteSequence</kbd> object of 12 bars using <kbd>concatenate_sequences</kbd> from <kbd>magenta.music.sequence_lib</kbd>. The second argument (<kbd>[4] * num_output</kbd>) is a list containing the time in seconds of each element of the first argument. We should remember that this is necessary because <kbd>NoteSequence</kbd> doesn't define a start and an end, so a 2 bars sequence ending with silence concatenated with another sequence of 2 bars won't result in a 4 bars sequence.</p>
<p style="padding-left: 60px">When calling the <kbd>interpolate</kbd> method, a <kbd>NoExtractedExamplesError</kbd> exception could occur if the input sequences are not quantized or an input sequence is empty, for example. Remember you also have to ask for the proper length or you'll receive <kbd>MultipleExtractedExamplesError</kbd>.</p>
<ol start="2">
<li>We can then call the <kbd>interpolate</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">num_output = <span>6</span><span><br/></span>total_bars = num_output * num_bar_per_sample<br/>generated_interpolate_sequence = \<br/><strong>interpolate</strong>("cat-drums_2bar_small.hikl",<br/>             generated_sample_sequences,<br/>             num_steps_per_sample,<br/>             num_output,<br/>             total_bars)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="3">
<li>Let's open the <kbd>output/merge/music_vae_00_TIMESTAMP.html</kbd> file, changing <kbd>TIMESTAMP</kbd> for the printed value in the console. Corresponding to our samples, we have this interpolated sequence:<span class="underline"><br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/35e0d58a-f0b8-4279-af0e-dc1019573af4.png"/></p>
<p style="padding-left: 60px">We've marked every 2 bars with a different background alpha. You can locate the first sample we've generated in the previous section between 0 and 4 seconds, with a darker background. Then, 4 new interpolated chunks can be located between 4 and 20 seconds. Finally, you can see the second input sample on the right between 20 and 24 seconds.</p>
<ol start="4">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synth, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>
<p>Interpolating between two sequences is a hard problem, but MusicVAE does it well and the result in our example is quite impressive. You should try other generations with different lengths and listen to them.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpolating from the command line</h1>
                </header>
            
            <article>
                
<p>You can also call the interpolation from the command line. The example from this section can be called using the following command line (you'll need to download the checkpoint by yourself):</p>
<pre>&gt;<strong> curl --output "checkpoints/cat-drums_2bar_small.hikl.tar"</strong> "https://storage.googleapis.com/magentadata/models/music_vae/checkpoints/cat-drums_2bar_small.hikl.tar"<strong><br/>&gt; music_vae_generate</strong> <strong>--config="cat-drums_2bar_small" --checkpoint_file="checkpoints/cat-drums_2bar_small.hikl.tar" --mode="interpolate" --num_outputs="6" --output_dir="output/interpolate" --input_midi_1="output/sample/SAMPLE_1.mid" --input_midi_2="output/sample/SAMPLE_2.mid"</strong></pre>
<p>By changing the <kbd>SAMPLE_1.mid</kbd> and <kbd>SAMPLE_2.mid</kbd> file names for a previous sampled file from the previous sampling section, you'll be able to interpolate between the two sequences.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Humanizing the sequence</h1>
                </header>
            
            <article>
                
<p>Finally, we'll be adding humanization (or <strong>groove</strong>) to the generated sequence. The groove models are part of GrooVAE (pronounced <em>groovay</em>) and are present in MusicVAE's code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the humanizing code</h1>
                </header>
            
            <article>
                
<p>Let's now write the last method of our example, the <kbd>groove</kbd> method:</p>
<ol>
<li>First, let's define the method, which takes <kbd>NoteSequence</kbd> as input and returns a humanized sequence:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span>groove(model_name: <span>str</span>,<br/>           interpolate_sequence: NoteSequence,<br/>           num_steps_per_sample: <span>int</span>,<br/>           num_output: <span>int</span>,<br/>           total_bars: <span>int</span>) -&gt; NoteSequence:<br/>  model = <strong>get_model</strong>(model_name)<br/><br/>  <span># Split the sequences in chunks of 4 seconds</span><span><br/></span><span>  </span>split_interpolate_sequences = mm.sequences_lib.<strong>split_note_sequence</strong>(<br/>      interpolate_sequence, <span>4</span>)<br/><br/>  <span># Uses the model to encode the list of sequences</span><span><br/></span>  encoding, mu, sigma = model.<strong>encode</strong>(<br/>      note_sequences=split_interpolate_sequences)<br/><br/>  <span># Uses the model to decode the encoding</span><span><br/></span>  groove_sequences = model.<strong>decode</strong>(<br/>      z=encoding, length=num_steps_per_sample)<br/><span><br/></span><span>  </span>groove_sequence = mm.sequences_lib.<strong>concatenate_sequences</strong>(<br/>      groove_sequences, [<span>4</span>] * num_output)<br/><span><br/></span><span>  </span>save_midi(groove_sequence, <span>"groove"</span>, model_name)<br/>  save_plot(groove_sequence, "groove", model_name,<br/>            plot_max_length_bar=total_bars, show_velocity=True,<br/>            bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])<br/><br/>  <span>return </span>groove_sequence</pre>
<p class="mce-root" style="padding-left: 60px">First, we download the model. Then, we split the sequence in chunks of 4 seconds, because we need chunks of 2 bars for the model to handle. We then call the <kbd>encode</kbd> function, followed by the <kbd>decode</kbd> function. Unfortunately, there isn't a <kbd>groove</kbd> method on the model yet.</p>
<p style="padding-left: 60px">The <kbd>encode</kbd> method takes a list of sequence that it will encode, returning the <kbd>encoding</kbd> vector (also called z or latent vector), <kbd>mu</kbd> and <kbd>sigma</kbd>. We won't be using <kbd>mu</kbd> and <kbd>sigma</kbd> here but we left them for clarity. The resulting shape of the encoding array is <em>(6, 256)</em>, where 6 is the number of split sequences, and 256 is the encoding size that is defined in the model, explained in a later section, <em>Building the hidden layer</em>.</p>
<p style="padding-left: 60px">As for the <kbd>interpolate</kbd> method, the call to the <kbd>encode</kbd> method might throw an exception if the sequences are not properly formed.</p>
<p style="padding-left: 60px">Then, the <kbd>decode</kbd> method takes the previous <kbd>encoding</kbd> value and the number of steps per sample and tries to reproduce the input, resulting in a list of 6 humanized sequences of 2 bars each.</p>
<p style="padding-left: 60px">Finally, we concatenate the sequences like in the interpolate code snippet.</p>
<ol start="2">
<li>Let's try calling the <kbd>groove</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">generated_groove_sequence = <strong>groove</strong>("groovae_2bar_humanize",<br/>                                   generated_interpolate_sequence,<br/>                                   num_steps_per_sample,<br/>                                   num_output,<br/>                                   total_bars)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">The returned sequence, <kbd>generated_groove_sequence</kbd>, is our final sequence for this example.</p>
<ol start="3">
<li>Let's open the <kbd>output/groove/music_vae_00_TIMESTAMP.html</kbd> file, changing <kbd>TIMESTAMP</kbd> for the printed value in the console. Corresponding to our interpolated sequence, we have this humanized sequence:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/807d45f4-2bea-4dab-bf69-41b102ec67fc.png"/></p>
<p style="padding-left: 60px">Let's look at the resulting plot file. First, the notes' velocities are dynamic now, for example, with notes being played louder to mark the end or the start of a beat like a real drummer would do. You can see an example of that on the bass drum between the 20 and 24 seconds mark. Then, notice that the notes are played with expressive timing, meaning the notes do not fall exactly on steps beginning and end. Finally, some notes are not being played anymore, while others have been added to the resulting score.</p>
<ol start="4">
<li>To listen to the generated MIDI, use your software synthesizer but <strong>NOT MuseScore</strong> since it will have a hard time with the expressive timing and you might hear a different score than what you actually have. For the software synth, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<p>To learn more about groove and humanization, you can refer to the last section, <em>Further reading</em>, for more information on the topic, which is thoroughly explained in the GrooVAE blog post and GrooVAE paper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Humanizing from the command line</h1>
                </header>
            
            <article>
                
<p>Unfortunately, the humanization methods cannot be called from the command line for now. We will see other ways of humanizing a sequence in <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More interpolation on melodies</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we've been doing sampling and interpolation on drum sequences. By changing the code a bit, we can also do the same thing on melodies. Unfortunately, you won't be able to humanize the sequence since the GrooVAE model was trained on percussion data:</p>
<div class="packt_tip">You can follow this example in the <kbd>chapter_04_example_02.py</kbd> <span>file </span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</div>
<ol>
<li>To make that happen, we change the calling code and keep the <kbd>sample</kbd> and <kbd>interpolate</kbd> methods as they are. We'll generate a sightly longer sequence with 10 interpolations instead of 6. Here is the code (warning: the checkpoint size is 1.6 GB):</li>
</ol>
<pre style="padding-left: 60px">num_output = <span>10</span><span><br/>num_steps_per_sample = num_bar_per_sample * DEFAULT_STEPS_PER_BAR<br/>total_bars = num_output * num_bar_per_sample<br/></span><br/>generated_sample_sequences = sample(<span>"cat-mel_2bar_big"</span>,<br/>                                    num_steps_per_sample)<span><br/></span>interpolate(<span>"cat-mel_2bar_big"</span>,<br/>            generated_sample_sequences,<br/>            num_steps_per_sample,<br/>            num_output,<br/>            total_bars)</pre>
<p style="padding-left: 60px">You'll notice we're using the <kbd>cat-mel_2bar_big</kbd> configuration for both the sampling and the interpolation.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>Let's open the generated <kbd>output/merge/cat-mel_2bar_big_00_TIMESTAMP.html</kbd> file by replacing <kbd>TIMESTAMP</kbd> with the proper value. A generated output looks like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1b5e2e30-1541-4519-bcfb-06ee60a37e56.png"/></p>
<ol start="3">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synth, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling the whole band</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we've been sampling and interpolating for drums and melodies. Now, we'll sample a trio of percussion, melody, and bass at the same time using one of the bigger models. This is perhaps one of the most impressive models because it can generate rather long sequences of 16 bars at once, using multiple instruments that work well together:</p>
<div class="packt_tip">You can follow this example in the <kbd>chapter_04_example_03.py</kbd> <span>file</span><span> </span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</div>
<ol>
<li>For that example, we use our <kbd>sample</kbd> method with the <kbd>hierdec-trio_16bar</kbd> pre-trained model name as an argument (warning: the checkpoint size is 2.6GB):</li>
</ol>
<pre>sample(<span>"hierdec-trio_16bar"</span>, num_steps_per_sample)</pre>
<ol start="2">
<li>Let's open the generated <kbd>output/sample/hierdec-trio_16bar_00_TIMESTAMP.html</kbd> file by replacing <kbd>TIMESTAMP</kbd> with the proper value. A generated output looks like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/48f1a28e-ceb2-4d46-9184-a7ebebf7edfb.png"/></p>
<p style="padding-left: 60px">By using the <kbd>coloring=Coloring.INSTRUMENT</kbd> parameter in Visual MIDI, we can color each instrument with a separate color. It is hard to read because the bass line is on the same pitch as the drum line, but you can see the three instruments in the diagram.</p>
<ol start="3">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synth, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<p>You can hear that the generated MIDI has three instruments, and your synthesizer should assign a different instrument sound for each track (normally, a piano, a bass, and a drum). This is the only pre-trained model in Magenta that can generate multiple instruments at the same time, see the first section, <em>Technical requirements</em>, for a link to the README, which lists all of the available pre-trained models.</p>
<p>What is interesting in that model is that the long term structure of the 16 bars sequence is kept using a special type of decoder called <kbd>HierarchicalLstmDecoder</kbd>. That architecture adds another layer between the latent code and the decoder, called a <strong>conductor</strong>, which is an RNN that outputs a new embedding for each bar of the output. The decoder layer then proceeds to decode each bar.</p>
<p>To learn more about the hierarchical encoder and decoder architecture, you can refer to the last section, <em>Further reading</em>, for more information on the topic, which is thoroughly explained in the MusicVAE blog post and MusicVAE paper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of other pre-trained models</h1>
                </header>
            
            <article>
                
<p>We already saw many pre-trained models present in MusicVAE and there are some more that are interesting but cannot be covered in depth here. Remember you can find the full list of them in the README, see the first section, <em>Technical requirements</em>, for the link to it.</p>
<p>Here's an overview of some of them we find interesting:</p>
<ul>
<li>The <kbd>nade-drums_2bar_full</kbd> model is a drums pre-trained model similar to the one from our example, but using the 61 classes from General MIDI instead of 9 classes. The model is bigger though. You can see which classes are encoded and what they correspond to in the <kbd>data.py</kbd> file in the <kbd>magenta.models.music_vae</kbd> module.</li>
<li>The <kbd>groovae_2bar_tap_fixed_velocity</kbd> pre-trained model converts a "tap" pattern into a full-fledged drum rhythm while keeping the same groove. A "tap" sequence is a sequence that you could be taking from another rhythm, or even by tapping on your desk with your finger. In other words, it is a single note sequence, with groove, that can be transformed into a drum pattern. Usage of this would be to record a bass line from a real instrument, then "tap" the rhythm (or convert it from the audio), and then feed it to the network to sample a drum pattern that fits the same groove as the bass line.</li>
<li>The <kbd>groovae_2bar_add_closed_hh</kbd> pre-trained model adds or replaces hi-hat on an existing groove.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding TensorFlow code</h1>
                </header>
            
            <article>
                
<p>In this section, we'll take a quick look at the TensorFlow code to understand a bit more how the sampling, interpolating, and humanizing code works. This will also make references to the first section of this chapter, <em>Continuous latent space in VAEs</em>, so that we make sense of both the theory and the hands-on practice we've had.</p>
<p>But first, let's do an overview of the model's initialization code. For this section, we'll take the <kbd>cat-drums_2bar_small</kbd> configuration as an example and the same model initialization code we've been using for this chapter, meaning <kbd>batch_size</kbd> of 8.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the VAE graph</h1>
                </header>
            
            <article>
                
<p>We'll start by looking at the <kbd>TrainedModel</kbd> constructor in the <kbd>models.music_vae.trained_model</kbd> module. By taking the configuration values, <kbd>z_size</kbd>, <kbd>enc_rnn_size</kbd>, and <kbd>dec_rnn_size</kbd>, from the config map we've already introduced in a previous section, <em>Initializing the model</em>, we can find relevant information about the encoder's RNN, the hidden layer, and the decoder's RNN.</p>
<p>Notice the encoder is <kbd>BidirectionalLstmEncoder</kbd> and the decoder is <kbd>CategoricalLstmDecoder</kbd>, both from the <kbd>magenta.models.music_vae.lstm_models</kbd> module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an encoder with BidirectionalLstmEncoder</h1>
                </header>
            
            <article>
                
<p>Let's first have a look at the encoder's RNN, which is initialized in the <kbd>BidirectionalLstmEncoder</kbd> class of the <kbd>magenta.models.music_vae.lstm_models</kbd> module, in the <kbd>build</kbd> method, where the encoding layer gets initialized as follows:</p>
<pre>lstm_utils.rnn_cell(<br/>    [<strong>layer_size</strong>],<br/>    hparams.dropout_keep_prob,<br/>    hparams.residual_encoder,<br/>    is_training)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You can see in the <kbd>rnn_cell</kbd> method from the <kbd>magenta.models.music_vae.lstm_utils</kbd> module that the layer is <kbd>LSTMBlockCell</kbd> (from the <kbd>tensorflow.contrib.rnn</kbd>  module) with 512 units and a dropout wrapper:</p>
<pre>cell = rnn.<strong>LSTMBlockCell</strong>(rnn_cell_size[i])<br/>cell = rnn.DropoutWrapper(cell, <span>input_keep_prob</span>=dropout_keep_prob)</pre>
<p>In the <kbd>DrumsConverter</kbd> class from the <kbd>magenta.models.music_vae.data</kbd> module (instantiated in the <kbd>configs.py</kbd> file), you can see that we use the same <kbd>MutltiDrumOneHotEncoding</kbd> class that we explained in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>:</p>
<pre><span>self</span>._oh_encoder_decoder = mm.<strong>MultiDrumOneHotEncoding</strong>(<br/>    <span>drum_type_pitches</span>=[(i,) <span>for </span>i <span>in </span><span>range</span>(num_classes)])</pre>
<p>The melody configurations will use the <kbd>OneHotMelodyConverter</kbd> class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a decoder with CategoricalLstmDecoder</h1>
                </header>
            
            <article>
                
<p class="mce-root">Then, let's look at the decoder's RNN initialization in the <kbd>BaseLstmDecoder</kbd> class of the <kbd>magenta.models.music_vae.lstm_models</kbd> module, in the <kbd>build</kbd> method, where the decoding layer get initialized as follows:</p>
<pre><span>self</span>._output_layer = layers_core.<strong>Dense</strong>(<br/>    output_depth, <span>name</span>=<span>'output_projection'</span>)<br/><span>self</span>._dec_cell = lstm_utils.rnn_cell(<br/>    hparams.<strong>dec_rnn_size</strong>, hparams.dropout_keep_prob,<br/>    hparams.residual_decoder, is_training)</pre>
<p>Here, <kbd>output_depth</kbd> will be 512. The output layer is initialized as a dense layer, followed by 2 layers of <kbd>LSTMBlockCell</kbd> of 256 units each.</p>
<p>You can also find the information on the encoder and decoder of your current configuration in the console during execution:</p>
<pre>INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder:<br/>INFO:tensorflow:Encoder Cells (bidirectional): units: [512]<br/>INFO:tensorflow:Decoder Cells: units: [256, 256]</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the hidden layer</h1>
                </header>
            
            <article>
                
<p>Finally, the hidden layer initialization is in the <kbd>MusicVAE</kbd> class of the <kbd>magenta.models.music_vae.base_model</kbd> module, in the <kbd>encode</kbd> method:</p>
<pre><strong>mu</strong> = tf.layers.dense(<br/>    <strong>encoder_output</strong>,<br/>    z_size,<br/>    <span>name</span>=<span>'encoder/mu'</span>,<br/>    <span>kernel_initializer</span>=tf.random_normal_initializer(<span>stddev</span>=<span>0.001</span>))<br/><strong>sigma</strong> = tf.layers.dense(<br/>    <strong>encoder_output</strong>,<br/>    z_size,<br/>    <span>activation</span>=tf.nn.softplus,<br/>    <span>name</span>=<span>'encoder/sigma'</span>,<br/>    <span>kernel_initializer</span>=tf.random_normal_initializer(<span>stddev</span>=<span>0.001</span>))<br/><br/><span>return </span>ds.MultivariateNormalDiag(<span>loc</span>=mu, <span>scale_diag</span>=sigma)</pre>
<p>Both the <kbd>mu</kbd> and <kbd>sigma</kbd> layers are densely connected to the previous <kbd>encoder_output</kbd> value with a shape of <em>(8, 256)</em>, where 8 corresponds to <kbd>batch_size</kbd> and 256 corresponds to <kbd>z_size</kbd>. The method returns <kbd>MultivariateNormalDiag</kbd>, which is a normal distribution with <kbd>mu</kbd> and <kbd>sigma</kbd> as parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f66e73a4-effb-4f2d-9cd1-2b88c67b8611.png" style="width:36.08em;height:22.58em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at the sample method</h1>
                </header>
            
            <article>
                
<p>Let's now look at the <kbd>sample</kbd> method content, located in the <kbd>TrainedModel</kbd> class of the <kbd>models.music_vae.trained_model</kbd> module. The core of the method is as follows:</p>
<pre><span>for </span>_ <span>in </span><span>range</span>(<span>int</span>(np.ceil(n / batch_size))):<br/>  <span>if </span><span>self</span>._z_input <span>is not None and not </span>same_z:<br/>    feed_dict[<span>self</span>._z_input] = (<br/>        np.random.<strong>randn</strong>(batch_size, z_size).astype(np.float32))<br/>  outputs.append(<span>self</span>._sess.<strong>run</strong>(<span>self</span>._outputs, feed_dict))<br/>samples = np.vstack(outputs)[:n]</pre>
<p>The method will split the number of required samples, <kbd>n</kbd>, in batches of maximum <kbd>batch_size</kbd>, then sample <kbd>z_input</kbd> from the standard normal distribution of size <em>(8, 256)</em> using <kbd>randn</kbd>, and finally run the model using those values. Remember, <kbd>z</kbd> is the embedding, so essentially what we are doing here is instantiating the latent variables and then decoding them.</p>
<p class="mce-root">Remembering what we saw in the previous section, <em>Sampling from the same area of the latent space</em>, we know that <kbd>z</kbd> might be sampled only once if we are reusing the same <kbd>z</kbd> variable.</p>
<p>The samples are then converted back to sequences by calling the one-hot decoding of the samples:</p>
<pre><span>self</span>._config.data_converter.to_items(samples)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at the interpolate method</h1>
                </header>
            
            <article>
                
<p>The interpolate method, located in the <kbd>TrainedModel</kbd> class, is pretty short:</p>
<pre>_, mu, _ = <span>self</span>.<strong>encode</strong>([start_sequence, end_sequence], assert_same_length)<br/>z = np.array([<strong>_slerp</strong>(mu[<span>0</span>], mu[<span>1</span>], t)<br/>              <span>for </span>t <span>in </span>np.linspace(<span>0</span>, <span>1</span>, num_steps)])<br/><span>return </span><span>self</span>.<strong>decode</strong>(<br/>    <span>length</span>=length,<br/>    <span>z</span>=z,<br/>    <span>temperature</span>=temperature)</pre>
<p>What we are doing here is encoding the start and end sequence and getting back only the <kbd>mu</kbd> value from the encodings, using it to instantiate <kbd>z</kbd>, then decoding <kbd>z</kbd> for the resulting list of interpolated sequences.</p>
<p class="mce-root"/>
<p>But what is that <kbd>_slerp</kbd> method that instantiates <kbd>z</kbd>? Well, "slerp" stands for "spherical linear interpolation", and it calculates the direction between the first sequence and the second sequence so that the interpolation can move in the latent space in the proper direction.</p>
<p>We won't worry too much about the implementation details of the <kbd>slerp</kbd> method; we'll just remember the diagram from the section, <em>The latent space in standard autoencoders</em>, which showed how moving in a specific direction in the latent space would resulting in a transition from one sequence to another. By decoding at regular intervals along that direction, we end up with sequences that progressively goes from one to another.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at the groove method</h1>
                </header>
            
            <article>
                
<p>Finally, let's have a look at our <kbd>groove</kbd> method. As a reminder, the <kbd>groove</kbd> method is not present in Magenta so we had to write it ourselves:</p>
<pre>encoding, _, _ = model.<strong>encode</strong>(split_interpolate_sequences)<br/>groove_sequences = model.<strong>decode</strong>(encoding, num_steps_per_sample)</pre>
<p>Apart from variable naming, this code snippet is almost identical to the <kbd>interpolate</kbd> method, but instead of using the µ value to instantiate the latent variables to move in a direction, we're just encoding the sequences and then decoding them with the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at sampling, interpolating, and humanizing scores using a variational autoencoder with the MusicVAE and GrooVAE models.</p>
<p>We first explained what is latent space in AE and how dimensionality reduction is used in an encoder and decoder pair to force the network to learn important features during the training phase. We also learned about VAEs and their continuous latent space, making it possible to sample any point in the space as well as interpolate smoothly between two points, both very useful tools in music generation.</p>
<p>Then, we wrote code to sample and transform a sequence. We learned how to initialize a model from a pre-trained checkpoint, sample the latent space, interpolate between two sequences, and humanize a sequence. Along the way, we've learned important information on VAEs, such as the definition of the loss function and the KL divergence.</p>
<p class="mce-root"/>
<p>Finally, we looked at TensorFlow code to understand how the VAE graph is built. We showed the building code for the encoder, the decoder, and the hidden layer and explained the layers configurations and shapes. We also looked at the sample, interpolate, and groove methods, by explaining their implementations.</p>
<p>This chapter marks the end of the content aimed at models generating symbolic data. With the previous chapters, we've had a deep look at the most important models for generating and handling MIDI. The next chapter, <em>Audio Generation with NSynth and GANSynth</em>, will look at generating sub-symbolic content, such as audio.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the main use of the encoder and decoder pair in AE and what is a major shortcoming of such design?</li>
<li>How is the loss function defined in AE?</li>
<li>What is the main improvement in VAE on AE, and how is that achieved?</li>
<li>What is KL divergence and what is its impact on the loss function?</li>
<li>What is the code to sample <kbd>z</kbd> with a batch size of 4 and <kbd>z</kbd> size of 512?</li>
<li>What is the usage of the <strong>slerp</strong> method during interpolation?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>MusicVAE: Creating a palette for musical scores with machine learning</strong>: Magenta's team blog post on MusicVAE, explaining in more detail what we've seen in this chapter (<a href="https://magenta.tensorflow.org/music-vae">magenta.tensorflow.org/music-vae</a>)</li>
<li><strong>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</strong>: Magenta's team paper on MusicVAE, a very approachable and interesting read (<a href="https://arxiv.org/abs/1803.05428">arxiv.org/abs/1803.05428</a>)</li>
<li><strong>GrooVAE: Generating and Controlling Expressive Drum Performances</strong>: Magenta's team blog post on GrooveVAE, explaining in more detail what we've seen in this chapter (<a href="https://magenta.tensorflow.org/groovae">magenta.tensorflow.org/groovae</a>)</li>
<li><strong>Learning to Groove with Inverse Sequence Transformations</strong>: Magenta's team paper on GrooVAE, very approachable and interesting read (<a href="https://arxiv.org/abs/1905.06118">arxiv.org/abs/1905.06118</a>)</li>
<li><strong>Groove MIDI Dataset</strong>: The dataset used for the GrooVAE training, composed of 13.6 hours of aligned MIDI and synthesized audio (<a href="https://magenta.tensorflow.org/datasets/groove">magenta.tensorflow.org/datasets/groove</a>)</li>
<li><strong>Using Artificial Intelligence to Augment Human Intelligence</strong>: An interesting read on AI interfaces enabled by latent space type models (<a href="https://distill.pub/2017/aia/">distill.pub/2017/aia/</a>)</li>
<li><strong>Intuitively Understanding Variational Autoencoders</strong>: An intuitive introduction to VAE, very clear (<a href="https://www.topbots.com/intuitively-understanding-variational-autoencoders/">www.topbots.com/intuitively-understanding-variational-autoencoders/</a>)</li>
<li><strong>Tutorial - What is a variational autoencoder?</strong>: A more in-depth overview of VAEs (<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial">jaan.io/what-is-variational-autoencoder-vae-tutorial</a>)</li>
<li><strong>Autoencoders — Guide and Code in TensorFlow 2.0</strong>: Hands-on code for AE and VAE in TensorFlow 2.0 (<a href="https://medium.com/red-buffer/autoencoders-guide-and-code-in-tensorflow-2-0-a4101571ce56">medium.com/red-buffer/autoencoders-guide-and-code-in-tensorflow-2-0-a4101571ce56</a>)</li>
<li><strong>Kullback-Leibler Divergence Explained</strong>: The KL divergence explained from a statistical viewpoint (<a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained</a>)</li>
<li><strong>An Introduction to Variational Autoencoders</strong>: Good and complete paper on VAEs (<a href="https://arxiv.org/pdf/1906.02691.pdf">arxiv.org/pdf/1906.02691.pdf</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>