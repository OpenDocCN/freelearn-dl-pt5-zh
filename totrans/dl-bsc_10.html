<html><head></head><body>
		<div>
			<div id="_idContainer365" class="Content">
			</div>
		</div>
		<div id="_idContainer366" class="Content">
			<h1 id="_idParaDest-221"><a id="_idTextAnchor231"/>Appendix A</h1>
		</div>
		<div id="_idContainer367" class="Content">
			<h2><a id="_idTextAnchor232"/>About</h2>
			<p>This section is included to assist the students to perform the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the activity.</p>
		</div>
		<div id="_idContainer397" class="Content">
			<h2 id="_idParaDest-222"><a id="_idTextAnchor233"/>Computational Graph of the Softmax-with-Loss Layer</h2>
			<p>The following figure is the computational graph of the Softmax-with-Loss layer and obtains backward propagation. We will call the softmax function the Softmax layer, the cross-entropy error the <strong class="bold">Cross-Entropy Error</strong> layer, and the layer where these two are combined the Softmax-with-Loss layer. You can represent the Softmax-with-Loss layer with the computational graph provided in <em class="italics">Figure A.1</em>: Entropy:</p>
			<div>
				<div id="_idContainer368" class="IMG---Figure">
					<img src="image/Figure_A.1.jpg" alt="Figure A.1: Computational graph of the Softmax-with-Loss layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.1: Computational graph of the Softmax-with-Loss layer</h6>
			<p>The computational graph shown in <em class="italics">Figure A.1</em> assumes that there is a neural network that classifies three classes. The input from the previous layer is (a<span class="P---Subscript">1</span>, a<span class="P---Subscript">2</span>, a<span class="P---Subscript">3</span>), and the Softmax layer outputs (y<span class="P---Subscript">1</span>, y<span class="P---Subscript">2</span>, y<span class="P---Subscript">3</span>). The label is (t<span class="P---Subscript">1</span>, t<span class="P---Subscript">2</span>, t<span class="P---Subscript">3</span>) and the Cross-Entropy Error layer outputs the loss, L.</p>
			<p>This appendix shows that the result of backward propagation of the Softmax-with-Loss layer will be (<span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">1 </span>− <span lang="en-US" xml:lang="en-US">t</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">1</span>, <span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">2</span> − <span lang="en-US" xml:lang="en-US">t</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">2</span>, <span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">3</span> − <span lang="en-US" xml:lang="en-US">t</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">3</span>), as shown in <em class="italics">Figure A.1</em>.</p>
			<h3 id="_idParaDest-223"><a id="_idTextAnchor234"/>Forward Propagation</h3>
			<p>The computational graph shown in <em class="italics">Figure A.1</em> does not show the details of the Softmax layer and the Cross-Entropy Error layer. Here, we will start by describing the details of the two layers.</p>
			<p>First, let's look at the Softmax layer. We can represent the softmax function with the following equation:</p>
			<table id="table001-6" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer369">
									<img src="image/Figure_A.1a.png" alt="95"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(A.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Therefore, we can show the Softmax layer with the computational graph provided in <em class="italics">Figure A.2</em>. Here, S stands for the sum of exponentials, which is the denominator in equation (A.1). The final output is (<span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">1</span>, <span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">2</span>, <span lang="en-US" xml:lang="en-US">y</span><span class="P---Subscript" lang="en-US" xml:lang="en-US">3</span>).</p>
			<div>
				<div id="_idContainer370" class="IMG---Figure">
					<img src="image/Figure_A.2.jpg" alt="Figure A.2: Computational graph of the Softmax layer (forward propagation only)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.2: Computational graph of the Softmax layer (forward propagation only)</h6>
			<p>Next, let's look at the Cross-Entropy Error layer. The following equation shows the cross-entropy error:</p>
			<table id="table002-5" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer371">
									<img src="image/Figure_A.2a.png" alt="97"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(A.2)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Based on equation (A.2), we can draw the computational graph of the Cross-Entropy Error layer as shown in <em class="italics">Figure A.3</em>.</p>
			<p>The computational graph shown in <em class="italics">Figure A.3</em> just shows equation (A. 2) as a computational graph. Therefore, I think that there is nothing particularly difficult about this.</p>
			<div>
				<div id="_idContainer372" class="IMG---Figure">
					<img src="image/Figure_A.3.jpg" alt="Figure A.3: Computational graph of the Cross-Entropy Error layer (forward propagation only)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.3: Computational graph of the Cross-Entropy Error layer (forward propagation only)</h6>
			<p>Now, let's look at backward propagation:</p>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor235"/>Backward Propagation</h3>
			<p>First, let's look at backward propagation of the Cross-Entropy Error layer. We can draw backward propagation of the Cross-Entropy Error layer as follows:</p>
			<div>
				<div id="_idContainer373" class="IMG---Figure">
					<img src="image/Figure_A.4.jpg" alt="Figure A.4: Backward propagation of the Cross-Entropy Error layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.4: Backward propagation of the Cross-Entropy Error layer</h6>
			<p>Please note the following to obtain backward propagation of this computational graph:</p>
			<ul>
				<li>The initial value of backward propagation (the rightmost value of backward propagation in <em class="italics">Figure A.4</em>) is 1 (because <img src="image/Figure_A.4a.png" alt="98"/>).</li>
				<li>For backward propagation of the "x" node, the "reversed value" of the input signal for forward propagation multiplied by the derivative from the upper stream is passed downstream.</li>
				<li>For the "+" node, the derivative from the upper stream is passed without changing it.</li>
				<li>The backward propagation of the "log" node observes the following equations:<div id="_idContainer375" class="IMG---Figure"><img src="image/Figure_A.4b.jpg" alt="99"/></div></li>
			</ul>
			<p>Based on this, we can obtain backward propagation of the Cross-Entropy Error layer easily. As a result, the value <img src="image/Figure_A.4d.png" alt="100"/> will be the input to the backward propagation of the Softmax layer.</p>
			<p>Next, let's look at backward propagation of the Softmax layer. Because the Softmax layer is a little complicated, I want to check its backward propagation step by step:</p>
			<p><strong class="bold">Step 1:</strong></p>
			<div>
				<div id="_idContainer377" class="IMG---Figure">
					<img src="image/Figure_A.5.jpg" alt="Figure A.5: Step 1&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.5: Step 1</h6>
			<p>The values of the backward propagation arrive from the previous layer (Cross-Entropy Error layer).</p>
			<p><strong class="bold">Step 2:</strong></p>
			<div>
				<div id="_idContainer378" class="IMG---Figure">
					<img src="image/Figure_A.6.jpg" alt="Figure A.6: Step 2&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.6: Step 2</h6>
			<p>The "x" node "reverses" the values of forward propagation for multiplication. Here, the following calculation is performed:</p>
			<table id="table003-5" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer379">
									<img src="image/Figure_A.6a.png" alt="101"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>                    (A.3)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p><strong class="bold">Step 3:</strong></p>
			<div>
				<div id="_idContainer380" class="IMG---Figure">
					<img src="image/Figure_A.7.jpg" alt="Figure A.7: Step 3&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.7: Step 3</h6>
			<p>If the flow branches into multiple values in forward propagation, the separated values are added in backward propagation. Therefore, three separate values of backward propagation, <img src="image/Figure_A.7a.png" alt="102"/>, are added here. The backward propagation of <em class="italics">/</em> is conducted for the added values, resulting in <img src="image/Figure_A.7b.png" alt="103"/>. Here, (<em class="italics">t</em><span class="P---Subscript">1</span>, <em class="italics">t</em><span class="P---Subscript">2</span>, <em class="italics">t</em><span class="P---Subscript">3</span>) is the label and a "one-hot vector." A one-hot vector means that one of (<em class="italics">t</em><span class="P---Subscript">1</span>, <em class="italics">t</em><span class="P---Subscript">2</span>, <em class="italics">t</em><span class="P---Subscript">3</span>) is 1 and the others are all 0s. Therefore, the sum of (<em class="italics">t</em><span class="P---Subscript">1</span>, <em class="italics">t</em><span class="P---Subscript">2</span>, <em class="italics">t</em><span class="P---Subscript">3</span>) is 1.</p>
			<p><strong class="bold">Step 4:</strong></p>
			<div>
				<div id="_idContainer383" class="IMG---Figure">
					<img src="image/Figure_A.8.jpg" alt="Figure A.8: Step 4&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.8: Step 4</h6>
			<p>The "+" node only passes the value without changing it.</p>
			<p><strong class="bold">Step 5:</strong></p>
			<div>
				<div id="_idContainer384" class="IMG---Figure">
					<img src="image/Figure_A.9.jpg" alt="Figure A.9: Step 5&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.9: Step 5</h6>
			<p>The "x" node "reverses" the values for multiplication. Here, <img src="image/Figure_A.9a.png" alt="104"/> is used to transform the equation.</p>
			<p><strong class="bold">Step 6:</strong></p>
			<div>
				<div id="_idContainer386" class="IMG---Figure">
					<img src="image/Figure_A.10.jpg" alt="Figure A.10: Step 6&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.10: Step 6</h6>
			<p>In the "exp" node, the following equations hold true:</p>
			<table id="table004-4" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_A.10a.png" alt="105"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(A.4)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Thus, the sum of the two separate inputs, which are multiplied by exp(a<span class="P---Subscript">1</span>), is the backward propagation to obtain. We can write this as <img src="image/Figure_A.10c.png" alt="106"/> and obtain <img src="image/Figure_A.10d.png" alt="107"/> after transformation. Thus, in the node where the input of forward propagation is <img src="image/Figure_A.10e.png" alt="108"/>, backward propagation is <img src="image/Figure_A.10f.png" alt="109"/>. For <img src="image/Figure_A.10g.png" alt="110"/> and <img src="image/Figure_A.10h.png" alt="111"/>, we can use the same procedure (the results are <img src="image/Figure_A.10i.png" alt="112"/> and <img src="image/Figure_A.10j.png" alt="113"/>, respectively). With this, it is easy to show that we can achieve the same result even if we want to classify n classes instead of three classes.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor236"/>Summary</h2>
			<p>Here, the computational graph of the Softmax-with-Loss layer was shown in detail, and its backward propagation was obtained. <em class="italics">Figure A.11</em> shows the complete computational graph of the Softmax-with-Loss layer:</p>
			<div>
				<div id="_idContainer396" class="IMG---Figure">
					<img src="image/Figure_A.11.jpg" alt="Figure A.11: Computational graph of the Softmax-with-Loss layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure A.11: Computational graph of the Softmax-with-Loss layer</h6>
			<p>The computational graph shown in <em class="italics">Figure A.11</em> looks complicated. However, if you advance step by step using computational graphs, obtaining derivatives (the procedure of backward propagation) will be much less troublesome. When you encounter a layer that looks complicated (such as the Batch Normalization layer), other than the Softmax-with-Loss layer described here, you can use this procedure. This will be easier to understand in practice rather than only looking at equations.</p>
		</div>
	</body></html>