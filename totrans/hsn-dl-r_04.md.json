["```py\nsigmoid = function(x) {\n  1 / (1 + exp(-x))\n}\n```", "```py\nvals <- tibble(x = seq(-10, 10, 1), sigmoid_x = sigmoid(seq(-10, 10, 1)))\n```", "```py\np <- ggplot(vals, aes(x, sigmoid_x))\n```", "```py\np <- p + geom_point()\np + stat_function(fun = sigmoid, n = 1000)\n```", "```py\nvals <- tibble(x = seq(-10, 10, 1), tanh_x = tanh(seq(-10, 10, 1)))\n```", "```py\np <- ggplot(vals, aes(x, tanh_x))\n```", "```py\np <- p + geom_point()\np + stat_function(fun = tanh, n = 1000)\n```", "```py\nrelu <- function(x){dplyr::if_else(x > 0, x, 0)}\n```", "```py\nvals <- tibble(x = seq(-10, 10, 1), relu_x = relu(seq(-10, 10, 1)))\n```", "```py\np <- ggplot(vals, aes(x, relu_x))\np <- p + geom_point()\np + geom_line() \n```", "```py\nleaky_relu <- function(x,a){dplyr::if_else(x > 0, x, x*a)}\n```", "```py\nvals <- tibble(x = seq(-10, 10, 1), leaky_relu_x = leaky_relu(seq(-10, 10, 1),0.01))\n```", "```py\np <- ggplot(vals, aes(x, leaky_relu_x))\np <- p + geom_point()\np + geom_line() \n```", "```py\nswish <- function(x){x * sigmoid(x)}\n```", "```py\nvals <- tibble(x = seq(-10, 10, 1), swish_x = swish(seq(-10, 10, 1)))\n```", "```py\np <- ggplot(vals, aes(x, swish_x))\np <- p + geom_point()\np + geom_line()\n```", "```py\nsoftmax <- function(x) {exp(x) / sum(exp(x))}\n```", "```py\nresults <- softmax(c(2,3,6,9))\nresults\n\n[1] 0.0008658387 0.0023535935 0.0472731888 0.9495073791\n```", "```py\nsum(results)\n\n[1] 1\n```", "```py\nartificial_neuron <- function(input) {  as.vector(ifelse(input %*% weights > 0, 1, 0)) \n}\n```", "```py\nlinear_fits <- function(w, to_add = TRUE, line_type = 1) {curve(-w[1] / w[2] * x - w[3] / w[2], xlim = c(-1, 2), ylim = c(-1, 2), col = \"black\",lty = line_type, lwd = 2, xlab = \"Input Value A\", ylab = \"Input Value B\", add = to_add)}\n```", "```py\ninput <- matrix(c(1, 0,\n                 0, 0,\n                  1, 1,\n                  0, 1), ncol = 2, byrow = TRUE)  \ninput <- cbind(input, 1) \noutput <- c(0, 1, 0, 1)\nweights <- c(0.12, 0.18, 0.24)\nlearning_rate <- 0.2\n```", "```py\nlinear_fits(weights, to_add = FALSE)\npoints(input[ , 1:2], pch = (output + 21))\n```", "```py\nweights <- weights + learning_rate * (output[1] - artificial_neuron(input[1, ])) * input[1, ]\nlinear_fits(weights)\n```", "```py\nweights <- weights + learning_rate * (output[2] - artificial_neuron(input[2, ])) * input[2, ]\nlinear_fits(weights)\n```", "```py\nweights <- weights + learning_rate * (output[3] - artificial_neuron(input[3, ])) * input[3, ]\nlinear_fits(weights)\n```", "```py\nweights <- weights + learning_rate * (output[4] - artificial_neuron(input[4, ])) * input[4, ]\nlinear_fits(weights, line_type = 2)\n```", "```py\nlibrary(tidyverse)\n\nwbdc <- readr::read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", col_names = FALSE)\n```", "```py\nwbdc <- wbdc %>%\n  dplyr::mutate(target = dplyr::if_else(X2 == \"M\", 1, 0)) %>%\n  dplyr::select(-X2)\n```", "```py\nwbdc <- wbdc %>% dplyr::mutate_at(vars(-X1, -target), funs((. - min(.))/(max(.) - min(.)) ))\n```", "```py\ntrain <- wbdc %>% dplyr::sample_frac(.8)\ntest  <- dplyr::anti_join(wbdc, train, by = 'X1')\n\ntest <- test %>% dplyr::select(-X1)\ntrain <- train  %>% dplyr::select(-X1)\n```", "```py\nactual <- test$target\ntest <- test %>% dplyr::select(-target)\n```", "```py\nn <- names(train)\nformula <- as.formula(paste(\"target ~\", paste(n[!n == \"target\"], collapse = \" + \", sep = \"\")))\n```", "```py\nnet <- neuralnet::neuralnet(formula,\n                 data = train,\n                 hidden = c(15,15),\n                 linear.output = FALSE,\n                 act.fct = \"logistic\"\n                 )\n```", "```py\nprediction_list <- neuralnet::compute(net, test)\n```", "```py\npredictions <- as.vector(prediction_list$net.result)\nbinary_predictions <- dplyr::if_else(predictions > 0.5, 1, 0)\n```", "```py\nsum(binary_predictions == actual)/length(actual)\n```", "```py\nresults_table <- table(binary_predictions, actual)\nlibrary(caret) \ncaret::confusionMatrix(results_table)\n\n# Confusion Matrix and Statistics\n# \n# actual\n# binary_predictions  0  1\n# 0 67  2\n# 1  1 44\n# \n# Accuracy : 0.9737         \n# 95% CI : (0.925, 0.9945)\n# No Information Rate : 0.5965         \n# P-Value [Acc > NIR] : <2e-16         \n# \n# Kappa : 0.9451         \n# Mcnemar's Test P-Value : 1              \n# \n# Sensitivity : 0.9853         \n# Specificity : 0.9565         \n# Pos Pred Value : 0.9710         \n# Neg Pred Value : 0.9778         \n# Prevalence : 0.5965         \n# Detection Rate : 0.5877         \n# Detection Prevalence : 0.6053         \n# Balanced Accuracy : 0.9709         \n# \n# 'Positive' Class : 0        \n```", "```py\nlibrary(Metrics)\nMetrics::auc(actual, predictions)\n```", "```py\nbp_net <- neuralnet::neuralnet(formula,\n                    data = train,\n                    hidden = c(15,15),\n                    linear.output = FALSE,\n                    act.fct = \"logistic\",\n                    algorithm = \"backprop\",\n                    learningrate = 0.00001,\n                    threshold = 0.3,\n                    stepmax = 1e6\n)\n```", "```py\nprediction_list <- neuralnet::compute(bp_net, test)\npredictions <- as.vector(prediction_list$net.result)\nbinary_predictions <- dplyr::if_else(predictions > 0.5, 1, 0)\nresults_table <- table(binary_predictions, actual)\ncaret::confusionMatrix(results_table)\n```", "```py\nMetrics::auc(actual, predictions)\n```"]