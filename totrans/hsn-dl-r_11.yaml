- en: Generative Adversarial Networks for Faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we used a **long short-term memory** (**LSTM**) model on
    a time-series forecasting task. In this chapter, we will create a generator model,
    which means the model will not output predictions but rather files (in this case,
    images). We created a generator model in [Chapter 7](a000fdb8-7d3e-436b-abd9-9fbc9b3c33f0.xhtml), *Deep
    Learning for Natural Language Processing*; however, in that case, we just generated
    latent features. Here, we will describe the main components and applications of
    **generative adversarial networks** (**GANs**). You will learn about the common
    applications of GANs and how to build a face generation model using a GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we will investigate the architecture of a GAN.
    A GAN is composed of two competing neural networks, one of which is known as the
    **generator model**. It takes random data and creates synthetic target data. The
    other part of a GAN is the **discriminator model**. This model takes two pieces
    of input—the synthetic target data and the real target data—and it determines
    which is the real target data. After understanding this process, we will code
    our own GAN model for face recognition and generation using the Keras package
    and images from the *labeled faces in the wild* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the generator model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the discriminator model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing and preprocessing a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code files used in this chapter at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A GAN is a modeling algorithm that pits two neural networks against each other.
    One of them uses random data to create output. The other evaluates the real target
    data and the generated output and determines which is real. Over time, the first
    neural network creates better fake target data and the second neural network continues
    to try and determine which is the real target data. The two neural networks continue
    to compete and the models both improve to create increasingly realistic synthetic
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the term, we can see how this modeling technique differs from
    others. First, it is generative, which means that the goal is to generate data.
    This is in contrast to other models, such as classification or regression, that
    predict probabilities or values. Next, it is adversarial. That is, there are two
    models that are set to compete against each other. Generally, we have one model
    and it trains on data that can be evaluated and improved using a variety of metrics.
    However, in this case, we have one model that seeks to improve prediction performance
    and that is the discriminator model. In addition, we have another model that creates
    fake images to try to reduce the performance of the discriminator model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We generally think of two main categories for machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: Where the model uses labeled target data to make predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: Where the model identifies patterns without any
    labeled target data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we can get even more granular with unsupervised learning. GANs belong
    to a special subset of unsupervised learning that uses learned patterns from unlabeled
    data to generate synthetic data, rather than just classifying the data. However,
    this introduces a problem for us. Since the goal is to generate data, there are
    no direct metrics we can use to evaluate the performance. The relative success
    or failure of a GAN model is based largely on the subjective interpretation of
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this GAN, we will use images as our input. All images can be represented
    as a matrix of values for grayscale images, or three matrices of values for color
    images. The values of the matrix range from `0` to `255` and correspond with the
    intensity of the pixel value at that location. For example, a pixel value of `255`
    means high intensity or black for a grayscale image and a value of `0` means low
    intensity or white. The dimensions of the matrix correspond with the pixel width
    and height of the image. Color images are represented as a three-dimensional array.
    This can be thought of as three matrices overlapping, with each corresponding
    to the intensity of the red, green, and blue pixel values for the image. Let''s
    take a look at a sample image and see how it is represented as a matrix of values.
    To do this, we will read in the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87a19f79-255f-4257-8689-28a682e5f076.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To read in this shape, we will use the `OpenImageR` package. This package reads
    the file in as a four-dimensional array. In this case, we only want the fourth
    dimension, which contains the grayscale values. To read in this file and then
    look at a small segment, we run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will see the following printed to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f0f973f-de91-4e56-b295-b0d5e639b42b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see how these values represent the top of the left bell on the alarm
    clock. The zeroes are the white space and we see a gradient in the second rows
    and the top rows, showing the curve. In this way, we can see how images can be
    expressed as a matrix of values between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the generator model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator model is the neural network that creates synthetic target data
    out of random inputs. In this case, we will use a **convolutional neural network**
    (**CNN**) in reverse. What this means is that we will start with a vector of data
    points and create a fully connected layer, then reshape the data into the size
    that we want it to be. As a middle step, we will make the target shape only half
    the size and then we will upsample using a transposed convolution layer. In the
    end, we have an array of normalized pixel values that is the same shape as our
    target array. This then becomes the data object that will be used to try to fool
    the discriminator model. This array of synthetic values will, over time, be trained
    to resemble the values in the target data object so that the discriminator model
    cannot predict, with a high probability, which is the true data image. We will
    define the discriminator model using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will define that our entry point will be a 100-dimensional vector.
    Everything we do to define our models will be done with Keras. So, we load the
    Keras model at this step. We then define our input shape as a vector with 100
    values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we will pass a vector to this model. In this step, we tell the model
    what we will be passing in the later step. Using the following code, we declare
    that the input to this model will be a vector with 100 values that we will later
    populate with random values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this step, we can see that we have a special data object, called
    `Tensor`, in our data environment. The object contains the type, name, shape,
    and data type of the layer. Your data environment will look as in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b1b1edff-a48c-4d71-9b2c-596f14425005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After this, we define how our random values will be processed, transformed,
    and reshaped to create a synthetic array that matches our target arrays. The code
    to do this is long, but many of the parts are repeated. There are a few lines
    that are required while others can be modified. The `layer_dense` layer needs
    to contain the number of units that will appear later in the `layer_reshape` layer.
    In this case, we will create a shape that has a width and height of `25` and a
    depth of `128`. The depth is modifiable; however, the width and height must be
    set at half the size of the final image''s dimensions when using one transposed
    convolution layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `layer_conv_2d_transpose` layer uses a 2 x 2 stride to upsample and double
    the shape of the layer. In this step, the shape changes from 25 x 25 to 50 x 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The convolution applies filters that look for patterns and the normalization
    takes the results of the convolution step and normalizes the values. So the mean
    is close to 0 and the standard deviation is close to 1, and ReLU is used as our
    activation function. We will add these layers after our dense layer and our convolution
    layer using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we can continue to add additional convolution layers using the
    same pattern of convolution, normalization, and activation. Here, we will add
    four additional series of layers using the pattern we just described:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the very last step, the `filters` argument needs to be set to the number
    of channels for the image—in this case, three for the red, green, and blue channels
    of a color image. This completes the definition of our generator model. The entire
    generator model is defined using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will now see two objects in our environment. We
    have defined the connected tensors for input and the connected input for the output.
    Setting up our tensors in this way allows data to be input in batches using the
    `keras_model` function. Your data environment should look like the following now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e3087f68-2aa2-43f0-a2f2-8165b8996a54.png)'
  prefs: []
  type: TYPE_IMG
- en: After, we define that the input will be 100 random values and the output will
    be random values mapped to a data object with the same dimensions as our target
    image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then define `keras_model`, which takes the input and output as arguments,
    specifically. We pass in these defined tensor layers, at this point, to complete
    the definition of our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After defining the model, we can run the `summary` function on the generator
    model to helpfully see what is happening to the data at each layer. We define
    our generator and view the summary using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the `summary` function, we will see details about our model printed
    to our console, which looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dbfce84e-5c01-4adc-a852-90182cc49caf.png)'
  prefs: []
  type: TYPE_IMG
- en: From the console output, we can see that we start with one fully connected layer
    and after numerous intermediate layers, we end up with a final layer that matches
    the shape of our target image data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now have our generator completely defined. We have seen how we can insert
    random values and how those random values are then transformed to produce a synthetic
    image. The process of passing data to this model occurs later in the process.
    With a system in place to produce fake images, we now move on to defining the
    discriminator model, which will determine whether a given array of pixel data
    is a real or fake image.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the discriminator model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The discriminator model is the neural network that evaluates the synthetic target
    data and the real target data to determine which is the real one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The discriminator, in this case, is a CNN model; it takes a three-dimensional
    array as input. Often with CNNs, convolving layers and pooling layers are used
    to reshape the dimensions of the input—ultimately, to a fully connected layer.
    However, when using these layers to define a discriminator model in the context
    of creating a GAN, we instead use 2 x 2 strides in the convolving layers to reshape
    the input dimensions. In the end, a fully connected layer with one unit is passed
    through the sigmoid activation function to calculate the probability that a given
    input is real or fake. Let''s follow the following lines of code to define the
    discriminator model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the generator model, let''s start by defining the input shape.
    While the generator model started with a vector of 100 random values, our discriminator
    starts with input in the shape of our image data as that is what will be passed
    to the model. The dimensions for the image are used as arguments to define the
    input shape using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code adds another object to our data environment. Your Environment
    pane will now look as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f436752-c3cf-4c52-9257-e6e721ded803.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we process and transform our data. While the generator took a one-dimensional
    vector and created a three-dimensional array in the size of our image data, here
    we will do the opposite. We start with data that is in the shape of our image
    data as the first layer, as in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next layer that we will add to the discriminator is an activation layer.
    For the discriminator, we will use a Leaky ReLU activation function. The activation
    layer is added after our first convolution layer so that our code now looks like
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In our next convolution layer, we use strides of `2` to halve the height and
    width while doubling the depth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now continue to add more layers, using the same sequence of convolution
    layers, to the Leaky ReLU activation layer. The constraint is that—as mentioned—at
    each layer, the height and width are halved and the depth is doubled, so the height
    and width dimensions need to be such that they can be halved, with the output
    containing whole numbers, or you will receive an error. In our case, we will add
    three more sequences of layers so that our code now looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to add a layer to flatten our values to one dimension in preparation
    for our final output layer. When we add this layer, our code will look like the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we add a `dropout` layer that removes some data at random, which
    forces the model to work harder and slows down training, which produces a better
    generalizer. Adding this layer results in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we add a `dense` layer with just `1` unit, representing the probability
    that an image is real or fake. Adding this last layer will complete our discriminator
    model. The final discriminator model is defined with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, there are now four defined tensors in our data environment.
    Your data environment will look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15ca4d53-c985-4a9a-86ce-ea978a3f6b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After defining the input and output, both objects are passed as arguments to
    the `keras_model` function, as before, with the generator model. We define the
    discriminator model using the input and output definitions from the previous steps
    and then run the `summary` function to see the details of the model, using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, you will have details about the model printed
    to your console. The output to your console will look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdeb4ba8-60d2-492d-9077-403ea27f4b69.png)'
  prefs: []
  type: TYPE_IMG
- en: To view the details of our model, we can see the dimensions shifting as the
    input passes through convolution layers. We start with the input in the shape
    of our image data and at each layer, two dimensions are reduced while the third
    dimension is increased. In the end, we have one fully connected layer. We see
    that if we had added a few more convolution layers, we would have gotten to a
    point where we could no longer halve our data and still have a whole unit remaining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During this step, we will also define our optimizer, which is how the model
    will pass data back to improve future iterations of the model. We will calculate
    performance using `binary_crossentropy` and then use the `adam` optimizer to feed
    data back to the model from the error rate gradients. We define how we evaluate
    and incrementally improve our discriminator model using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We now have our generator model and discriminator defined. These are the two
    main building blocks for our GAN. In the next step, we will load in the real images
    and show you how to convert the images to numeric data. This is the third and
    final piece that we need before we assemble everything together to train our GAN
    and begin generating synthetic images.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and preprocessing a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will use a small subset of images from the labeled faces
    in the wild dataset. Specifically, we will use images of former United States
    president George W. Bush, since this is the image object that occurs most often
    in the dataset. Using the following code, we will bring in the image data and
    convert it to a format that can be inputted into our model. We start by loading
    the libraries and data files required.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries and data files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load all the libraries that we will use. We will use just one function
    from each of these libraries but we need each one to get our data in the proper
    format. The `jpeg` library will be used to read in the image data and store it
    as a matrix. The `purrr` package will be used to apply a function to our list
    of arrays. The `abind` package will be used to convert the list of arrays into
    one array. Finally, `OpenImageR` will be used to resize our data. We load all
    the libraries needed to bring in images and convert them to the proper format
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After loading the libraries, the next step is to bring over all the image files.
    The first step in this process is to change the working directory, for convenience,
    to the folder containing all the image files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have navigated to this folder, use the `list.files` function to bring
    over a vector of all the filenames. Lastly, we use the `map()` function from the
    `purrr` package to perform functions on every element in our vector and pass the
    results to a list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this case, every element in our vector is a file path. We pass each file
    path as an argument to the `readJPEG` function from the `jpeg` package. This function
    returns an array for every image, with all the pixel values represented as normalized
    values between `0` and `1`. This is convenient because this is the format we want
    for our neural networks. As noted before, the pixel values are ordinarily stored
    as integers between `0` and `255`; however, values bound between `0` and `1` work
    better when planning to pass data through a neural network. We import our images,
    convert all the pixel values to normalized values between `0` and `1`, and store
    all the formatted image data in a list of arrays using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we now have the list of arrays in our data environment.
    If we expand the object, we can see a sample of pixel values for the images in
    this set. After expanding the data object in your environment, it will look as
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/691d240f-f9c3-4e69-9a66-26e846a541a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Resizing our images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will resize the images using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This step is done for the purposes of this book to speed up the model execution
    time. In a real-world example, this step may not be necessary or desirable. However,
    knowing how to resize images is helpful in any case. We can resize every image
    by once again using the `map` function from the `purrr` package and also the `resizeImage`
    function from the `OpenImageR` package. In this case, `map` takes every element
    from the `image_list` object and passes it as an argument through the `resizeImage`
    function. So, every array will change from having dimensions of 250 x 250 to 50
    x 50\. We resize every image by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we can see the dimensions of our images have changed.
    If `image_list` is still expanded in the data Environment pane, then it will now
    look as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f5a164a7-3a67-43fe-ad90-4410f8363095.png)'
  prefs: []
  type: TYPE_IMG
- en: Merging arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we are done resizing, we will start merging the arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After our images are resized, there is just one last step to get the data in
    the proper format. The data is currently stored in a list of arrays; however,
    we need the data to all be in one four-dimensional array. The following code takes
    all our three-dimensional arrays and combines them along a new fourth dimension.
    We can combine all of our three-dimensional arrays into one four-dimensional array
    and then view the dimensions using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will print details about our image dimensions to the console, so
    we can now see the new shape of our four-dimensional array and how the fourth
    dimension corresponds with the number of image objects we have. You will see the
    following printed to your console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fde30a9a-28fa-4888-8a89-90c90ba806ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our data is now in the proper format and we can just do two last clean-up steps.
    We remove the list of arrays and the vector of file path names, since we no longer
    need these, and reset our working directory back to the root for our project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we have all the objects that we need to begin assembling
    our GAN model. Your data environment will look as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9aba0a85-13f3-4f1e-99bf-5413760b9499.png)'
  prefs: []
  type: TYPE_IMG
- en: With the data now imported to the environment and converted into the proper
    format, we are now ready to put everything together to create our GAN model. The
    data we just loaded is used with the discriminator model, along with the array
    created by the generator model. We will now write the code that combines the data,
    generator, and discriminator to create our GAN model.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the data is in the proper format and we have our discriminator and
    generator defined, we can put it all together to train our GAN. The final GAN
    model takes input from our target image dataset and the output is the probability
    that this is a real image after the real image data and the fake image data have
    been passed as input to the discriminator. We train our GAN model by running the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the GAN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We define the GAN model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step that we will perform is calling the `freeze_weights` function
    on the discriminator model. This is so that the weights for the discriminator
    model do not update during the training process. We want the weights to update
    for the generator and not for the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the input and output for `keras_model`, as we did
    with the generator and the discriminator. In this case, `keras_model` will be
    our final GAN model. Note here that the input will contain 100 values, which is
    the same as our generator, since the input to our GAN model will pass through
    our generator model and then continue through to our discriminator model, which
    will then produce the output for our model. We define the GAN model using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we now have the following objects in our data environment.
    We can see in all the details about the different tensor layers the path of the
    data through the entire GAN model pipeline. Your Environment pane will look like
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c9a7ab3f-43a1-4c3c-af50-c7e686700261.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the discriminator model, we need to define the compile step. We
    set it up in the same way that we did with the discriminator. The error is computed
    using the `binary_crossentropy` loss function and `adam` is used to iteratively
    improve the model. Defining how the final GAN model is compiled is done using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Passing data to the GAN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will pass data to the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, our model is ready and we can begin to pass data through to generate
    synthetic images. In order to store these data objects, we will need to create
    a directory within our project folder. We create a directory to hold our real
    and fake images by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: After running this code, you will see a new folder in your main project folder
    with the name `gan_images`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the GAN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have prepared our model, it is time to train it, using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training our GAN model is an iterative process and we will need to create a
    loop that selects and creates image arrays and then passes them through our GAN,
    which will calculate the probability that each image array belongs to the target
    class. However, if we start a loop here, then the effects on every line of code
    will not be seen until the entire code completes. For that reason, we will first
    walk through every line of code inside the loop and then, we will show the entire
    code wrapped in the `for` loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before entering the loop, we declare one variable that we will need inside
    our loop. The following code sets a value for the `first_row` variable. This will
    be used later when we subset our array. We start with the first three-dimensional
    array within our four-dimensional array. Later, when the following code is run
    in a loop, the value for `first_row` will change during every iteration to ensure
    a different subset of real images is passed to the discriminator model. We set
    the value of `first_row` for the first iteration of the loop by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Generating random images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training, we will use the model to create random images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create a matrix of random variables. The number of random
    variables should be set at the batch size times the size of the input shape for
    our generator model. The dimensions are then set so that the number of rows is
    equal to the batch size and the number of columns is equal to the length of the
    input shape defined here. In this case, `20` is used as the batch size and `100`
    is used as the length of the vector to be passed to the generator model. Both
    of these values are modifiable. Increasing either or both provides more data to
    the model, which could improve performance but will also increase runtime. We
    create our matrix of random values from a normal distribution by using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, a matrix will be created consisting of values selected
    from a normal (Gaussian) distribution. Every row in the matrix will be used to
    generate an image. The images are created by using random values. By selecting
    the object in our data environment, we can view it. After selecting the data object
    from the environment, you will see something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/45add777-2b12-486f-94d5-48a4a7cd804a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, using our matrix of random values, we will generate fake images. These
    fake images are created using the generator model that we defined earlier. The
    model takes the random values as input and the output is a four-dimensional array. The
    first dimension of the array corresponds with the batch size, which in this case
    is `20`, and the other three dimensions correspond to the dimensions of our image
    data. After generating synthetic data, we will sample a few values to show that
    the arrays have been created and populated with the random values. We create the
    array and view a portion of it by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we see a small section of the array that
    we created. Since this has a value of `1` for the first dimension and a value
    of `1` for the fourth dimension, then we know that the values will be used to
    represent the intensity of the red values for the first image. The preceding code
    prints values to the console. You will see something like the following printed
    to your console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/94cfbdcc-0b4b-42fc-ae4a-ea688b87d4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Earlier, we set the `first_row` value to indicate where we would like row-wise
    subsets to begin for every iteration. Next, we need to define the last row, which
    is equal to the value of the first row plus one less than the batch size. In this
    case, the batch size is `20`, so we use `19`. Also, while the `first_row` value
    begins at `1`, it will change dynamically throughout the iterations. We set the
    value of the last row for subsetting our data by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Selecting real images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we select the real images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the values for `first_row` and `last_row` to create a subset of
    the array containing our real target images. We will also run two lines to remove
    attributes from our data objects. This is not strictly necessary and at times,
    you may want data that is stored here. However, for demonstration purposes, we
    will remove it now so we can see the dimensions of all the arrays in the data
    environment window. The array of real images equal to the batch size to be used
    in an iteration of the model is created using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After running these lines, we can now see that `real_images` and `fake_images`
    are arrays of the same size. Expand both data objects in your Environment pane
    and you will see that your environment now looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/af9057de-5c7e-4eda-81e8-3256ba26e4f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Combining real and fake images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After differentiating the real and fake images, we will now merge them using
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our next step, we create an array with all the `0` values in the shape of
    our real images stacked on top of our fake images. That is to say, the first dimension
    is equal to twice the batch size, which in this case is `40`, and the remaining
    three dimensions are equal to the size of our image arrays. We create this placeholder
    array of zeroes by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will see a new object in our Environment window
    and we can see that it does have a first dimension that is twice the size of the
    other two arrays that we created. Your environment will now look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0f485eeb-0b22-4568-99cb-cd01482d1e38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will populate our placeholder array. For the top half of this array,
    we assign the values from the fake images that we generated and for the bottom
    half, we assign the values for the real images. We populate our array with values
    from the fake and real image data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will see that even for the small sample of data
    available in the Environment window, the values for `combined_images` have changed
    from all the zeroes, as seen earlier, to random values from our `fake_images`
    array. Your Environment window will now look as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aef705b9-a007-49ad-adef-40b8d38aed97.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating target labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we create target labels for all the images using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a matrix of labels; this is simply a matrix of binary values.
    We add `20` rows with values of `1` to label the fake images and `20` rows with
    values of `0` to label the real images. We create our matrix of label data using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, let''s click on the `labels` object to view it. We
    can see that it does contain 20 rows with a value of `1` and 20 rows with a value
    of `0`. The following is an image that you will see near the midpoint when viewing
    this matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0f5da36c-01d5-41d4-92bf-334ae969e1e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to add some noise to the labels. Just like using the dropout
    layer earlier, we want to introduce some noise and randomness throughout the modeling
    process to try to force the discriminator to generalize more and avoid overfitting.
    We add noise by applying a constant value to an array of random values selected
    from a uniform distribution that is the same length as our labels object and then
    adding it to the current values in the labels matrix. We add noise to our labels
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After we do this, we can look at the same subset of rows from the `labels`
    object and see that the values are now all slightly modified. The values in your
    `labels` object will be similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/74a808fc-90be-49a2-b0d5-194c90fca2aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Passing input to the discriminator model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will pass the inputs to the discriminator model using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we pass our combined images, containing a mix of real and fake images,
    as input to our discriminator model and we pass along the labels as the target
    variable for the model. We feed our independent and dependent variables to the
    input and output layers of our discriminator model using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running this code is the error rate for the discriminator. We
    can just run the name of the object to have this value printed to our console.
    After running the preceding code, your console will look as in the following screenshot,
    though the value may be slightly different:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e2ac21a8-ec92-44d7-b6c9-c716fb2c7642.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our next step will be to create another random matrix, which we will use as
    input to our GAN. It goes to the generator and in turn, the discriminator, as
    defined in our GAN model definition. We create the input for our GAN model using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we create an array that is the size of our batch. It is set to
    state that all the data objects are true:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass this matrix of random variables and the array to the GAN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running this code is a calculation of the error rate for the
    GAN. If many of the true target images were correctly identified, then the generator
    will make larger changes and if the generator creates images that are selected
    as real images, then fewer changes will be made during future iterations. If we
    run the line that only contains the object name, then we will receive a print
    out to our console. Your console will look similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6f89a105-6861-47b9-9c2f-3a45d1386007.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the row selector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will update the row selector with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step will be to reset the `first_row` value to get a different subset
    of the `real_image` data during the subsequent iteration. We reset the `first_row`
    value using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'After this code is run, the `first_row` value will either be set to the previous
    value plus the batch size of `20`, or if that number would result in a subset
    that is out of bounds, then the `first_row` value is set to a randomly drawn value
    between `1` and `10`. In this case, the value will be set to `21`. You will see
    a printout to your console, as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/34464036-eea3-4a6f-92df-a02a058af701.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we will evaluate the model using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to periodically print model diagnostics along with real and
    generated images for comparison and to track whether the synthetic images are
    being generated as expected. We print the model iteration and error rates and
    also save one real and one fake image in the directory that we created earlier
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we can see model diagnostics printed to our console.
    Your console will look as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8e4daa10-1762-4266-8d32-5ebbbf154a2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we can see generated and real images in the folder we created
    earlier. It will take quite a large number of rounds or epochs for any images
    to look like realistic faces, as in our real images set. However, even in earlier
    rounds, we can see the GAN begin to find features. The following is an original
    photo from our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/36581ad9-524e-4454-9e30-065b97f2ee50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a generated image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b5ea8b24-34c5-43a9-9b96-ba939b117169.png)'
  prefs: []
  type: TYPE_IMG
- en: This early synthetic image has captured a number of features already.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, the entire `for` loop for iteratively training our model is
    included in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Our GAN model is complete. You can continue to make a number of adjustments
    to see how it affects the synthetic images created. All through the model pipeline
    creation, we noted the values that need to be present in order to make the model
    work. However, huge portions can be modified. All modifications will result in
    different generated images. As noted before, there is no metric for a successful
    GAN. It will all depend on the end user's interpretation of the generated data.
    Adding more layers to the generator or discriminator, as well as adjusting the
    filter size, layer parameters, and learning rate, are all good options for modification
    as you continue to explore developing this particular type of deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we created a model that can take images of faces as input and
    generate faces as output. We used images from the labeled faces in the wild dataset.
    Using a GAN model, we generated an image with random values and then sampled an
    actual image. To generate an image, we took random values and reshaped them to
    the dimensions of the images in our dataset. We then fed this image—composed of
    random values—along with an actual image, to a model that reshaped the data down
    to a simple probability score, representing the likelihood that an image is real
    or fake. Through multiple iterations, the generator was trained to create images
    that were increasingly likely to be classified as real by the discriminator model.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will learn about another unsupervised deep learning
    technique called **reinforcement learning**. It is similar to GANs in that an
    agent performs a task and continually learns from failing until it can perform
    the task successfully. We will dive into the details of reinforcement learning
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
