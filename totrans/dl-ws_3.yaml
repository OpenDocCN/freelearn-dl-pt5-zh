- en: 3\. Image Classification with Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study **convolutional neural networks** (**CNNs**)
    and image classification. First, we will be introduced to the architecture of
    CNNs and how to implement them. We will then get hands-on experience of using
    TensorFlow to develop image classifiers. Finally, we will cover the concepts of
    transfer learning and fine-tuning and see how we can use state-of-the-art algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a good understanding of what CNNs
    are and how programming with TensorFlow works.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about traditional neural networks and a
    number of models, such as the perceptron. We learned how to train such models
    on structured data for regression or classification purposes. Now, we will learn
    how we can extend their application to the computer vision field.
  prefs: []
  type: TYPE_NORMAL
- en: Not so long ago, computers were perceived as computing engines that could only
    process well-defined and logical tasks. Humans, on the other hand, are more complex
    since we have five basic senses that help us see things, hear noises, feel things,
    taste foods, and smell odors. Computers were only calculators that could operate
    large volumes of logical operations, but they couldn't deal with complex data.
    Compared to the abilities of humans, computers had very clear limitations.
  prefs: []
  type: TYPE_NORMAL
- en: There were some rudimentary attempts to “*give sight*" to computers by processing
    and analyzing digital images. This field is called computer vision. But it was
    not until the advent of deep learning that we saw some incredible improvements
    and results. Nowadays, the field of computer vision has advanced to such an extent
    that, in some cases, computer vision AI systems are able to process and interpret
    certain types of images faster and more accurately than humans. You may have heard
    about the experiment where a group of 15 doctors in China competed against a deep
    learning system from the company BioMind AI for recognizing brain tumors from
    X-rays. The AI system took 15 minutes to accurately predict 87% of the 225 input
    images, while it took 30 minutes for the medical experts to achieve a score of
    66% on the same pool of images.
  prefs: []
  type: TYPE_NORMAL
- en: We've all heard about self-driving cars that can automatically make the right
    decisions depending on traffic conditions or drones that can detect sharks and
    automatically send alerts to lifeguards. All these amazing applications are possible
    thanks to the recent development of CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision can be split into four different domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification**, where we need to recognize the main object in an
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image classification and localization**, where we need to recognize and localize
    the main object in an image with a bounding box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection**, where we need to recognize multiple objects in an image
    with bounding boxes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image segmentation**, where we need to identify the boundaries of objects
    in an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the difference between the four domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Difference between the four domains of computer vision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Difference between the four domains of computer vision'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will only look at image classification, which is the most
    widely used application of CNN. This includes things such as car plate recognition,
    automatic categorization of the pictures taken with your mobile phone, or creating
    metadata used by search engines on databases of images.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re reading the print version of this book, you can download and browse
    the color versions of some of the images in this chapter by visiting the following
    link: [https://packt.live/2ZUu5G2](https://packt.live/2ZUu5G2 )'
  prefs: []
  type: TYPE_NORMAL
- en: Digital Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans can see through their eyes by transforming light into electrical signals
    that are then processed by the brain. But computers do not have physical eyes
    to capture light. They can only process information in digital forms composed
    of bits (0 or 1). So, to be able to “see", computers require a digitized version
    of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'A digital image is formed by a two-dimensional matrix of pixels. For a grayscale
    image, each of these pixels can take a value between 0 and 255 that represents
    its intensity or level of gray. A digital image can be composed of one channel
    for a black and white image or three channels (red, blue, and green) for a color
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Digital representation of an image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: Digital representation of an image'
  prefs: []
  type: TYPE_NORMAL
- en: 'A digital image is characterized by its dimensions (height, width, and channel):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Height:** How many pixels there are on the vertical axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Width:** How many pixels there are on the horizontal axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Channel:** How many channels there are. If there is only one channel, an
    image will be in grayscale. If there are three channels, the image will be colored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following digital image has dimensions (512, 512, 3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Dimensions of a digital image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3: Dimensions of a digital image'
  prefs: []
  type: TYPE_NORMAL
- en: Image Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how a digital image is represented, let's discuss how computers
    can use this information to find patterns that will be used to classify an image
    or localize objects. So, in order to get any useful or actionable information
    from an image, a computer has to resolve an image into a recognizable or known
    pattern. As for any machine learning algorithm, computer vision needs some features
    in order to learn patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike structured data, where each feature is well defined in advance and stored
    in separate columns, images don't follow any specific pattern. It is impossible
    to say, for instance, that the third line will always contain the eye of an animal
    or that the bottom left corner will always represent a red, round-shaped object.
    Images can be of anything and don't follow any structure. This is why they are
    considered to be unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: However, images do contain features. They contain different shapes (lines, circles,
    rectangles, and so on), colors (red, blue, orange, yellow, and so on), and specific
    characteristics related to different types of objects (hair, wheel, leaves, and
    so on). Our eyes and brain can easily analyze and interpret all these features
    and identify objects in images. Therefore, we need to simulate the same analytical
    process for computers. This is where **image filters** (also called kernels) come
    into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image filters are small matrices specialized in detecting a defined pattern.
    For instance, we can have a filter for detecting vertical lines only and another
    one only for horizontal lines. Computer vision systems run such filters in every
    part of the image and generate a new image with the detected patterns highlighted.
    These kinds of generated images are called **feature maps**. An example of a feature
    map where an edge-detection filter is used is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Example of a vertical edge feature map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: Example of a vertical edge feature map'
  prefs: []
  type: TYPE_NORMAL
- en: Such filters are widely used in image processing. If you've used Adobe Photoshop
    before (or any other image processing tool), you will have most likely used filters
    such as *Gaussian* and *Sharpen*.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know the basics of image processing, we can start our journey with
    CNNs. As we mentioned previously, computer vision relies on applying filters to
    an image to recognize different patterns or features and generate feature maps.
    But how are these filters applied to the pixels of an image? You could guess that
    there is some sort of mathematical operation behind it, and you would be absolutely
    right. This operation is called convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'A convolution operation is composed of two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: An element-wise product of two matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sum of the elements of a matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at an example of how to convolute two matrices, A and B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Examples of matrices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Examples of matrices'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to perform an element-wise multiplication with matrices A and
    B. We will get another matrix, C, as a result, with the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '1st row, 1st column: 5 × 1 = 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1st row, 2nd column: 10 × 0 = 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1st row, 3rd column: 15 × (-1) = -15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2nd row, 1st column: 10 × 2 = 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2nd row, 2nd column: 20 × 0 = 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2nd row, 3rd column: 30 × (-2) = -60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3rd row, 1st column: 100 × 1 = 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3rd row, 2nd column: 150 × 0 = 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3rd row, 3rd column: 200 × (-1) = -200'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An element-wise multiplication is different from a standard matrix multiplication,
    which operates at the row and column level rather than on each element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we just have to perform a sum on all elements of matrix C, which will
    give us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 5+0-15+20+0-60+100+0-200 = -150
  prefs: []
  type: TYPE_NORMAL
- en: 'The final result of the entire convolution operation on matrices A and B is
    -150, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: Sequence of the convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.6: Sequence of the convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, Matrix B is actually a filter (or kernel) called Sobel that
    is used for detecting vertical lines (there is also a variant for horizontal lines).
    Matrix A will be a portion of an image with the same dimensions as the filter
    (this is mandatory in order to perform element-wise multiplication).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A filter is, in general, a square matrix such as (3,3) or (5,5).
  prefs: []
  type: TYPE_NORMAL
- en: For a CNN, filters are actually parameters that will be learned (that is, defined)
    during the training process. So, the values of each filter that will be used will
    be set by the CNN itself. This is an important concept to go through before we
    learn how to train a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.01: Implementing a Convolution Operation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will use TensorFlow to implement a convolution operation
    on two matrices: `[[1,2,3],[4,5,6],[7,8,9]]` and `[[1,0,-1],[1,0,-1],[1,0,-1]]`.
    Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and name it `Exercise 3.01`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `tensorflow` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a tensor called `A` from the first matrix, `([[1,2,3],[4,5,6],[7,8,9]])`.
    Print its value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a tensor called `B` from the first matrix, `([[1,0,-1],[1,0,-1],[1,0,-1]])`.
    Print its value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform an element-wise multiplication on `A` and `B` using `tf.math.multiply()`.
    Save the result in `mult_out` and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform an element-wise sum on `mult_out` using `tf.math.reduce_sum()`. Save
    the result in `conv_out` and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result of the convolution operation on the two matrices, `[[1,2,3],[4,5,6],[7,8,9]]`
    and `[[1,0,-1],[1,0,-1],[1,0,-1]]`, is `-6`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/320pEfC](https://packt.live/320pEfC).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZdeLFr](https://packt.live/2ZdeLFr).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we used the built-in functions of TensorFlow to perform a
    convolution operation on two matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned how to perform a single convolution operation. We learned
    that a convolution operation uses a filter of a specific size, say, (3, 3), that
    is, 3 × 3, and applies it on a portion of the image of a similar size. If we have
    a large image, let's say of size (512, 512), then we can just look at a very tiny
    part of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking tiny parts of the image at a time, we need to perform the same convolution
    operation on the entire space of a given image. To do so, we will apply a technique
    called sliding. As the name implies, sliding is where we apply the filter to an
    adjacent area of the previous convolution operation: we just slide the filter
    and apply convolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we start from the top-left corner of an image, we can slide the filter by
    one pixel at a time to the right. Once we get to the right edge, we can slide
    down the filter by one pixel. We repeat this sliding operation until we''ve applied
    convolution to the entire space of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Example of stride'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Example of stride'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than sliding by 1 pixel only, we can choose a bigger sliding window,
    such as 2 or 3 pixels. The parameter defining the value of this sliding window
    is called **stride**. With a bigger stride value, there will be fewer overlapping
    pixels, but the resulting feature map will have smaller dimensions, so you will
    be losing a bit of information.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we applied a Sobel filter on an image that has been
    split horizontally with dark values on the left-hand side and white ones on the
    right-hand side. The resulting feature map has high values (800) in the middle,
    which indicates that the Sobel filter found a vertical line in that area. This
    is how sliding convolution helps to detect specific patterns in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned how a filter can go through all the pixels
    of an image with pixel sliding. Combined with the convolution operation, this
    process helps to detect patterns (that is, extract features) in an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying a convolution to an image will result in a feature map that has smaller
    dimensions than the input image. A technique called padding can be used in order
    to get the exact same dimensions for the feature map as for the input image. It
    consists of adding a layer of pixels with a value of `0` to the edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Example of padding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: Example of padding'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, the input image has the dimensions (6,6). Once padded,
    its dimensions increased to (8,8). Now, we can apply convolution on it with a
    filter of size (3,3):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Example of padded convolution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Example of padded convolution'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting image after convoluting the padded image is (6,6) in terms of
    its dimensions, which is the exact same dimensions as for the original input image.
    The resulting feature map has high values in the middle of the image, just like
    the previous example without padding. So, the filter can still find the same pattern
    in the image. But you may notice now that we have very low values (-800) on the
    left edge. This is actually fine as lower values mean the filter hasn't found
    any pattern in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formulas can be used for calculating the output dimensions of
    a feature map after a convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Formulas for calculating the output dimensions of a feature
    map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.10: Formulas for calculating the output dimensions of a feature map'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`w`: Width of the input image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h`: Height of the input image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p`: Number of pixels used on each side for padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f`: Filter size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s`: Number of pixels in the stride'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s apply this formula to the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`w` = 6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h` = 6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` = 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f` = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s` = 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, calculate the output dimensions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Output – dimensions of the feature map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.11: Output – dimensions of the feature map'
  prefs: []
  type: TYPE_NORMAL
- en: So, the dimensions of the resulting feature map are (6,6).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 2*, *Neural Networks*, you learned about traditional neural networks,
    such as perceptrons, that are composed of fully connected layers (also called
    dense layers). Each layer is composed of neurons that perform matrix multiplication,
    followed by a non-linear transformation with an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are actually very similar to traditional neural networks, but instead of
    using fully connected layers, they use convolutional layers. Each convolution
    layer will have a defined number of filters (or kernels) that will apply the convolution
    operation with a given stride on an input image with or without padding and can
    be followed by an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are widely used for image classification, where the network will have to
    predict the right class for a given input. This is exactly the same as classification
    problems for traditional machine learning algorithms. If the output can only be
    from two different classes, it will be a **binary classification**, such as recognizing
    dogs versus cats. If the output can be more than two classes, it will be a **multi-class
    classification** exercise, such as recognizing 20 different sorts of fruits.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make such predictions, the last layer of a CNN model needs to be
    a fully connected layer with the relevant activation function according to the
    type of prediction problem. You can use the following list of activation functions
    as a rule of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: List of activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: List of activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain a better perspective of its structure, here''s what a simple CNN model
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Structure of a simple CNN model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Structure of a simple CNN model'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have learned a lot about CNNs already. There is one more concept we need
    to go through in order to reduce the training time of a CNN before jumping into
    our first exercise: pooling layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling layers are used to reduce the dimensions of the feature maps of convolution
    layers. But why do we need to perform such downsampling? One of the main reasons
    is to reduce the number of calculations that are performed in the networks. Adding
    multiple layers of convolution with different filters can have a significant impact
    on the training time. Also, reducing the dimensions of feature maps can eliminate
    some of the noise in the feature map and help us focus only on the detected pattern.
    It is quite typical to add a pooling layer after each convolutional layer in order
    to reduce the size of the feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pooling operation acts very similarly to a filter, but rather than performing
    a convolution operation, it uses an aggregation function such as average or max
    (max is the most widely used function in the current CNN architecture). For instance,
    **max pooling** will look at a specific area of the feature map and find the maximum
    values of its pixels. Then, it will perform a stride and find the maximum value
    among the neighbor pixels. It will repeat this process until it processes the
    entire image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Max pooling with stride 2 on an input image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Max pooling with stride 2 on an input image'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we used a max pooling (which is the most widely used
    function for pooling) of size (2, 2) and a stride of 2\. We looked at the top-left
    corner of the feature map and found the maximum value among the pixels, 6, 8,
    1, and 2, and got a result of 8\. Then, we slid the max pooling by a stride of
    2 and performed the same operation on the group of pixels, that is, 6, 1, 7, and
    4\. We repeated the same operation on the bottom groups and got a new feature
    map of size (2,2).
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN model with max pooling will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: Example of the CNN architecture with max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Example of the CNN architecture with max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the preceding model can be used for recognizing handwritten digits
    (from 0 to 9). There are three convolution layers in this model, followed by a
    max pooling layer. The final layers are fully connected and are responsible for
    making the predictions of the digit that's been detected.
  prefs: []
  type: TYPE_NORMAL
- en: The overhead of adding pooling layers is much less than computing convolution.
    This is why they will speed up the training time.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs with TensorFlow and Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you've learned a lot about how CNN works under the hood. Now, it is
    finally time to see how we can implement what we have learned. We will be using
    the Keras API from TensorFlow 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras API provides a high-level API for building your own CNN architecture.
    Let's look at the main classes we will be using for CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to create a convolution layer, we will need to instantiate a `Conv2D()`
    class and specify the number of kernels, their size, the stride, padding, and
    activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have created a convolution layer with `64` kernels
    that are `(3, 3)` in dimension with a stride of `2`, a padding to get the same
    output dimension as the input (`padding='same'`), and ReLU as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about this class by going to TensorFlow''s documentation
    website: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to add a max pooling layer, you will have to use the `MaxPool2D()`
    class and specify its dimensions and stride, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we have instantiated a max pooling layer of size
    `(3,3)` with a stride of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about this class by going to TensorFlow''s documentation
    website: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a fully connected layer, we will use the `Dense()` class and specify the
    number of units and the activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows us how to create a fully connected layer that has `1`
    output unit and uses `sigmoid` as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, while manipulating input data, we may have to change its dimensions
    before feeding it to a CNN model. If we are using NumPy arrays, we can use the
    `reshape` method (as seen in *Chapter 1*, *Building Blocks of Deep Learning*),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have transformed the dimension of `features_train` to `(60000, 28,
    28, 1)`, which corresponds to the format (number of observations, height, width,
    channel). This is needed when working with grayscale images to add the channel
    dimension. In this example, the dimensions of a grayscale image, `(28,28)`, will
    be reshaped to `(28,28,1)`, and there will be `60000` images in total.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, you can use the `reshape` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned how to design a CNN in TensorFlow, it's time to put
    this all into practice on the famous MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about Reshape by going to TensorFlow''s documentation website:
    [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.02: Recognizing Handwritten Digits (MNIST) with CNN Using KERAS'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will be working on the MNIST dataset (which we worked on
    in *Chapter 2, Neural Networks*), which contains images of handwritten digits.
    However, this time, we will be using a CNN model. This dataset was originally
    shared by Yann Lecun, one of the most renowned deep learning researchers. We will
    build a CNN model and then train it to recognize handwritten digits. The CNN will
    be composed of two layers of convolution with 64 kernels each, followed by two
    fully connected layers that have 128 and 10 units, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow provides this dataset directly from its API. Perform the following
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about this dataset on TensorFlow''s website: [https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and name it `Exercise 3.02`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow.keras.datasets.mnist` as `mnist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `mnist` dataset using `mnist.load_data()` and save the results into
    `(features_train, label_train), (features_test, label_test)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the content of `label_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The label column contains numeric values that correspond to the 10 handwritten
    digits: `0` to `9`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training set is composed of `60000` observations of shape `28` by `28`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the `shape` of the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The testing set is composed of `10000` observations of shape `28` by `28`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reshape the training and testing sets with the dimensions `(number_observations,
    28, 28, 1)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Standardize `features_train` and `features_test` by dividing them by `255`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The results may still differ slightly after setting the seeds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate a `tf.keras.Sequential()` class and save it to a variable called
    `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `layers.Conv2D()` class with `64` kernels of shape `(3,3)`, `activation=''relu''`,
    and `input_shape=(28,28,1)`, and save it to a variable called `conv_layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `layers.Conv2D()` class with `64` kernels of shape `(3,3)` and
    `activation=''relu''` and save it to a variable called `conv_layer2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `layers.Flatten()` class with `128` neurons, `activation=''relu''`,
    and save it to a variable called `fc_layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `layers.Flatten()` class with `10` neurons, `activation=''softmax''`,
    and save it to a variable called `fc_layer2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the four layers you just defined to the model using `.add()`, add a `MaxPooling2D()`
    layer of size `(2,2)` in between each of the convolution layers, and add a `Flatten()`
    layer before the first fully connected layer to flatten the feature maps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.16: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding summary shows us that there are more than 240,000 parameters to
    be optimized with this model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the neural networks with the training set and specify `epochs=5`, `validation_split=0.2`,
    and `verbose=2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: Training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.17: Training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We trained our CNN on 48,000 samples, and we used 12,000 samples as the validation
    set. After training for five epochs, we achieved an accuracy score of `0.9951`
    for the training set and `0.9886` for the validation set. Our model is overfitting
    a bit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s evaluate the performance of the model on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this, we've achieved an accuracy score of `0.9903` on the testing set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2W2VLYl](https://packt.live/2W2VLYl).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iKAVGZ](https://packt.live/3iKAVGZ).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we designed and trained a CNN architecture to recognize the
    images of handwritten digit images from the MNIST dataset and achieved an almost
    perfect score.
  prefs: []
  type: TYPE_NORMAL
- en: Data Generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous exercise, we built our first multi-class CNN classifier on the
    MNIST dataset. We loaded the entire dataset into the model as it wasn't very big.
    But for bigger datasets, we will not be able to do this. Thankfully, Keras provides
    an API called **data generator** that we can use to load and transform data in
    batches.
  prefs: []
  type: TYPE_NORMAL
- en: Data generators are also very useful for image classification. Sometimes, an
    image dataset comes in the form of a folder with predefined structures for the
    training and testing sets and for the different classes (all images that belong
    to a class will be stored in the same folder). The data generator API will be
    able to understand this structure and feed the CNN model properly with the relevant
    images and corresponding information. This will save you a lot of time as you
    won't need to build a custom pipeline to load images from the different folders.
  prefs: []
  type: TYPE_NORMAL
- en: On top of this, data generators can divide the images into batches of images
    and feed them sequentially to the model. You don't have to load the entire dataset
    into memory in order to perform training. Let's see how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the `ImageDataGenerator` class from `tensorflow.keras.preprocessing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can instantiate it by providing all the image transformations we want
    it to perform. In the following example, we will just normalize all the images
    from the training set by dividing them by `255` so that all the pixels will have
    a value between `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After this step, we will create a data generator by using the `.flow_from_directory()`
    method and will specify the path to the training directory, `batch_size`, the
    `target_size` of the image, the shuffle, and the type of class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You need to create a separate data generator for the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can train our model using the `.fit_generator()` method by providing
    the data generators for the training and validation sets, the number of epochs,
    and the number of steps per epoch, which corresponds to the number of images divided
    by the batch size (as integer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This method is very similar to the `.fit()` method you saw earlier, but rather
    than training the CNN on the entire dataset in one go, it will train by batches
    of images using the data generator we defined. The number of steps defines how
    many batches will be required to process the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data generators are quite useful for loading data from folders and feeding the
    model in batches of images. But they can also perform some data processing, as
    shown in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.03: Classifying Cats versus Dogs with Data Generators'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be working on the cats versus dogs dataset, which
    contains images of dogs and cats. We will build two data generators for the training
    and validation sets and a CNN model to recognize images of dogs or cats. Perform
    the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we''ll be using is a modified version from the Kaggle cats versus
    dogs dataset: [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data).The
    modified version, which only uses a subset of 25,000 images, has been provided
    by Google at [https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and name it `Exercise 3.03`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `tensorflow` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `file_url` containing the link to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the aforementioned step, we are using the dataset stored at [https://packt.live/3jZKRNw](https://packt.live/3jZKRNw).
    If you have stored the dataset at any other URL, please change the highlighted
    path accordingly. Watch out for the slashes in the string below. Remember that
    the backslashes ( `\` ) are used to split the code across multiple lines, while
    the forward slashes ( `/` ) are part of the URL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Download the dataset using `tf.keras.get_file` with `''cats_and_dogs.zip'',
    origin=file_url, extract=True` as parameters and save the result to a variable
    called `zip_dir`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `pathlib` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `path` containing the full path to the `cats_and_dogs_filtered`
    directory using `pathlib.Path(zip_dir).parent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two variables called `train_dir` and `validation_dir` that take the
    full paths to the train and validation folders, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create four variables called `train_cats_dir`, `train_dogs_dir`, `validation_cats_dir`,
    and `validation_dogs_dir` that take the full paths to the cats and dogs folders
    for the train and validation sets, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `os` package. We will need this in the next step in order to count
    the number of images from a folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two variables called `total_train` and `total_val` that will get the
    number of images for the training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate two `ImageDataGenerator` classes and call them `train_image_generator`
    and `validation_image_generator`. These will rescale the images by dividing them
    by `255`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create three variables called `batch_size`, `img_height`, and `img_width` that
    take the values `16`, `100`, and `100`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a data generator called `train_data_gen` using `.flow_from_directory()`
    and specify the batch size, the path to the training folder, `shuffle=True`, the
    target size as `(img_height, img_width)`, and the class mode as `binary`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a data generator called `val_data_gen` using `.flow_from_directory()`
    and specify the batch size, paths to the validation folder, `shuffle=True`, the
    target size as `(img_height, img_width)`, and the class mode as `binary`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` (this is totally arbitrary) as the `seed` for `numpy` and `tensorflow`
    using `np.random_seed()` and `tf.random.set_seed()`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.Sequential()` class into a variable called `model`
    with the following layers: A convolution layer with `64` kernels of shape `3`,
    `ReLU` as the activation function, and the required input dimensions; a max pooling
    layer; a convolution layer with `128` kernels of shape `3` and `ReLU` as the activation
    function; a max pooling layer; a flatten layer; a fully connected layer with `128`
    units and `ReLU` as the activation function; a fully connected layer with `1`
    unit and `sigmoid` as the activation function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code will look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''binary_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a summary of the model using `.summary()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.18: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding summary shows us that there are more than `8,700,000` parameters
    to be optimized with this model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: Training output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.19: Training output'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The expected output will be close to the one shown. You may have slightly different
    accuracy values due to some randomness in weights initialization.
  prefs: []
  type: TYPE_NORMAL
- en: We've trained our CNN for five epochs and achieved an accuracy score of `0.85`
    for the training set, and `0.7113` for the validation set. Our model is overfitting
    quite a lot. You may want to try the training with different architectures to
    see whether you can improve this score and reduce overfitting. You can also try
    feeding this model with some images of cats or dogs of your choice and see the
    output predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31XQmp9](https://packt.live/31XQmp9).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZW10tW](https://packt.live/2ZW10tW).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you were introduced to data generators that can do
    a lot of the heavy lifting, such as feeding the model from folders rather than
    columnar data for you regarding data processing for neural networks. So far, we
    have seen how to create them, load data from a structured folder, and feed the
    model by batch. We only performed one image transformation with it: rescaling.
    However, data generators can perform many more image transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But why do we need to perform data augmentation? The answer is quite simple:
    to prevent overfitting. By performing data augmentation, we are increasing the
    number of images in a dataset. For one image, we can generate, for instance, 10
    different variants of the same image. So, the size of your dataset will be multiplied
    by 10\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, with data augmentation, we have a set of images with a broader range
    of visuals. For example, selfie pictures can be taken from different angles, but
    if your dataset only contains selfie pictures that are straight in terms of their
    orientation, your CNN model will not be able to interpret other images with different
    angles correctly. By performing data augmentation, you are helping your model
    generalize better to different types of images. However, as you may have guessed,
    there is one drawback: data augmentation will also increase the training time
    as you have to perform additional data transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick look at some of the different types of data argumentation
    that we can do.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Flipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Horizontal flipping returns an image that is flipped horizontally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20: Example of horizontal flipping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.20: Example of horizontal flipping'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Flipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vertical flipping will flip an image vertically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21: Example of vertical flipping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.21: Example of vertical flipping'
  prefs: []
  type: TYPE_NORMAL
- en: Zooming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An image can be zoomed in and provide different sizes of objects in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22: Example of zooming'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.22: Example of zooming'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Shifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Horizontal shifting, as its name implies, will shift the image along the horizontal
    axis but keep it the same size. With this transformation, the image may be cropped,
    and new pixels need to be generated to fill the void. A common technique is to
    copy the neighboring pixels or to fill that space with black pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23: Example of horizontal shifting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.23: Example of horizontal shifting'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Shifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vertical shifting is similar to horizontal shifting, but along the vertical
    axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24: Example of vertical shifting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24: Example of vertical shifting'
  prefs: []
  type: TYPE_NORMAL
- en: Rotating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A rotation with a particular angle can be performed on an image like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25: Example of rotating'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.25: Example of rotating'
  prefs: []
  type: TYPE_NORMAL
- en: Shearing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Shearing transforms the image by moving one of the edges along the axis of
    the edge. After doing this, the image distorts from a rectangle to a parallelogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26: Example of shearing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.26: Example of shearing'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `Keras`, all these data transformation techniques can be added to `ImageDataGenerator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a general understanding of data argumentation, let's look at
    how to implement it in our models in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.04: Image Classification (CIFAR-10) with Data Augmentation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be working on the CIFAR-10 dataset (Canadian Institute
    for Advanced Research), which is composed of 60,000 images of 10 different classes:
    airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We
    will build a CNN model and use data augmentation to recognize these categories.
    Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about this dataset on TensorFlow''s website: [https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and name it `Exercise 3.04`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow.keras.datasets.cifar10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the CIFAR-10 dataset using `cifar10.load_data()` and save the results
    to `(features_train, label_train), (features_test, label_test)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the shape of `features_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training set is composed of `50000` images that have the dimensions `(32,32,3)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create three variables called `batch_size`, `img_height`, and `img_width` that
    take the values `16`, `32`, and `32`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an `ImageDataGenerator` instance called `train_img_gen` with data augmentation:
    rescaling (by dividing by 255), `width_shift_range=0.1`, `height_shift_range=0.1`,
    and horizontal flipping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an `ImageDataGenerator` instance called `val_img_gen` with rescaling
    (by dividing by 255):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a data generator called `train_data_gen` using the `.flow()` method
    and specify the batch size, features, and labels from the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a data generator called `val_data_gen` using the `.flow()` method and
    specify the batch size, features, and labels from the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.Sequential()` class into a variable called `model`
    with the following layers: a convolution layer with `64` kernels of shape `3`,
    ReLU as the activation function, and the necessary input dimensions; a max pooling
    layer; a convolution layer with `128` kernels of shape `3` and ReLU as the activation
    function; a max pooling layer; a flatten layer; a fully connected layer with `128`
    units and ReLU as the activation function; a fully connected layer with `10` units
    and Softmax as the activation function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27: Training logs for the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_03_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.27: Training logs for the model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZLyQk](https://packt.live/31ZLyQk).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2OcmahS](https://packt.live/2OcmahS).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we trained our CNN on five epochs, and we achieved an accuracy
    score of `0.6713` on the training set and `0.6582` on the validation set. Our
    model is overfitting slightly, but its accuracy score is quite low. You may wish
    to try this on different architectures to see whether you can improve this score
    by, for instance, adding more convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The expected output for the preceding exercise will be close to the one shown
    (Figure 3.27). You may have slightly different accuracy values due to some randomness
    in weights initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Building a Multiclass Classifier Based on the Fashion MNIST Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will train a CNN to recognize images of clothing that
    belong to 10 different classes. You will apply some data augmentation techniques
    to reduce the risk of overfitting. You will be using the Fashion MNIST dataset
    provided by TensorFlow. Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset was shared by Han Xiao. You can read more about this dataset
    on TensorFlow''s website here: [https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)'
  prefs: []
  type: TYPE_NORMAL
- en: Import the Fashion MNIST dataset from TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a data generator with the following data augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the neural network architecture with the following layers: A convolutional
    layer with `Conv2D(64, (3,3), activation=''relu'')` followed by `MaxPooling2D(2,2)`;
    a convolutional layer with `Conv2D(64, (3,3), activation=''relu'')` followed by
    `MaxPooling2D(2,2)`; a flatten layer; a fully connected layer with `Dense(128,
    activation=relu)`; a fully connected layer with `Dense(10, activation=''softmax'')`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify an Adam optimizer with a learning rate of `0.001`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28: Training logs for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.28: Training logs for the model'
  prefs: []
  type: TYPE_NORMAL
- en: The expected accuracy scores should be around `0.82` for the training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 394.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Restoring Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how we can use data augmentation to generate
    different variants of an image. This will increase the size of the dataset but
    will also help the model train on a wider variety of images and help it generalize
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've trained your model, you will most likely want to deploy it in production
    and use it to make live predictions. To do so, you will need to save your model
    as a file. This file can then be loaded by your prediction service so that it
    can be used as an API or data science tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different components of a model that can be saved:'
  prefs: []
  type: TYPE_NORMAL
- en: The model's architecture with all the network and layers used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model's trained weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training configuration with the loss function, optimizer, and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In TensorFlow, you can save the entire model or each of these components separately.
    Let's learn how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the Entire Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To save all the components into a single artifact, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the saved model, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Saving the Architecture Only
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can save just the architecture of the model as a `json` object. Then, you
    will need to use the `json` package to save it to a file, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will load it back using the `json` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Saving the Weights Only
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can save just the weights of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will load them back after instantiating the architecture of your
    new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This is particularly useful if you want to train your model even more later.
    You will load the saved weights and keep training your model and updating its
    weights further.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: .h5 is the file extension used by default by TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned a lot about designing and training our own CNN models.
    But as you may have noticed, some of our models are not performing very well.
    This can be due to multiple reasons, such as the dataset being too small or our
    model requiring more training.
  prefs: []
  type: TYPE_NORMAL
- en: But training a CNN takes a lot of time. It would be great if we could reuse
    an existing architecture that has already been trained. Luckily for us, such an
    option does exist, and it is called transfer learning. TensorFlow provides different
    implementations of state-of-the-art models that have been trained on the ImageNet
    dataset (over 14 million images).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the list of available pretrained models in the TensorFlow documentation:
    [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a pretrained model, we need to import its implemented class. Here, we
    will be importing a `VGG16` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the input dimensions of the images from our dataset. Let''s
    say we have images of `(100,100, 3)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will instantiate a `VGG16` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a `VGG16` model trained on the `ImageNet` dataset. The `include_top=True`
    parameter is used to specify that we will be using the same last layers to predict
    ImageNet's 20,000 categories of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use this pretrained model to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'But what if we want to use this pretrained model to predict different classes
    other than the ones from ImageNet? In this situation, we will need to replace
    the last fully connected layers of the pretrained models that are used for prediction
    and train them on the new classes. These last few layers are referred to as the
    top (or head) of the model. We can do this by specifying `include_top=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we will need to freeze this model so that it can''t be trained
    (that is, its weights will not be updated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create a new fully connected layer with the parameter of our
    choice. In this example, we will add a `Dense` layer with `20` units and a `softmax`
    activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then add the new fully connected layer to our base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will train this model, but only the weights for the last layer
    will be updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We just created a new model from a pretrained model and adapted it in order
    to make predictions for our own dataset. We achieved this by replacing the last
    layers according to the predictions we want to make. Then, we trained only these
    new layers to make the right predictions. Using transfer learning, you leveraged
    the existing weights of the `VGG16` model, which were trained on ImageNet. This
    has saved you a lot of training time and can significantly increase the performance
    of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how to apply transfer learning and use pretrained
    models to make predictions on our own dataset. With this approach, we froze the
    entire network and trained only the last few layers that were responsible for
    making the predictions. The convolutional layers stay the same, so all the filters
    are set in advance and you are just reusing them.
  prefs: []
  type: TYPE_NORMAL
- en: But if the dataset you are using is very different from ImageNet, these pretrained
    filters may not be relevant. In this case, even using transfer learning will not
    help your model accurately predict the right outcomes. There is a solution for
    this, which is to only freeze a portion of the network and train the rest of the
    model rather than just the top layers, just like we do with **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: In the early layers of the networks, the filters tend to be quite generic. For
    instance, you may find filters that detect horizontal or vertical lines at that
    stage. The filters closer to the end of the network (close to the top or head)
    are usually more specific to the dataset you are training on. So, these are the
    ones we want to retrain. Let's learn how we can achieve this in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s instantiate a pretrained `VGG16` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to set the threshold for the layers so that they''re frozen. In
    this example, we will freeze the first 10 layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will iterate through these layers and freeze them individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will add our custom fully connected layer to our base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will train this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: In this case, our model will train and update all the weights from the threshold
    layer we defined. They will use the pretrained weights as the initialized values
    for the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, called fine-tuning, you can still leverage pretrained models
    by partially training them to fit your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Fruit Classification with Transfer Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will train a CNN to recognize images of fruits that belong
    to 120 different classes. We will use transfer learning and data augmentation
    to do so. We will be using the Fruits 360 dataset ([https://arxiv.org/abs/1712.00580](https://arxiv.org/abs/1712.00580)),
    which was originally shared by Horea Muresan, Mihai Oltean, *Fruit recognition
    from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue
    1, pp. 26-42, 2018.*
  prefs: []
  type: TYPE_NORMAL
- en: 'It contains more than 82,000 images of 120 different types of fruits. We will
    be using a subset of this dataset with more than 16,000 images. Perform the following
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be found here: [https://packt.live/3gEjHsX](https://packt.live/3gEjHsX
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Import the dataset and unzip the file using TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a data generator with the following data augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load a pretrained `VGG16` model from TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add two fully connected layers on top of `VGG16`: A fully connected layer with
    `Dense(1000, activation=''relu'')` and a fully connected layer with `Dense(120,
    activation=''softmax'')`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify an Adam optimizer with a learning rate of `0.001`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected accuracy scores should be around `0.89` to `0.91` for the training
    and validation sets. The output will be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29: Expected output of the activity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_03_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.29: Expected output of the activity'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 398.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started our journey in this chapter with an introduction to computer vision
    and image processing, where we learned the different applications of such technology,
    how digital images are represented, and analyzed this with filters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we dived into the basic elements of CNN. We learned what a convolution
    operation is, how filters work in detecting patterns, and what stride and padding
    are used for. After understanding these building blocks, we learned how to use
    TensorFlow to design CNN models. We built our own CNN architecture to recognize
    handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we went through data generators and learned how they can feed our
    model with batches of images rather than loading the entire dataset. We also learned
    how they can perform data augmentation transformations to expand the variety of
    images and help the model generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about saving a model and its configuration, but also about
    how to apply transfer learning and fine-tuning. These techniques are very useful
    for reusing pretrained models and adapting them to your own projects and datasets.
    This will save you a lot of time as you won't have to train the model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you will learn about another very interesting topic that
    is used for natural language processing: embeddings.'
  prefs: []
  type: TYPE_NORMAL
