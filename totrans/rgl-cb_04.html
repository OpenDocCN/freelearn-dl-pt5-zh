<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer139">
<h1 class="chapter-number" id="_idParaDest-101"><a id="_idTextAnchor100"/>4</h1>
<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Regularization with Tree-Based Models</h1>
<p>Tree-based models using ensemble learning such as Random Forest or Gradient Boosting are often seen as easy-to-use, state-of-the-art models for regular machine <span class="No-Break">learning tasks.</span></p>
<p>Many Kaggle competitions have been won with such models, as they can be quite robust and efficient at finding complex patterns in data. Knowing how to regularize and fine-tune them is key to having the very <span class="No-Break">best performance.</span></p>
<p>In this chapter, we’ll look at the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Building a <span class="No-Break">classification tree</span></li>
<li>Building <span class="No-Break">regression trees</span></li>
<li><a id="_idTextAnchor102"/>Regularizing a <span class="No-Break">decision tree</span></li>
<li>Training a Random <span class="No-Break">Forest algorithm</span></li>
<li>Regularization of <span class="No-Break">Random Forest</span></li>
<li>Training a boosting model <span class="No-Break">with XGBoost</span></li>
<li>Regularization <span class="No-Break">with XGBoost</span></li>
</ul>
<h1 id="_idParaDest-103"><a id="_idTextAnchor103"/>Technical requirements</h1>
<p>In this chapter, you will train and fine-tune several decision tree-based models, as well as visualize a tree. The following libraries will be required for <span class="No-Break">this chapter:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">Scikit-learn</span></li>
<li><span class="No-Break">Graphviz</span></li>
<li><span class="No-Break">XGBoost</span></li>
<li><span class="No-Break">pickle</span></li>
</ul>
<h1 id="_idParaDest-104"><a id="_idTextAnchor104"/>Building a classification tree</h1>
<p>Decision trees<a id="_idIndexMarker149"/> are a separate class of models in machine learning. Although a decision tree alone can be considered a weak learner, combined with the power of ensemble learning such as bagging or boosting, decision trees get great performances. Before digging into ensemble learning models and how to regularize them, in this recipe, we will review how decision trees work and how to use them on a classification task on the <span class="No-Break">iris dataset.</span></p>
<p>To give an intuition of the power of decision trees, let’s consider a use case. We would like to know whether to sell ice creams on the beach based on two input features: sun <span class="No-Break">and temperature.</span></p>
<p>We have the data in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> and would like to train a model <span class="No-Break">on it.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 4.1 – A circle if we should sell ice creams as a function of sun and temperature and a cross if we shouldn’t" height="433" src="image/B19629_04_01.jpg" width="476"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – A circle if we should sell ice creams as a function of sun and temperature and a cross if we shouldn’t</p>
<p>For a human, this seems quite easy. For a linear model though, not so much. If we try to use logistic regression on this data, it will end up drawing a decision line such as the left in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>. Even with features that are raised to a higher power level, the logistic regression would struggle and propose something such as the decision line on the right in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 4.2 – Potential result of a linear model at classifying this dataset: on the left with raw features, on the right with higher power features" height="577" src="image/B19629_04_02.jpg" width="1326"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Potential result of a linear model at classifying this dataset: on the left with raw features, on the right with higher power features</p>
<p>In a word, this data is<a id="_idIndexMarker150"/> not linearly separable. But it can be divided into two separate linearly <span class="No-Break">separable problems:</span></p>
<ul>
<li>Is the <span class="No-Break">weather sunny?</span></li>
<li>Is the <span class="No-Break">temperature warm?</span></li>
</ul>
<p>If we fulfill those two conditions, then we should sell ice cream. This can be summarized as the tree in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 4.3 – A decision tree correctly classifying all data points" height="433" src="image/B19629_04_03.jpg" width="536"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – A decision tree correctly classifying all data points</p>
<p>Let’s cover a bit of <a id="_idIndexMarker151"/><span class="No-Break">vocabulary here:</span></p>
<ul>
<li>We have <strong class="bold">Warm</strong>, which is the first decision node and the root node with two branches: <strong class="bold">Yes</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">No</strong></span><span class="No-Break">.</span></li>
<li>We have another decision node in <strong class="bold">Sunny</strong>. A decision node is any node containing two (sometimes <span class="No-Break">more) branches.</span></li>
<li>We have three leaves. A leaf does not have any branches and contains a <span class="No-Break">final prediction.</span></li>
<li>Just as for binary trees in computer science, the depth of the tree is the number of edges between the root node and the <span class="No-Break">lowest leaf</span></li>
</ul>
<p>If we go back to our dataset, the decision line would now look like the one in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> with not one but two lines combined, providing an <span class="No-Break">effective solution:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 4.4 – Result of a decision tree at classifying this dataset" height="437" src="image/B19629_04_04.jpg" width="491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Result of a decision tree at classifying this dataset</p>
<p>From now on, any new data will fall into one of those leaves, allowing it to be <span class="No-Break">classified correctly.</span></p>
<p>This is the power of decision trees: they can compute complex, nonlinear rules, allowing them great flexibility. A decision tree is trained<a id="_idIndexMarker152"/> using a <strong class="bold">greedy algorithm</strong>, meaning it only tries to optimize one step at <span class="No-Break">a time.</span></p>
<p>More specifically, it <a id="_idIndexMarker153"/>means the decision tree is not optimized globally, but one node at <span class="No-Break">a time.</span></p>
<p>This can be seen as a <span class="No-Break">recursive algorithm:</span></p>
<ol>
<li>Take all samples in <span class="No-Break">a node.</span></li>
<li>Find a threshold in a feature that minimizes the disorder of the splits. In other words, find the feature and threshold giving the best <span class="No-Break">class separation.</span></li>
<li>Split this into two <span class="No-Break">new nodes.</span></li>
<li>Go back to <em class="italic">step 1</em> until your node is pure (meaning that only one class remains) or any other condition, and thus <span class="No-Break">a leaf.</span></li>
</ol>
<p>But how do we actually choose the splits so that they are optimal? Of course, we use a loss function, which <a id="_idIndexMarker154"/>uses disorder measurement. Let’s dig into those two topics before wrapping <span class="No-Break">it up.</span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Disorder measurement</h2>
<p>For a classification <a id="_idIndexMarker155"/>tree to be effective, it must have as little disorder as possible in its leaves. Indeed, in the previous example, we assumed all leaves are pure. They contain samples from only one class. In reality, leaves may be impure and contain samples from <span class="No-Break">several classes.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If after training a tree a leaf remains impure, we would use the majority class of that leaf <span class="No-Break">for classification.</span></p>
<p>So, the idea is to minimize impurity, but how do we measure it? There are two ways: entropy and Gini impurity. Let’s have a look <span class="No-Break">at both.</span></p>
<h3>Entropy</h3>
<p>Entropy is a<a id="_idIndexMarker156"/> general<a id="_idIndexMarker157"/> word that is used in many contexts, such as physics and computer science. The entropy <strong class="bold">E</strong> we use here can be defined with the following equation, where p<span class="subscript">i</span> is the proportion of subclass <img alt="" height="18" src="image/Formula_04_001.png" width="20"/> in <span class="No-Break">a sample:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="" height="112" src="image/Formula_04_002.jpg" width="296"/>
</div>
</div>
<p>Let’s consider a concrete example as depicted in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 4.5 – A node with 10 samples of two classes: red and blue" height="269" src="image/B19629_04_05.jpg" width="553"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – A node with 10 samples of two classes: red and blue</p>
<p>In this example, the entropy would be <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="" height="102" src="image/Formula_04_003.jpg" width="722"/>
</div>
</div>
<p>Indeed, we<a id="_idIndexMarker158"/> have<a id="_idIndexMarker159"/> <span class="No-Break">the following:</span></p>
<ul>
<li><img alt="" height="21" src="image/Formula_04_004.png" width="49"/>=3/10, since we have three blue samples out <span class="No-Break">of 10</span></li>
<li><img alt="" height="21" src="image/Formula_04_005.png" width="42"/>=7/10, since we have seven blue samples out <span class="No-Break">of 10</span></li>
</ul>
<p>If we look at extreme cases, we understand entropy is well suited to <span class="No-Break">compute disorder:</span></p>
<ul>
<li>if <img alt="" height="20" src="image/Formula_04_006.png" width="49"/>=0, then <img alt="" height="20" src="image/Formula_04_007.png" width="42"/>=1 and E = 0</li>
<li>if <img alt="" height="20" src="image/Formula_04_008.png" width="49"/>pblue = <img alt="" height="20" src="image/Formula_04_009.png" width="42"/> = 0.5, then E = 1</li>
</ul>
<p>So, we understand that the entropy reaches a maximum value of one when the node contains perfectly mixed samples, and the entropy goes to zero when a node contains only one class. This is summarized by the curve in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 4.6 – Entropy as a function of p for two classes" height="863" src="image/B19629_04_06.jpg" width="1139"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Entropy as a function of p for two classes</p>
<h3>Gini impurity</h3>
<p>Gini impurity is <a id="_idIndexMarker160"/>another way to <a id="_idIndexMarker161"/>measure disorder. The formula of Gini impurity G is <span class="No-Break">quite simple:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="" height="94" src="image/Formula_04_010.jpg" width="317"/>
</div>
</div>
<p>Again, <img alt="" height="20" src="image/Formula_04_011.png" width="20"/> is the proportion of class in <span class="No-Break">the node.</span></p>
<p>Applied to the example node in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em>, the computation of the Gini impurity would lead to the <span class="No-Break">following result:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="" height="112" src="image/Formula_04_012.jpg" width="619"/>
</div>
</div>
<p>The result is quite different from entropy, but let’s check that the properties remain the same with <span class="No-Break">extreme values:</span></p>
<ul>
<li>if <img alt="" height="23" src="image/Formula_04_013.png" width="91"/>, then <img alt="" height="26" src="image/Formula_04_014.png" width="85"/> and G = 0</li>
<li>if <img alt="" height="23" src="image/Formula_04_015.png" width="121"/> = 0.5, then G = <span class="No-Break">0.5</span></li>
</ul>
<p>Indeed, the Gini impurity reaches a maximum value of 0.5 when the disorder is maximum and is equal to 0 when the node <span class="No-Break">is pure.</span></p>
<p class="callout-heading">Entropy or Gini?</p>
<p class="callout">That said, what should we use? Well, this can be seen as a hyperparameter, and scikit-learn’s implementation allows one to choose between entropy <span class="No-Break">and Gini.</span></p>
<p class="callout">In practice, the results are often the same for both. But Gini impurity is faster to compute (entropy<a id="_idIndexMarker162"/> involves <a id="_idIndexMarker163"/>more expensive log computations), so it is usually the <span class="No-Break">first choice.</span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor106"/>Loss function</h2>
<p>We have a disorder <a id="_idIndexMarker164"/>measurement, but what is the loss we <a id="_idIndexMarker165"/>should minimize? The ultimate goal is to make splits that minimize the disorder, one node at <span class="No-Break">a time.</span></p>
<p>Considering a decision node always has two children, we can define them as left and right nodes. Then, the loss for this node can be written <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="" height="95" src="image/Formula_04_016.jpg" width="619"/>
</div>
</div>
<p>Let’s break down <span class="No-Break">the formula:</span></p>
<ul>
<li>m, <img alt="" height="23" src="image/Formula_04_017.png" width="53"/>, and <img alt="" height="23" src="image/Formula_04_018.png" width="64"/> are the number of samples in each <span class="No-Break">node respectively</span></li>
<li><img alt="" height="28" src="image/Formula_04_019.png" width="47"/> and <img alt="" height="28" src="image/Formula_04_020.png" width="59"/> are the Gini impurities of the left and <span class="No-Break">right nodes</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Of course, this can be computed with entropy instead of <span class="No-Break">Gini impurity.</span></p>
<p>Assuming we choose a split decision, we then have a parent node and two children nodes defined by the split in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 4.7 – One parent node and two children nodes, with their respective Gini impurities" height="683" src="image/B19629_04_07.jpg" width="535"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – One parent node and two children nodes, with their respective Gini impurities</p>
<p>In this <a id="_idIndexMarker166"/>case, the <a id="_idIndexMarker167"/>loss <strong class="bold">L</strong> would be <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="" height="102" src="image/Formula_04_021.jpg" width="613"/>
</div>
</div>
<p>Using this loss computation, we are now able to minimize the impurity (thus maximizing the purity of <span class="No-Break">a node).</span></p>
<p class="callout-heading">Note</p>
<p class="callout">What we defined here as the loss is only the loss at a node level. Indeed, as stated earlier, the <a id="_idIndexMarker168"/>decision <a id="_idIndexMarker169"/>tree is trained using a greedy approach, not a <span class="No-Break">gradient descent.</span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor107"/>Getting ready</h2>
<p>Finally, before<a id="_idIndexMarker170"/> going into the practical details of this recipe, we need to have the following libraries installed: scikit-learn, <strong class="source-inline">graphviz</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">.</span></p>
<p>They can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install scikit-learn graphviz matplotlib</pre>
<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/>How to do it…</h2>
<p>Before actually training a decision tree, let’s quickly go through all the steps to train a <span class="No-Break">decision tree:</span></p>
<ol>
<li>We have a node containing samples of <span class="No-Break">N classes.</span></li>
<li>We iterate through all the features and all the possible values of <span class="No-Break">a feature.</span></li>
<li>For each feature value, we compute the Gini impurity and <span class="No-Break">the loss.</span></li>
<li>We keep the feature value with the lowest loss and split the node into two <span class="No-Break">children nodes.</span></li>
<li>Go back to <em class="italic">step 1</em> with both nodes until a node is pure (or the stop condition <span class="No-Break">is fulfilled).</span></li>
</ol>
<p>Using this approach, the decision tree will eventually find the right set of decisions to successfully separate any classes. Then, two cases are possible for <span class="No-Break">each leaf:</span></p>
<ul>
<li>If the leaf is pure, predict <span class="No-Break">this class</span></li>
<li>If the leaf is impure, predict the most <span class="No-Break">represented class</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">One way to test all the possible feature values is to use all the existing values in the dataset. Another is to use a linear split over the range of existing values in <span class="No-Break">the dataset.</span></p>
<p>Let’s now train a <a id="_idIndexMarker171"/>decision tree on the iris dataset <span class="No-Break">with scikit-learn:</span></p>
<ol>
<li>First, we need the required imports: <strong class="source-inline">matplotlib</strong> for data visualization (not necessary otherwise), <strong class="source-inline">load_iris</strong> for loading the dataset, <strong class="source-inline">train_test_split</strong> for splitting the data into training and test sets, and the <strong class="source-inline">DecisionTreeClassifier</strong> decision tree implementation <span class="No-Break">from scikit-learn:</span><pre class="source-code">
from matplotlib import pyplot as plt</pre><pre class="source-code">
from sklearn.datasets import load_iris</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.tree import DecisionTreeClassifier</pre></li>
<li>We can now load the data <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">load_iris</strong></span><span class="No-Break">:</span><pre class="source-code">
# Load the dataset</pre><pre class="source-code">
X, y = load_iris(return_X_y=True)</pre></li>
<li>We split the dataset into training and test sets with <strong class="source-inline">train_test_split</strong>, keeping the default parameters and only specifying the random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Split the dataset</pre><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, random_state=0)</pre></li>
<li>In this step, we display a two-dimensional projection of the data. This is just for pedagogical purposes but is <span class="No-Break">not mandatory:</span><pre class="source-code">
# Plot the training points</pre><pre class="source-code">
plt.scatter(X[:, 0], X[:, 1], c=y)</pre><pre class="source-code">
plt.xlabel('Sepal length')</pre><pre class="source-code">
plt.ylabel('Sepal width')</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 4.8 – The three iris classes as a function of the sepal width and sepal length (plot produced  by the code)" height="803" src="image/B19629_04_08.jpg" width="1060"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – The three iris classes as a function of the sepal width and sepal length (plot produced  by the code)</p>
<ol>
<li value="5">Instantiate <a id="_idIndexMarker172"/>the <strong class="source-inline">DecisionTreeClassifier</strong> model. We use the default <span class="No-Break">parameters here:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
dt = DecisionTreeClassifier()</pre></li>
<li>Train the model on the training set. We did not prepare the data with any preprocessing here because we have only quantitative features, and decision trees are not sensitive to scale, unlike linear models. But it would not hurt either to rescale the <span class="No-Break">quantitative features:</span><pre class="source-code">
# Fit the model on the training data</pre><pre class="source-code">
dt.fit(X_train, y_train)</pre></li>
<li>Finally, we evaluate the accuracy of the model on both the training and test sets, using the <strong class="source-inline">score()</strong> method of the <span class="No-Break">classification tree:</span><pre class="source-code">
# Compute the accuracy on training and test sets</pre><pre class="source-code">
print('Accuracy on training set:', dt.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('Accuracy on test set:', dt.score(</pre><pre class="source-code">
    X_test, y_test))</pre></li>
</ol>
<p>This prints the <span class="No-Break">following output:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy on training set: 1.0</strong>
<strong class="bold">Accuracy on test set: 0.9736842105263158</strong></pre>
<p>We are achieving <a id="_idIndexMarker173"/>satisfactory results, even if we are clearly facing overfitting with 100% accuracy on the <span class="No-Break">train set.</span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>There’s more…</h2>
<p>Unlike linear models, there are no weights associated with each feature since a tree is made up <span class="No-Break">of splits.</span></p>
<p>For visualization purposes, we can display the tree thanks to the <strong class="source-inline">graphviz</strong> library. This is mostly for pedagogical use or interest but is not necessarily <span class="No-Break">useful otherwise:</span></p>
<pre class="source-code">
from sklearn.tree import export_graphviz
import graphviz
# We load iris data again to retrieve features and classes names
iris = load_iris()
# We export the tree in graphviz format
graph_data = export_graphviz(
    dt,
    out_file=None,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True, rounded=True
)
# We load the tree again with graphviz library in order to display it
graphviz.Source(graph_data)</pre>
<p>Here is the tree <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 4.9 – Tree visualization produced by the graphviz library" height="738" src="image/B19629_04_09.jpg" width="835"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Tree visualization produced by the graphviz library</p>
<p>From this tree<a id="_idIndexMarker174"/> visualization, we can see that the 37 samples of the setosa class are fully classified right away at the first decision node (considering the data visualization, this should not be a surprise). The samples of classes virginica and versicolor seem to be much more intertwined in the provided features, thus the tree requires many more decision nodes to fully <span class="No-Break">discriminate them.</span></p>
<p>Unlike linear models, we do not have weights associated with each feature. But we can have a piece of somehow equivalent information, called feature importance, available with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">feature_importances</strong></span><span class="No-Break"> attribute:</span></p>
<pre class="source-code">
import numpy as np
plt.bar(iris.feature_names, dt.feature_importances_)
plt.xticks(rotation=45))
plt.ylabel('Feature importance')
plt.title('Feature importance for the decision tree')
plt.show()</pre>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="Figure 4.10 – Feature importance as a function of the feature name (histogram produced by the code)" height="507" src="image/B19629_04_10.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Feature importance as a function of the feature name (histogram produced by the code)</p>
<p>This feature<a id="_idIndexMarker175"/> importance is relative (meaning the sum of all feature importance is equal to 1) and is computed based on the number of samples classified thanks to <span class="No-Break">this feature.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Feature importance is computed based on the amount of reduction of the metric used for splitting (for example, Gini impurity or entropy). If one single feature allows to make all the splits, then<a id="_idIndexMarker176"/> this feature will have an importance <span class="No-Break">of 1.</span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>See also</h2>
<p>The sci-kit learning documentation <a id="_idIndexMarker177"/> on classification trees as available at the following <span class="No-Break">URL: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Building regression trees</h1>
<p>Before digging into<a id="_idIndexMarker178"/> the regularization of decision trees in general, let’s have a recipe for regression trees. Indeed, all the explanations in the previous recipe were assuming we have a classification task. Let’s explain how to apply it to a regression task and apply it to the California <span class="No-Break">housing dataset.</span></p>
<p>For regression trees, only a few steps need to be modified compared to classification trees: the inference and the loss computation. Besides that, the overall principle is <span class="No-Break">the same.</span></p>
<h3>The inference</h3>
<p>In order to make <a id="_idIndexMarker179"/>an<a id="_idIndexMarker180"/> inference, we can no longer use the most represented class in a leaf (or in the case of pure leaf, the only class). So, we use the average of the labels in <span class="No-Break">each node.</span></p>
<p>In the example proposed in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.11</em>, assuming this is a leaf, we would have an inference value that is the average of those 10 values equal to 14 in <span class="No-Break">this case.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 4.11 – The example of 10 samples with associated values" height="225" src="image/B19629_04_11.jpg" width="462"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – The example of 10 samples with associated values</p>
<h3>The loss</h3>
<p>Instead of <a id="_idIndexMarker181"/>using a disorder measurement to compute the loss, in regression trees, the mean squared error is used. So, the loss is <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="" height="95" src="image/Formula_04_022.jpg" width="721"/>
</div>
</div>
<p>Assume again a given split leading to the <img alt="" height="26" src="image/Formula_04_023.png" width="46"/> samples in the left node and the <img alt="" height="24" src="image/Formula_04_024.png" width="51"/> samples in the right node. The <strong class="bold">MSE</strong> for each split is computed using the average of the labels into <span class="No-Break">that node:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 4.12 – Example of a node split on a regression task" height="667" src="image/B19629_04_12.jpg" width="519"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Example of a node split on a regression task</p>
<p>If we take the example<a id="_idIndexMarker182"/> of the proposed split in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.12</em>, we have all we need to compute the <span class="No-Break">L loss:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="" height="102" src="image/Formula_04_025.jpg" width="590"/>
</div>
</div>
<p>Based on those two slight changes, we can train a regression tree with the same recursive, greedy algorithm as with <span class="No-Break">classification trees.</span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Getting ready</h2>
<p>Before getting practical, all we need for this recipe is to have the scikit-learn library installed. If not yet installed, just type the following command line in <span class="No-Break">your terminal:</span></p>
<pre class="source-code">
pip install scikit-learn</pre>
<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>How to do it…</h2>
<p>We will train a <a id="_idIndexMarker183"/>regression tree using the <strong class="source-inline">DecisionTreeRegressor</strong> class from scikit-learn on the California <span class="No-Break">housing dataset:</span></p>
<ol>
<li>First, the required imports: the <strong class="source-inline">fetch_california_housing</strong> function to load the California housing dataset, the <strong class="source-inline">train_test_split</strong> function to split data, and the <strong class="source-inline">DecisionTreeRegressor</strong> class with the regression <span class="No-Break">tree implementation:</span><pre class="source-code">
from sklearn.datasets import fetch_california_housing</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.tree import DecisionTreeRegressor</pre></li>
<li>Load the dataset using the <span class="No-Break"><strong class="source-inline">fetch_california_housing</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre></li>
<li>Split the data into training and test sets using the <span class="No-Break"><strong class="source-inline">train_test_split</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, test_size=0.2, random_state=0)</pre></li>
<li>Instantiate the <strong class="source-inline">DecisionTreeRegressor</strong> object. We just keep the default parameters here, but they can be customized at <span class="No-Break">this point:</span><pre class="source-code">
dt = DecisionTreeRegressor()</pre></li>
<li>Train the regression tree on the training set using the <strong class="source-inline">.fit()</strong> method of the <strong class="source-inline">DecisionTreeRegressor</strong> class. Note that we do not apply any specification preprocessing to the data because we have only quantitative features, and decision trees are not sensitive to feature <span class="No-Break">scale issues:</span><pre class="source-code">
dt.fit(X_train, y_train)</pre><pre class="source-code">
DecisionTreeRegressor()</pre></li>
<li>Evaluate the R2-score of the regression tree on both the training and test sets using the built-in <strong class="source-inline">.score()</strong> method of the <span class="No-Break">model class:</span><pre class="source-code">
print('R2-score on training set:', dt.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', dt.score(</pre><pre class="source-code">
    X_test, y_test))</pre></li>
</ol>
<p>This would show something <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 1.0</strong>
<strong class="bold">R2-score on test set: 0.5923572475948657</strong></pre>
<p>As we can see, we <a id="_idIndexMarker184"/>face here a strong overfitting, having a perfect R2-score on the training set, while having a much worse (but still decent overall) R2-score on the <span class="No-Break">test set.</span></p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>See also</h2>
<p>Look at the <a id="_idIndexMarker185"/>official <strong class="source-inline">DecisionTreeRegressor</strong> documentation more <span class="No-Break">information: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>Regularizing a decision tree</h1>
<p>In this recipe, we<a id="_idIndexMarker186"/> will look at the means to regularize decision trees. We will review and comment on a couple of methods for reference and provide a few more to <span class="No-Break">be explored.</span></p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Getting ready</h2>
<p>Obviously, we cannot use L1 or L2 regularization as we did with linear models. Since we have no weights for the features and no overall loss such as the mean squared error or the binary cross entropy, it is not possible to apply this <span class="No-Break">method here.</span></p>
<p>But we do have other ways to regularize, such as the max depth of the tree, the minimum number of samples per leaf, the minimum number of samples per split, the max number of features, or the minimum impurity decrease. In this recipe, we will look <span class="No-Break">at those.</span></p>
<p>To do that, we only need the following libraries: scikit-learn, <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">NumPy</strong>. Also, since we will provide some visualization to give some idea of regularization, we will<a id="_idIndexMarker187"/> use the following <span class="No-Break"><strong class="source-inline">plot_decision_function</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
def plot_decision_function(dt, X, y):
    # Create figure to draw chart
    plt.figure(2, figsize=(8, 6))
    # We create a grid of points contained within [x_min,
      #x_max]x[y_min, y_max] with step h=0.02
    x0_min, x0_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    x1_min, x1_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = .02  # step size of the grid
    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, h),
        np.arange(x1_min, x1_max, h))
    # Retrieve predictions for each point of the grid
    Z_dt = dt.predict(np.c_[xx0.ravel(), xx1.ravel()])
    Z_dt = Z_dt.reshape(xx0.shape)
    # Plot the decision boundary (label predicted assigned to a color)
    plt.pcolormesh(xx0, xx1, Z_dt, cmap=plt.cm.Paired)
    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k',
        cmap=plt.cm.Paired)
    # Format chart
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xticks(())
    plt.yticks(())
    plt.show()</pre>
<p>This function will allow us to visualize the decision function of our decision tree and get a better<a id="_idIndexMarker188"/> understanding of what is overfitting and regularization when it comes to a <span class="No-Break">classification tree.</span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>How to do it…</h2>
<p>We will give a recipe to regularize a decision tree based on the maximum depth, and then explore a few others in the <em class="italic">There’s </em><span class="No-Break"><em class="italic">more</em></span><span class="No-Break"> section.</span></p>
<p>The maximum depth is quite often one of the first hyperparameters to fine-tune when trying to regularize. Indeed, as we have seen earlier, decision trees can learn complex data patterns using more decision nodes. If not stopped, the decision trees may tend to overfit the data with too many consecutive <span class="No-Break">decision nodes.</span></p>
<p>We will now train a classification tree with a limited maximum depth on the <span class="No-Break">iris dataset:</span></p>
<ol>
<li>Make the <span class="No-Break">required imports:</span><ul><li>The <strong class="source-inline">load_iris</strong> function to load <span class="No-Break">the dataset</span></li><li>The <strong class="source-inline">train_test_split</strong> function to split the data into training and <span class="No-Break">test sets</span></li><li>The <span class="No-Break"><strong class="source-inline">DecisionTreeClassifier</strong></span><span class="No-Break"> class:</span><pre class="source-code">
from sklearn.datasets import load_iris</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.tree import DecisionTreeClassifier</pre></li></ul></li>
<li>Load the dataset using the <strong class="source-inline">load_iris</strong> function. In order to be able to fully visualize the effects of regularization, we also keep only two features out of four, so that we can display them on <span class="No-Break">a plot:</span><pre class="source-code">
X, y = load_iris(return_X_y=True)</pre><pre class="source-code">
# Keep only 2 features</pre><pre class="source-code">
X = X[:, :2]</pre></li>
<li>Split the data into training and test sets using the <strong class="source-inline">train_test_split</strong> function. We <a id="_idIndexMarker189"/>only specify the random state for reproducibility and let the other parameters be <span class="No-Break">by default:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, random_state=0)</pre></li>
<li>Instantiate a decision tree object, limiting the maximum depth to five with the <strong class="source-inline">max_depth=5</strong> parameter. We also set the random state to <strong class="source-inline">0</strong> <span class="No-Break">for reproducibility:</span><pre class="source-code">
dt = DecisionTreeClassifier(max_depth=5,</pre><pre class="source-code">
    random_state=0)</pre></li>
<li>Fit the classification tree on the training set using the <strong class="source-inline">.fit()</strong> method. As mentioned earlier, since the features are all quantitative and decision trees are not sensitive to the features scale, there is no need to <span class="No-Break">apply rescaling:</span><pre class="source-code">
dt.fit(X_train, y_train)</pre><pre class="source-code">
DecisionTreeClassifier(max_depth=5, random_state=0)</pre></li>
<li>Evaluate the model accuracy, using the <strong class="source-inline">.score()</strong> method of the <span class="No-Break"><strong class="source-inline">DecisionTreeClassifier</strong></span><span class="No-Break"> class:</span><pre class="source-code">
print('Accuracy on training set:', dt.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('Accuracy on test set:', dt.score(</pre><pre class="source-code">
    X_test, y_test))</pre></li>
</ol>
<p>This would print the <span class="No-Break">following output:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy on training set: 0.8660714285714286</strong>
<strong class="bold">Accuracy on test set: 0.6578947368421053</strong></pre>
<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>How it works…</h2>
<p>In order to have <a id="_idIndexMarker190"/>a better understanding of how it works, let’s look at the two dimensions of the iris dataset we retained. We will use the <strong class="source-inline">plot_decision_function()</strong> function defined in <em class="italic">Getting ready</em> to plot the decision function of a decision tree with no regularization (that is, <span class="No-Break">default hyperparameters):</span></p>
<pre class="source-code">
import numpy as np
from matplotlib import pyplot as plt
# Fit a decision tree over only 2 features
dt = DecisionTreeClassifier()
dt.fit(X_train[:, :2], y_train)
# Plot the decision tree decision function
plot_decision_function(dt, X_train[:, :2], y_train)</pre>
<p>Here is <span class="No-Break">the plot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 4.13 – Decision function of the model as a function of the sepal width and sepal length with a very complex and questionable decision function (plot produced by the code)" height="1247" src="image/B19629_04_13.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Decision function of the model as a function of the sepal width and sepal length with a very complex and questionable decision function (plot produced by the code)</p>
<p>From this plot, we can deduce we are typically facing overfitting. Indeed, the boundaries are really specific, sometimes<a id="_idIndexMarker191"/> trying to make a complex pattern for only one sample, instead of focusing on the <span class="No-Break">higher-level pattern.</span></p>
<p>Indeed, if we look at the accuracy score for both the training and test set, we have the <span class="No-Break">following results:</span></p>
<pre class="source-code">
# Compute the accuracy on training and test sets for only 2 features
print('Accuracy on training set:', dt.score(X_train[:, :2], y_train))
print('Accuracy on test set:', dt.score(X_test[:, :2], y_test))</pre>
<p>We’ll get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Accuracy on training set: 0.9375
Accuracy on test set: 0.631578947368421</pre>
<p>While the accuracy is about 94% on the training set, it is only about 63% on the test set. There is overfitting, and regularization may <span class="No-Break">be helpful.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The accuracy is far lower than in the first recipe because we use only two features for visualization and pedagogical purposes. The reasoning remains true if we keep the four <span class="No-Break">features, though.</span></p>
<p>Let’s now add regularization by limiting the maximum depth of the decision tree as we did in <span class="No-Break">this recipe:</span></p>
<pre class="source-code">
max_depth: int, default=None</pre>
<p>If the maximum <a id="_idIndexMarker192"/>depth of the tree is <strong class="bold">None</strong>, then nodes are expanded until all leaves are pure or until all leaves contain less than the <span class="No-Break"><strong class="source-inline">min_samples_split</strong></span><span class="No-Break"> samples.</span></p>
<p>It means that by default, the trees are expanded with no limit on the depth. The limit is then perhaps set by other factors and may go very deep. If we fix that by limiting the depth to <strong class="source-inline">5</strong>, let’s see the impact on the <span class="No-Break">decision function:</span></p>
<pre class="source-code">
# Fit a decision tree with max depth of 5 over only 2 features
dt = DecisionTreeClassifier(max_depth=5, random_state=0)
dt.fit(X_train[:, :2], y_train)
# Plot the decision tree decision function
plot_decision_function(dt, X_train[:, :2], y_train)</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 4.14 – Decision function with maximum depth regularization (plot produced by the code)" height="1247" src="image/B19629_04_14.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Decision function with maximum depth regularization (plot produced by the code)</p>
<p>By limiting the <a id="_idIndexMarker193"/>max depth to <strong class="source-inline">5</strong>, we get a less specific decision function, even if there seems to be some overfitting remaining. If we have a look again at the accuracy score, we can see that it actually <span class="No-Break">helped slightly:</span></p>
<pre class="source-code">
# Compute the accuracy on training and test sets for only 2 features
print('Accuracy on training set:', dt.score(X_train[:, :2], y_train))
print('Accuracy on test set:', dt.score(X_test[:, :2], y_test))</pre>
<p>This would provide the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Accuracy on training set: 0.8660714285714286
Accuracy on test set: 0.6578947368421053</pre>
<p>Indeed, the accuracy score on the test set climbed from 63% to 66%, while the accuracy on the training set decreased from 95% to 87%. This is typically what we can expect from regularization: this added bias (and thus decreased training set performances) and decreased the<a id="_idIndexMarker194"/> variance (and thus allowed us to <span class="No-Break">generalize better).</span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>There’s more…</h2>
<p>The maximum depth hyperparameter is really convenient because it’s easy to understand and fine-tune. But, there are many other hyperparameters that can help regularize decision trees. Let’s review some of them here. We will focus on the minimum sample hyperparameters and then propose a few <span class="No-Break">other hyperparameters.</span></p>
<h3>Minimum samples</h3>
<p>Other hyperparameters <a id="_idIndexMarker195"/>allowing us to <a id="_idIndexMarker196"/>regularize are the ones controlling the minimum number of samples per leaf and the minimum number of samples <span class="No-Break">per split.</span></p>
<p>The idea is rather straightforward and intuitive but more subtle than the maximum depth. In the decision tree we visualized earlier in this chapter, we could see that the first splits classify substantial amounts of samples. The first split successfully classified 37 samples as setosa while keeping 75 in the other split. On the other end of the decision tree, the lowest nodes are sometimes splitting over only three or <span class="No-Break">four samples.</span></p>
<p>Is splitting over only three samples significant? What if out of these three samples, there is an outlier? Generally, it does not sound like a promising idea to create a rule for only three samples if the end goal is to have a robust, <span class="No-Break">well-generalizing model.</span></p>
<p>We have two different but somewhat related hyperparameters that allow us to deal <span class="No-Break">with that:</span></p>
<ul>
<li><strong class="source-inline">min_samples_split</strong>: The minimum samples required to split an internal node. If a float is provided, then it uses a fraction of the total number <span class="No-Break">of samples.</span></li>
<li><strong class="source-inline">min_samples_leaf</strong>: The minimum samples required to be considered a leaf. If a float is provided, then it uses a fraction of the total number <span class="No-Break">of samples.</span></li>
</ul>
<p>While <strong class="source-inline">min_samples_split</strong> is acting at the decision node level, <strong class="source-inline">min_samples_leaf</strong> is acting only at the <span class="No-Break">leaf level.</span></p>
<p>Let’s see if that allows us to avoid overfitting in specific regions in our case. We set the minimum number of samples per split to 15 (while keeping all other parameters to default values). This is expected to regularize, since we know from the decision tree visualization<a id="_idIndexMarker197"/> some splits were for less<a id="_idIndexMarker198"/> than <span class="No-Break">five samples:</span></p>
<pre class="source-code">
# Fit a decision tree with min samples per split of 15 over only 2 features
dt = DecisionTreeClassifier(min_samples_split=15, random_state=0)
dt.fit(X_train[:, :2], y_train)
# Plot the decision tree decision function
plot_decision_function(dt, X_train[:, :2], y_train)</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 4.15 – Decision function with minimum samples per split regularization (plot produced by the code)" height="1247" src="image/B19629_04_15.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Decision function with minimum samples per split regularization (plot produced by the code)</p>
<p>The resulting decision function is a bit different than when regularizing with the maximum<a id="_idIndexMarker199"/> depth and seems to be indeed more <a id="_idIndexMarker200"/>regularized than without constraint on <span class="No-Break">this hyperparameter.</span></p>
<p>We can also look at the accuracy score to confirm the regularization <span class="No-Break">was successful:</span></p>
<pre class="source-code">
# Compute the accuracy on training and test sets for only 2 features
print('Accuracy on training set:', dt.score(X_train[:, :2],
    y_train))
print('Accuracy on test set:', dt.score(X_test[:, :2],
    y_test))</pre>
<p>We’ll get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Accuracy on training set: 0.85714285714285717
Accuracy on test set: 0.7368421052631579</pre>
<p>Compared to default hyperparameters, the accuracy score on the test set climbed from 63% to 74%, while the accuracy on the training set decreased from 95% to 86%. Compared to the maximum depth hyperparameter, we added slightly more regularization, and got slightly better results on the <span class="No-Break">test set.</span></p>
<p>In general, the hyperparameters on the number of samples (either per leaf or split) may allow a finer regularization than the maximum depth. Indeed, the max depth hyperparameter is setting a common hard limit to the whole decision tree. But it may happen that two nodes at the same depth level do not carry the same number of samples. One node may have hundreds of samples (and then a splitting is probably relevant), while another node may have just a <span class="No-Break">few samples.</span></p>
<p>The criterion for a minimum number of samples on its side is more subtle: no matter the depth in the tree, if a node does not have enough samples, then we decide it is not <span class="No-Break">worth splitting.</span></p>
<h3>Other hyperparameters</h3>
<p>Other<a id="_idIndexMarker201"/> hyperparameters can be used to <a id="_idIndexMarker202"/>regularize. We will not go through all the details for each of them, but rather list them and explain <span class="No-Break">them briefly:</span></p>
<ul>
<li><strong class="source-inline">max_features</strong>: By default, the decision tree is finding the best split among all features. You can choose to add randomness by setting another maximum number of features to use at each split. May add regularization by <span class="No-Break">adding noise.</span></li>
<li><strong class="source-inline">max_leaf_nodes</strong>: Set a straight limit on the number of leaves in the tree. Somewhat like the max depth hyperparameter, it will regularize by limiting the number of splits, giving priority to nodes having the highest <span class="No-Break">impurity reduction.</span></li>
<li><strong class="source-inline">min_impurity_decrease</strong>: This will split a node only if the impurity decrease is above the given threshold. This allows us to regularize by selecting highly impacting node <span class="No-Break">splits only.</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Although we did not mention regression trees, the behavior and principles are analogous, and <a id="_idIndexMarker203"/>the same hyperparameters<a id="_idIndexMarker204"/> can be fine-tuned with the <span class="No-Break">same behavior.</span></p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor120"/>See also</h2>
<p>The scikit-learn documentation is pretty explicit about all the hyperparameters and their potential<a id="_idIndexMarker205"/> <span class="No-Break">impact: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Training the Random Forest algorithm</h1>
<p>The Random Forest<a id="_idIndexMarker206"/> algorithm is an ensemble learning model, meaning it uses an ensemble of decision trees, hence <em class="italic">forest</em> in <span class="No-Break">its name.</span></p>
<p>In this recipe, we will explain how it works and then train a Random Forest model on the California <span class="No-Break">housing dataset.</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor122"/>Getting ready</h2>
<p>Ensemble learning is based somehow on the idea of collective intelligence. Let’s do a thought experiment to understand the power of <span class="No-Break">collective intelligence.</span></p>
<p>Let’s assume we have a bot that randomly answers correctly to any binary question 51% of the time. This would be considered inefficient <span class="No-Break">and unreliable.</span></p>
<p>But now, let’s also assume we are using not only one but an army of those randomly answering bots and use the majority vote as the final answer. If we have 1,000 of those bots, the majority vote will provide the right answer 75% of the time. If we have 10,000 bots, the majority vote will provide the right answer 97% of the time. This would turn a low-performing system into a remarkably <span class="No-Break">high-performing system.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">A strong assumption was made for this example: each bot must be independent of the others. Otherwise, this example does not hold true. Indeed, the extreme counter-example would be that all bots are answering the same answer to any question, in which case, no matter how many bots you use, the accuracy remains <span class="No-Break">at 51%.</span></p>
<p>This is the idea of collective intelligence, which relates somehow to human society. Most of the time, collective knowledge outperforms <span class="No-Break">individual knowledge.</span></p>
<p>This is also the idea behind ensemble models: an ensemble of weak learners can become a powerful<a id="_idIndexMarker207"/> model. To do that with Random Forest, we need to define two <span class="No-Break">key aspects:</span></p>
<ul>
<li>How to compute the <span class="No-Break">majority vote</span></li>
<li>How to ensure the independence of each decision tree in our model <span class="No-Break">with randomness</span></li>
</ul>
<h3>Majority vote</h3>
<p>To properly explain <a id="_idIndexMarker208"/>majority vote, let’s imagine we have an ensemble of three decision trees trained on a binary classification task. On a given sample, the predictions are <span class="No-Break">the following:</span></p>
<table class="T---Table" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">Tree</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="bold">Predicted probability of </strong><span class="No-Break"><strong class="bold">class 1</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">Class predictions</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Tree 1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">0.05</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Tree 2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">0.6</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Tree 3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">0.55</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Predictions</p>
<p>We have two pieces of information for each <span class="No-Break">decision tree:</span></p>
<ul>
<li>The predicted probability of <span class="No-Break">class 1</span></li>
<li>The predicted class (usually computed as class 1 if probability &gt; 0.5, class <span class="No-Break">0 otherwise)</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">The <strong class="source-inline">DecisionTreeClassifier.predict_proba()</strong> method allows us to get the prediction probability. It is computed by using the proportion of the given class in the <span class="No-Break">prediction leaf.</span></p>
<p>We could come up with many ways to compute the majority vote on such data, but let’s explore two, hard vote and <span class="No-Break">soft vote:</span></p>
<ul>
<li>A hard vote is the most intuitive one. This is the simple majority vote of the predicted classes. In our case, class 1 is predicted two times out of three. In this case, the hard majority vote is <span class="No-Break">class 1.</span></li>
<li>A soft vote uses the average probability and then applies a threshold. In our case, the average probability is 0.4, which is below the threshold of 0.5. In that case, the soft majority vote is <span class="No-Break">class 0.</span></li>
</ul>
<p>It is particularly interesting to note that, even if two out of three trees predicted class 1, the only tree that was really confident (having a high probability) was the tree predicting <span class="No-Break">class 0.</span></p>
<p>A real-life example would be, when facing <span class="No-Break">a question:</span></p>
<ul>
<li>Two friends give answer A, but <span class="No-Break">are unsure</span></li>
<li>One friend gives answer B but is <span class="No-Break">highly confident</span></li>
</ul>
<p>What would you do in such a case? The odds are you would listen to that highly confident <a id="_idIndexMarker209"/>friend. This is exactly what a soft majority vote is about: giving more power to highly confident trees. Most of the time, the soft vote outperforms the hard vote. Fortunately, Random Forest implementation in scikit-learn is based on the <span class="No-Break">soft vote.</span></p>
<h3>Bagging</h3>
<p>Bagging is a key <a id="_idIndexMarker210"/>concept in Random Forest to ensure the independence of decision trees and is made of bootstrapping and aggregating. Let’s see how those two steps are working together to get the best out of ensembling <span class="No-Break">decision trees.</span></p>
<p>Bootstrapping is random sampling with replacement. In simple terms, if we apply bootstrapping to samples with replacement, it means we will randomly pick samples in the dataset with replacement. What <em class="italic">with replacement</em> means is that, once a sample has been picked, it is not removed from the dataset and may be <span class="No-Break">picked again.</span></p>
<p>Let’s assume we have an initial dataset of 10 samples, either blue or red. If we use bootstrapping to select 10 samples in this initial dataset, we may have some samples missing, and some samples appearing several times. If we do that three independent times, we may have three slightly different datasets, such as in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.16</em>. We call those newly created <a id="_idIndexMarker211"/><span class="No-Break">datasets subsamples:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 4.16 – An example of bootstrapping an initial dataset three times and selecting 10 samples for replacement (the three created subsamples are slightly different)" height="656" src="image/B19629_04_16.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – An example of bootstrapping an initial dataset three times and selecting 10 samples for replacement (the three created subsamples are slightly different)</p>
<p>Since those subsamples are slightly different, in Random Forest, a decision tree is trained on each of those and ends up with hopefully independent models. The next step is the aggregating of those models’ results, through a soft majority vote. Once those two steps (bootstrapping and aggregating) are combined, this is what we call bagging. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.17</em> summarizes those <span class="No-Break">two steps:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 4.17 – Bootstrapping on the samples and then aggregating the results to end with an ensemble model" height="923" src="image/B19629_04_17.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Bootstrapping on the samples and then aggregating the results to end with an ensemble model</p>
<p>As we have seen, the <em class="italic">random</em> in Random Forest comes from the bootstrapping of the samples, meaning we randomly select a subsample of the original dataset for each trained decision tree. But in reality, other levels of randomness have been omitted for pedagogical <a id="_idIndexMarker212"/>reasons. Without going into all the details, there are three levels of randomness in<a id="_idIndexMarker213"/> a Random <span class="No-Break">Forest algorithm:</span></p>
<ul>
<li><strong class="bold">Bootstrapping of the samples</strong>: Samples are selected <span class="No-Break">with replacement</span></li>
<li><strong class="bold">Bootstrapping of the features</strong>: Features are selected <span class="No-Break">with replacement</span></li>
<li><strong class="bold">Feature selection of the best split of a node</strong>: By default, in scikit-learn, all features are used, thus there is no randomness at <span class="No-Break">this level</span></li>
</ul>
<p>Now that we have a solid enough understanding of how Random Forest works, let’s train a Random Forest algorithm on a regression task. To do so, we only need scikit-learn to be installed. If this has not already been done, just install it with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install scikit-learn</pre>
<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>How to do it…</h2>
<p>As with other <a id="_idIndexMarker214"/>machine learning models in scikit-learn, training a Random Forest algorithm is quite easy. There are two <span class="No-Break">main classes:</span></p>
<ul>
<li><strong class="source-inline">RandomForestRegressor</strong> for <span class="No-Break">regression tasks</span></li>
<li><strong class="source-inline">RandomForestClassifier</strong> for <span class="No-Break">classification tasks</span></li>
</ul>
<p>Here, we will use the <strong class="source-inline">RandomForestRegressor</strong> on the California <span class="No-Break">housing dataset:</span></p>
<ol>
<li>First, let’s make the required imports: <strong class="source-inline">fetch_california_housing</strong> to load the data, <strong class="source-inline">train_test_split</strong> for splitting the dataset, and <strong class="source-inline">RandomForestRegressor</strong> for the <span class="No-Break">model itself:</span><pre class="source-code">
from sklearn.datasets import fetch_california_housing</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.ensemble import RandomForestRegressor</pre></li>
<li>Load the dataset <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">fetch_california_housing</strong></span><span class="No-Break">:</span><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre></li>
<li>Split the data with <strong class="source-inline">train_test_split</strong>. Here, we just use the default parameters and set the random state to 0 <span class="No-Break">for reproducibility:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, random_state=0)</pre></li>
<li>Instantiate the <strong class="source-inline">RandomForestRegressor</strong> model. We just keep the default parameters of the class here for simplicity; we only specify the <span class="No-Break">random state:</span><pre class="source-code">
rf = RandomForestRegressor(random_state=0)</pre></li>
<li>Train the model on the training set with the <strong class="source-inline">.fit()</strong> method. This can take a few seconds <span class="No-Break">to compute:</span><pre class="source-code">
rf.fit(X_train, y_train)</pre><pre class="source-code">
RandomForestRegressor(random_state=0)</pre></li>
<li>Evaluate the R2-score on both the training and test set using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">score()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
# Display the accuracy on both training and test set</pre><pre class="source-code">
print('R2-score on training set:', rf.score(X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', rf.score(X_test, y_test))</pre></li>
</ol>
<p>Our output would be <span class="No-Break">as follows:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 0.9727159677969947</strong>
<strong class="bold">R2-score on test set: 0.7941678302821006</strong></pre>
<p>We have an R2-score on the training set of 97%, while on the test set, it is only 79%. This means we are facing<a id="_idIndexMarker215"/> overfitting, and we will see in the next recipe how to add regularization to <span class="No-Break">such models.</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>See also</h2>
<ul>
<li>The <a id="_idIndexMarker216"/>documentation of this class in <span class="No-Break">scikit-learn: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor</span></a></li>
<li>Likewise, there is the documentation of the Random Forest classifier for classification <span class="No-Break">tasks: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml</span></a></li>
</ul>
<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Regularization of Random Forest</h1>
<p>A Random Forest <a id="_idIndexMarker217"/>algorithm shares many hyperparameters with decision trees since a Random Forest is made up of trees. But a few more hyperparameters do exist, so in this recipe, we will present them and show how to use them to improve results on the California housing <span class="No-Break">dataset regression.</span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>Getting started</h2>
<p>Random Forests are known to be quite prone to overfitting. Even if it’s not a formal proof, in the previous recipe, we were indeed facing quite strong overfitting. But Random Forests, like decision trees, have many hyperparameters allowing us to try to reduce overfitting. As for a decision tree, we can use the <span class="No-Break">following hyperparameters:</span></p>
<ul>
<li><span class="No-Break">Maximum depth</span></li>
<li>Minimum samples <span class="No-Break">per leaf</span></li>
<li>Minimum samples <span class="No-Break">per split</span></li>
<li><span class="No-Break"><strong class="source-inline">max_features</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">max_leaf_nodes</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">min_impurity_decrease</strong></span></li>
</ul>
<p>But some other <a id="_idIndexMarker218"/>hyperparameters can be <span class="No-Break">fine-tuned too:</span></p>
<ul>
<li><strong class="source-inline">n_estimators</strong>: This is the number of decision trees trained in the <span class="No-Break">random forest.</span></li>
<li><strong class="source-inline">max_samples</strong>: The number of samples to draw from the given dataset to train each decision tree. A lower value would <span class="No-Break">add regularization.</span></li>
</ul>
<p>Technically speaking, for this recipe, it is assumed that scikit-learn <span class="No-Break">is installed.</span></p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor127"/>How to do it…</h2>
<p>In this recipe, we will try to add regularization by limiting the max number of features to the log of the total number of features. If you are reusing the same environment as for the previous recipe, you can jump directly to <span class="No-Break"><em class="italic">step 4</em></span><span class="No-Break">:</span></p>
<ol>
<li>As usual, let’s make the required imports: <strong class="source-inline">fetch_california_housing</strong> to load the data, <strong class="source-inline">train_test_split</strong> for splitting the dataset, and <strong class="source-inline">RandomForestRegressor</strong> for the <span class="No-Break">model itself:</span><pre class="source-code">
from sklearn.datasets import fetch_california_housing</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.ensemble import RandomForestRegressor</pre></li>
<li>Load the dataset <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">fetch_california_housing</strong></span><span class="No-Break">:</span><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre></li>
<li>Split the data with <strong class="source-inline">train_test_split</strong>. Here, we just use the default parameters, meaning we have a 75%25% split and set the random state to <strong class="source-inline">0</strong> <span class="No-Break">for reproducibility:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, random_state=0)</pre></li>
<li>Instantiate the <strong class="source-inline">RandomForestRegressor</strong> model. This time, we specify <strong class="source-inline">max_features='log2'</strong> so that for each split, only a random subset (of size <strong class="source-inline">log2(n)</strong>, assuming <a id="_idIndexMarker219"/><em class="italic">n</em> features) of all the features <span class="No-Break">is used:</span><pre class="source-code">
rf = RandomForestRegressor(max_features='log2', random_state=0)</pre></li>
<li>Train the model on the training set with the <strong class="source-inline">.fit()</strong> method. This may take a few seconds <span class="No-Break">to compute:</span><pre class="source-code">
rf.fit(X_train, y_train)</pre><pre class="source-code">
RandomForestRegressor(max_features='log2', random_state=0)</pre></li>
<li>Evaluate the R2-score on both the training and test set using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">score()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
print('R2-score on training set:', rf.score(X_train,</pre><pre class="source-code">
    y_train))</pre><pre class="source-code">
print('R2-score on test set:', rf.score(X_test,</pre><pre class="source-code">
    y_test))</pre></li>
</ol>
<p>This would return <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 0.9748218476882353</strong>
<strong class="bold">R2-score on test set: 0.8137208340736402</strong></pre>
<p>Compared to the previous recipe with default hyperparameters, it improved the R2-score on the test set from 79% to 81%, while not significantly changing the score on the <span class="No-Break">training set.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In this case, as in many others in machine learning, it might be tricky (or sometimes impossible) to have the performances on the train and test set meeting halfway, meaning that even if the R2-score is 97% on training and 79% on the test set, there is absolutely no guarantee you can improve the R2-score on the test set. Sometimes, even the best hyperparameters are not the right key to <span class="No-Break">improve performance.</span></p>
<p>In a word, all the regularization rules for decision trees can be applied to Random Forests, and a<a id="_idIndexMarker220"/> few more are available. As usual, an effective way to find the right set of hyperparameters is through hyperparameter optimization. Random Forest takes somewhat longer to train than a simple decision tree, so it may take <span class="No-Break">some time.</span></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>Training a boosting model with XGBoost</h1>
<p>Let’s now see <a id="_idIndexMarker221"/>another application of decision trees: boosting. While bagging (used in Random Forest models) is training several trees in parallel, boosting is about training trees sequentially. In this recipe, we will have a quick review of what is boosting, and then train a boosting model with XGBoost, a widely used <span class="No-Break">boosting library.</span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor129"/>Getting ready</h2>
<p>Let’s have a look at introducing limits of bagging, then see how boosting may address some of those <a id="_idIndexMarker222"/>limits and how. Finally, let’s train a model on the already prepared Titanic dataset <span class="No-Break">with XGBoost.</span></p>
<h3>Limits of bagging</h3>
<p>Let’s assume we <a id="_idIndexMarker223"/>have a binary classification task, and we trained Random Forest in three decision trees on two features. Bagging is expected to perform well if anywhere in the feature space, at least two out of three decision trees are right, as in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.18</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 4.18 – The absence of overlap in dashed circle areas highlights decision tree errors, demonstrating Random Forest’s strong performance" height="420" src="image/B19629_04_18.jpg" width="1059"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – The absence of overlap in dashed circle areas highlights decision tree errors, demonstrating Random Forest’s strong performance</p>
<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.18</em>, we observe that the areas inside the dashed circles are where a decision tree is wrong. Since they don’t overlap, at least two out of three decision trees are right everywhere. Thus, Random Forest is <span class="No-Break">performing well.</span></p>
<p>Unfortunately, always having two out of three decision trees right is a strong assumption. What happens if only one or fewer decision tree is right in the feature space? As represented in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.19</em>, the Random Forest algorithm starts <span class="No-Break">performing poorly.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 4.19 – When one or fewer out of three decision trees is right, Random Forest is performing poorly" height="420" src="image/B19629_04_19.jpg" width="1059"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – When one or fewer out of three decision trees is right, Random Forest is performing poorly</p>
<p>Let’s see how boosting can fix this issue by sequentially training decision trees to each try to fix the errors <a id="_idIndexMarker224"/>of the <span class="No-Break">previous one.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Those examples are simplified since Random Forest can be using soft vote, and thus predict a class that only a minority of trees predicted. But the principle remains <span class="No-Break">true overall.</span></p>
<h3>Gradient boosting principles</h3>
<p>Gradient boosting <a id="_idIndexMarker225"/>has several <a id="_idIndexMarker226"/>implementations with a some differences: XGBoost, CatBoost, and LightGBM all have pros and cons, and the details of each are beyond the scope of this book. Rather, we will explain some general principles of the gradient boosting algorithm, enough to give a high-level understanding of <span class="No-Break">the model.</span></p>
<p>The algorithm training can be summarized with the <span class="No-Break">following steps:</span></p>
<ol>
<li>Compute an average guess on the <img alt="" height="24" src="image/Formula_04_026.png" width="27"/> <span class="No-Break">training set.</span></li>
<li>Compute the pseudo-residuals of each sample toward the last guessed <span class="No-Break">prediction, <img alt="" height="23" src="image/Formula_04_027.png" width="113"/>.</span></li>
<li>Train a decision tree on the pseudo-residuals as labels, allowing to have <span class="No-Break">predictions <img alt="" height="23" src="image/Formula_04_028.png" width="17"/>.</span></li>
<li>Compute the weight <img alt="" height="19" src="image/Formula_04_029.png" width="19"/> of that decision tree based on <span class="No-Break">its performance.</span></li>
<li>Update the guess with the learning rate 𝜂, 𝛾<span class="subscript">i</span> and these <span class="No-Break">predicted pseudo-residuals:</span>
<img alt="" height="24" src="image/Formula_04_030.png" width="169"/><span class="No-Break">o.</span></li>
<li>Go back to <em class="italic">step 2</em>. with the updated <img alt="" height="23" src="image/Formula_04_031.png" width="38"/>, iterate until reaching the maximum number of decision trees or <span class="No-Break">another criterion.</span></li>
</ol>
<p>In the end, the <a id="_idIndexMarker227"/>final prediction will <a id="_idIndexMarker228"/>be <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="" height="120" src="image/Formula_04_032.jpg" width="390"/>
</div>
</div>
<p>The 𝜂 learning rate is a hyperparameter, <span class="No-Break">typically 0.001.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">All boosting implementations are a bit different. For example, not all boosting implementations have weights 𝛾 associated with their trees. But since this is the case for XGBoost that we will use here, it is worth mentioning it for a <span class="No-Break">better understanding.</span></p>
<p>In the end, having enough decision trees allows a model to perform well enough in most cases, hopefully. Unlike Random Forest, boosting models tend to avoid pitfalls such as having most number of wrong decision trees at the same place, since each decision tree is trying to fix <span class="No-Break">remaining errors.</span></p>
<p>Also, boosting models tend to be more robust and generalized than Random Forest models, making them really powerful in <span class="No-Break">many applications.</span></p>
<p>Finally, for this recipe, we will need the following libraries to be installed: <strong class="source-inline">pickle</strong> and <strong class="source-inline">xgboost</strong>. They can be installed using <strong class="source-inline">pip</strong> with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pickle xgboost</pre>
<p>We will also reuse a prepared Titanic dataset from a previous recipe to avoid spending too much time on the data preparation. This data can be downloaded at <a href="https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl">https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl</a> and should be added locally before doing <a id="_idIndexMarker229"/>the recipe with the<a id="_idIndexMarker230"/> following <span class="No-Break">command line:</span></p>
<pre class="source-code">
wget https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl</pre>
<h2 id="_idParaDest-130"><a id="_idTextAnchor130"/>How to do it…</h2>
<p>XGBoost is a very <a id="_idIndexMarker231"/>popular implementation of gradient boosting. It can be used with the same pattern as models in scikit-learn, using the <span class="No-Break">following methods:</span></p>
<ul>
<li><strong class="source-inline">fit(X, y)</strong> to train <span class="No-Break">the model</span></li>
<li><strong class="source-inline">predict(X)</strong> to <span class="No-Break">compute predictions</span></li>
<li><strong class="source-inline">score(X, y)</strong> to evaluate <span class="No-Break">the model</span></li>
</ul>
<p>Let’s use it on the Titanic dataset with the default parameters <span class="No-Break">downloaded locally:</span></p>
<ol>
<li>The first step is the required imports. Here, we need pickle to read the data from the binary format and the <strong class="source-inline">XGBoost</strong> classification model <span class="No-Break">class </span><span class="No-Break"><strong class="source-inline">XGBClassifier</strong></span><span class="No-Break">:</span><pre class="source-code">
import pickle</pre><pre class="source-code">
from xgboost import XGBClassifier</pre></li>
<li>We load the already prepared data using pickle. Note that we already get a split dataset because it was actually saved <span class="No-Break">this way:</span><pre class="source-code">
X_train, X_test, y_train, y_test = pickle.load(open(</pre><pre class="source-code">
    'prepared_titanic.pkl', 'rb'))</pre></li>
<li>Instantiate the boosting model. We specify <strong class="source-inline">use_label_encoder=False</strong> because our qualitative features are actually already encoded with one hot encoding and because this feature is about to <span class="No-Break">be deprecated:</span><pre class="source-code">
bst = XGBClassifier(use_label_encoder=False)</pre></li>
<li>Train the model on the training set using the <strong class="source-inline">.fit()</strong> method, exactly like we would do for a <span class="No-Break">scikit-learn model:</span><pre class="source-code">
# Train the model on training set</pre><pre class="source-code">
bst.fit(X_train, y_train)</pre></li>
<li>Compute the accuracy of the model on both training and test sets, using the <strong class="source-inline">.score()</strong> method. Again, this is the same as <span class="No-Break">in scikit-learn:</span><pre class="source-code">
print('Accuracy on training set:', bst.score(X_train,</pre><pre class="source-code">
    y_train))</pre><pre class="source-code">
print('Accuracy on test set:', bst.score(X_test,</pre><pre class="source-code">
    y_test))</pre></li>
</ol>
<p>We’ll get the <span class="No-Break">following now:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy on training set: 0.9747191011235955</strong>
<strong class="bold">Accuracy on test set: 0.8156424581005587</strong></pre>
<p>We notice overfitting: a 97% accuracy rate on the training set but only 81% on the test set. But in the<a id="_idIndexMarker232"/> end, the results of the test set are quite good, since it is quite hard to have much higher than 85% accuracy on the <span class="No-Break">Titanic dataset.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>See also</h2>
<ul>
<li>The XGBoost <a id="_idIndexMarker233"/>documentation quality is arguably not as good as scikit-learn’s, but it is still <span class="No-Break">useful: </span><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier"><span class="No-Break">https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier</span></a><span class="No-Break">.</span></li>
<li>There is also the <a id="_idIndexMarker234"/>regression counterpart class XGBRegressor and its <span class="No-Break">documentation: </span><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor"><span class="No-Break">https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor</span></a><span class="No-Break">.</span></li>
</ul>
<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>Regularization with XGBoost</h1>
<p>After a recipe<a id="_idIndexMarker235"/> introducing boosting and the use of XGBoost for classification, let’s now have a look at how to regularize such models. We will be using the same Titanic dataset and try to improve <span class="No-Break">test accuracy.</span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>Getting ready</h2>
<p>Just like Random Forest, an XGBoost model is made of decision trees. Consequently, it has some hyperparameters such as the maximum depth of trees (<strong class="source-inline">max_depth</strong>) or the number of trees (<strong class="source-inline">n_estimators</strong>) that can allow to regularize in the same way. It also has several other hyperparameters related to the decision trees that can <span class="No-Break">be fine-tuned:</span></p>
<ul>
<li><strong class="source-inline">subsample</strong>: The number of samples to randomly draw for training, equivalent to <strong class="source-inline">max_sample</strong> for scikit-learn’s decision trees. A smaller value may <span class="No-Break">add regularization.</span></li>
<li><strong class="source-inline">colsample_bytree</strong>: The number of features to randomly draw (equivalent to scikit-learn’s <strong class="source-inline">max_features</strong>) for each tree. A smaller value may <span class="No-Break">add regularization.</span></li>
<li><strong class="source-inline">colsample_bylevel</strong>: The number of features to randomly draw at the tree level. A smaller value may <span class="No-Break">add regularization.</span></li>
<li><strong class="source-inline">colsample_bynode</strong>: The number of features to randomly draw at the node level. A smaller value may <span class="No-Break">add regularization.</span></li>
</ul>
<p>Finally, more hyperparameters that are not shared with decision trees or Random Forest can allow fine-tuning the <span class="No-Break">XGBoost models:</span></p>
<ul>
<li><strong class="source-inline">learning_rate</strong>: The learning rate. A smaller learning rate may train even closer to the training set. Thus, a larger learning rate may regularize, although it may also degrade <span class="No-Break">the performances</span></li>
<li><strong class="source-inline">reg_alpha</strong>: The strength of the L1 regularization. A higher value implies <span class="No-Break">more regularization.</span></li>
<li><strong class="source-inline">reg_lambda</strong>: The strength of the L2 regularizations. A higher value implies <span class="No-Break">more regularization.</span></li>
</ul>
<p>Unlike other tree-based models seen so far, XGBoost allows L1 and L2 regularization too. Indeed, since each tree has an associated weight of 𝛾i, it is possible to add L1 or L2 regularization on those parameters, just the way it is done for <span class="No-Break">linear models.</span></p>
<p>Those are the main hyperparameters to fine-tune to optimize and properly regularize an XGBoost whenever needed. Although it’s a powerful, robust, and efficient model, it can be hard to fine-tune optimally, since there is quite a large number <span class="No-Break">of hyperparameters.</span></p>
<p>More practically, in this recipe, we will only add L1 regularization. To do so, all we need is to have XGBoost installed, as well as the Titanic-prepared data, as for the <span class="No-Break">previous recipe.</span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>How to do it…</h2>
<p>In this recipe, let’s add L1 regularization with the parameter <strong class="source-inline">reg_alpha</strong> in order to add bias and hopefully reduce the variance of the model. We will reuse the <strong class="source-inline">XGBClassifier</strong> model on the prepared Titanic data, as we did for the previous recipe. If your environment is still the same as the previous recipe, you can jump to <span class="No-Break"><em class="italic">step 3</em></span><span class="No-Break">.</span></p>
<ol>
<li>As usual, we start with the required imports: <strong class="source-inline">pickle</strong> to read the data from the binary format and the <strong class="source-inline">XGBoost</strong> classification model <span class="No-Break">class </span><span class="No-Break"><strong class="source-inline">XGBClassifier</strong></span><span class="No-Break">:</span><pre class="source-code">
import pickle</pre><pre class="source-code">
from xgboost import XGBClassifier</pre></li>
<li>Then, we load the already prepared data using <strong class="source-inline">pickle</strong>. It assumes the <strong class="source-inline">prepared_titanic.pkl</strong> file is <span class="No-Break">locally downloaded:</span><pre class="source-code">
X_train, X_test, y_train, y_test = pickle.load(open(</pre><pre class="source-code">
    'prepared_titanic.pkl', 'rb'))</pre></li>
<li>Instantiate the boosting model. Besides specifying <strong class="source-inline">use_label_encoder=False</strong>, we now specify <strong class="source-inline">reg_alpha=1</strong> to add <span class="No-Break"><strong class="source-inline">L1</strong></span><span class="No-Break"> regularization:</span><pre class="source-code">
bst = XGBClassifier(use_label_encoder=False,reg_alpha=1)</pre></li>
<li>Train the model on the training set using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
bst.fit(X_train, y_train)</pre></li>
<li>Finally, compute the accuracy of the model on both training and test sets, using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">score()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
print('Accuracy on training set:', bst.score(X_train,</pre><pre class="source-code">
    y_train))</pre><pre class="source-code">
print('Accuracy on test set:', bst.score(X_test,</pre><pre class="source-code">
    y_test))</pre></li>
</ol>
<p>This would print <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy on training set: 0.9410112359550562</strong>
<strong class="bold">Accuracy on test set: 0.8435754189944135</strong></pre>
<p>Compared to the previous recipe with default hyperparameters, adding L1 penalization allowed us to improve the results. The accuracy scores are now about 84% on the test set, and they lowered to 94% on the training set, effectively <span class="No-Break">adding regularization.</span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>There’s more…</h2>
<p>Finding the best hyperparameter set with XGBoost can be tricky as there are many hyperparameters. Of course, using hyperparameter optimization techniques is required to gain some <span class="No-Break">previous time.</span></p>
<p>For regression tasks, as mentioned earlier, the <strong class="source-inline">XGBoost</strong> library has an <strong class="source-inline">XGBRegressor</strong> class that makes them possible, with the same hyperparameters having the same effects <span class="No-Break">on regularization.</span></p>
</div>
</div></body></html>