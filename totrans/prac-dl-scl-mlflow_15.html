<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer102">
			<h1 id="_idParaDest-128"><em class="italic"><a id="_idTextAnchor127"/>Chapter 10</em>: Implementing DL Explainability with MLflow</h1>
			<p>The importance of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) explainability is now well established, as we learned in the previous chapter. In order to implement DL explainability in a real-world project, it is desirable to log the explainer and the explanations as artifacts, just like other model artifacts in the MLflow server, so that we can easily track and reproduce the explanation. The integration of DL explainability tools such as SHAP (<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>) with MLflow can support different implementation mechanisms, and it is important to understand how these integrations can be used for our DL explainability scenarios. In this chapter, we will explore several ways to integrate the SHAP explanations into MLflow by using different MLflow capabilities. As tools for explainability and DL models are both rapidly evolving, we will also highlight the current limitations and workarounds when using MLflow for DL explainability implementation. By the end of this chapter, you will feel comfortable implementing SHAP explanations and explainers using MLflow APIs for scalable model explainability.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding current MLflow explainability integration </li>
				<li>Implementing SHAP explanations using the MLflow artifact logging API</li>
				<li>Implementing SHAP explainers using the MLflow pyfunc API </li>
			</ul>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor128"/>Technical requirements</h1>
			<p>The following requirements are necessary to complete this chapter:</p>
			<ul>
				<li>MLflow full-fledged local server: This is the same one we have been using since <a href="B18120_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Tracking Models, Parameters, and Metrics</em>.</li>
				<li>The SHAP Python library: <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>.</li>
				<li>Spark 3.2.1 and PySpark 3.2.1: See the details in the <strong class="source-inline">README.md</strong> file of this chapter's GitHub repository.</li>
				<li>Code from the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10</a>.</li>
			</ul>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/>Understanding current MLflow explainability integration</h1>
			<p>MLflow has <a id="_idIndexMarker614"/>several ways to support explainability integration. When implementing explainability, we refer to two types of artifacts: explainers<a id="_idIndexMarker615"/> and explanations: </p>
			<ul>
				<li>An explainer<a id="_idIndexMarker616"/> is an<a id="_idIndexMarker617"/> explainability model, and a common <a id="_idIndexMarker618"/>one is a SHAP model that could be different kinds of SHAP explainers, such as <strong class="bold">TreeExplainer</strong>, <strong class="bold">KernelExplainer</strong>, and <strong class="bold">PartitionExplainer</strong> (<a href="https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html">https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html</a>). For computational efficiency, we <a id="_idIndexMarker619"/>usually choose <strong class="bold">PartitionExplainer</strong> for DL models.</li>
				<li>An explanation is an artifact that shows some form of output from the explainer, which could be text, numerical values, or plots. Explanations can happen in offline training or testing, or can happen during online production. Thus, we should be able to provide an explainer for offline evaluation or an explainer endpoint for online queries if we want to know why the model provides certain predictions.</li>
			</ul>
			<p>Here, we give a brief overview of the current capability as of MLflow version 1.25.1 (<a href="https://pypi.org/project/mlflow/1.25.1/">https://pypi.org/project/mlflow/1.25.1/</a>). There <a id="_idIndexMarker620"/>are four different ways to use MLflow for explainability as follows:</p>
			<ul>
				<li>Use<a id="_idIndexMarker621"/> the <strong class="source-inline">mlflow.log_artifact</strong> API (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact">https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact</a>) to log relevant explanation artifacts such as bar plots and Shapley values arrays. This gives maximum flexibility for logging explanations. This can be used either offline as batch processing or online when we automatically log a SHAP bar plot for a certain prediction. Note that logging an explanation for each prediction during online production scenarios is expensive, so we should provide a separate explanation API for on-demand queries.</li>
				<li>Use <a id="_idIndexMarker622"/>the <strong class="source-inline">mlflow.pyfunc.PythonModel</strong> API (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel</a>) to create an explainer that can be logged and loaded with MLflow's <strong class="source-inline">pyfunc</strong> methods, <strong class="source-inline">mlflow.pyfunc.log_model</strong> for logging and <strong class="source-inline">mlflow.pyfunc.load_model</strong> or <strong class="source-inline">mlflow.pyfunc.spark_udf</strong> for loading an explainer. This gives us maximum flexibility to create customized explainers as MLflow generic <strong class="source-inline">pyfunc</strong> models and can be used for either offline batch<a id="_idIndexMarker623"/> explanation <a id="_idIndexMarker624"/>or online as an <strong class="bold">Explanation as a Service</strong> (<strong class="bold">EaaS</strong>).</li>
				<li>Use<a id="_idIndexMarker625"/> the <strong class="source-inline">mlflow.shap</strong> API (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html">https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html</a>). This has some limitations. For example, the <strong class="source-inline">mlflow.shap.log_explainer</strong> method only supports scikit-learn and PyTorch models. The <strong class="source-inline">mlflow.shap.log_explanation</strong> method only supports <strong class="source-inline">shap.KernelExplainer</strong> (<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html">https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html</a>). This is very computationally intensive, as the computing time grows exponentially with respect to the number of features; thus, it is not feasible to compute explanations for even a moderate size dataset (see a posted GitHub issue <a href="https://github.com/mlflow/mlflow/issues/4071">https://github.com/mlflow/mlflow/issues/4071</a>). The existing examples provided by MLflow are for classical ML models in scikit-learn packages such as linear regression or random forest, with no DL model explainability examples (<a href="https://github.com/mlflow/mlflow/tree/master/examples/shap">https://github.com/mlflow/mlflow/tree/master/examples/shap</a>). We will show in later sections of this chapter that this API currently does not support the transformers-based SHAP explainers and explanations, thus we will not use this API in this chapter. We will highlight some of the issues as we walk through our examples in this chapter.</li>
				<li>Use<a id="_idIndexMarker626"/> the <strong class="source-inline">mlflow.evaluate</strong> API (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate">https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate</a>). This can be used for evaluation after the model is already trained and tested. This is an experimental feature and might change in the future. It supports MLflow <strong class="source-inline">pyfunc</strong> models. However, it has some limitations in that the evaluation dataset label values must be numeric or Boolean, all feature values must be numeric, and each feature column must only contain scalar values (<a href="https://www.mlflow.org/docs/latest/models.html#model-evaluation">https://www.mlflow.org/docs/latest/models.html#model-evaluation</a>). Again, existing examples provided by MLflow are only for classical ML models in scikit-learn packages (<a href="https://github.com/mlflow/mlflow/tree/master/examples/evaluation">https://github.com/mlflow/mlflow/tree/master/examples/evaluation</a>). We could use this API to just log the classifier metrics for an NLP sentiment model, but the explanation part will be skipped automatically by this API because it requires a feature column containing scalar values (an NLP model input is a text input). Thus, this is not applicable to the DL model explainability we need. So, we will not use this API in this chapter.</li>
			</ul>
			<p>Given that some of these APIs are still experimental and are still evolving, users should be aware of the limitations and workarounds to successfully implement explainability with MLflow. For<a id="_idIndexMarker627"/> DL model explainability, as we will learn in this chapter, it is quite challenging to implement using MLflow as the MLflow integration with SHAP is still a work-in-progress as of MLflow version 1.25.1. In the following sections, we will learn when and how to use these different APIs to implement explanations and log and load explainers for DL models.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Implementing a SHAP explanation using the MLflow artifact logging API</h1>
			<p>MLflow <a id="_idIndexMarker628"/>has a generic<a id="_idIndexMarker629"/> tracking API that can log any artifact: <strong class="source-inline">mlflow.log_artifact</strong>. However, the examples given in the MLflow documentation usually use scikit-learn and tabular numerical data for training, testing, and explaining. Here, we want to show how to use <strong class="source-inline">mlflow.log_artifact</strong> for an NLP sentimental DL model to log relevant artifacts, such as Shapley value arrays and Shapley value bar plots. You can check out the Python VS Code notebook, <strong class="source-inline">shap_mlflow_log_artifact.py</strong>, in this chapter's GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py</a>) to follow along with the steps:</p>
			<ol>
				<li>Make sure you have the prerequisites, including a local full-fledged MLflow server and the conda virtual environment, ready. Follow the instructions in the <strong class="source-inline">README.md</strong> (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md</a>) file in the <a href="B18120_10_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 10</em></a> folder to get these ready.</li>
				<li>Make sure you activate the <strong class="source-inline">chapter10-dl-explain</strong> virtual environment as follows before you start running any code in this chapter:<p class="source-code"><strong class="bold">conda activate chapter10-dl-explain</strong></p></li>
				<li>Import the relevant libraries at the beginning of the notebook as follows:<p class="source-code">import os</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import mlflow</p><p class="source-code">from mlflow.tracking import MlflowClient</p><p class="source-code">from mlflow.utils.file_utils import TempDir</p><p class="source-code">import shap</p><p class="source-code">import transformers</p><p class="source-code">from shap.plots import *</p><p class="source-code">import numpy as np</p></li>
				<li>The next step is to set up some environment variables. The first three environment variables are for the local MLflow URIs, and the fourth is for disabling a Hugging Face warning that arises due to a known Hugging Face tokenization issue:<p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "minio"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"</p><p class="source-code">os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://localhost:9000"</p><p class="source-code">os.environ["TOKENIZERS_PARALLELISM"] = "False"</p></li>
				<li>We will <a id="_idIndexMarker630"/>also<a id="_idIndexMarker631"/> need to set up the MLflow experiment and show the MLflow experiment ID as an output on the screen:<p class="source-code">EXPERIMENT_NAME = "dl_explain_chapter10"</p><p class="source-code">mlflow.set_tracking_uri('http://localhost')</p><p class="source-code">mlflow.set_experiment(EXPERIMENT_NAME)</p><p class="source-code">experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)</p><p class="source-code">print("experiment_id:", experiment.experiment_id)</p></li>
			</ol>
			<p>If you have been running the notebook, you should see an output like the following:</p>
			<p class="source-code"><strong class="bold">experiment_id: 14</strong></p>
			<p>This means the MLflow experiment ID for the experiment name <strong class="source-inline">dl_explain_chapter10</strong> is <strong class="source-inline">14</strong>. Note that, you could also set the MLflow tracking URI as an environment variable as follows:</p>
			<p class="source-code"><strong class="bold">export MLFLOW_TRACKING_URI=http://localhost</strong></p>
			<p>Here, we use MLflow's <strong class="source-inline">mlflow.set_tracking_uri</strong> API to define the URI location instead. Either way is fine.</p>
			<ol>
				<li value="6">Now we can create a DL model to classify a sentence into either positive or negative sentiment using Hugging Face's transformer pipeline API. Since this is already fine-tuned, we will focus on how to get the explainer and explanation for <a id="_idIndexMarker632"/>the <a id="_idIndexMarker633"/>model, rather than focusing on how to train or finetune a model:<p class="source-code">dl_model = transformers.pipeline('sentiment-analysis', return_all_scores=False)</p><p class="source-code">explainer = shap.Explainer(dl_model)</p><p class="source-code">shap_values = explainer(["Not a good movie to spend time on.", "This is a great movie."])</p></li>
			</ol>
			<p>The code snippets create a sentiment analysis model, <strong class="source-inline">dl_model</strong>, and then create a SHAP <strong class="source-inline">explainer</strong> for this model. Then we provide a list of two sentences for this explainer to get the <strong class="source-inline">shap_values</strong> object. This will be used for logging in MLflow.</p>
			<p>Given the <strong class="source-inline">shap_values</strong> object, we can now start a new MLflow run and log both the Shapley values and the bar plot that we saw in the previous chapter (<a href="B18120_09_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 9</em></a><em class="italic">, Fundamentals of Deep Learning Explainability</em>). The first line of code makes sure all active MLflow runs are ended. This is useful if we want to rerun this block of code multiple times interactively:</p>
			<p class="source-code">mlflow.end_run()</p>
			<p>Then we define two constants. One, <strong class="source-inline">artifact_root_path</strong>, is for the root path in the MLflow artifact store, which will be used to store all the SHAP explanation objects. The other, <strong class="source-inline">shap_bar_plot</strong>, is for the artifact filename, which will be used for the bar plot figure:</p>
			<p class="source-code">artifact_root_path = "model_explanations_shap"</p>
			<p class="source-code">artifact_file_name = 'shap_bar_plot'</p>
			<ol>
				<li value="7">We then start a new MLflow run, under which we will generate and log three SHAP files into the MLflow artifact store under the path <strong class="source-inline">model_explanations_shap</strong>:<p class="source-code">with mlflow.start_run() as run:</p><p class="source-code">   with TempDir() as temp_dir:</p><p class="source-code">        temp_dir_path = temp_dir.path()</p><p class="source-code">        print("temp directory for artifacts: {}".format(temp_dir_path))</p></li>
			</ol>
			<p>We also need<a id="_idIndexMarker634"/> to<a id="_idIndexMarker635"/> have a temporary local directory, as shown in the preceding code snippet to first save the SHAP files, and then log those files to the MLflow server. If you have run the notebook up to this point, you should see a temporary directory in the output like the following:</p>
			<p class="source-code"><strong class="bold">temp directory for artifacts: /var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpgw520wu1</strong></p>
			<ol>
				<li value="8">Now we are ready to generate the SHAP files and save them. The first one is the bar plot, which is a little bit tricky to save and log. Let's walk through the following code to understand how we do this:<p class="source-code">try:</p><p class="source-code">     plt.clf()</p><p class="source-code">     plt.subplots_adjust(bottom=0.2, left=0.4)</p><p class="source-code">     shap.plots.bar(shap_values[0, :, "NEGATIVE"],</p><p class="source-code">                    show=False)</p><p class="source-code">     plt.savefig(f"{temp_dir_path}/{artifact_file_name}")</p><p class="source-code">finally:</p><p class="source-code">     plt.close(plt.gcf())</p><p class="source-code">mlflow.log_artifact(f"{temp_dir_path}/{artifact_file_name}.png", artifact_root_path)</p></li>
			</ol>
			<p>Note that we are using <strong class="source-inline">matplotlib.pyplot</strong>, which was imported as <strong class="source-inline">plt</strong> to first clear the figure using <strong class="source-inline">plt.clf()</strong> and then create a subplot with some adjustments. Here, we define <strong class="source-inline">bottom=0.2</strong>, which means the position of the bottom edge of the subplots is at 20% of the figure height. Similarly, we adjust the left edge of the subplot. Then we use the <strong class="source-inline">shap.plots.bar</strong> SHAP API to plot the bar<a id="_idIndexMarker636"/> plot<a id="_idIndexMarker637"/> for the first sentence's feature contribution to the prediction, but with the <strong class="source-inline">show</strong> parameter to be <strong class="source-inline">False</strong>. This means, we will not see the plot in the interactive run, but the figure is stored in the pyplot <strong class="source-inline">plt</strong> variable, which can then be saved using <strong class="source-inline">plt.savefig</strong> to a local temporary directory with the filename prefix <strong class="source-inline">shap_bar_plot</strong>. <strong class="source-inline">pyplot</strong> will automatically add the file extension <strong class="source-inline">.png</strong> to the file once it is saved. So, this will save a local image file called <strong class="source-inline">shap_bar_plot.png</strong> in the temporary folder. The last statement calls MLflow's <strong class="source-inline">mlflow.log_artifact</strong> to upload this PNG file to the MLflow tracking server's artifact store in the root folder, <strong class="source-inline">model_explanations_shap</strong>. We also need to make sure that we close the current figure by calling <strong class="source-inline">plt.close(plt.gcf())</strong>.</p>
			<ol>
				<li value="9">In addition to logging the <strong class="source-inline">shap_bar_plot.png</strong> to the MLflow server, we also want to log the Shapley <strong class="source-inline">base_values</strong> array and <strong class="source-inline">shap_values</strong> array as NumPy arrays into the MLflow track server. This can be done through the following statements:<p class="source-code">np.save(f"{temp_dir_path}/shap_values", </p><p class="source-code">        shap_values.values)</p><p class="source-code">np.save(f"{temp_dir_path}/base_values", </p><p class="source-code">        shap_values.base_values)</p><p class="source-code">        mlflow.log_artifact(</p><p class="source-code">            f"{temp_dir_path}/shap_values.npy", </p><p class="source-code">            artifact_root_path)</p><p class="source-code">        mlflow.log_artifact(</p><p class="source-code">            f"{temp_dir_path}/base_values.npy", </p><p class="source-code">            artifact_root_path)      </p></li>
			</ol>
			<p>This will first <a id="_idIndexMarker638"/>save<a id="_idIndexMarker639"/> a local copy of <strong class="source-inline">shap_values.npy</strong> and <strong class="source-inline">base_values.npy</strong> in the local temporary folder and then upload it to the MLflow tracking server's artifact store.</p>
			<ol>
				<li value="10">If you followed the notebook up until here, you should be able to verify in the local MLflow server whether these artifacts are successfully stored. Go to the MLflow UI at the localhost – <strong class="source-inline">http://localhost/</strong> and then find the experiment <strong class="source-inline">dl_explain_chapter10</strong>. You should then be able to find the experiment you just ran. It should look something like <em class="italic">Figure 10.1</em>, where you can find three files in the <strong class="source-inline">model_explanations_shap</strong> folder: <strong class="source-inline">base_values.npy</strong>, <strong class="source-inline">shap_bar_plot.png</strong>, and <strong class="source-inline">shap_values.npy</strong>. <em class="italic">Figure 10.1</em> shows the bar plot of feature contribution of different tokens or words for the prediction result of the sentence – <strong class="source-inline">Not a good movie to spend time on</strong>. The URL for this experiment page is something like the following:<p class="source-code">http://localhost/#/experiments/14/runs/10f0655189f740aeb813a015f1f6e115</p></li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="Images/B18120_10_001.jpg" alt="Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image &#13;&#10;in the MLflow tracking server&#13;&#10;" width="1391" height="859"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image in the MLflow tracking server</p>
			<p>Alternatively, you can also use code to programmatically download these files stored in the<a id="_idIndexMarker640"/> MLflow<a id="_idIndexMarker641"/> tracking server and check them locally. We provide such code in the last cell of the notebook.</p>
			<ol>
				<li value="11">If you run the last cell block of the notebook code, which is to download the three files from the MLflow server we just saved and print them out, you should be able to see the following output, as displayed in <em class="italic">Figure 10.2</em>. The mechanism to download artifacts from the MLflow tracking server is to use the <strong class="source-inline">MlflowClient().download_artifacts</strong> API, where you provide the MLflow run ID (in our example, it is <strong class="source-inline">10f0655189f740aeb813a015f1f6e115</strong> ) and the artifact root path <strong class="source-inline">model_explanations_shap</strong> as the parameters to the API:<p class="source-code">downloaded_local_path = MlflowClient().download_artifacts(run.info.run_id, artifact_root_path)</p></li>
			</ol>
			<p>This will download all files in <strong class="source-inline">model_explanations_shap</strong> on the MLflow tracking server to <a id="_idIndexMarker642"/>a<a id="_idIndexMarker643"/> local path, which is the return variable <strong class="source-inline">downloaded_local_path</strong>:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="Images/B18120_10_002.jpg" alt="Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow tracking server to a local path and display them&#13;&#10;" width="842" height="603"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow tracking server to a local path and display them</p>
			<p>To display the two NumPy arrays, we need to call NumPy's <strong class="source-inline">load</strong> API to load them and then print them:</p>
			<p class="source-code">base_values = np.load(os.path.join(downloaded_local_path, "base_values.npy"), allow_pickle=True)</p>
			<p class="source-code">shap_values = np.load(os.path.join(downloaded_local_path, "shap_values.npy"), allow_pickle=True)</p>
			<p>Note that we need to set the <strong class="source-inline">allow_pickle</strong> parameter to <strong class="source-inline">True</strong> when calling the <strong class="source-inline">np.load</strong> API so that NumPy can correctly load these files back into memory. </p>
			<p>While you can run this notebook interactively in the VS Code environment, you can also run it in the command line as follows:</p>
			<p class="source-code"><strong class="bold">python shap_mlflow_log_artifact.py</strong></p>
			<p>This will produce all the output in the console and log all the artifacts into the MLflow server as we have seen in our interactive running of the notebook. </p>
			<p>If you have run the<a id="_idIndexMarker644"/> code <a id="_idIndexMarker645"/>so far, congratulations on the successful completion of implementing logging SHAP explanations to the MLflow tracking server using MLflow's <strong class="source-inline">mlflow.log_artifact</strong> API!  </p>
			<p>Although the process of logging all the explanations seems a little bit long, this approach does have the advantage of having no dependency on what kind of explainer is used since the explainer is defined outside of the MLflow artifact logging API.</p>
			<p>In the next section, we will see how to use the built-in <strong class="source-inline">mlflow.pyfunc.PythonModel</strong> API to log a SHAP explainer as an MLflow model and then deploy as an endpoint or use it in a batch mode as if it is a generic MLflow <strong class="source-inline">pyfunc</strong> model.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>Implementing a SHAP explainer using the MLflow pyfunc API</h1>
			<p>As we know <a id="_idIndexMarker646"/>from the previous <a id="_idIndexMarker647"/>section, a SHAP explainer can be used offline whenever needed by creating a new instance of an explainer using SHAP APIs. However, as the underlying DL models are often logged into the MLflow server, it is desirable to also log the corresponding explainer into the MLflow server, so that we not only keep track of the DL models, but also their explainers. In addition, we can use the generic MLflow pyfunc model logging and loading APIs for the explainer, thus unifying access to DL models and their explainers.</p>
			<p>In this section, we will learn step-by-step how to implement a SHAP explainer as a generic MLflow pyfunc model and how to use it for offline and online explanation. We will break the process up into three subsections:</p>
			<ul>
				<li>Creating and logging an MLflow pyfunc explainer</li>
				<li>Deploying an MLflow pyfunc explainer for an EaaS</li>
				<li>Using an MLflow pyfunc explainer for batching explanation</li>
			</ul>
			<p>Let's start with the first subsection on creating and logging a MLflow pyfunc explainer.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>Creating and logging an MLflow pyfunc explainer</h2>
			<p>In order<a id="_idIndexMarker648"/> to follow<a id="_idIndexMarker649"/> this section, please check out <strong class="source-inline">nlp_sentiment_classifier_explainer.py</strong> in the GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py</a>):</p>
			<ol>
				<li value="1">First, by subclassing <strong class="source-inline">mlflow.pyfunc.PythonModel</strong>, we can create a customized MLflow model that encapsulates a SHAP explainer. So, let's declare this class as follows:<p class="source-code">class SentimentAnalysisExplainer(mlflow.pyfunc.PythonModel):</p></li>
				<li>Next, we need to instantiate an explainer. Instead of creating an explainer in the <strong class="source-inline">init</strong> method of this class, we will use the <strong class="source-inline">load_context</strong> method to load a SHAP explainer for the Hugging Face NLP sentiment analysis classifier, as follows:<p class="source-code">def load_context(self, context):</p><p class="source-code">  from transformers import pipeline</p><p class="source-code">  import shap</p><p class="source-code">  self.explainer = shap.Explainer(pipeline('sentiment-analysis', return_all_scores=True))</p></li>
			</ol>
			<p>This will create a SHAP explainer whenever this <strong class="source-inline">SentimentAnalysisExplainer</strong> class is executed. Note that the sentiment <a id="_idIndexMarker650"/>classifier is a Hugging Face pipeline<a id="_idIndexMarker651"/> object, with the <strong class="source-inline">return_all_scores</strong> parameter set to <strong class="source-inline">True</strong>. This means that this will return the label and probability score for both positive and negative sentiment of each input text.</p>
			<p class="callout-heading">Avoid Runtime Errors for SHAP explainers</p>
			<p class="callout">If we<a id="_idIndexMarker652"/> implement <strong class="source-inline">self.explainer</strong> in the <strong class="source-inline">init</strong> method in this class, we will encounter a runtime error related to the SHAP package's <strong class="source-inline">_masked_model.py</strong> file, which complains about <strong class="bold">TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'</strong>. Any code implemented in the PythonModel class's <strong class="source-inline">init</strong> method will be serialized by MLflow, so it is clear that this runtime error comes from MLflow's serialization. However, implementing <strong class="source-inline">self.explainer</strong> in the <strong class="source-inline">load_context</strong> function avoids MLflow's serialization, and works correctly when invoking this explainer at runtime. </p>
			<ol>
				<li value="3">We will then implement the <strong class="source-inline">sentiment_classifier_explanation</strong> method, which takes an input of a pandas DataFrame row and produces a pickled <strong class="source-inline">shap_values</strong> output as an explanation for a single row of text input:<p class="source-code">def sentiment_classifier_explanation(self, row):</p><p class="source-code">  shap_values = self.explainer([row['text']])</p><p class="source-code">  return [pickle.dumps(shap_values)]</p></li>
			</ol>
			<p>Note that we need to use a pair of square brackets to enclose the <strong class="source-inline">row['text'] </strong>value so that it becomes a list not just a single value. This is because this SHAP explainer expects a list of texts, not just a single string. If we don't enclose the value within the square brackets, then the explainer will split the entire string character by character, treating each character as if it is a word, which is not what we want. Once we get the Shapley values as the output from the explainer as <strong class="source-inline">shap_values</strong>, we then need to serialize them using <strong class="source-inline">pickle.dumps</strong> before returning to the caller. MLflow pyfunc model input and output signature do not support complex object without serialization, so this pickling step <a id="_idIndexMarker653"/>makes <a id="_idIndexMarker654"/>sure that the model output signature is MLflow compliant. We will see the definition of this MLflow pyfunc explainer's input and output signature in <em class="italic">step 5</em> shortly.</p>
			<ol>
				<li value="4">Next, we need to implement the required <strong class="source-inline">predict</strong> method for this class. This will apply the <strong class="source-inline">sentiment_classifier_explanation</strong> method to the entire input pandas DataFrame, as follows:<p class="source-code">def predict(self, context, model_input):</p><p class="source-code">  model_input[['shap_values']] = model_input.apply(</p><p class="source-code">    self.sentiment_classifier_explanation, axis=1, </p><p class="source-code">    result_type='expand')</p><p class="source-code">  model_input.drop(['text'], axis=1, inplace=True)</p><p class="source-code">  return model_input</p></li>
			</ol>
			<p>This will produce a new column named <strong class="source-inline">shap_values</strong> for each row of the input pandas DataFrame in the <strong class="source-inline">text</strong> column. We then drop the <strong class="source-inline">text</strong> column and return a single-column <strong class="source-inline">shap_values</strong> DataFrame as the final prediction result: in this case, the explanation results as a DataFrame. </p>
			<ol>
				<li value="5">Now that we have the <strong class="source-inline">SentimentAnalysisExplainer</strong> class implementation, we can use the standard MLflow pyfunc model logging API to log this model into the MLflow tracking server. Before doing the MLflow logging, let's make sure we declare this explainer's model signature, as follows:<p class="source-code">input = json.dumps([{'name': 'text', 'type': 'string'}])</p><p class="source-code">output = json.dumps([{'name': 'shap_values', 'type': 'string'}])</p><p class="source-code">signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})</p></li>
			</ol>
			<p>These<a id="_idIndexMarker655"/> statements<a id="_idIndexMarker656"/> declare that the input is a DataFrame with a single <strong class="source-inline">string</strong> type <strong class="source-inline">text</strong> column and the output is a DataFrame with a single <strong class="source-inline">string</strong> type <strong class="source-inline">shap_values</strong> column. Recall that this <strong class="source-inline">shap_values</strong> column is a pickled serialized bytes string, which contains the Shapley values object.</p>
			<ol>
				<li value="6">Finally, we can implement the explainer logging step using the <strong class="source-inline">mlflow.pyfunc.log_model</strong> method in a task method, as follows:<p class="source-code">with mlflow.start_run() as mlrun:          </p><p class="source-code">  mlflow.pyfunc.log_model(</p><p class="source-code">    artifact_path=MODEL_ARTIFACT_PATH, </p><p class="source-code">    conda_env=CONDA_ENV,                           </p><p class="source-code">    python_model=SentimentAnalysisExplainer(), </p><p class="source-code">    signature=signature)</p></li>
			</ol>
			<p>There are four parameters in the <strong class="source-inline">log_model</strong> method that we use. The <strong class="source-inline">MODEL_ARTIFACT_PATH</strong> is the name of the folder in the MLflow tracking server where the explainer will be stored. Here, the value is defined as <strong class="source-inline">nlp_sentiment_classifier_explainer</strong> in the Python file you checked out. <strong class="source-inline">CONDA_ENV</strong> is the <strong class="source-inline">conda.yaml</strong> file in this chapter's root folder. The <strong class="source-inline">python_model</strong> parameter is the <strong class="source-inline">SentimentAnalysisExplainer</strong> class we just implemented, and <strong class="source-inline">signature</strong> is the explainer input and output signature we defined.</p>
			<ol>
				<li value="7">Now we are ready to run this whole file as follows in the command line:<p class="source-code"><strong class="bold">python nlp_sentiment_classifier_explainer.py</strong></p></li>
			</ol>
			<p>Assuming you have the local MLflow tracking server and environment variables set up correctly by following the <strong class="source-inline">README.md</strong> file for this chapter in the GitHub repository, this <a id="_idIndexMarker657"/>will produce the following two lines<a id="_idIndexMarker658"/> in the console output:</p>
			<p class="source-code"><strong class="bold">2022-05-11 17:49:32,181 Found credentials in environment variables.</strong></p>
			<p class="source-code"><strong class="bold">2022-05-11 17:49:32,384 finished logging nlp sentiment classifier explainer run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9</strong></p>
			<p>This means we have successfully logged the explainer in our local MLflow tracking server.</p>
			<ol>
				<li value="8">Go to the MLflow web UI at <strong class="source-inline">http://localhost/</strong> in the web browser and click the <strong class="source-inline">dl_explain_chapter10</strong> experiment folder. You should be able to find this run and the logged explainer in the <strong class="source-inline">Artifacts</strong> folder under <strong class="source-inline">nlp_sentiment_classifier_explainer</strong>, which should look as shown in <em class="italic">Figure 10.3</em>:</li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="Images/B18120_10_003.jpg" alt="Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model &#13;&#10;" width="1224" height="558"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model </p>
			<p>Notice that the <strong class="source-inline">MLmodel</strong> metadata shown in <em class="italic">Figure 10.3</em> does not differ much from the normal DL inference pipeline that we logged before as an MLflow pyfunc <a id="_idIndexMarker659"/>model<a id="_idIndexMarker660"/> except for the <strong class="source-inline">artifact_path</strong> name and the <strong class="source-inline">signature</strong>. That's the advantage of using this approach because now we can use the generic MLflow pyfunc model methods to load this explainer or deploy it as a service. </p>
			<p class="callout-heading">Problems with the mlflow.shap.log_explainer API</p>
			<p class="callout">As we mentioned<a id="_idIndexMarker661"/> earlier, MLflow has a <strong class="source-inline">mlflow.shap.log_explainer</strong> API that provides a method to log an explainer. However, this API does not support our NLP sentiment classifier explainer because our NLP pipeline is not a known model flavor that MLflow currently supports. Thus even though <strong class="source-inline">log_explainer</strong> can write this explainer object into the tracking server, when loading the explainer back into memory using the <strong class="source-inline">mlflow.shap.load_explainer</strong> API, it will fail with the following error message: <strong class="bold">TypeError: __init__() missing 1 required positional argument: 'pipeline'</strong>. Thus, we avoid using the <strong class="source-inline">mlflow.shap.log_explainer</strong> API in this book.</p>
			<p>Now that we have a logged explainer, we can use it in two ways: deploy it into a web service so that we can create an endpoint to establish an EaaS, or load the explainer directly through<a id="_idIndexMarker662"/> MLflow <a id="_idIndexMarker663"/>pyfunc <strong class="source-inline">load_model</strong> or <strong class="source-inline">spark_udf</strong> method using the MLflow <strong class="source-inline">run_id</strong>. Let's start with the web service deployment by setting up a local web service.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Deploying an MLflow pyfunc explainer for an EaaS</h2>
			<p>We can set<a id="_idIndexMarker664"/> up a local EaaS in a standard MLflow<a id="_idIndexMarker665"/> way since now the SHAP explainer is just like a generic MLflow pyfunc model. Perform the following steps to see how this can be implemented locally:</p>
			<ol>
				<li value="1">Run the following MLflow command to set up a local web service for the explainer we just logged. The <strong class="source-inline">run_id</strong> in this example is <strong class="source-inline">ad1edb09e5ea4d8ca0332b8bc2f5f6c9</strong>:<p class="source-code"><strong class="bold">mlflow models serve -m runs:/ ad1edb09e5ea4d8ca0332b8bc2f5f6c9/nlp_sentiment_classifier_explainer</strong></p></li>
			</ol>
			<p>This will produce the following console output:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="Images/B18120_10_004.jpg" alt=" Figure 10.4 – SHAP EaaS console output&#13;&#10;" width="1465" height="249"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 10.4 – SHAP EaaS console output</p>
			<p>Notice that in <em class="italic">Figure 10.4</em>, the default underlying pretrained language model is loaded after the <strong class="source-inline">gunicore</strong> HTTP server is up and running. This is because our implementation of the explainer was inside the <strong class="source-inline">load_context</strong> method, which is exactly what is to be expected: loading the explainer immediately after the web service is up and running.</p>
			<ol>
				<li value="2">In a different terminal window, type the following command to invoke the explainer web service at port <strong class="source-inline">5000</strong> of localhost with two sample texts as input:<p class="source-code"><strong class="bold">curl -X POST -H "Content-Type:application/json; format=pandas-split" --data '{"columns":["text"],"data":[["This is meh weather"], ["This is great weather"]]}' http://127.0.0.1:5000/invocations</strong></p></li>
			</ol>
			<p>This will <a id="_idIndexMarker666"/>produce<a id="_idIndexMarker667"/> the following output:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="Images/B18120_10_005.jpg" alt="Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS&#13;&#10;" width="1594" height="549"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS</p>
			<p>Note that in <em class="italic">Figure 10.5</em>, the column name is <strong class="source-inline">shap_values</strong>, while the values are pickled bytes hexadecimal data. These are not human readable, but can be converted back to the original <strong class="source-inline">shap_values</strong> using <strong class="source-inline">pickle.loads</strong> method at the caller side. So, if you see a response output like <em class="italic">Figure 10.5</em>, congratulations for setting up a local EaaS! You can deploy this explainer service just like other MLflow service deployments, as described in <a href="B18120_08_ePub.xhtml#_idTextAnchor095"><em class="italic">Chapter 8</em></a><em class="italic">, Deploying a DL Inference Pipeline at Scale</em>, since this explainer now can be called just like a generic MLflow pyfunc model service.</p>
			<p>Next, we will see how to use the MLflow pyfunc explainer for batch explanation.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>Using an MLflow pyfunc explainer for batch explanation</h2>
			<p>There are <a id="_idIndexMarker668"/>two ways to implement <a id="_idIndexMarker669"/>offline batch explanation using an MLflow pyfunc explainer:</p>
			<ul>
				<li>Load the pyfunc explainer as an MLflow pyfunc model to explain a given pandas DataFrame input.</li>
				<li>Load the pyfunc explainer as a PySpark UDF to explain a given PySpark DataFrame input.</li>
			</ul>
			<p>Let's start with loading the explainer as an MLflow pyfunc model.</p>
			<h3>Loading the MLflow pyfunc explainer as an MLflow pyfunc model</h3>
			<p>As we have<a id="_idIndexMarker670"/> already mentioned, another way <a id="_idIndexMarker671"/>to consume an MLflow logged explainer is to load the explainer in a local Python code using MLflow's pyfunc <strong class="source-inline">load_model</strong> method directly, instead of deploying it into a web service. This is very straightforward, and we will show you how it can be done. You can check out the code in the <strong class="source-inline">shap_mlflow_pyfunc_explainer.py</strong> file in the GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py</a>): </p>
			<ol>
				<li value="1">The first step is to load the logged explainer back into memory. The following code does this using <strong class="source-inline">mlflow.pyfunc.load_model</strong> and the explainer <strong class="source-inline">run_id</strong> URI:<p class="source-code">run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9"</p><p class="source-code">logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer'</p><p class="source-code">explainer = mlflow.pyfunc.load_model(logged_explainer)</p></li>
			</ol>
			<p>This should load the explainer as if it is just a generic MLflow pyfunc model. We can print out the metadata of the explainer by running the following code:</p>
			<p class="source-code">explainer</p>
			<p>This will show the following output:</p>
			<p class="source-code"><strong class="bold">mlflow.pyfunc.loaded_model: artifact_path: nlp_sentiment_classifier_explainer flavor: mlflow.pyfunc.model run_id: ad1edb09e5ea4d8ca0332b8bc2f5f6c9</strong></p>
			<p>This means this is a <strong class="source-inline">mlflow.pyfunc.model</strong> flavor, which is great news, since we can use the same MLflow pyfunc API to use this explainer.</p>
			<ol>
				<li value="2">Next, we will get some example data to test the newly loaded explainer:<p class="source-code">import datasets</p><p class="source-code">dataset = datasets.load_dataset("imdb", split="test")</p><p class="source-code">short_data = [v[:500] for v in dataset["text"][:20]]</p><p class="source-code">df_test = pd.DataFrame (short_data, columns = ['text'])</p></li>
			</ol>
			<p>This will<a id="_idIndexMarker672"/> load <a id="_idIndexMarker673"/>the IMDb test dataset, truncate each review text to 500 characters, and pick the first 20 rows to make a pandas DataFrame for explanation in the next step.</p>
			<ol>
				<li value="3">Now, we can run the explainer as follows:<p class="source-code">results = explainer.predict(df_test)</p></li>
			</ol>
			<p>This will run the SHAP partition explainer for the input DataFrame <strong class="source-inline">df_test</strong>. It will show the following output for each row of the DataFrame when it is running:</p>
			<p class="source-code"><strong class="bold">Partition explainer: 2it [00:38, 38.67s/it]</strong></p>
			<p>The result will be a pandas DataFrame with a single column, <strong class="source-inline">shap_values</strong>. This may take a few minutes as it needs to tokenize each row, execute the explainer, and serialize the output.</p>
			<ol>
				<li value="4">Once the explainer execution is done, we can check the results by deserializing the row content. Here is the code to check the first output:<p class="source-code">results_deserialized = pickle.loads(results['shap_values'][0])</p><p class="source-code">print(results_deserialized)</p></li>
			</ol>
			<p>This will print out the first row's <strong class="source-inline">shap_values</strong>. <em class="italic">Figure 10.6</em> shows a partial screenshot of the output of <strong class="source-inline">shap_values</strong>:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="Images/B18120_10_006.jpg" alt="Figure 10.6 – Partial output of the deserialized shap_values from the explanation&#13;&#10;" width="377" height="219"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Partial output of the deserialized shap_values from the explanation</p>
			<p>As we can<a id="_idIndexMarker674"/> see <a id="_idIndexMarker675"/>in <em class="italic">Figure 10.6</em>, the output of <strong class="source-inline">shap_values</strong> is no different from what we learned in <a href="B18120_09_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 9</em></a><em class="italic">, Fundamentals of Deep Learning Explainability</em>, when we did not use MLflow to log and load the explainer. We can also generate Shapley text plots to highlight the contribution of the texts to the predicted sentiment.</p>
			<ol>
				<li value="5">Run the following statement in the notebook to see the Shapely text plot:<p class="source-code">shap.plots.text(results_deserialized[:,:,"POSITIVE"])</p></li>
			</ol>
			<p>This will generate a plot displayed in <em class="italic">Figure 10.7</em>:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="Images/B18120_10_007.jpg" alt="Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow logged explainer&#13;&#10;" width="1188" height="218"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow logged explainer</p>
			<p>As can be seen in <em class="italic">Figure 10.7</em>, this review has a positive sentiment and the keywords or phrases that contribute to the predicted sentiment are <strong class="source-inline">good</strong>, <strong class="source-inline">love</strong>, and some other phrases highlighted in red. When you see this Shapley text plot, you should give yourself a round of applause, as you have finished learning how to use an MLflow logged explainer to generate batch explanation.</p>
			<p>As mentioned during the step-by-step implementation of this batch explanation, it is a little slow to do<a id="_idIndexMarker676"/> a<a id="_idIndexMarker677"/> large batch explanation using this pyfunc model approach. Luckily, we have another way to implement the batch explanation using the PySpark UDF function, which we will explain in the next subsection.</p>
			<h3>Loading the pyfunc explainer as a PySpark UDF </h3>
			<p>For scalable<a id="_idIndexMarker678"/> batch <a id="_idIndexMarker679"/>explanation, we can use Spark's distributed computing capability, which is supported by loading the pyfunc explainer as a PySpark UDF. There is no extra work to use this capability, since this is provided by the MLflow pyfunc API already through the <strong class="source-inline">mlflow.pyfunc.spark_udf</strong> method. We will show you how to implement this at-scale explanation step by step:</p>
			<ol>
				<li value="1">First, make sure you have worked through the <strong class="source-inline">README.md</strong> file (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md</a>) to install Spark, create and activate the <strong class="source-inline">chapter10-dl-pyspark-explain</strong> virtual environment, and set up all the environment variables before you run the PySpark UDF code to do the explanation at scale.</li>
				<li>Then you can start running the VS Code notebook, <strong class="source-inline">shap_mlflow_pyspark_explainer.py</strong>, which you can check out in the GitHub repository: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py</a>. Run the following command at <strong class="source-inline">chapter10/notebooks/</strong>:<p class="source-code"><strong class="bold">python shap_mlflow_pyspark_explainer.py</strong></p></li>
			</ol>
			<p>You will get <a id="_idIndexMarker680"/>the final output displayed <a id="_idIndexMarker681"/>in <em class="italic">Figure 10.8</em>, among quite a few lines of output preceding these final few lines:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="Images/B18120_10_008.jpg" alt="Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's shap_values along with their input texts&#13;&#10;" width="968" height="166"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's shap_values along with their input texts</p>
			<p>As can be seen in <em class="italic">Figure 10.8</em>, the PySpark UDF explainer's output is a PySpark DataFrame that has two columns: <strong class="source-inline">text</strong> and <strong class="source-inline">shap_values</strong>. The <strong class="source-inline">text</strong> column is the original input text, while the <strong class="source-inline">shap_values</strong> column contains the pickled serialized Shapley values, just like we saw in the previous subsection when we used the pyfunc explainer for the pandas DataFrame.</p>
			<p>Now let's see what's happening in the code. We will explain the key code blocks in the <strong class="source-inline">shap_mlflow_pyspark_explainer.py</strong> file. Since this is a VS Code notebook, you can run it either in the command line as we just did or interactively inside the VS Code IDE window.</p>
			<ol>
				<li value="3">The first key code block is to load the explainer using the <strong class="source-inline">mflow.pyfunc.spark_udf</strong> method, as follows:<p class="source-code">spark = SparkSession.builder.appName("Batch explanation with MLflow DL explainer").getOrCreate()</p><p class="source-code">run_id = "ad1edb09e5ea4d8ca0332b8bc2f5f6c9"</p><p class="source-code">logged_explainer = f'runs:/{run_id}/nlp_sentiment_classifier_explainer'</p><p class="source-code">explainer = mlflow.pyfunc.spark_udf(spark, model_uri=logged_explainer, result_type=StringType())</p></li>
			</ol>
			<p>The first <a id="_idIndexMarker682"/>statement<a id="_idIndexMarker683"/> is to initialize a <strong class="source-inline">SparkSession</strong> variable and then use <strong class="source-inline">run_id</strong> to load the logged explainer into memory. Run the explainer to get the metadata as follows:</p>
			<p class="source-code">explainer</p>
			<p>We will get the following result:</p>
			<p class="source-code"><strong class="bold">&lt;function mlflow.pyfunc.spark_udf.&lt;locals&gt;.udf(iterator: Iterator[Tuple[Union[pandas.core.series.Series, pandas.core.frame.DataFrame], ...]]) -&gt; Iterator[pandas.core.series.Series]&gt;</strong></p>
			<p>This means we now have a SHAP explainer wrapped as a Spark UDF function. This allows us to directly apply the SHAP explainer for an input PySpark DataFrame in the next step.</p>
			<ol>
				<li value="4">We load the IMDb test dataset as before to get a list of <strong class="source-inline">short_data</strong>, and then create a PySpark DataFrame for the top 20 rows of the test dataset for explanation:<p class="source-code">df_pandas = pd.DataFrame (short_data, columns = ['text'])</p><p class="source-code">spark_df = spark.createDataFrame(df_pandas)</p><p class="source-code">spark_df = spark_df.withColumn('shap_values', explainer())</p></li>
			</ol>
			<p>Note the last statement, which uses PySpark's <strong class="source-inline">withColumn</strong> function to add a new <strong class="source-inline">shap_values</strong> column to the input DataFrame, <strong class="source-inline">spark_df</strong>, which originally contained only one column, <strong class="source-inline">text</strong>. This is a natural way to use Spark's parallel and distributed computing capability. If you have run both the previous non-Spark approach using the MLflow pyfunc <strong class="source-inline">load_model</strong> method and the current PySpark UDF one, you will notice that the Spark approach runs much faster, even on a local computer. This allows us to do SHAP explanation at scale for many instances of input texts.</p>
			<ol>
				<li value="5">Finally, to verify the results, we show the <strong class="source-inline">spark_df</strong> DataFrame's top two rows, which was illustrated in <em class="italic">Figure 10.8</em>.</li>
			</ol>
			<p>By now, with MLflow's pyfunc Spark UDF wrapped SHAP explainer, we can confidently do large-scale<a id="_idIndexMarker684"/> batch<a id="_idIndexMarker685"/> explanation. Congratulations!</p>
			<p>Let's now summarize what we have learned in this chapter in the next section.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>Summary</h1>
			<p>In this chapter, we first reviewed the existing approaches in the MLflow APIs that could be used for implementing explainability. Two existing MLflow APIs, <strong class="source-inline">mlflow.shap</strong> and <strong class="source-inline">mlflow.evaluate</strong>, have limitations, thus cannot be used for the complex DL models and pipelines explainability scenarios we need. We then focused on two main approaches to implement SHAP explanations and explainers within the MLflow API framework: <strong class="source-inline">mlflow.log_artifact</strong> for logging explanations and <strong class="source-inline">mlflow.pyfunc.PythonModel</strong> for logging a SHAP explainer. Using the <strong class="source-inline">log_artifact</strong> API can allow us to log Shapley values and explanation plots into the MLflow tracking server. Using <strong class="source-inline">mlflow.pyfunc.PythonModel</strong> allows us to log a SHAP explainer as a MLflow pyfunc model, thus opening doors to deploy a SHAP explainer as a web service to create an EaaS endpoint. It also opens doors to use SHAP explainers through the MLflow pyfunc <strong class="source-inline">load_model</strong> or <strong class="source-inline">spark_udf</strong> API for large-scale offline batch explanation. This enables us to confidently implement explainability at scale for DL models.</p>
			<p>As the field of explainability continues to evolve, MLflow's integration with SHAP and other explainability toolboxes will also continue to improve. Interested readers are encouraged to continue their learning journey through the links provided in the further reading section. Happy continuous learning and growing!</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor136"/>Further reading</h1>
			<ul>
				<li>Shapley Values at Scale: <a href="https://neowaylabs.github.io/data-science/shapley-values-at-scale/">https://neowaylabs.github.io/data-science/shapley-values-at-scale/</a></li>
				<li>Scaling SHAP Calculations With PySpark and Pandas UDF: <a href="https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html">https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html</a></li>
				<li>Speeding up Shapley value computation using Ray, a distributed computing system: <a href="https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/">https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/</a></li>
				<li>Interpreting an NLP model with LIME and SHAP: <a href="mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4">https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4</a></li>
				<li>Model Evaluation in MLflow: <a href="https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html">https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html</a></li>
			</ul>
		</div>
	</div></body></html>