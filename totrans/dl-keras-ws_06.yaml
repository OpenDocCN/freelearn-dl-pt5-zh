- en: 6\. Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers model evaluation in depth. We will discuss alternatives
    to accuracy to evaluate the performance of a model when standard techniques are
    not feasible, especially where there are imbalanced classes. Finally, we will
    utilize confusion matrices, sensitivity, specificity, precision, FPR, ROC curves,
    and AUC scores to evaluate the performance of classifiers. By the end of this
    chapter, you will have an in-depth understanding of accuracy and null accuracy
    and will be able to understand and combat the challenges of imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered `regularization` techniques for neural networks.
    `Regularization` is an important technique when it comes to combatting how a model
    overfits the training data and helps the model perform well on new, unseen data
    examples. One of the regularization techniques we covered involved `L1` and `L2`
    weight regularizations, in which penalization is added to the weights. The other
    regularization technique we learned about was `dropout regularization`, in which
    some units of layers are randomly removed from the model fitting process at each
    iteration. Both regularization techniques are designed to prevent individual weights
    or units by influencing them too strongly and allowing them to generalize as well.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about some different evaluation techniques other
    than `accuracy`. For any data scientist, the first step after building a model
    is to evaluate it, and the easiest way to evaluate a model is through its accuracy.
    However, in real-world scenarios, particularly where there are classification
    tasks with highly imbalanced classes such as for predicting the presence of hurricanes,
    predicting the presence of a rare disease, or predicting if someone will default
    on a loan, evaluating the model using its accuracy score is not the best evaluation
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores core concepts such as imbalanced datasets and how different
    evaluation techniques can be used to work through these imbalanced datasets. This
    chapter begins with an introduction to accuracy and its limitations. Then, we
    will explore the concepts of `null accuracy`, `imbalanced datasets`, `sensitivity`,
    `specificity`, `precision`, `false positives`, `ROC curves`, and `AUC scores`.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand accuracy properly, let's explore model evaluation. Model evaluation
    is an integral part of the model development process. Once you've built your model
    and executed it, the next step is to evaluate your model.
  prefs: []
  type: TYPE_NORMAL
- en: A model is built on a `training dataset` and evaluating a model's performance
    on the same training dataset is bad practice in data science. Once a model has
    been trained on a training dataset, it should be evaluated on a dataset that is
    completely different from the training dataset. This dataset is known as the `test
    dataset`. The objective should always be to build a model that generalizes, which
    means the model should produce similar (but not the same) results, or relatively
    similar results, on any dataset. This can only be achieved if we evaluate the
    model on data that is unknown to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model evaluation process requires a metric that can quantify a model''s
    performance. The simplest metric for model evaluation is accuracy. `Accuracy`
    is the fraction of predictions that our model gets right. This is the formula
    for calculating `accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy = (Number of correct predictions) / (Total number of predictions)*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have `10` records and `7` are predicted correctly, then we
    can say that the accuracy of our model is `70%`. This is calculated as `7/10`
    = `0.7` or `70%`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Null accuracy` is the accuracy that can be achieved by predicting the most
    frequent class. If we don''t run an algorithm and just predict accuracy based
    on the most frequent outcome, then the accuracy that''s calculated based on this
    prediction is known as `null accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Null accuracy = (Total number of instances of the frequently occurring class)
    / (Total number of instances)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '10 actual outcomes: [1,0,0,0,0,0,0,0,1,0].'
  prefs: []
  type: TYPE_NORMAL
- en: '`Prediction`: [0,0,0,0,0,0,0,0,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '`Null accuracy` = 8/10 = 0.8 or 80%'
  prefs: []
  type: TYPE_NORMAL
- en: So, our null accuracy is `80%`, meaning we are correct `80%` of the time. This
    means we have achieved `80%` accuracy without running an algorithm. Always remember
    that when null accuracy is high, it means that the distribution of response variables
    is skewed in favor of the frequently occurring class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's work on an exercise to find the null accuracy of a dataset. The null accuracy
    of a dataset can be found by using the `value_count` function in the pandas library.
    The `value_count` function returns a series containing counts of unique values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the Jupyter Notebooks for the exercises and activities in this chapter are
    available on GitHub at [https://packt.live/37jHNUR](https://packt.live/37jHNUR).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.01: Calculating Null Accuracy on a Pacific Hurricanes Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have a dataset documenting whether a `hurricane` has been observed in the
    Pacific Ocean that has two columns, `Date` and `hurricane`. The `Date` column
    indicates the date of the observation, while the `hurricane` column indicates
    whether there was a hurricane on that date. Rows with a `hurricane` value of `1`
    means there was a hurricane, while `0` means there was no hurricane. Find the
    `null accuracy` of the dataset by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook. Import all the required libraries and load the `pacific_hurricanes.csv`
    file into the `data` folder from this book''s GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1: Data exploration of the pacific hurricanes dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.1: Data exploration of the pacific hurricanes dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the built-in `value_count` function from the pandas library to get the
    distribution for the data of the `hurricane` column. The `value_count` function
    shows the total instances of unique values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `value_count` function and set the `normalize` parameter to `True`.
    To find the null accuracy, you will have to index the `pandas` series that was
    produced for index `0` to get the proportion of values related to no hurricanes
    occurring on a given day:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The calculated `null accuracy` of the dataset is `92.4126%`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can see that our dataset has a very high null accuracy of `92.4126%`.
    So, if we just make a dumb model that predicts the majority class for all outcomes,
    our model will be `92.4126%` accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31FtQBm](https://packt.live/31FtQBm).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ArNwNT](https://packt.live/2ArNwNT).
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, in *Activity 6.01*, *Computing the Accuracy and Null
    Accuracy of a Neural Network When We Change the Train/Test Split,* we will see
    how null accuracy changes as we change the `test`/`train` split.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and Limitations of Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The advantages of accuracy are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Easy to use**: Accuracy is very easy to compute and understand as it is just
    a simple fraction formula.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Popular compared to other techniques**: Since it is the easiest metric to
    compute, it is also the most popular and is universally accepted as the first
    step of evaluating a model. Most introductory books on data science teach accuracy
    as an evaluation metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Good for comparing different models**: Suppose you are trying to solve a
    problem with different models. You can always trust the model that gives the highest
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The limitations of accuracy are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`response`/`dependent` variable. If we get an accuracy of `80%` in our model,
    we have no idea how the response variable is distributed and what the null accuracy
    of the dataset is. If the null accuracy of our dataset is above `70%`, then an
    `80%` accurate model is pretty useless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Accuracy` also gives us no information about `type 1` and `type 2` errors
    of the model. A `type 1` error is when a class is `negative` and we have predicted
    it as `positive`, while a `type 2` error is when a class is positive and we have
    predicted it as negative. We will be studying both of these errors later in this
    chapter. In the next section, we will cover the imbalanced datasets. The accuracy
    scores for models classifying imbalanced datasets can be particularly misleading,
    which is why other evaluation metrics are useful for model evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imbalanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imbalanced datasets are a distinct case for classification problems where the
    class distribution varies between the classes. In such datasets, one class is
    overwhelmingly dominant. In other words, the `null accuracy` of an imbalanced
    dataset is very high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example of credit card fraud. If we have a dataset of credit card
    transactions, then we will find that, of all the transactions, a very minuscule
    number of transactions were fraudulent and the majority of transactions were normal
    transactions. If `1` represents a fraudulent transaction and `0` represents a
    normal transaction, then there will be many 0s and hardly any 1s. The `null accuracy`
    of the dataset may be more than `99%`. This means that the majority class (in
    this case, `0`) is overwhelmingly greater than the minority class (in this case,
    `1`). Such sets are imbalanced datasets. Consider the following figure, which
    shows a general imbalanced dataset `scatter plot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: A general imbalanced dataset scatter plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: A general imbalanced dataset scatter plot'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot shows a generalized scatter plot of an imbalanced dataset,
    where the stars represent the minority class and the circles represent the majority
    class. As we can see, there are many more circles than stars; this can make it
    difficult for machine learning models to distinguish between the two classes.
    In the next section, we will cover some approaches to working with imbalanced
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Imbalanced Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In machine learning, there are two ways of overcoming the shortcomings of imbalanced
    datasets, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`90%`, then sampling techniques struggle to give the correct representation
    of majority-minority classes in the data and our model may overfit. So, the best
    way is to modify our evaluation techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying model evaluation techniques**: When working with highly imbalanced
    datasets, it is better to modify model evaluation techniques. This is the most
    robust way to get good results, which means using these methods will likely achieve
    good results on new, unseen data. There are many evaluation metrics other than
    accuracy that can be modified to evaluate a model. To learn about all those techniques,
    it is important to understand the concept of the confusion matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A `confusion matrix` describes the performance of the classification model.
    In other words, a confusion matrix is a way to summarize classifier performance.
    The following table shows a basic representation of a confusion matrix and represents
    how the predicted results by the model compared to the true values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Basic representation of a confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Basic representation of a confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over the meanings of the abbreviations that were used in the preceding
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TN** (**True negative**): This is the count of outcomes that were originally
    negative and were predicted negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP** (**False positive**): This is the count of outcomes that were originally
    negative but were predicted positive. This error is also called a **type 1 error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN** (**False negative**): This is the count of outcomes that were originally
    positive but were predicted negative. This error is also called a **type 2 error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TP** (**True positive**): This is the count of outcomes that were originally
    positive and were predicted as positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to maximize the values in the **TN** and **TP** boxes in the preceding
    table, that is, the true negatives and true positives, and minimize the values
    in the **FN** and **FP** boxes, that is, the false negatives and false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is an example of a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The aim of all machine learning and deep learning algorithms is to maximize
    TN and TP and minimize FN and FP. The following example code calculates TN, FP,
    FN, and TP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy does not help us understand type 1 and type 2 errors.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Computed from a Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The metrics that can be derived from a `confusion matrix` are `sensitivity`,
    `specificity`, `precision`, `FP rate`, `ROC`, and `AUC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1`, divided by the total number of patients who are actually `1`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sensitivity = TP / (TP+FN)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sensitivity refers to how often the prediction is correct when the actual value
    is positive. In cases such as building a model to predict patient readmission
    at a hospital, we need our model to be highly sensitive. We need 1 to be predicted
    as `1`. If a `0` is predicted as `1`, it is acceptable, but if a `1` is predicted
    as `0`, it means a patient who was readmitted is predicted as not readmitted,
    and this will cause severe penalties for the hospital.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`0` divided by the total number of patients who were actually `0`. Specificity
    is also known as the true negative rate:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specificity = TN / (TN+FP)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Specificity refers to how often the prediction is correct when the actual value
    is negative. There are cases, such as spam email detection, where we need our
    algorithm to be more specific. The model predicts `1` when an email is spam and
    `0` when it isn't. We want the model to predict `0` as always `0`, because if
    a non-spam email is classified as spam, important emails may end up in the spam
    folder. Sensitivity can be compromised here because some spam emails may arrive
    in our inbox, but non-spam emails should never go to the spam folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we discussed previously, whether a model should be sensitive or specific
    totally depends on the business problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Precision**: This is the true positive prediction divided by the total number
    of positive predictions. Precision refers to how often we are correct when the
    value predicted is positive:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precision = TP / (TP+FP)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`FPR` is calculated as the ratio between the number of false-positive events
    and the total number of actual negative events. `FPR` refers to how often we are
    incorrect when the actual value is negative. `FPR` is also equal to `1` - specificity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*False positive rate = FP / (FP+TN)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`ROC curve`. A `ROC curve` is a plot between the true positive rate (`sensitivity`)
    and the `FPR` (`1 - specificity`). The following plot shows an example of an `ROC
    curve`:![Figure 6.4: An example of ROC curve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B15777_06_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.4: An example of ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'To decide which `ROC curve` is the best among multiple curves, we need to look
    at the empty space on the upper left of the curve—the smaller the space, the better
    the result. The following plot shows an example of multiple `ROC curves`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: An example of multiple ROC curves'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.5: An example of multiple ROC curves'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The red curve is better than the blue curve because it leaves less space in
    the upper-left corner.
  prefs: []
  type: TYPE_NORMAL
- en: The `ROC curve` of a model tells us the relationship between `sensitivity` and
    `specificity`.
  prefs: []
  type: TYPE_NORMAL
- en: '`ROC curve`. Sometimes, `AUC` is also written as `AUROC`, meaning the area
    under the `ROC curve`. Basically, `AUC` is a numeric value that represents the
    area under a `ROC curve`. The larger the area under the `ROC`, the better, and
    the bigger the `AUC score`, the better. The preceding plot shows us an example
    of an `AUC`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding plot, the `AUC` of the red curve is greater than the `AUC`
    of the blue curve, which means the `AUC` of the red curve is better than the AUC
    of the blue curve. There is no standard rule for the `AUC score`, but here are
    some generally acceptable values and how they relate to model quality:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6: General acceptable AUC score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.6: General acceptable AUC score'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the theory behind the various metrics, let's complete
    some activities and exercises to implement what we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.02: Computing Accuracy and Null Accuracy with APS Failure for Scania
    Trucks Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset that we will be using in this exercise consists of data that's been
    collected from heavy Scania trucks in everyday usage that have failed in some
    way. The system in focus is the `Air pressure system` (`APS`), which generates
    pressurized air that is utilized in various functions in a truck, such as braking
    and gear changes. The positive class in the dataset represents component failures
    for a specific component in the APS, while the negative class represents failures
    for components not related to the APS.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this exercise is to predict which trucks have had failures
    due to the APS so that the repair and maintenance mechanics have the information
    they can work with when checking why the truck failed and which area of the truck
    needs to be inspected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this exercise can be downloaded from this book's GitHub repository
    at [https://packt.live/2SGEEsH](https://packt.live/2SGEEsH).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this exercise, you may get slightly different results due to the
    random nature of the internal mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preprocessing and exploratory data analysis**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries. Load the dataset using the pandas `read_csv`
    function and explore the first `five` rows of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following table shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7: First five rows of the patient readmission dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.7: First five rows of the patient readmission dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Describe the feature values in the dataset using the `describe` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following table shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8: Numerical metadata of the patient readmission dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.8: Numerical metadata of the patient readmission dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Independent variables are also known as explanatory variables, while dependent
    variables are also known as `response variables`. Also, remember that indexing
    in Python starts from `0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Explore `y` using the `head` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following table shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9: The first five rows of the y variable of the patient readmission
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.9: The first five rows of the y variable of the patient readmission
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into test and train sets by using the `train_test_split` function
    from the scikit-learn library. To make sure we all get the same results, set the
    `random_state` parameter to `42`. The data is split with an `80:20 ratio`, meaning
    `80%` of the data is `training data` and the remaining `20%` is `test data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale the training data using the `StandardScaler` function and use the scaler
    to scale the `test data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `sc.fit_transform()` function transforms the data and the data is converted
    into a `NumPy` array. We may need the data for further analysis in the DataFrame
    objects, so the `pd.DataFrame()` function reconverts data into a DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This completes the data preprocessing part of this exercise. Now, we need to
    build a neural network and calculate the `accuracy`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the libraries that are required for creating the neural network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add `five` hidden layers of the `Dense` class and the add `Dropout` after each.
    Build the first hidden layer so that it has a size of `64` and with a dropout
    rate of `0.5`. The second hidden layer will have a size of `32` and a dropout
    rate of `0.4`. The third hidden layer will have a size of `16` and a dropout rate
    of `0.3`. The fourth hidden layer will have a size of `8` and dropout rate of
    `0.2`. The final hidden layer will have a size of `4` and a dropout rate of `0.1`.
    Each hidden layer will have a `ReLU activation` function and the kernel initializer
    will be set to `uniform`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an output `Dense` layer with a `sigmoid` activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since the output is binary, we are using the `sigmoid` function. If the output
    is multiclass (that is, more than two classes), then the `softmax` function should
    be used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the network and fit the model. Calculate the accuracy during the training
    process by setting `metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model with `100` epochs, a batch size of `20`, and a validation split
    of `20%`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the `test` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model returns an accuracy of `98.9917%`. But is it good enough? We can only
    get the answer to this question by comparing it with the null accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Compute the null accuracy:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The null accuracy can be calculated using the `value_count` function of the
    pandas library, which we used in *Exercise 6.01*, *Calculating Null Accuracy on
    a Pacific Hurricanes Dataset*, of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `null` accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we have obtained the null accuracy of the model. As we conclude this
    exercise, the following points must be noted: the accuracy of our model is `98.9917%`,
    approximately. Under ideal conditions, `98.9917%` accuracy is very `good` accuracy,
    but here, the `null accuracy` is `very high`, which helps put our model''s performance
    into perspective. The `null accuracy` of our model is `98.2333%`. Since the `null
    accuracy` of the model is `so high`, an `accuracy` of `98.9917%` is not significant
    but certainly respectable, and `accuracy` in such cases is not the correct metric
    to evaluate an algorithm with.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31FUb2d](https://packt.live/31FUb2d).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3goL0ax](https://packt.live/3goL0ax).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's go through activity on computing the accuracy and null accuracy of
    the neural network model when we change the train/test split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Computing the Accuracy and Null Accuracy of a Neural Network
    When We Change the Train/Test Split'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A train/test split is a random sampling technique. In this activity, we will
    see that our null accuracy and accuracy will be affected by changing the `train`/`test`
    split. To implement this, the part of the code where the train/test split was
    defined has to be changed. We will use the same dataset that we used in *Exercise
    6.02*, *Computing Accuracy and Null Accuracy with APS Failure for Scania Trucks
    Data*. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import all the necessary dependencies and load the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change `test_size` and `random_state` from `0.20` to `0.30` and `42` to `13`,
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the data using the `StandardScaler` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the libraries that are required to build a neural network architecture
    and initiate the `Sequential` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the `Dense` layers with `Dropout`. Set the first hidden layer so that it
    has a size of `64` with a dropout rate of `0.5`, the second hidden layer so that
    it has a size of `32` with a dropout rate of `0.4`, the third hidden layer so
    that is has a size of `16` with a dropout rate of `0.3`, the fourth hidden layer
    so that it has a size of `8` with a dropout rate of `0.2`, and the final hidden
    layer so that it has a size of `4` with a dropout rate of `0.1`. Set all the activation
    functions to `ReLU`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an output `Dense` layer with the `sigmoid` activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the network and fit the model using accuracy. Fit the model with 100
    epochs and a batch size of 20.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the training data while saving the results from the fit process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the number of values in each class of the test target dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the null accuracy using the pandas `value_count` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this activity, you may get slightly different results due to the random nature
    of internal mathematical operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can see that the accuracy and null accuracy will change as we change
    the `train`/`test` split. We will not cover any sampling techniques in this chapter
    as we have a very highly imbalanced dataset, and sampling techniques will not
    yield any fruitful results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 430.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next exercise and compute the metrics that have been derived
    from the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.03: Deriving and Computing Metrics Based on a Confusion Matrix'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset that we will be using in this exercise consists of data that has
    been collected from heavy Scania trucks in everyday usage that have failed in
    some way. The system that's in focus is the `Air Pressure System` (`APS`), which
    generates pressurized air that is utilized in various functions in a truck, such
    as braking and gear changes. The positive class in the dataset represents component
    failures for a specific component in the APS, while the negative class represents
    failures for components not related to the APS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of this exercise is to predict which trucks have had failures
    due to the APS, much like we did in the previous exercise. We will derive the
    sensitivity, specificity, precision, and false positive rate of the neural network
    model to evaluate its performance. Finally, we will adjust the threshold value
    and recompute the sensitivity and specificity. Follow these steps to complete
    this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this exercise can be downloaded from this book's GitHub repository
    at [https://packt.live/2SGEEsH](https://packt.live/2SGEEsH).
  prefs: []
  type: TYPE_NORMAL
- en: You may get slightly different results due to the random nature of internal
    mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries and load the data using the pandas `read_csv`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, split the data into training and test datasets using the `train_test_split`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Following this, scale the feature data so that it has a `mean` of `0` and a
    `standard deviation` of `1` using the `StandardScaler` function. Fit the scaler
    to the `training data` and apply it to the `test data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import the `Keras` libraries that are required to create the model. Instantiate
    a `Keras` model of the `Sequential` class and add five hidden layers to the model,
    including dropout for each layer. The first hidden layer should have a size of
    `64` and a dropout rate of `0.5`. The second hidden layer should have a size of
    `32` and a dropout rate of `0.4`. The third hidden layer should have a size of
    `16` and a dropout rate of `0.3`. The fourth hidden layer should have a size of
    `8` and a dropout rate of `0.2`. The final hidden layer should have a size of
    `4` and a dropout rate of `0.1`. All the hidden layers should have `ReLU activation`
    functions and have `kernel_initializer = ''uniform''`. Add a final output layer
    to the model with a `sigmoid activation` function. Compile the model by calculating
    the accuracy metric during the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, fit the model to the training data by training for `100` epochs with
    `batch_size=20` and `validation_split=0.2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model has finished fitting to the `training data`, create a variable
    that is the result of the model''s prediction on the `test data` using the model''s
    `predict` and `predict_proba` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, compute the predicted class by setting the value of the prediction on
    the `test set` to `1` if the value is above `0.5` and `0` if it''s below `0.5`.
    Compute the `confusion matrix` using the `confusion_matrix` function from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Always use `y_test` as the first parameter and `y_pred_class1` as the second
    parameter so that you always get the correct results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the true negative (`TN`), false negative (`FN`), false positive (`FP`),
    and true positive (`TP`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using `y_test` and `y_pred_class1` in that order is necessary because if they
    are used in reverse order, the matrix will still be computed without errors, but
    will be incorrect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the `sensitivity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `specificity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `precision`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `false positive rate`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following image shows the output of the values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10: Metrics summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.10: Metrics summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sensitivity is inversely proportional to specificity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we discussed previously, our model should be more sensitive, but it looks
    more specific and less sensitive. So, how do we solve this? The answer lies in
    the threshold probabilities. The sensitivity of the model can be increased by
    adjusting the threshold value for classifying the dependent variable as `1` or
    `0`. Recall that, originally, we set the value of `y_pred_class1` to greater than
    `0.5`. Let's change the threshold to `0.3` and rerun the code to check the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Go to *step 7*, change the threshold from `0.5` to `0.3`, and rerun the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a `confusion matrix` and calculate the `specificity` and `sensitivity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For comparison, the following is the previous `confusion matrix` with a `threshold`
    of `0.5`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Always remember that the original values of `y_test` should be passed as the
    first parameter and `y_pred` as the second parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the various components of the `confusion matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the new `sensitivity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `specificity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There is a clear increase in `sensitivity` and `specificity` after decreasing
    the threshold:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11: Sensitivity and specificity comparison'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15777_06_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.11: Sensitivity and specificity comparison'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, clearly, decreasing the threshold value increases the sensitivity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the data distribution. To understand why decreasing the threshold
    value increases the sensitivity, we need to see a histogram of our predicted probabilities.
    Recall that we created the `y_pred_prob` variable to predict the probabilities
    of the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot shows the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12: A histogram of the probabilities of patient readmission from
    the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15777_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12: A histogram of the probabilities of patient readmission from the
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This histogram clearly shows that most of the probabilities for the predicted
    classifier lie in a range from `0.0` to `0.1`, which is indeed very low. Unless
    we set the threshold very low, we cannot increase the sensitivity of the model.
    Also, note that sensitivity is inversely proportional to specificity, so when
    one increases, the other decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31E6v32](https://packt.live/31E6v32).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3gquh6y](https://packt.live/3gquh6y).
  prefs: []
  type: TYPE_NORMAL
- en: There is no universal value of the threshold, though the value of `0.5` is commonly
    used as a default. One method for selecting the threshold is to plot a histogram
    and then select the threshold manually. In our case, any threshold between `0.1`
    and `0.7` can be used as the model as there are few predictions between those
    values, as can be seen from the histogram that was produced at the end of the
    previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Another method for choosing the threshold is to plot the `ROC curve`, which
    plots the true positive rate as a function of the false positive rate. Depending
    on your tolerance for each, the threshold value can be selected. Plotting the
    `ROC curve` is also a good technique if we wish to evaluate the performance of
    the model because the area under the `ROC curve` is a direct measure of the model's
    performance. In the next activity, we will explore the performance of our model
    using the `ROC curve` and the `AUC score`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.02: Calculating the ROC Curve and AUC Score'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ROC curve` and `AUC score` is an effective way to easily evaluate the
    performance of a binary classifier. In this activity, we will plot the `ROC curve`
    and calculate the `AUC score` of a model. We will use the same dataset and train
    the same model that we used in *Exercise 6.03*, *Deriving and Computing Metrics
    Based on a Confusion Matrix*. Use the APS failure data and calculate the `ROC
    curve` and `AUC score`. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import all the necessary dependencies and load the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and test datasets using the `train_test_split`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the training and test data using the `StandardScaler` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the libraries that are required to build a neural network architecture
    and initiate the `Sequential` class. Add five `Dense` layers with `Dropout`. Set
    the first hidden layer so that it has a size of `64` with a dropout rate of `0.5`,
    the second hidden layer so that it has a size of `32` with a dropout rate of `0.4`,
    the third hidden layer so that it has a size of `16` with a dropout rate of `0.3`,
    the fourth hidden layer so that it has a size of `8` with a dropout rate of `0.2`,
    and the final hidden layer so that it has a size of `4`, with a dropout rate of
    `0.1`. Set all the activation functions to `ReLU`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an output `Dense` layer with the `sigmoid` activation function. Compile
    the network then fit the model using accuracy. Fit the model with `100` epochs
    and a batch size of `20`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the training data, saving the results from the fit process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable representing the predicted classes of the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the false positive rate and true positive rate using the `roc_curve`
    function from `sklearn.metrics`. The false positive rate and true positive rate
    are the first and second of three return variables. Pass the true values and the
    predicted values to the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the ROC curve, which is the true positive rate as a function of the false
    positive rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the AUC score using the `roc_auc_score` from `sklearn.metrics` while
    passing the true values and predicted values of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After implementing these steps, you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 434.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we learned how to calculate a `ROC` and an `AUC score` with
    the APS failure dataset. We also learned how specificity and sensitivity change
    with different threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered model evaluation and accuracy in depth. We learned
    how accuracy is not the most appropriate technique for evaluation when our dataset
    is imbalanced. We also learned how to compute a confusion matrix using scikit-learn
    and how to derive other metrics, such as sensitivity, specificity, precision,
    and false positive rate.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we understood how to use threshold values to adjust metrics and how
    `ROC curves` and `AUC scores` help us evaluate our models. It is very common to
    deal with imbalanced datasets in real-life problems. Problems such as credit card
    fraud detection, disease prediction, and spam email detection all have imbalanced
    data in different proportions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about a different kind of neural network
    architecture (convolutional neural networks) that performs well on image classification
    tasks. We will test performance by classifying images into two classes and experiment
    with different architectures and activation functions.
  prefs: []
  type: TYPE_NORMAL
