<html><head></head><body>
		<div>
			<div id="_idContainer125" class="Content">
			</div>
		</div>
		<div id="_idContainer126" class="Content">
			<h1 id="_idParaDest-127"><a id="_idTextAnchor128"/>6. Model Evaluation</h1>
		</div>
		<div id="_idContainer139" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter covers model evaluation in depth. We will discuss alternatives to accuracy to evaluate the performance of a model when standard techniques are not feasible, especially where there are imbalanced classes. Finally, we will utilize confusion matrices, sensitivity, specificity, precision, FPR, ROC curves, and AUC scores to evaluate the performance of classifiers. By the end of this chapter, you will have an in-depth understanding of accuracy and null accuracy and will be able to understand and combat the challenges of imbalanced datasets.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>Introduction</h1>
			<p>In the previous chapter, we covered <strong class="source-inline">regularization</strong> techniques for neural networks. <strong class="source-inline">Regularization</strong> is an important technique when it comes to combatting how a model overfits the training data and helps the model perform well on new, unseen data examples. One of the regularization techniques we covered involved <strong class="source-inline">L1</strong> and <strong class="source-inline">L2</strong> weight regularizations, in which penalization is added to the weights. The other regularization technique we learned about was <strong class="source-inline">dropout regularization</strong>, in which some units of layers are randomly removed from the model fitting process at each iteration. Both regularization techniques are designed to prevent individual weights or units by influencing them too strongly and allowing them to generalize as well.</p>
			<p>In this chapter, we will learn about some different evaluation techniques other than <strong class="source-inline">accuracy</strong>. For any data scientist, the first step after building a model is to evaluate it, and the easiest way to evaluate a model is through its accuracy. However, in real-world scenarios, particularly where there are classification tasks with highly imbalanced classes such as for predicting the presence of hurricanes, predicting the presence of a rare disease, or predicting if someone will default on a loan, evaluating the model using its accuracy score is not the best evaluation technique.</p>
			<p>This chapter explores core concepts such as imbalanced datasets and how different evaluation techniques can be used to work through these imbalanced datasets. This chapter begins with an introduction to accuracy and its limitations. Then, we will explore the concepts of <strong class="source-inline">null accuracy</strong>, <strong class="source-inline">imbalanced datasets</strong>, <strong class="source-inline">sensitivity</strong>, <strong class="source-inline">specificity</strong>, <strong class="source-inline">precision</strong>, <strong class="source-inline">false positives</strong>, <strong class="source-inline">ROC curves</strong>, and <strong class="source-inline">AUC scores</strong>.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor130"/>Accuracy</h1>
			<p>To understand accuracy properly, let's explore model evaluation. Model evaluation is an integral part of the model development process. Once you've built your model and executed it, the next step is to evaluate your model. </p>
			<p>A model is built on a <strong class="source-inline">training dataset</strong> and evaluating a model's performance on the same training dataset is bad practice in data science. Once a model has been trained on a training dataset, it should be evaluated on a dataset that is completely different from the training dataset. This dataset is known as the <strong class="source-inline">test dataset</strong>. The objective should always be to build a model that generalizes, which means the model should produce similar (but not the same) results, or relatively similar results, on any dataset. This can only be achieved if we evaluate the model on data that is unknown to it.</p>
			<p>The model evaluation process requires a metric that can quantify a model's performance. The simplest metric for model evaluation is accuracy. <strong class="source-inline">Accuracy</strong> is the fraction of predictions that our model gets right. This is the formula for calculating <strong class="source-inline">accuracy</strong>:</p>
			<p><em class="italic">Accuracy = (Number of correct predictions) / (Total number of predictions)</em></p>
			<p>For example, if we have <strong class="source-inline">10</strong> records and <strong class="source-inline">7</strong> are predicted correctly, then we can say that the accuracy of our model is <strong class="source-inline">70%</strong>. This is calculated as <strong class="source-inline">7/10</strong> = <strong class="source-inline">0.7</strong> or <strong class="source-inline">70%</strong>.</p>
			<p><strong class="source-inline">Null accuracy</strong> is the accuracy that can be achieved by predicting the most frequent class. If we don't run an algorithm and just predict accuracy based on the most frequent outcome, then the accuracy that's calculated based on this prediction is known as <strong class="source-inline">null accuracy</strong>:</p>
			<p><em class="italic">Null accuracy = (Total number of instances of the frequently occurring class) / (Total number of instances)</em></p>
			<p>Take a look at this example:</p>
			<p>10 actual outcomes: [1,0,0,0,0,0,0,0,1,0].</p>
			<p><strong class="source-inline">Prediction</strong>: [0,0,0,0,0,0,0,0,0,0]</p>
			<p><strong class="source-inline">Null accuracy</strong> = 8/10 = 0.8 or 80% </p>
			<p>So, our null accuracy is <strong class="source-inline">80%</strong>, meaning we are correct <strong class="source-inline">80%</strong> of the time. This means we have achieved <strong class="source-inline">80%</strong> accuracy without running an algorithm. Always remember that when null accuracy is high, it means that the distribution of response variables is skewed in favor of the frequently occurring class.</p>
			<p>Let's work on an exercise to find the null accuracy of a dataset. The null accuracy of a dataset can be found by using the <strong class="source-inline">value_count</strong> function in the pandas library. The <strong class="source-inline">value_count</strong> function returns a series containing counts of unique values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the Jupyter Notebooks for the exercises and activities in this chapter are available on GitHub at <a href="https://packt.live/37jHNUR">https://packt.live/37jHNUR</a>.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Exercise 6.01: Calculating Null Accuracy on a Pacific Hurricanes Dataset</h2>
			<p>We have a dataset documenting whether a <strong class="source-inline">hurricane</strong> has been observed in the Pacific Ocean that has two columns, <strong class="source-inline">Date</strong> and <strong class="source-inline">hurricane</strong>. The <strong class="source-inline">Date</strong> column indicates the date of the observation, while the <strong class="source-inline">hurricane</strong> column indicates whether there was a hurricane on that date. Rows with a <strong class="source-inline">hurricane</strong> value of <strong class="source-inline">1</strong> means there was a hurricane, while <strong class="source-inline">0</strong> means there was no hurricane. Find the <strong class="source-inline">null accuracy</strong> of the dataset by following these steps:</p>
			<ol>
				<li>Open a Jupyter notebook. Import all the required libraries and load the <strong class="source-inline">pacific_hurricanes.csv</strong> file into the <strong class="source-inline">data</strong> folder from this book's GitHub repository:<p class="source-code"># Import the data</p><p class="source-code">import pandas as pd</p><p class="source-code">df = pd.read_csv("../data/pacific_hurricanes.csv")</p><p class="source-code">df.head() </p><p>The following is the output of the preceding code:</p><div id="_idContainer127" class="IMG---Figure"><img src="image/B15777_06_01.jpg" alt="Figure 6.1: Data exploration of the pacific hurricanes dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.1: Data exploration of the pacific hurricanes dataset</p></li>
				<li>Use the built-in <strong class="source-inline">value_count</strong> function from the pandas library to get the distribution for the data of the <strong class="source-inline">hurricane</strong> column. The <strong class="source-inline">value_count</strong> function shows the total instances of unique values:<p class="source-code">df['hurricane'].value_counts()</p><p>The preceding code produces the following output:</p><p class="source-code">0 22435</p><p class="source-code">1 1842</p><p class="source-code">Name: hurricane, dtype: int64</p></li>
				<li>Use the <strong class="source-inline">value_count</strong> function and set the <strong class="source-inline">normalize</strong> parameter to <strong class="source-inline">True</strong>. To find the null accuracy, you will have to index the <strong class="source-inline">pandas</strong> series that was produced for index <strong class="source-inline">0</strong> to get the proportion of values related to no hurricanes occurring on a given day:<p class="source-code">df['hurricane'].value_counts(normalize=True).loc[0]</p><p>The preceding code produces the following output:</p><p class="source-code">0.9241257156979857</p><p>The calculated <strong class="source-inline">null accuracy</strong> of the dataset is <strong class="source-inline">92.4126%</strong>.</p></li>
			</ol>
			<p>Here, we can see that our dataset has a very high null accuracy of <strong class="source-inline">92.4126%</strong>. So, if we just make a dumb model that predicts the majority class for all outcomes, our model will be <strong class="source-inline">92.4126%</strong> accurate.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31FtQBm">https://packt.live/31FtQBm</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ArNwNT">https://packt.live/2ArNwNT</a>.</p>
			<p>Later in this chapter, in <em class="italic">Activity 6.01</em>, <em class="italic">Computing the Accuracy and Null Accuracy of a Neural Network When We Change the Train/Test Split,</em> we will see how null accuracy changes as we change the <strong class="source-inline">test</strong>/<strong class="source-inline">train</strong> split.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>Advantages and Limitations of Accuracy</h2>
			<p>The advantages of accuracy are as follows:</p>
			<ul>
				<li><strong class="bold">Easy to use</strong>: Accuracy is very easy to compute and understand as it is just a simple fraction formula.</li>
				<li><strong class="bold">Popular compared to other techniques</strong>: Since it is the easiest metric to compute, it is also the most popular and is universally accepted as the first step of evaluating a model. Most introductory books on data science teach accuracy as an evaluation metric.</li>
				<li><strong class="bold">Good for comparing different models</strong>: Suppose you are trying to solve a problem with different models. You can always trust the model that gives the highest accuracy.</li>
			</ul>
			<p>The limitations of accuracy are as follows:</p>
			<ul>
				<li><strong class="bold">No representation of response variable distribution</strong>: Accuracy doesn't give us an idea of the distribution of the <strong class="source-inline">response</strong>/<strong class="source-inline">dependent</strong> variable. If we get an accuracy of <strong class="source-inline">80%</strong> in our model, we have no idea how the response variable is distributed and what the null accuracy of the dataset is. If the null accuracy of our dataset is above <strong class="source-inline">70%</strong>, then an <strong class="source-inline">80%</strong> accurate model is pretty useless.</li>
				<li><strong class="bold">Type 1 and type 2 errors</strong>: <strong class="source-inline">Accuracy</strong> also gives us no information about <strong class="source-inline">type 1</strong> and <strong class="source-inline">type 2</strong> errors of the model. A <strong class="source-inline">type 1</strong> error is when a class is <strong class="source-inline">negative</strong> and we have predicted it as <strong class="source-inline">positive</strong>, while a <strong class="source-inline">type 2</strong> error is when a class is positive and we have predicted it as negative. We will be studying both of these errors later in this chapter. In the next section, we will cover the imbalanced datasets. The accuracy scores for models classifying imbalanced datasets can be particularly misleading, which is why other evaluation metrics are useful for model evaluation.</li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Imbalanced Datasets</h1>
			<p>Imbalanced datasets are a distinct case for classification problems where the class distribution varies between the classes. In such datasets, one class is overwhelmingly dominant. In other words, the <strong class="source-inline">null accuracy</strong> of an imbalanced dataset is very high. </p>
			<p>Consider an example of credit card fraud. If we have a dataset of credit card transactions, then we will find that, of all the transactions, a very minuscule number of transactions were fraudulent and the majority of transactions were normal transactions. If <strong class="source-inline">1</strong> represents a fraudulent transaction and <strong class="source-inline">0</strong> represents a normal transaction, then there will be many 0s and hardly any 1s. The <strong class="source-inline">null accuracy</strong> of the dataset may be more than <strong class="source-inline">99%</strong>. This means that the majority class (in this case, <strong class="source-inline">0</strong>) is overwhelmingly greater than the minority class (in this case, <strong class="source-inline">1</strong>). Such sets are imbalanced datasets. Consider the following figure, which shows a general imbalanced dataset <strong class="source-inline">scatter plot</strong>:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B15777_06_02.jpg" alt="Figure 6.2: A general imbalanced dataset scatter plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2: A general imbalanced dataset scatter plot</p>
			<p>The preceding plot shows a generalized scatter plot of an imbalanced dataset, where the stars represent the minority class and the circles represent the majority class. As we can see, there are many more circles than stars; this can make it difficult for machine learning models to distinguish between the two classes. In the next section, we will cover some approaches to working with imbalanced datasets.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Working with Imbalanced Datasets</h2>
			<p>In machine learning, there are two ways of overcoming the shortcomings of imbalanced datasets, which are as follows:</p>
			<ul>
				<li><strong class="bold">Sampling techniques</strong>: One way we can mitigate the imbalance of a dataset is by using special sampling techniques with which we can select our training and testing data in such a way that there is an adequate representation of all the classes. There are many such techniques—for instance, oversampling the minority class (meaning we take more samples from the minority class) or undersampling the majority class (meaning we take a smaller sample from the majority class). However, if the data is highly imbalanced with null accuracies above <strong class="source-inline">90%</strong>, then sampling techniques struggle to give the correct representation of majority-minority classes in the data and our model may overfit. So, the best way is to modify our evaluation techniques.</li>
				<li><strong class="bold">Modifying model evaluation techniques</strong>: When working with highly imbalanced datasets, it is better to modify model evaluation techniques. This is the most robust way to get good results, which means using these methods will likely achieve good results on new, unseen data. There are many evaluation metrics other than accuracy that can be modified to evaluate a model. To learn about all those techniques, it is important to understand the concept of the confusion matrix.</li>
			</ul>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor135"/>Confusion Matrix</h1>
			<p>A <strong class="source-inline">confusion matrix</strong> describes the performance of the classification model. In other words, a confusion matrix is a way to summarize classifier performance. The following table shows a basic representation of a confusion matrix and represents how the predicted results by the model compared to the true values:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B15777_06_03.jpg" alt="Figure 6.3: Basic representation of a confusion matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Basic representation of a confusion matrix</p>
			<p>Let's go over the meanings of the abbreviations that were used in the preceding table:</p>
			<ul>
				<li><strong class="bold">TN</strong> (<strong class="bold">True negative</strong>): This is the count of outcomes that were originally negative and were predicted negative.</li>
				<li><strong class="bold">FP</strong> (<strong class="bold">False positive</strong>): This is the count of outcomes that were originally negative but were predicted positive. This error is also called a <strong class="bold">type 1 error</strong>.</li>
				<li><strong class="bold">FN</strong> (<strong class="bold">False negative</strong>): This is the count of outcomes that were originally positive but were predicted negative. This error is also called a <strong class="bold">type 2 error</strong>.</li>
				<li><strong class="bold">TP</strong> (<strong class="bold">True positive</strong>): This is the count of outcomes that were originally positive and were predicted as positive.</li>
			</ul>
			<p>The goal is to maximize the values in the <strong class="bold">TN</strong> and <strong class="bold">TP</strong> boxes in the preceding table, that is, the true negatives and true positives, and minimize the values in the <strong class="bold">FN</strong> and <strong class="bold">FP</strong> boxes, that is, the false negatives and false positives.</p>
			<p>The following code is an example of a confusion matrix:</p>
			<p class="source-code">from sklearn.metrics import confusion_matrix</p>
			<p class="source-code">cm = confusion_matrix(y_test,y_pred_class)</p>
			<p class="source-code">print(cm)</p>
			<p>The preceding code produces the following output:</p>
			<p class="source-code">array([[89, 2],</p>
			<p class="source-code">       [13, 4]], dtype=int64)</p>
			<p>The aim of all machine learning and deep learning algorithms is to maximize TN and TP and minimize FN and FP. The following example code calculates TN, FP, FN, and TP:</p>
			<p class="source-code"># True Negative</p>
			<p class="source-code">TN = cm[0,0]</p>
			<p class="source-code"># False Negative</p>
			<p class="source-code">FN = cm[1,0]</p>
			<p class="source-code"># False Positives</p>
			<p class="source-code">FP = cm[0,1]</p>
			<p class="source-code"># True Positives</p>
			<p class="source-code">TP = cm[1,1]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Accuracy does not help us understand type 1 and type 2 errors.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor136"/>Metrics Computed from a Confusion Matrix</h2>
			<p>The metrics that can be derived from a <strong class="source-inline">confusion matrix</strong> are <strong class="source-inline">sensitivity</strong>, <strong class="source-inline">specificity</strong>, <strong class="source-inline">precision</strong>, <strong class="source-inline">FP rate</strong>, <strong class="source-inline">ROC</strong>, and <strong class="source-inline">AUC</strong>:</p>
			<ul>
				<li><strong class="bold">Sensitivity</strong>: This is the number of positive predictions divided by the total actual number of positives. Sensitivity is also known as recall or true positive. In our case, it is the total number of patients classified as <strong class="source-inline">1</strong>, divided by the total number of patients who are actually <strong class="source-inline">1</strong>:<p><em class="italic">Sensitivity = TP / (TP+FN)</em></p><p>Sensitivity refers to how often the prediction is correct when the actual value is positive. In cases such as building a model to predict patient readmission at a hospital, we need our model to be highly sensitive. We need 1 to be predicted as <strong class="source-inline">1</strong>. If a <strong class="source-inline">0</strong> is predicted as <strong class="source-inline">1</strong>, it is acceptable, but if a <strong class="source-inline">1</strong> is predicted as <strong class="source-inline">0</strong>, it means a patient who was readmitted is predicted as not readmitted, and this will cause severe penalties for the hospital.</p></li>
				<li><strong class="bold">Specificity</strong>: This is the number of negative predictions divided by the total number of actual negatives. To use the previous example, it would be readmission predicted as <strong class="source-inline">0</strong> divided by the total number of patients who were actually <strong class="source-inline">0</strong>. Specificity is also known as the true negative rate:<p><em class="italic">Specificity = TN / (TN+FP)</em></p><p>Specificity refers to how often the prediction is correct when the actual value is negative. There are cases, such as spam email detection, where we need our algorithm to be more specific. The model predicts <strong class="source-inline">1</strong> when an email is spam and <strong class="source-inline">0</strong> when it isn't. We want the model to predict <strong class="source-inline">0</strong> as always <strong class="source-inline">0</strong>, because if a non-spam email is classified as spam, important emails may end up in the spam folder. Sensitivity can be compromised here because some spam emails may arrive in our inbox, but non-spam emails should never go to the spam folder.</p><p class="callout-heading">Note</p><p class="callout">As we discussed previously, whether a model should be sensitive or specific totally depends on the business problem.</p></li>
				<li><strong class="bold">Precision</strong>: This is the true positive prediction divided by the total number of positive predictions. Precision refers to how often we are correct when the value predicted is positive:<p><em class="italic">Precision = TP / (TP+FP)</em></p></li>
				<li><strong class="bold">False Positive Rate</strong> (<strong class="bold">FPR</strong>): The <strong class="source-inline">FPR</strong> is calculated as the ratio between the number of false-positive events and the total number of actual negative events. <strong class="source-inline">FPR</strong> refers to how often we are incorrect when the actual value is negative. <strong class="source-inline">FPR</strong> is also equal to <strong class="source-inline">1</strong> - specificity:<p><em class="italic">False positive rate = FP / (FP+TN)</em></p></li>
				<li><strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>)<strong class="bold"> curve</strong>: Another important way to evaluate a classification model is by using a <strong class="source-inline">ROC curve</strong>. A <strong class="source-inline">ROC curve</strong> is a plot between the true positive rate (<strong class="source-inline">sensitivity</strong>) and the <strong class="source-inline">FPR</strong> (<strong class="source-inline">1 - specificity</strong>). The following plot shows an example of an <strong class="source-inline">ROC curve</strong>:<div id="_idContainer130" class="IMG---Figure"><img src="image/B15777_06_04.jpg" alt="Figure 6.4: An example of ROC curve&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 6.4: An example of ROC curve</p>
			<p>To decide which <strong class="source-inline">ROC curve</strong> is the best among multiple curves, we need to look at the empty space on the upper left of the curve—the smaller the space, the better the result. The following plot shows an example of multiple <strong class="source-inline">ROC curves</strong>:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B15777_06_05.jpg" alt="Figure 6.5: An example of multiple ROC curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5: An example of multiple ROC curves</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The red curve is better than the blue curve because it leaves less space in the upper-left corner.</p>
			<p>The <strong class="source-inline">ROC curve</strong> of a model tells us the relationship between <strong class="source-inline">sensitivity</strong> and <strong class="source-inline">specificity</strong>.</p>
			<ul>
				<li><strong class="bold">Area Under Curve</strong> (<strong class="bold">AUC</strong>): This is the area under the <strong class="source-inline">ROC curve</strong>. Sometimes, <strong class="source-inline">AUC</strong> is also written as <strong class="source-inline">AUROC</strong>, meaning the area under the <strong class="source-inline">ROC curve</strong>. Basically, <strong class="source-inline">AUC</strong> is a numeric value that represents the area under a <strong class="source-inline">ROC curve</strong>. The larger the area under the <strong class="source-inline">ROC</strong>, the better, and the bigger the <strong class="source-inline">AUC score</strong>, the better. The preceding plot shows us an example of an <strong class="source-inline">AUC</strong>.<p>In the preceding plot, the <strong class="source-inline">AUC</strong> of the red curve is greater than the <strong class="source-inline">AUC</strong> of the blue curve, which means the <strong class="source-inline">AUC</strong> of the red curve is better than the AUC of the blue curve. There is no standard rule for the <strong class="source-inline">AUC score</strong>, but here are some generally acceptable values and how they relate to model quality:</p></li>
			</ul>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B15777_06_06.jpg" alt="Figure 6.6: General acceptable AUC score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6: General acceptable AUC score</p>
			<p>Now that we understand the theory behind the various metrics, let's complete some activities and exercises to implement what we have learned.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor137"/>Exercise 6.02: Computing Accuracy and Null Accuracy with APS Failure for Scania Trucks Data</h2>
			<p>The dataset that we will be using in this exercise consists of data that's been collected from heavy Scania trucks in everyday usage that have failed in some way. The system in focus is the <strong class="source-inline">Air pressure system</strong> (<strong class="source-inline">APS</strong>), which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The positive class in the dataset represents component failures for a specific component in the APS, while the negative class represents failures for components not related to the APS.</p>
			<p>The objective of this exercise is to predict which trucks have had failures due to the APS so that the repair and maintenance mechanics have the information they can work with when checking why the truck failed and which area of the truck needs to be inspected.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset for this exercise can be downloaded from this book's GitHub repository at <a href="https://packt.live/2SGEEsH">https://packt.live/2SGEEsH</a>.</p>
			<p class="callout">Throughout this exercise, you may get slightly different results due to the random nature of the internal mathematical operations.</p>
			<p><strong class="bold">Data preprocessing and exploratory data analysis</strong>:</p>
			<ol>
				<li value="1">Import the required libraries. Load the dataset using the pandas <strong class="source-inline">read_csv</strong> function and explore the first <strong class="source-inline">five</strong> rows of the dataset:<p class="source-code">#import the libraries</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code"># Load the Data</p><p class="source-code">X = pd.read_csv("../data/aps_failure_training_feats.csv")</p><p class="source-code">y = pd.read_csv("../data/aps_failure_training_target.csv")</p><p class="source-code"># use the head function view the first 5 rows of the data</p><p class="source-code">X.head()</p><p>The following table shows the output of the preceding code:</p><div id="_idContainer133" class="IMG---Figure"><img src="image/B15777_06_07.jpg" alt="Figure 6.7: First five rows of the patient readmission dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.7: First five rows of the patient readmission dataset</p></li>
				<li>Describe the feature values in the dataset using the <strong class="source-inline">describe</strong> method:<p class="source-code"># Summary of Numerical Data</p><p class="source-code">X.describe()</p><p>The following table shows the output of the preceding code:</p><div id="_idContainer134" class="IMG---Figure"><img src="image/B15777_06_08.jpg" alt="Figure 6.8: Numerical metadata of the patient readmission dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.8: Numerical metadata of the patient readmission dataset</p><p class="callout-heading">Note</p><p class="callout">Independent variables are also known as explanatory variables, while dependent variables are also known as <strong class="source-inline">response variables</strong>. Also, remember that indexing in Python starts from <strong class="source-inline">0</strong>.</p></li>
				<li>Explore <strong class="source-inline">y</strong> using the <strong class="source-inline">head</strong> function:<p class="source-code">y.head()</p><p>The following table shows the output of the preceding code:</p><div id="_idContainer135" class="IMG---Figure"><img src="image/B15777_06_09.jpg" alt="Figure 6.9: The first five rows of the y variable of the patient readmission dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.9: The first five rows of the y variable of the patient readmission dataset</p></li>
				<li>Split the data into test and train sets by using the <strong class="source-inline">train_test_split</strong> function from the scikit-learn library. To make sure we all get the same results, set the <strong class="source-inline">random_state</strong> parameter to <strong class="source-inline">42</strong>. The data is split with an <strong class="source-inline">80:20 ratio</strong>, meaning <strong class="source-inline">80%</strong> of the data is <strong class="source-inline">training data</strong> and the remaining <strong class="source-inline">20%</strong> is <strong class="source-inline">test data</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed = 42</p><p class="source-code">X_train, X_test, \</p><p class="source-code">y_train, y_test= train_test_split(X, y, test_size=0.20, \</p><p class="source-code">                                  random_state=seed)</p></li>
				<li>Scale the training data using the <strong class="source-inline">StandardScaler</strong> function and use the scaler to scale the <strong class="source-inline">test data</strong>:<p class="source-code"># Initialize StandardScaler</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code"># Transform the training data</p><p class="source-code">X_train = sc.fit_transform(X_train)</p><p class="source-code">X_train = pd.DataFrame(X_train,columns=X_test.columns)</p><p class="source-code"># Transform the testing data</p><p class="source-code">X_test = sc.transform(X_test)</p><p class="source-code">X_test = pd.DataFrame(X_test,columns=X_train.columns)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">sc.fit_transform()</strong> function transforms the data and the data is converted into a <strong class="source-inline">NumPy</strong> array. We may need the data for further analysis in the DataFrame objects, so the <strong class="source-inline">pd.DataFrame()</strong> function reconverts data into a DataFrame.</p><p>This completes the data preprocessing part of this exercise. Now, we need to build a neural network and calculate the <strong class="source-inline">accuracy</strong>.</p></li>
				<li>Import the libraries that are required for creating the neural network architecture:<p class="source-code"># Import the relevant Keras libraries</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code">from tensorflow import random</p></li>
				<li>Initiate the <strong class="source-inline">Sequential</strong> class:<p class="source-code"># Initiate the Model with Sequential Class</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add <strong class="source-inline">five</strong> hidden layers of the <strong class="source-inline">Dense</strong> class and the add <strong class="source-inline">Dropout</strong> after each. Build the first hidden layer so that it has a size of <strong class="source-inline">64</strong> and with a dropout rate of <strong class="source-inline">0.5</strong>. The second hidden layer will have a size of <strong class="source-inline">32</strong> and a dropout rate of <strong class="source-inline">0.4</strong>. The third hidden layer will have a size of <strong class="source-inline">16</strong> and a dropout rate of <strong class="source-inline">0.3</strong>. The fourth hidden layer will have a size of <strong class="source-inline">8</strong> and dropout rate of <strong class="source-inline">0.2</strong>. The final hidden layer will have a size of <strong class="source-inline">4</strong> and a dropout rate of <strong class="source-inline">0.1</strong>. Each hidden layer will have a <strong class="source-inline">ReLU activation</strong> function and the kernel initializer will be set to <strong class="source-inline">uniform</strong>:<p class="source-code"># Add the hidden dense layers and with dropout Layer</p><p class="source-code">model.add(Dense(units=64, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.5))</p><p class="source-code">model.add(Dense(units=32, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.4))</p><p class="source-code">model.add(Dense(units=16, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.3))</p><p class="source-code">model.add(Dense(units=8, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.2))</p><p class="source-code">model.add(Dense(units=4, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.1))</p></li>
				<li>Add an output <strong class="source-inline">Dense</strong> layer with a <strong class="source-inline">sigmoid</strong> activation function:<p class="source-code"># Add Output Dense Layer</p><p class="source-code">model.add(Dense(units=1, activation='sigmoid', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="callout-heading">Note</p><p class="callout">Since the output is binary, we are using the <strong class="source-inline">sigmoid</strong> function. If the output is multiclass (that is, more than two classes), then the <strong class="source-inline">softmax</strong> function should be used.</p></li>
				<li>Compile the network and fit the model. Calculate the accuracy during the training process by setting <strong class="source-inline">metrics=['accuracy']</strong>:<p class="source-code"># Compile the model</p><p class="source-code">model.compile(optimizer='adam', \</p><p class="source-code">              loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model with <strong class="source-inline">100</strong> epochs, a batch size of <strong class="source-inline">20</strong>, and a validation split of <strong class="source-inline">20%</strong>:<p class="source-code">#Fit the Model</p><p class="source-code">model.fit(X_train, y_train, epochs=100, \</p><p class="source-code">          batch_size=20, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p></li>
				<li>Evaluate the model on the <strong class="source-inline">test</strong> dataset:<p class="source-code">test_loss, test_acc = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'The loss on the test set is {test_loss:.4f} \</p><p class="source-code">and the accuracy is {test_acc*100:.4f}%')</p><p>The preceding code produces the following output:</p><p class="source-code">12000/12000 [==============================] - 0s 20us/step</p><p class="source-code">The loss on the test set is 0.0802 and the accuracy is 98.9917%</p><p>The model returns an accuracy of <strong class="source-inline">98.9917%</strong>. But is it good enough? We can only get the answer to this question by comparing it with the null accuracy.</p><p><strong class="bold">Compute the null accuracy:</strong></p></li>
				<li>The null accuracy can be calculated using the <strong class="source-inline">value_count</strong> function of the pandas library, which we used in <em class="italic">Exercise 6.01</em>, <em class="italic">Calculating Null Accuracy on a Pacific Hurricanes Dataset</em>, of this chapter:<p class="source-code">"""</p><p class="source-code">Use the value_count function to calculate distinct class values</p><p class="source-code">"""</p><p class="source-code">y_test['class'].value_counts()</p><p>The preceding code produces the following output:</p><p class="source-code">0    11788</p><p class="source-code">1      212</p><p class="source-code">Name: class, dtype: int64</p></li>
				<li>Calculate the <strong class="source-inline">null</strong> accuracy:<p class="source-code"># Calculate the null accuracy </p><p class="source-code">y_test['class'].value_counts(normalize=True).loc[0]</p><p>The preceding code produces the following output:</p><p class="source-code">0.9823333333333333</p><p>Here, we have obtained the null accuracy of the model. As we conclude this exercise, the following points must be noted: the accuracy of our model is <strong class="source-inline">98.9917%</strong>, approximately. Under ideal conditions, <strong class="source-inline">98.9917%</strong> accuracy is very <strong class="source-inline">good</strong> accuracy, but here, the <strong class="source-inline">null accuracy</strong> is <strong class="source-inline">very high</strong>, which helps put our model's performance into perspective. The <strong class="source-inline">null accuracy</strong> of our model is <strong class="source-inline">98.2333%</strong>. Since the <strong class="source-inline">null accuracy</strong> of the model is <strong class="source-inline">so high</strong>, an <strong class="source-inline">accuracy</strong> of <strong class="source-inline">98.9917%</strong> is not significant but certainly respectable, and <strong class="source-inline">accuracy</strong> in such cases is not the correct metric to evaluate an algorithm with.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31FUb2d">https://packt.live/31FUb2d</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3goL0ax">https://packt.live/3goL0ax</a>.</p></li>
			</ol>
			<p>Now, let's go through activity on computing the accuracy and null accuracy of the neural network model when we change the train/test split.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor138"/>Activity 6.01: Computing the Accuracy and Null Accuracy of a Neural Network When We Change the Train/Test Split</h2>
			<p>A train/test split is a random sampling technique. In this activity, we will see that our null accuracy and accuracy will be affected by changing the <strong class="source-inline">train</strong>/<strong class="source-inline">test</strong> split. To implement this, the part of the code where the train/test split was defined has to be changed. We will use the same dataset that we used in <em class="italic">Exercise 6.02</em>, <em class="italic">Computing Accuracy and Null Accuracy with APS Failure for Scania Trucks Data</em>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the necessary dependencies and load the dataset.</li>
				<li>Change <strong class="source-inline">test_size</strong> and <strong class="source-inline">random_state</strong> from <strong class="source-inline">0.20</strong> to <strong class="source-inline">0.30</strong> and <strong class="source-inline">42</strong> to <strong class="source-inline">13</strong>, respectively.</li>
				<li>Scale the data using the <strong class="source-inline">StandardScaler</strong> function.</li>
				<li>Import the libraries that are required to build a neural network architecture and initiate the <strong class="source-inline">Sequential</strong> class.</li>
				<li>Add the <strong class="source-inline">Dense</strong> layers with <strong class="source-inline">Dropout</strong>. Set the first hidden layer so that it has a size of <strong class="source-inline">64</strong> with a dropout rate of <strong class="source-inline">0.5</strong>, the second hidden layer so that it has a size of <strong class="source-inline">32</strong> with a dropout rate of <strong class="source-inline">0.4</strong>, the third hidden layer so that is has a size of <strong class="source-inline">16</strong> with a dropout rate of <strong class="source-inline">0.3</strong>, the fourth hidden layer so that it has a size of <strong class="source-inline">8</strong> with a dropout rate of <strong class="source-inline">0.2</strong>, and the final hidden layer so that it has a size of <strong class="source-inline">4</strong> with a dropout rate of <strong class="source-inline">0.1</strong>. Set all the activation functions to <strong class="source-inline">ReLU</strong>.</li>
				<li>Add an output <strong class="source-inline">Dense</strong> layer with the <strong class="source-inline">sigmoid</strong> activation function.</li>
				<li>Compile the network and fit the model using accuracy. Fit the model with 100 epochs and a batch size of 20.</li>
				<li>Fit the model to the training data while saving the results from the fit process.</li>
				<li>Evaluate the model on the test dataset.</li>
				<li>Count the number of values in each class of the test target dataset.</li>
				<li>Calculate the null accuracy using the pandas <strong class="source-inline">value_count</strong> function.<p class="callout-heading">Note</p><p class="callout">In this activity, you may get slightly different results due to the random nature of internal mathematical operations.</p></li>
			</ol>
			<p>Here, we can see that the accuracy and null accuracy will change as we change the <strong class="source-inline">train</strong>/<strong class="source-inline">test</strong> split. We will not cover any sampling techniques in this chapter as we have a very highly imbalanced dataset, and sampling techniques will not yield any fruitful results.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 430.</p>
			<p>Let's move on to the next exercise and compute the metrics that have been derived from the confusion matrix.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/>Exercise 6.03: Deriving and Computing Metrics Based on a Confusion Matrix</h2>
			<p>The dataset that we will be using in this exercise consists of data that has been collected from heavy Scania trucks in everyday usage that have failed in some way. The system that's in focus is the <strong class="source-inline">Air Pressure System</strong> (<strong class="source-inline">APS</strong>), which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The positive class in the dataset represents component failures for a specific component in the APS, while the negative class represents failures for components not related to the APS.</p>
			<p>The objective of this exercise is to predict which trucks have had failures due to the APS, much like we did in the previous exercise. We will derive the sensitivity, specificity, precision, and false positive rate of the neural network model to evaluate its performance. Finally, we will adjust the threshold value and recompute the sensitivity and specificity. Follow these steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset for this exercise can be downloaded from this book's GitHub repository at <a href="https://packt.live/2SGEEsH">https://packt.live/2SGEEsH</a>.</p>
			<p class="callout">You may get slightly different results due to the random nature of internal mathematical operations.</p>
			<ol>
				<li value="1">Import the necessary libraries and load the data using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code"># Import the libraries</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code"># Load the Data</p><p class="source-code">X = pd.read_csv("../data/aps_failure_training_feats.csv")</p><p class="source-code">y = pd.read_csv("../data/aps_failure_training_target.csv")</p></li>
				<li>Next, split the data into training and test datasets using the <strong class="source-inline">train_test_split</strong> function:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">seed = 42</p><p class="source-code">X_train, X_test, \</p><p class="source-code">y_train, y_test = train_test_split(X, y, \</p><p class="source-code">                  test_size=0.20, random_state=seed)</p></li>
				<li>Following this, scale the feature data so that it has a <strong class="source-inline">mean</strong> of <strong class="source-inline">0</strong> and a <strong class="source-inline">standard deviation</strong> of <strong class="source-inline">1</strong> using the <strong class="source-inline">StandardScaler</strong> function. Fit the scaler to the <strong class="source-inline">training data</strong> and apply it to the <strong class="source-inline">test data</strong>:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">sc = StandardScaler()</p><p class="source-code"># Transform the training data</p><p class="source-code">X_train = sc.fit_transform(X_train)</p><p class="source-code">X_train = pd.DataFrame(X_train,columns=X_test.columns)</p><p class="source-code"># Transform the testing data</p><p class="source-code">X_test = sc.transform(X_test)</p><p class="source-code">X_test = pd.DataFrame(X_test,columns=X_train.columns)</p></li>
				<li>Next, import the <strong class="source-inline">Keras</strong> libraries that are required to create the model. Instantiate a <strong class="source-inline">Keras</strong> model of the <strong class="source-inline">Sequential</strong> class and add five hidden layers to the model, including dropout for each layer. The first hidden layer should have a size of <strong class="source-inline">64</strong> and a dropout rate of <strong class="source-inline">0.5</strong>. The second hidden layer should have a size of <strong class="source-inline">32</strong> and a dropout rate of <strong class="source-inline">0.4</strong>. The third hidden layer should have a size of <strong class="source-inline">16</strong> and a dropout rate of <strong class="source-inline">0.3</strong>. The fourth hidden layer should have a size of <strong class="source-inline">8</strong> and a dropout rate of <strong class="source-inline">0.2</strong>. The final hidden layer should have a size of <strong class="source-inline">4</strong> and a dropout rate of <strong class="source-inline">0.1</strong>. All the hidden layers should have <strong class="source-inline">ReLU activation</strong> functions and have <strong class="source-inline">kernel_initializer = 'uniform'</strong>. Add a final output layer to the model with a <strong class="source-inline">sigmoid activation</strong> function. Compile the model by calculating the accuracy metric during the training process:<p class="source-code"># Import the relevant Keras libraries</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">from keras.layers import Dropout</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(seed)</p><p class="source-code">random.set_seed(seed)</p><p class="source-code">model = Sequential()</p><p class="source-code"># Add the hidden dense layers and with dropout Layer</p><p class="source-code">model.add(Dense(units=64, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform', \</p><p class="source-code">                input_dim=X_train.shape[1]))</p><p class="source-code">model.add(Dropout(rate=0.5))</p><p class="source-code">model.add(Dense(units=32, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.4))</p><p class="source-code">model.add(Dense(units=16, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.3))</p><p class="source-code">model.add(Dense(units=8, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.2))</p><p class="source-code">model.add(Dense(units=4, activation='relu', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code">model.add(Dropout(rate=0.1))</p><p class="source-code"># Add Output Dense Layer</p><p class="source-code">model.add(Dense(units=1, activation='sigmoid', \</p><p class="source-code">                kernel_initializer='uniform'))</p><p class="source-code"># Compile the Model</p><p class="source-code">model.compile(optimizer='adam', \</p><p class="source-code">              loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Next, fit the model to the training data by training for <strong class="source-inline">100</strong> epochs with <strong class="source-inline">batch_size=20</strong> and <strong class="source-inline">validation_split=0.2</strong>:<p class="source-code">model.fit(X_train, y_train, epochs=100, \</p><p class="source-code">          batch_size=20, verbose=1, \</p><p class="source-code">          validation_split=0.2, shuffle=False)</p></li>
				<li>Once the model has finished fitting to the <strong class="source-inline">training data</strong>, create a variable that is the result of the model's prediction on the <strong class="source-inline">test data</strong> using the model's <strong class="source-inline">predict</strong> and <strong class="source-inline">predict_proba</strong> methods:<p class="source-code">y_pred = model.predict(X_test)</p><p class="source-code">y_pred_prob = model.predict_proba(X_test)</p></li>
				<li>Next, compute the predicted class by setting the value of the prediction on the <strong class="source-inline">test set</strong> to <strong class="source-inline">1</strong> if the value is above <strong class="source-inline">0.5</strong> and <strong class="source-inline">0</strong> if it's below <strong class="source-inline">0.5</strong>. Compute the <strong class="source-inline">confusion matrix</strong> using the <strong class="source-inline">confusion_matrix</strong> function from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">y_pred_class1 = y_pred &gt; 0.5</p><p class="source-code">cm = confusion_matrix(y_test, y_pred_class1)</p><p class="source-code">print(cm)</p><p>The preceding code produces the following output:</p><p class="source-code">[[11730  58]</p><p class="source-code"> [   69 143]]</p><p>Always use <strong class="source-inline">y_test</strong> as the first parameter and <strong class="source-inline">y_pred_class1</strong> as the second parameter so that you always get the correct results.</p></li>
				<li>Calculate the true negative (<strong class="source-inline">TN</strong>), false negative (<strong class="source-inline">FN</strong>), false positive (<strong class="source-inline">FP</strong>), and true positive (<strong class="source-inline">TP</strong>):<p class="source-code"># True Negative</p><p class="source-code">TN = cm[0,0]</p><p class="source-code"># False Negative</p><p class="source-code">FN = cm[1,0]</p><p class="source-code"># False Positives</p><p class="source-code">FP = cm[0,1]</p><p class="source-code"># True Positives</p><p class="source-code">TP = cm[1,1]</p><p class="callout-heading">Note</p><p class="callout">Using <strong class="source-inline">y_test</strong> and <strong class="source-inline">y_pred_class1</strong> in that order is necessary because if they are used in reverse order, the matrix will still be computed without errors, but will be incorrect.</p></li>
				<li>Calculate the <strong class="source-inline">sensitivity</strong>:<p class="source-code"># Calculating Sensitivity</p><p class="source-code">Sensitivity = TP / (TP + FN)</p><p class="source-code">print(f'Sensitivity: {Sensitivity:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Sensitivity: 0.6745</p></li>
				<li>Calculate the <strong class="source-inline">specificity</strong>:<p class="source-code"># Calculating Specificity</p><p class="source-code">Specificity = TN / (TN + FP)</p><p class="source-code">print(f'Specificity: {Specificity:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Specificity: 0.9951</p></li>
				<li>Calculate the <strong class="source-inline">precision</strong>:<p class="source-code"># Precision</p><p class="source-code">Precision = TP / (TP + FP)</p><p class="source-code">print(f'Precision: {Precision:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Precision: 0.7114</p></li>
				<li>Calculate the <strong class="source-inline">false positive rate</strong>:<p class="source-code"># Calculate False positive rate</p><p class="source-code">False_Positive_rate = FP / (FP + TN)</p><p class="source-code">print(f'False positive rate: \</p><p class="source-code">      {False_Positive_rate:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">False positive rate: 0.0049</p><p>The following image shows the output of the values:</p><div id="_idContainer136" class="IMG---Figure"><img src="image/B15777_06_10.jpg" alt="Figure 6.10: Metrics summary&#13;&#10;"/></div><p class="figure-caption">Figure 6.10: Metrics summary</p><p class="callout-heading">Note</p><p class="callout">Sensitivity is inversely proportional to specificity.</p><p>As we discussed previously, our model should be more sensitive, but it looks more specific and less sensitive. So, how do we solve this? The answer lies in the threshold probabilities. The sensitivity of the model can be increased by adjusting the threshold value for classifying the dependent variable as <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong>. Recall that, originally, we set the value of <strong class="source-inline">y_pred_class1</strong> to greater than <strong class="source-inline">0.5</strong>. Let's change the threshold to <strong class="source-inline">0.3</strong> and rerun the code to check the results.</p></li>
				<li>Go to <em class="italic">step 7</em>, change the threshold from <strong class="source-inline">0.5</strong> to <strong class="source-inline">0.3</strong>, and rerun the code:<p class="source-code">y_pred_class2 = y_pred &gt; 0.3</p></li>
				<li>Now, create a <strong class="source-inline">confusion matrix</strong> and calculate the <strong class="source-inline">specificity</strong> and <strong class="source-inline">sensitivity</strong>:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">cm = confusion_matrix(y_test,y_pred_class2)</p><p class="source-code">print(cm)</p><p>The preceding code produces the following output:</p><p class="source-code">[[11700  88]</p><p class="source-code"> [   58 154]]</p><p>For comparison, the following is the previous <strong class="source-inline">confusion matrix</strong> with a <strong class="source-inline">threshold</strong> of <strong class="source-inline">0.5</strong>:</p><p class="source-code">[[11730  58]</p><p class="source-code"> [   69 143]]</p><p class="callout-heading">Note</p><p class="callout">Always remember that the original values of <strong class="source-inline">y_test</strong> should be passed as the first parameter and <strong class="source-inline">y_pred</strong> as the second parameter.</p></li>
				<li>Compute the various components of the <strong class="source-inline">confusion matrix</strong>:<p class="source-code"># True Negative</p><p class="source-code">TN = cm[0,0]</p><p class="source-code"># False Negative</p><p class="source-code">FN = cm[1,0]</p><p class="source-code"># False Positives</p><p class="source-code">FP = cm[0,1]</p><p class="source-code"># True Positives</p><p class="source-code">TP = cm[1,1]</p></li>
				<li>Calculate the new <strong class="source-inline">sensitivity</strong>:<p class="source-code"># Calculating Sensitivity</p><p class="source-code">Sensitivity = TP / (TP + FN)</p><p class="source-code">print(f'Sensitivity: {Sensitivity:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Sensitivity: 0.7264</p></li>
				<li>Calculate the <strong class="source-inline">specificity</strong>:<p class="source-code"># Calculating Specificity</p><p class="source-code">Specificity = TN / (TN + FP)</p><p class="source-code">print(f'Specificity: {Specificity:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Specificity: 0.9925</p><p>There is a clear increase in <strong class="source-inline">sensitivity</strong> and <strong class="source-inline">specificity</strong> after decreasing the threshold:</p><div id="_idContainer137" class="IMG---Figure"><img src="image/B15777_06_11.jpg" alt="Figure 6.11: Sensitivity and specificity comparison&#13;&#10;"/></div><p class="figure-caption">Figure 6.11: Sensitivity and specificity comparison</p><p>So, clearly, decreasing the threshold value increases the sensitivity.</p></li>
				<li>Visualize the data distribution. To understand why decreasing the threshold value increases the sensitivity, we need to see a histogram of our predicted probabilities. Recall that we created the <strong class="source-inline">y_pred_prob</strong> variable to predict the probabilities of the classifier:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code"># histogram of class distribution</p><p class="source-code">plt.hist(y_pred_prob, bins=100)</p><p class="source-code">plt.title("Histogram of Predicted Probabilities")</p><p class="source-code">plt.xlabel("Predicted Probabilities of APS failure")</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.show()</p><p>The following plot shows the output of the preceding code:</p></li>
			</ol>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B15777_06_12.jpg" alt="Figure 6.12: A histogram of the probabilities of patient readmission from the dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12: A histogram of the probabilities of patient readmission from the dataset</p>
			<p>This histogram clearly shows that most of the probabilities for the predicted classifier lie in a range from <strong class="source-inline">0.0</strong> to <strong class="source-inline">0.1</strong>, which is indeed very low. Unless we set the threshold very low, we cannot increase the sensitivity of the model. Also, note that sensitivity is inversely proportional to specificity, so when one increases, the other decreases.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31E6v32">https://packt.live/31E6v32</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gquh6y">https://packt.live/3gquh6y</a>.</p>
			<p>There is no universal value of the threshold, though the value of <strong class="source-inline">0.5</strong> is commonly used as a default. One method for selecting the threshold is to plot a histogram and then select the threshold manually. In our case, any threshold between <strong class="source-inline">0.1</strong> and <strong class="source-inline">0.7</strong> can be used as the model as there are few predictions between those values, as can be seen from the histogram that was produced at the end of the previous exercise. </p>
			<p>Another method for choosing the threshold is to plot the <strong class="source-inline">ROC curve</strong>, which plots the true positive rate as a function of the false positive rate. Depending on your tolerance for each, the threshold value can be selected. Plotting the <strong class="source-inline">ROC curve</strong> is also a good technique if we wish to evaluate the performance of the model because the area under the <strong class="source-inline">ROC curve</strong> is a direct measure of the model's performance. In the next activity, we will explore the performance of our model using the <strong class="source-inline">ROC curve</strong> and the <strong class="source-inline">AUC score</strong>.</p>
			<h2 id="_idParaDest-139">A<a id="_idTextAnchor140"/>ctivity 6.02: Calculating the ROC Curve and AUC Score</h2>
			<p>The <strong class="source-inline">ROC curve</strong> and <strong class="source-inline">AUC score</strong> is an effective way to easily evaluate the performance of a binary classifier. In this activity, we will plot the <strong class="source-inline">ROC curve</strong> and calculate the <strong class="source-inline">AUC score</strong> of a model. We will use the same dataset and train the same model that we used in <em class="italic">Exercise 6.03</em>, <em class="italic">Deriving and Computing Metrics Based on a Confusion Matrix</em>. Use the APS failure data and calculate the <strong class="source-inline">ROC curve</strong> and <strong class="source-inline">AUC score</strong>. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the necessary dependencies and load the dataset.</li>
				<li>Split the data into training and test datasets using the <strong class="source-inline">train_test_split</strong> function. </li>
				<li>Scale the training and test data using the <strong class="source-inline">StandardScaler</strong> function.</li>
				<li>Import the libraries that are required to build a neural network architecture and initiate the <strong class="source-inline">Sequential</strong> class. Add five <strong class="source-inline">Dense</strong> layers with <strong class="source-inline">Dropout</strong>. Set the first hidden layer so that it has a size of <strong class="source-inline">64</strong> with a dropout rate of <strong class="source-inline">0.5</strong>, the second hidden layer so that it has a size of <strong class="source-inline">32</strong> with a dropout rate of <strong class="source-inline">0.4</strong>, the third hidden layer so that it has a size of <strong class="source-inline">16</strong> with a dropout rate of <strong class="source-inline">0.3</strong>, the fourth hidden layer so that it has a size of <strong class="source-inline">8</strong> with a dropout rate of <strong class="source-inline">0.2</strong>, and the final hidden layer so that it has a size of <strong class="source-inline">4</strong>, with a dropout rate of <strong class="source-inline">0.1</strong>. Set all the activation functions to <strong class="source-inline">ReLU</strong>. </li>
				<li>Add an output <strong class="source-inline">Dense</strong> layer with the <strong class="source-inline">sigmoid</strong> activation function. Compile the network then fit the model using accuracy. Fit the model with <strong class="source-inline">100</strong> epochs and a batch size of <strong class="source-inline">20</strong>.</li>
				<li>Fit the model to the training data, saving the results from the fit process.</li>
				<li>Create a variable representing the predicted classes of the test dataset.</li>
				<li>Calculate the false positive rate and true positive rate using the <strong class="source-inline">roc_curve</strong> function from <strong class="source-inline">sklearn.metrics</strong>. The false positive rate and true positive rate are the first and second of three return variables. Pass the true values and the predicted values to the function.</li>
				<li>Plot the ROC curve, which is the true positive rate as a function of the false positive rate.</li>
				<li>Calculate the AUC score using the <strong class="source-inline">roc_auc_score</strong> from <strong class="source-inline">sklearn.metrics</strong> while passing the true values and predicted values of the model.</li>
			</ol>
			<p>After implementing these steps, you should get the following output:</p>
			<p class="source-code">0.944787151628455</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 434.</p>
			<p>In this activity, we learned how to calculate a <strong class="source-inline">ROC</strong> and an <strong class="source-inline">AUC score</strong> with the APS failure dataset. We also learned how specificity and sensitivity change with different threshold values.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/>Summary</h1>
			<p>In this chapter, we covered model evaluation and accuracy in depth. We learned how accuracy is not the most appropriate technique for evaluation when our dataset is imbalanced. We also learned how to compute a confusion matrix using scikit-learn and how to derive other metrics, such as sensitivity, specificity, precision, and false positive rate.</p>
			<p>Finally, we understood how to use threshold values to adjust metrics and how <strong class="source-inline">ROC curves</strong> and <strong class="source-inline">AUC scores</strong> help us evaluate our models. It is very common to deal with imbalanced datasets in real-life problems. Problems such as credit card fraud detection, disease prediction, and spam email detection all have imbalanced data in different proportions.</p>
			<p>In the next chapter, we will learn about a different kind of neural network architecture (convolutional neural networks) that performs well on image classification tasks. We will test performance by classifying images into two classes and experiment with different architectures and activation functions.</p>
		</div>
		<div>
			<div id="_idContainer140" class="Basic-Text-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer141" class="Basic-Text-Frame">
			</div>
		</div>
	</body></html>