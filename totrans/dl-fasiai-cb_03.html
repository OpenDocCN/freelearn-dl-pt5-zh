<html><head></head><body>
		<div id="_idContainer109">
			<h1 id="_idParaDest-81"><em class="italic"><a id="_idTextAnchor083"/>Chapter 3</em>: Training Models with Tabular Data</h1>
			<p>In the previous chapter, we learned how to ingest various kinds of datasets using fastai and how to clean up datasets. In this chapter, we are going to get into the details of training a model with fastai using tabular data. <strong class="bold">Tabular data</strong>, which is data organized in rows and columns that you would find in a spreadsheet file or a database table, is critical to most businesses. The fastai framework acknowledges the importance of tabular data by providing a full suite of features to support deep learning applications based on tabular data.</p>
			<p>To explore deep learning with tabular data in fastai, we will return to the <strong class="source-inline">ADULT_SAMPLE</strong> dataset, one of the datasets we examined in <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a><em class="italic">, Exploring and Cleaning Up Data with fastai</em>. By using this dataset, we will train a deep learning model, while also learning about the <strong class="source-inline">TabularDataLoaders</strong> (used to define the training and test datasets) and <strong class="source-inline">tabular_learner</strong> (used to define and train the model) objects.</p>
			<p>We will also look at datasets outside the set of curated datasets to learn how we can ingest non-curated datasets to train deep learning models in fastai. We will wrap up this chapter by exploring what makes a tabular dataset a decent candidate for training a fastai deep learning model and how to save a trained model.</p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Training a model in fastai with a curated tabular dataset</li>
				<li>Training a model in fastai with a non-curated tabular dataset</li>
				<li>Training a model with a standalone dataset</li>
				<li>Assessing whether a tabular dataset is a good candidate for fastai</li>
				<li>Saving a trained tabular model</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>Technical requirements</h1>
			<p>Ensure that you have completed the setup sections in <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, and have a working Gradient instance or Colab setup. Ensure that you have cloned the repository for this book (<a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook</a>) and have access to the <strong class="source-inline">ch3</strong> folder. This folder contains the code samples described in this chapter.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor085"/>Training a model in fastai with a curated tabular dataset</h1>
			<p>In <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a>, <em class="italic">Exploring and Cleaning Up Data with fastai</em>, you learned how to ingest and examine the <strong class="source-inline">ADULT_SAMPLE</strong> curated tabular dataset. In this recipe, we will go through the process of training a <a id="_idIndexMarker163"/>deep learning model on this dataset using fastai. This will give you an <a id="_idIndexMarker164"/>overview of the <em class="italic">happy path</em> to creating a tabular deep learning model with fastai. The goal of this recipe is to use this dataset to train a <a id="_idIndexMarker165"/>deep learning model with fastai, which predicts whether the person described in a particular record will have a salary above or below 50k.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor086"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_tabular_datasets.ipynb</strong> notebook in the <strong class="source-inline">ch3</strong> directory of your repository.</p>
			<p>I am grateful for the opportunity to include the ADULT_SAMPLE dataset featured in this section.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Ron Kohavi. (1996) <em class="italic">Scaling Up the Accuracy of Naive-Bayes Classifers: a Decision-Tree Hybrid  </em>(<a href="http://robotics.stanford.edu/~ronnyk/nbtree.pdf">http://robotics.stanford.edu/~ronnyk/nbtree.pdf</a>). </p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor087"/>How to do it…</h2>
			<p>In this recipe, you will be running through the <strong class="source-inline">training_with_tabular_datasets.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li>Run the cells in the notebook up to the <strong class="source-inline">Define transforms</strong>,<strong class="source-inline"> dependent variable</strong>,<strong class="source-inline"> continuous and categorical columns</strong> cell. By running these cells, you will be setting up the notebook and ingesting the <strong class="source-inline">ADULT_SAMPLE</strong> curated tabular dataset into a pandas DataFrame, which you will use through the rest of this notebook. These cells are identical to the ones in the notebook shown in <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a>, <em class="italic">Exploring and Cleaning Up Data with fastai</em>, for examining tabular curated datasets <strong class="source-inline">examining_tabular_datasets.ipynb</strong>.</li>
				<li>Run the first new cell in the notebook with the following code:<p class="source-code">procs = [FillMissing,Categorify]</p><p class="source-code">dep_var = 'salary'</p><p class="source-code">cont,cat = cont_cat_split(df, 1, dep_var=dep_var)</p><p>This cell sets the following values:</p><p>a) <strong class="source-inline">procs</strong>: This is a list of the <a id="_idIndexMarker166"/>transformations that will be applied in the <strong class="source-inline">TabularDataLoaders</strong> object. <strong class="source-inline">FillMissing</strong> specifies <a id="_idIndexMarker167"/>that missing the values in a column will be <a id="_idIndexMarker168"/>replaced with the median value for the column. <strong class="source-inline">Categorify</strong> specifies that the values in categorical columns will be replaced with numeric identifiers.</p><p>b) <strong class="source-inline">dep_var</strong>: This will be used to identify which column in the dataset contains the dependent variable. This column is also known as the target value or the y value for the model – it is the value that the trained model will predict. For this model, we are predicting the value for the <strong class="source-inline">salary</strong> column.</p><p>c) <strong class="source-inline">cont</strong> and <strong class="source-inline">cat</strong>: These are lists of continuous and categorical columns from the <strong class="source-inline">df</strong> DataFrame that are returned by <strong class="source-inline">cont_cat_split</strong>, respectively. This function is a major benefit that fastai provides for tabular deep learning models. It saves a lot of repetitive coding by automatically detecting which columns are continuous (that is, can take on an unlimited set of values, such as currency amounts, physical dimensions, or counts of objects) or categorical (that is, can only take a finite set of distinct values, such as states of the US or days of the week).</p></li>
				<li>Run the next cell with the following code to define the <strong class="source-inline">TabularDataLoaders</strong> object called <strong class="source-inline">dls</strong>. Note that some of the parameters specified in the definition of this object (such as batch size) are usually associated with the training <a id="_idIndexMarker169"/>process rather than defining the training dataset:<p class="source-code">dls=TabularDataLoaders.from_df(</p><p class="source-code">df, path, procs= procs,</p><p class="source-code">cat_names= cat, cont_names = cont, </p><p class="source-code">y_names = dep_var, </p><p class="source-code">valid_idx=list(range(1024,1260)), bs=64)</p><p>The <a id="_idIndexMarker170"/>definition of the <strong class="source-inline">TabularDataLoaders</strong> object uses the following arguments:</p><p>a) <strong class="source-inline">df, path, procs</strong>: The DataFrame containing the ingested dataset, the path object for the dataset, and the list of <a id="_idIndexMarker171"/>transformations defined in the previous step.</p><p>b) <strong class="source-inline">cat_names, cont_names</strong>: The lists of categorical and continuous columns defined in the previous step.</p><p>c) <strong class="source-inline">y_names</strong>: The column containing the dependent variable/target values.</p><p>d) <strong class="source-inline">valid_idx</strong>: The index values of the subset of rows of the <strong class="source-inline">df</strong> DataFrame that will be reserved as the validation dataset for the training process.</p><p>e) <strong class="source-inline">bs</strong>: Batch size for the training process.</p></li>
				<li>Run the cell with the following code to define and train the model. The first line specifies that the model is being defined using the <strong class="source-inline">TabularDataLoaders</strong> object, which was defined in the previous step with the default number of layers, and using accuracy as the metric that was optimized in the training process. The second line triggers the training process for three epochs; that is, three iterations through the entire dataset:<p class="source-code">learn = tabular_learner(dls,layers=[200,100], metrics=accuracy)</p><p class="source-code">learn.fit_one_cycle(3)</p><p>The output of this <a id="_idIndexMarker172"/>cell will be the <a id="_idIndexMarker173"/>training results by epoch. The results include the epoch's number, the training loss, the validation loss, and the elapsed time for each epoch:</p><div id="_idContainer078" class="IMG---Figure"><img src="image/B16216_03_01.jpg" alt="Figure 3.1 – Training results&#13;&#10;"/></div><p class="figure-caption">Figure 3.1 – Training results</p></li>
				<li>Run the following cell to <a id="_idIndexMarker174"/>get a sample result of the trained model's predictions. You can compare the <strong class="source-inline">salary</strong> column with the <strong class="source-inline">salary_pred</strong> column (the prediction made by the model – highlighted in the preceding screenshot) to get a snapshot of how the model performed for this sample of rows from the dataset. In this sample set, the model's predictions match the actual values of the dependent variable in the <strong class="source-inline">salary</strong> column:<div id="_idContainer079" class="IMG---Figure"><img src="image/B16216_03_02.jpg" alt="Figure 3.2 – Output of show_results&#13;&#10;"/></div><p class="figure-caption">Figure 3.2 – Output of show_results</p></li>
				<li>Run the following cell to get a <a id="_idIndexMarker175"/>summary of the structure of the trained model:<p class="source-code">learn.summary()</p><p>The output of this <a id="_idIndexMarker176"/>cell includes the <a id="_idIndexMarker177"/>following details:</p><p>a) A list of all the layers that make up the model:</p></li>
			</ol>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B16216_03_03.jpg" alt="Figure 3.3 – List of layers that make up the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – List of layers that make up the model</p>
			<p>b) The parameters in the <a id="_idIndexMarker178"/>trained model, the optimizer and loss functions, and the callbacks used. Callbacks (<a href="https://docs.fast.ai/callback.core.html">https://docs.fast.ai/callback.core.html</a>) specify actions to be taken <a id="_idIndexMarker179"/>during the training process, such as stopping the <a id="_idIndexMarker180"/>training process prior to executing all the epochs. In the case of this trained model, callbacks are automatically specified by fastai to track the number of <a id="_idIndexMarker181"/>epochs done (<strong class="source-inline">TrainEvalCallback</strong>), track losses and metrics by batch/epoch (<strong class="source-inline">Recorder</strong>), and display the progress bars shown while the model is being trained (<strong class="source-inline">ProgressCallBack</strong>):</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B16216_03_04.jpg" alt="Figure 3.4 – Additional details in the output of summary()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Additional details in the output of summary()</p>
			<p>You have now completely trained a deep learning model with a curated tabular dataset using fastai.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor088"/>How it works…</h2>
			<p>As you saw in this recipe, once the <a id="_idIndexMarker182"/>data has been ingested, it only takes a few lines of code with fastai to get a trained deep learning model. The simplicity and compactness of fastai code is partially down to fastai making reasonable default choices when possible.</p>
			<p>For example, fastai determines <a id="_idIndexMarker183"/>that when the target column is categorical, the model should be predicting choices in a category (in the case of this model, a binary choice of whether a person's salary is above or below 50 k) rather than continuous values. The following are additional benefits <a id="_idIndexMarker184"/>provided by fastai for training deep learning models with tabular data:</p>
			<ul>
				<li>Detecting which columns in a DataFrame are categorical or continuous</li>
				<li>Selecting appropriate callbacks for the training process</li>
				<li>Defining the layers for the deep learning model (including embedding layers for the categorical columns)</li>
			</ul>
			<p>Let's compare what we <a id="_idIndexMarker185"/>would have to do for a Keras deep learning model. In a Keras deep learning model for a tabular dataset, each of these characteristics of the model would have to be <a id="_idIndexMarker186"/>explicitly coded, resulting in longer, more complex code to create the model. In addition, somebody learning about deep learning models for <a id="_idIndexMarker187"/>tabular datasets would face a bigger challenge by needing to deal with many more details to get their first working trained model. The bottom line is that fastai makes it possible to get to a basic deep learning model faster.</p>
			<p>Before moving on to the next recipe, it's worth digging into the model from this recipe because the other recipes in this chapter will follow the same pattern. A deeply detailed description of the model is beyond the scope of this book, so we will just focus on some highlights here. </p>
			<p>As shown in the recipe, the model is defined as a <strong class="source-inline">tabular_learner</strong> object (documentation here: <a href="https://docs.fast.ai/tabular.learner.html">https://docs.fast.ai/tabular.learner.html</a>). This object is a specialization of the fastai <strong class="source-inline">learner</strong> object which you first saw in the <em class="italic">Understanding the world in four applications: tables, text, recommender systems, and images</em> section of <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with fastai</em>. You can see the structure of this model from the output of <strong class="source-inline">learn.summary()</strong>. The beginning of the output specifies <strong class="source-inline">Input shape: ['64 x 9', '64 x 6']</strong> - the second dimension of the first element corresponds with the number of categorical columns, and the second dimension of the second element corresponds with the number of continuous columns. There are embedding layers (documentation here: <a href="https://docs.fast.ai/layers.html#Embeddings">https://docs.fast.ai/layers.html#Embeddings</a>) defined for the categorical columns (the columns in the <strong class="source-inline">cat</strong> list) and a <strong class="source-inline">BatchNorm</strong> layer (documentation here: <a href="https://docs.fast.ai/layers.html#BatchNorm-layers">https://docs.fast.ai/layers.html#BatchNorm-layers</a>) for the continuous columns. </p>
			<p>To see additional details about the structure of the model, see the output of <strong class="source-inline">learn.model</strong>. In particular, you can see that for the final <strong class="source-inline">Linear</strong> layer, <strong class="source-inline">out_features = 2</strong>, corresponds with the binary output of the model (the individual's income is above/below 50 k).</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor089"/>Training a model in fastai with a non-curated tabular dataset</h1>
			<p>In <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a>, <em class="italic">Exploring and Cleaning Up Data with fastai</em>, you reviewed the curated datasets provided by fastai. In the <a id="_idIndexMarker188"/>previous recipe, you created a deep learning model that had been trained on one of these <a id="_idIndexMarker189"/>curated datasets. What if <a id="_idIndexMarker190"/>you want to train a fastai model for a tabular dataset that is not one of these curated datasets? </p>
			<p>In this recipe, we will go <a id="_idIndexMarker191"/>through the process of ingesting a non-curated dataset – the Kaggle house prices dataset (<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>) – and training a deep learning model on it. This dataset presents some additional challenges. Compared to a curated fastai dataset, there are additional steps required to ingest the dataset, and its structure requires special handling to deal with missing values. </p>
			<p>The goal of this recipe is to use this dataset to train a deep learning model, that then predicts whether a house has a sale price that is above or below the average price for the dataset.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor090"/>Getting ready</h2>
			<p>To complete this recipe you will <a id="_idIndexMarker192"/>need a Kaggle ID. If you don't already have one, you can get one here: <a href="https://www.kaggle.com/account/login">https://www.kaggle.com/account/login</a>. Once you have your Kaggle ID, complete the <a id="_idIndexMarker193"/>following steps to get the token for accessing the Kaggle house prices dataset from within your <a id="_idIndexMarker194"/>notebook:</p>
			<ol>
				<li value="1">Log in with  your Kaggle ID, click on <a id="_idIndexMarker195"/>your account (top right), and then click on <strong class="bold">Account</strong>:<div id="_idContainer082" class="IMG---Figure"><img src="image/B16216_03_05.jpg" alt="Figure 3.5 – Kaggle account menu&#13;&#10;"/></div><p class="figure-caption">Figure 3.5 – Kaggle account menu</p></li>
				<li>On the <strong class="bold">Account</strong> page, scroll down to the API section and click on <strong class="bold">Create New API Token</strong>. The <strong class="source-inline">kaggle.json</strong> file will be downloaded on your local system:<div id="_idContainer083" class="IMG---Figure"><img src="image/B16216_03_06.jpg" alt="Figure 3.6 – Selecting Create New API Token to get a new Kaggle API token&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – Selecting Create New API Token to get a new Kaggle API token</p></li>
				<li>In your Gradient <a id="_idIndexMarker196"/>environment, open a Terminal session, go to the <strong class="source-inline">/root</strong> directory, and create a new <a id="_idIndexMarker197"/>directory called <strong class="source-inline">.kaggle</strong>.</li>
				<li>Upload the <strong class="source-inline">kaggle.json</strong> file <a id="_idIndexMarker198"/>that you downloaded in <em class="italic">Step 2</em> to the new directory that you just created; that is, <strong class="source-inline">/root/.kaggle</strong>.</li>
			</ol>
			<p>These steps will prepare you to use the <a id="_idIndexMarker199"/>Kaggle house price dataset (<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>) in this recipe.</p>
			<p>I am grateful for the opportunity to include the house price dataset featured in this section. </p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Dean De Cock (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project (<a href="http://jse.amstat.org/v19n3/decock.pdf">http://jse.amstat.org/v19n3/decock.pdf)</a> Journal of Statistics Education Volume 19, Number 3(2011), (<a href="http://www.amstat.org/publications/jse/v19n3/decock.pdf">www.amstat.org/publications/jse/v19n3/decock.pdf</a>)</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor091"/>How to do it…</h2>
			<p>In this recipe, you will be running through the <strong class="source-inline">accessing_non_curated_datasets.ipynb</strong> notebook, as well as the fastai dataset documentation, to understand the datasets that fastai curates. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">If you have not already done so, install the <strong class="source-inline">kaggle</strong> library by running the following command:<p class="source-code"><strong class="bold">pip install kaggle</strong></p></li>
				<li>Run the first three cells of the notebook to load the libraries that you will need for this recipe and prepare the notebook for fastai.</li>
				<li>In your Gradient <a id="_idIndexMarker200"/>environment, go to the <strong class="source-inline">/root/.kaggle</strong> directory and open the <strong class="source-inline">kaggle.json</strong> file. The contents of this file should look like this, with your ID and your 32-character key as the first and second values, respectively:<p class="source-code">{"username":&lt;YOUR ID&gt;,"key":&lt;YOUR KEY&gt;}</p></li>
				<li>Copy the contents of your <strong class="source-inline">kaggle.json</strong> file. </li>
				<li>In your copy of the <strong class="source-inline">accessing_non_curated_datasets.ipynb</strong> notebook, paste <a id="_idIndexMarker201"/>the content of the <strong class="source-inline">kaggle.json</strong> file into single quotes to assign the value to the variable creds. Then, run the cell:<p class="source-code">creds = '{"username":&lt;YOUR ID&gt;,"key":&lt;YOUR KEY&gt;}'</p></li>
				<li>Run this cell to set the <a id="_idIndexMarker202"/>credential path for the dataset:<p class="source-code">cred_path = Path('~/.kaggle/kaggle.json').expanduser()</p></li>
				<li>Run this cell to set the path for your dataset:<p class="source-code">path = URLs.path('house_price')</p></li>
				<li>Run this cell to create the target directory for the dataset, download the dataset, unzip the dataset in the target directory, and list the contents of the target directory:<p class="source-code">if not path.exists():</p><p class="source-code">    path.mkdir()</p><p class="source-code">    api.competition_download_cli('house-prices-advanced-regression-techniques', path=path)</p><p class="source-code">    file_extract(path/'house-prices-advanced-regression-techniques.zip')</p><p class="source-code">    path.ls(file_type='text')</p></li>
				<li>The output of the <strong class="source-inline">path.ls()</strong> function shows the structure of the dataset. In this recipe, we will use <strong class="source-inline">train.csv</strong> to train the <a id="_idIndexMarker203"/>deep learning model and then use <strong class="source-inline">test.csv</strong> to exercise the trained model:<p class="source-code">[Path('/storage/archive/house_price/sample_submission.csv'),</p><p class="source-code">Path('/storage/archive/house_price/data_description.txt'),</p><p class="source-code">Path('/storage/archive/house_price/train.csv'),</p><p class="source-code">Path('/storage/archive/house_price/test.csv')]</p></li>
				<li>Run this cell to <a id="_idIndexMarker204"/>ingest the <strong class="source-inline">train.csv</strong> file into a pandas DataFrame called <strong class="source-inline">df_train</strong>:<p class="source-code">df_train = pd.read_csv(path/'train.csv')</p></li>
				<li>Run this cell to <a id="_idIndexMarker205"/>ingest the <strong class="source-inline">test.csv</strong> file into a pandas DataFrame called <strong class="source-inline">df_test</strong>:<p class="source-code">df_test = pd.read_csv(path/'test.csv')</p></li>
				<li>Run the <strong class="source-inline">shape</strong> command to get the dimensions of <strong class="source-inline">df_train</strong> and <strong class="source-inline">df_test</strong>. Notice that <strong class="source-inline">df_test</strong> has one fewer columns than <strong class="source-inline">df_train</strong> – can you think of why this would be the case and which column is missing from <strong class="source-inline">df_test</strong>?</li>
				<li>Run this cell to define the <strong class="source-inline">under_over()</strong> function, which returns <strong class="source-inline">'0'</strong> if the input value is less than the mean and <strong class="source-inline">'1'</strong> if not:<p class="source-code">def under_over(x,mean_x):</p><p class="source-code">    if (x &lt;= mean_x):</p><p class="source-code">        returner = '0'</p><p class="source-code">    else:</p><p class="source-code">        returner = '1'</p><p class="source-code">    return(returner)</p></li>
				<li>Run this cell to use the <strong class="source-inline">under_over()</strong> function, which will replace the values in the <strong class="source-inline">SalePrice</strong> column with <a id="_idIndexMarker206"/>indicators of <a id="_idIndexMarker207"/>whether the value was above or below the average for the column:<p class="source-code">mean_sp = int(df_train['SalePrice'].mean())</p><p class="source-code">df_train['SalePrice'] = df_train['SalePrice'].apply(lambda x: under_over(x,mean_sp))</p><p class="source-code">df_train.head()</p></li>
				<li>When you <a id="_idIndexMarker208"/>display the contents of the <strong class="source-inline">df_train</strong> DataFrame, you will see that the values in the <strong class="source-inline">SalePrice</strong> column have been replaced with zeros and ones:<div id="_idContainer084" class="IMG---Figure"><img src="image/B16216_03_07.jpg" alt="Figure 3.7 – The values in the SalePrice column have been replaced&#13;&#10;"/></div><p class="figure-caption">Figure 3.7 – The values in the SalePrice column have been replaced</p></li>
				<li>Run this cell to see the count of each of the new values in the <strong class="source-inline">SalePrice</strong> column:<p class="source-code">df_train['SalePrice'].value_counts()</p></li>
				<li>Run this cell to define the <a id="_idIndexMarker209"/>transformation to apply to the dataset, the column that contains the dependent variable (target), and the continuous and categorical columns:<p class="source-code">dep_var = 'SalePrice'</p><p class="source-code">cont,cat = cont_cat_split(df_train, 1, dep_var=dep_var)</p><p>This cell sets the following values:</p><p>a) <strong class="source-inline">procs</strong>: This is a list of the <a id="_idIndexMarker210"/>transformations that will be applied to the <strong class="source-inline">TabularDataLoaders</strong> object. <strong class="source-inline">FillMissing</strong> specifies that the missing values in a column are replaced with the median value for the column. <strong class="source-inline">Categorify</strong> specifies that the values in the categorical columns are replaced with numeric identifiers.</p><p>b) <strong class="source-inline">dep_var</strong>: This will be used to <a id="_idIndexMarker211"/>identify which column in the dataset contains the dependent variable; that is, the column that contains the value that we want the model to predict. For this model, we are predicting the value for the <strong class="source-inline">SalePrice</strong> column.</p><p>c) <strong class="source-inline">cont</strong> and <strong class="source-inline">cat</strong>: These are lists of continuous and categorical columns from the <strong class="source-inline">df_train</strong> DataFrame returned by using <strong class="source-inline">cont_cat_split</strong>. </p></li>
				<li>Run this cell to check for missing values in the <strong class="source-inline">df_train</strong> DataFrame. The first line gets a count of missing values in the DataFrame, while the second line defines a new DataFrame, <strong class="source-inline">df_train_missing</strong>, that has a row for each column in the original DataFrame that has at least one missing value. The columns of this DataFrame are the names of the columns with missing values, the missing value count for each column, and the proportion of values that are missing in the column:<p class="source-code">count = df_train.isna().sum()</p><p class="source-code">df_train_missing = (pd.concat([count.rename('missing_count'),count.div(len(df_train)).rename('missing_ratio')],axis = 1).loc[count.ne(0)])</p><p>Looking at the values in <strong class="source-inline">df_train_missing</strong>, we can <a id="_idIndexMarker212"/>see that some columns have a large number of missing values:</p><div id="_idContainer085" class="IMG---Figure"><img src="image/B16216_03_08.jpg" alt="Figure 3.8 – Rows from df_train_missing&#13;&#10;"/></div><p class="figure-caption">Figure 3.8 – Rows from df_train_missing</p></li>
				<li>Run this cell to deal with the <a id="_idIndexMarker213"/>missing values in the <strong class="source-inline">df_train</strong> and <strong class="source-inline">df_test</strong> DataFrames. The first two statements <a id="_idIndexMarker214"/>replace the missing values in the categorical columns with the most common non-missing value, while the second two statements replace the missing values in the continuous columns with zeros:<p class="source-code">df_train[cat] = df_train[cat].fillna(df_train[cat].mode().iloc[0])</p><p class="source-code">df_test[cat] = df_test[cat].fillna(df_test[cat].mode().iloc[0])</p><p class="source-code">df_train[cont] = df_train[cont].fillna(0.0)</p><p class="source-code">df_test[cont] = df_test[cont].fillna(0.0)</p></li>
				<li>Run this cell to check for <a id="_idIndexMarker215"/>missing values in the <strong class="source-inline">df_train</strong> DataFrame once more:<p class="source-code">count = df_train.isna().sum()</p><p class="source-code">df_train_missing = (pd.concat([count.rename('missing_count'),count.div(len(df_train)).rename('missing_ratio')],axis = 1).loc[count.ne(0)])</p></li>
				<li>Now, when you check <a id="_idIndexMarker216"/>the contents of the <strong class="source-inline">df_train_missing</strong> DataFrame, it will be empty, confirming that <a id="_idIndexMarker217"/>all the missing values have been dealt with:<div id="_idContainer086" class="IMG---Figure"><img src="image/B16216_03_09.jpg" alt="Figure 3.9 – Confirmation that the missing values have been dealt with&#13;&#10;"/></div><p class="figure-caption">Figure 3.9 – Confirmation that the missing values have been dealt with</p></li>
				<li>Run this cell to <a id="_idIndexMarker218"/>create a <strong class="source-inline">TabularDataLoaders</strong> object. The first line defines the transformation procedures to be applied in the <strong class="source-inline">TabularDataLoaders</strong> object, while the second line defines the <strong class="source-inline">TabularDataLoaders</strong> object:<p class="source-code">procs = [Categorify, Normalize]</p><p class="source-code">dls_house=TabularDataLoaders.from_df(</p><p class="source-code">    df_train,path,procs= procs,</p><p class="source-code">    cat_names= cat, cont_names = cont, y_names = dep_var, </p><p class="source-code">    valid_idx=list(range((df_train.shape[0]-100),df_train.shape[0])), </p><p class="source-code">    bs=64)</p><p>Here are the arguments for the definition of the <strong class="source-inline">TabularDataLoaders</strong> object:</p><p>a) <strong class="source-inline">procs</strong>: This is a list of the <a id="_idIndexMarker219"/>transformations that will be applied to the <strong class="source-inline">TabularDataLoaders</strong> object. <strong class="source-inline">Normalize</strong> specifies that the values are all scaled to a consistent range. <strong class="source-inline">Categorify</strong> specifies that the values in the categorical columns are replaced with numeric identifiers.</p><p>b) <strong class="source-inline">df_train, path, procs</strong>: The DataFrame containing the ingested dataset, the path object for the dataset, and the list of transformations defined in the previous step.</p><p>c) <strong class="source-inline">cat_names, cont_names</strong>: The lists of categorical and continuous columns.</p><p>d) <strong class="source-inline">y_names</strong>: The column <a id="_idIndexMarker220"/>containing the dependent variable/target values.</p><p>e) <strong class="source-inline">valid_idx</strong>: The index values of the subset of rows of the <strong class="source-inline">df</strong> DataFrame that will be reserved as the validation dataset for the training process.</p><p>f) <strong class="source-inline">bs</strong>: The batch size for the training process.</p></li>
				<li>Run this cell to define and train the deep learning model. The first line specifies that the model is being <a id="_idIndexMarker221"/>created using the <strong class="source-inline">TabularDataLoaders</strong> object we defined in the previous step, with the default number of layers and accuracy as the metrics being optimized in the <a id="_idIndexMarker222"/>training process. The second line triggers the training process for <strong class="source-inline">5</strong> epochs:<p class="source-code">learn = tabular_learner(dls_house, layers=[200,100], metrics=accuracy)</p><p class="source-code">learn.fit_one_cycle(5)</p><p>The training process produces an output that shows the training loss, validation loss, and accuracy for each epoch. This means we have trained a model that can predict whether the cost of a <a id="_idIndexMarker223"/>given property from the validation set is above or below the average with 92% accuracy. Can you think of some reasons why the accuracy of this model is somewhat better than the accuracy of the model you trained in the previous recipe?</p><div id="_idContainer087" class="IMG---Figure"><img src="image/B16216_03_10.jpg" alt="Figure 3.10 – Results of training on the non-curated dataset&#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – Results of training on the non-curated dataset</p></li>
				<li>You can run this cell to <a id="_idIndexMarker224"/>apply the trained model to the test dataset. Note that because of the structure of this dataset, the test dataset does not include y-dependent values, which means that while you can <a id="_idIndexMarker225"/>apply the model to the test records, you don't have any way of assessing the accuracy of the predictions that the model makes on the test set:<p class="source-code">dl = learn.dls.test_dl(df_test)</p></li>
				<li>Run this cell to get a sample of predictions that the trained model makes on the test dataset:<p class="source-code">learn.show_results()</p><p>The output of <strong class="source-inline">learn.show_results()</strong> lets you see the result of applying the trained model to the dataset:</p></li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B16216_03_11.jpg" alt="Figure 3.11 – The subset of results from applying the model trained to a non-curated dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – The subset of results from applying the model trained to a non-curated dataset</p>
			<p>You have now gone through the <a id="_idIndexMarker226"/>process of using a non-curated dataset to train a fastai deep learning model.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor092"/>How it works…</h2>
			<p>In this recipe, you learned how to <a id="_idIndexMarker227"/>adapt to a different kind of dataset where the train and test data is separate. In this case, you cannot rely <a id="_idIndexMarker228"/>on the transformations in the <strong class="source-inline">TabularDataLoaders</strong> object to deal with missing data. That's why <a id="_idIndexMarker229"/>the code associated with this recipe deals with the missing values in each of the train and test datasets individually.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor093"/>Training a model with a standalone dataset</h1>
			<p>In the previous recipes in this chapter, we <a id="_idIndexMarker230"/>looked at training fastai models on a curated tabular dataset and a dataset directly loaded from Kaggle. In this recipe, we are going to examine how to train a model with a dataset that is from a self-standing file. The <a id="_idIndexMarker231"/>dataset we will use in this recipe is made up of property listings in Kuala Lumpur, Malaysia and is available from the Kaggle site at <a href="https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur">https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur</a>.</p>
			<p>This dataset is not like the tabular datasets we have seen so far. The datasets we have already encountered have been well-behaved and have only required a small amount of cleanup. The Kualu Lumpur property dataset, by contrast, is a real-world dataset. In addition to missing values, it contains many errors and irregularities. It is also large enough (over 50k records) to give deep learning a decent chance to be useful on it.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>Getting ready</h2>
			<p>Ensure you have followed the steps in <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, so that you have a fastai environment set up. Confirm that you can open the <strong class="source-inline">training_model_standalone_tabular_dataset.ipynb</strong> notebook in the <strong class="source-inline">ch3</strong> directory of your repository. Also, ensure that you have uploaded the data file by following these steps:</p>
			<ol>
				<li value="1"> Download <strong class="source-inline">data_kaggle.csv.zip</strong> from <a href="https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur">https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur</a>.</li>
				<li>Unzip the downloaded file to extract <strong class="source-inline">data_kaggle.csv</strong>.</li>
				<li>From the Terminal in your Gradient environment, make your current directory <strong class="source-inline">/storage/archive</strong>:<p class="source-code"><strong class="bold">cd /storage/archive</strong></p></li>
				<li>Create a folder called <strong class="source-inline">/storage/archive/kl_property:</strong><p class="source-code"><strong class="bold">mkdir kl_property</strong></p></li>
				<li>Upload <strong class="source-inline">data_kaggle.csv</strong> to <strong class="source-inline">/storage/archive/kl_property</strong>. You can use the upload button in JupyterLab in Gradient to do the upload, but you need to do so by <a id="_idIndexMarker232"/>performing several steps:<p>a) From the Terminal in your <a id="_idIndexMarker233"/>Gradient environment, make <strong class="source-inline">/notebooks</strong> your current directory:</p><p class="source-code"><strong class="bold">cd /notebooks</strong></p><p>b) Make a new directory called <strong class="source-inline">/notebooks/temp</strong>:</p><p class="source-code"><strong class="bold">mkdir temp</strong></p><p>c) In the JupyterLab file browser, make <strong class="source-inline">temp</strong> your current folder, select the upload b<a id="_idTextAnchor095"/>utton, as shown in the following screenshot, and select the <strong class="source-inline">data_kaggle.csv</strong> file from your local system folder where you extracted it in <em class="italic">step 2</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B16216_03_12.jpg" alt="Figure 3.12 – Upload button in JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Upload button in JupyterLab</p>
			<p>d) From the Termin<a id="_idTextAnchor096"/>al in your Gradient environment, copy <strong class="source-inline">data_kaggle.csv</strong> into <strong class="source-inline">/storage/archive/kl_property</strong>:</p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/data_kaggle.csv /storage/archive/kl_property/data_kaggle.csv</strong></p>
			<p>I want to acknowledge the dataset featured in this section and express my gratitude for the opportunity to include it in the book.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Jas S (2019). <em class="italic">Property Listings in Kuala Lumpur</em> (<a href="https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur">https://www.kaggle.com/dragonduck/property-listings-in-kuala-lumpur</a>)</p>
			<p>Let's see how to go about it in the next section.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor097"/>How to do it…</h2>
			<p>In this recipe, you will be <a id="_idIndexMarker234"/>running through the <strong class="source-inline">training_model_standalone_tabular_dataset.ipynb</strong> notebook to train a model using the Kuala Lumpur property prices dataset. </p>
			<p>Once you have the <a id="_idIndexMarker235"/>notebook open in your fastai environment, follow these steps:</p>
			<ol>
				<li value="1">Run the first three cells to import the necessary libraries and set up the notebook for fastai.</li>
				<li>Run this cell to associate <strong class="source-inline">path</strong> with the directory that you copied the <strong class="source-inline">data_kaggle.csv</strong> file into:<p class="source-code">path = URLs.path('kl_property')</p></li>
				<li>Run this cell to ingest the dataset into the <strong class="source-inline">df_train</strong> DataFrame:<p class="source-code">df_train = pd.read_csv(path/'data_kaggle.csv')</p></li>
				<li>Run the following cell to see the first few rows of the dataset:<p class="source-code">df_train.head()</p><p>The output of <strong class="source-inline">df_train.head()</strong> shows a sample of rows from the dataset:</p><div id="_idContainer090" class="IMG---Figure"><img src="image/B16216_03_13.jpg" alt="Figure 3.13 – A sample of rows from the Kuala Lumpur property dataset&#13;&#10;"/></div><p class="figure-caption">Figure 3.13 – A sample of rows from the Kuala Lumpur property dataset</p></li>
				<li>Note the values in the <strong class="source-inline">Price</strong> column. The goal of this recipe is to train a deep learning model to predict whether values in this column are above or below the average. To do this, we <a id="_idIndexMarker236"/>need to start with numeric values in this column. You can see three problems just from this small sample of data:<p>a) The values include <strong class="screen-inline">RM</strong>, the symbol for the ringgit, the Malaysian currency.</p><p>b) The values include a comma thousand separator.</p><p>c) Some rows have a missing value in this column.</p><p>Over the next few cells of the notebook, we will deal with these problems so that we end up with this column containing entirely valid numeric values.</p></li>
				<li>Run this cell to get the <a id="_idIndexMarker237"/>number of rows in the dataset. The first element of the output is the number of rows in the DataFrame:<p class="source-code">df_train.shape</p></li>
				<li>Run the cells to define the <strong class="source-inline">remove_currency()</strong> and <strong class="source-inline">remove_after_space()</strong> functions. You will need these functions to clean up the dataset.</li>
				<li>Run this cell to address the problems with the <strong class="source-inline">Price</strong> column demonstrated by the output of <strong class="source-inline">df_train.head()</strong>. This cell drops rows in the dataset where the <strong class="source-inline">Price</strong> value is <a id="_idIndexMarker238"/>missing (first statement), removes the currency symbol from the <strong class="source-inline">Price</strong> column (second statement), and converts the values in the <strong class="source-inline">Price</strong> column into numeric values after removing any commas:<p class="source-code">df_train.dropna(subset=['Price'], inplace=True)</p><p class="source-code">df_train['Price'] = df_train['Price'].apply(lambda x: remove_currency("RM ",x))</p><p class="source-code">df_train['Price'] = pd.to_numeric(df_train['Price'].str.replace(',',''), errors='coerce')</p></li>
				<li>Run this cell to get the <a id="_idIndexMarker239"/>number of rows in the dataset again. It's important to know this number because we have eliminated some rows from the dataset in the last cell, and we will remove more as we clean up the <strong class="source-inline">Size</strong> column. By checking the shape of the DataFrame as we are removing rows, we can be sure we're not losing too much information:<div id="_idContainer091" class="IMG---Figure"><img src="image/B16216_03_14.jpg" alt="Figure 3.14 – Getting the shape of the DataFrame prior to dropping rows&#13;&#10;"/></div><p class="figure-caption">Figure 3.14 – Getting the shape of the DataFrame prior to dropping rows</p></li>
				<li>The <strong class="source-inline">Size</strong> column has a lot of useful information in it, but we have to do some work to get it ready to help train a deep learning model. To start with, run the following cell to see some examples of the values in the <strong class="source-inline">Size</strong> column. Here is the information we want to extract from this column:<p>a) Extract the prefix (for example, <strong class="source-inline">Built-up</strong>, <strong class="source-inline">Land area</strong>, and so on) into a new column.</p><p>b) Get a single numeric <a id="_idIndexMarker240"/>value for the remainder of the contents of the column; that is, replace 1,335 sq. ft. with 1,335 and replace 22 x 80 sq. ft. with 1,760.</p><p>c) Drop rows where the suffix in the <strong class="source-inline">Size</strong> column cannot yield a numeric value.</p><p>The output of <strong class="source-inline">df_train['Size'].head()</strong> shows examples of the values in the <strong class="source-inline">Size</strong> column:</p><div id="_idContainer092" class="IMG---Figure"><img src="image/B16216_03_15.jpg" alt="Figure 3.15 – Examples of values in the Size column&#13;&#10;"/></div><p class="figure-caption">Figure 3.15 – Examples of values in the Size column</p><p>Here are the <a id="_idIndexMarker241"/>operations we want to perform on the <strong class="source-inline">Size</strong> column to prepare it to train a deep learning model:</p><p>a) Extract the prefix (for example, <strong class="source-inline">Built-up</strong>, <strong class="source-inline">Land area</strong>, and so on) into a new column.</p><p>b) Where possible, get a single numeric value for the remainder of the contents of the <strong class="source-inline">Size</strong> column. For example, we want to replace 1,335 sq. ft. with 1,335 and replace 22 x 80 sq. ft. with 1,760.</p><p>c) For rows where it is not <a id="_idIndexMarker242"/>possible to get a numeric value from the remainder of the contents of the <strong class="source-inline">Size</strong> column, drop the row.</p></li>
				<li>Run the cell to define the <strong class="source-inline">clean_up_size()</strong> function. You will use this function to perform the following set of cleanup steps on the <strong class="source-inline">Size</strong> column. The result will be a DataFrame where all values in the <strong class="source-inline">Size</strong> column are numeric values representing the area of the property. Here are some of the transformations that are performed by the <strong class="source-inline">clean_up_size()</strong> function:<p>a) Lowercase all the values in the <strong class="source-inline">Size</strong> column.</p><p>b) Split the <strong class="source-inline">Size</strong> column into a new column (<strong class="source-inline">Size_type</strong>) that contains the non-numeric information and a remainder <strong class="source-inline">Size</strong> column that contains the numeric <a id="_idIndexMarker243"/>information about the area of the property.</p><p>c) Replace the missing values in the new <strong class="source-inline">Size</strong> column with <strong class="source-inline">0</strong>.</p><p>d) Remove the rows that do not contain any digits.</p><p>e) Remove the rows that contain problematic substrings, as listed in <strong class="source-inline">clean_up_list</strong>.</p><p>f) Replace extraneous characters so that all the <strong class="source-inline">Size</strong> entries are either numeric or of the <strong class="source-inline">numerica * numericb</strong> form.</p><p>g) Replace values of the <strong class="source-inline">numerica * numericb</strong> form with the product of the two values; that is, <strong class="source-inline">numerica</strong> and <strong class="source-inline">numericb</strong>.</p></li>
				<li>Run this cell to execute the <strong class="source-inline">clean_up_size()</strong> function:<p class="source-code">clean_up_list = ["-","\+",'\'','\~',"xx","sf","acre","#"]</p><p class="source-code">df_train = clean_up_size(df_train,clean_up_list)</p></li>
				<li>Run the <strong class="source-inline">shape</strong> command again to get the shape of the DataFrame after dropping the rows that don't contain enough data to be useful. This confirms that we lost about 2% of the <a id="_idIndexMarker244"/>rows from the original dataset after removing the offending rows:<div id="_idContainer093" class="IMG---Figure"><img src="image/B16216_03_16.jpg" alt="Figure 3.16 – Getting the shape of the DataFrame after cleanup&#13;&#10;"/></div><p class="figure-caption">Figure 3.16 – Getting the shape of the DataFrame after cleanup</p></li>
				<li>Run <strong class="source-inline">df_train.head()</strong> to see a sample of the DataFrame after the cleanup steps. Note that now, we <a id="_idIndexMarker245"/>have the following:<p>a) There is a new <strong class="source-inline">Size_type</strong> column.</p><p>b) The <strong class="source-inline">Size</strong> column contains numeric values.</p><p>The output of <strong class="source-inline">df_train.head()</strong> shows what the DataFrame looks like after the <strong class="source-inline">Price</strong> and <strong class="source-inline">Size</strong> columns have been cleaned up:</p><div id="_idContainer094" class="IMG---Figure"><img src="image/B16216_03_17.jpg" alt="Figure 3.17 – The DataFrame after performing the cleanup steps on the Price and Size columns&#13;&#10;"/></div><p class="figure-caption">Figure 3.17 – The DataFrame after performing the cleanup steps on the Price and Size columns</p></li>
				<li>Run the cell to <a id="_idIndexMarker246"/>define the <strong class="source-inline">under_over()</strong> function. You will run this function to replace the values in the <strong class="source-inline">Price</strong> column with an indicator of whether the price is under or over the average price.</li>
				<li>Run the following cell to replace the <a id="_idIndexMarker247"/>values in the <strong class="source-inline">Price</strong> column with indicators of whether the price is above or below the average:<p class="source-code">mean_sp = int(df_train['Price'].mean())</p><p class="source-code">if categorical_target:</p><p class="source-code">    df_train['Price'] = df_train['Price'].apply(lambda x: under_over(x,mean_sp))</p><p>After running this cell, the values in the <strong class="source-inline">Price</strong> column will be replaced:</p><div id="_idContainer095" class="IMG---Figure"><img src="image/B16216_03_18.jpg" alt="Figure 3.18 – The DataFrame after replacing the Price values with under/over average indicators&#13;&#10;"/></div><p class="figure-caption">Figure 3.18 – The DataFrame after replacing the Price values with under/over average indicators</p></li>
				<li>Run this cell to define the <a id="_idIndexMarker248"/>transformations to be applied to <strong class="source-inline">TabularDataLoaders</strong> object, the target column (<strong class="source-inline">Price</strong>), and the continuous and categorical column lists:<p class="source-code">procs = [FillMissing,Categorify]</p><p class="source-code">dep_var = 'Price'</p><p class="source-code">cont,cat = cont_cat_split(df_train, 1, dep_var=dep_var)</p></li>
				<li>Run this cell to define the <strong class="source-inline">TabularDataLoaders</strong> object with the arguments that you have defined so far in the notebook, including the dataset (<strong class="source-inline">df_train</strong>), the list of transformations (<strong class="source-inline">procs</strong>) to be applied to the dataset, the continuous and categorical column lists (<strong class="source-inline">cont</strong> and <strong class="source-inline">cat</strong>), and the dependent variable (<strong class="source-inline">dep_var</strong>):<p class="source-code">dls = TabularDataLoaders.from_df(df_train,path,procs= procs, </p><p class="source-code">         cat_names= cat, cont_names = cont, </p><p class="source-code">         y_names = dep_var,</p><p class="source-code">         valid_idx=list(range((df_train.shape[0]-5000),df_train.shape[0])), </p><p class="source-code">         bs=64)</p></li>
				<li>Run this cell to fit the model and see the model's performance. Your accuracy and loss may be slightly different, but <a id="_idIndexMarker249"/>you should see over 90% accuracy, which is good for a model that's been trained on fewer than 100k records:<div id="_idContainer096" class="IMG---Figure"><img src="image/B16216_03_19.jpg" alt="Figure 3.19 – Results of fitting the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.19 – Results of fitting the model</p></li>
				<li>Run this cell to see the <a id="_idIndexMarker250"/>results in the <a id="_idIndexMarker251"/>validation set. You will see that for this set of results, the model correctly predicts whether a property will have a price above or below the average:</li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B16216_03_20.jpg" alt="Figure 3.20 – Model predictions of whether properties will have a price above or below the average&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Model predictions of whether properties will have a price above or below the average</p>
			<p>Congratulations! You <a id="_idIndexMarker252"/>have trained a deep learning model with fastai on a dataset that required non-trivial cleanup before it could be used to train the model.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor098"/>How it works…</h2>
			<p>In this recipe, you saw that while fastai provides utilities that make it easier to train a deep learning model on a tabular dataset, you still have to ensure that the dataset is capable of training the model. This means that non-numeric values need to be removed from numeric columns. This may require iterative cleanup steps, as you saw while working through the notebook featured in this recipe.</p>
			<p>The dataset featured in this recipe contains the kind of anomalies and inconsistencies that are typical of real-world datasets, so the techniques you exercised in this recipe (including pandas manipulations to remove rows that had problematic values in certain columns, as well as string replacement techniques) will be applicable to other real-world datasets.</p>
			<p>Even with a messy dataset, fastai makes it easy to get a high performing deep learning model, thanks to it <a id="_idIndexMarker253"/>picking intelligent defaults and automating key operations (such as identifying categorical and continuous columns and dealing with missing values).</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor099"/>Assessing whether a tabular dataset is a good candidate for fastai</h1>
			<p>So far in this chapter, we have <a id="_idIndexMarker254"/>created three deep learning models for <a id="_idIndexMarker255"/>tabular datasets using fastai. But what if you want to determine whether a new dataset is a good candidate for training a deep learning model with fastai? In this recipe, we'll go through the process of assessing whether a dataset is a good candidate for deep learning with fastai.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor100"/>Getting ready</h2>
			<p>Ensure you have followed the steps in <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, to get a fastai environment set up.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor101"/>How to do it…</h2>
			<p>As you have seen so far in this chapter, you <a id="_idIndexMarker256"/>have many choices surrounding datasets that could possibly be applied to deep learning. To assess whether a <a id="_idIndexMarker257"/>dataset is a good candidate, we will go through the process of creating a new notebook from scratch and ingesting data from an online API. Follow these steps:</p>
			<ol>
				<li value="1">Create a new notebook in Gradient. You can do this in Gradient JupyterLab by following these steps:<p>a) Click on the new launcher button (<strong class="bold">+</strong>) in the main window:</p><div id="_idContainer098" class="IMG---Figure"><img src="image/B16216_03_21.jpg" alt="Figure 3.21 – Opening New Launcher in JupyterLab&#13;&#10;"/></div><p class="figure-caption">Figure 3.21 – Getting a new launcher in JupyterLab</p><p>b) When the <a id="_idIndexMarker258"/>launcher <a id="_idIndexMarker259"/>pane opens, click on <strong class="bold">Python 3</strong> to open a new notebook:</p><div id="_idContainer099" class="IMG---Figure"><img src="image/B16216_03_22.jpg" alt="Figure 3.22 – The Launcher pane in JupyterLab&#13;&#10;"/></div><p class="figure-caption">Figure 3.22 – The launcher pane in JupyterLab</p></li>
				<li>In the <a id="_idIndexMarker260"/>new notebook, create and run two new cells with the <a id="_idIndexMarker261"/>statements required to set up the notebook:<div id="_idContainer100" class="IMG---Figure"><img src="image/B16216_03_23.jpg" alt="Figure 3.23 – The cells to set up a fastai notebook for tabular datasets&#13;&#10;"/></div><p class="figure-caption">Figure 3.23 – The cells to set up a fastai notebook for tabular datasets</p></li>
				<li>Run the following cell to import the <a id="_idIndexMarker262"/>additional libraries <a id="_idIndexMarker263"/>required to investigate this dataset:<p class="source-code">! pip install pandas_datareader</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import os</p><p class="source-code">import yaml</p><p class="source-code"># For reading stock data</p><p class="source-code">from pandas_datareader.data import DataReader</p><p class="source-code"># For time stamps</p><p class="source-code">from datetime import datetime</p></li>
				<li> Run the following cell to load a dataset of stock prices for the company AstraZeneca (stock ticker = <strong class="source-inline">AZN</strong>):<p class="source-code">df = DataReader('AZN', 'stooq')</p></li>
				<li>Check the output of <strong class="source-inline">df.head()</strong> to see the contents of the dataframe:<div id="_idContainer101" class="IMG---Figure"><img src="image/B16216_03_24.jpg" alt="Figure 3.24 – Sample of the stock prices dataset&#13;&#10;"/></div><p class="figure-caption">Figure 3.24 – Sample of the stock prices dataset</p></li>
				<li>Get the shape of the dataframe <strong class="source-inline">df</strong> to determine how many rows are in the dataframe. Do you think this dataset is big enough to successfully train a deep learning model?<div id="_idContainer102" class="IMG---Figure"><img src="image/B16216_03_25.jpg" alt="Figure 3.25 – Getting the shape of df&#13;&#10;"/></div><p class="figure-caption">Figure 3.25 – Getting the shape of df</p></li>
				<li>Run the following cell to prepare to check the dataframe for missing values<p class="source-code">count = df.isna().sum()</p><p class="source-code">df_missing = (pd.concat([count.rename('missing_count'),count.div(len(df)).rename('missing_ratio')],axis = 1).loc[count.ne(0)])</p></li>
				<li>Now confirm the dataset has no missing values<div id="_idContainer103" class="IMG---Figure"><img src="image/B16216_03_26.jpg" alt="Figure 3.26 – Confirming the dataset has no missing values&#13;&#10;"/></div><p class="figure-caption">Figure 3.26 – Confirming the dataset has no missing values</p></li>
				<li>Run the following cell to set the parameters for the training run:<p class="source-code">dep_var = 'Close'</p><p class="source-code"># define columns that are continuous / categorical</p><p class="source-code">cont,cat = cont_cat_split(df, 1, dep_var=dep_var)</p></li>
				<li>Run the following cell to define the TabularDataLoaders object::<p class="source-code">procs = [Normalize]</p><p class="source-code">dls = TabularDataLoaders.from_df(df,procs= procs,</p><p class="source-code">        cat_names= cat, cont_names = cont, </p><p class="source-code">        y_names = dep_var, </p><p class="source-code">        valid_idx=list(range((df.shape[0]-50),df.shape[0])), bs=64)</p></li>
				<li>Run the following cell to define and train the model::<p class="source-code">learn = tabular_learner(dls, metrics=accuracy)</p><p class="source-code">learn.fit_one_cycle(30)</p></li>
				<li> You can see from the following output that the performance of the model is poor:<div id="_idContainer104" class="IMG---Figure"><img src="image/B16216_03_27.jpg" alt="Figure 3.27 – Poor performance when training the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.27 – Poor performance when training the model</p></li>
				<li>Now we want to make some changes to try to get a model that has better performance. To start with, run the following cell to define a function to create a new target column:<p class="source-code">def get_target(value,threshold):</p><p class="source-code">    '''return based on whether the input value is greater than or less than input threshold'''</p><p class="source-code">    if value &lt;= threshold:</p><p class="source-code">        return_value = "0"</p><p class="source-code">    else:</p><p class="source-code">        return_value = "1"</p><p class="source-code">    return(return_value)</p></li>
				<li>Run the following cell to define the new target column:<p class="source-code">threshold = df['Close'].mean()</p><p class="source-code">df['target'] = df['Close'].apply(lambda x: get_target(x,threshold))</p></li>
				<li>Run the following cell to specify that the new <strong class="source-inline">target</strong> column is the dependent variable and to limit <strong class="source-inline">cont</strong>, the set of continuous columns used to train the model. Note that in the first model in this section, the dependent variable was <strong class="source-inline">Close</strong>, a continuous column. That means the first model was trying to predict a continuous value. Because <strong class="source-inline">target</strong> is a categorical column, the new model will predict a categorical value rather than a continuous value:<p class="source-code">dep_var = 'target'</p><p class="source-code">cont = ['High', 'Low', 'Open', 'Volume']</p></li>
				<li>Run the following cell to train a new model using the new dependent variable and the new set of continuous columns:<p class="source-code">dls = TabularDataLoaders.from_df(df,procs= procs, </p><p class="source-code">    cat_names= cat, cont_names = cont, </p><p class="source-code">    y_names = dep_var, </p><p class="source-code">    valid_idx=list(range((df.shape[0]-50),df.shape[0])), </p><p class="source-code">    bs=64)</p><p class="source-code">learn = tabular_learner(dls, metrics=accuracy)</p><p class="source-code">learn.fit_one_cycle(30)</p></li>
				<li>You can see that this new model has much better performance than the previous model:</li>
			</ol>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B16216_03_28.jpg" alt="Figure 3.28 – Improved performance with the second model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28 – Improved performance with the second model</p>
			<p>In this recipe, you tried two variations on a deep learning model for a dataset of stock price information. The first model had a continuous dependent variable and used fastai defaults throughout. Unlike the other recipes in this chapter, the first model had poor performance. If you attempt to train the first model with more epochs you will see that the performance does not improve. The second model, where the dependent variable is changed from continuous to categorical, has much better performance.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor102"/>How it works…</h2>
			<p>The first model in this section was unsuccessful. Attempting to predict a continuous value with a deep learning model trained on 1.3 k records is not likely to work. Generally speaking, you need a training set that is an order of magnitude bigger, in the hundreds of thousands or millions of records, to predict a continuous outcome.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor103"/>Saving a trained tabular model</h1>
			<p>So far, we have trained a series of fastai deep learning models on tabular datasets. These models are available to us in the <a id="_idIndexMarker264"/>Python session where we train the model, but what can we do to save the models so that we can use them later in a different session? In this recipe, we will learn how to save a fastai deep learning model to a file and access that model in another Python session.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor104"/>Getting ready</h2>
			<p>Ensure you have followed the steps in <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, to get a fastai environment set up. Confirm that you can open the <strong class="source-inline">saving_models_trained_with_tabular_datasets.ipynb</strong> and <strong class="source-inline">loading_saved_models_trained_with_tabular_datasets.ipynb</strong> notebooks in the <strong class="source-inline">ch3</strong> directory of your repository.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor105"/>How to do it…</h2>
			<p>In this recipe, you will be running through the <strong class="source-inline">saving_models_trained_with_tabular_datasets.ipynb</strong> notebook to train a model – the same model that you trained in the first recipe of this chapter – and save it. Then, you will use the <strong class="source-inline">loading_saved_models_trained_with_tabular_datasets.ipynb</strong> notebook to load and exercise a saved fastai model.</p>
			<p>Once you have the <strong class="source-inline">saving_models_trained_with_tabular_datasets.ipynb</strong> notebook open in your fastai environment, follow these steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Save the trained model</strong> cell. By running these cells, you will be ingesting the <strong class="source-inline">ADULT_SAMPLE</strong> curated tabular dataset into a pandas DataFrame and training a fastai model on it.</li>
				<li>Run the next two cells to set the value of the path for the model to a writable directory. Ensure that the directory that you set learn.path to exists and is writable.</li>
				<li>Run this cell to <a id="_idIndexMarker265"/>save the trained model to the <strong class="source-inline">adult_sample_model.pkl</strong> file:<p class="source-code">learn.export('adult_sample_model.pkl')</p></li>
				<li>Now that you have saved the trained model into a file, you must load it into another notebook to test the process you would go through to retrieve the model in a new Python session, and then use the saved model to make a prediction on the test data. </li>
				<li>Open the <strong class="source-inline">loading_saved_models_trained_with_tabular_datasets.ipynb</strong> notebook in a Gradient session.</li>
				<li>Run the cells up to the <strong class="source-inline">Load the saved, trained model</strong> cell to load the required libraries and set up the notebook.</li>
				<li>Run this cell to load the model you saved earlier in this recipe into this new notebook. Ensure that you specify the path where you saved the model earlier in this recipe:<p class="source-code">learn = load_learner('/notebooks/temp/models/adult_sample_model.pkl')</p></li>
				<li>Run this cell to load the test dataset into a DataFrame:<p class="source-code">df_test = pd.read_csv('adult_sample_test.csv')</p></li>
				<li>Run this cell to select the first row of the test dataset and apply the trained model to get a prediction for this data point:<p class="source-code">test_sample = df_test.iloc[0]</p><p class="source-code">learn.predict(test_sample)</p></li>
				<li>The result includes the <a id="_idIndexMarker266"/>prediction of the model on this data point. You can see that for this data point, the model is predicting a <strong class="source-inline">salary</strong> value of <strong class="source-inline">1.0</strong>, which means that it is predicting that this individual will have a salary of over 50k:</li>
			</ol>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B16216_03_29.jpg" alt="Figure 3.29 – Results of applying the saved model to a test data point&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29 – Results of applying the saved model to a test data point</p>
			<p>Congratulations! You have successfully saved a fastai deep learning model, loaded the saved model in a new notebook, and applied the saved model to get a prediction on a row of test data.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor106"/>How it works…</h2>
			<p>The fastai framework includes support to make it easy to save deep learning models to your filesystem using the <strong class="source-inline">export()</strong> method of <strong class="source-inline">learner</strong> objects. In this recipe, you saw an example of how you can save a trained model to a pickle file. You also learned how to load the pickle file back into Python and then apply the trained model to a new data example. This is a peek ahead at the process of performing inference on a deployed model. In <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a>, <em class="italic">Deployment and Model Maintenance</em>, you will see complete examples of performing inference on a deployed model.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/>Test your knowledge</h1>
			<p>Now that you have completed the recipes in this chapter, follow the steps shown here to exercise what you have learned. You will do this by adapting one of the notebooks you worked through in this chapter so that it works with a new dataset. </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/>Getting ready</h2>
			<p>Follow these steps to <a id="_idIndexMarker267"/>upload a new tabular dataset: </p>
			<ol>
				<li value="1">Go to the site for the Kaggle competition on future sales prediction (<a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data">https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data</a>) and accept the conditions for the competition to get access to the datasets associated with the competition.</li>
				<li>Download the <strong class="source-inline">sales_train.csv.zip</strong> and <strong class="source-inline">test.csv.zip</strong> files.</li>
				<li>Unzip the downloaded files to extract <strong class="source-inline">sales_train.csv</strong> and <strong class="source-inline">test.csv</strong>.</li>
				<li>From the Terminal in your Gradient environment, make your current directory <strong class="source-inline">/storage/archive</strong>:<p class="source-code"><strong class="bold">cd /storage/archive</strong></p></li>
				<li>Create a folder called <strong class="source-inline">/storage/archive/price_prediction</strong>:<p class="source-code"><strong class="bold">mkdir price_prediction</strong></p></li>
				<li>Upload <strong class="source-inline">sales_train.csv</strong> and <strong class="source-inline">test.csv</strong> to <strong class="source-inline">/storage/archive/price_prediction</strong>. You can use the upload button in JupyterLab in Gradient to do the upload via the <strong class="source-inline">/notebooks/temp</strong> directory that you created earlier in this chapter:<p>a) In the JupyterLab file browser, make <strong class="source-inline">temp</strong> your current folder, select the upload button, as shown in the <a id="_idIndexMarker268"/>following screenshot, and select <strong class="source-inline">sales_train.csv</strong> and <strong class="source-inline">test.csv</strong> from the local system folder where you extracted them in <em class="italic">step 2</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B16216_03_30.jpg" alt="Figure 3.30 – Upload button in JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.30 – Upload button in JupyterLab</p>
			<p>b) From the Terminal in your Gradient environment, make <strong class="source-inline">/storage/archive/price_prediction</strong> your current directory:</p>
			<p class="source-code"><strong class="bold">cd /storage/archive/price_prediction</strong></p>
			<p>c) Copy <strong class="source-inline">sales_train.csv</strong>, <strong class="source-inline">test.csv</strong>, and <strong class="source-inline">data_kaggle.csv</strong> into <strong class="source-inline">/storage/archive/price_prediction</strong>:</p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/sales_train.csv  sales_train.csv </strong></p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/test.csv</strong><strong class="bold">  test.csv</strong></p>
			<p>Now that you have uploaded the dataset, it's time to <a id="_idIndexMarker269"/>create a notebook to ingest the dataset:</p>
			<ol>
				<li value="1">Make a copy of the <strong class="source-inline">training_model_standalone_tabular_dataset.ipynb</strong> notebook that you worked through in the <em class="italic">Training a model with a standalone dataset</em> recipe. Call the copy <strong class="source-inline">training_model_new_tabular_dataset.ipynb</strong>.</li>
				<li> In your new notebook, update <a id="_idIndexMarker270"/>the cells that ingest the training dataset:<p class="source-code">path = URLs.path('price_prediction') </p><p class="source-code">df_train = pd.read_csv(path/'sales_train.csv')</p></li>
				<li>The output of <strong class="source-inline">df_train.head()</strong> should show you the structure of the dataset. You can find a description of the columns of this dataset at <a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data?select=sales_train.csv">https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data?select=sales_train.csv</a>:</li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B16216_03_31.jpg" alt="Figure 3.31 – A sample of the contents of sales_train.csv&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.31 – A sample of the contents of sales_train.csv</p>
			<p>Congratulations! You have ingested another standalone tabular dataset into a fastai notebook. You can apply similar techniques to make other tabular datasets available so that they can used in fastai solutions.</p>
			<p>Now that you have ingested the dataset, consider the steps you would take to prepare this dataset for training a deep learning model. How would you deal with any missing values? Are there tests you can <a id="_idIndexMarker271"/>apply to some of the columns to detect and correct incorrect values? The Kaggle competition predicts the total sales for every product and store for the next month. Completing a fastai model to tackle this problem is beyond the scope of this book, but consider how you might refactor the dataset to prepare it for this problem.</p>
		</div>
	</body></html>