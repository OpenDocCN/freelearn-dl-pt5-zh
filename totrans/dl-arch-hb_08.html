<html><head></head><body>
<div id="_idContainer096">
<h1 class="chapter-number" id="_idParaDest-123"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-124"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.2.1">Exploring Supervised Deep Learning</span></h1>
<p><em class="italic"><span class="koboSpan" id="kobo.3.1">Chapters 2</span></em><span class="koboSpan" id="kobo.4.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.5.1">6</span></em><span class="koboSpan" id="kobo.6.1"> explored the core workhorse behind </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">deep learning</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">DL</span></strong><span class="koboSpan" id="kobo.10.1">) technology and included some minimal technical implementations for easy digestion. </span><span class="koboSpan" id="kobo.10.2">It is important to understand the intricacies of how </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.11.1">different </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">neural networks</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.14.1">NNs</span></strong><span class="koboSpan" id="kobo.15.1">) work. </span><span class="koboSpan" id="kobo.15.2">One reason is that when things go wrong</span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.16.1"> with any NN model, you can identify what the root cause is and mitigate it. </span><span class="koboSpan" id="kobo.16.2">Those chapters are also important to showcase how flexible DL architectures are to solve different types of real-world problems. </span><span class="koboSpan" id="kobo.16.3">But what are the problems exactly? </span><span class="koboSpan" id="kobo.16.4">Also, how should we train a DL model effectively in </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">varying situations?</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">In this chapter, we will attempt to answer the preceding two points specifically for supervised deep learning, but we will leave answering the same questions for unsupervised deep learning for the next chapter. </span><span class="koboSpan" id="kobo.18.2">This chapter will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Exploring supervised use cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">problem types</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Implementing neural network layers for foundational </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">problem types</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Training supervised deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">models effectively</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Exploring general techniques to realize and improve supervised deep </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">learning-based solutions</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Breaking down the multitask paradigm in supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">deep learning</span></span></li>
</ul>
<h1 id="_idParaDest-125"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.30.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.31.1">This chapter includes some practical implementations in the Python programming language. </span><span class="koboSpan" id="kobo.31.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">pytorch</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">catalyst==22.04</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">numpy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">scikit-learn</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">at </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<h1 id="_idParaDest-126"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.41.1">Exploring supervised use cases and problem types</span></h1>
<p><span class="koboSpan" id="kobo.42.1">Supervised learning</span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.43.1"> requireslabeled</span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.44.1"> data. </span><span class="koboSpan" id="kobo.44.2">Labels, targets, and ground truth all refer to the same thing. </span><span class="koboSpan" id="kobo.44.3">The provided labels</span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.45.1"> essentially supervise the learning process of the </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">machine learning</span></strong><span class="koboSpan" id="kobo.47.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.48.1">ML</span></strong><span class="koboSpan" id="kobo.49.1">) model and provide the feedback needed for a DL model to generate gradients and update itself. </span><span class="koboSpan" id="kobo.49.2">Labels can exist in many different forms. </span><span class="koboSpan" id="kobo.49.3">They are </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">continuous numerical format</span></strong><span class="koboSpan" id="kobo.51.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">categorical format</span></strong><span class="koboSpan" id="kobo.53.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">text format</span></strong><span class="koboSpan" id="kobo.55.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">multiple categorical formats</span></strong><span class="koboSpan" id="kobo.57.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">image format</span></strong><span class="koboSpan" id="kobo.59.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">video format</span></strong><span class="koboSpan" id="kobo.61.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.62.1">audio format</span></strong><span class="koboSpan" id="kobo.63.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">multiple target formats</span></strong><span class="koboSpan" id="kobo.65.1">. </span><span class="koboSpan" id="kobo.65.2">All of these are then categorized as either of the following supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">problem types:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">Binary classification</span></strong><span class="koboSpan" id="kobo.68.1">: This is when the target has categorical</span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.69.1"> data with only two </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">unique values.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Multiclassification</span></strong><span class="koboSpan" id="kobo.72.1">: This is when the target has categorical data </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.73.1">with more than two </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">unique values.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Regression</span></strong><span class="koboSpan" id="kobo.76.1">: This is when the target has continuous</span><a id="_idIndexMarker576"/> <span class="No-Break"><span class="koboSpan" id="kobo.77.1">numerical data.</span></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.78.1">Multi-target/problem</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Multilabel</span></strong><span class="koboSpan" id="kobo.81.1">: This is when the target</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.82.1"> has more than one binary associated with a single </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">data row.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Multi-regression</span></strong><span class="koboSpan" id="kobo.85.1">: This is when the target has more than one regression target associated with a single </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">data row.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Multiple problems</span></strong><span class="koboSpan" id="kobo.88.1">: Either multiple targets associated with a single data row in a single dataset or a chain of problems that is sequential in nature and multiple models</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.89.1"> are learned through </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">different datasets.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.91.1">Supervised representation learning</span></strong><span class="koboSpan" id="kobo.92.1">: This can be in many forms, and the main goal is to learn</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.93.1"> meaningful data representations given input data. </span><span class="koboSpan" id="kobo.93.2">The results of learned representation can subsequently</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.94.1"> be utilized for many purposes, including </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">transfer learning</span></strong><span class="koboSpan" id="kobo.96.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.97.1">TL</span></strong><span class="koboSpan" id="kobo.98.1">), and to realize </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">recommendation</span></span><span class="No-Break"><a id="_idIndexMarker581"/></span><span class="No-Break"><span class="koboSpan" id="kobo.100.1"> systems.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.101.1">The definition of these problems may be hard to understand by itself. </span><span class="koboSpan" id="kobo.101.2">To create a better understanding of the different problems, we will check out an extensive set of use cases that can take advantage of DL technologies. </span><span class="koboSpan" id="kobo.101.3">Given the problems specified previously, the following table</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.102.1"> lists their </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">use cases:</span></span></p>
<table class="No-Table-Style" id="table001-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.104.1">Problem Types</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.105.1">Supervised Deep Learning Model </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.106.1">Use Cases</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.107.1">Binary </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">classification</span></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><span class="koboSpan" id="kobo.109.1">Gender prediction of babies with </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">ultrasound imagery</span></span></li>
<li><span class="koboSpan" id="kobo.111.1">Semiconductor chip-quality rejection prediction in manufacturing with </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">image data</span></span></li>
<li><span class="koboSpan" id="kobo.113.1">Using email data that can contain text, images, documents, audio, or any data to </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">predict spam</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.115.1">Multiclassification</span></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><span class="koboSpan" id="kobo.116.1">Document data </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">topic classification</span></span></li>
<li><span class="koboSpan" id="kobo.118.1">Hate/toxic speech or </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">text classification</span></span></li>
<li><span class="koboSpan" id="kobo.120.1">General image </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">object classification</span></span></li>
<li><span class="koboSpan" id="kobo.122.1">Sentiment prediction of text or </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">speech data</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.124.1">Regression</span></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.125.1">Click-through rate</span></strong><span class="koboSpan" id="kobo.126.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.127.1">CTR</span></strong><span class="koboSpan" id="kobo.128.1">) prediction of advertisements with image or text </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">or both</span></span></li>
<li><span class="koboSpan" id="kobo.130.1">Human age prediction with a facial </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">input image</span></span></li>
<li><span class="koboSpan" id="kobo.132.1">Predicting GPS location from </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">an image</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.134.1">Multi-target</span></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.135.1">Text topic classification</span></strong><span class="koboSpan" id="kobo.136.1">: Multilabel, multiple topics can exist in a single text </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">data row.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.138.1">Image object detection</span></strong><span class="koboSpan" id="kobo.139.1">: A multiple-target problem consisting of multiple regression targets and multiclass classification. </span><span class="koboSpan" id="kobo.139.2">First, with an image bounding box as multiple regression targets, a single </span><em class="italic"><span class="koboSpan" id="kobo.140.1">x</span></em><span class="koboSpan" id="kobo.141.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.142.1">y</span></em><span class="koboSpan" id="kobo.143.1"> numerical coordinate, and its width and height as the two extra targets forms a rectangular-shaped bounding box. </span><span class="koboSpan" id="kobo.143.2">Next, the bounding box will be used to extract a cropped image for multiclassification purposes to predict the type </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">of object.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Image segmentation</span></strong><span class="koboSpan" id="kobo.146.1">: A kind of multilabel problem, where every pixel will serve as </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">binary targets.</span></span></li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.148.1">Supervised representation </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">learning</span></span></p>
</td>
<td class="No-Table-Style">
<ul>
<li><span class="koboSpan" id="kobo.150.1">Face feature representation for </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">face recognition.</span></span></li>
<li><span class="koboSpan" id="kobo.152.1">Audio representation for speaker recognition using </span><strong class="bold"><span class="koboSpan" id="kobo.153.1">K-Nearest </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.154.1">Neighbors</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.155.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.156.1">KNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.158.1">Representing categories with their own representative feature vectors. </span><span class="koboSpan" id="kobo.158.2">This can be achieved with a method called </span><strong class="bold"><span class="koboSpan" id="kobo.159.1">categorical embeddings,</span></strong><span class="koboSpan" id="kobo.160.1"> which is an NN layer type that holds a feature vector for each category in a categorical feature column. </span><span class="koboSpan" id="kobo.160.2">It is learnable and serves as a lookup table. </span><span class="koboSpan" id="kobo.160.3">The method can reduce the feature dimensions of high-cardinality categorical data when compared to basic one-hot-encoding but still maintain around the </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">same performance.</span></span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.162.1">Table 8.1 – A table of DL problem use cases</span></p>
<p><span class="koboSpan" id="kobo.163.1">Binary classification, multiclass classification, and regression problems are rather straightforward to approach. </span><span class="koboSpan" id="kobo.163.2">Multi-target types, however, pose complicated setups and require more architecting to be done depending on the nature of the problem. </span><span class="koboSpan" id="kobo.163.3">Multi-target tasks also can be straightforward, such as multilabel or multi-regression problems. </span><span class="koboSpan" id="kobo.163.4">This task falls into the bigger envelope of multitask solutions and will be discussed further in the </span><em class="italic"><span class="koboSpan" id="kobo.164.1">Exploring general techniques to realize and improve DL-based solutions</span></em><span class="koboSpan" id="kobo.165.1"> section of</span><a id="_idIndexMarker583"/> <span class="No-Break"><span class="koboSpan" id="kobo.166.1">this </span></span><span class="No-Break"><a id="_idIndexMarker584"/></span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">Next, we will implement the basic NN layers of realizing the foundational problems, which include binary classification, multiclass classification, and </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">regression problems.</span></span></p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.170.1">Implementing neural network layers for foundational problem types</span></h1>
<p><span class="koboSpan" id="kobo.171.1">In </span><em class="italic"><span class="koboSpan" id="kobo.172.1">Chapters 2</span></em><span class="koboSpan" id="kobo.173.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.174.1">7</span></em><span class="koboSpan" id="kobo.175.1">, although many</span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.176.1"> types of NN layers were introduced, the core layers for the problem types were either not used or not explained. </span><span class="koboSpan" id="kobo.176.2">Here, we will go through each of them for clarity </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">and intuition.</span></span></p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.178.1">Implementing the binary classification layer</span></h2>
<p><span class="koboSpan" id="kobo.179.1">Binary means two options</span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.180.1"> for categorical </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.181.1">data. </span><span class="koboSpan" id="kobo.181.2">Note that this does not necessarily mean a strict rule for the categories to be true or false nor positive or negative in the raw data. </span><span class="koboSpan" id="kobo.181.3">The two options can be in any format possible in terms of raw data, in strings, numbers, or symbols. </span><span class="koboSpan" id="kobo.181.4">However, note that NNs can always only produce numerical outputs. </span><span class="koboSpan" id="kobo.181.5">This means that the target itself has to be represented numerically, for which the optimal numbers are the binary values of zero and one. </span><span class="koboSpan" id="kobo.181.6">This means that the data column to be used as a target for training with only two unique values must go through preprocessing to map itself into zero </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">or one.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">Generally, there are two ways to define binary outputs in NNs. </span><span class="koboSpan" id="kobo.183.2">The first is to use a linear layer with a size of one. </span><span class="koboSpan" id="kobo.183.3">The second method is to use a linear layer with a size of two. </span><span class="koboSpan" id="kobo.183.4">There is no significant difference between the two in terms of task-quality metrics, but method one takes slightly less space for storage and memory, so feel free to always use that version. </span><span class="koboSpan" id="kobo.183.5">The outputs from method 1 will be constrained to values between </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">0</span></strong><span class="koboSpan" id="kobo.185.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.186.1">1</span></strong><span class="koboSpan" id="kobo.187.1"> by using a sigmoid layer. </span><span class="koboSpan" id="kobo.187.2">For method 2, the outputs need to be passed into a softmax layer so that the probabilities for the two outputs will add up to one. </span><span class="koboSpan" id="kobo.187.3">Both methods usually can be optimized using </span><strong class="bold"><span class="koboSpan" id="kobo.188.1">cross-entropy</span></strong><span class="koboSpan" id="kobo.189.1">. </span><span class="koboSpan" id="kobo.189.2">Cross-entropy</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.190.1"> is also known </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.191.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.192.1">log loss</span></strong><span class="koboSpan" id="kobo.193.1">. </span><span class="koboSpan" id="kobo.193.2">Log loss measures the difference between predicted probabilities and true labels using a logarithmic scale. </span><span class="koboSpan" id="kobo.193.3">This scale penalizes incorrect predictions more heavily, emphasizing the importance of a model’s ability to assign high probabilities to the correct class and low probabilities to the </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">incorrect class.</span></span></p>
<p><span class="koboSpan" id="kobo.195.1">Translating the layer from method one into actual </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">pytorch</span></strong><span class="koboSpan" id="kobo.197.1"> code will look like the following using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">nn</span></strong><span class="koboSpan" id="kobo.199.1"> module </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">torch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.203.1">
from torch import nn
final_fc_layer = nn.Sequential(
  nn.Linear(10, 1),
  nn.Softmax(),
)</span></pre> <p><span class="koboSpan" id="kobo.204.1">Next, we will implement the multiclass </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">classification layer.</span></span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.206.1">Implementing the multiclass classification layer</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.207.1">Multiclass</span></strong><span class="koboSpan" id="kobo.208.1"> means the data </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.209.1">contains multiple </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.210.1">categories with more than two categories, differentiating it from binary classification. </span><span class="koboSpan" id="kobo.210.2">Likewise, in multiclass classification, the raw data can be in any format possible and must go through a preprocessing task to convert the raw categories into ordered numbers that start from 0. </span><span class="koboSpan" id="kobo.210.3">The ordered numbers, however, do not signify an actual ordering, nor do they encode any order information that the NN model can leverage. </span><span class="koboSpan" id="kobo.210.4">The output linear layer for multiclass problems needs to be configured with the same number of neurons as the number of unique classes. </span><span class="koboSpan" id="kobo.210.5">A softmax layer is similarly applied after the output linear layer to make sure the probabilities for the final outputs will add up to one. </span><span class="koboSpan" id="kobo.210.6">Cross-entropy is also used here as the standard loss. </span><span class="koboSpan" id="kobo.210.7">During the inference stage, the class index with the highest probability will then be used as the predicted class. </span><span class="koboSpan" id="kobo.210.8">Translating the multiclass prediction layer into </span><strong class="source-inline"><span class="koboSpan" id="kobo.211.1">pytorch</span></strong><span class="koboSpan" id="kobo.212.1"> code for a 100-class multiclass classification problem with 10 logits will look </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.214.1">
final_fc_layer = nn.Sequential(
  nn.Linear(10, 100),
  nn.Softmax(),
)</span></pre> <p><span class="koboSpan" id="kobo.215.1">Another sub-problem of multiclass classification is when the classes are ordinal. </span><span class="koboSpan" id="kobo.215.2">This sub-problem and task is called ordinal classification. </span><span class="koboSpan" id="kobo.215.3">This means that the classes have an incremental relationship with each other. </span><span class="koboSpan" id="kobo.215.4">A plain multiclass classification layer strategy represents ordinal classes sub-optimally as the classes in the multiclass are considered to have an equal relationship with each other. </span><span class="koboSpan" id="kobo.215.5">A good strategy here to add the information of ordinal classes is to utilize a technique based on the multilabel classification task, which is a multiple-binary </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">classification task.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">Let’s say that we have five ordinal classes represented as numerical numbers from 1 to 5 for simplicity. </span><span class="koboSpan" id="kobo.217.2">In reality, this could be represented by any categorical data. </span><span class="koboSpan" id="kobo.217.3">In an NN, five binary classification heads would be used for this case where the classes will be assigned to the respective head in an ascending ordered manner. </span><span class="koboSpan" id="kobo.217.4">The raw predictions from this NN will be consumed in a way where the final predicted ordinal class will be derived from the position of the furthest consecutive positive binary prediction. </span><span class="koboSpan" id="kobo.217.5">Once there is a negative prediction, the rest of the prediction heads on the right will then be ignored. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.218.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.219.1">.1</span></em><span class="koboSpan" id="kobo.220.1"> depicts this strategy by simulating the output predictions of the five binary </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">classification heads:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.222.1"><img alt="Figure 8.1 – Ordinal classification processing strategy using the output predictions of the five binary classification heads" src="image/B18187_08_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.223.1">Figure 8.1 – Ordinal classification processing strategy using the output predictions of the five binary classification heads</span></p>
<p><span class="koboSpan" id="kobo.224.1">The learning process will be, as usual, using the cross-entropy loss for multiple binary targets. </span><span class="koboSpan" id="kobo.224.2">Additionally, the performance at every epoch can be monitored using robust metrics that don’t depend on probabilities such as </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.225.1">recall or </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.226.1">precision. </span><span class="koboSpan" id="kobo.226.2">The ordinal encoding method allows the model to learn that the targets have an </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">ordinal relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">Next, we will dive into the implementation of a </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">regression layer.</span></span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.230.1">Implementing a regression layer</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.231.1">Regression</span></strong><span class="koboSpan" id="kobo.232.1"> means a single </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.233.1">numerical </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.234.1">target and prediction. </span><span class="koboSpan" id="kobo.234.2">Regression can be realized in an NN by simply having a linear layer with a single neuron so that an unbounded single numerical output can be obtained without any activation layers. </span><span class="koboSpan" id="kobo.234.3">Translating the layer into actual </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">pytorch</span></strong><span class="koboSpan" id="kobo.236.1"> code will look like the following using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">nn</span></strong><span class="koboSpan" id="kobo.238.1"> module </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">torch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.242.1">
final_fc_layer = nn.Linear(10, 1)</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.243.1">Mean squared error</span></strong><span class="koboSpan" id="kobo.244.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.245.1">MSE</span></strong><span class="koboSpan" id="kobo.246.1">) is the </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.247.1">standard loss here to act as the error for optimization. </span><span class="koboSpan" id="kobo.247.2">However, one of the risks of unbounded numerical predictions is that values can skew out of acceptable bounds. </span><span class="koboSpan" id="kobo.247.3">If a bounded range is known, one way this can be enforced in an NN model is by using a scaling method such as min-max scaling to map target values into the known boundaries. </span><span class="koboSpan" id="kobo.247.4">Once min-max scaling is used, the target values will then be in the range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">0</span></strong><span class="koboSpan" id="kobo.249.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">1</span></strong><span class="koboSpan" id="kobo.251.1">. </span><span class="koboSpan" id="kobo.251.2">Coupled with the scaling of target values, the bounds can then be enforced in the NN by using the sigmoid layer, which similarly scales activation values between </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">0</span></strong><span class="koboSpan" id="kobo.253.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">1</span></strong><span class="koboSpan" id="kobo.255.1">. </span><span class="koboSpan" id="kobo.255.2">During the inference stage, the predicted values can then be mapped into actual values by descaling values between </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">0</span></strong><span class="koboSpan" id="kobo.257.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">1</span></strong><span class="koboSpan" id="kobo.259.1"> into the known minimum and maximum boundaries </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">specified earlier.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">The unbounded method allows for some form of generalization by allowing extrapolation, and the bounded method allows the addition of informed bias to the NN. </span><span class="koboSpan" id="kobo.261.2">Both methods have their own benefits and disadvantages, and thus the choice of approach needs to be evaluated on a </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">case-by-case basis.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">Next, we will dive into the implementation of </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">representation layers.</span></span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.265.1">Implementing representation layers</span></h2>
<p><span class="koboSpan" id="kobo.266.1">Most methods focus on the interactions </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.267.1">between architectures for different data modalities or the training methods that optimize the</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.268.1"> represented features. </span><span class="koboSpan" id="kobo.268.2">These are topics we will dive into further in the next topic after this. </span><span class="koboSpan" id="kobo.268.3">One key layer type that truly represents the representation layer is the embedding layer. </span><span class="koboSpan" id="kobo.268.4">Embeddings are a type of layer structure that maps categorical data types into learnable vectors. </span><span class="koboSpan" id="kobo.268.5">Through this layer, each category will be able to learn a representation that is able to perform well against the specified target. </span><span class="koboSpan" id="kobo.268.6">The method can be used for converting text word tokens into more representative features or plainly as a replacement for one-hot encoding. </span><span class="koboSpan" id="kobo.268.7">Categorical embeddings make it possible to automate the feature engineering process for categorical data types. </span><span class="koboSpan" id="kobo.268.8">One-hot encoding produces an encoding that enforces the same distance between all the categories to every other category. </span><span class="koboSpan" id="kobo.268.9">Categorical embedding, however, allows for the possibility of obtaining an appropriate distance based on its interactions with the target variable and with other data if any extra </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">data exists.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">However, categorical embeddings are also not a silver bullet for all ML use cases, even if they are decoupled from an actual NN model after training and just act as a featurizer. </span><span class="koboSpan" id="kobo.270.2">They can sometimes perform better against one-hot-encoding in general and, vice versa, can happen other times to perform worse against one-hot-encoding. </span><span class="koboSpan" id="kobo.270.3">The method still remains a key method to experiment with for any dataset with categorical data </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">as input.</span></span></p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.272.1">Training supervised deep learning models effectively</span></h1>
<p><span class="koboSpan" id="kobo.273.1">In </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.275.1">, </span><em class="italic"><span class="koboSpan" id="kobo.276.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.277.1">, it is </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.278.1">emphasized that ML projects have a cyclical life cycle. </span><span class="koboSpan" id="kobo.278.2">In other words, a lot of iterative processes are carried out in the course of the project’s lifetime. </span><span class="koboSpan" id="kobo.278.3">To train supervised deep learning models effectively, there are a lot of general directions that should be taken based on different conditions, but the one that absolutely stands out across every problem is proper tooling. </span><span class="koboSpan" id="kobo.278.4">The tooling is more commonly known </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.279.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.280.1">ML operations</span></strong><span class="koboSpan" id="kobo.281.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.282.1">MLOps</span></strong><span class="koboSpan" id="kobo.283.1">). </span><span class="koboSpan" id="kobo.283.2">Good MLOps systems for DL are easy to use and provide versioning methods for datasets and model experiments, visualization methods, easy ways to use DL libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">pytorch</span></strong><span class="koboSpan" id="kobo.285.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">keras</span></strong><span class="koboSpan" id="kobo.287.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">tensorflow</span></strong><span class="koboSpan" id="kobo.289.1">, ease of deployment, ease of model comparisons using different metrics, ease of model tuning, good visualization of model training monitoring, and, finally, good feedback about the progress (this can be sent through messages and notifications for alerts). </span><span class="koboSpan" id="kobo.289.2">If no advanced tools that truly simplify the entire process are at your disposal, you can focus on the important bits of making a model work well instead of dealing with infrastructure issues such as coordinating the saving of models into different folders like </span><strong class="bold"><span class="koboSpan" id="kobo.290.1">DataRobot</span></strong><span class="koboSpan" id="kobo.291.1">, a paid-for </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.292.1">tool, then open sourced tools such as MLflow, Kubeflow, or Metaflow will be the next-best alternative. </span><span class="koboSpan" id="kobo.292.2">Once the tool of choice is picked, carrying out training in DL models will be a breeze. </span><span class="koboSpan" id="kobo.292.3">We will be using MLflow as an example tool to demonstrate some effective methods for DL model training in the </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">following steps:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.294.1">Data preparation</span></span></li>
<li><span class="koboSpan" id="kobo.295.1">Configuring and tuning </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">DL hyperparameters</span></span></li>
<li><span class="koboSpan" id="kobo.297.1">Executing, visualizing, tracking, and </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">comparing experiments</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.299.1">Additionally, we will explore </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.300.1">some extra tips when building a model before ending </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">this topic.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">Let’s explore each </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">in detail.</span></span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.304.1">Preparing the data for DL training</span></h2>
<p><span class="koboSpan" id="kobo.305.1">Data is the core of any ML model. </span><span class="koboSpan" id="kobo.305.2">Data </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.306.1">ultimately determines the achievable model performance, the quality of the final trained model, and the validity of the final trained model. </span><span class="koboSpan" id="kobo.306.2">In </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.307.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.308.1">, </span><em class="italic"><span class="koboSpan" id="kobo.309.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.310.1">, we explored what it takes for a dataset setup to be DL-worthy, along with the qualities needed when acquiring data, coupled</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.311.1"> with </span><strong class="bold"><span class="koboSpan" id="kobo.312.1">exploratory data analysis</span></strong><span class="koboSpan" id="kobo.313.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.314.1">EDA</span></strong><span class="koboSpan" id="kobo.315.1">) to verify causality and validity. </span><span class="koboSpan" id="kobo.315.2">The general idea there was to identify and add extra features and data modalities that have causal effects toward the desired target. </span><span class="koboSpan" id="kobo.315.3">In this section, we will cover more in-depth essential steps that convert the data into a DL trainable state </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">listed here:</span></span></p>
<ol>
<li><span class="No-Break"><span class="koboSpan" id="kobo.317.1">Data partitioning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.318.1">Data representation</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.319.1">Data augmentation</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.320.1">Partitioning the data for DL training</span></h3>
<p><span class="koboSpan" id="kobo.321.1">The first step we will cover is</span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.322.1"> data partitioning. </span><span class="koboSpan" id="kobo.322.2">Having a good data-partitioning strategy for training, validating, and testing your model is essential for a good-performing model. </span><span class="koboSpan" id="kobo.322.3">The training partition will be the partition that will strictly be used for training. </span><span class="koboSpan" id="kobo.322.4">The validation partition will be the partition that will strictly be used for validating a model during </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.323.1">training. </span><span class="koboSpan" id="kobo.323.2">In DL, a </span><strong class="bold"><span class="koboSpan" id="kobo.324.1">validation partition</span></strong><span class="koboSpan" id="kobo.325.1"> is often used as a guide to signal when to stop training or extract the best-performing weights using external data out of the training data. </span><span class="koboSpan" id="kobo.325.2">Since the validation data will affect the learning process of the model and add some bias that will cause overfitting toward the nature of the out-of-training validation data, a testing partition will be the final partition that will be used to verify the generalizability of the trained model. </span><span class="koboSpan" id="kobo.325.3">The testing partition is also known as the holdout partition. </span><span class="koboSpan" id="kobo.325.4">To be extra safe in preventing overfitting and to ensure the generalizability of the model, the validation partition can also be used exclusively to be validated only once after the model is trained instead of being used for validation at every epoch. </span><span class="koboSpan" id="kobo.325.5">This strategy, however, requires that a smaller internal validation partition is created from the original training partition. </span><span class="koboSpan" id="kobo.325.6">The following figure depicts the two </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">different strategies:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.327.1"><img alt="Figure 8.2 – Two different cross-validation data-partitioning strategies" src="image/B18187_08_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.328.1">Figure 8.2 – Two different cross-validation data-partitioning strategies</span></p>
<p><span class="koboSpan" id="kobo.329.1">This process of partitioning the data is </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.330.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.331.1">cross-validation</span></strong><span class="koboSpan" id="kobo.332.1">. </span><span class="koboSpan" id="kobo.332.2">The preceding figure shows simple cross-validation strategies with only a single partitioning setting. </span><span class="koboSpan" id="kobo.332.3">This might create issues where the model’s performance metrics reported are biased toward a specific resulting partitioning setting. </span><span class="koboSpan" id="kobo.332.4">The resulting partitioning may have some inherent distribution or nature that allowed results to perform particularly well or badly toward it. </span><span class="koboSpan" id="kobo.332.5">When that happens, having mismatched expectations of performance during the deployment stage will create more operational issues. </span><span class="koboSpan" id="kobo.332.6">To safely remove the possibility of such </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.333.1">bias, </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">k-fold cross-validation</span></strong><span class="koboSpan" id="kobo.335.1"> is typically used to report a more comprehensive validation score that could better </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.336.1">reflect the performance of the model in the wild. </span><span class="koboSpan" id="kobo.336.2">To perform this partitioning method, a single testing set is removed from the original dataset, and validation scores are averaged across different ordered </span><em class="italic"><span class="koboSpan" id="kobo.337.1">k</span></em><span class="koboSpan" id="kobo.338.1"> cross-validation training and validation partitions. </span><span class="koboSpan" id="kobo.338.2">This is better visualized in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.339.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.340.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.342.1"><img alt="Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting bias" src="image/B18187_08_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.343.1">Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting bias</span></p>
<p><span class="koboSpan" id="kobo.344.1">Finally, for testing performance reporting and deployment purposes, the model either gets retrained on the training and validation dataset combined or the model trained in the first fold is extracted for </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">deployment purposes.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">Recall that stratified partitioning is a recommended strategy to split your data into the three mentioned partitions. </span><span class="koboSpan" id="kobo.346.2">This means that the data will be approximately evenly separated into three partitions based on the label associated with the dataset. </span><span class="koboSpan" id="kobo.346.3">This ensures that no labels get left out in any partition, which could potentially cause misinformation. </span><span class="koboSpan" id="kobo.346.4">Take a simple case of binary classification where the dataset is randomly partitioned into three partitions of training, validation, and testing with prespecified sizes. </span><span class="koboSpan" id="kobo.346.5">Since each data row was randomly placed into one of the three partitions, there is a probability the partitions will only contain one label from the two binary labels. </span><span class="koboSpan" id="kobo.346.6">Since the validation or testing partition is usually assigned with smaller data sizes, they have more potential to face this issue. </span><span class="koboSpan" id="kobo.346.7">Let’s say the model is mistakenly trained to predict only a single label. </span><span class="koboSpan" id="kobo.346.8">If the label is exactly the label that exclusively exists in the validation and testing of ML learning practitioners, we will mistakenly think the model is doing extremely well, but in fact, it is useless. </span><span class="koboSpan" id="kobo.346.9">You never know when you will be unlucky when doing full random partitioning, so use stratified random partitioning whenever </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">you can!</span></span></p>
<p><span class="koboSpan" id="kobo.348.1">The data-partitioning strategy described here builds only a single model that will be utilized during inference mode in model deployment. </span><span class="koboSpan" id="kobo.348.2">This strategy is the standard option when the inference runtime of the final model setup is a concern and having a faster model is more important than having small improvements in the accuracy metrics. </span><span class="koboSpan" id="kobo.348.3">When it is okay to trade runtime for some accuracy performance, an alternative strategy</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.349.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">k-fold cross-validation ensemble</span></strong><span class="koboSpan" id="kobo.351.1"> can be used. </span><span class="koboSpan" id="kobo.351.2">This is a method that is widely advocated in many ML competitions, especially in the ones hosted on Kaggle. </span><span class="koboSpan" id="kobo.351.3">The method uses the </span><em class="italic"><span class="koboSpan" id="kobo.352.1">k</span></em><span class="koboSpan" id="kobo.353.1">-fold cross-validation described previously but actually uses </span><em class="italic"><span class="koboSpan" id="kobo.354.1">k</span></em><span class="koboSpan" id="kobo.355.1"> models trained during cross-validation and performs an ensemble of the k model’s predictions. </span><span class="koboSpan" id="kobo.355.2">An ensembling method called blending aggregates predictions of models and almost always improves the accuracy metrics from a single model. </span><span class="koboSpan" id="kobo.355.3">This process can be thought of as a method that leverages the best ideas and expertise of each </span><em class="italic"><span class="koboSpan" id="kobo.356.1">k</span></em><span class="koboSpan" id="kobo.357.1"> model, making the final outcome better as an aggregate. </span><span class="koboSpan" id="kobo.357.2">This aggregate can be as simple as an average or median of the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.358.1">k</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.359.1"> predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">A final tip before moving on to the next method is to always remember to make sure partitions match when comparing models between experiments. </span><span class="koboSpan" id="kobo.360.2">Misinformation often happens in the field when two models are separately </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.361.1">developed using different data-partitioning strategies and data partitions. </span><span class="koboSpan" id="kobo.361.2">Even when one of the models achieves a significant performance advantage over the other, it does not mean anything and will not amount to any </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">meaningful comparisons.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">Next, we will dive into the data representation component for different </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">data modalities.</span></span></p>
<h3><span class="koboSpan" id="kobo.365.1">Representing different data modalities for training DL models</span></h3>
<p><span class="koboSpan" id="kobo.366.1">So far, we have brushed over the </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.367.1">utilization of numerical, categorical, text, audio, image, and video modalities. </span><span class="koboSpan" id="kobo.367.2">These are the most common modalities utilized across multiple industries. </span><span class="koboSpan" id="kobo.367.3">Representing different data modalities is a complicated topic as, in addition to the common modalities, there are actually a lot of rare data modalities out there. </span><span class="koboSpan" id="kobo.367.4">Examples of rare modalities are chemical formulas (a special structured form of textual data), document data (another special form of textual data with complex positional information), and graph data. </span><span class="koboSpan" id="kobo.367.5">In this section, we will only discuss the representation of the common unstructured modalities here to ensure the relevancy of content to our readers. </span><span class="koboSpan" id="kobo.367.6">Both numerical and categorical </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.368.1">data are considered structured data and are have been covered properly in previous sections. </span><span class="koboSpan" id="kobo.368.2">Let’s now start with the text </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">data modality.</span></span></p>
<h4><span class="koboSpan" id="kobo.370.1">Representing text data for supervised deep learning</span></h4>
<p><span class="koboSpan" id="kobo.371.1">Text data representation in</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.372.1"> general has </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.373.1">improved tremendously over the years. </span><span class="koboSpan" id="kobo.373.2">The following list shows a few </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">relevant methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.375.1">Term frequency-inverse document frequency (TF-IDF) with N-grams</span></strong><span class="koboSpan" id="kobo.376.1">: The term here is implemented with</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.377.1"> N-grams. </span><span class="koboSpan" id="kobo.377.2">An N-gram is an adjacent sequence of </span><em class="italic"><span class="koboSpan" id="kobo.378.1">n</span></em><span class="koboSpan" id="kobo.379.1"> textual characters. </span><span class="koboSpan" id="kobo.379.2">N-grams are produced by a method called tokenization. </span><span class="koboSpan" id="kobo.379.3">The tokenization can be a representation as low level as single characters, or it can be a higher-level representation such as words. </span><span class="koboSpan" id="kobo.379.4">Once represented as N-grams, TF-IDF is computed using the </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">following formula.</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.381.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.382.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.383.1">-</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.384.1">I</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.385.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.386.1">F</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.387.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.388.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.389.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.390.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.391.1">m</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.392.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.393.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.394.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.395.1">q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.396.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.397.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.398.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.399.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.400.1">y</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.401.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.402.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.403.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.404.1">v</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.405.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.406.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.407.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.408.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.409.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.410.1">o</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.411.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.412.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.413.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.414.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.415.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.416.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.417.1">f</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.418.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.419.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.420.1">q</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.421.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.422.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.423.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.424.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.425.1">y</span></span></span></p>
<p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.426.1">Term frequency</span></strong><span class="koboSpan" id="kobo.427.1"> is simply the count array of a </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.428.1">single row. </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">Inverse document frequency</span></strong><span class="koboSpan" id="kobo.430.1"> is computed through the </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.432.1">I</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.433.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.434.1">F</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.435.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Function_v-normal"><span class="koboSpan" id="kobo.436.1">log</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.437.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.438.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.439.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.440.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.441.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.442.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.443.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.444.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.445.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.446.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.447.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.448.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.449.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.450.1">t</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.451.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.452.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.453.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.454.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.455.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.456.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.457.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.458.1">   </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.459.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.460.1">_____________________________________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.461.1">    </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.462.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.463.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.464.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.465.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.466.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.467.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.468.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.469.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.470.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.471.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.472.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.473.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.474.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.475.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.476.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.477.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.478.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.479.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.480.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.481.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.482.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.483.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.484.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.485.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.486.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.487.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.488.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.489.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.490.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.491.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.492.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.493.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.494.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.495.1">m</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.496.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.497.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.498.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.499.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.500.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.501.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.502.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.503.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.504.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.505.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.506.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.507.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.508.1">)</span></span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.509.1">The representation is an efficient and lightweight way to extract useful information from text, where words that are rare have higher values and more frequent words such as “the” and “and” will be suppressed. </span><span class="koboSpan" id="kobo.509.2">Outputs of TF-IDF can be directly fed into a simple </span><strong class="bold"><span class="koboSpan" id="kobo.510.1">multilayer perceptron</span></strong><span class="koboSpan" id="kobo.511.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.512.1">MLP</span></strong><span class="koboSpan" id="kobo.513.1">) or any </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.514.1">ML model to produce a predictive model. </span><span class="koboSpan" id="kobo.514.2">In simpler use cases, this representation will be enough to achieve a good metric performance. </span><span class="koboSpan" id="kobo.514.3">However, in more complex use cases that require the decoding of complex interactions that can happen with the different compositions of text and labels, it </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">will underperform.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.516.1">Word/token embeddings</span></strong><span class="koboSpan" id="kobo.517.1">: Word </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.518.1">embeddings can be trained from scratch or pre-trained from a bigger dataset. </span><span class="koboSpan" id="kobo.518.2">Pre-trained embeddings can be pre-trained in either a supervised fashion or an unsupervised fashion, usually on a larger dataset. </span><span class="koboSpan" id="kobo.518.3">However, the embedding method suffers from the issue of token mismatch during the training, evaluation, and testing stages. </span><span class="koboSpan" id="kobo.518.4">This means that it is required to perform a lot of tinkering with the way the specific text token is preprocessed before looking up to the embeddings table. </span><span class="koboSpan" id="kobo.518.5">This occurrence is known as </span><strong class="bold"><span class="koboSpan" id="kobo.519.1">out of vocabulary</span></strong><span class="koboSpan" id="kobo.520.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.521.1">OOV</span></strong><span class="koboSpan" id="kobo.522.1">) during</span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.523.1"> the evaluation and testing stages. </span><span class="koboSpan" id="kobo.523.2">In the training stage, different variations of the same word will have their own meaning, which is inefficient in terms of learning and resource-space utilization. </span><span class="koboSpan" id="kobo.523.3">In practice, methods such as stemming, lemmatization, lowercasing, and known word-to-word replacements are applied to mitigate OOV, but the problem won’t be mitigated completely. </span><span class="koboSpan" id="kobo.523.4">These word embeddings can be paired with either </span><strong class="bold"><span class="koboSpan" id="kobo.524.1">recurrent NNs</span></strong><span class="koboSpan" id="kobo.525.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.526.1">RNNs</span></strong><span class="koboSpan" id="kobo.527.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">or </span></span><span class="No-Break"><a id="_idIndexMarker621"/></span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">transformers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.530.1">Subword-based tokenization</span></strong><span class="koboSpan" id="kobo.531.1">: This </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.532.1">family of methods attempts to solve the token mismatch issue and the large vocabulary size of tokens. </span><em class="italic"><span class="koboSpan" id="kobo.533.1">Subword</span></em><span class="koboSpan" id="kobo.534.1"> might sound unintuitive, as we as humans use full words to perceive the meaning of text. </span><span class="koboSpan" id="kobo.534.2">This family of algorithms only performs subword tokenization when the word can’t be identified or is considered to be rare. </span><span class="koboSpan" id="kobo.534.3">For common words, they will remain full word tokens. </span><span class="koboSpan" id="kobo.534.4">Examples of such methods </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.535.1">include </span><strong class="bold"><span class="koboSpan" id="kobo.536.1">byte-pair encoding</span></strong><span class="koboSpan" id="kobo.537.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.538.1">BPE</span></strong><span class="koboSpan" id="kobo.539.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.540.1">WordPiece</span></strong><span class="koboSpan" id="kobo.541.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.542.1">SentencePiece</span></strong><span class="koboSpan" id="kobo.543.1">. </span><span class="koboSpan" id="kobo.543.2">We </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.544.1">will go</span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.545.1"> through these methods briefly as a </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">simple guide:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.547.1">BPE tokenization</span></strong><span class="koboSpan" id="kobo.548.1">: BPE treats the text as characters and groups up the most common consecutive characters iteratively during training. </span><span class="koboSpan" id="kobo.548.2">The number of iterations determines when the training iterations to group up the most common characters should be stopped. </span><span class="koboSpan" id="kobo.548.3">This formulation allows for rare words to remain as subword tokens and common words to be grouped up into a single token. </span><span class="koboSpan" id="kobo.548.4">This representation is notably used by </span><strong class="bold"><span class="koboSpan" id="kobo.549.1">generative pre-trained transformer</span></strong><span class="koboSpan" id="kobo.550.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.551.1">GPT</span></strong><span class="koboSpan" id="kobo.552.1">) models. </span><span class="koboSpan" id="kobo.552.2">However, this </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.553.1">representation faces an issue where there will be multiple ways to encode a </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">particular word.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.555.1">WordPiece</span></strong><span class="koboSpan" id="kobo.556.1">: WordPiece improves upon BPE by utilizing a language model to choose the most likely pair of tokens to group up. </span><span class="koboSpan" id="kobo.556.2">This enforces a kind of intelligent choice when deciding the way to encode a particular word. </span><span class="koboSpan" id="kobo.556.3">This algorithm</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.557.1"> is utilized</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.558.1"> by </span><strong class="bold"><span class="koboSpan" id="kobo.559.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.560.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.561.1">BERT</span></strong><span class="koboSpan" id="kobo.562.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.563.1">Efficiently Learning an Encoder that Classifies Token Replacements </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.564.1">Accurately</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.565.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.566.1">ELECTRA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">).</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.568.1">SentencePiece</span></strong><span class="koboSpan" id="kobo.569.1">: SentencePiece is a method that optimizes the tokens generated by base tokenizers such as BPE. </span><span class="koboSpan" id="kobo.569.2">It uses a couple of components, such as using a form of Unicode text conversion to ensure no language-dependent logic exists and using a method called subword regularization that performs a form of subword token group augmentation (probabilistically and randomly choose a single sample from the top-k predicted subword token to group up using language models) to solve multiple representation issues. </span><span class="koboSpan" id="kobo.569.3">This algorithm is used by XLNet and </span><strong class="bold"><span class="koboSpan" id="kobo.570.1">A Lite BERT</span></strong><span class="koboSpan" id="kobo.571.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.572.1">ALBERT</span></strong><span class="koboSpan" id="kobo.573.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">most </span></span><span class="No-Break"><a id="_idIndexMarker629"/></span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">notably.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.576.1">Text data are represented as tokens for DL. </span><span class="koboSpan" id="kobo.576.2">This also means that there will be strict limits to the number of tokens so that the NN model can be initialized with the right parameters. </span><span class="koboSpan" id="kobo.576.3">Pick a token size limit that is reasonable for your use case based on what’s needed to get a good model performance. </span><span class="koboSpan" id="kobo.576.4">Since the size limit will affect model size, be sure to make sure the model size doesn’t get so big that it overshoots your inference </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">runtime requirements.</span></span></p>
<p><span class="koboSpan" id="kobo.578.1">In terms of missing text data, which can happen in real-world use cases with multimodal data, using an empty string is the most natural way to work as an imputation method. </span><span class="koboSpan" id="kobo.578.2">Under the hood, these models would usually use all zeros to </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.579.1">represent </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.580.1">the </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">text array.</span></span></p>
<p><span class="koboSpan" id="kobo.582.1">Now that we’ve briefly covered supervised text data representations, let’s discover supervised audio data </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">representations next.</span></span></p>
<h4><span class="koboSpan" id="kobo.584.1">Representing audio data for DL</span></h4>
<p><span class="koboSpan" id="kobo.585.1">Audio data is time-series </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.586.1">data with one</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.587.1"> or two arrays of values where each value in the array represents a single piece of audio data at a specific timestamp. </span><span class="koboSpan" id="kobo.587.2">Audio data can be either represented as a simple normalized form from the original raw data, represented as something called a spectrogram, which is a two-dimensional data, or</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.588.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">Mel-frequency cepstral </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.590.1">coefficients</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.591.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.592.1">MFCCs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.594.1">A spectrogram is the resulting output of a process</span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.595.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.596.1">short-time Fourier transform</span></strong><span class="koboSpan" id="kobo.597.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.598.1">STFT</span></strong><span class="koboSpan" id="kobo.599.1">) that can decompose raw audio data into its respective signal across a range of frequencies. </span><span class="koboSpan" id="kobo.599.2">Raw audio data contains combined audio signals that can be from any audio frequency. </span><span class="koboSpan" id="kobo.599.3">Breaking down audio data into its signal-by-frequency representation allows ML algorithms to have an additional degree of freedom when attempting to identify key patterns. </span><span class="koboSpan" id="kobo.599.4">In most audio use</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.600.1"> cases, </span><strong class="bold"><span class="koboSpan" id="kobo.601.1">spectrogram</span></strong><span class="koboSpan" id="kobo.602.1">-based models perform much better than using raw audio data directly. </span><span class="koboSpan" id="kobo.602.2">Spectrograms exist in two-dimensional format. </span><span class="koboSpan" id="kobo.602.3">This means that it can be treated</span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.603.1"> as an image and can be fed into image-based models such as </span><strong class="bold"><span class="koboSpan" id="kobo.604.1">Residual Networks</span></strong><span class="koboSpan" id="kobo.605.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.606.1">ResNets</span></strong><span class="koboSpan" id="kobo.607.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.608.1">Vision Transformers</span></strong><span class="koboSpan" id="kobo.609.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.610.1">ViTs</span></strong><span class="koboSpan" id="kobo.611.1">). </span><span class="koboSpan" id="kobo.611.2">However, there</span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.612.1"> are some models that are capable of taking advantage of the raw unprocessed audio data, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">wav2vec 2.0</span></strong><span class="koboSpan" id="kobo.614.1">, which is a type </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">of transformer.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">The STFT process has hyperparameters that can affect the resulting representation. </span><span class="koboSpan" id="kobo.616.2">The following list summarizes these hyperparameters </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.617.1">and tips on how to set it </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">up properly:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Sampling rate</span></strong><span class="koboSpan" id="kobo.620.1">: This specifies the samples per second (Hertz/Hz) parameter that will be used before applying STFT. </span><span class="koboSpan" id="kobo.620.2">Audio data might be recorded in different sampling rates, and to build a model, this data will be required to be unified to a single sampling rate through resampling algorithms. </span><span class="koboSpan" id="kobo.620.3">The most typically used value is 16,000 Hz. </span><span class="koboSpan" id="kobo.620.4">As this value will affect the size and runtime of the model given a fixed time window a model is built to handle, be sure to only increase it if it’s necessary in terms of </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">metric performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.622.1">STFT window length</span></strong><span class="koboSpan" id="kobo.623.1">: Each window size will be responsible for the data for a fixed duration and specific position. </span><span class="koboSpan" id="kobo.623.2">Each window will produce a single value at the same time window for a range of frequencies. </span><span class="koboSpan" id="kobo.623.3">The typical value of this parameter is 4096 or 2048. </span><span class="koboSpan" id="kobo.623.4">Configure this based on the prediction resolution you require for your use case if there is a strict requirement there. </span><span class="koboSpan" id="kobo.623.5">This parameter will also affect the </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">model size.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Window stride</span></strong><span class="koboSpan" id="kobo.626.1">: This is similar to a convolutional layer filter stride and does not have many significant tuning methods. </span><span class="koboSpan" id="kobo.626.2">Using a small percentage of the window length, such as 10%, should be a good </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">enough setting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.628.1">Whether to use Mel scaling</span></strong><span class="koboSpan" id="kobo.629.1">: A </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">Mel scale</span></strong><span class="koboSpan" id="kobo.631.1"> is a logarithmic transformation of the audio signal’s frequency. </span><span class="koboSpan" id="kobo.631.2">Fundamentally, it is a transformation to mimic how humans perceive audio. </span><span class="koboSpan" id="kobo.631.3">It makes higher-frequency changes matter less and lower-frequency changes matter more. </span><span class="koboSpan" id="kobo.631.4">Use this when it involves some form of human judgment to improve the </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">metric performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.633.1">As for empty audio rows, imputing them with a single pre-generated random noise audio or using an array of zeros with the same length should work well in </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">multimodal datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.635.1">Now that we’ve briefly covered supervised audio data representations, let’s discover </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.636.1">supervised image and video data </span><a id="_idIndexMarker641"/><span class="No-Break"><span class="koboSpan" id="kobo.637.1">representations next.</span></span></p>
<h4><span class="koboSpan" id="kobo.638.1">Representing image and video data for DL</span></h4>
<p><span class="koboSpan" id="kobo.639.1">Image data doesn’t require</span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.640.1"> a lot of </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.641.1">introduction here as we have gone through a few tutorials using them directly. </span><span class="koboSpan" id="kobo.641.2">The key is to perform some sort of normalization before feeding it to NN models such as CNNs, and the NN will extract great representations. </span><span class="koboSpan" id="kobo.641.3">Transformers have also been making tight competition with CNNs in image-based tasks and can be used to both extract representative features and predict directly on the task. </span><span class="koboSpan" id="kobo.641.4">One thing to note is that unless the resolution of the image is crucial in identifying certain patterns, it is usually much more effective as a model to utilize a smaller image resolution. </span><span class="koboSpan" id="kobo.641.5">One might not be able to visually certain patterns when the resolution of the image is smaller, but a computer would still be able to. </span><span class="koboSpan" id="kobo.641.6">The resolution of the image affects the runtime of the training, the runtime of the model in production, and sometimes the model size, as for transformers, so make sure this is </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">done conservatively.</span></span></p>
<p><span class="koboSpan" id="kobo.643.1">Video data, however, is an extended form of image data where a number of images are aligned sequentially to form a video. </span><span class="koboSpan" id="kobo.643.2">This means that video data is a form of sequential data just like text without absolute timestamp information. </span><span class="koboSpan" id="kobo.643.3">Each sequential image is known </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.644.1">as a </span><strong class="bold"><span class="koboSpan" id="kobo.645.1">frame</span></strong><span class="koboSpan" id="kobo.646.1">. </span><span class="koboSpan" id="kobo.646.2">Video can have a variety of frame rates. </span><span class="koboSpan" id="kobo.646.3">Commonly, this would be in rates of 24, 30, or 48 </span><strong class="bold"><span class="koboSpan" id="kobo.647.1">frames per second</span></strong><span class="koboSpan" id="kobo.648.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.649.1">FPS</span></strong><span class="koboSpan" id="kobo.650.1">) but </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.651.1">can generally be any number. </span><span class="koboSpan" id="kobo.651.2">For </span><strong class="bold"><span class="koboSpan" id="kobo.652.1">computer vision</span></strong><span class="koboSpan" id="kobo.653.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.654.1">CV</span></strong><span class="koboSpan" id="kobo.655.1">) use</span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.656.1"> cases, make sure to set a low FPS so that the processing load can be reduced depending on the use case. </span><span class="koboSpan" id="kobo.656.2">For example, the use case of lip reading has lower FPS requirements than for asking a model to identify whether a person is running or not. </span><span class="koboSpan" id="kobo.656.3">For the frame resolution, the same guide for image resolution applies here. </span><span class="koboSpan" id="kobo.656.4">Once the video properties have been decided, representative features have to be learned and extracted. </span><span class="koboSpan" id="kobo.656.5">The current SoTA features are extracted through models similar to image-based use cases. </span><span class="koboSpan" id="kobo.656.6">Examples of such models are 3D CNNs and </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">3D transformers.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">There is, however, an intersection of these two data types, which are images extracted through video data. </span><span class="koboSpan" id="kobo.658.2">For this type of image data, it is possible to reduce the probability of predictive models making wrong predictions. </span><span class="koboSpan" id="kobo.658.3">ML models are not perfect predictors, so whenever there is a chance to reduce incorrect predictions such as false positives or false negatives without compromising the true predictions, do consider taking it. </span><span class="koboSpan" id="kobo.658.4">Consider using manual image processing techniques from the OpenCV library to perform any preliminary steps before a model takes the image as input. </span><span class="koboSpan" id="kobo.658.5">For example, the motion detection technique in OpenCV can be used as a preliminary condition checker before feeding the video array to the DL model. </span><span class="koboSpan" id="kobo.658.6">Since motion is required to identify most use cases of video data, it doesn’t make sense to predict anything if nothing is moving. </span><span class="koboSpan" id="kobo.658.7">This also reduces any false predictions that can happen from predicting on multiple unchanged video frames. </span><span class="koboSpan" id="kobo.658.8">The motion detector in OpenCV utilizes a simple change in pixel value without using a probabilistic model and thus is a far more reliable indicator </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">of motion.</span></span></p>
<p><span class="koboSpan" id="kobo.660.1">Now that we’ve covered representing different data modalities, let’s move on to the topic of </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">data augmentation.</span></span></p>
<h3><span class="koboSpan" id="kobo.662.1">Augmenting the data for training better DL models</span></h3>
<p><span class="koboSpan" id="kobo.663.1">Augmentation is widely used </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.664.1">in DL to increase the generalization of the resulting trained model and increase the metric performance of the model. </span><span class="koboSpan" id="kobo.664.2">By accounting for the additional unique variations brought in by augmentation, the model would be able to attend to these unique variations on external data during the validation, testing, and inference stage. </span><span class="koboSpan" id="kobo.664.3">Naturally, this will also reduce any over-dependence on a specific pattern and any benefits that come with a sufficiently sized dataset. </span><span class="koboSpan" id="kobo.664.4">Augmentation increases the amount of training data and thus the variations of patterns in the training data. </span><span class="koboSpan" id="kobo.664.5">The process is usually done randomly and individually in every training iteration in memory. </span><span class="koboSpan" id="kobo.664.6">This makes sure there are no limitations to the additional data variations available for training and also removes the need for additional storage. </span><span class="koboSpan" id="kobo.664.7">However, you can’t just randomly use all the available types of augmentation known for a specific modality. </span><span class="koboSpan" id="kobo.664.8">The following list shows the different types of augmentation you can perform on </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">your data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.666.1">Image</span></strong><span class="koboSpan" id="kobo.667.1">: Image sharpening through </span><strong class="bold"><span class="koboSpan" id="kobo.668.1">Contrast Limited Adaptive Histogram Equalization</span></strong><span class="koboSpan" id="kobo.669.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.670.1">CLAHE</span></strong><span class="koboSpan" id="kobo.671.1">), hue</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.672.1"> and saturation variation, color channel shuffling, contrast variation, brightness variation, horizontal/vertical flip, grayscale conversion, blurring, image masking, mixup (weighted combination of images and their labels), cutmix (mixup, but only by random patches from the original image), and more. </span><span class="koboSpan" id="kobo.672.2">Look into </span><a href="https://github.com/albumentations-team/albumentations"><span class="koboSpan" id="kobo.673.1">https://github.com/albumentations-team/albumentations</span></a><span class="koboSpan" id="kobo.674.1"> for more than </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">70 augmentations!</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.676.1">Text</span></strong><span class="koboSpan" id="kobo.677.1">: Synonym replacement, back translation (a process that translates a text into another language and then translates it back into the original language), </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">and more.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.679.1">Video</span></strong><span class="koboSpan" id="kobo.680.1">: Video mixup (same as image mixup but for videos), all the same augmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">for images.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.682.1">Audio</span></strong><span class="koboSpan" id="kobo.683.1">: Random white noise, random pink noise, reverberation, stretching the time, back resampling (similar to back translation, but changing the sampling rate instead), random STFT window types, random masking, random bandpass noise, additional noise from randomly chosen real audio clips (you can either get this from YouTube or from permissive public audio datasets). </span><span class="koboSpan" id="kobo.683.2">These can be accessed through the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">librosa</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.685.1"> library.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.686.1">Choosing the type of augmentation to use requires some understanding of the expected environment that you will face when you deploy a model. </span><span class="koboSpan" id="kobo.686.2">Bad choices add noise to the model and might confuse the model during the training process, resulting in a degraded metric performance. </span><span class="koboSpan" id="kobo.686.3">Good choices revolve around estimating the variations that can realistically happen in the wild. </span><span class="koboSpan" id="kobo.686.4">Let’s take an example of a manufacturing use case where the goal is to deploy an image-based model that will predict product characteristics on a conveyor belt for sorting purposes using a camera sensor. </span><span class="koboSpan" id="kobo.686.5">If you can assume the camera will be fixed almost perfectly straight on the machine in all the setups, using image rotation augmentation likely wouldn’t be smart. </span><span class="koboSpan" id="kobo.686.6">Even if you want to use the augmentation, the rotation variation you should use should only be in the low end, such as below 10 degrees variation. </span><span class="koboSpan" id="kobo.686.7">Grayscale augmentation would also be unintuitive if the cameras do not provide </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">grayscale images.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">This concludes the data </span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.689.1">preparation stage of training effective models. </span><span class="koboSpan" id="kobo.689.2">Next, we will dive into the model training stage of </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">the workflow.</span></span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.691.1">Configuring and tuning DL hyperparameters</span></h2>
<p><span class="koboSpan" id="kobo.692.1">Hyperparameter </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.693.1">configuration and tuning play a crucial role in training DL models effectively. </span><span class="koboSpan" id="kobo.693.2">They control the learning process of the model and can significantly impact the model’s performance, generalization, and convergence. </span><span class="koboSpan" id="kobo.693.3">In this section, we will discuss some essential hyperparameters and their impact on training </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">DL models.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">The most general set of impactful hyperparameters that need to be configured for training each NN model are its epochs, early stopping epochs, and learning rate. </span><span class="koboSpan" id="kobo.695.2">These three parameters are considered a set of parameters that together form </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.696.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.697.1">learning schedule</span></strong><span class="koboSpan" id="kobo.698.1">. </span><span class="koboSpan" id="kobo.698.2">There have been a few notable learning schedules that focused on obtaining the best model with the least amount of time spent. </span><span class="koboSpan" id="kobo.698.3">However, they depend a lot on the initial estimation of the total number of epochs a model can converge with the method. </span><span class="koboSpan" id="kobo.698.4">Methods that depend on an estimation of the total number of epochs needed are fragile, and their configuration strategies are not easily transferable from one problem to another. </span><span class="koboSpan" id="kobo.698.5">Here, we will focus on using a validation dataset to track the number of epochs needed to achieve the best possible </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">metric performance.</span></span></p>
<p><span class="koboSpan" id="kobo.700.1">Early stopping epochs is a parameter that controls how many epochs you want to keep training before you stop. </span><span class="koboSpan" id="kobo.700.2">This strategy means that the epochs’ hyperparameter can either be set to an infinite number or a very large number so that the best-performing model on the validation dataset can be found. </span><span class="koboSpan" id="kobo.700.3">Early stopping reduces the number of training epochs you need to spend dynamically based on the validation dataset. </span><span class="koboSpan" id="kobo.700.4">By saving the best-performing model weights on the validation dataset, when the model is stopped early, you will then be able to load the best-performing weights. </span><span class="koboSpan" id="kobo.700.5">The typical early stopping epoch </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">10</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.704.1">As for the learning rate, there are two general directions that work consistently well in practice. </span><span class="koboSpan" id="kobo.704.2">One is to immediately start with a large learning rate such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.705.1">0.1</span></strong><span class="koboSpan" id="kobo.706.1"> and to gradually decay the learning rate. </span><span class="koboSpan" id="kobo.706.2">The gradual decay of the learning rate can be through percentage reductions from validation score monitoring when it doesn’t improve after </span><strong class="source-inline"><span class="koboSpan" id="kobo.707.1">3</span></strong><span class="koboSpan" id="kobo.708.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.709.1">5</span></strong><span class="koboSpan" id="kobo.710.1"> epochs. </span><span class="koboSpan" id="kobo.710.2">The second method is to use a smaller learning rate as a warmup method in initializing a base weight for the NN before using method 1. </span><span class="koboSpan" id="kobo.710.3">In the initial stage of learning, as models are initially in a randomized state, the learning process can be very unstable where the loss will seem to not follow a proper improvement trend. </span><span class="koboSpan" id="kobo.710.4">Using a warmup helps to initialize the foundation needed to make stable loss progressions. </span><span class="koboSpan" id="kobo.710.5">Note that if pre-trained weights are used to initialize the model, a warmup is usually not needed as the model will already be in a stable state, especially if the pre-trained weights are obtained from a </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">similar dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.712.1">Batch size is another crucial hyperparameter in the training of DL models, as it determines the number of training samples used in a single update of the model’s weights during the optimization process. </span><span class="koboSpan" id="kobo.712.2">The choice of batch size can significantly impact the model’s training speed, memory requirements, and convergence. </span><span class="koboSpan" id="kobo.712.3">Smaller batch sizes, such as 16 or 32, provide a more accurate estimate of the gradient, leading to more stable convergence, but may require more training iterations and can be slower due to less parallelism in computation. </span><span class="koboSpan" id="kobo.712.4">On the other hand, larger batch sizes, such as 128 or 256, increase the level of parallelism, speeding up the training process and reducing memory requirements, but may lead to a less accurate gradient estimate and potentially less stable convergence. </span><span class="koboSpan" id="kobo.712.5">In practice, it’s essential to experiment with different batch sizes to find the one that provides the best balance between training speed and convergence stability for your specific problem. </span><span class="koboSpan" id="kobo.712.6">Additionally, modern DL frameworks often support adaptive batch size techniques, which can automatically adjust the batch size during training to optimize the </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">learning process.</span></span></p>
<p><span class="koboSpan" id="kobo.714.1">The foundational strategy discussed here is robust and can easily obtain the best-performing model most of the time while sacrificing some additional training time. </span><span class="koboSpan" id="kobo.714.2">It is worth noting that regularization methods, optimizers, and different activation functions have been covered in </span><a href="B18187_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.715.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.716.1">, </span><em class="italic"><span class="koboSpan" id="kobo.717.1">Designing Deep Learning Architectures</span></em><span class="koboSpan" id="kobo.718.1">, and I encourage you to refer to that chapter for more information on </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">those topics.</span></span></p>
<p><span class="koboSpan" id="kobo.720.1">As much as there can be a manual strategy for the configuration of these hyperparameters, there will always be space to tune the hyperparameters further to optimize the metric performance of the model. </span><span class="koboSpan" id="kobo.720.2">Commonly, tuning can be executed through either grid search, random search, or tuning through more intelligent searching mechanisms. </span><span class="koboSpan" id="kobo.720.3">Grid search, otherwise known as brute-force searching, explores and validates all possible combinations of specified hyperparameter values to identify the optimal configuration for a given problem through cross-validation. </span><span class="koboSpan" id="kobo.720.4">For more intelligent tuning methods, refer back to </span><a href="B18187_07.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.721.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.722.1">, </span><em class="italic"><span class="koboSpan" id="kobo.723.1">Deep Neural Architecture Search</span></em><span class="koboSpan" id="kobo.724.1">, for more insights on it. </span><span class="koboSpan" id="kobo.724.2">Additionally, as model evaluation metrics contribute to this hyperparameter-tuning process, we will explore more on this in </span><a href="B18187_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.725.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.726.1">, </span><em class="italic"><span class="koboSpan" id="kobo.727.1">Exploring Model </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.728.1">Evaluation Methods</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.730.1">The core of hyperparameter tuning depends on the process and workflow to iteratively execute, visualize, track, and compare modeling experiments, with </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.731.1">each configuration being part of a modeling experiment. </span><span class="koboSpan" id="kobo.731.2">This brings us to the next topic, diving into the actual workflow of training DL </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">models effectively.</span></span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.733.1">Executing, visualizing, tracking, and comparing experiments</span></h2>
<p><span class="koboSpan" id="kobo.734.1">The key to executing ML projects </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.735.1">effectively </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.736.1">is to </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.737.1">iterate </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.738.1">quickly between experiments. </span><span class="koboSpan" id="kobo.738.2">A lot of exploration is needed in any ML project, both in the initial stage of the project to gauge the viability of the use case, and in the later stage to improve the model’s performance. </span><span class="koboSpan" id="kobo.738.3">When this exploration process can be optimized, things meant to fail can fail quickly, and things that are viable can succeed quickly. </span><span class="koboSpan" id="kobo.738.4">Failure in ML projects is very common in practice. </span><span class="koboSpan" id="kobo.738.5">Once we acknowledge that and fail quickly, we can utilize the recovered time to tackle more valuable use cases. </span><span class="koboSpan" id="kobo.738.6">A good MLOps platform will help us execute, visualize, track, and compare experiments effectively </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">and efficiently.</span></span></p>
<p><span class="koboSpan" id="kobo.740.1">Let’s go through an example practically with the Iris dataset and an MLP using the MLflow MLOps platform. </span><span class="koboSpan" id="kobo.740.2">We will also be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.741.1">catalyst</span></strong><span class="koboSpan" id="kobo.742.1"> library, which is also considered to be an MLOps platform, albeit partially and mostly focused on providing common </span><strong class="source-inline"><span class="koboSpan" id="kobo.743.1">pytorch</span></strong><span class="koboSpan" id="kobo.744.1"> DL model training tools. </span><span class="koboSpan" id="kobo.744.2">Since </span><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">catalyst</span></strong><span class="koboSpan" id="kobo.746.1"> provides most of the model versioning and model storing mechanisms, we will only utilize the tracking feature in MLflow. </span><span class="koboSpan" id="kobo.746.2">The steps for this example are </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.748.1">First, let’s import all the </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.750.1">
import json
import osimport numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from catalyst import dl, utils
from catalyst.contrib.datasets import MNIST
from sklearn import datasets
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from torch import nn as nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from catalyst.loggers.mlflow import MLflowLogger</span></pre></li> <li><span class="koboSpan" id="kobo.751.1">Next, as we will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.752.1">pytorch</span></strong><span class="koboSpan" id="kobo.753.1">-based MLP, we will again set the random seed </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.755.1">pytorch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.757.1">
torch.manual_seed(0)</span></pre></li> <li><span class="koboSpan" id="kobo.758.1">The dataset we will be using for the practical implementation here is the Iris dataset again. </span><span class="koboSpan" id="kobo.758.2">The dataset consists of the </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.759.1">petal and</span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.760.1"> sepal </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.761.1">lengths of </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.762.1">various flowers with three different iris types. </span><span class="koboSpan" id="kobo.762.2">We will now load </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">this dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.764.1">
iris = datasets.load_iris()
iris_input_dataset = iris['data']
target = torch.from_numpy(iris['target'])</span></pre></li> <li><span class="koboSpan" id="kobo.765.1">Scaling is a type of regularization method that can reduce memorization and reduce bias. </span><span class="koboSpan" id="kobo.765.2">Let’s perform a straightforward minimum and maximum </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">scaling here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.767.1">
scaler = MinMaxScaler()
scaler.fit(iris_input_dataset)
iris_input_dataset = torch.from_numpy(scaler.transform(iris_input_dataset)).float()</span></pre></li> <li><span class="koboSpan" id="kobo.768.1">To train a model, we need a proper cross-validation strategy to verify the validity and performance of the model. </span><span class="koboSpan" id="kobo.768.2">We will use 77% of the data for training and 33% for validation. </span><span class="koboSpan" id="kobo.768.3">Let’s prepare the data </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">for cross-validation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.770.1">
X_train, X_test, y_train, y_test = train_test_split(iris_input_dataset, target, test_size=0.33, random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.771.1">Next, we will need to prepare the data loaders using the prepared data in </span><strong class="source-inline"><span class="koboSpan" id="kobo.772.1">numpy</span></strong><span class="koboSpan" id="kobo.773.1"> format </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">for cross-validation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.775.1">
training_dataset = TensorDataset(X_train, y_train)
validation_dataset =  TensorDataset(X_test, y_test)
train_loader = DataLoader(training_dataset, batch_size=10, num_workers=1)
valid_loader = DataLoader(validation_dataset, batch_size=10, num_workers=1)
loaders = {"train": train_loader, "valid": valid_loader}</span></pre></li> <li><span class="koboSpan" id="kobo.776.1">Since this is a multiclass problem of three classes, we will use the cross-entropy loss </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.778.1">pytorch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.780.1">
criterion = nn.CrossEntropyLoss()</span></pre></li> <li><span class="koboSpan" id="kobo.781.1">We will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.782.1">pytorch</span></strong><span class="koboSpan" id="kobo.783.1"> high-level wrapper library called </span><strong class="source-inline"><span class="koboSpan" id="kobo.784.1">catalyst</span></strong><span class="koboSpan" id="kobo.785.1"> here. </span><span class="koboSpan" id="kobo.785.2">To train a model in </span><strong class="source-inline"><span class="koboSpan" id="kobo.786.1">catalyst</span></strong><span class="koboSpan" id="kobo.787.1">, we have to define a model trainer class instance called </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">a runner:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.789.1">
runner = dl.SupervisedRunner(
    input_key="features", output_key="logits", target_key="targets", loss_key="loss"
)</span></pre></li> <li><span class="koboSpan" id="kobo.790.1">We will be using an MLP in </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.791.1">this </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.792.1">project </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.793.1">with an MLP constructor class that allows us to specify the input data size, the</span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.794.1"> hidden layer configuration, and the output data size. </span><span class="koboSpan" id="kobo.794.2">The hidden layer configuration is a list of layer sizes that simultaneously specifies the number of layers and the layer size at each layer. </span><span class="koboSpan" id="kobo.794.3">Let’s say that we want to randomly obtain 20 different hidden layer configurations and find out which performs the best on the validation partition. </span><span class="koboSpan" id="kobo.794.4">Let’s first define a method that generates the </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">configuration randomly:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.796.1">
def get_random_configurations(
  number_of_configurations, rng
):
  layer_configurations = []
  for _ in range(number_of_configurations):
    layer_configuration = []
    number_of_hidden_layers = rng.randint(low=1, high=6)
    for _ in range(number_of_hidden_layers):
      layer_configuration.append(rng.randint(low=2, high=100))
      layer_configurations.append(
        layer_configuration
        )
      layer_configurations = np.array(
        layer_configurations
      )
  return layer_configurations
rng = np.random.RandomState(1234)
number_of_configurations = 20
layer_configurations = get_random_configurations(
  number_of_configurations, rng
)</span></pre></li> <li><span class="koboSpan" id="kobo.797.1">Now, let’s define a method that will allow us to train and evaluate the different </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">MLP configurations:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.799.1">
def train_and_evaluate_mlp(
    trial_number, layer_configuration, epochs,
):</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.800.1">The trial number here plainly differentiates the different experiments. </span><span class="koboSpan" id="kobo.800.2">Apart from layer configuration, we can also configure the epochs that we want to run. </span><span class="koboSpan" id="kobo.800.3">In this method, we will create an MLP model instance based on the layer configuration </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">passed in:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.802.1">    model = MLP(
        input_layer_size=iris_input_dataset.shape[1],
        layer_configuration=layer_configuration,
        output_layer_size=len(np.unique(target)),
    )</span></pre></li> <li><span class="koboSpan" id="kobo.803.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">Adam</span></strong><span class="koboSpan" id="kobo.805.1"> optimizer</span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.806.1"> for </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.807.1">gradient </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.808.1">descent </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.809.1">and set the </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">checkpoint directory:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.811.1">
    optimizer = optim.Adam(model.parameters(), lr=0.02)
checkpoint_logdir = "experiments"</span></pre></li> <li><span class="koboSpan" id="kobo.812.1">Next, we will define the MLflow logger helper class available in </span><strong class="source-inline"><span class="koboSpan" id="kobo.813.1">catalyst</span></strong><span class="koboSpan" id="kobo.814.1"> to log experiments in MLflow format. </span><span class="koboSpan" id="kobo.814.2">In this setup, we log the mean and standard deviation of the training and validation </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">log loss:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.816.1">
    loggers = {
      "mlflow": MLflowLogger(
        experiment="test_exp", run="test_run"
      )
    }</span></pre></li> <li><span class="koboSpan" id="kobo.817.1">Finally, we will start the training process that trains for the specified number </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">of epochs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.819.1">
    runner.train(
        model=model,
        hparams=hparams,
        criterion=criterion,
        optimizer=optimizer,
        loaders=loaders,
        num_epochs=epochs,
        callbacks=[
            dl.CheckpointCallback(
                logdir=checkpoint_logdir,
                #save_n_best=0,
                loader_key="valid",
                metric_key="loss",
                mode="model",
            )
        ],
        logdir="./logs",
        valid_loader="valid",
        valid_metric="loss",
        minimize_valid_metric=True,
        verbose=verbose,
        loggers=loggers
    )</span></pre></li> <li><span class="koboSpan" id="kobo.820.1">As the last code step, we will </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.821.1">loop through each randomly generated layer configuration and perform the </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.822.1">training and </span><a id="_idIndexMarker671"/><span class="No-Break"><span class="koboSpan" id="kobo.823.1">evaluation</span></span><span class="No-Break"><a id="_idIndexMarker672"/></span><span class="No-Break"><span class="koboSpan" id="kobo.824.1"> process:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.825.1">
for layer_config in layer_configurations:
    train_and_evaluate_mlp(
        trial_number,
        layer_config,
        epochs=10,
        load_on_stage_start=False
    )</span></pre></li> <li><span class="koboSpan" id="kobo.826.1">Now, we need to start up the MLflow server service. </span><span class="koboSpan" id="kobo.826.2">We can do this by running the following command in the command line in the same directory as the directory that contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">introduced code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.828.1">
 mlflow server –backend-store-uri mlruns</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.829.1">After running this command, the same directory should contain the following file named </span><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">.catalyst</span></strong><span class="koboSpan" id="kobo.831.1">, which instructs </span><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">catalyst</span></strong><span class="koboSpan" id="kobo.833.1"> to enable MLflow support. </span><span class="koboSpan" id="kobo.833.2">This file should have the </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">following content:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.835.1">[catalyst]
cv_required = false
mlflow_required = true
ml_required = true
neptune_required = false
optuna_required = false</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.836.1">Once the command is executed, and by opening the HTTP website link, you should see the screen of MLflow, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.837.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.838.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.840.1"><img alt="Figure 8.4 – M﻿Lflow interface" src="image/B18187_08_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.841.1">Figure 8.4 – MLflow interface</span></p>
<p><span class="koboSpan" id="kobo.842.1">The interface shows a convenient way to visualize performance differences between different experiments while showing the utilized parameters. </span><span class="koboSpan" id="kobo.842.2">The numerical metric values can be sorted to obtain the best-performing model on the validation partition, as shown in the figure. </span><span class="koboSpan" id="kobo.842.3">The process of preparing data and training a model requires iterative comparisons to be made between different setups or experiments. </span><span class="koboSpan" id="kobo.842.4">Experiments can be compared more objectively with quantitative metrics with their experimentation parameters. </span><span class="koboSpan" id="kobo.842.5">When displayed visually automatically through code in an interface instead of plugging it manually</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.843.1"> into an</span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.844.1"> Excel </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.845.1">or Google</span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.846.1"> sheet, this makes the process much more dependable and organized. </span><span class="koboSpan" id="kobo.846.2">Additionally, if you click into any of the experiments, you’ll be able to check out the loss curves at each epoch interactively, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.847.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.848.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.850.1"><img alt="Figure 8.5 – M﻿Lflow interface showing an example loss curve of the best model" src="image/B18187_08_005.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.851.1">Figure 8.5 – MLflow interface showing an example loss curve of the best model</span></p>
<p><span class="koboSpan" id="kobo.852.1">While ensuring speed of iteration, it is also required to organize and track all artifacts you generated for your model properly. </span><span class="koboSpan" id="kobo.852.2">This means that you need to version your model, your dataset, and any key components that affect the resulting model output. </span><span class="koboSpan" id="kobo.852.3">Artifacts can be model weights, metric performance reports, performance plots, embedding visualization, and loss visualization plots. </span><span class="koboSpan" id="kobo.852.4">This task can obviously be done manually through manual coding. </span><span class="koboSpan" id="kobo.852.5">However, organizing the artifacts of models’ built-in experiments gets messy when the number of experiments goes up. </span><span class="koboSpan" id="kobo.852.6">Any custom files, graphs, and metrics can be tied into each of these experiment records and viewed in the MLflow interface. </span><span class="koboSpan" id="kobo.852.7">Experiments here can differ by using </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.853.1">different </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.854.1">models, different </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.855.1">datasets, different featurization methods, different hyperparameters of a model, or a different sample size of the </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.856.1">same dataset. </span><span class="koboSpan" id="kobo.856.2">Additionally, models can be stored directly in MLflow’s model registry, which allows MLflow to deploy the </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">model directly.</span></span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.858.1">Exploring model-building tips</span></h2>
<p><span class="koboSpan" id="kobo.859.1">This practical content serves </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.860.1">as an example of how an MLOps platform such as MLflow can ease the process of building and choosing the right model programmatically and visually. </span><span class="koboSpan" id="kobo.860.2">As much as MLOps is great and helps in training models efficiently, there are a few things that an MLOps platform does not handle for you but are considered key components before a model can be properly utilized and have its predictions consumed. </span><span class="koboSpan" id="kobo.860.3">These components are </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">listed next:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.862.1">Prediction consistency validation test</span></strong><span class="koboSpan" id="kobo.863.1">: This is a test that ensures the predictions made by the same trained model are consistent on the same data. </span><span class="koboSpan" id="kobo.863.2">A model’s predictions can’t be utilized if its logic is not deterministic. </span><span class="koboSpan" id="kobo.863.3">This will be discussed further in </span><a href="B18187_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.864.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.865.1">, </span><em class="italic"><span class="koboSpan" id="kobo.866.1">Exploring Model </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.867.1">Evaluation Methods</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.869.1">Reproducibility of model training experiments</span></strong><span class="koboSpan" id="kobo.870.1">: Reproducibility is a responsibility that has to be upheld by the ML engineer. </span><span class="koboSpan" id="kobo.870.2">This means that we have to ensure that a model can be deterministically reproduced using the configuration that was used to train it the second time around. </span><span class="koboSpan" id="kobo.870.3">As the ML life cycle is an iterative cyclical process, when there is a need to retrain the model with new data, the same performance can be approximately achieved again. </span><span class="koboSpan" id="kobo.870.4">The trick to ensuring reproducibility is to ensure all components that require random data generation are seeded deterministically. </span><span class="koboSpan" id="kobo.870.5">A seed ensures that the random number generator generates random numbers deterministically. </span><span class="koboSpan" id="kobo.870.6">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.871.1">pytorch</span></strong><span class="koboSpan" id="kobo.872.1">, this can be done globally through the </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.874.1">
import torch
import random
import numpy as np
torch.manual_seed(0)
random.seed(seed)
np.random.seed(seed)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.875.1">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.876.1">tensorflow</span></strong><span class="koboSpan" id="kobo.877.1"> an</span><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.878.1">d </span><strong class="source-inline"><span class="koboSpan" id="kobo.879.1">keras</span></strong><span class="koboSpan" id="kobo.880.1">, this can be done globally through the </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">following code:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.882.1">import tensorflow as tf
tf.keras.utils.set_random_seed(seed)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.883.1">This method automatically seeds both the </span><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">random</span></strong><span class="koboSpan" id="kobo.885.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.886.1">numpy</span></strong><span class="koboSpan" id="kobo.887.1"> libraries. </span><span class="koboSpan" id="kobo.887.2">These global settings can help to set the random seed for layers that did not explicitly set random number generator </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">seeds locally.</span></span></p></li> </ul>
<p><span class="koboSpan" id="kobo.889.1">One last piece of advice in experimentation is to make sure a baseline is created at the start of the project. </span><span class="koboSpan" id="kobo.889.2">A baseline is the simplest version of a solution possible. </span><span class="koboSpan" id="kobo.889.3">The solution can even be a non-DL model with simple features. </span><span class="koboSpan" id="kobo.889.4">Having a baseline can help ensure that any improvements or complications you add are justified by metric performance monitoring. </span><span class="koboSpan" id="kobo.889.5">Refrain from adding complications for the sake of them. </span><span class="koboSpan" id="kobo.889.6">Remember that the value</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.890.1"> of an ML project is not how complicated the process is but the results that can be extracted </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.892.1">Next, we will dive into actual techniques that can be used to realize and improve a solution that </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">utilizes DL.</span></span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.894.1">Exploring general techniques to realize and improve supervised deep learning based solutions</span></h1>
<p><span class="koboSpan" id="kobo.895.1">Notice that earlier in the chapter </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.896.1">we focused on use cases based on problem types and not the problems themselves. </span><span class="koboSpan" id="kobo.896.2">Solutions in turn solve and take care of the problem. </span><span class="koboSpan" id="kobo.896.3">DL and ML in general are great solvers of issues related to staffing difficulties and for the automation of mundane tasks. </span><span class="koboSpan" id="kobo.896.4">Furthermore, ML models in computers can process data much quicker than an average human can, allowing a much quicker response time and much more efficient scaling of any process. </span><span class="koboSpan" id="kobo.896.5">In many cases, ML models can help to increase the accuracy and efficiency of processes. </span><span class="koboSpan" id="kobo.896.6">Sometimes, they improve current processes, and other times, they make previously unachievable processes possible. </span><span class="koboSpan" id="kobo.896.7">However, a single DL model may or may not be enough to solve the problem. </span><span class="koboSpan" id="kobo.896.8">Let’s take an example of a solution that </span><em class="italic"><span class="koboSpan" id="kobo.897.1">can</span></em><span class="koboSpan" id="kobo.898.1"> be solved sufficiently with a single </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">DL model.</span></span></p>
<p><span class="koboSpan" id="kobo.900.1">Consider the use case of using a DL model to predict the genders of babies with ultrasound imagery. </span><span class="koboSpan" id="kobo.900.2">Traditionally, a doctor would perform a visual-based gender analysis of the resulting ultrasound imagery of a baby in the mother’s womb in real time and offline before finally providing their prediction of the gender. </span><span class="koboSpan" id="kobo.900.3">Based on the amount of prior experience and knowledge, the doctor would have different levels of competency and accuracy in decoding the gender. </span><span class="koboSpan" id="kobo.900.4">Things might get more complicated when there are abnormalities in the baby. </span><span class="koboSpan" id="kobo.900.5">The probable underlying problem would be that experienced and capable doctors are scarce and expensive to hire. </span><span class="koboSpan" id="kobo.900.6">If we had a system that could decode the gender from the ultrasound imagery automatically, it would either be of good assistance to the judgment of real doctors or a replacement as a cheaper alternative. </span><span class="koboSpan" id="kobo.900.7">The same analogies can be applied to identifying diseases or symptoms in any advanced imaging results such as </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">X-ray images.</span></span></p>
<p><span class="koboSpan" id="kobo.902.1">This example depicts a DL model as a component of a solution and a solution where a single DL model is enough to obtain the desired output. </span><span class="koboSpan" id="kobo.902.2">This is an example of staffing issues but not so much on the efficiency side. </span><span class="koboSpan" id="kobo.902.3">Note that for some use cases, it is required to explain in some form the reasons that drove the predictions that were made. </span><span class="koboSpan" id="kobo.902.4">In other words, you’d have to explain the decisions that were made by the model. </span><span class="koboSpan" id="kobo.902.5">To provide assistance to a doctor, pinpointing where and which types of patterns contributed to the decision would be more helpful than the decision itself, as doctors would be able to utilize the extra information to make their own decisions. </span><span class="koboSpan" id="kobo.902.6">This will be thoroughly introduced in </span><a href="B18187_11.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.903.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.904.1">, </span><em class="italic"><span class="koboSpan" id="kobo.905.1">Explaining Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.906.1">Network Predictions</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.908.1">Explanations aside, not all solutions to problems can be accomplished by a single ML or DL model alone. </span><span class="koboSpan" id="kobo.908.2">At times, DL models have to be coupled with general ML methods, and at others, multiple DL models have to be coupled together. </span><span class="koboSpan" id="kobo.908.3">In some sp</span><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.909.1">ecial cases, intermediate data needs to be specially processed and prepared before feeding it to the next task in a pipeline. </span><span class="koboSpan" id="kobo.909.2">Creating and architecting logical pipelines are essential when dealing with such problems. </span><span class="koboSpan" id="kobo.909.3">Let’s take an example of a problem and solution that requires multiple datasets, multiple DL models, and constructing a </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">task pipeline.</span></span></p>
<p><span class="koboSpan" id="kobo.911.1">Consider the problem of finding criminals who have just robbed a bank. </span><span class="koboSpan" id="kobo.911.2">By using CCTV cameras deployed in the city, you can use a face detection and recognition solution if you have identified the face of the criminals that did the robbery. </span><span class="koboSpan" id="kobo.911.3">The following figure shows an example solution task pipeline for </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">this problem:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.913.1"><img alt="Figure 8.6 – Task pipeline of the solution for finding criminals through CCTV cameras" src="image/B18187_08_006.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.914.1">Figure 8.6 – Task pipeline of the solution for finding criminals through CCTV cameras</span></p>
<p><span class="koboSpan" id="kobo.915.1">Face detection is an</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.916.1"> image-object-detection process where there is an image bounding box regressor and a binary classifier that predicts whether the bounding box is a face or not. </span><span class="koboSpan" id="kobo.916.2">The representative facial features extraction </span><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.917.1">utilizes a DL model that can be trained using supervised representation learning methods that are trained against the goal of optimizing the discriminative effects of facial features against the facial features of different persons. </span><span class="koboSpan" id="kobo.917.2">Next, a separate task is needed to build the database of criminal facial features that will be passed into the KNN ML algorithm to find the matched facial ID based on queried facial features obtained from CCTV cameras deployed in the city. </span><span class="koboSpan" id="kobo.917.3">This solution shows the need to break a solution into multiple components in order to obtain the final result of finding </span><span class="No-Break"><span class="koboSpan" id="kobo.918.1">the criminals.</span></span></p>
<p><span class="koboSpan" id="kobo.919.1">The preceding example</span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.920.1"> is part of a larger paradigm called multitask learning and multitask problems. </span><span class="koboSpan" id="kobo.920.2">The multitask paradigm is a set of topics that allows for greater advancement in the ML space, not only for DL but definitely much more achievable through DL, due to its inherent flexibility. </span><span class="koboSpan" id="kobo.920.3">In the next topic, we will dive into the </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">multitask paradigm.</span></span></p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.922.1">Breaking down the multitask paradigm in supervised deep learning</span></h1>
<p><span class="koboSpan" id="kobo.923.1">Multitask is a paradigm that </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.924.1">covers a wide spectrum of tasks that involves the execution of ML models on multiple problems coupled with their respective datasets to achieve a goal. </span><span class="koboSpan" id="kobo.924.2">This paradigm is usually built based on </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">two reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.926.1">To achieve better predictive performance </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">and generalization.</span></span></li>
<li><span class="koboSpan" id="kobo.928.1">To break down complicated goals into smaller tasks that are directly solvable using separate ML models. </span><span class="koboSpan" id="kobo.928.2">This reiterates the point made in the </span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">previous topic.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.930.1">Let’s dive into four multitask techniques, starting with </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">multitask pipelines.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.932.1">Multitask pipelines</span></h2>
<p><span class="koboSpan" id="kobo.933.1">This variation of multitask</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.934.1"> systems revolves around realizing solutions that can’t be directly solved by using a single ML model. </span><span class="koboSpan" id="kobo.934.2">Breaking down highly complicated tasks into smaller tasks can allow solutions to be made with multiple ML</span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.935.1"> models handling different smaller tasks. </span><span class="koboSpan" id="kobo.935.2">These tasks can be sequential or parallel in their paths and generally form a </span><strong class="bold"><span class="koboSpan" id="kobo.936.1">directed acyclic graph</span></strong><span class="koboSpan" id="kobo.937.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.938.1">DAG</span></strong><span class="koboSpan" id="kobo.939.1">)-like pipeline, similar to the example shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.940.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.941.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.943.1">However, this does not mean that the tasks should exclusively be ML models. </span><span class="koboSpan" id="kobo.943.2">Problems for different industries and businesses can be in many forms, and being flexible in assigning components needed to produce a solution is key to deriving value from ML technology. </span><span class="koboSpan" id="kobo.943.3">For example, if human supervision is needed to accomplish a certain task after breaking down the larger task, do not hesitate to utilize it along with ML models to achieve value. </span><span class="koboSpan" id="kobo.943.4">Let’s go through another</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.944.1"> use case that utilizes multitask pipelines to create a solution, which is </span><strong class="bold"><span class="koboSpan" id="kobo.945.1">recommendation systems</span></strong><span class="koboSpan" id="kobo.946.1">. </span><span class="koboSpan" id="kobo.946.2">First, we need to perform either supervised or unsupervised representation learning for feature extraction. </span><span class="koboSpan" id="kobo.946.3">Second, using the features extracted, create a database used to match extracted query features. </span><span class="koboSpan" id="kobo.946.4">Third, obtain the top-k closest data from the database and apply a regression model to predict the rank of the top-k data for fine-tuned e-tuned </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">high-performance ranking.</span></span></p>
<p><span class="koboSpan" id="kobo.948.1">Next, we will discover another paradigm of multitasking, </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.950.1">TL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">.</span></span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.952.1">TL</span></h2>
<p><span class="koboSpan" id="kobo.953.1">TL is a technique that </span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.954.1">involves using what was learned from one task</span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.955.1"> in another task. </span><span class="koboSpan" id="kobo.955.2">The core reasons can be one of </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.957.1">Increasing the </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">metric performance.</span></span></li>
<li><span class="koboSpan" id="kobo.959.1">Decreasing the required number of epochs needed for the network to reach a state </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">of convergence.</span></span></li>
<li><span class="koboSpan" id="kobo.961.1">Allowing more stable learning trajectories. </span><span class="koboSpan" id="kobo.961.2">In other cases, networks just take longer to converge when the initial learning process is unstable. </span><span class="koboSpan" id="kobo.961.3">However, in some other cases, networks cannot converge at all when networks don’t have a stable foundation to start learning. </span><span class="koboSpan" id="kobo.961.4">TL can help models that originally fail to learn anything reach convergence in the </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">learning process.</span></span></li>
<li><span class="koboSpan" id="kobo.963.1">Increasing generalization and reducing the probability of overfitting. </span><span class="koboSpan" id="kobo.963.2">When the second task involves only a small subset of variations from the actual data population, knowledge learned from a first task that covers a wider range of variations helps to prevent </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">narrow oversights.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.965.1">Concretely, TL in DL is achieved by using the network parameters that were learned </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.966.1">from the first task in the second task. </span><span class="koboSpan" id="kobo.966.2">The parameters involve all the weights and biases that are</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.967.1"> associated with a network. </span><span class="koboSpan" id="kobo.967.2">The parameters</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.968.1"> can be used as an initialization step for the same network instead of the usual randomly initialized parameters. </span><span class="koboSpan" id="kobo.968.2">These are known as </span><strong class="bold"><span class="koboSpan" id="kobo.969.1">pre-trained weights</span></strong><span class="koboSpan" id="kobo.970.1">. </span><span class="koboSpan" id="kobo.970.2">The process of network learning with pre-trained weights is called </span><strong class="bold"><span class="koboSpan" id="kobo.971.1">fine-tuning</span></strong><span class="koboSpan" id="kobo.972.1">. </span><span class="koboSpan" id="kobo.972.2">Additionally, the network parameters can also opt to be completely frozen and plainly act as a </span><strong class="bold"><span class="koboSpan" id="kobo.973.1">featurizer</span></strong><span class="koboSpan" id="kobo.974.1"> component that provides features for another </span><a id="_idIndexMarker695"/><span class="No-Break"><span class="koboSpan" id="kobo.975.1">SL algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.976.1">There are a couple of automated strategies that focus on improving the results you can get with fine-tuning. </span><span class="koboSpan" id="kobo.976.2">However, these methods are not silver bullets and can take a lot of time to carry out. </span><span class="koboSpan" id="kobo.976.3">The practical strategy to achieve a better performance using TL is to choose the number of layers you want to train by gauging the transferability component of the two tasks. </span><em class="italic"><span class="koboSpan" id="kobo.977.1">Table 8.2</span></em><span class="koboSpan" id="kobo.978.1"> shows an easy way to decide on a TL strategy based on task similarity and dataset size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">second task:</span></span></p>
<table class="No-Table-Style" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style" colspan="2" rowspan="2"/>
<td class="No-Table-Style" colspan="2">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.980.1">Dataset Size</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.981.1">Small</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"> </strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.982.1">Big</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p><span class="No-Break"><span class="koboSpan" id="kobo.983.1">Task similarity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.984.1">Low</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.985.1">Train the entire network </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">as usual.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.987.1">Train the entire network </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">as usual.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.989.1">High</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.990.1">Freeze all base network parameters, add an extra linear prediction layer, and only train this linear layer on </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">the dataset.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.992.1">Train the entire network </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">as usual.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.994.1">Table 8.2 – Deep TL (DTL) strategy guide</span></p>
<p><span class="koboSpan" id="kobo.995.1">For clarity purposes, let’s say “big” is when the dataset has at least 10,000 examples. </span><span class="koboSpan" id="kobo.995.2">For task transferability/similarity, human intuition is required to obtain an evaluation on a case-by-case basis. </span><span class="koboSpan" id="kobo.995.3">Here in the guide, we assume that a big dataset size means a dataset with large variations that represent the population adequately. </span><span class="koboSpan" id="kobo.995.4">A hidden component not presented in the preceding figure, however, is the size of the dataset of the first task. </span><span class="koboSpan" id="kobo.995.5">TL has the best impact when the task similarity is high, the second task dataset size is small, and, additionally, when the dataset size of the first task is big. </span><span class="koboSpan" id="kobo.995.6">The size of the first dataset usually also limits the range of NN sizes that can be used. </span><span class="koboSpan" id="kobo.995.7">Let’s say that the first dataset size is small; the best-performing models in this case are usually smaller-sized models. </span><span class="koboSpan" id="kobo.995.8">When TL is highly beneficial, even when the dataset size of the second dataset size is medium or big, the smaller models can still outperform bigger-sized models. </span><span class="koboSpan" id="kobo.995.9">An act of balancing is required in complex cases such as this to obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">ideal model.</span></span></p>
<p><span class="koboSpan" id="kobo.997.1">One prominent issue in TL is </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.998.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.999.1">catastrophic forgetting</span></strong><span class="koboSpan" id="kobo.1000.1">. </span><span class="koboSpan" id="kobo.1000.2">This is a phenomenon where the network performance regresses to earlier tasks as the network trains on new tasks. </span><span class="koboSpan" id="kobo.1000.3">If the performance of the previous task is not of concern to you, this issue can be ignored. </span><span class="koboSpan" id="kobo.1000.4">Practically, if it is required to maintain the performance of the previous task, you can follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1002.1">Use a unified metric that takes care of the performance of the first task and second task by additionally validating on the validation dataset of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">first task.</span></span></li>
<li><span class="koboSpan" id="kobo.1004.1">Combine the dataset from the first task and second task and train it as a single model. </span><span class="koboSpan" id="kobo.1004.2">If the targets are not relevant to each other, use different fully connected layer </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">prediction heads.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1006.1">Lastly, there is an additional popular technique for TL </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.1007.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.1008.1">knowledge distillation</span></strong><span class="koboSpan" id="kobo.1009.1">. </span><span class="koboSpan" id="kobo.1009.2">This method involves two models, where one pre-trained teacher model is used to distill its knowledge to a student model. </span><span class="koboSpan" id="kobo.1009.3">Typically, the teacher model is a bigger model that has the capacity to learn more accurate information but is slower in runtime, and the student model is a smaller model that can be run at reasonable speeds during runtime. </span><span class="koboSpan" id="kobo.1009.4">The method distills knowledge by using an additional similarity-based loss of a chosen layer output between the teacher and student model, which is typically the logit layer, on top of the base cross-entropy loss. </span><span class="koboSpan" id="kobo.1009.5">This method encourages the student model to produce similar features to the </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.1010.1">teacher </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.1011.1">model. </span><span class="koboSpan" id="kobo.1011.2">The technique is typically used to obtain a smaller model with better accuracy than if trained without knowledge distillation, so the deployment infrastructure can be cheaper. </span><span class="koboSpan" id="kobo.1011.3">This technique will be practically introduced in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1012.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.1013.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1014.1">Exploring Bias and Fairness</span></em><span class="koboSpan" id="kobo.1015.1">, as a key technique to also </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">mitigate bias.</span></span></p>
<p><span class="koboSpan" id="kobo.1017.1">Next, we will dive into another type of multitask execution, called multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">objective learning.</span></span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.1019.1">Multiple objective learning</span></h2>
<p><span class="koboSpan" id="kobo.1020.1">Multiple objective </span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.1021.1">learning is a type of </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.1022.1">multitasking process that involves training with simultaneously different goals. </span><span class="koboSpan" id="kobo.1022.2">Different goals direct the learning trajectory of a network toward different paths. </span><span class="koboSpan" id="kobo.1022.3">Multiple objective learning can be further broken down into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">following options:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1024.1">Multiple losses on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1025.1">same outputs.</span></span></li>
<li><span class="koboSpan" id="kobo.1026.1">Multiple targets, which are taken care of by separate NN prediction heads, each with their respective losses. </span><span class="koboSpan" id="kobo.1026.2">This can be further broken down into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">following categories:</span></span><ul><li><span class="koboSpan" id="kobo.1028.1">Multiple targets with real impact </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">and usage.</span></span></li><li><span class="koboSpan" id="kobo.1030.1">A single or multiple main targets and a single or multiple auxiliary targets. </span><span class="koboSpan" id="kobo.1030.2">Auxiliary targets are paired with their own losses called </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">auxiliary losses.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.1032.1">Aside from option </span><em class="italic"><span class="koboSpan" id="kobo.1033.1">2(A)</span></em><span class="koboSpan" id="kobo.1034.1">, the other options (that is, </span><em class="italic"><span class="koboSpan" id="kobo.1035.1">1</span></em><span class="koboSpan" id="kobo.1036.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1037.1">2(B)</span></em><span class="koboSpan" id="kobo.1038.1"> ) for multiple objective learning are mainly used to improve metric performance. </span><span class="koboSpan" id="kobo.1038.2">Metrics can be as simple as accuracy or log loss, or more intricate, such as the degree of bias toward a minority class. </span><span class="koboSpan" id="kobo.1038.3">A simple more straightforward example of multiple objective learning is the multilabel target type. </span><span class="koboSpan" id="kobo.1038.4">Multilabel is where multiple labels can be associated with a single data row. </span><span class="koboSpan" id="kobo.1038.5">This means that the setup will be a multiple binary classification target, which is the case </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1040.1">2(A)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1042.1">Multiple targets and their associated losses mean there might be issues of conflicting gradients during the learning process. </span><span class="koboSpan" id="kobo.1042.2">This phenomenon is more commonly</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.1043.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.1044.1">negative transfer</span></strong><span class="koboSpan" id="kobo.1045.1">. </span><span class="koboSpan" id="kobo.1045.2">A more extreme case of negative transfer is when gradients from the two losses cancel each other out when they have the same magnitude in exactly opposite directions. </span><span class="koboSpan" id="kobo.1045.3">This will block the learning process of the model where the model will never converge. </span><span class="koboSpan" id="kobo.1045.4">In reality, this issue can be at a lower scale and dampen the speed of convergence or, worse, introduce huge fluctuations that make it hard to learn anything. </span><span class="koboSpan" id="kobo.1045.5">Unfortunately, there are </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.1046.1">no silver-bullet </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.1047.1">mitigation methods here other than to understand the background behind why a model learns poorly. </span><span class="koboSpan" id="kobo.1047.2">Iterative experiments are usually required to figure out how to balance these losses properly to encourage a stable </span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">learning process.</span></span></p>
<p><span class="koboSpan" id="kobo.1049.1">Next, we will dive into multimodal </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">NN training.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.1051.1">Multimodal NN training</span></h2>
<p><span class="koboSpan" id="kobo.1052.1">Multimodal NNs are a</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.1053.1"> type of multitask system in the sense that networks responsible for different modalities learn in the same task in</span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.1054.1"> completely different paths. </span><span class="koboSpan" id="kobo.1054.2">A common method of handling multimodality in NNs is to assign different neural blocks at the initial stage for different data modalities. </span><span class="koboSpan" id="kobo.1054.3">Neural blocks contain the networks specific to each modality. </span><span class="koboSpan" id="kobo.1054.4">The neural blocks for different modalities will then be merged using a series of intermediate fully connected layers and an output fully connected layer. </span><span class="koboSpan" id="kobo.1054.5">This is depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1055.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1056.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.1058.1"><img alt="Figure 8.7 – Typical multimodal NN structure" src="image/B18187_08_007.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1059.1">Figure 8.7 – Typical multimodal NN structure</span></p>
<p><span class="koboSpan" id="kobo.1060.1">The idea of leveraging multimodality is that the additional data input can allow for more comprehensive patterns to be identified and thus should improve the overall metric performance. </span><span class="koboSpan" id="kobo.1060.2">In reality, this commonly will not be the case without careful handling of the training process. </span><span class="koboSpan" id="kobo.1060.3">Different modalities exist in entirely different distributions and learn at different rates with different paths. </span><span class="koboSpan" id="kobo.1060.4">A single global optimization strategy applied to all the data modalities will likely produce suboptimal results. </span><span class="koboSpan" id="kobo.1060.5">A common and effective strategy is to do </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1062.1">Pretrain the individual modality neural block (unimodal) with a temporary prediction output layer until a certain degree </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">of convergence</span></span></li>
<li><span class="koboSpan" id="kobo.1064.1">Remove the temporary prediction output layer and train the multimodal NN as usual with the pre-trained weights from the unimodal </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">training process</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1066.1">Other than that, freezing the weights of the unimodal trained NN and only training the multimodal aggregation</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.1067.1"> fully connected </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.1068.1">layer prediction head is also a sound strategy. </span><span class="koboSpan" id="kobo.1068.2">Many more complex strategies exist to tackle this issue but are out of scope in </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">this book.</span></span></p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.1070.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1071.1">In this chapter, we explored supervised deep learning, including the types of problems it can be used to solve and the techniques for implementing and training DL models. </span><span class="koboSpan" id="kobo.1071.2">Supervised deep learning involves training a model on labeled data to make predictions on new data. </span><span class="koboSpan" id="kobo.1071.3">We also covered a variety of supervised learning use cases on different problem types, including binary classification, multiclassification, regression, and multitask and representation learning. </span><span class="koboSpan" id="kobo.1071.4">The chapter also covered techniques for training DL models effectively, including regularization and hyperparameter tuning, and provided practical implementations in the Python programming language using popular </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">DL frameworks.</span></span></p>
<p><span class="koboSpan" id="kobo.1073.1">Supervised deep learning can be used for a wide range of real-world applications in tasks such as image classification, </span><strong class="bold"><span class="koboSpan" id="kobo.1074.1">natural language processing</span></strong><span class="koboSpan" id="kobo.1075.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1076.1">NLP</span></strong><span class="koboSpan" id="kobo.1077.1">), and speech recognition. </span><span class="koboSpan" id="kobo.1077.2">With the knowledge provided in this chapter, you should be able to identify supervised learning applications and train DL </span><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">models effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.1079.1">In the next chapter, we will explore </span><strong class="bold"><span class="koboSpan" id="kobo.1080.1">unsupervised learning</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1081.1">for DL.</span></span></p>
</div>
</body></html>