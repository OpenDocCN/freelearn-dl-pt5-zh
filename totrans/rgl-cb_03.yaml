- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization with Linear Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A huge part of **machine learning** (**ML**) is made up of linear models. Although
    sometimes considered less powerful than their nonlinear counterparts (such as
    tree-based models or deep learning models), linear models do address many concrete,
    valuable problems. Customer churn and advertising optimization are just a couple
    of problems where linear models may be the right solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Training a linear regression with scikit-learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with ridge regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with lasso regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with elastic net regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a logistic regression model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a logistic regression model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right regularization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have learned how to use and regularize some
    of the most commonly used linear models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, besides loading data, you will learn how to fit and compute
    inferences with several linear models. In order to do so, the following libraries
    are required:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a linear regression model with scikit-learn
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is one the most basic ML models we can use, but it is very
    useful. Most people used linear regression in high school without talking about
    ML, and still use it on a regular basis within spreadsheets. In this recipe, we
    will explain the basics of linear regression, and then train and evaluate a linear
    regression model using scikit-learn on the California housing dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is not a complicated model, but it is still useful to understand
    what is under the hood to get the best out of it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The way linear regression works is pretty straightforward. Heading back to the
    real estate price example, if we consider a feature *x* such as the apartment
    surface and a label *y* such as the apartment price, a common solution would be
    to find *a* and *b* such that *y = ax +* *b*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this is not so simple in real life. There is usually no *a* and
    *b* that makes this equality always respected. It is more likely that we can define
    a function `h(x)` that aims to give a value as close as possible to *y*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Also, we may have not just one feature *x*, but several features *x*1, *x*2,…,
    *x*n, representing apartment surface, location, floor, number of rooms, exponent
    features, and so on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'By this logic, we would end up with a prediction *h(x)* that may look like
    the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_03_002.png)is the weight associated to the feature ![](img/Formula_03_003.png)
    *x*j, and *b* is a bias term. This is just a generalization of the previous *y
    = ax + b* to *n* features. This formula allows a linear regression to predict
    virtually any real number.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our ML model is to find the set of *w* and *b* values that minimizes
    prediction errors on the training set. By this, we mean finding the parameters
    *w* and *b* so that *h(x)* and *y* are as close as possible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to achieve that is to minimize the loss *L*, that can be defined here
    as a slightly modified **mean squared** **error** (**MSE**):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_03_005.png) is the ground truth of the sample *i* in the training
    set, and *m* is the number of samples in the training set.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The loss is usually a representation of the difference between the ground truth
    and the predictions. Hence, minimizing the loss allows the model to predict values
    that are as close as possible to the ground truth.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing this mean squared error would allow us to find the set of *w* and
    *b* values so that the prediction *h(x)* is as close as possible to the ground
    truth *y*. Schematically, this can be represented as finding the *w* that minimizes
    the loss, as shown in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross](img/B19629_03_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is: how do we find the set of values to minimize the loss?
    There are several ways of solving this problem. One commonly used technique in
    ML is gradient descent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: What is gradient descent? In a few words, it is going down the curve to the
    minimum value in the preceding figure.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'How does this work? It’s a multi-step process:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Start with random values of the parameters *w* and *b*. Random values are usually
    defined using normal distribution centered on zero. This is why having scaled
    features may help significantly for convergence.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss for the given data and current values of *w* and *b*. As defined
    earlier, we may use the mean squared error to compute the loss *L*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure is a good representation of the situation at this point:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The loss function with the global minimum at the red cross,
    and a possible random initial state at the blue cross](img/B19629_03_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The loss function with the global minimum at the red cross, and
    a possible random initial state at the blue cross
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the loss gradient with respect to each parameter ![](img/Formula_03_006.png)
    . This is nothing more than the slope of the loss for a given parameter, which
    can be computed with the following equations:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_03_007.jpg)![](img/Formula_03_008.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: One may notice that the slope is expected to decrease as we get closer to the
    minimum. Indeed, as we get close to the minimum, the error tends to zero and so
    does the slope, based on these equations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply gradient descent to parameters. Apply the gradient descent to parameters,
    with a user-defined learning rate *α*. This is computed using the following formulas:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_03_009.jpg)![](img/Formula_03_010.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'This allows us to take a step toward the minimum, as represented in the following
    figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The gradient descent allows us to take one step down the loss
    function, allowing us to get closer to the global minimum](img/B19629_03_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The gradient descent allows us to take one step down the loss function,
    allowing us to get closer to the global minimum
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Iterate through *steps 2 to 4* until convergence or max iteration. This would
    allow us to reach the optimal parameters, as represented in the following figure:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum](img/B19629_03_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: A learning rate *α* that is too large would miss the global minimum, or even
    diverge, while one that is too small would take forever to converge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete this recipe, the following libraries have to be installed: `numpy`,
    `matplotlib`, and `sklearn`. They can be installed with `pip` in the terminal,
    with the following command line:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, all this procedure is fully implemented in scikit-learn, and the
    only thing you need to do is fully reuse this library. Let’s now train a linear
    regression on the California housing dataset provided by scikit-learn.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch_california_housing`: A function that allows us to load the dataset'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_test_split`: A function that allows us to split the data'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`StandardScaler`: A class that allows us to rescale the data'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LinearRegression`: The class that contains the implementation of the linear
    regression'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is what the code looks like:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`X` with `X*X`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`train_test_split` function, with `test_size=0.2`, meaning we end up having
    80% of the data in the training set, and 20% in the test set, split at random.
    This is shown here:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Prepare the data**: Since we have only quantitative features here, the only
    preparation we need is rescaling. We can use the standard scaler of scikit-learn.
    We need to instantiate it, then fit it on the training set and transform the training
    set, and finally we transform the test set. Feel free to use any other rescaler:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Fit the model on the training set. The model must be instantiated, and here
    we use the default parameters, so nothing is specified. Once the model is instantiated,
    we can use the `.fit()` method on the training set:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Evaluate the model on both the training and test set. Here, we use the `.score()`
    method of the `LinearRegression` class, which provides `R2-score`, but you can
    use any other metric provided in `sklearn.metrics` that suits regression:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we can see, there is a significant difference between the train and test
    set’s scores, indicating model overfitting on the train set. To address this problem,
    regularization techniques will be proposed in the following recipes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the model has been trained, we can access all the parameters *w* (here,
    16 values for 16 input features) as well as the intercept *b*, with the attributes,
    `.coef_`, and the `.intercept_` value of the `LinearRegression` object respectively:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以通过 `LinearRegression` 对象的属性 `.coef_` 和 `.intercept_` 分别访问所有的参数
    *w*（这里是16个值，对应16个输入特征）以及截距 *b*：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we plot these values, we will notice that their values range between approximately
    `-8` and `2` on this dataset:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制这些值，我们会注意到它们在这个数据集中的值大约在 `-8` 和 `2` 之间：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is a visual representation of this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可视化表示：
- en: '![Figure 3.5 – Learned values of each weight of the linear regression model.
    The range of the values is quite large, from -8 to 2](img/B19629_03_05.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 线性回归模型中每个权重的学习值。值的范围相当大，从 -8 到 2](img/B19629_03_05.jpg)'
- en: Figure 3.5 – Learned values of each weight of the linear regression model. The
    range of the values is quite large, from -8 to 2
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 线性回归模型中每个权重的学习值。值的范围相当大，从 -8 到 2
- en: See also
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见其他
- en: 'To have a full understanding of how to use linear regression using scikit-learn,
    it is good practice to check the official documentation of the class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面了解如何使用 scikit-learn 进行线性回归，最好参考该类的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml)。
- en: We now have a good understanding of how linear regression works, and we will
    see in the next section how to regularize it with penalization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对线性回归的工作原理有了较好的理解，接下来我们将看到如何通过惩罚进行正则化。
- en: Regularizing with ridge regression
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ridge 回归进行正则化
- en: A very common and useful way to regularize a linear regression is through penalization
    of the loss function. In this recipe, after reviewing what it means to add penalization
    to the loss function in the case of ridge regression, we will train a ridge model
    on the same California housing dataset as in the previous recipe, and see how
    it can improve the score thanks to regularization.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见且有用的正则化线性回归的方法是通过惩罚损失函数。在本示例中，经过回顾将惩罚项添加到 ridge 回归的损失函数之后，我们将在与之前示例相同的加利福尼亚住房数据集上训练一个
    ridge 模型，看看它如何通过正则化提高评分。
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'One way to make sure that a model’s parameters are not going to overfit is
    to keep them close to zero: if the parameters do not have the possibility to evolve
    freely, they are less likely to overfit.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型参数不会过拟合的一种方法是保持它们接近零：如果参数无法自由变化，它们就不太可能过拟合。
- en: 'To that end, ridge regression adds a new term (regularization term) to the
    loss ![](img/Formula_03_011.png):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，ridge 回归将一个新的项（正则化项）添加到损失函数中！[](img/Formula_03_011.png)：
- en: '![](img/Formula_03_012.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_012.jpg)'
- en: 'Where ![](img/Formula_03_013.png)is the *L2* norm of *w*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_03_013.png) 是 *L2* 范数的 *w*：
- en: '![](img/Formula_03_014.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_014.jpg)'
- en: 'With this loss, we intuitively understand that high values of weights *w* are
    not possible, and thus overfitting is less likely. Also, *𝜆* is a hyperparameter
    (it can be fine-tuned) allowing us to control the regularization level:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个损失函数，我们可以直观地理解，较大的权重 *w* 是不可能的，因此过拟合的可能性较小。此外，*𝜆* 是一个超参数（可以微调），允许我们控制正则化的程度：
- en: A high value of *𝜆* means high regularization
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*𝜆* 的较高值表示较强的正则化'
- en: A value of *𝜆**=0* means no regularization, for example, regular linear regression
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*𝜆* = 0 表示没有正则化，例如普通的线性回归'
- en: 'The gradient descent formulas are slightly updated to the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的公式稍作更新，如下所示：
- en: '![](img/Formula_03_015.jpg)![](img/Formula_03_016.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_015.jpg)![](img/Formula_03_016.jpg)'
- en: 'Let’s now see how to use ridge regression with scikit-learn: the same libraries
    required in the previous recipe must be installed: `numpy`, `sklearn`, and `matplotlib`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何使用 ridge 回归与 scikit-learn：与前一个示例中一样，需要安装相同的库：`numpy`、`sklearn` 和
    `matplotlib`。
- en: Also, we assume the data is already downloaded and prepared from the previous
    recipe. To download, split and prepare the data, refer to the previous recipe.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们假设数据已经通过上一示例下载并准备好。有关如何下载、拆分和准备数据的详细信息，请参考前一个示例。
- en: How to do it…
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Let’s assume we are reusing the same data from the previous recipe. We will
    just train and evaluate another model on this exact same data, including the feature
    engineering with the squared features. The related implementation in scikit-learn
    for ridge regression is the `Ridge` class, where the `alpha` class attribute is
    equivalent to the *𝜆* in the preceding equation. Let’s use it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在重复使用前一个示例中的相同数据。我们将在完全相同的数据上训练和评估另一个模型，包括具有平方特征的特征工程。scikit-learn中岭回归的相关实现是`Ridge`类，其中`alpha`类属性等同于前述方程中的*𝜆*。让我们来使用它：
- en: 'Import the `Ridge` class from scikit-learn:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中导入`Ridge`类：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then instantiate a ridge model. A regularization parameter of `alpha=5000`
    has been selected here, but every dataset may need a very specific hyperparameter
    value to perform best. Next, train the model on the training set (previously prepared)
    with the `.fit()` method, which is shown as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们实例化一个岭回归模型。在这里选择了正则化参数`alpha=5000`，但每个数据集可能需要一个非常具体的超参数值来表现最佳。接下来，用`.fit()`方法在训练集上训练模型（事先准备好的），如下所示：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then evaluate the model. Here, we compute and display the R2-score provided
    by the `.score()` of the ridge class, but any other regression metric could be
    used:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们评估模型。在这里，我们计算并显示由岭回归类的`.score()`提供的R2分数，但也可以使用任何其他回归指标：
- en: '[PRE26]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We will notice that we are getting better results on the test set compared to
    the linear regression model (with no regularization) by allowing the R2-score
    on the test set to be slightly above `0.5`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会注意到，与线性回归模型（没有正则化）相比，我们在测试集上获得了更好的结果，允许测试集上的R2分数略高于`0.5`。
- en: There’s more…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also print the weights and plot them, and compare those values to the
    ones of regular linear regression, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印权重并绘制它们，将这些值与普通线性回归的值进行比较，如下所示：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here is the output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For visualization, we can also plot these values with the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化，我们还可以使用以下代码绘制这些值：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This code outputs the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出如下：
- en: '![Figure 3.6 – Learned values of each weight of the ridge regression model.
    Note that some are positive, some negative, and none are purely equal to zero.
    Also, the range is much smaller than without regularization](img/B19629_03_06.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 - 岭回归模型每个权重的学习值。请注意，一些是正数，一些是负数，并且没有一个完全等于零。此外，范围比无正则化时小得多](img/B19629_03_06.jpg)'
- en: Figure 3.6 – Learned values of each weight of the ridge regression model. Note
    that some are positive, some negative, and none are purely equal to zero. Also,
    the range is much smaller than without regularization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 - 岭回归模型每个权重的学习值。请注意，一些是正数，一些是负数，并且没有一个完全等于零。此外，范围比无正则化时小得多
- en: The weight values are now ranging from `-0.2` to .`0.5`, which is indeed much
    smaller than with no penalization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在权重值的范围是从`-0.2`到`.0.5`，确实比没有惩罚时要小得多。
- en: As expected, this adding regularization results in a test set R2-score that
    is much better and closer to the train set than without regularization.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，添加正则化导致测试集的R2分数比没有正则化时更好，且接近训练集的分数。
- en: See also
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about all the possible parameters and hyperparameters
    of the Ridge regression, you can take a look at the official scikit-learn documentation:
    [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有关岭回归的所有可能参数和超参数的更多信息，您可以查看官方scikit-learn文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml)。
- en: Regularizing with lasso regression
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用套索回归进行正则化
- en: '**Lasso** regression stands for **Least Absolute Shrinkage and Selection Operator**.
    This is a regularization method that is conceptually very close to ridge regression.
    In some cases, lasso regression outperforms ridge regression, which is why it’s
    useful to know what it does and how to use it. In this recipe, we will briefly
    explain what lasso regression is and then train a model using scikit-learn on
    the same California housing dataset.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**套索**回归代表**最小绝对值收缩和选择算子**。这是一个正则化方法，在某些情况下，套索回归优于岭回归，因此了解其作用和如何使用它非常有用。在这个示例中，我们将简要解释套索回归是什么，然后在相同的加利福尼亚房屋数据集上使用scikit-learn训练模型。'
- en: Getting ready
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Instead of using the L2-norm, lasso uses the L1-norm, so that the loss ![](img/Formula_03_017.png)
    is the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_018.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: While ridge regression tends to decrease weights close to zero quite smoothly,
    lasso is more drastic. Lasso, having a much steeper loss, tends to set weights
    to zero quite quickly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the ridge regression recipe, we’ll use the same libraries and assume
    they are installed: `numpy`, `sklearn`, and `matplotlib`. Also, we’ll assume the
    data is already downloaded and prepared.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit-learn implementation of lasso is available with the `Lasso` class.
    Like in the `Ridge` class, `alpha` is the term that allows control of this regularization:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The value of alpha is 0 means no regularization
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large value of alpha means high regularization
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again, we will reuse the same, already prepared dataset that we used for linear
    regression and ridge regression:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Lasso` class from scikit-learn:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We instantiate a lasso model with a value of `alpha=0.2`, which provides pretty
    good results and low overfitting, as we will see right away. However, feel free
    to test other values, as each dataset may have its very unique optimal value.
    Next, train the model on the training set using the `.fit()` method of the `Lasso`
    class:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Evaluate the lasso model on the training and test datasets, using the R2-score,
    implemented in the `.score()` method of the `Lasso` class:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This code outputs the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here we see an improvement when compared to the linear regression with no penalization,
    and we have improved against the ridge regression too, having an R2-score of about
    `0.57` on the test set.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we plot again for the weights instances, we now have the following values:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This outputs the plot in *Figure 3**.14*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Learned values of each weight of the lasso model. Note that,
    unlike the ridge model, several weights are set to zero](img/B19629_03_07.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Learned values of each weight of the lasso model. Note that, unlike
    the ridge model, several weights are set to zero
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: As expected, some values are set to 0, with an overall range of `-0.5` to `0.7`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization allowed us to significantly improve performance on the
    test set in this case, while also reducing overfitting.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the official documentation for more information on the lasso
    class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Another technique called group lasso can be useful for regularization; find
    out more about it here: [https://group-lasso.readthedocs.io/en/latest/](https://group-lasso.readthedocs.io/en/latest/).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with elastic net regression
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic net regression, besides having a very fancy name, is nothing more than
    a combination of ridge and lasso penalization. It’s a regularization method that
    can be of help in some specific cases. Let’s have a look at what it means in terms
    of loss, and then train a model on the California housing dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea with elastic net is to have both L1 and L2 regularization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the loss ![](img/Formula_03_019.png) is the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_020.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: The two hyperparameters, ![](img/Formula_03_021.png) and ![](img/Formula_03_022.png),
    can be fine-tuned.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into detail on the equations for the gradient descent, since deriving
    them is straightforward as soon as ridge and lasso are clear.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: To train a model, we again need the `sklearn` library, which we already installed
    in previous recipes. Also, we again assume that the California housing dataset
    is already downloaded and prepared.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In scikit-learn, elastic net is implemented in the `ElasticNet` class. However,
    instead of having two hyperparameters, ![](img/Formula_03_023.png) and ![](img/Formula_03_024.png),
    they are using two hyperparameters, `alpha` and `l1_ratio`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_025.png)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/Formula_03_026.png)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Let’s now apply this to our already prepared California housing dataset:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `ElasticNet` class from scikit-learn:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Instantiate an elastic net model. Values of `alpha=0.1` and `l1_ratio=0.5`
    have been chosen, but other values can be tested. Then train the model on the
    training set, using the `.fit()` method of the `ElasticNet` class:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Evaluate the elastic net model on the training and test dataset, using the
    R2-score computed with the `.``score()` method:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output for it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In this case, the results are not an improvement upon lasso regularization:
    perhaps a better fine-tuning of the hyperparameters is required to achieve equivalent
    performance.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: While more complicated to fine-tune because of having two hyperparameters, elastic
    net regression may offer more flexibility in regularization than the ridge or
    lasso regularizations. The use of hyperparameter optimization is the recommended
    method to find the right set of hyperparameters for any specific task.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In practice, elastic net regression is probably less widely used than the ridge
    and lasso regressions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation of scikit-learn for elastic net regression can be
    found here: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is really close to linear regression conceptually. Once
    linear regression is fully understood, logistic regression is just a couple of
    tricks away. But unlike linear regression, logistic regression is most commonly
    used for classification tasks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first explain what logistic regression is, and then train a model on the
    breast cancer dataset using scikit-learn.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike linear regression, logistic regression’s output is limited to a range
    of `0` to `1`. The first idea is exactly the same as linear regression, having
    for each feature ![](img/Formula_03_028.png) a parameter ![](img/Formula_03_027.png):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_029.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'There is one more step to limit the range to `0` to `1`, which is to apply
    the logistic function to this output *z*. As a reminder, the logistic function
    (also called the sigmoid function, although it’s a more generic function) is the
    following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_030.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'The logistic function has an S-shape, with values ranging from `0` to `1`,
    with a value of `0.5` on *x = 0*, as shown here in *Figure 3**.8*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0](img/B19629_03_08.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, by applying the sigmoid function, we have the logistic regression
    prediction *h(x)* as the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_031.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'This ensures an output value *h(x)* in the range of `0` to `1`. But it does
    not yet allow us to have a classification. The final step is to apply a threshold
    (for example, `0.5`) to have a classification prediction:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_032.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'As we did for linear regression, we now need to define a loss `L` that is to
    be minimized, in order to optimize the parameters ![](img/Formula_03_033.png)
    and *b*. The commonly used loss is the so-called **binary** **cross entropy**:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_034.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'We can see four extreme cases here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '*if y = 1 and h(x)**≃**1: L* *≃* *0*'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 1 and h(x)**≃**0: L* *≃* *+∞*'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 0 and h(x)**≃**0: L* *≃* *0*'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 0 and h(x)**≃**1: L* *≃* *+∞*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So now for us to have the expected behavior, which is, a loss that tends to
    0 indicating a highly accurate prediction, and that increases for a wrong prediction.
    This is represented in the following figure:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and
    y = 1](img/B19629_03_09.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and y
    = 1
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Again, one way to optimize logistic regression is through gradient descent,
    the exact same way as for linear regression. As a matter of fact, the equations
    are exactly the same, even if we can’t prove it here.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: To get ready for this recipe, all we need is to have the `scikit-learn` library
    installed.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is fully implemented in scikit-learn as the `LogisticRegression`
    class. Unlike linear regression in scikit-learn, logistic regression has regularization
    implemented directly into one single class. The following parameters are the ones
    to tweak in order to fine-tune regularization:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '`penalty`: This can be either `''l1''`, `''l2''`, `''elasticnet''`, or `''none''`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`C`: This is a float, inverse to regularization strength; the smaller the value,
    the greater the regularization'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply logistic regression with no regularization to
    the breast cancer dataset provided by scikit-learn. We will first load and prepare
    the data, and then train and evaluate the logistic regression model, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `load_breast_cancer` function, which will allow us to load the dataset,
    and the `LogisticRegression` class:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Load the dataset using the `load_breast_cancer` function:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Split the dataset using the `train_test_split` function. We assume it has been
    imported in a previous recipe. Otherwise, you will need to import it with `from
    sklearn.model_selection import train_test_split`. We choose `test_size=0.2` so
    that we have a training size of 80% and a test size of 20%:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We then prepare the data. Since the dataset is only composed of quantitative
    data, we just apply rescaling using the standard scaler: we instantiate it, fit
    it on the training set, and transform this same training set with `fit_transform()`.
    Finally, we rescale the test set with `.transform()`. Again, we assume the standard
    scaler has been imported from a previous recipe, otherwise, we need to import
    it with `from sklearn.preprocessing` `import StandardScaler`:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then we instantiate the logistic regression, and we specify `penalty=''none''`
    here so that we don’t use any penalization of the loss for pedagogical reasons.
    Check out the next recipe to see how penalization works. Then we train the logistic
    regression model on the training set with the `.``fit()` method:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Evaluate the model on both the training and the test set. The `.score()` method
    of the `LogisticRegression` class uses the accuracy metric, but any other metric
    can be used:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'This code outputs the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We are facing rather strong overfitting here, with classification accuracy of
    100% on the training set but only about 94% on the test set. This is a good start,
    but in the next recipe, we will use regularization to help improve the test accuracy.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing a logistic regression model
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression uses the same trick as linear regression to add regularization:
    it adds penalization to the loss. In this recipe, we will first briefly explain
    how penalization affects the loss, and how to add regularization using scikit-learn
    on the breast cancer dataset that we prepared in the previous recipe.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like linear regression, it is very easy to add a regularization term to
    the loss `L`, either an L1- or L2-norm of the parameters *w*. For example, the
    loss with an L2-norm would be the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_035.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: As we did for ridge regression, we’ve added a squared sum of the weights, with
    a hyperparameter in front of it. To keep as close as possible to the scikit-learn
    implementation, we will use 1/C instead of 𝜆 for the regularization hyperparameter,
    but the idea remains the same.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we assume the following libraries are already installed from
    previous recipes: `sklearn` and `matplotlib`. Also, we assume the data from the
    breast cancer dataset is already loaded and prepared from the previous recipe,
    so that we can directly reuse it.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now try to improve the test accuracy we had on the previous recipe by
    adding L2 regularization:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the logistic regression model. Here, we choose an L2 penalization
    and a regularization value of `C=0.1`. A lower value of `C` means greater regularization:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Fit the logistic regression model on the training set, with the `.``fit()`
    method:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Evaluate the model on both training and test set. We use the `.score()` method
    here, providing the accuracy score:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: As we can see here, adding L2 regularization allowed us to climb up to 98% accuracy
    on the test set, which is quite an improvement from about 94% without regularization.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Another reminder
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The best way to find the right regularization value for `C` is with hyperparameter
    optimization.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of curiosity, we can plot the train and test accuracy here for several
    values of regularization strength:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Here is the graph for it:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets](img/B19629_03_10.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'This plot can actually be read from right to left. We can see that as the value
    of `C` decreases (thus increasing regularization), the train accuracy keeps on
    decreasing, as the regularization gets higher and higher. On the other hand, decreasing
    `C` (thus increasing regularization) first allows us to improve the test results:
    the model is generalizing more and more. But at some point, adding regularization
    (decreasing `C` more) does not help any further, and even hurts the test accuracy.
    Indeed, adding too much regularization creates a high-bias model.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `LogisticRegression` implementation of scikit-learn allows us to not only
    use L2 penalization but also L1 and elastic net, just like linear regression,
    which allows us to have some flexibility on the best regularization for any given
    dataset and task.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation can be checked for more details: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right regularization
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear models share this regularization method with L1 and L2 penalization.
    The only difference in the implementation is the fact that linear regression has
    its own class for each regularization type, as mentioned here:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '`LinearRegression` for no regularization'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RidgeRegression` for L2 regularization'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lasso` for L1 regularization'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ElasticNet` for both L1 and L2'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression has an integrated implementation, passing `L1` or `L2` as
    the class parameter.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: With `C` parameter for `L2` regularization for both the `SVC` classification
    class and the `SVR` regression class.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'But for linear regression as well as logistic regression, one question remains:
    should we use L1 or L2 regularization?'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will provide some practical tips about whether to use L1
    or L2 penalization in some cases and then we will perform a grid search on the
    breast cancer dataset using logistic regression to find the best regularization.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no absolute answer to what penalization is best. Most of the time,
    the only way to find out is by doing hyperparameter optimization. But in some
    cases, data itself or external constraints may give hints about which regularization
    to use. Let’s have a quick look. First, let’s compare L1 and L2 regularization,
    then let’s explore hyperparameter optimization.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: L1 versus L2 regularization
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: L1 and L2 regularization have intrinsic differences that can sometimes help
    us make an educated guess upfront, and then save computational time. Let’s have
    a look at these differences.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, L1 regularization tends to set some weights to zero, and thus
    may allow feature selection. Thus, if we have a dataset with many features, it
    can be helpful to have this information, and then just remove those features in
    the future.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Also, L1 regularization uses the absolute value of weights in the loss, making
    it more robust to outliers compared to L2 regularization. If our dataset could
    contain outliers, it is to be considered.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in terms of computing speed, L2 regularization is adding a quadratic
    term and is as a consequence less computationally expensive than L1 regularization.
    If training speed is of concern, L2 regularization should be considered before
    L1.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we can consider the following cases to be ones where the choice
    can be made up front based on data or other constraints such as computation resources:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'The data contains numerous features, many of lesser importance: L1 regularization'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data contains many outliers: L1 regularization'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training computation resources are of concern: L2 regularization'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more practical and perhaps pragmatic approach is just to do hyperparameter
    optimization, with L1 or L2 being a hyperparameter (elastic net regression could
    be added too).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: We will use hyperparameter optimization with grid search as implemented by scikit-learn,
    optimizing a logistic regression model with both L1 and L2 regularization on the
    breast cancer dataset.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we expect the `scikit-learn` library to be installed from a
    previous recipe. We also assume that the breast cancer data is already loaded
    and prepared from the *Training a logistic regression* *model* recipe.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will perform a grid search on a given set of hyperparameters, more specifically,
    L1 and L2 penalization with several values of penalization, `C`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the `GridSearchCV` class from `sklearn`:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Then we define the parameter grid. This is the space of hyperparameters that
    we want to test. Here, we will try both L1 and L2 penalization, and for each penalization,
    we will try the `C` values in `[0.01, 0.03, 0.06, 0.1, 0.3, 0.6]` as follows:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Next, we instantiate the grid search object. Several parameters are passed
    here:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LogisticRegression`, for which we specify the solver to be `liblinear` (check
    the *See also* section for more information) so that it can handle both L1 and
    L2 penalization. We assume the class is already imported from a previous recipe;
    otherwise, you can import it with `from` `sklearn.linear_model` `import` `LogisticRegression`.'
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scoring=''accuracy''`, but it can be any other relevant metric.'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5` cross-validation folds here with `cv=5`, as it is pretty standard, but
    depending on the size of the dataset, other values may be just fine too:'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Train the grid on the training data with the `.fit()` method. Then we can display
    the best hyperparameters found out of curiosity, using the `best_params_` attribute:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: In this case, the hyperparameters appear to be `C=0.06` with L2 penalization.
    We can now evaluate the model.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluate the model on both the training and the test sets, using the `.score()`
    method that computes accuracy. Using `.score()` or `.predict()` directly on the
    grid object will automatically compute the best model predictions:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: In this case, this improved the performance on the test set, although it’s not
    always that easy. But the method remains the same and can be applied to any dataset.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The grid search official documentation can be found here: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about the solvers and penalizations available in scikit-learn
    can be found here: [https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
