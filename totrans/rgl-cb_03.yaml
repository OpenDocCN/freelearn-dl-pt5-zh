- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization with Linear Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A huge part of **machine learning** (**ML**) is made up of linear models. Although
    sometimes considered less powerful than their nonlinear counterparts (such as
    tree-based models or deep learning models), linear models do address many concrete,
    valuable problems. Customer churn and advertising optimization are just a couple
    of problems where linear models may be the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a linear regression with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with lasso regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with elastic net regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have learned how to use and regularize some
    of the most commonly used linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, besides loading data, you will learn how to fit and compute
    inferences with several linear models. In order to do so, the following libraries
    are required:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a linear regression model with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is one the most basic ML models we can use, but it is very
    useful. Most people used linear regression in high school without talking about
    ML, and still use it on a regular basis within spreadsheets. In this recipe, we
    will explain the basics of linear regression, and then train and evaluate a linear
    regression model using scikit-learn on the California housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is not a complicated model, but it is still useful to understand
    what is under the hood to get the best out of it.
  prefs: []
  type: TYPE_NORMAL
- en: The way linear regression works is pretty straightforward. Heading back to the
    real estate price example, if we consider a feature *x* such as the apartment
    surface and a label *y* such as the apartment price, a common solution would be
    to find *a* and *b* such that *y = ax +* *b*.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this is not so simple in real life. There is usually no *a* and
    *b* that makes this equality always respected. It is more likely that we can define
    a function `h(x)` that aims to give a value as close as possible to *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we may have not just one feature *x*, but several features *x*1, *x*2,…,
    *x*n, representing apartment surface, location, floor, number of rooms, exponent
    features, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this logic, we would end up with a prediction *h(x)* that may look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_03_002.png)is the weight associated to the feature ![](img/Formula_03_003.png)
    *x*j, and *b* is a bias term. This is just a generalization of the previous *y
    = ax + b* to *n* features. This formula allows a linear regression to predict
    virtually any real number.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our ML model is to find the set of *w* and *b* values that minimizes
    prediction errors on the training set. By this, we mean finding the parameters
    *w* and *b* so that *h(x)* and *y* are as close as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to achieve that is to minimize the loss *L*, that can be defined here
    as a slightly modified **mean squared** **error** (**MSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_03_005.png) is the ground truth of the sample *i* in the training
    set, and *m* is the number of samples in the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The loss is usually a representation of the difference between the ground truth
    and the predictions. Hence, minimizing the loss allows the model to predict values
    that are as close as possible to the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing this mean squared error would allow us to find the set of *w* and
    *b* values so that the prediction *h(x)* is as close as possible to the ground
    truth *y*. Schematically, this can be represented as finding the *w* that minimizes
    the loss, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross](img/B19629_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Loss function as a function of a parameter theta, having a global
    minimum at the cross
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is: how do we find the set of values to minimize the loss?
    There are several ways of solving this problem. One commonly used technique in
    ML is gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: What is gradient descent? In a few words, it is going down the curve to the
    minimum value in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does this work? It’s a multi-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with random values of the parameters *w* and *b*. Random values are usually
    defined using normal distribution centered on zero. This is why having scaled
    features may help significantly for convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss for the given data and current values of *w* and *b*. As defined
    earlier, we may use the mean squared error to compute the loss *L*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure is a good representation of the situation at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The loss function with the global minimum at the red cross,
    and a possible random initial state at the blue cross](img/B19629_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The loss function with the global minimum at the red cross, and
    a possible random initial state at the blue cross
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the loss gradient with respect to each parameter ![](img/Formula_03_006.png)
    . This is nothing more than the slope of the loss for a given parameter, which
    can be computed with the following equations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_03_007.jpg)![](img/Formula_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: One may notice that the slope is expected to decrease as we get closer to the
    minimum. Indeed, as we get close to the minimum, the error tends to zero and so
    does the slope, based on these equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply gradient descent to parameters. Apply the gradient descent to parameters,
    with a user-defined learning rate *α*. This is computed using the following formulas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_03_009.jpg)![](img/Formula_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This allows us to take a step toward the minimum, as represented in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The gradient descent allows us to take one step down the loss
    function, allowing us to get closer to the global minimum](img/B19629_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The gradient descent allows us to take one step down the loss function,
    allowing us to get closer to the global minimum
  prefs: []
  type: TYPE_NORMAL
- en: 'Iterate through *steps 2 to 4* until convergence or max iteration. This would
    allow us to reach the optimal parameters, as represented in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum](img/B19629_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – With enough iterations and a convex loss function, the parameters
    will converge to the global minimum
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A learning rate *α* that is too large would miss the global minimum, or even
    diverge, while one that is too small would take forever to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete this recipe, the following libraries have to be installed: `numpy`,
    `matplotlib`, and `sklearn`. They can be installed with `pip` in the terminal,
    with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, all this procedure is fully implemented in scikit-learn, and the
    only thing you need to do is fully reuse this library. Let’s now train a linear
    regression on the California housing dataset provided by scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch_california_housing`: A function that allows us to load the dataset'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_test_split`: A function that allows us to split the data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`StandardScaler`: A class that allows us to rescale the data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LinearRegression`: The class that contains the implementation of the linear
    regression'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is what the code looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`X` with `X*X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`train_test_split` function, with `test_size=0.2`, meaning we end up having
    80% of the data in the training set, and 20% in the test set, split at random.
    This is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Prepare the data**: Since we have only quantitative features here, the only
    preparation we need is rescaling. We can use the standard scaler of scikit-learn.
    We need to instantiate it, then fit it on the training set and transform the training
    set, and finally we transform the test set. Feel free to use any other rescaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model on the training set. The model must be instantiated, and here
    we use the default parameters, so nothing is specified. Once the model is instantiated,
    we can use the `.fit()` method on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on both the training and test set. Here, we use the `.score()`
    method of the `LinearRegression` class, which provides `R2-score`, but you can
    use any other metric provided in `sklearn.metrics` that suits regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, there is a significant difference between the train and test
    set’s scores, indicating model overfitting on the train set. To address this problem,
    regularization techniques will be proposed in the following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the model has been trained, we can access all the parameters *w* (here,
    16 values for 16 input features) as well as the intercept *b*, with the attributes,
    `.coef_`, and the `.intercept_` value of the `LinearRegression` object respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot these values, we will notice that their values range between approximately
    `-8` and `2` on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a visual representation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Learned values of each weight of the linear regression model.
    The range of the values is quite large, from -8 to 2](img/B19629_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Learned values of each weight of the linear regression model. The
    range of the values is quite large, from -8 to 2
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To have a full understanding of how to use linear regression using scikit-learn,
    it is good practice to check the official documentation of the class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: We now have a good understanding of how linear regression works, and we will
    see in the next section how to regularize it with penalization.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very common and useful way to regularize a linear regression is through penalization
    of the loss function. In this recipe, after reviewing what it means to add penalization
    to the loss function in the case of ridge regression, we will train a ridge model
    on the same California housing dataset as in the previous recipe, and see how
    it can improve the score thanks to regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to make sure that a model’s parameters are not going to overfit is
    to keep them close to zero: if the parameters do not have the possibility to evolve
    freely, they are less likely to overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, ridge regression adds a new term (regularization term) to the
    loss ![](img/Formula_03_011.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/Formula_03_013.png)is the *L2* norm of *w*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this loss, we intuitively understand that high values of weights *w* are
    not possible, and thus overfitting is less likely. Also, *𝜆* is a hyperparameter
    (it can be fine-tuned) allowing us to control the regularization level:'
  prefs: []
  type: TYPE_NORMAL
- en: A high value of *𝜆* means high regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of *𝜆**=0* means no regularization, for example, regular linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The gradient descent formulas are slightly updated to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_015.jpg)![](img/Formula_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s now see how to use ridge regression with scikit-learn: the same libraries
    required in the previous recipe must be installed: `numpy`, `sklearn`, and `matplotlib`.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, we assume the data is already downloaded and prepared from the previous
    recipe. To download, split and prepare the data, refer to the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we are reusing the same data from the previous recipe. We will
    just train and evaluate another model on this exact same data, including the feature
    engineering with the squared features. The related implementation in scikit-learn
    for ridge regression is the `Ridge` class, where the `alpha` class attribute is
    equivalent to the *𝜆* in the preceding equation. Let’s use it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Ridge` class from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then instantiate a ridge model. A regularization parameter of `alpha=5000`
    has been selected here, but every dataset may need a very specific hyperparameter
    value to perform best. Next, train the model on the training set (previously prepared)
    with the `.fit()` method, which is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then evaluate the model. Here, we compute and display the R2-score provided
    by the `.score()` of the ridge class, but any other regression metric could be
    used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We will notice that we are getting better results on the test set compared to
    the linear regression model (with no regularization) by allowing the R2-score
    on the test set to be slightly above `0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also print the weights and plot them, and compare those values to the
    ones of regular linear regression, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For visualization, we can also plot these values with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Learned values of each weight of the ridge regression model.
    Note that some are positive, some negative, and none are purely equal to zero.
    Also, the range is much smaller than without regularization](img/B19629_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Learned values of each weight of the ridge regression model. Note
    that some are positive, some negative, and none are purely equal to zero. Also,
    the range is much smaller than without regularization
  prefs: []
  type: TYPE_NORMAL
- en: The weight values are now ranging from `-0.2` to .`0.5`, which is indeed much
    smaller than with no penalization.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, this adding regularization results in a test set R2-score that
    is much better and closer to the train set than without regularization.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information about all the possible parameters and hyperparameters
    of the Ridge regression, you can take a look at the official scikit-learn documentation:
    [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with lasso regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Lasso** regression stands for **Least Absolute Shrinkage and Selection Operator**.
    This is a regularization method that is conceptually very close to ridge regression.
    In some cases, lasso regression outperforms ridge regression, which is why it’s
    useful to know what it does and how to use it. In this recipe, we will briefly
    explain what lasso regression is and then train a model using scikit-learn on
    the same California housing dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of using the L2-norm, lasso uses the L1-norm, so that the loss ![](img/Formula_03_017.png)
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While ridge regression tends to decrease weights close to zero quite smoothly,
    lasso is more drastic. Lasso, having a much steeper loss, tends to set weights
    to zero quite quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the ridge regression recipe, we’ll use the same libraries and assume
    they are installed: `numpy`, `sklearn`, and `matplotlib`. Also, we’ll assume the
    data is already downloaded and prepared.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit-learn implementation of lasso is available with the `Lasso` class.
    Like in the `Ridge` class, `alpha` is the term that allows control of this regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of alpha is 0 means no regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large value of alpha means high regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again, we will reuse the same, already prepared dataset that we used for linear
    regression and ridge regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `Lasso` class from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We instantiate a lasso model with a value of `alpha=0.2`, which provides pretty
    good results and low overfitting, as we will see right away. However, feel free
    to test other values, as each dataset may have its very unique optimal value.
    Next, train the model on the training set using the `.fit()` method of the `Lasso`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the lasso model on the training and test datasets, using the R2-score,
    implemented in the `.score()` method of the `Lasso` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here we see an improvement when compared to the linear regression with no penalization,
    and we have improved against the ridge regression too, having an R2-score of about
    `0.57` on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we plot again for the weights instances, we now have the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the plot in *Figure 3**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Learned values of each weight of the lasso model. Note that,
    unlike the ridge model, several weights are set to zero](img/B19629_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Learned values of each weight of the lasso model. Note that, unlike
    the ridge model, several weights are set to zero
  prefs: []
  type: TYPE_NORMAL
- en: As expected, some values are set to 0, with an overall range of `-0.5` to `0.7`.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization allowed us to significantly improve performance on the
    test set in this case, while also reducing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the official documentation for more information on the lasso
    class: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another technique called group lasso can be useful for regularization; find
    out more about it here: [https://group-lasso.readthedocs.io/en/latest/](https://group-lasso.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with elastic net regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic net regression, besides having a very fancy name, is nothing more than
    a combination of ridge and lasso penalization. It’s a regularization method that
    can be of help in some specific cases. Let’s have a look at what it means in terms
    of loss, and then train a model on the California housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea with elastic net is to have both L1 and L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the loss ![](img/Formula_03_019.png) is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The two hyperparameters, ![](img/Formula_03_021.png) and ![](img/Formula_03_022.png),
    can be fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into detail on the equations for the gradient descent, since deriving
    them is straightforward as soon as ridge and lasso are clear.
  prefs: []
  type: TYPE_NORMAL
- en: To train a model, we again need the `sklearn` library, which we already installed
    in previous recipes. Also, we again assume that the California housing dataset
    is already downloaded and prepared.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In scikit-learn, elastic net is implemented in the `ElasticNet` class. However,
    instead of having two hyperparameters, ![](img/Formula_03_023.png) and ![](img/Formula_03_024.png),
    they are using two hyperparameters, `alpha` and `l1_ratio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_025.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/Formula_03_026.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Let’s now apply this to our already prepared California housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `ElasticNet` class from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate an elastic net model. Values of `alpha=0.1` and `l1_ratio=0.5`
    have been chosen, but other values can be tested. Then train the model on the
    training set, using the `.fit()` method of the `ElasticNet` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the elastic net model on the training and test dataset, using the
    R2-score computed with the `.``score()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the results are not an improvement upon lasso regularization:
    perhaps a better fine-tuning of the hyperparameters is required to achieve equivalent
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: While more complicated to fine-tune because of having two hyperparameters, elastic
    net regression may offer more flexibility in regularization than the ridge or
    lasso regularizations. The use of hyperparameter optimization is the recommended
    method to find the right set of hyperparameters for any specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In practice, elastic net regression is probably less widely used than the ridge
    and lasso regressions.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation of scikit-learn for elastic net regression can be
    found here: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is really close to linear regression conceptually. Once
    linear regression is fully understood, logistic regression is just a couple of
    tricks away. But unlike linear regression, logistic regression is most commonly
    used for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first explain what logistic regression is, and then train a model on the
    breast cancer dataset using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike linear regression, logistic regression’s output is limited to a range
    of `0` to `1`. The first idea is exactly the same as linear regression, having
    for each feature ![](img/Formula_03_028.png) a parameter ![](img/Formula_03_027.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is one more step to limit the range to `0` to `1`, which is to apply
    the logistic function to this output *z*. As a reminder, the logistic function
    (also called the sigmoid function, although it’s a more generic function) is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic function has an S-shape, with values ranging from `0` to `1`,
    with a value of `0.5` on *x = 0*, as shown here in *Figure 3**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0](img/B19629_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, by applying the sigmoid function, we have the logistic regression
    prediction *h(x)* as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This ensures an output value *h(x)* in the range of `0` to `1`. But it does
    not yet allow us to have a classification. The final step is to apply a threshold
    (for example, `0.5`) to have a classification prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we did for linear regression, we now need to define a loss `L` that is to
    be minimized, in order to optimize the parameters ![](img/Formula_03_033.png)
    and *b*. The commonly used loss is the so-called **binary** **cross entropy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see four extreme cases here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*if y = 1 and h(x)**≃**1: L* *≃* *0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 1 and h(x)**≃**0: L* *≃* *+∞*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 0 and h(x)**≃**0: L* *≃* *0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*if y = 0 and h(x)**≃**1: L* *≃* *+∞*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So now for us to have the expected behavior, which is, a loss that tends to
    0 indicating a highly accurate prediction, and that increases for a wrong prediction.
    This is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and
    y = 1](img/B19629_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and y
    = 1
  prefs: []
  type: TYPE_NORMAL
- en: Again, one way to optimize logistic regression is through gradient descent,
    the exact same way as for linear regression. As a matter of fact, the equations
    are exactly the same, even if we can’t prove it here.
  prefs: []
  type: TYPE_NORMAL
- en: To get ready for this recipe, all we need is to have the `scikit-learn` library
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is fully implemented in scikit-learn as the `LogisticRegression`
    class. Unlike linear regression in scikit-learn, logistic regression has regularization
    implemented directly into one single class. The following parameters are the ones
    to tweak in order to fine-tune regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '`penalty`: This can be either `''l1''`, `''l2''`, `''elasticnet''`, or `''none''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`C`: This is a float, inverse to regularization strength; the smaller the value,
    the greater the regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply logistic regression with no regularization to
    the breast cancer dataset provided by scikit-learn. We will first load and prepare
    the data, and then train and evaluate the logistic regression model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `load_breast_cancer` function, which will allow us to load the dataset,
    and the `LogisticRegression` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using the `load_breast_cancer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset using the `train_test_split` function. We assume it has been
    imported in a previous recipe. Otherwise, you will need to import it with `from
    sklearn.model_selection import train_test_split`. We choose `test_size=0.2` so
    that we have a training size of 80% and a test size of 20%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then prepare the data. Since the dataset is only composed of quantitative
    data, we just apply rescaling using the standard scaler: we instantiate it, fit
    it on the training set, and transform this same training set with `fit_transform()`.
    Finally, we rescale the test set with `.transform()`. Again, we assume the standard
    scaler has been imported from a previous recipe, otherwise, we need to import
    it with `from sklearn.preprocessing` `import StandardScaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we instantiate the logistic regression, and we specify `penalty=''none''`
    here so that we don’t use any penalization of the loss for pedagogical reasons.
    Check out the next recipe to see how penalization works. Then we train the logistic
    regression model on the training set with the `.``fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on both the training and the test set. The `.score()` method
    of the `LogisticRegression` class uses the accuracy metric, but any other metric
    can be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We are facing rather strong overfitting here, with classification accuracy of
    100% on the training set but only about 94% on the test set. This is a good start,
    but in the next recipe, we will use regularization to help improve the test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing a logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression uses the same trick as linear regression to add regularization:
    it adds penalization to the loss. In this recipe, we will first briefly explain
    how penalization affects the loss, and how to add regularization using scikit-learn
    on the breast cancer dataset that we prepared in the previous recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like linear regression, it is very easy to add a regularization term to
    the loss `L`, either an L1- or L2-norm of the parameters *w*. For example, the
    loss with an L2-norm would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we did for ridge regression, we’ve added a squared sum of the weights, with
    a hyperparameter in front of it. To keep as close as possible to the scikit-learn
    implementation, we will use 1/C instead of 𝜆 for the regularization hyperparameter,
    but the idea remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we assume the following libraries are already installed from
    previous recipes: `sklearn` and `matplotlib`. Also, we assume the data from the
    breast cancer dataset is already loaded and prepared from the previous recipe,
    so that we can directly reuse it.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now try to improve the test accuracy we had on the previous recipe by
    adding L2 regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the logistic regression model. Here, we choose an L2 penalization
    and a regularization value of `C=0.1`. A lower value of `C` means greater regularization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the logistic regression model on the training set, with the `.``fit()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on both training and test set. We use the `.score()` method
    here, providing the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see here, adding L2 regularization allowed us to climb up to 98% accuracy
    on the test set, which is quite an improvement from about 94% without regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Another reminder
  prefs: []
  type: TYPE_NORMAL
- en: The best way to find the right regularization value for `C` is with hyperparameter
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of curiosity, we can plot the train and test accuracy here for several
    values of regularization strength:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the graph for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets](img/B19629_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Accuracy as a function of the C parameter, for both the training
    and test sets
  prefs: []
  type: TYPE_NORMAL
- en: 'This plot can actually be read from right to left. We can see that as the value
    of `C` decreases (thus increasing regularization), the train accuracy keeps on
    decreasing, as the regularization gets higher and higher. On the other hand, decreasing
    `C` (thus increasing regularization) first allows us to improve the test results:
    the model is generalizing more and more. But at some point, adding regularization
    (decreasing `C` more) does not help any further, and even hurts the test accuracy.
    Indeed, adding too much regularization creates a high-bias model.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `LogisticRegression` implementation of scikit-learn allows us to not only
    use L2 penalization but also L1 and elastic net, just like linear regression,
    which allows us to have some flexibility on the best regularization for any given
    dataset and task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation can be checked for more details: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear models share this regularization method with L1 and L2 penalization.
    The only difference in the implementation is the fact that linear regression has
    its own class for each regularization type, as mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LinearRegression` for no regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RidgeRegression` for L2 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lasso` for L1 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ElasticNet` for both L1 and L2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression has an integrated implementation, passing `L1` or `L2` as
    the class parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `C` parameter for `L2` regularization for both the `SVC` classification
    class and the `SVR` regression class.
  prefs: []
  type: TYPE_NORMAL
- en: 'But for linear regression as well as logistic regression, one question remains:
    should we use L1 or L2 regularization?'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will provide some practical tips about whether to use L1
    or L2 penalization in some cases and then we will perform a grid search on the
    breast cancer dataset using logistic regression to find the best regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no absolute answer to what penalization is best. Most of the time,
    the only way to find out is by doing hyperparameter optimization. But in some
    cases, data itself or external constraints may give hints about which regularization
    to use. Let’s have a quick look. First, let’s compare L1 and L2 regularization,
    then let’s explore hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: L1 versus L2 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: L1 and L2 regularization have intrinsic differences that can sometimes help
    us make an educated guess upfront, and then save computational time. Let’s have
    a look at these differences.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, L1 regularization tends to set some weights to zero, and thus
    may allow feature selection. Thus, if we have a dataset with many features, it
    can be helpful to have this information, and then just remove those features in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: Also, L1 regularization uses the absolute value of weights in the loss, making
    it more robust to outliers compared to L2 regularization. If our dataset could
    contain outliers, it is to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in terms of computing speed, L2 regularization is adding a quadratic
    term and is as a consequence less computationally expensive than L1 regularization.
    If training speed is of concern, L2 regularization should be considered before
    L1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we can consider the following cases to be ones where the choice
    can be made up front based on data or other constraints such as computation resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data contains numerous features, many of lesser importance: L1 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data contains many outliers: L1 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training computation resources are of concern: L2 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more practical and perhaps pragmatic approach is just to do hyperparameter
    optimization, with L1 or L2 being a hyperparameter (elastic net regression could
    be added too).
  prefs: []
  type: TYPE_NORMAL
- en: We will use hyperparameter optimization with grid search as implemented by scikit-learn,
    optimizing a logistic regression model with both L1 and L2 regularization on the
    breast cancer dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we expect the `scikit-learn` library to be installed from a
    previous recipe. We also assume that the breast cancer data is already loaded
    and prepared from the *Training a logistic regression* *model* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will perform a grid search on a given set of hyperparameters, more specifically,
    L1 and L2 penalization with several values of penalization, `C`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the `GridSearchCV` class from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we define the parameter grid. This is the space of hyperparameters that
    we want to test. Here, we will try both L1 and L2 penalization, and for each penalization,
    we will try the `C` values in `[0.01, 0.03, 0.06, 0.1, 0.3, 0.6]` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we instantiate the grid search object. Several parameters are passed
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LogisticRegression`, for which we specify the solver to be `liblinear` (check
    the *See also* section for more information) so that it can handle both L1 and
    L2 penalization. We assume the class is already imported from a previous recipe;
    otherwise, you can import it with `from` `sklearn.linear_model` `import` `LogisticRegression`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scoring=''accuracy''`, but it can be any other relevant metric.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5` cross-validation folds here with `cv=5`, as it is pretty standard, but
    depending on the size of the dataset, other values may be just fine too:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the grid on the training data with the `.fit()` method. Then we can display
    the best hyperparameters found out of curiosity, using the `best_params_` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, the hyperparameters appear to be `C=0.06` with L2 penalization.
    We can now evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluate the model on both the training and the test sets, using the `.score()`
    method that computes accuracy. Using `.score()` or `.predict()` directly on the
    grid object will automatically compute the best model predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, this improved the performance on the test set, although it’s not
    always that easy. But the method remains the same and can be applied to any dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The grid search official documentation can be found here: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about the solvers and penalizations available in scikit-learn
    can be found here: [https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression).'
  prefs: []
  type: TYPE_NORMAL
