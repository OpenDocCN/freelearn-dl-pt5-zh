- en: '*Chapter 8*: Extended fastai and Deployment Features'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, you have learned how to ingest and explore datasets using
    fastai, how to train fastai models with tabular, text, and image datasets, and
    how to deploy fastai models. Throughout the book so far, the emphasis has been
    on covering as much of the functionality of fastai as possible using the highest-level
    fastai API. In particular, we have emphasized using `dataloaders` objects as the
    basis for defining the datasets used to train the model. Up to this point in the
    book, we have taken the *happy path* whenever possible. To demonstrate how to
    accomplish tasks using fastai, we have chosen the most straightforward way possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to take some steps off the *happy path* to explore
    additional features of fastai. You will learn how to track what is happening with
    your model more closely, how to control the training process, and generally how
    to take advantage of more of the capabilities that fastai has to offer. We are
    also going to cover some more advanced topics related to model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting more details about models trained with tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting more details about image classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model with augmented data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using callbacks to get the most out of your training cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making your model deployments available to others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying thumbnails in your image classification model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will be using both your cloud environment and your local
    environment for model deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have completed the setup sections from [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, and have a working Gradient instance or Colab setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you have completed the steps described in the *Setting up fastai
    on your local system* recipe in [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, to set up fastai on your local system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you have cloned the repo for the book from [https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook](https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook)
    and have access to the `ch8` folder. This folder contains the code samples described
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting more details about models trained with tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Training a model in fastai with a curated tabular dataset* recipe of
    [*Chapter 3*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083)*, Training Models
    with Tabular Data*, you trained a fastai model on a tabular dataset and used accuracy
    as the metric. In this recipe, you will learn how to get additional metrics for
    this model: **precision** and **recall**. Precision is the ratio of true positives
    divided by true positives plus false positives. Recall is the ratio of true positives
    divided by true positives plus false negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: These are useful metrics. For example, the model we are training in this recipe
    is predicting whether an individual's income is over 50,000\. If it is critical
    to avoid false positives – that is, predicting an income over 50,000 when the
    individual has an income less than that – then we want precision to be as high
    as possible. This recipe will show you how to add these useful metrics to the
    training process for a fastai model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_tabular_datasets_metrics.ipynb`
    notebook in the `ch8` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, you will be running through the `training_with_tabular_datasets_metrics.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Define and train model` cell to import
    the required libraries, set up your notebook, and prepare the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `recall_instance = Recall()` – defines a recall metric object. Note that
    you will get an error if you put `Recall` directly in the metrics list for the
    model. Instead, you need to define a recall metric object, such as `recall_instance`,
    and include that object in the metrics list. See the fastai documentation ([https://docs.fast.ai/metrics.html#Recall](https://docs.fast.ai/metrics.html#Recall))
    for more details on this metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `precision_instance = Precision()` – defines a precision metric object. You
    will get an error if you put `Precision` directly in the metrics list, so you
    need to define the `precision_instance` object first and then include that object
    in the metrics list for the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `metrics=[accuracy,recall_instance,precision_instance]` – specifies that
    the model will be trained with accuracy, recall, and precision as metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell, as shown in *Figure 8.1*, includes accuracy as well
    as recall and precision for each epoch of the training run:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Training output including recall and precision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Training output including recall and precision
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained a model with tabular data and generated recall
    and precision metrics for the training process.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have asked yourself how I knew that you could not include `Recall`
    and `Precision` directly in the metrics list for the model and that you needed
    to define objects first and then include those objects in the metrics list. The
    simple answer is that it was trial and error. More specifically, when I attempted
    to include `Recall` and `Precision` directly in the metrics list, I got the following
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When I searched for this error, I came across this post in the fastai forum:
    [https://forums.fast.ai/t/problem-with-f1scoremulti-metric/63721](https://forums.fast.ai/t/problem-with-f1scoremulti-metric/63721).
    The post explained the reason for the error and that to get around it I needed
    to define the `Recall` and `Precision` objects first and then include them in
    the metrics list.'
  prefs: []
  type: TYPE_NORMAL
- en: This experience is an example of both a weakness and a strength of fastai. The
    weakness is that the documentation for `Precision` and `Recall` is missing an
    essential detail – you cannot use them directly in the metrics list. The strength
    is that the fastai forum provides clear and accurate resolutions for issues like
    this and demonstrates the strength of the fastai community.
  prefs: []
  type: TYPE_NORMAL
- en: Getting more details about image classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Training a classification model with a simple curated vision dataset*
    recipe of [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)*, Training
    Models with Visual Data*, you trained an image classification model using the
    `CIFAR` curated dataset. The code to train and exercise the model was straightforward
    because we took advantage of the highest-level structures in fastai. In this recipe,
    we will revisit this image classification model and explore techniques in fastai
    to get additional information about the model and its performance, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining the **pipeline** that fastai generates to prepare the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting a chart of the training and validation loss during the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the images where the model performs worst
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the **confusion matrix** to get a snapshot of where the model is
    not doing well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the model to the test set and examining the model's performance on
    the test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we are going to expand the recipe where we trained the `CIFAR`
    curated dataset. By taking advantage of the additional features of fastai, we
    will be able to understand our model better.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_image_datasets_datablock.ipynb`
    notebook in the `ch8` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `training_with_image_datasets_datablock.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the following cell to ensure that `model_path` points to a writeable
    directory in your Gradient or Colab instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the cells in the notebook up to the `Define a DataBlock` cell to import
    the required libraries, set up your notebook, and ingest the `CIFAR` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `DataBlock` object. By defining a `DataBlock`
    object explicitly, we will be able to do additional actions that we couldn''t
    do directly on a `dataloaders` object, such as getting a summary of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `blocks = (ImageBlock, CategoryBlock)` – specifies that the input to the
    model is images (`ImageBlock`) and the target is a categorization of the input
    images (`CategoryBlock`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `get_items=get_image_files` – specifies that the `get_image_files` function
    is called to get the input to the `DataBlock` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `splitter=RandomSplitter(seed=42)` – specifies how the validation set is
    defined from the training set. By default, 20% of the training set is randomly
    selected to make up the validation set. By specifying a value for `seed`, this
    call to `RandomSplitter` produces consistent results across multiple runs. See
    the documentation for `RandomSplitter` ([https://docs.fast.ai/data.transforms.html#RandomSplitter](https://docs.fast.ai/data.transforms.html#RandomSplitter))
    for more details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `get_y=parent_label` – specifies that the labels for the images (that is,
    the categories to which the images belong) are defined by the directories where
    the images are located in the input dataset. For example, on Gradient, cat images
    in the training set are found in the `/storage/data/cifar10/train/cat` directory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `dataloaders` object using the `DataBlock`
    object `db` that you created in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `db.dataloaders` – specifies that the `dataloaders` object is created using
    the `DataBlock` object `db`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `path/'train'` – specifies that the input to this model is only the training
    subset of the `CIFAR` dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get a summary of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at the key parts of the output of this cell. First, the output
    shows details about the input dataset, including the source directory, the size
    of the whole dataset, and the size of the training and validation sets, as shown
    in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16216_8_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.2 – Summary description of the input dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the output shows the pipeline that fastai applies to a single input sample,
    including the source directory of the sample, the image object that is created
    for the sample, and the label (category) for the sample, as shown in *Figure 8.3*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Summary description of the pipeline for one image file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.3 – Summary description of the pipeline for one image file
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the output shows the pipeline that fastai applies to build a single batch,
    that is, converting the image objects that are output from the sample pipeline
    into tensors. As shown in *Figure 8.4*, the 32 x 32-pixel image objects are converted
    to 3 x 32 x 32 tensors, where the first dimension contains color information about
    the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Summary description of the pipeline applied to a single batch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.4 – Summary description of the pipeline applied to a single batch
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, the output shows the transformations applied to the batches as a whole,
    as shown in *Figure 8.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Summary description of the pipeline applied to all batches'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.5 – Summary description of the pipeline applied to all batches
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `DataBlock` object for the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that unlike the `DataBlock` object for the training set, we define `db_test`
    with an explicit value for `valid_pct`. We set this value to 99% because we won't
    be doing any training of the model when we apply the test set to it, so there
    is no need to hold back any of the test set for training. We don't set `valid_pct`
    to `1.0` because that value will produce an error when you apply a summary to
    `db_test`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Define and train the model` cell to
    examine the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define the model with a `cnn_learner` object. Note
    that because you defined a `dataloaders` object from the `DataBlock` object you
    get the best of both worlds: the additional features (such as a summary) available
    only with a `DataBlock` object along with the familiar code pattern for `dataloaders`
    objects that you have used for most of the recipes in this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note the `cbs=ShowGraphCallback()` parameter. With this parameter, the output
    of the training process includes a graph of training and validation loss, as shown
    in *Figure 8.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Training and validation loss graph'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.6 – Training and validation loss graph
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This graph contains the same data as the table of training results that you
    get by default from the training process, as shown in *Figure 8.7*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Training and validation loss table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.7 – Training and validation loss table
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to save the trained model. We update the path for the
    model temporarily to a directory that is writeable in Gradient so that we can
    save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `save_path = learn.path` – specifies that the current path for the model
    is saved to `save_path`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `learn.path = Path(model_path)` – specifies that the path for the model is
    set to a writeable directory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `learn.save('cifar_save_'+modifier)` – saves the model. We will load the
    saved model later to exercise the model with the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `learn.path = save_path` – resets the path for the model to its original
    value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to confirm the performance of the trained model in terms
    of accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second value in the output should match the accuracy you saw in the final
    epoch of the training, as shown in *Figure 8.8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Output of validate'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.8 – Output of validate
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the cells up to the `Examine the top loss examples and confusion matrix`
    cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to see the samples where the model has the biggest loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `interp = ClassificationInterpretation.from_learner(learn)` – specifies that
    `interp` is an interpretation object for the `learn` model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `interp.plot_top_losses(9, figsize=(15,11))` – specifies that the nine images
    with the highest losses should be displayed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output shows examples of the images where the model has the biggest loss
    along with the predicted contents of the image and the actual contents of the
    image. You can think of these as the images where the model did the worst job
    predicting what''s in the images. *Figure 8.9* shows a subset of the output. For
    example, for the first displayed image, the model predicted the image contained
    a bird while the image is actually labeled as a cat:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Examples of images with the most loss'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.9 – Examples of images with the most loss
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to generate a confusion matrix for the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is a confusion matrix that summarizes the performance
    of the trained model, as shown in *Figure 8.10*. A confusion matrix is an *N*
    x *N* matrix, where *N* is the number of target classes. It compares the actual
    target class values (the vertical axis) with the predicted values (the horizontal
    axis). The diagonal of the matrix shows the cases where the model made the correct
    prediction, while all the entries off the diagonal are cases where the model made
    an incorrect prediction. For example, in the confusion matrix shown in *Figure
    8.10*, in 138 instances the model predicted that an image of a dog was a cat,
    and in 166 instances it predicted that an image of a cat was a dog:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Confusion matrix for the trained model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.10 – Confusion matrix for the trained model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have examined the performance of the model with the training set,
    let''s examine how the model does with the test set. To do this, we will define
    a new `dataloaders` object using the test set, define a model with this `dataloaders`
    object, load the saved weights from the trained model, and then do the same steps
    to evaluate the model performance that we did with the model trained on the training
    set. To begin, run the following cell to create a new `dataloaders` object `dls_test`
    that is defined with the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a new model object, `learn_test`, that is
    based on the `dataloaders` object you created in the previous step. Note that
    the model definition is identical to the model you defined for the training set
    in *Step 8* except that it uses the `dataloaders` object `dls_test` that was defined
    with the test dataset in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to load the saved weights from the model trained with
    the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `learn_test.path = Path(model_path)` – specifies that the path for the `learn_test`
    model is changed to the directory where the model weights were saved in *Step
    10*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `learn_test.load('cifar_save_'+modifier')` – specifies that the `learn_test`
    model gets loaded with the weights from the model trained with the training set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we are all set to exercise the model with the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see the overall accuracy of the model on the test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second value in the output is the accuracy of the model on the test set,
    as shown in *Figure 8.11*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Output of validate on the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.11 – Output of validate on the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see the images in the test set where the model has
    the biggest loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows examples of the images in the test set where there was the
    biggest loss along with the predicted contents of the image and the actual contents
    of the image in the test set. *Figure 8.12* shows a subset of the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Sample images from the test set where the model performed worst'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.12 – Sample images from the test set where the model performed worst
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get the confusion matrix for the model applied to
    the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is a confusion matrix that summarizes the performance
    of the model on the test set, as shown in *Figure 8.13*. Note that the numbers
    in this confusion matrix are smaller than the numbers in the confusion matrix
    for the model applied to the training set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Confusion matrix for the model applied to the test set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13 – Confusion matrix for the model applied to the test set
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have worked through an image classification model to see
    the benefit of additional information that fastai can provide. You have also learned
    how to apply the model to the entire test set and examine the model's performance
    on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's instructive to compare the model we created in the recipe in this section
    with the model we created in the *Training a classification model with a simple
    curated vision dataset* recipe of [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)*,
    Training Models with Visual Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the definition of the `dataloaders` object from the [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)*,
    Training Models with Visual Data*, recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the definition of the `dataloaders` object from this recipe. Unlike
    the previous `dataloaders` definition, this definition makes use of the `DataBlock`
    object `db`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the definition of the `DataBlock` object `db`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: What is the benefit of defining the `dataloaders` object using the `DataBlock`
    object?
  prefs: []
  type: TYPE_NORMAL
- en: First, by starting with a `DataBlock` object, you have more control over the
    details of how the dataset is set up. You can explicitly define the function that
    defines the input dataset (the function assigned to `get_items`) as well as the
    function that defines the label (the function assigned to `get_y`). You may recall
    that we took advantage of this flexibility in the *Training a multi-image classification
    model with a curated vision dataset* recipe of [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)*,
    Training Models with Visual Data*. In that recipe, we needed to ensure that the
    input dataset excluded images that had no annotation. By using a `DataBlock` object
    in that recipe, we were able to define a custom function to assign to `get_items`
    that excluded images with no annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we can take advantage of some additional functions in fastai if we have
    a `DataBlock` object. In this recipe, we were able to apply the `summary()` function
    to the `DataBlock` object to see the pipeline that fastai applied to the input
    dataset. The `summary()` function cannot be applied to a `dataloaders` object,
    so we would have missed out on the additional details about the data pipeline
    if we had not defined a `DataBlock` object.
  prefs: []
  type: TYPE_NORMAL
- en: If a `DataBlock` object is so useful, why didn't we use one in the *Training
    a classification model with a simple curated vision dataset* recipe of [*Chapter
    6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)*, Training Models with Visual
    Data*? We only used a `dataloaders` object in that recipe (instead of starting
    with a `DataBlock` object) because that recipe was relatively simple – we didn't
    need the additional flexibility of a `DataBlock` object. Throughout this book,
    we have stuck with the highest-level APIs for fastai whenever we could, including
    in that recipe. Simplicity is a key benefit of fastai, so if it's possible to
    go with the highest-level APIs (including using `dataloaders` directly), it makes
    sense to keep it simple and stick with the highest-level APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model with augmented data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, you learned about some additional facilities that fastai
    provides to keep track of your model and you learned how to apply the test set
    to the model trained on the training set. In this recipe, you will learn how to
    combine these techniques with another technique that fastai makes it easy to incorporate
    in your model training: **data augmentation**. With data augmentation, you can
    expand your training dataset to include variations on the original training samples
    and, potentially, improve the performance of the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.14* shows some results of augmentation applied to an image from the
    `CIFAR` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Augmentation applied to an image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.14 – Augmentation applied to an image
  prefs: []
  type: TYPE_NORMAL
- en: You can see in these examples that the augmentations applied to the image include
    flipping the image on the vertical axis, rotating the image, and adjusting the
    brightness of the image.
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous recipe, in this recipe, we are going to work with an image
    classification model trained on the `CIFAR` curated dataset, but in this recipe,
    we will experiment with augmenting the images in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_image_datasets_datablock_augmented.ipynb`
    notebook in the `ch8` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, you will be running through the `training_with_image_datasets_datablock_augmented.ipynb`
    notebook. Here is a high-level summary of what you will do in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model with non-augmented data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model with augmented data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise the model trained with non-augmented data with the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise the model trained with augmented data with the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have the notebook open in your fastai environment, complete the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the following cell to ensure that `model_path` points to a writeable
    directory in your Gradient or Colab instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the cells in the notebook up to the `Try augmenting the training set` cell
    to import the required libraries, set up your notebook, and train a model on the
    non-augmented `CIFAR` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to create a new `DataBlock` object `db2` that incorporates
    augmentation and a `dataloaders` object `dls2` that is defined with this new `DataBlock`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `db2 = db.new(batch_tfms=aug_transforms())` – defines a new `DataBlock`
    object `db2` that incorporates the default augmentation defined by `aug_transforms()`.
    See the fastai documentation for details on `aug_transforms()`: [https://docs.fast.ai/vision.augment.html#aug_transforms](https://docs.fast.ai/vision.augment.html#aug_transforms).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `dls2 = db2.dataloaders(path/'train',bs=32)` – defines a new `dataloaders`
    object `dls2` based on the `DataBlock` object `db2`. Now, `dls2` is defined with
    the training subset of the input dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get a summary of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell gives us details about the pipeline that is applied
    to the dataset. First, the output shows details about the input dataset, including
    the source directory, the size of the whole dataset, and the size of the training
    and validation sets, as shown in *Figure 8.15*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Summary description of the input dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.15 – Summary description of the input dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the output shows the pipeline that fastai applies to a single input sample,
    including the source directory of the sample, the image object that is created
    for the sample, and the label (category) for the sample, as shown in *Figure 8.16*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Summary description of the pipeline for one image file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.16 – Summary description of the pipeline for one image file
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the output shows the pipeline that fastai applies to build a single batch,
    that is, converting the image objects that are output from the sample pipeline
    into tensors. As shown in *Figure 8.17*, the 32 x 32-pixel image objects are converted
    to 3 x 32 x 32 tensors, where the first dimension contains color information about
    the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Summary description of the pipeline applied to a single batch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.17 – Summary description of the pipeline applied to a single batch
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These first three portions of the output are identical to the same portions
    of the output of `summary()` from the *Getting more details about image classification
    models* recipe. The final portion, however, is different and describes the transformations
    that are applied to images in the augmentation process, as shown in *Figure 8.18*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Description of the pipeline (including augmentation transformations)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.18 – Description of the pipeline (including augmentation transformations)
    applied to all batches
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see examples of the augmentation transformations
    being applied to an image in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `unique=True` argument specifies that we want to see examples of augmentations
    applied to a single image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An example of the output of this cell is shown in *Figure 8.19*: note the variations
    in the augmented images, including being flipped on the vertical axis, having
    varying brightness, and having varying degrees of vertical space taken up by the
    truck:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Augmented versions of an image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_019.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.19 – Augmented versions of an image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a model to be trained with the augmented dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to train the model with the augmented dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell shows the training loss, validation loss, and validation
    accuracy, as shown in *Figure 8.20*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Output of training the model with the augmented dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.20 – Output of training the model with the augmented dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to save the model that was trained on the augmented
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the cells up to the `Examine the performance of the model trained on the
    augmented dataset on the test set` cell to get an idea of the performance of the
    model trained on the non-augmented dataset making predictions on the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `dataloaders` object `dls_test` associated
    with the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define the `learn_augment_test` model to be exercised
    on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to load the `learn_augment_test` model with the weights
    saved from training the model with the augmented dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we have a `learn_augment_test` learner object that has the weights from
    training with the augmented dataset and is associated with the test dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get the overall accuracy of the `learn_augment_test`
    model exercised on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell shows the accuracy of the model on the test set, as
    shown in *Figure 8.21*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Accuracy of the model trained on the augmented dataset applied
    to the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.21 – Accuracy of the model trained on the augmented dataset applied
    to the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get examples of the images from the test dataset
    where the model trained on the augmented dataset has the biggest loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "The output of this cell shows the images from the test set where the model\
    \ trained on the augmented dataset has the biggest loss. *Figure 8.22* shows examples\
    \ from \Lthis output:"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Examples of images where the model trained on'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: the augmented dataset has the biggest losses
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.22 – Examples of images where the model trained on the augmented dataset
    has the biggest losses
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see the confusion matrix for the model trained on
    the augmented dataset applied to the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the confusion matrix shown in *Figure 8.23*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Confusion matrix for the model trained on the augmented dataset
    applied to the test set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.23 – Confusion matrix for the model trained on the augmented dataset
    applied to the test set
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained a fastai image classification model using
    an augmented dataset and exercised the trained model on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we went through the process of training an image classification
    model on an augmented dataset. The notebook we worked through in this recipe also
    included training an image classification on a non-augmented dataset. Let's compare
    the performance between the models to see whether it was worthwhile to use the
    augmented dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare the training results between the two models, as shown in *Figure
    8.24*, the performance of the model trained on the non-augmented dataset seems
    to be better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Comparison of training results when training with and without
    augmented data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.24 – Comparison of training results when training with and without
    augmented data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compare the performance of the two models on the test dataset. *Figure
    8.25* shows the output of `validate()` for the model trained on the non-augmented
    and augmented datasets, applied to the test set. Again, the model trained on the
    non-augmented dataset seems to be better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Comparison of validate() results when training with and without
    augmented data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.25 – Comparison of validate() results when training with and without
    augmented data
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.26* shows the confusion matrixes for the model trained on the non-augmented
    and augmented datasets, applied to the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Comparison of confusion matrix when training with and without
    augmented data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.26 – Comparison of confusion matrix when training with and without
    augmented data
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the model trained with the augmented dataset does not seem to have
    better performance on the test set than the model trained with the non-augmented
    data. This may be disappointing, but it's OK.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this recipe was to demonstrate to you how to take advantage
    of the augmented data facilities in fastai, not to do a thorough analysis to get
    the best possible results with augmented data. Not every application will benefit
    from augmentation, and we only tried the default augmentation approach. fastai
    offers a wide range of augmentation options for image models, as described in
    the fastai documentation: [https://docs.fast.ai/vision.augment.html](https://docs.fast.ai/vision.augment.html).
    It is possible that this particular dataset had characteristics (such as relatively
    low resolution) that made augmentation less effective. It''s also possible that
    the set of augmentations included in the default fastai augmentation approach
    were not ideal for this dataset, and a custom set of augmentations could have
    produced better results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How does `aug_transforms()`, the function applied in this recipe, augment the
    images in the training set? We can get a clue by comparing the pipeline summary
    for the model trained with the non-augmented training set with the model trained
    with augmented data. *Figure 8.27* shows the final section of the output of `summary()`
    for the non-augmented `DataBlock` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Output of summary for the non-augmented DataBlock object'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.27 – Output of summary for the non-augmented DataBlock object
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.28* shows the final section of the output of `summary()` for the
    augmented `DataBlock` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – Output of summary for the non-augmented DataBlock object'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.28 – Output of summary for the non-augmented DataBlock object
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing the section of the summary output for these two models, you can
    understand what transformations are being applied on the augmented dataset, as
    shown in *Figure 8.29*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29 – Transformations applied in aug_transforms()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.29 – Transformations applied in aug_transforms()
  prefs: []
  type: TYPE_NORMAL
- en: You have now examined some of the differences between training an image classification
    model with an augmented dataset and training with a non-augmented dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using callbacks to get the most out of your training cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this book, we have kicked off the training process for a fastai model
    by applying `fit_one_cycle` or `fine_tune` to the learner object and have then
    just let the training run through the specified number of epochs. For many of
    the models we have trained in this book, this approach has been good enough, particularly
    for models where each epoch takes a long time and we only train for one or two
    epochs. But what about models where we want to train the model for 10 or more
    epochs? If we simply let the training process go to the end, we face the problem
    shown in the training results shown in *Figure 8.30*. In *Figure 8.30*, we see
    the result of training a tabular model for 10 epochs with `metric` set to `accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – Results from a 10-epoch run training a model with tabular data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_030.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.30 – Results from a 10-epoch run training a model with tabular data
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this training process is to get a model with the best accuracy.
    With this goal in mind, there are two problems with the results shown in *Figure
    8.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: The best accuracy is in epoch 5, but the model we get at the end of the training
    process has the accuracy from the last epoch. The output of `validate()` for this
    model, shown in *Figure 8.31*, shows that the accuracy for the model is not the
    best accuracy from the training run:![Figure 8.31 – Output of validate() for the
    model trained with the 10-epoch run
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_8_031.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.31 – Output of validate() for the model trained with the 10-epoch run
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that if the accuracy was still steadily improving up to the end of the
    training run, the model would likely not be overfitting yet, though you would
    want to validate this by exercising the trained model with the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The training run keeps going after the accuracy is no longer improving, so training
    capacity is being wasted. For a run like this one, where every epoch completes
    in less than a minute, it isn't really a problem. However, consider the impact
    of wasted training cycles if each epoch took an hour and you were running on a
    paid Gradient instance. In that case, you could end up wasting many dollars on
    training cycles that didn't improve the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Luckily, fastai includes a solution to both of these problems: **callbacks**.
    You can use callbacks to control the training process and automatically take actions
    during training. In this recipe, you will learn how to specify callbacks in fastai
    that stop the training process when the model isn''t getting any better and save
    the best model from the training run. You will revisit the model you trained in
    the *Training a model in fastai with a curated tabular dataset* recipe of [*Chapter
    3*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083)*, Training Models with Tabular
    Data*, but this time you will control the training process for the model using
    fastai callbacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_tabular_datasets_callbacks.ipynb`
    notebook in the `ch8` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, you will be running through the `training_with_tabular_datasets_callbacks.ipynb`
    notebook. In this recipe, you will train three variations of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Training with no callbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training with a single callback to end the training process when it has stopped
    improving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training with two callbacks: one to end the training process when it has stopped
    improving, and one to save the best model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have the notebook open in your fastai environment, complete the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the following cell to ensure that `model_path` points to a writeable
    directory in your Gradient or Colab instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the cells in the notebook up to the `Define and train the model with no
    callbacks` cell to import the required libraries, set up your notebook, and define
    a `dataloaders` object for the `ADULT_SAMPLE` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By calling `set_seed()` this way and defining a `dataloaders` object before
    each training run, we can get consistent training results across multiple training
    runs. You will see the significance of having consistent training results when
    we compare the results for training with and without callbacks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define and train a model with no callbacks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The call to `set_seed()` specifies that the random seeds related to the model
    are set to `42` (an arbitrary value) and that results are reproducible, which
    is exactly what we want.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell shows the training loss, validation loss, and accuracy
    by epoch, as well as the overall time taken for the training run, as shown in
    *Figure 8.32*. Note that the training run goes for all 10 epochs, even though
    the accuracy stops improving after epoch 2:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.32 – Results of training a model with no callbacks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_032.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.32 – Results of training a model with no callbacks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get the accuracy for the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is shown in *Figure 8.33*. The accuracy for the model
    is the accuracy achieved in epoch 9, the final epoch of the training run:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.33 – Output of validate() for a model trained with no callbacks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_033.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.33 – Output of validate() for a model trained with no callbacks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define and train a model with one callback – early
    stopping when the model''s accuracy no longer improves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `fit` statement for this model includes the definition of an `EarlyStoppingCallback`
    callback. Here are the arguments used to define the callback:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `monitor='accuracy'` – specifies that the callback is tracking the value
    of the accuracy metric. When the accuracy metric stops improving for the specified
    period, the callback will be triggered.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `min_delta=0.01` – specifies that the callback pays attention to changes
    in accuracy between epochs that are at least 0.01\. That is, if the change in
    accuracy between epochs is less than 0.01, the callback ignores the change.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `patience=3` – specifies that the callback is triggered when the accuracy
    metric stops improving for 3 epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell shows the training loss, validation loss, and accuracy
    by epoch, as well as the overall time taken for the training run, as shown in
    *Figure 8.34*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.34 – Results of training a model with an early stopping callback'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_034.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.34 – Results of training a model with an early stopping callback
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that now the training stops after epoch 5\. You can see that the metric
    being tracked by the callback, `accuracy`, stops improving after epoch 2\. Once
    3 more epochs have passed (the value set for the `patience` parameter for the
    callback), the training process stops early, at epoch 5, even though the `fit_one_cycle()`
    call specifies a run of 10 epochs. So, with an early stopping callback, we save
    4 epochs and about 25% of the training time (51 seconds with the early stopping
    callback versus 68 seconds with no callbacks).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get the accuracy for the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is shown in *Figure 8.35*. The accuracy for the model
    is the accuracy achieved in epoch 5, the final epoch of the training run:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.35 – Output of validate() for a model trained with an early stopping
    callback'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_035.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.35 – Output of validate() for a model trained with an early stopping
    callback
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With an early stopping callback, we reduce the training time compared to a
    model with no callbacks, but the accuracy of the trained model is lower than the
    best accuracy achieved during the training run. Let''s train another model that
    includes a callback to save the best model. This callback will ensure that the
    accuracy of the trained model is the best result we get from the training run.
    To do this, start by running the following cell to define and train a model with
    two callbacks – early stopping when the model''s accuracy no longer improves,
    and saving the best model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to the definition of an `EarlyStoppingCallback` callback like the
    one specified in *Step 5*, the `fit` statement for this model also includes a
    `SaveModelCallback` callback. Here the arguments used to define this callback
    are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `monitor='accuracy'` – specifies that the `SaveModelCallback` callback is
    tracking the value of the accuracy metric. The model will be saved for epochs
    where the accuracy hits a new high-water mark.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `min_delta=0.01` – specifies that the callback pays attention to changes
    in accuracy between epochs that are at least 0.01.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that this cell also includes statements to adjust the model path to a directory
    that is writeable in Gradient. If you don't change the model's path to a writeable
    directory, you will get an error in Gradient when you run this cell. Also, note
    that you may see the **Saved filed doesn't contain an optimizer state** warning
    message when you run this cell – you don't need to worry about this message for
    the purposes of this recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell shows the training loss, validation loss, and accuracy
    by epoch, as well as the overall time taken for the training run, as shown in
    *Figure 8.36*. Note that the training stops after epoch 5 thanks to the early
    stopping callback:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.36 – Results of training a model with an early stopping callback
    and a model saving callback'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_8_036.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.36 – Results of training a model with an early stopping callback and
    a model saving callback
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to get the accuracy for the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is shown in *Figure 8.37*. The accuracy for the model
    is the accuracy achieved in epoch 2, the best accuracy during the training run:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.37 – Output of validate() for a model trained with early stopping
    and model saving callbacks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_037.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.37 – Output of validate() for a model trained with early stopping and
    model saving callbacks
  prefs: []
  type: TYPE_NORMAL
- en: With both callbacks, we avoid running epochs that won't improve the performance
    of the model and we end up with a trained model with the best performance out
    of the epochs of the training run.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully applied callbacks in fastai to optimize
    the training process so that you get the most out of the training cycle for your
    models.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe demonstrated how you can use fastai callbacks to control the training
    process so you get the best results from the system capacity and time that you
    apply to training. There are a few additional details about fastai callbacks that
    are worth reviewing. In this section of the recipe, we will dig a bit deeper into
    how callbacks are used in fastai.
  prefs: []
  type: TYPE_NORMAL
- en: The set_seed() function is not the default function for fastai
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to get clear comparisons between the performance of the model with
    and without callbacks, we needed to control random differences between training
    runs. That is, we want to train the model multiple times and get consistent loss
    and accuracy for epochs between training runs. For example, if the accuracy reported
    in epoch 1 is `0.826271` for the first training run, we want to get exactly the
    same accuracy in epoch 1 for each subsequent training run. By ensuring consistent
    performance between training runs, we can do *apples-to-apples* comparisons between
    runs, focusing on the impact of the callbacks rather than random differences between
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we used the `set_seed()` function to control random differences
    between training runs. You may have noticed that while fastai documentation includes
    a `set_seed()` function ([https://docs.fast.ai/torch_core.html#set_seed](https://docs.fast.ai/torch_core.html#set_seed)),
    we don''t use that function in the recipe. Instead, we use the following function,
    which is adapted from code shared in this forum discussion – [https://github.com/fastai/fastai/issues/2832](https://github.com/fastai/fastai/issues/2832):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Why use this custom `set_seed()` function instead of the default `set_seed()`
    function documented by fastai? The simple reason is that the default `set_seed()`
    function did not work as expected – I didn't get consistent training results with
    it. With the `set_seed()` function listed previously, on the other hand, I was
    able to get consistent training results.
  prefs: []
  type: TYPE_NORMAL
- en: The Callbacks section of summary() doesn't include the callbacks defined in
    the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may notice that the end of the `training_with_tabular_datasets_callbacks.ipynb`
    notebook includes calls to the `summary()` function for the learner objects for
    the three models that you trained in the recipe. You might expect the `Callbacks`
    section of the output of the `summary()` function for the two models that include
    callbacks, `learn_es` and `learn_es_sm`, would list the early stopping and model
    saving callbacks, but that is not the case. *Figure 8.38* shows the `Callbacks`
    section for the two models that have explicitly defined callbacks, and this section
    is identical to the `Callbacks` section for the model with no callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.38 – Callbacks section of summary() output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.38 – Callbacks section of summary() output
  prefs: []
  type: TYPE_NORMAL
- en: Why doesn't the `Callbacks` section of `summary()` output include the callbacks
    defined with the model? The `summary()` function just lists the callbacks defined
    by fastai itself, not the callbacks that you define.
  prefs: []
  type: TYPE_NORMAL
- en: Is there anything else you can do with callbacks in fastai?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, we used callbacks to ensure that the training cycle was as
    efficient as possible, but that''s just scratching the surface of what you can
    do with callbacks in fastai. The fastai framework supports a broad range of callbacks
    to control aspects of the training process. In fact, in this book you have come
    across several kinds of fastai callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking callbacks** – the early stopping and model saving callbacks that
    we used in this recipe are examples of tracking callbacks. This category of callbacks
    is documented here: [https://docs.fast.ai/callback.tracker.html](https://docs.fast.ai/callback.tracker.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ShowGraphCallback` callback to display a graph of training and validation
    loss. Progress and logging callbacks are documented here: [https://docs.fast.ai/callback.progress.html](https://docs.fast.ai/callback.progress.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to_fp16()` to specify mixed-precision training for the language model you
    trained in that section. Callbacks for mixed-precision training are documented
    here: [https://docs.fast.ai/callback.fp16.html](https://docs.fast.ai/callback.fp16.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The callbacks that you have used so far by working through the recipes in this
    book show you some of the power and flexibility that you can get by using callbacks
    with fastai.
  prefs: []
  type: TYPE_NORMAL
- en: Making your model deployments available to others
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*, Deployment
    and Model Maintenance*, you deployed a couple of fastai models. To get a prediction,
    you pointed your browser to `localhost:5000` and that opened up `home.html` where
    you set your scoring parameters, requested a prediction, and then got a prediction
    back in `show-prediction.html`. All this happened on your local system. Through
    the web deployments done in [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, you can only get to the deployment on your
    local system because `localhost` is only accessible on your local system. What
    if you wanted to share these deployments with friends to allow them to try out
    your models on their own computers?
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to do this is using a tool called `localhost` on your computer
    with people working on other computers. In this recipe, we will go through steps
    that show you how to use ngrok to make your deployments available to others.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Follow the instructions at [https://dashboard.ngrok.com/get-started](https://dashboard.ngrok.com/get-started)
    to set up a free ngrok account and to set up ngrok on your local system. Note
    the directory where you install ngrok – you will need it in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the help of ngrok, you can get a URL you can share with your friends so
    they can try out your fastai model deployments. This recipe will show you how
    to share your deployment of the tabular model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To share the deployment you created in the *Deploying a fastai model trained
    on a tabular dataset* recipe of [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Make the directory where you installed ngrok your current directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following command on the command line/terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the deployment of the tabular model by making `deploy_tabular` your current
    directory and entering the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On a computer other than your local system, point the browser at the `https`
    `Forwarding` URL that you copied in *Step 2*. You should see `home.html`, as shown
    in *Figure 8.40*:![Figure 8.40 – home.html
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_8_040.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.40 – home.html
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In `home.html`, select `Get prediction` and confirm that you see a prediction
    in `show-prediction.html`, as shown in *Figure 8.41*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.41 – show-prediction.html'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_041.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.41 – show-prediction.html
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully shared one of your fastai model deployments
    so that it is available to users on other systems with whom you share the ngrok
    forwarding URL.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you run ngrok, you create a secure tunnel to `localhost` on your local
    system. When you share the forwarding URL that is returned by ngrok, the people
    who receive the URL can point their browser to that URL to see what is being served
    at `localhost` on your local system.
  prefs: []
  type: TYPE_NORMAL
- en: 'You specify the port that the ngrok tunnel points to when you start ngrok.
    For example, when you entered the following command in this recipe, you specified
    that the ngrok forwarding URL points to `localhost:5000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now you have some background on how ngrok makes it easy for you to share your
    model deployment with users on other systems.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying thumbnails in your image classification model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you created a deployment for the image classification model in the *Deploying
    a fastai model trained on an image dataset* recipe of [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, there was something useful missing: a thumbnail
    of the image that you selected in `home.html`. In this recipe, you will see how
    to update `home.html` to display a thumbnail of the image that the user selects.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have followed the steps in the *Deploying a fastai model trained
    on an image dataset* recipe of [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, to do the deployment of the image classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you will be making updates to `home.html` in your image classification
    model deployment so that a thumbnail of the image you selected gets displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make these updates, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Make the directory for the image classification deployment, `deploy_image`,
    your current directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make the `templates` subdirectory your current directory by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open `home.html` in an editor. I like to use Notepad++ (see [https://notepad-plus-plus.org/](https://notepad-plus-plus.org/)),
    but you can use the editor of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the control for the file dialog to specify an `onchange` action of calling
    the `getFile()` JavaScript function. After the update, the control for the file
    dialog should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a new JavaScript function in `home.html` called `getFile()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this function definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `file_list = document.getElementById("image_field").files;` – specifies that
    `file_list` contains the list of files associated with the `image_field` file
    selector
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `img_f = document.createElement("img");` – defines a new image element called
    `img_f` on the page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `img_f.setAttribute("id","displayImage");` – assigns the `displayImage` ID
    to the new image element
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `document.body.appendChild(img_f);` – adds the new image element to the bottom
    of the page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) `document.getElementById("displayImage").src = URL.createObjectURL(file_list[0]);`
    – specifies that the file selected in the file dialog is displayed in the new
    image element
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save your changes to `home.html` and make `deploy_image` your current directory
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Flask server by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open `localhost:5000` in your browser to display `home.html`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the `test_images` directory. If everything works, you should see a thumbnail
    of the image you selected at the bottom of the page, as shown in *Figure 8.42*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.42 – home.html with a thumbnail of the selected image displayed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_042.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.42 – home.html with a thumbnail of the selected image displayed
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have updated the deployment for the image classification
    model so that you see a thumbnail when you select an image.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you have seen a small example of dynamically creating an element
    in an HTML page. Unlike all the other elements in `home.html`, the image element
    where we show the thumbnail is not there when `home.html` is first loaded. The
    image element only gets created when an image file has been selected and the `getFile()`
    function gets called. Why didn't we just have the image element hardcoded to be
    there when the file is first loaded, like the other controls?
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had hardcoded the image element, then when you loaded `home.html`, before
    an image file had been selected, you would see an ugly missing-image graphic where
    the thumbnail should be, as shown in *Figure 8.43*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.43 – home.html with a hardcoded image element'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_043.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.43 – home.html with a hardcoded image element
  prefs: []
  type: TYPE_NORMAL
- en: By dynamically creating the image element only after an image had been selected,
    we can avoid the messy missing-image graphic.
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that in the *Maintaining your fastai model* recipe of [*Chapter
    7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*, Deployment and Model Maintenance*,
    I mentioned that in a professional deployment you would not have to manually adjust
    the controls in `home.html` when the dataset schema got new values for categorical
    columns or brand-new columns. You could use the technique described in this recipe,
    dynamically creating controls, for most of the controls in `home.html` to make
    the deployment easier to adapt to schema changes.
  prefs: []
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have reviewed a broad range of topics, from taking full
    advantage of the information that fastai provides about models to making your
    web deployments available to users outside of your local system. In this section,
    you will get the opportunity to exercise some of the concepts you learned about
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Explore the value of repeatable results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *Using callbacks to get the most out of your training cycle* recipe,
    you made a call to the `set_seed()` function prior to training each of the models.
    In that recipe, I stated that these calls were necessary to ensure repeatable
    results for multiple training cycles. Test out this assertion yourself by following
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, make a copy of the `training_with_tabular_datasets_callbacks.ipynb` notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update your new notebook by commenting out the first call to `set_seed()` and
    rerun the whole notebook. What differences do you see in the output of `fit_one_cycle()`
    between the model with no callbacks and the model with an early stopping callback?
    What about differences in the output of `fit_one_cycle()` between the model with
    an early stopping callback and the model with both an early stopping and a model
    saving callback?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update your new notebook again by commenting out the second call to `set_seed()`
    and rerun the whole notebook. Now what differences do you see in the output of
    `fit_one_cycle()` between the model with no callbacks and the model with an early
    stopping callback? What about differences in the output of `fit_one_cycle()` between
    the model with an early stopping callback and the model with both an early stopping
    and a model saving callback?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, update your notebook again by commenting out the final call to `set_seed()`
    and rerun the whole notebook again. Compare the results of the models again. Has
    anything changed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! Hopefully, by following these steps, you have been able to
    prove to yourself the value of repeatable results. When you are comparing different
    variations of a model and you want to ensure you are getting an *apples-to-apples*
    comparison between variations, it can be very valuable to use the facilities in
    fastai to control the random initialization of aspects of the model so that you
    are guaranteed consistent results from multiple training runs.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying multiple thumbnails in your image classification model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *Displaying thumbnails in your image classification model deployment*
    recipe, you learned how to enhance the image classification deployment from the
    *Deploying a fastai model trained on an image dataset* recipe of [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*,
    Deployment and Model Maintenance*, by showing a thumbnail of the image selected
    for classification. You may recall that in the *Test your knowledge* section of
    [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*, Deployment and
    Model Maintenance*, you went through the exercise of enhancing the image classification
    model deployment by allowing the user to select multiple images for classification.
    What if you want to combine these two enhancements by allowing the user to select
    multiple images for classification and showing thumbnails of each selected image?
    Go through the following steps to explore how you would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure you have completed the exercise in the *Test your knowledge* section
    of [*Chapter 7*](B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178)*, Deployment
    and Model Maintenance*, to create a deployment of the image classification model
    that allows the user to select multiple images to be classified. You will be updating
    the code you completed in that section so that it shows thumbnails of the selected
    images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make a copy, called `deploy_image_multi_test`, of the `deploy_image_test` directory
    where you created the multi-image classification deployment. To do this, make
    the directory that contains `deploy_image_test` your current directory and run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make `deploy_image_multi_test/templates` your current directory. You will be
    making updates to the `home.html` file in this directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `home.html`, update the control for the file dialog to specify an `onchange`
    action of calling the `getFile()` JavaScript function. After the update, the control
    for the file dialog should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a new JavaScript function in `home.html` called `getFile()`. This function
    will be a generalization of the `getFile()` function you defined in the *Displaying
    thumbnails in your image classification model deployment* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this function definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `file_list = document.getElementById("image_field").files;` – specifies that
    `file_list` contains the list of files associated with the `image_field` file
    selector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `var di_string = "displayImage"` – defines the `di_string` string that will
    be used as the prefix of the IDs of the image elements that will be added to the
    page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `for (file_item of file_list)` – specifies that the `for` loop iterates through
    the items in `file_list`. For each item in `file_list`, an image element will
    be created to display the image associated with the item.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `img_f[i] = document.createElement("img");` – defines a new `img_f[i]`image
    element on the page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) `var di_1 = di_string.concat(i)` – defines a `di_1` string using the `dl_string`
    prefix and the `i` index. For example, the first time through the loop, the value
    of `di_1` will be `displayImage1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) `img_f[i].setAttribute("id",di_1);` – assigns the `di_1` ID to the `img_f[i]`
    image element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: g) `document.body.appendChild(img_f[i]);` – adds the `img_f[i]` image element
    to the bottom of the page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: h) `document.getElementById(di_1).src = URL.createObjectURL(file_item);` – specifies
    that the image file associated with `file_item` is displayed in the `img_f[i]`
    image element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With these changes, thumbnails for the files that the user selects in the file
    dialog will be displayed at the bottom of `home.html`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now test whether everything works. Save your changes to `home.html`, make `deploy_image_multi_test`
    your current directory, and start the Flask server by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open `localhost:5000` in your browser to display `home.html`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the `test_images` directory. If everything works, you should see thumbnails
    of each of the images you selected at the bottom of the page, as shown in *Figure
    8.44*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.44 – home.html showing thumbnails for multiple selected images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_8_044.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.44 – home.html showing thumbnails for multiple selected images
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have combined two enhancements to the image classification
    deployment to allow your users to select multiple images for the model to classify
    and see thumbnails for the images they selected.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and additional resources on fastai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, you have explored a broad range of the capabilities of the fastai
    framework. By adapting the recipes in this book, you should be able to apply fastai
    to create deep learning models to make predictions on a wide variety of datasets.
    You will also be able to deploy your models in simple web applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many more capabilities in fastai beyond those covered in this book.
    Here are some additional fastai resources that you can use to learn more about
    the platform:'
  prefs: []
  type: TYPE_NORMAL
- en: To dig deeper into fastai, you can check out the online documentation for the
    framework (https://docs.fast.ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want a comprehensive guide to fastai, I highly recommend the outstanding
    book from Jeremy Howard (the originator of fastai) and Sylvain Gugger *Deep Learning
    for Coders with Fastai and PyTorch* ([https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeremy Howard's YouTube channel ([https://www.youtube.com/user/howardjeremyp](https://www.youtube.com/user/howardjeremyp))
    is another excellent source of information about fastai, including videos of Howard's
    deep learning course built on fastai, *Practical Deep Learning for Coders* ([https://course.fast.ai/](https://course.fast.ai/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you are ready to go even deeper, Zachary Mueller's *Walk with fastai* site
    ([https://walkwithfastai.com/](https://walkwithfastai.com/)) is a fantastic resource
    that consolidates many insights from the fastai forum ([https://forums.fast.ai/](https://forums.fast.ai/))
    along with Mueller's own encyclopedic understanding of the platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this book and following the recipes in
    it. I hope that you have found this book useful, and I encourage you to apply
    what you have learned to do great things by using fastai to harness the power
    of deep learning.
  prefs: []
  type: TYPE_NORMAL
