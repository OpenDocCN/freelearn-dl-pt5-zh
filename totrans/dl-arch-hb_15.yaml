- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Deploying Deep Learning Models to Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境
- en: In the previous chapters, we delved into the intricacies of data preparation,
    **deep learning** (**DL**) model development, and how to deliver insightful outcomes
    from our DL models. Through meticulous data analysis, feature engineering, model
    optimization, and model analysis, we have learned the techniques to ensure our
    DL models can perform well and as desired. As we transition into the next phase
    of our journey, the focus now shifts toward deploying these DL models in production
    environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们深入探讨了数据准备、**深度学习**（**DL**）模型开发的复杂性，以及如何从我们的DL模型中获得有价值的见解。通过细致的数据分析、特征工程、模型优化和模型分析，我们已经掌握了确保DL模型表现良好并按预期运行的技术。随着我们进入下一个阶段，焦点将转向在生产环境中部署这些DL模型。
- en: Reaching the stage of deploying a DL model to production is a significant accomplishment,
    considering that most models don’t make it that far. If your project has reached
    this milestone, it signifies that you have successfully satisfied stakeholders,
    presented valuable insights, and performed thorough value and metric analysis.
    Congratulations, as you are now one step closer to joining the small percentage
    of successful projects amidst countless attempts. It’s worth noting that, according
    to a 2022 Gartner survey highlighted by VentureBeat, which was executed online
    from October to December 2021 with 699 respondents from organizations in the US,
    Germany, and the UK, only around half (54%) of AI models make it into production.
    Furthermore, the 2023 State of AI Infrastructure Survey, published by Run AI,
    an AI resource management solutions provider, reported that in over 88% of the
    companies surveyed, less than half of the AI models reached the production stage.
    This involved 450 industry professionals across the US and Western Europe. These
    two surveys emphasize the challenges faced in this process and the significance
    of reaching this stage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境的阶段是一个重要的成就，因为大多数模型无法走到这一步。如果你的项目已经达到了这一里程碑，这意味着你已经成功满足了利益相关者的需求，提供了有价值的见解，并进行了全面的价值和指标分析。恭喜你，你离加入成功项目的少数群体又近了一步。值得注意的是，根据2022年Gartner的一项调查，VentureBeat进行了报道，这项调查在线进行，时间为2021年10月至12月，收集了来自美国、德国和英国的699名受访者的数据，约有一半（54%）的AI模型最终进入生产阶段。此外，由AI资源管理解决方案提供商Run
    AI发布的2023年《AI基础设施状态调查》报告显示，在超过88%的受访公司中，进入生产阶段的AI模型不到一半。该调查涉及450名来自美国和西欧的行业专业人士。这两项调查突显了这个过程中的挑战，以及达到这一阶段的重要性。
- en: The ultimate goal here is to make these DL models accessible to end users, in
    an intuitive way, enabling them to harness the full potential of DL in real-world
    applications. In this chapter, we will explore the various strategies, tools,
    and best practices to seamlessly integrate our DL models into production systems,
    ensuring scalability, reliability, and ease of use for a diverse range of users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最终目标是让这些DL模型以直观的方式对最终用户可用，使他们能够在实际应用中充分发挥DL的潜力。在本章中，我们将探讨将DL模型无缝集成到生产系统中的各种策略、工具和最佳实践，确保可扩展性、可靠性和用户友好性，适应各种不同的用户需求。
- en: 'Specifically, we will be going through the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将讨论以下主题：
- en: Exploring the crucial components for DL model deployment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习模型部署的关键组件
- en: Identifying key DL model deployment requirements
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定深度学习模型部署的关键需求
- en: Choosing the right DL model deployment options
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的深度学习模型部署选项
- en: Exploring deployment decisions based on practical use cases
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨基于实际使用案例的部署决策
- en: Discovering general recommendations for DL deployment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习（DL）模型部署的通用建议
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ONNX、TensorRT和NVIDIA Triton服务器部署语言模型
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will have a practical topic in the last section of this chapter. This tutorial
    requires you to have a Linux machine with an NVIDIA GPU device ideally in Ubuntu
    with Python 3.10 and the `nvidia-docker` tool installed. Additionally, we will
    require the following Python libraries to be installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一部分，我们将进行一个实践性主题。本教程要求你拥有一台配备NVIDIA GPU设备的Linux机器，最好是在Ubuntu上安装Python
    3.10和`nvidia-docker`工具。此外，我们还需要安装以下Python库：
- en: '`numpy`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`transformers==4.21.3`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.21.3`'
- en: '`nvidia-tensorrt==8.4.1.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-tensorrt==8.4.1.5`'
- en: '`torch==1.12.0`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch==1.12.0`'
- en: '`transformers-deploy`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-deploy`'
- en: '`tritonclient`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tritonclient`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在 GitHub 上获取：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15)。
- en: Exploring the crucial components for DL model deployment
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度学习模型部署的关键组件
- en: So, what does it take to deploy a DL model? It starts with having a holistic
    view of each required component and defining clear requirements that guide decision-making
    for every aspect. This approach ensures alignment with the business goals and
    requirements, maximizing the chances of a successful deployment. With careful
    planning, diligent execution, and a focus on meeting the needs of the business,
    you can increase the likelihood of successfully deploying your DL model and unlocking
    its value for users. We will start by discovering components that are required
    to deploy a DL model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，部署深度学习模型需要什么？它从全面了解每个必需的组件开始，并定义清晰的需求，以指导每个方面的决策。这种方法确保与业务目标和需求保持一致，最大化成功部署的机会。通过精心规划、严谨执行和专注于满足业务需求，你可以增加成功部署深度学习模型并为用户释放其价值的可能性。我们将从探索部署深度学习模型所需的组件开始。
- en: 'Deploying a DL model to production involves more than just the trained model
    itself. It requires seamless collaboration among various components, working together
    to enable users to effectively extract value from the model’s predictions. These
    components are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境不仅仅涉及训练好的模型本身。它需要各个组件之间的无缝协作，共同工作以帮助用户有效地从模型的预测中提取价值。以下是这些组件：
- en: '**Architectural choices**: The overall design and structure of the deployment
    system. Should the model be implemented as a separate service, microservice, or
    directly part of an existing service? Should the model be hosted on the cloud
    or on-premises? Another aspect to consider is whether to use container orchestration
    platforms, such as Kubernetes, Docker Swarm, or Apache Mesos, to manage and scale
    deployments of deep learning models in containerized applications.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构选择**：部署系统的整体设计和结构。模型应该作为一个独立的服务、微服务，还是直接作为现有服务的一部分来实现？模型应该托管在云端还是本地？另一个需要考虑的方面是，是否使用容器编排平台，如
    Kubernetes、Docker Swarm 或 Apache Mesos，来管理和扩展深度学习模型在容器化应用中的部署。'
- en: These platforms provide flexible deployment across multiple machines, cloud
    providers, or on-premises infrastructure, and can be used in conjunction with
    other tools and services for efficient management of containerized applications
    and microservices.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些平台提供灵活的部署方式，可以跨多台机器、云服务提供商或本地基础设施进行部署，并可与其他工具和服务结合使用，以高效管理容器化应用程序和微服务。
- en: '**Hardware/physical infrastructure choices**: This involves the decision of
    which physical computing device you want to use and the choice of each of the
    components that make up the computing device. Should the model be run on a CPU,
    GPU, TPU, or an **Artificial Neural Engine** (**ANE**) in an iPhone?'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件/物理基础设施选择**：这涉及到选择你希望使用的物理计算设备，以及组成该计算设备的各个组件的选择。模型应该在 CPU、GPU、TPU，还是 iPhone
    中的 **人工神经引擎**（**ANE**）上运行？'
- en: '**Model packaging methods and frameworks**: This is a component that involves
    serializing the model’s architecture, weights, and configuration into a file or
    container format, allowing for easy distribution, deployment, and usage across
    various environments. Usually, the DL framework will provide out-of-the-box support
    for model packaging. Do you have architectural choices and hardware infrastructure
    choices or preferences that require the model to be packaged in a specific way?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型打包方法和框架**：这是一个涉及将模型的架构、权重和配置序列化为文件或容器格式的组件，从而实现模型在各种环境中的分发、部署和使用。通常，深度学习框架会提供开箱即用的模型打包支持。你是否有架构选择和硬件基础设施偏好，要求模型以特定方式进行打包？'
- en: '**Model safety, trust, and reliability component**: This encompasses the measures
    taken to ensure that the deployed model is secure, trustworthy, and reliable in
    making accurate predictions. It involves implementing guardrails to prevent misuse
    or unintended behavior, ensuring model consistency, monitoring model performance,
    and providing prediction explanations to help users understand and trust the model’s
    output. Ensuring data privacy and compliance with relevant regulations is also
    a critical aspect of this component. Are there any specific safety, trust, or
    reliability requirements that must be met for the deployment of your DL model?'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型安全性、可信度和可靠性组件**：这包括采取措施确保部署的模型在做出准确预测时是安全、可信和可靠的。它涉及到实施防护措施以防止滥用或意外行为，确保模型的一致性，监控模型性能，并提供预测解释，以帮助用户理解和信任模型的输出。确保数据隐私并遵守相关法规也是该组件的一个关键方面。是否有任何特定的安全性、可信度或可靠性要求，必须在你的深度学习模型部署中得到满足？'
- en: '**Security and authentication methods**: These involve protecting your DL model
    and its associated infrastructure, as well as controlling access to the model
    by implementing suitable authentication, authorization, and encryption mechanisms.
    This ensures that only authorized users can access and interact with the model,
    preventing unauthorized access, data breaches, and potential misuse of the model.
    What are the necessary security and authentication requirements for your DL model
    deployment, and how will they be integrated into your system?'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性和身份验证方法**：这些涉及到保护你的深度学习模型及其相关基础设施，以及通过实施合适的身份验证、授权和加密机制来控制对模型的访问。这确保只有授权用户才能访问和与模型进行交互，防止未授权访问、数据泄露以及潜在的模型滥用。你的深度学习模型部署需要哪些安全性和身份验证要求？这些要求将如何集成到你的系统中？'
- en: '**Communication protocols**: These define the rules and formats for exchanging
    data between the deployed model and other components or users in the system. It
    involves selecting appropriate protocols based on the requirements, such as latency,
    reliability, and data formats. Examples of communication protocols are HTTP, RESTful
    APIs, gRPC, server-sent events, and WebSockets. What communication protocols best
    suit your DL model deployment, and how will they be implemented to enable seamless
    interaction between the model and its users?'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信协议**：这些定义了在部署的模型和系统中的其他组件或用户之间交换数据的规则和格式。它涉及根据要求选择适当的协议，例如延迟、可靠性和数据格式。通信协议的示例包括HTTP、RESTful
    API、gRPC、服务器推送事件和WebSocket。哪些通信协议最适合你的深度学习模型部署？它们将如何实现，以便模型和用户之间能够无缝互动？'
- en: '**User interfaces**: These are the visual components and interaction methods
    that allow users or downstream systems to access, interact with, and obtain predictions
    from the deployed DL model. User interfaces can be web-based, mobile, desktop
    applications, APIs, or even voice-activated systems, depending on the use case
    and target audience. Designing user-friendly and intuitive interfaces is essential
    to ensure that users can easily understand and make the most of the model’s predictions.
    What type of user interface is best suited for your DL model deployment, and how
    will it be designed to provide an optimal user experience while effectively delivering
    the model’s capabilities? Here are some examples of user interface design challenges
    specific to DL models:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：这些是视觉组件和交互方式，允许用户或下游系统访问、与部署的深度学习模型进行交互并获取预测。用户界面可以是基于网页、移动端、桌面应用程序、API，甚至是语音激活系统，具体取决于使用场景和目标用户群体。设计用户友好且直观的界面至关重要，以确保用户能够轻松理解并充分利用模型的预测功能。什么样的用户界面最适合你的深度学习模型部署？如何设计界面，以提供最佳的用户体验，同时有效地传递模型的能力？以下是一些特定于深度学习模型的用户界面设计挑战示例：'
- en: '**Visualizing complex data**: DL models often work with multi-dimensional data,
    which can be challenging to display in a user-friendly manner. Designers may need
    to devise innovative ways to visualize and represent such data, making it accessible
    and understandable for users.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化复杂数据**：深度学习模型通常处理多维数据，这可能会让数据以用户友好的方式展示变得具有挑战性。设计人员可能需要设计创新的方式来可视化和呈现这些数据，使用户能够访问并理解这些数据。'
- en: '**Handling real-time data**: In scenarios where DL models process and analyze
    real-time data, the user interface must efficiently manage data streaming and
    updates, ensuring that users receive timely and accurate information without being
    overwhelmed.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理实时数据**：在深度学习模型处理和分析实时数据的场景中，用户界面必须有效地管理数据流和更新，确保用户及时获得准确的信息，同时避免信息过载。'
- en: '**Facilitating model interactions**: Users may need to interact with the DL
    model to adjust parameters, provide feedback, or request additional information.
    Designing intuitive UI elements for these interactions is crucial to ensure users
    can effectively engage with the model.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进模型交互**：用户可能需要与深度学习模型互动，以调整参数、提供反馈或请求额外信息。设计直观的用户界面元素以支持这些交互至关重要，确保用户能够有效地与模型互动。'
- en: '**Interpreting model output**: DL models can produce complex and nuanced output,
    which may be challenging for users to understand and act upon. Designers must
    find ways to present model predictions in a clear and actionable manner while
    also providing contextual information to help users interpret the results.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释模型输出**：深度学习模型可能会产生复杂且微妙的输出，这可能会使用户理解和操作变得困难。设计者必须找到方法，以清晰且可操作的方式呈现模型预测结果，同时提供上下文信息，帮助用户解释结果。'
- en: '**Managing uncertainty**: DL models may produce predictions with varying degrees
    of confidence or uncertainty. Designers should consider how to communicate this
    uncertainty to users, ensuring that they are aware of the limitations and potential
    risks associated with the model’s output.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理不确定性**：深度学习模型可能会产生具有不同置信度或不确定性的预测。设计者应考虑如何将这种不确定性传达给用户，确保他们意识到模型输出的局限性和潜在风险。'
- en: '**Accessibility and inclusivity**: User interfaces for DL models should cater
    to a diverse range of users, including those with different abilities, languages,
    and cultural backgrounds. Designers must ensure that their interfaces are accessible
    and inclusive, taking into account various user needs and preferences.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性与包容性**：深度学习模型的用户界面应当适应各种不同背景的用户，包括有不同能力、语言和文化背景的用户。设计师必须确保他们的界面具有可访问性和包容性，考虑到各种用户需求和偏好。'
- en: '**Monitoring and logging components**: These tools allow you to track the performance,
    usage, and health of your DL model in real time. By collecting and analyzing relevant
    metrics, logs, and alerts, this component helps identify potential issues, optimize
    the model’s performance, and ensure a stable deployment environment. How will
    you implement monitoring and logging to track your DL model’s health and performance,
    and what metrics will be crucial to measure its success?'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控与日志组件**：这些工具允许你实时跟踪深度学习模型的性能、使用情况和健康状况。通过收集和分析相关的指标、日志和警报，该组件有助于发现潜在问题、优化模型性能，并确保稳定的部署环境。你将如何实施监控和日志记录，以追踪深度学习模型的健康状况和性能，并且哪些指标对衡量其成功至关重要？'
- en: '**Continuous integration/continuous deployment** (**CI/CD**): This process
    involves the automated building, testing, and deployment of your DL model whenever
    changes are made to its code, data, or infrastructure. CI/CD streamlines the development
    life cycle, enabling faster iterations and improvements while ensuring that the
    deployed model remains up-to-date and reliable. What CI/CD practices and tools
    will you adopt to maintain a seamless deployment pipeline for your DL model?'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成/持续部署** (**CI/CD**)：该过程涉及每当模型的代码、数据或基础设施发生变化时，自动构建、测试和部署你的深度学习模型。CI/CD简化了开发生命周期，实现了更快速的迭代和改进，同时确保已部署的模型保持最新并且可靠。你将采用哪些CI/CD实践和工具，以维持你的深度学习模型的无缝部署流程？'
- en: With numerous options available for each of these components, it’s essential
    to have a strategy to decide which ones to use. The first logical step in this
    process is to define the specific requirements that will guide decision-making
    for each component. In the next section, we will discuss how to establish these
    requirements, ensuring that your choices align with your business goals.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个组件都有众多选项的情况下，制定决策策略至关重要。这个过程的第一步是定义具体的需求，以指导每个组件的决策。在下一节中，我们将讨论如何建立这些需求，确保你的选择与业务目标相符。
- en: Identifying key DL model deployment requirements
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定深度学习模型部署的关键要求
- en: 'To determine the most suitable deployment strategy from a variety of options,
    it is essential to identify and define seven key requirements. These are latency
    and availability, cost, scalability, model hardware, data privacy, safety, and
    trust and reliability requirements. Let’s dive into each of these requirements
    in detail:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从多种选项中确定最合适的部署策略，必须识别并定义七个关键要求。这些要求包括延迟和可用性、成本、可扩展性、模型硬件、数据隐私、安全性，以及信任和可靠性要求。让我们详细探讨每一个要求：
- en: '**Latency and availability requirements**: These are two closely connected
    components and should be defined together. Availability requirements refer to
    the desired level of uptime and accessibility of the model’s prediction. Latency
    requirements refer to the maximum acceptable delay or response time that the models
    must meet to provide timely predictions or results. A deployment with a low availability
    requirement usually can tolerate high latency predictions, and vice versa. One
    reason is that a low-latency capable infrastructure can’t ensure low latency if
    it is not available when model predictions are requested. However, there are edge
    cases that can require complete availability and low latency only for a short
    period but can be unavailable for the rest of the time, which is considered low
    availability but with low latency requirements. Here are a few best practices
    when determining latency and availability requirements:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟和可用性要求**：这两者是紧密相关的组件，应一同定义。可用性要求指的是模型预测的期望运行时间和可访问性水平。延迟要求指的是模型必须满足的最大可接受延迟或响应时间，以提供及时的预测或结果。如果一个部署具有较低的可用性要求，通常可以容忍较高的延迟预测，反之亦然。原因之一是，如果低延迟基础设施在请求模型预测时不可用，那么它就无法确保低延迟。然而，也有一些特殊情况，只在短时间内需要完全可用且低延迟，而其他时间可以不可用，这被视为低可用性但具有低延迟要求。以下是确定延迟和可用性要求时的一些最佳实践：'
- en: Consider the expectations and needs of the end users or applications utilizing
    the DL model. Consult with stakeholders to understand the desired response times
    and availability levels they expect.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用 DL 模型的最终用户或应用程序的期望和需求。与相关方沟通，了解他们期望的响应时间和可用性水平。
- en: Assess the impact of latency and availability on the overall system or business
    process. Identify critical points where delays can significantly affect user experience
    or business operations. Is waiting for a minimum of 1 hour for predictions going
    to provide the value the business wanted?
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估延迟和可用性对整体系统或业务流程的影响。识别延迟可能显著影响用户体验或业务运营的关键点。等待至少 1 小时才能获得预测，是否能够提供业务所需的价值？
- en: Identify time windows or periods where availability is particularly crucial.
    Determine if the DL model needs to be available 24/7 or if there are specific
    hours or events when high availability is essential.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定可用性特别重要的时间窗口或时段。确定 DL 模型是否需要全天候 24/7 可用，或是否有特定的时段或事件需要高可用性。
- en: Set both the ideal and maximum latency and availability thresholds. The maximum
    is usually where a significant value can still be obtained, and an ideal condition
    would just slightly increase that value.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置理想和最大延迟与可用性阈值。最大值通常是指仍能获得显著价值的水平，而理想条件则是稍微提高该值的情况。
- en: '**Cost requirements**: Budget constraints are a critical consideration in any
    business, and it is essential to determine the maximum cost you are willing to
    allocate for deploying a machine learning model based on the expected value it
    will bring. To ensure that the expenses do not exceed what the organization is
    willing to invest, it is advisable to conduct a cost-benefit analysis. This analysis
    will involve evaluating the cost implications of various components within the
    deployment infrastructure, including achieving higher levels of latency and availability.
    By carefully balancing the desired requirements against the associated infrastructure
    costs and operational complexities, you can make informed decisions that align
    with the overall financial goals of your organization while still leveraging the
    benefits of machine learning.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本要求**：预算限制是任何企业的关键考虑因素，因此必须确定您愿意为部署机器学习模型分配的最大成本，以便根据模型预期带来的价值进行规划。为了确保费用不会超过组织愿意投入的金额，建议进行成本效益分析。此分析将评估部署基础设施中各个组件的成本影响，包括如何实现更高的延迟和可用性水平。通过仔细平衡所需要求与相关基础设施成本及操作复杂性，您可以做出与组织整体财务目标相一致的明智决策，同时依然能利用机器学习的优势。'
- en: '**Scalability requirements**: Scalability is the ability of a deployment infrastructure
    to handle an increase or decrease in workload demands without compromising performance
    or quality. It is essential to determine the scalability requirements of your
    deep learning model, as this will impact the choice of deployment strategy and
    infrastructure. Do you expect the model usage to grow over time? How fast do you
    expect it to grow? Do you need to scale horizontally (adding more instances of
    the model) or vertically (increasing the resources of existing instances)? Having
    an expectation regarding the utilization growth rate will allow you to choose
    appropriate components and decisions in operationalizing your model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性要求**：可扩展性是指部署基础设施能够在不影响性能或质量的情况下，应对工作负载需求的增加或减少。确定深度学习模型的可扩展性要求至关重要，因为这将影响部署策略和基础设施的选择。您是否预期模型使用量会随着时间增长？您预期它会增长多快？您需要横向扩展（增加更多模型实例）还是纵向扩展（增加现有实例的资源）？对使用增长速率的预期将帮助您在模型落地时做出适当的组件选择和决策。'
- en: '**Model hardware requirements**: The choice of hardware for deploying a DL
    model is crucial as it can significantly impact the performance, latency, and
    cost of the overall deployment. To properly identify hardware requirements, consider
    the following:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型硬件要求**：选择用于部署深度学习模型的硬件至关重要，因为它会显著影响整体部署的性能、延迟和成本。为了正确识别硬件需求，请考虑以下几点：'
- en: '**Compatibility**: Ensure the chosen hardware is compatible with the frameworks
    and libraries used to develop the DL model. This includes checking if the hardware
    can support specific functions, such as GPU acceleration, that may be essential
    for model performance.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性**：确保所选硬件与用于开发深度学习模型的框架和库兼容。这包括检查硬件是否能支持特定功能，例如可能对模型性能至关重要的GPU加速。'
- en: '**Processing power**: Evaluate the processing power required to efficiently
    run the model, including the number of cores, memory, and storage. Consider how
    the model’s complexity and size may impact hardware requirements.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理能力**：评估高效运行模型所需的处理能力，包括核心数量、内存和存储。考虑模型的复杂性和大小可能对硬件要求的影响。'
- en: '**Power consumption and heat dissipation**: The power consumption and heat
    dissipation of the chosen hardware can affect the overall operational cost and
    the environmental footprint of the deployment. Choose hardware that balances performance
    with energy efficiency.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功耗和散热**：所选硬件的功耗和散热情况会影响整体运营成本和部署的环境足迹。选择能在性能与能源效率之间找到平衡的硬件。'
- en: '**Future-proofing**: Consider the expected lifespan of the hardware and its
    ability to accommodate future updates or improvements to the model. Opt for hardware
    that can easily be upgraded or replaced if necessary.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未来适应性**：考虑硬件的预期使用寿命及其是否能够适应模型未来的更新或改进。选择可以轻松升级或更换的硬件，以应对可能的需求。'
- en: '**Integration**: Ensure the hardware can be seamlessly integrated with the
    rest of the deployment infrastructure and any other relevant systems or components.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：确保硬件能够无缝集成到其他部署基础设施及相关系统或组件中。'
- en: By thoroughly assessing model hardware requirements, you can make informed decisions
    that ensure optimal performance while minimizing costs and potential bottlenecks
    in your DL model deployment.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过彻底评估模型的硬件要求，您可以做出明智的决策，确保在优化性能的同时，最小化成本和潜在的瓶颈。
- en: '**Data privacy requirements**: Ensuring the privacy and security of data used
    in the DL model and predictions by it is crucial as it can impact the trust and
    compliance of the deployment. To identify and address data privacy requirements,
    consider the following:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私要求**：确保深度学习模型及其预测所使用数据的隐私和安全至关重要，因为这会影响部署的信任和合规性。为识别并解决数据隐私要求，请考虑以下方面：'
- en: '**Regulatory compliance**: Understand the data protection regulations and industry
    standards applicable to your organization, such as GDPR, HIPAA, or CCPA. Ensure
    that the deployment strategy and infrastructure comply with these regulations.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性要求**：了解适用于您组织的数据保护法规和行业标准，如GDPR、HIPAA或CCPA。确保部署策略和基础设施符合这些法规。'
- en: '**Data storage and processing locations**: Assess where the data will be stored
    and processed during the deployment. Determine if any data residency requirements
    or restrictions exist, such as the need to store data in a specific geographic
    region.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储和处理地点**：评估数据在部署过程中存储和处理的位置。确定是否存在数据驻留要求或限制，如需要将数据存储在特定的地理区域。'
- en: '**Data access controls**: All DL applications should have the requirement to
    implement appropriate access controls to ensure that only authorized users or
    systems can access the data. This includes implementing authentication, authorization,
    and encryption mechanisms.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问控制**：所有深度学习应用都应要求实施适当的访问控制，以确保只有授权的用户或系统可以访问数据。这包括实现身份验证、授权和加密机制。'
- en: '**Data retention and deletion policies**: Check if there are legal and regulatory
    requirements for data retention and deletion. Ensure that the deployment infrastructure
    supports these policies and allows for secure data disposal if necessary.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据保留和删除政策**：检查是否有数据保留和删除的法律和合规要求。确保部署基础设施支持这些政策，并在必要时允许安全删除数据。'
- en: '**Data monitoring and auditing**: Check if there is a need to implement monitoring
    and auditing mechanisms to track data usage and access throughout the deployment.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据监控与审计**：检查是否需要实施监控和审计机制，以跟踪部署过程中数据的使用和访问。'
- en: '**Data breach response plan**: Such a plan should include roles and responsibilities,
    communication channels, and remediation actions. Check if there is a need to develop
    a data breach response plan that outlines the steps to be taken when there’s a
    data breach or a security incident.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据泄露响应计划**：此类计划应包括角色与职责、沟通渠道和修复措施。检查是否需要制定数据泄露响应计划，列出发生数据泄露或安全事件时的应对步骤。'
- en: '**Safety requirements**: Reflect on the potential legal and ethical boundaries
    that the model must comply with in the specific region you want to deploy your
    model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性要求**：反思模型在特定地区部署时必须遵守的法律和伦理边界。'
- en: '**Trust and reliability requirements**: Trust and reliability for machine learning
    models refer to the confidence in a model’s consistent performance, accuracy,
    and adherence to ethical and regulatory standards during its deployment and operation.
    Consider these questions when determining requirements:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任与可靠性要求**：机器学习模型的信任与可靠性指的是对模型在部署和运行过程中保持一致的性能、准确性，以及遵守伦理和合规标准的信心。在确定要求时，请考虑以下问题：'
- en: How frequently will the model be updated or modified?
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型多久更新或修改一次？
- en: Is tracking multiple model versions necessary?
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有必要追踪多个模型版本？
- en: Will the model face concept or data drift in its operating environment?
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在其运行环境中是否会面临概念漂移或数据漂移？
- en: How important is efficient error detection and resolution?
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的错误检测和解决有多重要？
- en: How often will the model receive updates or new features?
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型多久会收到更新或新增功能？
- en: Is adapting to user feedback or changing requirements essential?
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应用户反馈或变化的需求是否至关重要？
- en: Are there opportunities to leverage advances in DL to improve the model?
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有机会利用深度学习的进展来改进模型？
- en: Is maintaining a stable and secure production environment a priority?
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护稳定和安全的生产环境是否是优先事项？
- en: How critical is the model’s performance to its users or business functions?
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能对其用户或业务功能有多关键？
- en: Are there strict SLAs or regulatory requirements related to performance?
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有与性能相关的严格服务水平协议（SLA）或监管要求？
- en: Is consistent performance across different environments and configurations important?
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同环境和配置中，性能一致性是否重要？
- en: Do the model’s predictions have significant consequences, making consistency
    essential for user trust and success?
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的预测是否具有重大影响，从而使得一致性对用户信任和成功至关重要？
- en: Some of these requirements are best determined early on in the planning stage.
    For instance, defining latency requirements from the outset allows you to select
    an appropriate model that ensures runtime duration falls within the specified
    latency constraints. Having explored the types of requirements that need to be
    defined and the approximate methods for defining them, we are now prepared to
    discuss choosing the right deployment options.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些要求最好在规划阶段早期确定。例如，从一开始就定义延迟要求，可以帮助您选择一个合适的模型，以确保运行时持续时间符合指定的延迟限制。在探讨了需要定义的要求类型和定义这些要求的大致方法后，我们现在可以讨论如何选择正确的部署选项。
- en: Choosing the right DL model deployment options
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的深度学习（DL）模型部署选项
- en: Selecting the right deployment options for your DL model is a crucial step in
    ensuring optimal performance, scalability, and cost-effectiveness. To assist you
    in making an informed decision, we will explore recommended options based on different
    requirements. These recommendations encompass various aspects, such as hardware
    and physical infrastructure, monitoring and logging components, and deployment
    strategies. By carefully evaluating your model’s characteristics, resource constraints,
    and desired outcomes, you should be able to identify the most suitable deployment
    solution that aligns with your objectives while maximizing efficiency and return
    on investment through this guide. The tangible deployment components we will explore
    here are architectural decisions, computing hardware, model packaging and frameworks,
    communication protocols, and user interfaces. Let’s dive into each component one
    by one, starting with architectural choices.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习（DL）模型选择正确的部署选项是确保最佳性能、可扩展性和成本效益的关键步骤。为了帮助您做出明智的决策，我们将根据不同的需求探讨推荐的选项。这些建议涵盖多个方面，例如硬件和物理基础设施、监控和日志记录组件，以及部署策略。通过仔细评估模型的特性、资源限制和预期结果，您应该能够识别最合适的部署解决方案，以最大化效率并通过本指南实现投资回报率。我们将在这里探讨的实际部署组件包括架构决策、计算硬件、模型打包与框架、通信协议和用户接口。让我们一一深入探讨每个组件，从架构选择开始。
- en: Architectural choices
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构选择
- en: 'Architectural choices for a machine learning service involve designing the
    infrastructure, data pipelines, and deployment methods for efficient and reliable
    operations. We will start with service placement considerations:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习服务的架构选择涉及设计基础设施、数据管道和部署方法，以确保高效和可靠的操作。我们将从服务部署的考虑因素开始：
- en: '**Microservice**: **Deploy the Deep Learning** (**DL**) model as a small, loosely
    coupled, and independently deployable service with its own APIs. A microservice
    is a software architecture design pattern where an application is structured as
    a collection of small, loosely coupled, and independently deployable services.
    Each microservice is responsible for a specific functionality or domain within
    the application and communicates with other microservices through well-defined
    **Application Programming Interfaces** (**APIs**). So, when deploying as a microservice,
    a prerequisite is that other components are also implemented as a microservice.
    Its advantages are as follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微服务**：**将深度学习（DL）模型**部署为一个小型、松散耦合、可独立部署的服务，并拥有自己的API。微服务是一种软件架构设计模式，将应用程序构建为一组小型、松散耦合且可独立部署的服务。每个微服务负责应用程序中的特定功能或领域，并通过明确定义的**应用程序接口（API）**与其他微服务进行通信。因此，当以微服务形式部署时，前提是其他组件也必须以微服务的方式实现。其优势如下：'
- en: Better scalability
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的可扩展性
- en: Easier updates and maintenance
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更便捷的更新和维护
- en: Higher resilience
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的弹性
- en: Flexibility in technology choices
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术选择的灵活性
- en: 'Choose this microservice in the following circumstances:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择该微服务：
- en: When model usage is expected to grow
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预计模型使用量会增长时
- en: When frequent updates are needed
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要频繁更新时
- en: When integration with various external systems is required
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要与各种外部系统集成时
- en: When high resilience is crucial
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当高可用性至关重要时
- en: '**Standalone service:** Deploy the DL model as a separate, independent service,
    that is not a microservice. Consider a movie recommendation application - a microservice
    approach would be to create a **Review Analysis Service** microservice that processes
    movie reviews using a DL model. It has its own API, data storage, and deployment
    pipeline, operating independently from other services in the application. For
    a separate service approach in the same application, a **Movie Recommendation
    Service** combines user preference management, movie review analysis (using the
    DL model), and recommendation generation. It''s more monolithic, combining related
    functionalities, with its own API but no separate microservice for review analysis.
    Its advantages are as follows:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立服务**：将深度学习模型作为一个独立的服务部署，而不是微服务。以电影推荐应用为例——微服务方法是创建一个**评论分析服务**微服务，使用深度学习模型处理电影评论。它有自己的
    API、数据存储和部署管道，并且独立于应用中的其他服务运行。在同一个应用中采用独立服务方法，**电影推荐服务**结合了用户偏好管理、电影评论分析（使用深度学习模型）和推荐生成。它更具单体结构，整合了相关功能，拥有自己的
    API，但没有为评论分析单独设置微服务。其优点如下：'
- en: Easier management and administration
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易的管理和维护
- en: Better suited for complex applications
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更适合复杂的应用
- en: Consolidated resources and data access
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中的资源和数据访问
- en: Simplified communication between components
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件之间的简化通信
- en: More predictable performance
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更可预测的性能
- en: 'Choose this standalone service in the following circumstances:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择此独立服务：
- en: When the application has a limited number of services
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用中服务数量有限时
- en: When the model is complex and requires a more monolithic approach
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型复杂并且需要更单体化的方法时
- en: When the scope of the model does not change frequently
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型的范围变化不频繁时
- en: When a balance between resilience and complexity is preferred
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当优先考虑弹性与复杂性之间的平衡时
- en: '**Part of the existing service**: Integrate the DL model into an existing service
    of an application or system. The advantages are as follows:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有服务的一部分**：将深度学习模型集成到应用或系统的现有服务中。其优点如下：'
- en: Less complexity
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的复杂性
- en: Improved performance
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的性能
- en: Easier data synchronization
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易的数据同步
- en: Potential cost savings
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在的成本节约
- en: 'Choose to integrate with an existing service in the following circumstances:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择与现有服务集成：
- en: When model usage growth is limited
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型使用量增长有限时
- en: When infrequent updates or modifications are needed
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当不需要频繁更新或修改时
- en: When you have limited integration with external systems
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与外部系统的集成有限时
- en: When high resilience is not crucial
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当高弹性不是关键要求时
- en: 'Decide between microservice or integrating with an existing service by considering
    scalability, update frequency, integration requirements, and resilience. Align
    these factors with your specific requirements to make the best decision for your
    DL model deployment. Next, we will go through recommendations for choosing the
    physical deployment environment:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务和与现有服务集成之间做出决定时，应考虑可扩展性、更新频率、集成需求和弹性等因素。根据这些因素与您的具体需求对齐，以便做出最佳的深度学习模型部署决策。接下来，我们将讨论选择物理部署环境的建议：
- en: '**Cloud**: Cloud deployments are suitable when you require high availability
    and can tolerate moderate latency. They minimize upfront costs and offer flexible
    pay-as-you-go pricing models. Cloud-based infrastructure provides virtually unlimited
    resources, allows for rapid auto-scaling, and typically offers high uptime guarantees
    and managed services. However, you need to carefully evaluate the cloud providers’
    security offerings and ensure compatibility with your DL framework and libraries.
    A few companies that offer GPU are AWS, GCP, Microsoft Azure, and IBM Cloud.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云**：当您需要高可用性并且能容忍适度的延迟时，云部署是合适的。它们减少了前期成本，并提供灵活的按需付费定价模型。基于云的基础设施提供几乎无限的资源，允许快速自动扩展，并且通常提供高可用性的保证和托管服务。然而，您需要仔细评估云服务提供商的安全性，并确保其与您的深度学习框架和库兼容。提供
    GPU 服务的公司包括 AWS、GCP、Microsoft Azure 和 IBM Cloud。'
- en: '**Server on-premises**: Server on-premises deployments give you more control
    over your hardware and network resources, making them ideal for low latency and
    high availability within a specific geographical region. They require an upfront
    investment in terms of hardware and maintenance but can provide long-term cost
    savings, especially if you have high and consistent resource demands. On-premises
    deployments also offer more control over security measures and data privacy but
    require more effort in maintaining and updating security measures. Ensure compatibility
    with your DL framework and libraries.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地服务器**：本地服务器部署让你对硬件和网络资源拥有更多的控制权，非常适合在特定地理区域内实现低延迟和高可用性。这类部署需要在硬件和维护方面的前期投资，但如果你有高且稳定的资源需求，长期来看可以节省成本。与云端部署相比，本地部署在安全措施和数据隐私方面提供更多的控制权，但也需要更多的精力来维护和更新安全措施。确保与深度学习框架及库的兼容性。'
- en: '**Edge on-premises**: Also known as edge computing, this approach processes
    data close to the source, offering extremely low latency and improved security
    and data privacy. Edge deployments are suitable when data processing and storage
    need to happen close to the source, and they can reduce data transfer costs. However,
    managing security across multiple edge devices and ensuring compatibility with
    your DL framework and libraries can be challenging. Edge deployments offer scalability
    in terms of distributing processing across multiple edge devices but may require
    more management and maintenance efforts.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘本地部署**：也叫做边缘计算，这种方法将数据处理靠近数据源，提供极低的延迟、提升的安全性和数据隐私保护。边缘部署适用于需要将数据处理和存储靠近数据源的场景，并且能够减少数据传输成本。然而，跨多个边缘设备管理安全性，并确保与深度学习框架及库的兼容性，可能是一个挑战。边缘部署在分布式处理方面具有可扩展性，但可能需要更多的管理和维护工作。'
- en: 'Next, we will dive into container orchestration platforms, which have a significant
    impact on how applications and services are designed, deployed, and managed within
    a system. A container is a lightweight, standalone, and executable software package
    that includes everything needed to run a piece of software, including the code,
    runtime, system tools, libraries, and settings. Containers are isolated from each
    other and from the host system, allowing them to run consistently across different
    computing environments. There are two main types of container technologies: Docker
    containers and Linux containers (LXC).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨容器编排平台，它们对应用程序和服务在系统中的设计、部署和管理方式产生了深远影响。容器是一个轻量级、独立的可执行软件包，包含了运行软件所需的所有内容，包括代码、运行时、系统工具、库和设置。容器彼此隔离，也与主机系统隔离，使它们能够在不同的计算环境中一致地运行。容器技术主要有两种类型：Docker
    容器和 Linux 容器（LXC）。
- en: 'Container orchestration platforms help manage and scale deployments of deep
    learning models in containerized applications, utilizing technologies such as
    Docker containers or LXC. These platforms provide flexible deployment across multiple
    machines, cloud providers, or on-premises infrastructure. They can be used in
    conjunction with other tools and services, enabling efficient management of containerized
    applications and microservices. Some popular container orchestration platforms
    to choose among are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排平台帮助管理和扩展容器化应用程序中深度学习模型的部署，利用 Docker 容器或 LXC 等技术。这些平台提供跨多台机器、云服务商或本地基础设施的灵活部署。它们可以与其他工具和服务结合使用，从而实现对容器化应用和微服务的高效管理。以下是一些流行的容器编排平台：
- en: '**Kubernetes (open source):** Kubernetes is an open source container orchestration
    platform that automates the deployment, scaling, and management of containerized
    applications, including deep learning models. It works with various container
    technologies, including Docker and LXC.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes（开源）**：Kubernetes 是一个开源的容器编排平台，能够自动化部署、扩展和管理容器化应用程序，包括深度学习模型。它与多种容器技术兼容，包括
    Docker 和 LXC。'
- en: '**Docker Swarm (open source):** Docker Swarm is a native clustering and scheduling
    tool for Docker containers. It is tightly integrated with the Docker ecosystem,
    providing a simple way to deploy and manage containerized applications. While
    not as feature-rich as Kubernetes, Docker Swarm is known for its ease of use and
    faster setup.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Swarm（开源）**：Docker Swarm 是 Docker 容器的原生集群和调度工具。它与 Docker 生态系统紧密集成，提供了一种简单的方式来部署和管理容器化应用程序。虽然功能上不如
    Kubernetes 丰富，Docker Swarm 以其易用性和更快速的设置而闻名。'
- en: '**Apache Mesos (open source):** Apache Mesos is a distributed systems kernel
    that abstracts CPU, memory, and storage resources away from machines, enabling
    fault-tolerant and elastic distributed systems. It can be used in conjunction
    with other frameworks such as Marathon or DC/OS to provide container orchestration
    capabilities for deploying and managing deep learning models.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos（开源）**：Apache Mesos是一个分布式系统内核，它将CPU、内存和存储资源从机器中抽象出来，使得系统具有容错性和弹性。它可以与其他框架（如Marathon或DC/OS）结合使用，提供容器编排功能，用于部署和管理深度学习模型。'
- en: '**Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Service
    (ECS) (paid-for services):** These are managed container orchestration services
    provided by AWS. EKS is a managed Kubernetes service, while ECS is a proprietary
    container orchestration platform from AWS. Both services simplify the deployment,
    scaling, and management of containerized applications on AWS infrastructure.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic Kubernetes Service (EKS) 和 Amazon Elastic Container Service
    (ECS)（收费服务）**：这些是AWS提供的托管容器编排服务。EKS是一个托管的Kubernetes服务，而ECS是AWS自有的容器编排平台。这两项服务简化了在AWS基础设施上部署、扩展和管理容器化应用程序的过程。'
- en: Choose a container orchestration platform that best suits your deep learning
    deployment requirements, such as flexibility, scalability, compatibility with
    your preferred container technology, cloud provider, and integration with other
    tools and services.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最适合你深度学习部署需求的容器编排平台，例如灵活性、可扩展性、与你偏好的容器技术的兼容性、云提供商以及与其他工具和服务的集成。
- en: 'Next, we will dive into architectural trade-offs between real-time and batch
    predictions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨实时预测和批量预测之间的架构权衡：
- en: '**Real-time predictions**: It’s recommended to have the model always loaded
    in memory to reduce latency and respond quickly to requests. This setup is suitable
    for applications where immediate response is critical, such as autonomous vehicles,
    live chatbots, or fraud detection systems. Here are some recommendations when
    using this option:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时预测**：建议将模型始终加载到内存中，以减少延迟并快速响应请求。此设置适用于即时响应至关重要的应用场景，如自动驾驶汽车、实时聊天机器人或欺诈检测系统。使用此选项时，以下是一些建议：'
- en: Use a dedicated server or cloud instance with enough memory and processing power
    to handle the model and concurrent requests
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一台专用服务器或云实例，确保有足够的内存和处理能力来处理模型和并发请求
- en: Optimize the model for inference by using techniques such as quantization, pruning,
    or model distillation
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量化、剪枝或模型蒸馏等技术优化模型推理
- en: Implement a load balancer if necessary to distribute incoming requests across
    multiple instances of the model
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，实施负载均衡器以将传入请求分配到多个模型实例
- en: Monitor resource usage and performance to ensure the system meets real-time
    requirements and scales as needed
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控资源使用情况和性能，确保系统满足实时要求，并根据需要进行扩展
- en: Have a queue system to ensure workers are not overloaded or implement autoscaling
    to handle overload cases
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用队列系统确保工作人员不会过载，或者实施自动扩展来处理超载情况
- en: '**On-demand batch predictions**: Batch predictions are suitable for scenarios
    where real-time responses are not crucial, and predictions can be processed in
    groups. This setup requires extra time to spin up worker infrastructure, initialize
    the model, and load trained model weights. Here are some recommendations when
    using this option:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按需批量预测**：批量预测适用于实时响应不重要的场景，且预测可以按批次处理。此设置需要额外的时间来启动工作基础设施、初始化模型并加载训练好的模型权重。使用此选项时，以下是一些建议：'
- en: Use a queue system such as RabbitMQ or Amazon SQS to manage incoming prediction
    requests
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用队列系统，如RabbitMQ或Amazon SQS，来管理传入的预测请求
- en: Set up a batch processing system that initializes the model and loads weights
    when processing starts
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置批处理系统，当处理开始时初始化模型并加载权重
- en: Optimize the batch size to balance processing time and resource usage
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化批处理大小，以平衡处理时间和资源使用
- en: Implement auto-scaling to handle variable workloads and ensure efficient use
    of resources
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施自动扩展来处理变化的工作负载并确保资源的高效使用
- en: Next, we will explore computer hardware choices and recommendations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨计算硬件选择和建议。
- en: Computing hardware choices
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算硬件选择
- en: 'Selecting hardware to carry out model computations is all about trading off
    cost, availability, and runtime. Let’s explore the different options, along with
    recommendations on when to opt for each option:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU**: CPUs are a versatile and cost-effective option for deploying DL models.
    They are compatible with most frameworks and libraries and provide decent performance
    for less complex models. CPUs are a good choice when cost constraints are a priority,
    and you don’t require the high processing power that GPUs or TPUs offer.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU**: GPUs provide faster processing and better parallelization, significantly
    reducing latency and improving performance. They are ideal for complex models
    that demand high processing power. GPUs are an excellent choice when you require
    low latency and high availability, but they come with higher costs compared to
    CPUs.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPU**: TPUs are specialized hardware designed for machine learning tasks,
    offering high performance and efficient processing. They are particularly suitable
    for large models or computationally intensive tasks. TPUs are a great option when
    you need exceptional processing power and low latency but be aware of the potential
    higher costs and that it is only available in GCP and usable only in TensorFlow!'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Neural Engines** (**ANEs**): ANEs are specialized AI accelerators
    found in devices such as iPhones. They provide efficient processing for DL tasks
    on edge devices, offering low latency and energy-efficient performance. ANEs are
    a good choice when your application requires user interface requirements on an
    iPhone, which is an edge device. Note that it is only compatible with the CoreML
    framework and that the ONNX weights format is needed to convert weights easily
    to CoreML.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FPGA**: FPGAs are highly customizable (its hardware circuitry can be programmed!)
    and energy-efficient hardware and are suitable for deploying DL models that require
    low latency and adaptability. The con here is the need to have deep expertise
    in the FPGA programming language and circuit development to successfully allow
    inference with a trained neural network efficiently. This is an out-of-bounds
    device for most teams.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore model packaging and framework choices and recommendations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Model packaging and frameworks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This controls how DL models are executed and where recommendations can depend
    on the compute hardware used, as well as portability and runtime requirements.
    Here are some popular examples, along with recommendations on when to use them:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Original framework packaging**: You can take advantage of specific optimizations
    and features provided by the framework, potentially improving performance. However,
    certain cases may require compatibility with specific hardware options, such as
    using a TPU, which is only supported by the TensorFlow framework, so if you have
    a TPU and you stick with PyTorch, you will not be able to use the TPU.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open Neural Network Exchange** (**ONNX**) **framework**: ONNX provides an
    open standard for representing DL models, allowing you to convert your model to
    different frameworks and run it on various hardware platforms. Using ONNX can
    increase the flexibility and portability of your model, enabling you to choose
    from a wider range of hardware and infrastructure options. Moreover, it allows
    you to leverage optimizations and features provided by different DL frameworks.
    A convenient and general solution to address packaging issues is to convert your
    model into the ONNX format, which can then be easily converted into other formats
    as needed. This approach streamlines the process and ensures smooth integration
    with various hardware and framework options, such as leveraging ANE in an iPhone
    to accelerate your deep learning model within an app.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放神经网络交换** (**ONNX**) **框架**：ONNX 提供了一种开放标准，用于表示深度学习模型，允许你将模型转换为不同的框架，并在各种硬件平台上运行。使用
    ONNX 可以提高模型的灵活性和可移植性，使你能够从更广泛的硬件和基础设施选项中进行选择。此外，它还可以让你利用不同深度学习框架提供的优化和功能。一种方便且通用的解决方案是将模型转换为
    ONNX 格式，之后可以根据需要轻松转换为其他格式。这种方法简化了过程，并确保与各种硬件和框架选项的顺利集成，例如在 iPhone 中利用 ANE 加速应用中的深度学习模型。'
- en: '**ONNX Runtime**: This is an inference accelerator that’s designed to accelerate
    DL model inference in any hardware by leveraging compute and memory optimizations.
    It is faster to run a model in ONNX Runtime than to run it in their native DL
    framework, such as TensorFlow or PyTorch.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ONNX Runtime**：这是一种推理加速器，旨在通过利用计算和内存优化来加速在任何硬件上的深度学习模型推理。在 ONNX Runtime 中运行模型比在本地深度学习框架（如
    TensorFlow 或 PyTorch）中运行更快。'
- en: '**TensorRT**: This is a high-performance DL inference optimizer and runtime/compiler
    library from NVIDIA that’s designed to accelerate DL model inference on NVIDIA
    GPUs. It supports TensorFlow and ONNX and provides easy ways to convert model
    weights so that they’re compatible with its framework, offering fast and efficient
    model deployment. TensorRT allows faster model inference speed in GPUs by collectively
    tuning the model at a lower level, leveraging different GPU internal hardware
    capabilities to maximize the model efficiency during inference. As ONNX weights
    are compatible with TensorRT, a typical path to convert PyTorch model weights
    into a TensorRT-compatible weight format is to convert PyTorch model weights into
    ONNX weights. On an NVIDIA GPU, TensorRT is known to be faster than ONNX Runtime.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT**：这是 NVIDIA 提供的高性能深度学习推理优化器和运行时/编译器库，旨在加速 NVIDIA GPU 上的深度学习模型推理。它支持
    TensorFlow 和 ONNX，并提供便捷的方式将模型权重转换为与其框架兼容的格式，从而实现快速高效的模型部署。TensorRT 通过在较低层次调优模型，利用不同
    GPU 内部硬件能力，提高 GPU 上的模型推理速度，最大化推理时的模型效率。由于 ONNX 权重与 TensorRT 兼容，将 PyTorch 模型权重转换为
    TensorRT 兼容的权重格式的典型路径是先将 PyTorch 模型权重转换为 ONNX 权重。在 NVIDIA GPU 上，TensorRT 的推理速度被认为比
    ONNX Runtime 更快。'
- en: '**Open Visual Inference & Neural Network Optimization** (**OpenVINO**): This
    is a toolkit from Intel that accelerates DL model inference across Intel hardware,
    including CPUs, GPUs, and FPGAs. It supports TensorFlow, ONNX, and other frameworks,
    offering optimized model deployment in diverse environments.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放视觉推理与神经网络优化** (**OpenVINO**)：这是英特尔提供的一款工具包，旨在加速深度学习模型在英特尔硬件（包括 CPU、GPU
    和 FPGA）上的推理。它支持 TensorFlow、ONNX 以及其他框架，提供在多种环境中的优化模型部署。'
- en: Next, we will explore communication protocol choices and recommendations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨通信协议的选择和建议。
- en: Communication protocols to use
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的通信协议
- en: 'The protocol you should use can depend on the runtime requirements, network
    load requirements, user interface chosen, mode of deployment, and compute requirements.
    Here are some examples, along with their recommendations:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用的协议取决于运行时需求、网络负载需求、所选的用户界面、部署模式以及计算需求。以下是一些示例及其推荐：
- en: '**MQTT**: Use MQTT when you need a lightweight, low-latency protocol for devices
    with limited resources, such as IoT devices, and real-time communication and status
    updates are essential for your application. Power consumption and heat dissipation
    are important factors.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MQTT**：当你需要一个轻量级、低延迟的协议，适用于资源有限的设备（如物联网设备），并且实时通信和状态更新对你的应用至关重要时，可以使用 MQTT。功耗和散热是重要的考虑因素。'
- en: '**HTTP or REST API**: Choose this when you require a well-supported and easy-to-implement
    protocol for web services and data exchange, your application follows a request-response
    communication pattern, and finally where compliance with data protection regulations
    and data privacy is crucial.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gRPC**: Opt for gRPC when you need a high-performance, low-latency protocol
    for large-scale distributed systems or microservices, bidirectional streaming,
    and support for multiple programming languages are essential.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-Sent Events** (**SSE**) **or WebSockets**: Use them when real-time
    notifications or live updates are critical for your web application. If you require
    unidirectional communication between server and client, use SSE. If you require
    bidirectional communication between server and client, use WebSockets. A notable
    domain that requires these communication protocols is live collaborative tools
    with machine learning. Here are some examples:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammarly uses Websockets
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT uses SSE
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore user interface choices and recommendations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: User interfaces
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When designing user interfaces for machine learning applications, it is essential
    to consider factors such as user experience, accessibility, responsiveness, and
    adaptability. Here are some recommendations for user interfaces:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**Web applications**:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are suitable for cross-platform access as users can access the application
    through a web browser
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use popular web development frameworks such as React, Angular, or Vue.js to
    build responsive and interactive user interfaces
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A sentiment analysis tool that allows users to input
    text and receive sentiment scores by interacting with a machine learning model
    through a web-based interface'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For web applications, you need to also choose a web framework wisely according
    to the benefits it provides, along with the latency trade-offs. Refer to https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db
    for an estimate of the latency you will get for different web frameworks for a
    single web API query.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Mobile applications**:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for on-the-go access to machine learning features through smartphones
    and tablets
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop native apps for iOS and Android platforms using Swift or Kotlin, or
    use cross-platform frameworks such as React Native or Flutter
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A mobile app that uses a machine learning model for image
    recognition to identify plants or animals by analyzing user-captured photos'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desktop applications**:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitable for users who require a dedicated, platform-specific application with
    offline functionality
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use technologies such as Electron or Qt for cross-platform desktop applications
    or platform-specific languages such as C# for Windows or Swift for macOS
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A video editing software with built-in machine learning-powered
    features such as object tracking, automatic color grading, or scene detection'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice User** **Interfaces** (**VUI**):'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for hands-free interaction with machine learning-powered services through
    voice commands
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于通过语音命令与机器学习驱动的服务进行免提交互
- en: Integrate with popular voice assistant platforms such as Amazon Alexa, Google
    Assistant, or Apple Siri
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成流行的语音助手平台，如亚马逊 Alexa、谷歌助手或苹果 Siri
- en: '**Example use case**: A voice-activated home automation system that uses natural
    language processing to control smart devices based on user commands'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个语音激活的家庭自动化系统，使用自然语言处理根据用户命令控制智能设备'
- en: '**Conversational** **UI (chatbots)**:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话式** **UI（聊天机器人）**：'
- en: Suitable for engaging users more naturally and interactively through text or
    voice conversations
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于通过文本或语音对话更自然、互动地吸引用户
- en: Use chatbot development platforms such as Dialogflow, Rasa, or Microsoft Bot
    Framework
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聊天机器人开发平台，如 Dialogflow、Rasa 或 Microsoft Bot Framework
- en: '**Example use case**: A customer support chatbot that uses machine learning-powered
    natural language understanding to answer user queries and provide assistance'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个客户支持聊天机器人，使用机器学习驱动的自然语言理解来回答用户查询并提供帮助'
- en: '**Augmented reality** (**AR**) **and virtual** **reality** (**VR**):'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强现实** (**AR**) **和虚拟** **现实** (**VR**)：'
- en: Ideal for immersive and interactive experiences that combine the real and digital
    worlds
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于将现实世界和数字世界结合的沉浸式和互动式体验
- en: Use AR/VR development platforms such as Unity or Unreal Engine and integrate
    machine learning models for object recognition, motion tracking, or scene understanding
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AR/VR 开发平台，如 Unity 或 Unreal Engine，并集成机器学习模型进行物体识别、运动跟踪或场景理解
- en: '**Example use case**: A virtual training simulator that uses deep learning
    models to analyze and assess user performance in real time. In this AR/VR application,
    users can practice various skills, such as medical procedures, mechanical repairs,
    or emergency response scenarios. The deep learning model evaluates the user’s
    actions through visual input, provides instant feedback, and offers personalized
    guidance for improvement, enhancing the learning experience and accelerating skill
    development.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个虚拟训练模拟器，使用深度学习模型实时分析和评估用户表现。在这个 AR/VR 应用中，用户可以练习各种技能，如医疗程序、机械修理或紧急响应场景。深度学习模型通过视觉输入评估用户的操作，提供即时反馈，并为改进提供个性化指导，增强学习体验并加速技能发展。'
- en: '**API-based user interface**: An API-based user interface provides a flexible
    and scalable way to integrate your machine learning model with various applications,
    platforms, and services. This approach allows developers to build custom user
    interfaces or incorporate machine learning-powered features into existing applications,
    expanding the reach and impact of your model. This is suitable for enabling other
    applications, systems, or services to access and interact with your machine learning
    model programmatically. Two recommendations for this approach are as follows:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于 API 的用户界面**：基于 API 的用户界面提供了一种灵活且可扩展的方式，将你的机器学习模型与各种应用、平台和服务进行集成。这种方法允许开发者构建自定义用户界面或将机器学习驱动的功能整合到现有应用中，从而扩展模型的覆盖面和影响力。这适用于使其他应用、系统或服务能够以编程方式访问和与机器学习模型进行交互。以下是该方法的两个建议：'
- en: Use REST, gRPC, SSE, Websockets, or MQTT to create well-structured and documented
    APIs that expose the machine learning model’s functionality to external clients
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 REST、gRPC、SSE、Websockets 或 MQTT 创建结构良好且有文档的 API，将机器学习模型的功能暴露给外部客户端
- en: Implement authentication and authorization mechanisms (for example, API keys
    and OAuth) to ensure secure access to the API
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现身份验证和授权机制（例如，API 密钥和 OAuth）以确保对 API 的安全访问
- en: '**Example use case**: A sentiment analysis API that allows developers to integrate
    machine learning-powered sentiment analysis into their applications by sending
    text data and receiving sentiment scores through API calls'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个情感分析 API，允许开发者通过 API 调用发送文本数据并接收情感评分，将机器学习驱动的情感分析集成到他们的应用中'
- en: Choosing the right deployment options for your DL model involves carefully evaluating
    architectural choices, hardware options, communication protocols, and user interfaces
    that best align with your specific requirements and objectives. By considering
    factors such as scalability, update frequency, integration needs, and resilience,
    you can select the most suitable deployment solution that maximizes efficiency
    and ROI.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合的部署选项涉及仔细评估架构选择、硬件选项、通信协议和用户界面，确保它们最能与特定需求和目标对接。通过考虑可扩展性、更新频率、集成需求和韧性等因素，你可以选择最合适的部署解决方案，从而最大化效率和投资回报率。
- en: Next, let’s discuss some practical examples for deciding on components when
    deploying DL models in production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论在部署深度学习模型时决策组件的一些实际示例。
- en: Exploring deployment decisions based on practical use cases
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于实际用例探讨部署决策
- en: 'In this section, we will explore practical deployment decisions for DL models
    in production, focusing on two distinct use cases: a sentiment analysis application
    for an e-commerce company and a face detection and recognition system for security
    cameras. By examining these real-world scenarios, we will gain valuable insights
    into establishing robust deployment strategies tailored to specific needs and
    objectives.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨深度学习（DL）模型在生产环境中的实际部署决策，重点关注两个不同的使用案例：一个用于电子商务公司的情感分析应用和一个用于安全摄像头的面部检测与识别系统。通过分析这些真实世界的场景，我们将获得建立稳健部署策略的宝贵见解，以满足特定需求和目标。
- en: Exploring deployment decisions for a sentiment analysis application
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨情感分析应用的部署决策
- en: 'Suppose you are developing a sentiment analysis application to be used by an
    e-commerce company to analyze customer reviews in real-time. The system needs
    to process a large number of reviews every day, and low latency is essential to
    provide immediate insights for the company. In this case, your choices could be
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在开发一个情感分析应用，供电子商务公司用于实时分析客户评论。该系统需要每天处理大量评论，并且低延迟至关重要，以便为公司提供即时的洞察。在这种情况下，你的选择可能如下：
- en: '**Architectural choice**: As an independent service, as it would allow better
    scalability and easier updates to handle the growing number of requests.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构选择**：作为独立服务部署，因为它可以提供更好的可扩展性，并且在处理不断增长的请求时更易于更新。'
- en: '**Hardware/infrastructure choice**: GPU on a cloud service, as it provides
    better parallelization and processing power for a large number of simultaneous
    requests.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件/基础设施选择**：云服务上的GPU，因为它提供更好的并行处理能力和大规模并发请求的处理能力。'
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型包装与框架**：ONNX和TensorRT，因为它们提供高效的模型部署和推理加速。'
- en: '**Safety, trust, and reliability**: Implement monitoring for data drift and
    model performance, regularly retrain the model on updated data, and ensure compliance
    with data privacy regulations. For example, anonymize user information and avoid
    storing **personally identifiable information** (**PII**) in the analysis as it
    can infringe upon data protection regulations, such as the GDPR in the European
    Union or the CCPA in the United States, depending on the country the application
    is intended to be deployed in.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性、信任与可靠性**：实施数据漂移和模型性能的监控，定期用更新后的数据重新训练模型，并确保符合数据隐私法规。例如，对用户信息进行匿名化处理，并避免在分析中存储**个人身份信息**（**PII**），因为这可能会违反数据保护法规，如欧盟的GDPR或美国的CCPA，具体取决于应用部署的国家。'
- en: '**Communication protocol**: RESTful APIs or gRPC, as they are well-suited for
    web services and can handle a large number of requests with low latency.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信协议**：RESTful API或gRPC，因为它们非常适合用于Web服务，并能以低延迟处理大量请求。'
- en: '**User interface**: A web-based dashboard where the company’s staff can monitor
    the sentiment analysis results in real time.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：基于网页的仪表板，公司的员工可以在其中实时监控情感分析结果。'
- en: Exploring deployment decisions for a face detection and recognition system for
    security cameras
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨安全摄像头的面部检测与识别系统的部署决策
- en: 'Suppose you are building an object detection system for security cameras that
    need to detect intruders in real time. In this case, your choices could be as
    follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在为需要实时检测入侵者的安全摄像头构建物体检测系统。在这种情况下，你的选择可能如下：
- en: '**Architectural choice**: Edge on-premises, as it provides low latency and
    improved security by processing data close to the source. This choice also reduces
    the time needed for data to travel through the network, as no video streaming
    to some cloud server is needed.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware/infrastructure choice**: GPU or TPU on the edge device, depending
    on the compatibility with the DL framework and the model’s complexity.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety, trust, and reliability**: Implement monitoring for model performance
    and ensure compliance with local regulations related to video surveillance such
    as data privacy, retention policies, and consent requirements, to maintain ethical
    and legal standards in video analytics. For example, the facial images shouldn’t
    be stored, only the extracted facial features, as this can infringe personal data
    protection-related regulations, depending on the country it is intended to be
    deployed in.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication protocol**: MQTT or WebSockets, as they provide low-latency
    communication between edge devices and the central monitoring system.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User interface**: A desktop application that displays real-time video feeds
    with object detection overlays for security personnel to monitor.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By considering the specific requirements of each use case, you can make informed
    decisions on the components required for deploying DL models in production. Now,
    let’s move on to some general recommendations for successful DL model deployment.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Discovering general recommendations for DL deployment
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will discover DL deployment recommendations related to three verticals,
    namely model safety, trust, and reliability assurance, model latency optimization,
    and tools that help abstract model deployment-related decisions and ease the model
    deployment process. We will dive into the three verticals one by one.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Model safety, trust, and reliability assurance
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring model safety, trust, and reliability is a crucial aspect of deploying
    DL systems. In this section, we will explore various recommendations and best
    practices to help you establish a robust framework for maintaining the integrity
    of your models. This includes compliance with regulations, implementing guardrails,
    prediction consistency, comprehensive testing, staging and production deployment
    strategies, usability tests, retraining and updating deployed models, human-in-the-loop
    decision-making, and model governance. By adopting these measures, you can effectively
    mitigate risks, enhance performance, and foster user trust in your DL deployment.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Comply with regulations and implement guardrails
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regulatory compliance and guardrails are essential components of responsible
    deep learning deployment, ensuring that your model adheres to relevant laws, industry
    standards, and ethical guidelines. Implementing a robust compliance framework
    not only mitigates legal and reputational risks but also fosters trust among users
    and stakeholders. It’s a very broad topic, so here are a few examples to learn
    from:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '**Content moderation for social media platforms**: Compliance with community
    guidelines and regional laws can be achieved by implementing AI-powered filters
    for detecting and flagging inappropriate content, setting up a human review process
    for ambiguous cases, and providing users with a transparent mechanism to appeal
    decisions.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI-powered recruitment tools**: Compliance with anti-discrimination laws
    can involve steps such as monitoring bias and fairness metric performance and
    ensuring that any automated decisions are transparent and explainable to both
    employers and applicants.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial recognition systems**: Compliance with privacy and ethical guidelines
    can be achieved through steps such as obtaining explicit consent from individuals
    before collecting and processing their biometric data, implementing robust data
    security measures, and ensuring transparency about the system’s capabilities and
    limitations.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DL-based video surveillance systems, such as people detection**: Compliance
    with privacy and ethical guidelines can be achieved through measures such as setting
    up clear signage to inform the public about the presence of surveillance cameras,
    restricting data access to authorized personnel, and adhering to data retention
    and deletion policies as per local regulations.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems (YouTube, Netflix, and Tiktok)**: Ensuring compliance
    with data protection regulations can involve steps such as implementing privacy-preserving
    data processing techniques, providing users with the ability to opt out of personalized
    recommendations, and being transparent about data collection and usage policies.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative AI**: Compliance can be achieved by using content filtering mechanisms
    to prevent harmful content generation, which includes hate speech, explicit material,
    and content that encourages criminal activities, or prevent dangerous recommendations
    about medical issues.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we continue to explore model safety, trust, and reliability assurance, let’s
    examine the vital aspect of ensuring prediction consistency in DL deployment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Ensure prediction consistency
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prediction consistency is all about a model’s ability to generate the same predictions
    when faced with the same input data, no matter the hardware, pre/post serialization
    and loading, infrastructure, or whether it’s a single row or a random batch. Inconsistent
    predictions can lead to mismatched expectations of a model’s accuracy and overall
    performance. To maintain consistency across various factors, it’s essential to
    track and replicate the environmental dependencies involved in training, evaluation,
    and inference. Tools such as Docker can help create isolated environments with
    specific dependencies, ensuring a seamless experience and eliminating potential
    issues. Additionally, consider making automated tests to objectively prevent any
    inconsistency from going through, essentially working as a guardrail.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will discuss the significance of comprehensive testing in
    maintaining a reliable DL deployment.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Other than prediction consistency tests, generally, comprehensive testing will
    ensure that your DL model and system perform as expected at all times and meet
    user requirements. DL systems are essentially software systems and require similar
    things to ensure a successful deployment. The test components are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit, integration, and functional tests**: Unit, integration, and functional
    testing are essential for ensuring the reliability, maintainability, and overall
    quality of software components. Here’s why they are important:'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit testing**: This focuses on individual components or functions, verifying
    their correctness and isolating potential issues early in development. This helps
    catch bugs before they propagate, reduces debugging time, and improves code maintainability.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration testing**: This validates the interactions between different
    components, ensuring they work together as intended. This helps identify interface
    issues, data flow problems, and inconsistencies that can arise when combining
    components, ensuring a smooth integration.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional testing**: This assesses the software’s ability to fulfill its
    intended purpose and meet user requirements. Testing end-to-end functionality
    ensures that the software operates correctly in real-world scenarios and delivers
    a positive user experience.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover and recovery testing**: Verify the model’s ability to recover from
    failures, such as hardware or software crashes, and maintain high availability
    in the face of unexpected disruptions.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load stress testing**: Evaluate the model’s performance under various load
    conditions to identify bottlenecks and ensure it can handle the expected user
    traffic. These tests can also help you catch errors such as GPU memory overflow,
    CPU overload, or insufficient storage.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broad and diverse testing**: The model may not be able to handle unexpected
    input data, edge cases, or system failures gracefully, causing crashes or undesired
    behavior. Thinking up all the possible ways the system will be used can help you
    catch issues with the system.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adopting the staging and production deployment steps**: Embracing a staging
    and production strategy in DL production deployment is highly beneficial for ensuring
    model reliability and performance. This approach involves setting up separate
    environments for testing (staging) and final deployment (production), allowing
    you to validate your model’s behavior and identify potential issues before the
    model goes live. By adopting this strategy, you can minimize the risks associated
    with deploying untested models, streamline the process of identifying and resolving
    issues, and enhance the overall reliability of your DL solutions. Ensure the pipeline
    can continuously be in production for 24 hours without failure.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability tests**: Usability tests focus on ensuring that software applications
    deliver an effective, efficient, and satisfying user experience. Both automated
    and manual tests are useful and complementary to each other:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manual usability testing**: This involves real users interacting with the
    software to identify potential usability issues, understand user behavior, and
    gather qualitative feedback. Manual testing helps uncover problems that may not
    be detectable through automated testing, such as confusing navigation, unclear
    instructions, or subjective preferences. This human-centric approach provides
    valuable insights into how users perceive the software and identifies areas for
    improvement.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated usability testing**: This complements manual testing by using tools
    and scripts to simulate user interactions, validate user interface elements, and
    check for accessibility and responsiveness. Automated testing offers several advantages,
    including increased efficiency, speed, and coverage, as well as the ability to
    consistently test across multiple devices, platforms, and browsers. This helps
    with identifying usability issues that may not be apparent during manual testing,
    ensuring a consistent and high-quality user experience.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s consider the importance of retraining and updating the deployed
    model to ensure its continued effectiveness and relevance.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Retraining and updating the deployed model
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A retraining and updating strategy is crucial for maintaining the effectiveness
    of your DL model as it addresses the potential need for regular updates in response
    to changing data patterns. By periodically retraining your model on fresh, relevant
    data, you can ensure it stays current and continues to deliver accurate predictions.
    This not only helps maintain the model’s performance but also keeps it in tune
    with evolving trends and user requirements. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will delve deeper into the importance of
    retraining and updating, exploring its benefits and best practices to help you
    successfully implement this strategy in your DL deployment practically.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance our DL deployment, we will explore the benefits of adopting
    a human-in-the-loop decision-making flow.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a human-in-the-loop decision-making flow
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating human-in-the-loop scenarios into your DL deployment can greatly
    enhance model performance and reliability, either as a permanent solution or by
    triggering alerts when certain conditions are met. By involving human experts
    in the decision-making process, you can bridge the gap between the model’s predictions
    and real-world complexities, allowing for more accurate and nuanced decisions.
    This collaborative approach enables continuous improvement by leveraging human
    expertise to validate, correct, and fine-tune the model’s output. Additionally,
    human-in-the-loop systems foster trust and accountability as users can be confident
    that complex or high-stakes decisions are not made solely by algorithms but are
    also supported by human judgment and oversight.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will delve into the crucial role of model governance in overseeing
    and managing the overall DL deployment process.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Model governance
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Monitoring and governance play a pivotal role in ensuring the ongoing effectiveness
    and reliability of your deep learning deployment. By tracking various aspects
    of your model, such as data drift and concept drift monitoring, you can identify
    and address issues that may affect its performance over time. Data drift monitoring
    helps detect changes in the underlying data distribution, while concept drift
    monitoring focuses on shifts in the relationships between input features and target
    variables. Establishing a robust monitoring and governance framework enables you
    to proactively manage your model’s performance and maintain its accuracy in the
    face of evolving trends and conditions. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will explore these aspects in greater detail,
    along with other critical components of model monitoring and governance, to help
    you develop a comprehensive strategy for maintaining your DL deployment’s effectiveness.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore the recommendations for model latency optimization.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing model latency
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming you have chosen an ideal architecture, trained a model, extracted
    insights, selected the inference model compiler/acceleration framework, and selected
    the target hardware infrastructure and architecture for hosting your models, there
    are additional steps you can take to improve model latency at this stage. The
    following techniques can be employed:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '**Model pruning**: Remove unnecessary neurons or weights in the neural network
    without affecting the overall performance significantly. Pruning techniques include
    weight pruning, neuron pruning, and filter pruning. This can reduce model size
    and computational requirements, resulting in faster inference times.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model quantization**: Reduce the precision of model parameters (for example,
    weights and biases) from 32-bit floating-point numbers to lower bit-width representations
    such as 16-bit or 8-bit integers. Quantization can accelerate model inference
    without significant loss in accuracy, especially when deploying DL models on hardware
    with limited computational resources.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model distillation**: Train a smaller, faster “student” model to mimic the
    behavior of a larger, slower “teacher” model. The student model learns from the
    teacher model’s outputs, achieving comparable performance with reduced complexity
    and faster inference times. This method was demonstrated in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model parallelism**: In model parallelism, different parts of a neural network
    are distributed across multiple devices or processors, allowing concurrent computation
    on different portions of the model. For huge models that cannot fit entirely within
    the memory of a single GPU, this method is an essential step. For models that
    have highly parallel operations, latency can be reduced significantly. Model parallelism
    can be achieved at various parallelism levels, such as layer-level, pipeline-level,
    or tensor-slicing level parallelism.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch inference**: Process multiple input samples simultaneously through
    batch processing, enabling the model to make better use of the underlying hardware,
    leading to faster overall inference times.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore tools that abstract deployment.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Tools that abstract deployment
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are numerous tools and platforms available that help abstract the model
    deployment process, making it easier and more efficient to deploy machine learning
    models in various environments. Here’s an overview of some popular tools and platforms,
    including both open source and paid-for tools:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow serving (open source tool)**: A flexible, high-performance serving
    system for deploying TensorFlow models in a production environment that provides
    out-of-the-box support for model versioning, REST and gRPC APIs, and efficient
    model serving on GPUs and CPUs.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TorchServe (open source tool)**: The PyTorch equivalent of TensorFlow Serving.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Extended (TFX) (open source tool)**: An end-to-end platform for
    deploying, managing, and maintaining machine learning pipelines in production.
    TFX integrates with TensorFlow, TensorFlow Serving, and other tools to provide
    a seamless deployment experience.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow (open source tool)**: An open source platform that streamlines the
    end-to-end machine learning life cycle, including experimentation, reproducibility,
    deployment, and monitoring. It supports multiple languages and machine learning
    libraries, making it a versatile choice for diverse projects.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow (open source tool)**: A Kubernetes-integrated solution that’s designed
    to facilitate the creation, coordination, deployment, and execution of adaptable
    and transportable machine learning tasks. It simplifies the deployment process
    by providing a consistent and unified environment across different cloud providers
    and on-premises infrastructure.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlit (open source tool)**: A Python library that enables developers
    to quickly build and deploy custom web applications for machine learning and data
    science projects. Streamlit simplifies the process of creating interactive web
    apps with minimal coding, making it easier to share and deploy models through
    web apps.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NVIDIA Triton (open source tool)**: An open source tool that can be used
    to deploy DL models. It natively supports many frameworks, most notably TensorRT,
    Pytorch, ONNX Runtime, OpenVINO, and a general Python backend that allows you
    to wrap and run any DL framework and Python code. It provides predictions through
    HTTP REST APIs and the gRPC protocol. It also natively provides the Prometheus-compatible
    and standard time series performance metric logs, which can be subsequently used
    for model monitoring in the Grafana dashboard. It also allows us to configure
    custom metrics in its C API. Most relevantly, it eases multiple GPU utilization
    and GPU memory assignments. We will be exploring this tool practically in the
    next section.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure ML deployment (paid-for tool)**: Microsoft Azure’s machine learning
    service that simplifies model deployment in the cloud. It provides tools for managing,
    monitoring, and scaling deployed models, and supports popular frameworks such
    as TensorFlow and PyTorch.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataRobot (paid-for tool)**: DataRobot is an automated machine learning platform
    that simplifies the process of building, deploying, and maintaining machine learning
    models. It provides a wide range of tools and features, including customization,
    model versioning, monitoring, and collaboration. We will be exploring the usage
    of this platform in [*Chapter 18*](B18187_18.xhtml#_idTextAnchor265), *Exploring
    the DataRobot* *AI Platform*.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Vertex AI (paid-for tool)**: A managed machine learning platform from
    Google Cloud that streamlines the end-to-end machine learning workflow, including
    model training, deployment, and management. It integrates with TensorFlow, PyTorch,
    and other popular frameworks.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon SageMaker (paid-for tool)**: A fully managed machine learning service
    from AWS that allows developers to build, train, and deploy machine learning models
    quickly and easily. It supports multiple frameworks and provides tools for model
    versioning, monitoring, and scaling.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools and platforms help simplify and streamline the model deployment
    process, enabling developers to efficiently deploy their machine learning models
    in various environments.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Successful DL deployment requires addressing key aspects such as safety, trust,
    reliability, and latency optimization while leveraging tools and platforms that
    simplify the process. By adhering to these recommendations and utilizing appropriate
    tools, developers can effectively deploy and manage their DL models in various
    environments, ensuring consistent and reliable performance.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: It should be apparent by now that many trade-offs and criteria need to be evaluated
    and considered before you can make a DL deployment system component choice. However,
    if you don’t have access to a paid tool, have a DL model, and have access to a
    GPU machine that has enough RAM to host your model, three tools are a no-brainer
    to choose from. In the next section, we will dive into a topic that both reveals
    those three tools and practically uses them.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three tools are ONNX, TensorRT, and NVIDIA Triton Server. ONNX and TensorRT
    are meant to perform GPU-based inference acceleration, while NVIDIA Triton Server
    is meant to host HTTP or GRPC APIs. We will explore these three tools practically
    in this section. TensorRT is known to perform the best model optimization toward
    the GPU to speed up inference, while NVIDIA Triton Server is a battle-tested tool
    for hosting DP models that have compatibility with TensorRT natively. ONNX, on
    the other hand, is an intermediate framework in the setup, which we will use primarily
    to host the weight formats that are directly supported by TensorRT.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'In this practical tutorial, we will be deploying a Hugging Face-sourced language
    model that can be supported on most NVIDIA GPU devices. We will be converting
    our PyTorch-based language model from Hugging Face into ONNX weights, which will
    allow TensorRT to load the Hugging Face language model. Then, we will create the
    code and configuration required by the NVIDIA Triton Server framework to host
    the language model. NVIDIA Triton Server supports two ways of deploying a model,
    which is to deploy the DL model with its pre-processing and post-processing methods
    as a single pipeline all embedded into a Python class, and to deploy the DL model
    by logically separating the pipeline into separate components. *Figure 15**.1*
    depicts both approaches with a pipeline that requires two models:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Two approaches for configuring the model deployment with two
    models in the pipeline](img/B18187_15_01.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Two approaches for configuring the model deployment with two models
    in the pipeline
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'An intuitive and straightforward way to deploy your DL model pipeline is to
    go with the first approach. However, the second approach, which involves breaking
    down and separating each component into its configuration, provides multiple benefits:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '**Modularity and reusability**: The modularity aspect allows the individual
    parts to be reused across different pipelines or projects. Additionally, it allows
    easier component swapping while maintaining other components in the pipeline.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and flexibility**: This allows you to deploy different components
    to different GPUs and assign different instances of each component running at
    one time. Additionally, this method allows CPU-bound methods to not get tied to
    a GPU.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism and latency reduction**: Native parallelism can be enabled through
    parallel branches instead of you needing to implement it in Python code.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following two approaches as ways to organize a factory assembly
    line:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: In the first approach, the entire assembly process is combined into a single
    pipeline. This means that all components are processed and assembled sequentially
    in one integrated process. This can be easier to set up and manage, but it may
    not be as flexible or scalable as the second approach.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second approach, the assembly process is broken down into separate, modular
    components that can be individually managed and optimized. This allows for greater
    flexibility and scalability as each component can be fine-tuned or replaced without
    it affecting the entire pipeline. Additionally, this approach enables parallel
    processing, where multiple components can be processed simultaneously, potentially
    reducing overall latency and improving efficiency.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things simple, we will be demonstrating the first approach here.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Practically deploying a DL model with the single pipeline approach
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, we will explore the process of deploying a DL model using
    ONNX, TensorRT, and NVIDIA Triton Server. While deploying the model using NVIDIA
    Triton Server, you may encounter issues related to model loading, configuration,
    or inference. Here are some troubleshooting tips:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the model files, configuration files, and other required files are
    in the correct locations and have the proper file permissions
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the model configuration file (`config.pbtxt`) has the correct settings,
    such as input and output tensor names, data types, and dimensions
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the NVIDIA Triton Server logs for any error messages or warnings that
    could provide insights into the issue
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the necessary dependencies, such as the DL framework, ONNX, and
    TensorRT, are installed and compatible with your system and hardware
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start the practical tutorial in a step-by-step manner:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the `transformer-deploy` repository by running the
    following code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, we can use the following command:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with torch.no_grad():'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: torch.onnx.export(
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: model_pytorch,
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: args=tuple(inputs_pytorch.values()),
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f=output_path,
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: opset_version=13,
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: do_constant_folding=True,
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: input_names=input_names,
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: output_names=output_names,
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dynamic_axes=dynamic_axis,
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: training=TrainingMode.EVAL,
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'of this process. This is what happens when we use the transformer-deploy tool:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: trt_engine = builder.build_serialized_network(network_def, config)
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'engine: ICudaEngine = runtime.deserialize_cuda_engine(trt_engine)'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'with open(engine_file_path, "wb") as f:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f.write(engine.serialize())
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will execute the command that will convert the chosen Hugging Face
    text generation model into a TensorRT serialized engine:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result of running the command in *step 4* is as follows:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we will move the model into new folders at the following path:'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, it is time to create the code and configurations required to host this
    TensorRT model in NVIDIA Triton Server. Following approach 1 from *Figure 15**.1*,
    we need to define a Python class to initialize and perform inference using the
    Hugging Face tokenizer and the TensorRT engine. Let’s start by importing the necessary
    Python libraries into this deployment Python code file:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we must specify the model and path where we stored the serialized TensorRT
    model:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we must define the `TritonPythonModel` class interface, starting with
    the initialization method:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The class name must be the same as the initialization method name – that is,
    `initialize`. This code loads the pre-trained tokenizer from the Hugging Face
    library methods and downloads the tokenizer from the internet. Note that for production
    deployment of a model, it is advised to have a managed instance of the tokenizer
    weights or any model weights somewhere to ensure a reliable deployment process.
    Additionally, the code loads the serialized TensorRT engine.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to define the actual inference part of the tokenizer and model,
    as follows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code should live under the Python code `model.py` file under the `models/transformer_tensorrt_text_generation/1/model.py`
    path.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The folder named `1` is to symbolize the version of the `transformer_tensorrt_text_generation`
    model name.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final file we need is a configuration file that specifies the name of the
    model, the max batch size of the model, the backend type of the model (in this
    case, Python), the input type, the name and dimensions of the model, the output
    type, the name and dimensions of the model, the number of instances of this pipeline,
    and finally whether to use GPU or CPU. The file needs to be named `config.pbtxt`.
    The content of this file for our usage is as follows:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should be stored under the following file path:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we have all the code and configuration needed to run NVIDIA Triton Server
    and deploy our language model, which is an easy-to-use `nvidia-docker`-based deployment
    with a publicly available and downloadable image. The language model can be deployed
    on NVIDIA Triton Server by executing the following command:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: import tritonclient.http as httpclient
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also need to specify the model name defined in the `config.pbtxt` file from
    *step 12*, along with the model version, as follows:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will define the client using the `httpclient` helper tool, define the
    input data, configure the output data so that it’s obtained according to the `config.pbtxt`
    specified output name, and print the generated text:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running `python triton_client.py` in the command line will return the following
    response:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: And with that, we are done with the practical tutorial!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: This topic serves to show the minimal workflow needed to deploy a language model
    with acceleration with NVIDIA Triton Server, which is not too different from an
    audio DL model or a computer vision DL model. Try the workflow presented here
    using other language models and try to play around with all the settings! Note
    that there can be some issues with either the conversion or the optimization stage
    due to highly custom layers from new language models, so you will either need
    to work on fixing it in the base libraries themselves or raise it to the respective
    teams and wait for it to be upgraded.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying specifically with language models, there are a few more tools
    that can be used for deployment that might be worth considering due to their high-level
    abstraction of the Hugging Face models and out-of-the-box official support of
    selected LLMs. These are as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '`baichuan-inc/Baichuan-7B`), BLOOM (`bigscience/bloom` and `bigscience/bloomz`),
    GPT-2 (`gpt2` and `gpt2-xl`), GPT BigCode (`bigcode/starcoder` and `bigcode/gpt_bigcode-santacoder`),
    GPT-J (`EleutherAI/gpt-j-6b` and `nomic-ai/gpt4all-j`), GPT-NeoX (`EleutherAI/gpt-neox-20b`,
    `databricks/dolly-v2-12b`, and `stabilityai/stablelm-tuned-alpha-7b`), LLaMA and
    LLaMA-2 (`meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`,
    and `openlm-research/open_llama_13b`), MPT (`mosaicml/mpt-7b` and `mosaicml/mpt-30b`),
    and OPT (`facebook/opt-66b` and `facebook/opt-iml-max-30b`)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CTranslate2** (https://github.com/OpenNMT/CTranslate2): This boasts efficient
    inference with support of the following models:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-decoder models**: Transformer base/big, M2M-100, NLLB, BART, mBART,
    Pegasus, T5, and Whisper'
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder-only models**: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, CodeGen,
    GPTBigCode, and Falcon'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-only** **models**: BERT'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-generation-interface** (https://github.com/huggingface/text-generation-inference):
    This is not as efficient without an accelerator but it provides manually performed
    offline optimizations for the following models: BLOOM, FLAN-T5, Galactica, GPT-Neox,
    Llama, OPT, SantaCoder, Starcoder, Falcon 7B, Falcon 40B, MPT, and Llama V2'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenLLM** ([https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)):
    This boasts integration with Langchain and Hugging Face agents but without using
    acceleration/compiler libraries'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mlc-llm** ([https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)):
    This boasts support on a variety of devices, such as mobile phones'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a final point, the practical deployment example presented here wouldn’t be
    as effective without us following the recommendations and guidelines presented
    in the previous topic, so be sure to follow through with every one of them!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the various aspects of deploying DL models in production
    environments, focusing on key components, requirements, and strategies. We discussed
    architectural choices, hardware infrastructure, model packaging, safety, trust,
    reliability, security, authentication, communication protocols, user interfaces,
    monitoring, and logging components, along with continuous integration and deployment.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also provided a step-by-step guide for choosing the right deployment
    options based on specific needs, such as latency, availability, scalability, cost,
    model hardware, data privacy, and safety requirements. We also explored general
    recommendations for ensuring model safety, trust, and reliability, optimizing
    model latency, and utilizing tools that simplify the deployment process.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: A practical tutorial on deploying a language model with ONNX, TensorRT, and
    NVIDIA Triton Server was presented, showcasing a minimal workflow needed for accelerated
    deployment using NVIDIA Triton Server.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: By understanding and implementing the strategies and best practices presented
    in this chapter, you can successfully deploy DL models in production with the
    most sensible choice for each component required and unlock their full potential.
    To build on this success path, we need to make sure we don’t forget about our
    model after we deploy it and always consider monitoring our deployed model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the many aspects of monitoring that we
    need to consider to ensure the continued success of our machine learning use case.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
