- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Deep Learning Models to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we delved into the intricacies of data preparation,
    **deep learning** (**DL**) model development, and how to deliver insightful outcomes
    from our DL models. Through meticulous data analysis, feature engineering, model
    optimization, and model analysis, we have learned the techniques to ensure our
    DL models can perform well and as desired. As we transition into the next phase
    of our journey, the focus now shifts toward deploying these DL models in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Reaching the stage of deploying a DL model to production is a significant accomplishment,
    considering that most models don’t make it that far. If your project has reached
    this milestone, it signifies that you have successfully satisfied stakeholders,
    presented valuable insights, and performed thorough value and metric analysis.
    Congratulations, as you are now one step closer to joining the small percentage
    of successful projects amidst countless attempts. It’s worth noting that, according
    to a 2022 Gartner survey highlighted by VentureBeat, which was executed online
    from October to December 2021 with 699 respondents from organizations in the US,
    Germany, and the UK, only around half (54%) of AI models make it into production.
    Furthermore, the 2023 State of AI Infrastructure Survey, published by Run AI,
    an AI resource management solutions provider, reported that in over 88% of the
    companies surveyed, less than half of the AI models reached the production stage.
    This involved 450 industry professionals across the US and Western Europe. These
    two surveys emphasize the challenges faced in this process and the significance
    of reaching this stage.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal here is to make these DL models accessible to end users, in
    an intuitive way, enabling them to harness the full potential of DL in real-world
    applications. In this chapter, we will explore the various strategies, tools,
    and best practices to seamlessly integrate our DL models into production systems,
    ensuring scalability, reliability, and ease of use for a diverse range of users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will be going through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the crucial components for DL model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying key DL model deployment requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right DL model deployment options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring deployment decisions based on practical use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering general recommendations for DL deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will have a practical topic in the last section of this chapter. This tutorial
    requires you to have a Linux machine with an NVIDIA GPU device ideally in Ubuntu
    with Python 3.10 and the `nvidia-docker` tool installed. Additionally, we will
    require the following Python libraries to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers==4.21.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nvidia-tensorrt==8.4.1.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch==1.12.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers-deploy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tritonclient`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the crucial components for DL model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what does it take to deploy a DL model? It starts with having a holistic
    view of each required component and defining clear requirements that guide decision-making
    for every aspect. This approach ensures alignment with the business goals and
    requirements, maximizing the chances of a successful deployment. With careful
    planning, diligent execution, and a focus on meeting the needs of the business,
    you can increase the likelihood of successfully deploying your DL model and unlocking
    its value for users. We will start by discovering components that are required
    to deploy a DL model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying a DL model to production involves more than just the trained model
    itself. It requires seamless collaboration among various components, working together
    to enable users to effectively extract value from the model’s predictions. These
    components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architectural choices**: The overall design and structure of the deployment
    system. Should the model be implemented as a separate service, microservice, or
    directly part of an existing service? Should the model be hosted on the cloud
    or on-premises? Another aspect to consider is whether to use container orchestration
    platforms, such as Kubernetes, Docker Swarm, or Apache Mesos, to manage and scale
    deployments of deep learning models in containerized applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These platforms provide flexible deployment across multiple machines, cloud
    providers, or on-premises infrastructure, and can be used in conjunction with
    other tools and services for efficient management of containerized applications
    and microservices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Hardware/physical infrastructure choices**: This involves the decision of
    which physical computing device you want to use and the choice of each of the
    components that make up the computing device. Should the model be run on a CPU,
    GPU, TPU, or an **Artificial Neural Engine** (**ANE**) in an iPhone?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packaging methods and frameworks**: This is a component that involves
    serializing the model’s architecture, weights, and configuration into a file or
    container format, allowing for easy distribution, deployment, and usage across
    various environments. Usually, the DL framework will provide out-of-the-box support
    for model packaging. Do you have architectural choices and hardware infrastructure
    choices or preferences that require the model to be packaged in a specific way?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model safety, trust, and reliability component**: This encompasses the measures
    taken to ensure that the deployed model is secure, trustworthy, and reliable in
    making accurate predictions. It involves implementing guardrails to prevent misuse
    or unintended behavior, ensuring model consistency, monitoring model performance,
    and providing prediction explanations to help users understand and trust the model’s
    output. Ensuring data privacy and compliance with relevant regulations is also
    a critical aspect of this component. Are there any specific safety, trust, or
    reliability requirements that must be met for the deployment of your DL model?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and authentication methods**: These involve protecting your DL model
    and its associated infrastructure, as well as controlling access to the model
    by implementing suitable authentication, authorization, and encryption mechanisms.
    This ensures that only authorized users can access and interact with the model,
    preventing unauthorized access, data breaches, and potential misuse of the model.
    What are the necessary security and authentication requirements for your DL model
    deployment, and how will they be integrated into your system?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication protocols**: These define the rules and formats for exchanging
    data between the deployed model and other components or users in the system. It
    involves selecting appropriate protocols based on the requirements, such as latency,
    reliability, and data formats. Examples of communication protocols are HTTP, RESTful
    APIs, gRPC, server-sent events, and WebSockets. What communication protocols best
    suit your DL model deployment, and how will they be implemented to enable seamless
    interaction between the model and its users?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User interfaces**: These are the visual components and interaction methods
    that allow users or downstream systems to access, interact with, and obtain predictions
    from the deployed DL model. User interfaces can be web-based, mobile, desktop
    applications, APIs, or even voice-activated systems, depending on the use case
    and target audience. Designing user-friendly and intuitive interfaces is essential
    to ensure that users can easily understand and make the most of the model’s predictions.
    What type of user interface is best suited for your DL model deployment, and how
    will it be designed to provide an optimal user experience while effectively delivering
    the model’s capabilities? Here are some examples of user interface design challenges
    specific to DL models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualizing complex data**: DL models often work with multi-dimensional data,
    which can be challenging to display in a user-friendly manner. Designers may need
    to devise innovative ways to visualize and represent such data, making it accessible
    and understandable for users.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling real-time data**: In scenarios where DL models process and analyze
    real-time data, the user interface must efficiently manage data streaming and
    updates, ensuring that users receive timely and accurate information without being
    overwhelmed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitating model interactions**: Users may need to interact with the DL
    model to adjust parameters, provide feedback, or request additional information.
    Designing intuitive UI elements for these interactions is crucial to ensure users
    can effectively engage with the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpreting model output**: DL models can produce complex and nuanced output,
    which may be challenging for users to understand and act upon. Designers must
    find ways to present model predictions in a clear and actionable manner while
    also providing contextual information to help users interpret the results.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managing uncertainty**: DL models may produce predictions with varying degrees
    of confidence or uncertainty. Designers should consider how to communicate this
    uncertainty to users, ensuring that they are aware of the limitations and potential
    risks associated with the model’s output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility and inclusivity**: User interfaces for DL models should cater
    to a diverse range of users, including those with different abilities, languages,
    and cultural backgrounds. Designers must ensure that their interfaces are accessible
    and inclusive, taking into account various user needs and preferences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and logging components**: These tools allow you to track the performance,
    usage, and health of your DL model in real time. By collecting and analyzing relevant
    metrics, logs, and alerts, this component helps identify potential issues, optimize
    the model’s performance, and ensure a stable deployment environment. How will
    you implement monitoring and logging to track your DL model’s health and performance,
    and what metrics will be crucial to measure its success?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous integration/continuous deployment** (**CI/CD**): This process
    involves the automated building, testing, and deployment of your DL model whenever
    changes are made to its code, data, or infrastructure. CI/CD streamlines the development
    life cycle, enabling faster iterations and improvements while ensuring that the
    deployed model remains up-to-date and reliable. What CI/CD practices and tools
    will you adopt to maintain a seamless deployment pipeline for your DL model?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With numerous options available for each of these components, it’s essential
    to have a strategy to decide which ones to use. The first logical step in this
    process is to define the specific requirements that will guide decision-making
    for each component. In the next section, we will discuss how to establish these
    requirements, ensuring that your choices align with your business goals.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying key DL model deployment requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To determine the most suitable deployment strategy from a variety of options,
    it is essential to identify and define seven key requirements. These are latency
    and availability, cost, scalability, model hardware, data privacy, safety, and
    trust and reliability requirements. Let’s dive into each of these requirements
    in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency and availability requirements**: These are two closely connected
    components and should be defined together. Availability requirements refer to
    the desired level of uptime and accessibility of the model’s prediction. Latency
    requirements refer to the maximum acceptable delay or response time that the models
    must meet to provide timely predictions or results. A deployment with a low availability
    requirement usually can tolerate high latency predictions, and vice versa. One
    reason is that a low-latency capable infrastructure can’t ensure low latency if
    it is not available when model predictions are requested. However, there are edge
    cases that can require complete availability and low latency only for a short
    period but can be unavailable for the rest of the time, which is considered low
    availability but with low latency requirements. Here are a few best practices
    when determining latency and availability requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the expectations and needs of the end users or applications utilizing
    the DL model. Consult with stakeholders to understand the desired response times
    and availability levels they expect.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the impact of latency and availability on the overall system or business
    process. Identify critical points where delays can significantly affect user experience
    or business operations. Is waiting for a minimum of 1 hour for predictions going
    to provide the value the business wanted?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify time windows or periods where availability is particularly crucial.
    Determine if the DL model needs to be available 24/7 or if there are specific
    hours or events when high availability is essential.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set both the ideal and maximum latency and availability thresholds. The maximum
    is usually where a significant value can still be obtained, and an ideal condition
    would just slightly increase that value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost requirements**: Budget constraints are a critical consideration in any
    business, and it is essential to determine the maximum cost you are willing to
    allocate for deploying a machine learning model based on the expected value it
    will bring. To ensure that the expenses do not exceed what the organization is
    willing to invest, it is advisable to conduct a cost-benefit analysis. This analysis
    will involve evaluating the cost implications of various components within the
    deployment infrastructure, including achieving higher levels of latency and availability.
    By carefully balancing the desired requirements against the associated infrastructure
    costs and operational complexities, you can make informed decisions that align
    with the overall financial goals of your organization while still leveraging the
    benefits of machine learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability requirements**: Scalability is the ability of a deployment infrastructure
    to handle an increase or decrease in workload demands without compromising performance
    or quality. It is essential to determine the scalability requirements of your
    deep learning model, as this will impact the choice of deployment strategy and
    infrastructure. Do you expect the model usage to grow over time? How fast do you
    expect it to grow? Do you need to scale horizontally (adding more instances of
    the model) or vertically (increasing the resources of existing instances)? Having
    an expectation regarding the utilization growth rate will allow you to choose
    appropriate components and decisions in operationalizing your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model hardware requirements**: The choice of hardware for deploying a DL
    model is crucial as it can significantly impact the performance, latency, and
    cost of the overall deployment. To properly identify hardware requirements, consider
    the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility**: Ensure the chosen hardware is compatible with the frameworks
    and libraries used to develop the DL model. This includes checking if the hardware
    can support specific functions, such as GPU acceleration, that may be essential
    for model performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing power**: Evaluate the processing power required to efficiently
    run the model, including the number of cores, memory, and storage. Consider how
    the model’s complexity and size may impact hardware requirements.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power consumption and heat dissipation**: The power consumption and heat
    dissipation of the chosen hardware can affect the overall operational cost and
    the environmental footprint of the deployment. Choose hardware that balances performance
    with energy efficiency.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future-proofing**: Consider the expected lifespan of the hardware and its
    ability to accommodate future updates or improvements to the model. Opt for hardware
    that can easily be upgraded or replaced if necessary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**: Ensure the hardware can be seamlessly integrated with the
    rest of the deployment infrastructure and any other relevant systems or components.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By thoroughly assessing model hardware requirements, you can make informed decisions
    that ensure optimal performance while minimizing costs and potential bottlenecks
    in your DL model deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data privacy requirements**: Ensuring the privacy and security of data used
    in the DL model and predictions by it is crucial as it can impact the trust and
    compliance of the deployment. To identify and address data privacy requirements,
    consider the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory compliance**: Understand the data protection regulations and industry
    standards applicable to your organization, such as GDPR, HIPAA, or CCPA. Ensure
    that the deployment strategy and infrastructure comply with these regulations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data storage and processing locations**: Assess where the data will be stored
    and processed during the deployment. Determine if any data residency requirements
    or restrictions exist, such as the need to store data in a specific geographic
    region.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access controls**: All DL applications should have the requirement to
    implement appropriate access controls to ensure that only authorized users or
    systems can access the data. This includes implementing authentication, authorization,
    and encryption mechanisms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data retention and deletion policies**: Check if there are legal and regulatory
    requirements for data retention and deletion. Ensure that the deployment infrastructure
    supports these policies and allows for secure data disposal if necessary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data monitoring and auditing**: Check if there is a need to implement monitoring
    and auditing mechanisms to track data usage and access throughout the deployment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data breach response plan**: Such a plan should include roles and responsibilities,
    communication channels, and remediation actions. Check if there is a need to develop
    a data breach response plan that outlines the steps to be taken when there’s a
    data breach or a security incident.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety requirements**: Reflect on the potential legal and ethical boundaries
    that the model must comply with in the specific region you want to deploy your
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trust and reliability requirements**: Trust and reliability for machine learning
    models refer to the confidence in a model’s consistent performance, accuracy,
    and adherence to ethical and regulatory standards during its deployment and operation.
    Consider these questions when determining requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How frequently will the model be updated or modified?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is tracking multiple model versions necessary?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the model face concept or data drift in its operating environment?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How important is efficient error detection and resolution?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How often will the model receive updates or new features?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is adapting to user feedback or changing requirements essential?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there opportunities to leverage advances in DL to improve the model?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is maintaining a stable and secure production environment a priority?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How critical is the model’s performance to its users or business functions?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there strict SLAs or regulatory requirements related to performance?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is consistent performance across different environments and configurations important?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the model’s predictions have significant consequences, making consistency
    essential for user trust and success?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these requirements are best determined early on in the planning stage.
    For instance, defining latency requirements from the outset allows you to select
    an appropriate model that ensures runtime duration falls within the specified
    latency constraints. Having explored the types of requirements that need to be
    defined and the approximate methods for defining them, we are now prepared to
    discuss choosing the right deployment options.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right DL model deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting the right deployment options for your DL model is a crucial step in
    ensuring optimal performance, scalability, and cost-effectiveness. To assist you
    in making an informed decision, we will explore recommended options based on different
    requirements. These recommendations encompass various aspects, such as hardware
    and physical infrastructure, monitoring and logging components, and deployment
    strategies. By carefully evaluating your model’s characteristics, resource constraints,
    and desired outcomes, you should be able to identify the most suitable deployment
    solution that aligns with your objectives while maximizing efficiency and return
    on investment through this guide. The tangible deployment components we will explore
    here are architectural decisions, computing hardware, model packaging and frameworks,
    communication protocols, and user interfaces. Let’s dive into each component one
    by one, starting with architectural choices.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Architectural choices for a machine learning service involve designing the
    infrastructure, data pipelines, and deployment methods for efficient and reliable
    operations. We will start with service placement considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microservice**: **Deploy the Deep Learning** (**DL**) model as a small, loosely
    coupled, and independently deployable service with its own APIs. A microservice
    is a software architecture design pattern where an application is structured as
    a collection of small, loosely coupled, and independently deployable services.
    Each microservice is responsible for a specific functionality or domain within
    the application and communicates with other microservices through well-defined
    **Application Programming Interfaces** (**APIs**). So, when deploying as a microservice,
    a prerequisite is that other components are also implemented as a microservice.
    Its advantages are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better scalability
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier updates and maintenance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher resilience
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility in technology choices
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choose this microservice in the following circumstances:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When model usage is expected to grow
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When frequent updates are needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When integration with various external systems is required
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When high resilience is crucial
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standalone service:** Deploy the DL model as a separate, independent service,
    that is not a microservice. Consider a movie recommendation application - a microservice
    approach would be to create a **Review Analysis Service** microservice that processes
    movie reviews using a DL model. It has its own API, data storage, and deployment
    pipeline, operating independently from other services in the application. For
    a separate service approach in the same application, a **Movie Recommendation
    Service** combines user preference management, movie review analysis (using the
    DL model), and recommendation generation. It''s more monolithic, combining related
    functionalities, with its own API but no separate microservice for review analysis.
    Its advantages are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier management and administration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Better suited for complex applications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consolidated resources and data access
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplified communication between components
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: More predictable performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choose this standalone service in the following circumstances:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the application has a limited number of services
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When the model is complex and requires a more monolithic approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When the scope of the model does not change frequently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When a balance between resilience and complexity is preferred
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part of the existing service**: Integrate the DL model into an existing service
    of an application or system. The advantages are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less complexity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier data synchronization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential cost savings
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choose to integrate with an existing service in the following circumstances:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When model usage growth is limited
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When infrequent updates or modifications are needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When you have limited integration with external systems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When high resilience is not crucial
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decide between microservice or integrating with an existing service by considering
    scalability, update frequency, integration requirements, and resilience. Align
    these factors with your specific requirements to make the best decision for your
    DL model deployment. Next, we will go through recommendations for choosing the
    physical deployment environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud**: Cloud deployments are suitable when you require high availability
    and can tolerate moderate latency. They minimize upfront costs and offer flexible
    pay-as-you-go pricing models. Cloud-based infrastructure provides virtually unlimited
    resources, allows for rapid auto-scaling, and typically offers high uptime guarantees
    and managed services. However, you need to carefully evaluate the cloud providers’
    security offerings and ensure compatibility with your DL framework and libraries.
    A few companies that offer GPU are AWS, GCP, Microsoft Azure, and IBM Cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server on-premises**: Server on-premises deployments give you more control
    over your hardware and network resources, making them ideal for low latency and
    high availability within a specific geographical region. They require an upfront
    investment in terms of hardware and maintenance but can provide long-term cost
    savings, especially if you have high and consistent resource demands. On-premises
    deployments also offer more control over security measures and data privacy but
    require more effort in maintaining and updating security measures. Ensure compatibility
    with your DL framework and libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge on-premises**: Also known as edge computing, this approach processes
    data close to the source, offering extremely low latency and improved security
    and data privacy. Edge deployments are suitable when data processing and storage
    need to happen close to the source, and they can reduce data transfer costs. However,
    managing security across multiple edge devices and ensuring compatibility with
    your DL framework and libraries can be challenging. Edge deployments offer scalability
    in terms of distributing processing across multiple edge devices but may require
    more management and maintenance efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will dive into container orchestration platforms, which have a significant
    impact on how applications and services are designed, deployed, and managed within
    a system. A container is a lightweight, standalone, and executable software package
    that includes everything needed to run a piece of software, including the code,
    runtime, system tools, libraries, and settings. Containers are isolated from each
    other and from the host system, allowing them to run consistently across different
    computing environments. There are two main types of container technologies: Docker
    containers and Linux containers (LXC).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Container orchestration platforms help manage and scale deployments of deep
    learning models in containerized applications, utilizing technologies such as
    Docker containers or LXC. These platforms provide flexible deployment across multiple
    machines, cloud providers, or on-premises infrastructure. They can be used in
    conjunction with other tools and services, enabling efficient management of containerized
    applications and microservices. Some popular container orchestration platforms
    to choose among are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes (open source):** Kubernetes is an open source container orchestration
    platform that automates the deployment, scaling, and management of containerized
    applications, including deep learning models. It works with various container
    technologies, including Docker and LXC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Swarm (open source):** Docker Swarm is a native clustering and scheduling
    tool for Docker containers. It is tightly integrated with the Docker ecosystem,
    providing a simple way to deploy and manage containerized applications. While
    not as feature-rich as Kubernetes, Docker Swarm is known for its ease of use and
    faster setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mesos (open source):** Apache Mesos is a distributed systems kernel
    that abstracts CPU, memory, and storage resources away from machines, enabling
    fault-tolerant and elastic distributed systems. It can be used in conjunction
    with other frameworks such as Marathon or DC/OS to provide container orchestration
    capabilities for deploying and managing deep learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Service
    (ECS) (paid-for services):** These are managed container orchestration services
    provided by AWS. EKS is a managed Kubernetes service, while ECS is a proprietary
    container orchestration platform from AWS. Both services simplify the deployment,
    scaling, and management of containerized applications on AWS infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a container orchestration platform that best suits your deep learning
    deployment requirements, such as flexibility, scalability, compatibility with
    your preferred container technology, cloud provider, and integration with other
    tools and services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will dive into architectural trade-offs between real-time and batch
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time predictions**: It’s recommended to have the model always loaded
    in memory to reduce latency and respond quickly to requests. This setup is suitable
    for applications where immediate response is critical, such as autonomous vehicles,
    live chatbots, or fraud detection systems. Here are some recommendations when
    using this option:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a dedicated server or cloud instance with enough memory and processing power
    to handle the model and concurrent requests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the model for inference by using techniques such as quantization, pruning,
    or model distillation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a load balancer if necessary to distribute incoming requests across
    multiple instances of the model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor resource usage and performance to ensure the system meets real-time
    requirements and scales as needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a queue system to ensure workers are not overloaded or implement autoscaling
    to handle overload cases
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-demand batch predictions**: Batch predictions are suitable for scenarios
    where real-time responses are not crucial, and predictions can be processed in
    groups. This setup requires extra time to spin up worker infrastructure, initialize
    the model, and load trained model weights. Here are some recommendations when
    using this option:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a queue system such as RabbitMQ or Amazon SQS to manage incoming prediction
    requests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a batch processing system that initializes the model and loads weights
    when processing starts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the batch size to balance processing time and resource usage
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement auto-scaling to handle variable workloads and ensure efficient use
    of resources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore computer hardware choices and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Computing hardware choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Selecting hardware to carry out model computations is all about trading off
    cost, availability, and runtime. Let’s explore the different options, along with
    recommendations on when to opt for each option:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU**: CPUs are a versatile and cost-effective option for deploying DL models.
    They are compatible with most frameworks and libraries and provide decent performance
    for less complex models. CPUs are a good choice when cost constraints are a priority,
    and you don’t require the high processing power that GPUs or TPUs offer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU**: GPUs provide faster processing and better parallelization, significantly
    reducing latency and improving performance. They are ideal for complex models
    that demand high processing power. GPUs are an excellent choice when you require
    low latency and high availability, but they come with higher costs compared to
    CPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPU**: TPUs are specialized hardware designed for machine learning tasks,
    offering high performance and efficient processing. They are particularly suitable
    for large models or computationally intensive tasks. TPUs are a great option when
    you need exceptional processing power and low latency but be aware of the potential
    higher costs and that it is only available in GCP and usable only in TensorFlow!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Neural Engines** (**ANEs**): ANEs are specialized AI accelerators
    found in devices such as iPhones. They provide efficient processing for DL tasks
    on edge devices, offering low latency and energy-efficient performance. ANEs are
    a good choice when your application requires user interface requirements on an
    iPhone, which is an edge device. Note that it is only compatible with the CoreML
    framework and that the ONNX weights format is needed to convert weights easily
    to CoreML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FPGA**: FPGAs are highly customizable (its hardware circuitry can be programmed!)
    and energy-efficient hardware and are suitable for deploying DL models that require
    low latency and adaptability. The con here is the need to have deep expertise
    in the FPGA programming language and circuit development to successfully allow
    inference with a trained neural network efficiently. This is an out-of-bounds
    device for most teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore model packaging and framework choices and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Model packaging and frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This controls how DL models are executed and where recommendations can depend
    on the compute hardware used, as well as portability and runtime requirements.
    Here are some popular examples, along with recommendations on when to use them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Original framework packaging**: You can take advantage of specific optimizations
    and features provided by the framework, potentially improving performance. However,
    certain cases may require compatibility with specific hardware options, such as
    using a TPU, which is only supported by the TensorFlow framework, so if you have
    a TPU and you stick with PyTorch, you will not be able to use the TPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open Neural Network Exchange** (**ONNX**) **framework**: ONNX provides an
    open standard for representing DL models, allowing you to convert your model to
    different frameworks and run it on various hardware platforms. Using ONNX can
    increase the flexibility and portability of your model, enabling you to choose
    from a wider range of hardware and infrastructure options. Moreover, it allows
    you to leverage optimizations and features provided by different DL frameworks.
    A convenient and general solution to address packaging issues is to convert your
    model into the ONNX format, which can then be easily converted into other formats
    as needed. This approach streamlines the process and ensures smooth integration
    with various hardware and framework options, such as leveraging ANE in an iPhone
    to accelerate your deep learning model within an app.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ONNX Runtime**: This is an inference accelerator that’s designed to accelerate
    DL model inference in any hardware by leveraging compute and memory optimizations.
    It is faster to run a model in ONNX Runtime than to run it in their native DL
    framework, such as TensorFlow or PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT**: This is a high-performance DL inference optimizer and runtime/compiler
    library from NVIDIA that’s designed to accelerate DL model inference on NVIDIA
    GPUs. It supports TensorFlow and ONNX and provides easy ways to convert model
    weights so that they’re compatible with its framework, offering fast and efficient
    model deployment. TensorRT allows faster model inference speed in GPUs by collectively
    tuning the model at a lower level, leveraging different GPU internal hardware
    capabilities to maximize the model efficiency during inference. As ONNX weights
    are compatible with TensorRT, a typical path to convert PyTorch model weights
    into a TensorRT-compatible weight format is to convert PyTorch model weights into
    ONNX weights. On an NVIDIA GPU, TensorRT is known to be faster than ONNX Runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open Visual Inference & Neural Network Optimization** (**OpenVINO**): This
    is a toolkit from Intel that accelerates DL model inference across Intel hardware,
    including CPUs, GPUs, and FPGAs. It supports TensorFlow, ONNX, and other frameworks,
    offering optimized model deployment in diverse environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore communication protocol choices and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Communication protocols to use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The protocol you should use can depend on the runtime requirements, network
    load requirements, user interface chosen, mode of deployment, and compute requirements.
    Here are some examples, along with their recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MQTT**: Use MQTT when you need a lightweight, low-latency protocol for devices
    with limited resources, such as IoT devices, and real-time communication and status
    updates are essential for your application. Power consumption and heat dissipation
    are important factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP or REST API**: Choose this when you require a well-supported and easy-to-implement
    protocol for web services and data exchange, your application follows a request-response
    communication pattern, and finally where compliance with data protection regulations
    and data privacy is crucial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gRPC**: Opt for gRPC when you need a high-performance, low-latency protocol
    for large-scale distributed systems or microservices, bidirectional streaming,
    and support for multiple programming languages are essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-Sent Events** (**SSE**) **or WebSockets**: Use them when real-time
    notifications or live updates are critical for your web application. If you require
    unidirectional communication between server and client, use SSE. If you require
    bidirectional communication between server and client, use WebSockets. A notable
    domain that requires these communication protocols is live collaborative tools
    with machine learning. Here are some examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammarly uses Websockets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT uses SSE
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore user interface choices and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: User interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When designing user interfaces for machine learning applications, it is essential
    to consider factors such as user experience, accessibility, responsiveness, and
    adaptability. Here are some recommendations for user interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web applications**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are suitable for cross-platform access as users can access the application
    through a web browser
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use popular web development frameworks such as React, Angular, or Vue.js to
    build responsive and interactive user interfaces
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A sentiment analysis tool that allows users to input
    text and receive sentiment scores by interacting with a machine learning model
    through a web-based interface'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For web applications, you need to also choose a web framework wisely according
    to the benefits it provides, along with the latency trade-offs. Refer to https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db
    for an estimate of the latency you will get for different web frameworks for a
    single web API query.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Mobile applications**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for on-the-go access to machine learning features through smartphones
    and tablets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop native apps for iOS and Android platforms using Swift or Kotlin, or
    use cross-platform frameworks such as React Native or Flutter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A mobile app that uses a machine learning model for image
    recognition to identify plants or animals by analyzing user-captured photos'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desktop applications**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitable for users who require a dedicated, platform-specific application with
    offline functionality
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use technologies such as Electron or Qt for cross-platform desktop applications
    or platform-specific languages such as C# for Windows or Swift for macOS
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A video editing software with built-in machine learning-powered
    features such as object tracking, automatic color grading, or scene detection'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice User** **Interfaces** (**VUI**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for hands-free interaction with machine learning-powered services through
    voice commands
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate with popular voice assistant platforms such as Amazon Alexa, Google
    Assistant, or Apple Siri
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A voice-activated home automation system that uses natural
    language processing to control smart devices based on user commands'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversational** **UI (chatbots)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitable for engaging users more naturally and interactively through text or
    voice conversations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use chatbot development platforms such as Dialogflow, Rasa, or Microsoft Bot
    Framework
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A customer support chatbot that uses machine learning-powered
    natural language understanding to answer user queries and provide assistance'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented reality** (**AR**) **and virtual** **reality** (**VR**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideal for immersive and interactive experiences that combine the real and digital
    worlds
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AR/VR development platforms such as Unity or Unreal Engine and integrate
    machine learning models for object recognition, motion tracking, or scene understanding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A virtual training simulator that uses deep learning
    models to analyze and assess user performance in real time. In this AR/VR application,
    users can practice various skills, such as medical procedures, mechanical repairs,
    or emergency response scenarios. The deep learning model evaluates the user’s
    actions through visual input, provides instant feedback, and offers personalized
    guidance for improvement, enhancing the learning experience and accelerating skill
    development.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API-based user interface**: An API-based user interface provides a flexible
    and scalable way to integrate your machine learning model with various applications,
    platforms, and services. This approach allows developers to build custom user
    interfaces or incorporate machine learning-powered features into existing applications,
    expanding the reach and impact of your model. This is suitable for enabling other
    applications, systems, or services to access and interact with your machine learning
    model programmatically. Two recommendations for this approach are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use REST, gRPC, SSE, Websockets, or MQTT to create well-structured and documented
    APIs that expose the machine learning model’s functionality to external clients
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement authentication and authorization mechanisms (for example, API keys
    and OAuth) to ensure secure access to the API
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example use case**: A sentiment analysis API that allows developers to integrate
    machine learning-powered sentiment analysis into their applications by sending
    text data and receiving sentiment scores through API calls'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right deployment options for your DL model involves carefully evaluating
    architectural choices, hardware options, communication protocols, and user interfaces
    that best align with your specific requirements and objectives. By considering
    factors such as scalability, update frequency, integration needs, and resilience,
    you can select the most suitable deployment solution that maximizes efficiency
    and ROI.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss some practical examples for deciding on components when
    deploying DL models in production.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring deployment decisions based on practical use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore practical deployment decisions for DL models
    in production, focusing on two distinct use cases: a sentiment analysis application
    for an e-commerce company and a face detection and recognition system for security
    cameras. By examining these real-world scenarios, we will gain valuable insights
    into establishing robust deployment strategies tailored to specific needs and
    objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring deployment decisions for a sentiment analysis application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you are developing a sentiment analysis application to be used by an
    e-commerce company to analyze customer reviews in real-time. The system needs
    to process a large number of reviews every day, and low latency is essential to
    provide immediate insights for the company. In this case, your choices could be
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architectural choice**: As an independent service, as it would allow better
    scalability and easier updates to handle the growing number of requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware/infrastructure choice**: GPU on a cloud service, as it provides
    better parallelization and processing power for a large number of simultaneous
    requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety, trust, and reliability**: Implement monitoring for data drift and
    model performance, regularly retrain the model on updated data, and ensure compliance
    with data privacy regulations. For example, anonymize user information and avoid
    storing **personally identifiable information** (**PII**) in the analysis as it
    can infringe upon data protection regulations, such as the GDPR in the European
    Union or the CCPA in the United States, depending on the country the application
    is intended to be deployed in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication protocol**: RESTful APIs or gRPC, as they are well-suited for
    web services and can handle a large number of requests with low latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User interface**: A web-based dashboard where the company’s staff can monitor
    the sentiment analysis results in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring deployment decisions for a face detection and recognition system for
    security cameras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you are building an object detection system for security cameras that
    need to detect intruders in real time. In this case, your choices could be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architectural choice**: Edge on-premises, as it provides low latency and
    improved security by processing data close to the source. This choice also reduces
    the time needed for data to travel through the network, as no video streaming
    to some cloud server is needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware/infrastructure choice**: GPU or TPU on the edge device, depending
    on the compatibility with the DL framework and the model’s complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety, trust, and reliability**: Implement monitoring for model performance
    and ensure compliance with local regulations related to video surveillance such
    as data privacy, retention policies, and consent requirements, to maintain ethical
    and legal standards in video analytics. For example, the facial images shouldn’t
    be stored, only the extracted facial features, as this can infringe personal data
    protection-related regulations, depending on the country it is intended to be
    deployed in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication protocol**: MQTT or WebSockets, as they provide low-latency
    communication between edge devices and the central monitoring system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User interface**: A desktop application that displays real-time video feeds
    with object detection overlays for security personnel to monitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By considering the specific requirements of each use case, you can make informed
    decisions on the components required for deploying DL models in production. Now,
    let’s move on to some general recommendations for successful DL model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering general recommendations for DL deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will discover DL deployment recommendations related to three verticals,
    namely model safety, trust, and reliability assurance, model latency optimization,
    and tools that help abstract model deployment-related decisions and ease the model
    deployment process. We will dive into the three verticals one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Model safety, trust, and reliability assurance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring model safety, trust, and reliability is a crucial aspect of deploying
    DL systems. In this section, we will explore various recommendations and best
    practices to help you establish a robust framework for maintaining the integrity
    of your models. This includes compliance with regulations, implementing guardrails,
    prediction consistency, comprehensive testing, staging and production deployment
    strategies, usability tests, retraining and updating deployed models, human-in-the-loop
    decision-making, and model governance. By adopting these measures, you can effectively
    mitigate risks, enhance performance, and foster user trust in your DL deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Comply with regulations and implement guardrails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regulatory compliance and guardrails are essential components of responsible
    deep learning deployment, ensuring that your model adheres to relevant laws, industry
    standards, and ethical guidelines. Implementing a robust compliance framework
    not only mitigates legal and reputational risks but also fosters trust among users
    and stakeholders. It’s a very broad topic, so here are a few examples to learn
    from:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content moderation for social media platforms**: Compliance with community
    guidelines and regional laws can be achieved by implementing AI-powered filters
    for detecting and flagging inappropriate content, setting up a human review process
    for ambiguous cases, and providing users with a transparent mechanism to appeal
    decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI-powered recruitment tools**: Compliance with anti-discrimination laws
    can involve steps such as monitoring bias and fairness metric performance and
    ensuring that any automated decisions are transparent and explainable to both
    employers and applicants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial recognition systems**: Compliance with privacy and ethical guidelines
    can be achieved through steps such as obtaining explicit consent from individuals
    before collecting and processing their biometric data, implementing robust data
    security measures, and ensuring transparency about the system’s capabilities and
    limitations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DL-based video surveillance systems, such as people detection**: Compliance
    with privacy and ethical guidelines can be achieved through measures such as setting
    up clear signage to inform the public about the presence of surveillance cameras,
    restricting data access to authorized personnel, and adhering to data retention
    and deletion policies as per local regulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems (YouTube, Netflix, and Tiktok)**: Ensuring compliance
    with data protection regulations can involve steps such as implementing privacy-preserving
    data processing techniques, providing users with the ability to opt out of personalized
    recommendations, and being transparent about data collection and usage policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative AI**: Compliance can be achieved by using content filtering mechanisms
    to prevent harmful content generation, which includes hate speech, explicit material,
    and content that encourages criminal activities, or prevent dangerous recommendations
    about medical issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we continue to explore model safety, trust, and reliability assurance, let’s
    examine the vital aspect of ensuring prediction consistency in DL deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure prediction consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prediction consistency is all about a model’s ability to generate the same predictions
    when faced with the same input data, no matter the hardware, pre/post serialization
    and loading, infrastructure, or whether it’s a single row or a random batch. Inconsistent
    predictions can lead to mismatched expectations of a model’s accuracy and overall
    performance. To maintain consistency across various factors, it’s essential to
    track and replicate the environmental dependencies involved in training, evaluation,
    and inference. Tools such as Docker can help create isolated environments with
    specific dependencies, ensuring a seamless experience and eliminating potential
    issues. Additionally, consider making automated tests to objectively prevent any
    inconsistency from going through, essentially working as a guardrail.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will discuss the significance of comprehensive testing in
    maintaining a reliable DL deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Other than prediction consistency tests, generally, comprehensive testing will
    ensure that your DL model and system perform as expected at all times and meet
    user requirements. DL systems are essentially software systems and require similar
    things to ensure a successful deployment. The test components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit, integration, and functional tests**: Unit, integration, and functional
    testing are essential for ensuring the reliability, maintainability, and overall
    quality of software components. Here’s why they are important:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit testing**: This focuses on individual components or functions, verifying
    their correctness and isolating potential issues early in development. This helps
    catch bugs before they propagate, reduces debugging time, and improves code maintainability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration testing**: This validates the interactions between different
    components, ensuring they work together as intended. This helps identify interface
    issues, data flow problems, and inconsistencies that can arise when combining
    components, ensuring a smooth integration.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional testing**: This assesses the software’s ability to fulfill its
    intended purpose and meet user requirements. Testing end-to-end functionality
    ensures that the software operates correctly in real-world scenarios and delivers
    a positive user experience.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover and recovery testing**: Verify the model’s ability to recover from
    failures, such as hardware or software crashes, and maintain high availability
    in the face of unexpected disruptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load stress testing**: Evaluate the model’s performance under various load
    conditions to identify bottlenecks and ensure it can handle the expected user
    traffic. These tests can also help you catch errors such as GPU memory overflow,
    CPU overload, or insufficient storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broad and diverse testing**: The model may not be able to handle unexpected
    input data, edge cases, or system failures gracefully, causing crashes or undesired
    behavior. Thinking up all the possible ways the system will be used can help you
    catch issues with the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adopting the staging and production deployment steps**: Embracing a staging
    and production strategy in DL production deployment is highly beneficial for ensuring
    model reliability and performance. This approach involves setting up separate
    environments for testing (staging) and final deployment (production), allowing
    you to validate your model’s behavior and identify potential issues before the
    model goes live. By adopting this strategy, you can minimize the risks associated
    with deploying untested models, streamline the process of identifying and resolving
    issues, and enhance the overall reliability of your DL solutions. Ensure the pipeline
    can continuously be in production for 24 hours without failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability tests**: Usability tests focus on ensuring that software applications
    deliver an effective, efficient, and satisfying user experience. Both automated
    and manual tests are useful and complementary to each other:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manual usability testing**: This involves real users interacting with the
    software to identify potential usability issues, understand user behavior, and
    gather qualitative feedback. Manual testing helps uncover problems that may not
    be detectable through automated testing, such as confusing navigation, unclear
    instructions, or subjective preferences. This human-centric approach provides
    valuable insights into how users perceive the software and identifies areas for
    improvement.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated usability testing**: This complements manual testing by using tools
    and scripts to simulate user interactions, validate user interface elements, and
    check for accessibility and responsiveness. Automated testing offers several advantages,
    including increased efficiency, speed, and coverage, as well as the ability to
    consistently test across multiple devices, platforms, and browsers. This helps
    with identifying usability issues that may not be apparent during manual testing,
    ensuring a consistent and high-quality user experience.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s consider the importance of retraining and updating the deployed
    model to ensure its continued effectiveness and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining and updating the deployed model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A retraining and updating strategy is crucial for maintaining the effectiveness
    of your DL model as it addresses the potential need for regular updates in response
    to changing data patterns. By periodically retraining your model on fresh, relevant
    data, you can ensure it stays current and continues to deliver accurate predictions.
    This not only helps maintain the model’s performance but also keeps it in tune
    with evolving trends and user requirements. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will delve deeper into the importance of
    retraining and updating, exploring its benefits and best practices to help you
    successfully implement this strategy in your DL deployment practically.
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance our DL deployment, we will explore the benefits of adopting
    a human-in-the-loop decision-making flow.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a human-in-the-loop decision-making flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating human-in-the-loop scenarios into your DL deployment can greatly
    enhance model performance and reliability, either as a permanent solution or by
    triggering alerts when certain conditions are met. By involving human experts
    in the decision-making process, you can bridge the gap between the model’s predictions
    and real-world complexities, allowing for more accurate and nuanced decisions.
    This collaborative approach enables continuous improvement by leveraging human
    expertise to validate, correct, and fine-tune the model’s output. Additionally,
    human-in-the-loop systems foster trust and accountability as users can be confident
    that complex or high-stakes decisions are not made solely by algorithms but are
    also supported by human judgment and oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will delve into the crucial role of model governance in overseeing
    and managing the overall DL deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: Model governance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Monitoring and governance play a pivotal role in ensuring the ongoing effectiveness
    and reliability of your deep learning deployment. By tracking various aspects
    of your model, such as data drift and concept drift monitoring, you can identify
    and address issues that may affect its performance over time. Data drift monitoring
    helps detect changes in the underlying data distribution, while concept drift
    monitoring focuses on shifts in the relationships between input features and target
    variables. Establishing a robust monitoring and governance framework enables you
    to proactively manage your model’s performance and maintain its accuracy in the
    face of evolving trends and conditions. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will explore these aspects in greater detail,
    along with other critical components of model monitoring and governance, to help
    you develop a comprehensive strategy for maintaining your DL deployment’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore the recommendations for model latency optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing model latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming you have chosen an ideal architecture, trained a model, extracted
    insights, selected the inference model compiler/acceleration framework, and selected
    the target hardware infrastructure and architecture for hosting your models, there
    are additional steps you can take to improve model latency at this stage. The
    following techniques can be employed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model pruning**: Remove unnecessary neurons or weights in the neural network
    without affecting the overall performance significantly. Pruning techniques include
    weight pruning, neuron pruning, and filter pruning. This can reduce model size
    and computational requirements, resulting in faster inference times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model quantization**: Reduce the precision of model parameters (for example,
    weights and biases) from 32-bit floating-point numbers to lower bit-width representations
    such as 16-bit or 8-bit integers. Quantization can accelerate model inference
    without significant loss in accuracy, especially when deploying DL models on hardware
    with limited computational resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model distillation**: Train a smaller, faster “student” model to mimic the
    behavior of a larger, slower “teacher” model. The student model learns from the
    teacher model’s outputs, achieving comparable performance with reduced complexity
    and faster inference times. This method was demonstrated in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model parallelism**: In model parallelism, different parts of a neural network
    are distributed across multiple devices or processors, allowing concurrent computation
    on different portions of the model. For huge models that cannot fit entirely within
    the memory of a single GPU, this method is an essential step. For models that
    have highly parallel operations, latency can be reduced significantly. Model parallelism
    can be achieved at various parallelism levels, such as layer-level, pipeline-level,
    or tensor-slicing level parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch inference**: Process multiple input samples simultaneously through
    batch processing, enabling the model to make better use of the underlying hardware,
    leading to faster overall inference times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore tools that abstract deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Tools that abstract deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are numerous tools and platforms available that help abstract the model
    deployment process, making it easier and more efficient to deploy machine learning
    models in various environments. Here’s an overview of some popular tools and platforms,
    including both open source and paid-for tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow serving (open source tool)**: A flexible, high-performance serving
    system for deploying TensorFlow models in a production environment that provides
    out-of-the-box support for model versioning, REST and gRPC APIs, and efficient
    model serving on GPUs and CPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TorchServe (open source tool)**: The PyTorch equivalent of TensorFlow Serving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Extended (TFX) (open source tool)**: An end-to-end platform for
    deploying, managing, and maintaining machine learning pipelines in production.
    TFX integrates with TensorFlow, TensorFlow Serving, and other tools to provide
    a seamless deployment experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow (open source tool)**: An open source platform that streamlines the
    end-to-end machine learning life cycle, including experimentation, reproducibility,
    deployment, and monitoring. It supports multiple languages and machine learning
    libraries, making it a versatile choice for diverse projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow (open source tool)**: A Kubernetes-integrated solution that’s designed
    to facilitate the creation, coordination, deployment, and execution of adaptable
    and transportable machine learning tasks. It simplifies the deployment process
    by providing a consistent and unified environment across different cloud providers
    and on-premises infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlit (open source tool)**: A Python library that enables developers
    to quickly build and deploy custom web applications for machine learning and data
    science projects. Streamlit simplifies the process of creating interactive web
    apps with minimal coding, making it easier to share and deploy models through
    web apps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NVIDIA Triton (open source tool)**: An open source tool that can be used
    to deploy DL models. It natively supports many frameworks, most notably TensorRT,
    Pytorch, ONNX Runtime, OpenVINO, and a general Python backend that allows you
    to wrap and run any DL framework and Python code. It provides predictions through
    HTTP REST APIs and the gRPC protocol. It also natively provides the Prometheus-compatible
    and standard time series performance metric logs, which can be subsequently used
    for model monitoring in the Grafana dashboard. It also allows us to configure
    custom metrics in its C API. Most relevantly, it eases multiple GPU utilization
    and GPU memory assignments. We will be exploring this tool practically in the
    next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure ML deployment (paid-for tool)**: Microsoft Azure’s machine learning
    service that simplifies model deployment in the cloud. It provides tools for managing,
    monitoring, and scaling deployed models, and supports popular frameworks such
    as TensorFlow and PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataRobot (paid-for tool)**: DataRobot is an automated machine learning platform
    that simplifies the process of building, deploying, and maintaining machine learning
    models. It provides a wide range of tools and features, including customization,
    model versioning, monitoring, and collaboration. We will be exploring the usage
    of this platform in [*Chapter 18*](B18187_18.xhtml#_idTextAnchor265), *Exploring
    the DataRobot* *AI Platform*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Vertex AI (paid-for tool)**: A managed machine learning platform from
    Google Cloud that streamlines the end-to-end machine learning workflow, including
    model training, deployment, and management. It integrates with TensorFlow, PyTorch,
    and other popular frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon SageMaker (paid-for tool)**: A fully managed machine learning service
    from AWS that allows developers to build, train, and deploy machine learning models
    quickly and easily. It supports multiple frameworks and provides tools for model
    versioning, monitoring, and scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools and platforms help simplify and streamline the model deployment
    process, enabling developers to efficiently deploy their machine learning models
    in various environments.
  prefs: []
  type: TYPE_NORMAL
- en: Successful DL deployment requires addressing key aspects such as safety, trust,
    reliability, and latency optimization while leveraging tools and platforms that
    simplify the process. By adhering to these recommendations and utilizing appropriate
    tools, developers can effectively deploy and manage their DL models in various
    environments, ensuring consistent and reliable performance.
  prefs: []
  type: TYPE_NORMAL
- en: It should be apparent by now that many trade-offs and criteria need to be evaluated
    and considered before you can make a DL deployment system component choice. However,
    if you don’t have access to a paid tool, have a DL model, and have access to a
    GPU machine that has enough RAM to host your model, three tools are a no-brainer
    to choose from. In the next section, we will dive into a topic that both reveals
    those three tools and practically uses them.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three tools are ONNX, TensorRT, and NVIDIA Triton Server. ONNX and TensorRT
    are meant to perform GPU-based inference acceleration, while NVIDIA Triton Server
    is meant to host HTTP or GRPC APIs. We will explore these three tools practically
    in this section. TensorRT is known to perform the best model optimization toward
    the GPU to speed up inference, while NVIDIA Triton Server is a battle-tested tool
    for hosting DP models that have compatibility with TensorRT natively. ONNX, on
    the other hand, is an intermediate framework in the setup, which we will use primarily
    to host the weight formats that are directly supported by TensorRT.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this practical tutorial, we will be deploying a Hugging Face-sourced language
    model that can be supported on most NVIDIA GPU devices. We will be converting
    our PyTorch-based language model from Hugging Face into ONNX weights, which will
    allow TensorRT to load the Hugging Face language model. Then, we will create the
    code and configuration required by the NVIDIA Triton Server framework to host
    the language model. NVIDIA Triton Server supports two ways of deploying a model,
    which is to deploy the DL model with its pre-processing and post-processing methods
    as a single pipeline all embedded into a Python class, and to deploy the DL model
    by logically separating the pipeline into separate components. *Figure 15**.1*
    depicts both approaches with a pipeline that requires two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Two approaches for configuring the model deployment with two
    models in the pipeline](img/B18187_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Two approaches for configuring the model deployment with two models
    in the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'An intuitive and straightforward way to deploy your DL model pipeline is to
    go with the first approach. However, the second approach, which involves breaking
    down and separating each component into its configuration, provides multiple benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modularity and reusability**: The modularity aspect allows the individual
    parts to be reused across different pipelines or projects. Additionally, it allows
    easier component swapping while maintaining other components in the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and flexibility**: This allows you to deploy different components
    to different GPUs and assign different instances of each component running at
    one time. Additionally, this method allows CPU-bound methods to not get tied to
    a GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism and latency reduction**: Native parallelism can be enabled through
    parallel branches instead of you needing to implement it in Python code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following two approaches as ways to organize a factory assembly
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first approach, the entire assembly process is combined into a single
    pipeline. This means that all components are processed and assembled sequentially
    in one integrated process. This can be easier to set up and manage, but it may
    not be as flexible or scalable as the second approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second approach, the assembly process is broken down into separate, modular
    components that can be individually managed and optimized. This allows for greater
    flexibility and scalability as each component can be fine-tuned or replaced without
    it affecting the entire pipeline. Additionally, this approach enables parallel
    processing, where multiple components can be processed simultaneously, potentially
    reducing overall latency and improving efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things simple, we will be demonstrating the first approach here.
  prefs: []
  type: TYPE_NORMAL
- en: Practically deploying a DL model with the single pipeline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, we will explore the process of deploying a DL model using
    ONNX, TensorRT, and NVIDIA Triton Server. While deploying the model using NVIDIA
    Triton Server, you may encounter issues related to model loading, configuration,
    or inference. Here are some troubleshooting tips:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the model files, configuration files, and other required files are
    in the correct locations and have the proper file permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the model configuration file (`config.pbtxt`) has the correct settings,
    such as input and output tensor names, data types, and dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the NVIDIA Triton Server logs for any error messages or warnings that
    could provide insights into the issue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the necessary dependencies, such as the DL framework, ONNX, and
    TensorRT, are installed and compatible with your system and hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start the practical tutorial in a step-by-step manner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the `transformer-deploy` repository by running the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, we can use the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'with torch.no_grad():'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: torch.onnx.export(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: model_pytorch,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: args=tuple(inputs_pytorch.values()),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f=output_path,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: opset_version=13,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: do_constant_folding=True,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: input_names=input_names,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: output_names=output_names,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dynamic_axes=dynamic_axis,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: training=TrainingMode.EVAL,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'of this process. This is what happens when we use the transformer-deploy tool:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: trt_engine = builder.build_serialized_network(network_def, config)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'engine: ICudaEngine = runtime.deserialize_cuda_engine(trt_engine)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'with open(engine_file_path, "wb") as f:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f.write(engine.serialize())
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will execute the command that will convert the chosen Hugging Face
    text generation model into a TensorRT serialized engine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of running the command in *step 4* is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will move the model into new folders at the following path:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it is time to create the code and configurations required to host this
    TensorRT model in NVIDIA Triton Server. Following approach 1 from *Figure 15**.1*,
    we need to define a Python class to initialize and perform inference using the
    Hugging Face tokenizer and the TensorRT engine. Let’s start by importing the necessary
    Python libraries into this deployment Python code file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must specify the model and path where we stored the serialized TensorRT
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must define the `TritonPythonModel` class interface, starting with
    the initialization method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The class name must be the same as the initialization method name – that is,
    `initialize`. This code loads the pre-trained tokenizer from the Hugging Face
    library methods and downloads the tokenizer from the internet. Note that for production
    deployment of a model, it is advised to have a managed instance of the tokenizer
    weights or any model weights somewhere to ensure a reliable deployment process.
    Additionally, the code loads the serialized TensorRT engine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to define the actual inference part of the tokenizer and model,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code should live under the Python code `model.py` file under the `models/transformer_tensorrt_text_generation/1/model.py`
    path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The folder named `1` is to symbolize the version of the `transformer_tensorrt_text_generation`
    model name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final file we need is a configuration file that specifies the name of the
    model, the max batch size of the model, the backend type of the model (in this
    case, Python), the input type, the name and dimensions of the model, the output
    type, the name and dimensions of the model, the number of instances of this pipeline,
    and finally whether to use GPU or CPU. The file needs to be named `config.pbtxt`.
    The content of this file for our usage is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should be stored under the following file path:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have all the code and configuration needed to run NVIDIA Triton Server
    and deploy our language model, which is an easy-to-use `nvidia-docker`-based deployment
    with a publicly available and downloadable image. The language model can be deployed
    on NVIDIA Triton Server by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import tritonclient.http as httpclient
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to specify the model name defined in the `config.pbtxt` file from
    *step 12*, along with the model version, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define the client using the `httpclient` helper tool, define the
    input data, configure the output data so that it’s obtained according to the `config.pbtxt`
    specified output name, and print the generated text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running `python triton_client.py` in the command line will return the following
    response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And with that, we are done with the practical tutorial!
  prefs: []
  type: TYPE_NORMAL
- en: This topic serves to show the minimal workflow needed to deploy a language model
    with acceleration with NVIDIA Triton Server, which is not too different from an
    audio DL model or a computer vision DL model. Try the workflow presented here
    using other language models and try to play around with all the settings! Note
    that there can be some issues with either the conversion or the optimization stage
    due to highly custom layers from new language models, so you will either need
    to work on fixing it in the base libraries themselves or raise it to the respective
    teams and wait for it to be upgraded.
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying specifically with language models, there are a few more tools
    that can be used for deployment that might be worth considering due to their high-level
    abstraction of the Hugging Face models and out-of-the-box official support of
    selected LLMs. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`baichuan-inc/Baichuan-7B`), BLOOM (`bigscience/bloom` and `bigscience/bloomz`),
    GPT-2 (`gpt2` and `gpt2-xl`), GPT BigCode (`bigcode/starcoder` and `bigcode/gpt_bigcode-santacoder`),
    GPT-J (`EleutherAI/gpt-j-6b` and `nomic-ai/gpt4all-j`), GPT-NeoX (`EleutherAI/gpt-neox-20b`,
    `databricks/dolly-v2-12b`, and `stabilityai/stablelm-tuned-alpha-7b`), LLaMA and
    LLaMA-2 (`meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`,
    and `openlm-research/open_llama_13b`), MPT (`mosaicml/mpt-7b` and `mosaicml/mpt-30b`),
    and OPT (`facebook/opt-66b` and `facebook/opt-iml-max-30b`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CTranslate2** (https://github.com/OpenNMT/CTranslate2): This boasts efficient
    inference with support of the following models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-decoder models**: Transformer base/big, M2M-100, NLLB, BART, mBART,
    Pegasus, T5, and Whisper'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder-only models**: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, CodeGen,
    GPTBigCode, and Falcon'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-only** **models**: BERT'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-generation-interface** (https://github.com/huggingface/text-generation-inference):
    This is not as efficient without an accelerator but it provides manually performed
    offline optimizations for the following models: BLOOM, FLAN-T5, Galactica, GPT-Neox,
    Llama, OPT, SantaCoder, Starcoder, Falcon 7B, Falcon 40B, MPT, and Llama V2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenLLM** ([https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)):
    This boasts integration with Langchain and Hugging Face agents but without using
    acceleration/compiler libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mlc-llm** ([https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)):
    This boasts support on a variety of devices, such as mobile phones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a final point, the practical deployment example presented here wouldn’t be
    as effective without us following the recommendations and guidelines presented
    in the previous topic, so be sure to follow through with every one of them!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the various aspects of deploying DL models in production
    environments, focusing on key components, requirements, and strategies. We discussed
    architectural choices, hardware infrastructure, model packaging, safety, trust,
    reliability, security, authentication, communication protocols, user interfaces,
    monitoring, and logging components, along with continuous integration and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also provided a step-by-step guide for choosing the right deployment
    options based on specific needs, such as latency, availability, scalability, cost,
    model hardware, data privacy, and safety requirements. We also explored general
    recommendations for ensuring model safety, trust, and reliability, optimizing
    model latency, and utilizing tools that simplify the deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: A practical tutorial on deploying a language model with ONNX, TensorRT, and
    NVIDIA Triton Server was presented, showcasing a minimal workflow needed for accelerated
    deployment using NVIDIA Triton Server.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding and implementing the strategies and best practices presented
    in this chapter, you can successfully deploy DL models in production with the
    most sensible choice for each component required and unlock their full potential.
    To build on this success path, we need to make sure we don’t forget about our
    model after we deploy it and always consider monitoring our deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the many aspects of monitoring that we
    need to consider to ensure the continued success of our machine learning use case.
  prefs: []
  type: TYPE_NORMAL
