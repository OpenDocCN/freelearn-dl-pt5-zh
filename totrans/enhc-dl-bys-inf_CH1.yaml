- en: Chapter 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian Inference in the Age of Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: Over the last fifteen years, **machine learning** (**ML**) has gone from a relatively
    little-known field to a buzzword in the tech community. This is due in no small
    part to the impressive feats of **neural networks** (**NNs**). Once a niche underdog
    in the field, **deep learning**’s accomplishments in almost every conceivable
    application have resulted in a near-meteoric rise in its popularity. Its success
    has been so pervasive that, rather than being impressed by features afforded by
    deep learning, we’ve come to *expect* them. From applying filters in social networking
    apps, through to relying on Google Translate when on vacation abroad, it’s undeniable
    that deep learning is now well and truly embedded in the technology landscape.
  prefs: []
  type: TYPE_NORMAL
- en: But, despite all of its impressive accomplishments, and the variety of products
    and features it’s afforded us, deep learning has not yet surmounted its final
    hurdle. As sophisticated neural networks are increasingly applied in mission-critical
    and safety-critical applications, the questions around their robustness become
    more and more pertinent. The black-box nature of many deep learning algorithms
    makes them daunting candidates for safety-savvy solutions architects - so much
    so that many would prefer sub-standard performance over the potential risks of
    an opaque system.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we conquer the apprehension surrounding deep learning and ensure
    that we create more robust, trustworthy models? While some of the answers to this
    lie down the path of **explainable artificial intelligence** (**XAI**), an important
    building block lies in the field of **Bayesian deep** **learning** (**BDL**).
    Through this book, you will discover the fundamental principles behind BDL through
    practical examples, allowing you to develop a strong understanding of the field,
    and equipping you with the knowledge and tools you need to build your own BDL
    models.
  prefs: []
  type: TYPE_NORMAL
- en: But, before we get started, let’s delve deeper into the justifications of BDL,
    and why typical deep learning methods may not be as robust as we’d like. In this
    chapter, we’ll learn about some of the key successes and failures of deep learning,
    and how BDL can help us to avoid the potentially tragic consequences of standard
    deep models. We’ll then outline the core topics of the rest of the book, before
    introducing you to the libraries and data that we’ll be using in practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'These topics will be covered in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Wonders of the deep learning age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the limitations of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the work environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.1 Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Wonders of the deep learning age
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last 10 to 15 years, we’ve seen a dramatic shift in the landscape of
    ML thanks to the enormous success of deep learning. Perhaps one of the most impressive
    feats of the universal impact of deep learning is that it has affected fields
    from medical imaging and manufacturing all the way through to tools for translation
    and content creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'While deep learning has only seen great success over recent years, many of
    its core principles are already well established. Researchers have been working
    with neural networks for some time – in fact, one could argue that the first neural
    network was introduced by Frank Rosenblatt as early as 1957! This, of course,
    wasn’t as sophisticated as the models we have today, but it was an important component
    of these models: the perceptron, as shown in *Figure* *1.1*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/perceptron_diagram.JPG)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Diagram of a single perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: 'The 1980s saw the introduction of many now-familiar concepts, with the introduction
    of **convolutional neural networks** (**CNNs**) by Kunihiko Fukushima in 1980,
    and the development of the **recurrent neural network** (**RNN**) by John Hopfield
    in 1982\. The 1980s and 1990s saw further maturation of these technologies: Yann
    LeCun famously applied back-propagation to create a CNN capable of recognizing
    hand-written digits in 1989, and the crucial concept of long short-term memory
    RNNs was introduced by Hochreiter and Schmidhuber in 1997.'
  prefs: []
  type: TYPE_NORMAL
- en: But, while we had the foundation of today’s powerful models before the turn
    of the century, it wasn’t until the introduction of modern GPUs that the field
    really took off. With the introduction of accelerated training and inference afforded
    by GPUs, it became possible to develop networks with dozens (or even hundreds)
    of layers. This opened the door to incredibly sophisticated neural network architectures
    capable of learning compact feature representations of complex, high-dimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Diagram of AlexNet'
  prefs: []
  type: TYPE_NORMAL
- en: One of the first highly influential network architectures was AlexNet. This
    network, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, comprised
    11 layers and was capable of classifying images into one of 1,000 possible classes.
    It achieved unprecedented performance on the ImageNet Large Scale Visual Recognition
    Challenge in 2012, illustrating the power of deep networks. AlexNet was the first
    of an array of influential neural network architectures, and the following years
    saw the introduction of many now-familiar architectures, including VGG Net, the
    Inception architectures, ResNet, EfficientNet, YOLO... The list goes on!
  prefs: []
  type: TYPE_NORMAL
- en: But NNs weren’t just successful in computer vision applications. In 2014, work
    by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio demonstrated that end-to-end
    NN models could be used to obtain state-of-the-art results in machine translation.
    This was a watershed moment for the field, and large-scale machine translation
    services quickly adopted these end-to-end networks, spurring further advancements
    in natural language processing. Fast-forward to today, and these concepts have
    matured to produce the **transformer** architecture – an architecture that has
    had a dramatic effect on deep learning through its ability to learn rich feature
    embeddings through self-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: With the impressive flexibility granted to them by the wide variety of architectures,
    neural networks have now achieved state-of-the-art performance in applications
    across almost every conceivable field, and they’re now a familiar part of our
    daily lives. Whether it’s the facial recognition we use on our mobile devices,
    translation services such as Google Translate, or speech recognition in our smart
    devices, it’s clear that these networks are not just competitive in image classification
    challenges, they’re now an important part of the technologies we’re developing,
    and they’re even capable of *outperforming* *humans*.
  prefs: []
  type: TYPE_NORMAL
- en: While reports of deep learning models outperforming human experts are becoming
    more and more frequent, the most profound examples are perhaps those in medical
    imaging. In 2020, a network developed by researchers at Imperial College London
    and Google Health outperformed six radiologists when detecting breast cancer from
    mammograms. A few months later, a study from February 2021 demonstrated that a
    deep learning model was able to outperform two human experts in diagnosing gallbladder
    disorders. Another study published later that year showed that a CNN outperformed
    157 dermatologists in detecting melanoma from images of skin abnormalities.
  prefs: []
  type: TYPE_NORMAL
- en: All of the applications we’ve discussed so far have been supervised applications
    of ML, in which models have been trained for classification or regression problems.
    However, some of the most impressive feats of deep learning are found in other
    applications, including generative modeling and reinforcement learning. Perhaps
    one of the most famous examples of the latter is **AlphaGo**, a reinforcement
    learning model developed by DeepMind. The algorithm, as indicated in its name,
    was trained to play the game Go via reinforcement learning. Unlike some games,
    such as chess, which can be solved via fairly straightforward artificial intelligence
    methods, Go is far more challenging from a computational standpoint. This is due
    to the sophisticated nature of the game – the many possible combinations of moves
    are difficult for more traditional approaches. Thus, when AlphaGo successfully
    beat Go champions Fan Hui and Lee Sedol, in 2015 and 2016 respectively, this was
    big news.
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind went on to further refine AlphaGo by creating a version which learned
    by playing games against itself – AlphaGo Zero. This model was superior to any
    previous model, achieving superhuman performance in Go. The algorithm at the core
    of its success, AlphaZero, went on to achieve superhuman performance in a range
    of other games, proving the algorithm’s ability to generalize to other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant milestone for deep learning over the last decade was the
    introduction of **Generative Adversarial Networks**, or **GANs**. GANs work by
    employing two networks. The goal of the first network is to generate data with
    the same statistical qualities as a training set. The goal of the second network
    is to classify the output of the first network, using what it has learned from
    the dataset. Because the first network is not trained directly on the data, it
    does not learn to simply replicate data – instead, it effectively learns to deceive
    the second network. This is why the term *adversarial* is used. Through this process,
    the first network is able to learn which kinds of outputs successfully deceive
    the second network, and thus is able to generate content that matches the data
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs can produce particularly impressive outputs. For example, the following
    image was generated by the StyleGAN2 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Face generated by StyleGAN2 from thispersondoesnotexist.com.'
  prefs: []
  type: TYPE_NORMAL
- en: But GANs aren’t just useful for generating realistic faces; they have practical
    applications in many other fields, such as suggesting molecular combinations for
    drug discovery. They are also a powerful tool for improving other ML methods through
    data augmentation – using the GAN-generated data to augment datasets.
  prefs: []
  type: TYPE_NORMAL
- en: All these successes may make deep learning seem infallible. While its achievements
    are impressive, they don’t tell the whole story. In the next section, we’ll learn
    about some of deep learning’s failings, and start to understand how Bayesian approaches
    may help to avoid these in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Understanding the limitations of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve seen, deep learning has achieved some remarkable feats, and it’s undeniable
    that it’s revolutionizing the way that we deal with data and predictive modeling.
    But deep learning’s short history also comprises darker tales: stories that bring
    with them crucial lessons for developing systems that are more robust, and, crucially,
    safer.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll introduce a couple of key cases in which deep learning
    failed, and we will discuss how a Bayesian perspective could have helped to produce
    a better outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Bias in deep learning systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll start with a textbook example of **bias**, a crucial problem faced by
    data-driven methods. This example centers around Amazon. Now a household name,
    the e-commerce company started out by revolutionizing the world of book retail,
    before becoming literally *the* one-stop shop for just about anything: from garden
    furniture to a new laptop, or even a home security system, if you can imagine
    it, you can probably purchase it on Amazon. The company has also been responsible
    for significant strides technologically, often as a means of improving its infrastructure
    in order to enable its expansion. From hardware infrastructure to theoretical
    and technological leaps in optimization methods, what started out as an e-commerce
    organization has now become one of the key figures in technology.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While these technological leaps often set the standard for the industry, this
    example did the opposite: demonstrating a key weakness of data-driven methods.
    The case we’re referring to is that of Amazon’s AI recruiting software. With automation
    playing such a key role in so much of Amazon’s success, it made sense to expand
    this automation to reviewing resumes. In 2014, Amazon’s ML engineers deployed
    a tool to do just that. Trained on the previous 10 year’s worth of applicants,
    the tool was designed to learn to identify favorable traits from the company’s
    enormous pool of applicants. However, in 2015 it became clear that it had latched
    onto certain features that resulted in deeply undesirable behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The issue was largely due to the underlying data: because of the nature of
    the tech industry at the time, Amazon’s dataset of resumes was dominated by male
    applicants. This resulted in tremendous inequity in the model’s predictions: it
    effectively learned to favor men, becoming hugely biased against female applicants.
    The discriminatory behavior of the model resulted in the project being abandoned
    by Amazon, and it now serves as a key example of bias for the AI community.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important factor to consider in the problem presented here is that this
    bias isn’t just driven by *explicit* information, such as a person’s name (which
    could be a clue as to their gender): algorithms learn latent information, which
    can then drive bias. This means the problem can’t simply be solved by anonymizing
    people – it’s up to the engineers and scientists to ensure that bias is evaluated
    comprehensively so that the algorithms we deploy are fair. While Bayesian methods
    can’t make bias disappear, they present us with a range of tools that can help
    with these problems. As we’ll see later in the book, Bayesian methods give us
    the ability to determine whether data is in-distribution or **out-of-distribution**
    (**OOD**). In this case, Amazon could have used this capability of Bayesian methods:
    separating the OOD data and analyzing it to understand why it was OOD. Was it
    picking up on things that were relevant, such as applicants with the wrong kind
    of experience? Or was it picking up on something irrelevant and discriminatory,
    such as the applicant’s gender? This could have helped Amazon’s ML team to spot
    the undesirable behavior early, allowing them to develop an unbiased solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 The danger of over-confident predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another widely referenced example of a deep learning failure is illustrated
    in the paper *Robust Physical-World Attacks on Deep Learning Visual Classification*
    by Kevin Eykholt *et al.* ( [https://arxiv.org/abs/1707.08945](https://arxiv.org/abs/1707.08945)).
    This paper played an important role in highlighting the issue of **adversarial
    attacks** on deep learning models: slightly modifying input data so that the model
    produces an incorrect prediction. In one of the key examples from their paper,
    they stick white and black stickers to a stop sign. While the modifications to
    the sign were subtle, the computer vision model interpreted the modified sign
    as a Speed Limit 45 sign.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/adversarial_illustration.PNG)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Illustration of the effect of a simple adversarial attack on a
    model interpreting a stop sign.'
  prefs: []
  type: TYPE_NORMAL
- en: At first, this may seem inconsequential, but if we take a step back and consider
    the amount of work that Tesla, Uber, and others have dedicated towards self-driving
    cars, it’s easy to see how this sort of adversarial perturbation could lead to
    catastrophic consequences. In the case of this sign, this misclassification could
    lead to a self-driving car bypassing a stop sign, hurtling into traffic at an
    intersection. This would obviously not be good for the passengers or other road
    users. In fact, an incident not too dissimilar to what we’re describing here happened
    in 2016 when a Tesla Model S collided with a truck in northern Florida ( [https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC](https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC)).
    According to Tesla, the trailer being pulled by the truck wasn’t detected by Tesla’s
    autopilot as it couldn’t distinguish it from the backdrop of the bright sky behind
    the trailer. The driver also didn’t notice the trailer, ultimately resulting in
    a fatal collision. But what if the decision processes used by the autopilot were
    more sophisticated? One of the key themes throughout this book is that of making
    *robust* decisions with our ML systems, particularly in mission-critical or safety-critical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: While this traffic sign example provides an intuitive illustration of the dangers
    associated with misclassifications, this applies to a vast range of other scenarios,
    from robotics equipment used for manufacturing through to automated surgical procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having some idea of confidence (or uncertainty) is an important step towards
    improving the robustness of these systems and ensuring consistently safe behavior.
    In the case of the stop sign, simply having a model that ”knows when it doesn’t
    know” can prevent potentially tragic outcomes. As we’ll see later in the book,
    BDL methods allow us to detect adversarial inputs through their uncertainty estimates.
    In our self-driving car example, this could be incorporated in the logic so that,
    if the model is uncertain, the car safely comes to a stop, switching to manual
    mode to allow the driver to safely navigate the situation. This is the *wisdom*
    that comes with uncertainty-aware models: allowing us to design models that know
    their limitations, and thus are more robust in unexpected scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Shifting trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our last examples look at the challenge of dealing with data that changes over
    time – a common problem in real-world applications. The first problem we’ll consider,
    typically referred to as **dataset shift** or **covariate shift**, occurs when
    the data encountered by a model at inference time changes relative to the data
    the model was trained on. This is often due to the dynamic nature of real-world
    problems and the fact that training sets – even very large training sets – rarely
    represent the total variation present in the phenomena they represent. An important
    example of this can be found in the paper *Systematic Review of Approaches to
    Preserve Machine Learning* *Performance in the Presence of Temporal Dataset Shift
    in Clinical Medicine*, in which Lin Lawrence Guo *et al.* highlight concerns around
    dataset shift ( [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/)).
    Their work shows that there is relatively little literature on tackling issues
    related to dataset shift in ML models applied in clinical settings. This is problematic
    because clinical data is dynamic. Let’s consider an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we have a model that’s trained to automatically prescribe medication
    for a patient given their symptoms. A patient complains to a physician about respiratory
    symptoms, and the physician uses the model to prescribe medication. Because of
    the data presented to the model, it prescribes antibiotics. This works for many
    patients for a while, but over time something changes: a new disease becomes prevalent
    in the population. The new disease happens to have very similar symptoms to the
    bacterial infection that was going around previously, but this is caused by a
    virus. Because the model isn’t capable of adapting to dataset shift, it continues
    recommending antibiotics. Not only will they not help the patients, but it could
    contribute to antibiotic resistance within the local population.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to be robust to these shifts in real-world data, models need to be
    sensitive to dataset shift. One way to do this is through the use of Bayesian
    methods, which provide uncertainty estimates. Applying this to our automatic prescriber
    example, the model becomes sensitive to small changes in the data when capable
    of producing uncertainty estimates. For example, there may be subtle differences
    in symptoms, such as a different type of cough, associated with our new viral
    infection. This will cause the uncertainty associated with the model predictions
    to rise, indicating that the model needs to be updated with new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A related issue, referred to as **catastrophic forgetting**, is caused by models
    adapting to changes in data. Given our example, this sounds like a good thing:
    if models are adapting to changes in the data, then they’re always up to date,
    right? Unfortunately, it’s not quite so simple. Catastrophic forgetting occurs
    when models learn from new data, but ”forget” about past data in the process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say an ML algorithm is developed to identify fraudulent documents.
    It may work very well at first, but fraudsters will quickly notice that methods
    that used to fool automated document verification no longer work, so they develop
    new methods. While a few of these methods get through, the model – using its uncertainty
    estimates – notices that it needs to adapt to the new data. The model updates
    its dataset, focusing on the current popular attack methods, and runs a few more
    training iterations. Once again, it successfully thwarts the fraudsters, but,
    much to the surprise of the model’s designers, the model has started letting through
    older, less sophisticated attacks: attacks that used to be easy for it to identify.'
  prefs: []
  type: TYPE_NORMAL
- en: In training on the new data, the model’s parameters have changed. Because there
    wasn’t sufficient support for the old data in the updated dataset, the model has
    lost information about old associations between the inputs (documents) and their
    classification (whether or not they’re fraudulent).
  prefs: []
  type: TYPE_NORMAL
- en: While this example used uncertainty estimates to tackle the issue of dataset
    shift, it could have further leveraged them to ensure that its dataset was balanced.
    This can be done using methods such as **uncertainty sampling**, which look to
    sample from uncertain regions, ensuring that the dataset used to train the model
    captures all available information from current and past data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Core topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aim of this book is to provide you with the tools and knowledge you need
    to develop your own BDL solutions. To this end, while we assume some familiarity
    with concepts of statistical learning and deep learning, we will still provide
    a refresher of these fundamental concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002),
    we’ll go over some of the key concepts from Bayesian inference, including probabilities
    and model uncertainty estimates. In [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003), we’ll cover important key aspects of
    deep learning, including learning via backpropagation, and popular varieties of
    NNs. With these fundamentals covered, we’ll start to explore BDL in [*Chapter 4*](CH4.xhtml#x1-490004),
    [*Introducing Bayesian Deep Learning*](CH4.xhtml#x1-490004). In *Chapters 5* and
    *6* we’ll delve deeper into BDL; we’ll first learn about principled methods, before
    going on to understand more practical methods for approximating Bayesian neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical Considerations for Bayesian
    Deep Learning*](CH7.xhtml#x1-1130007), we’ll explore some practical considerations
    for BDL, helping us to understand how best to apply these methods to real-world
    problems. By [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian* *Deep Learning*](CH8.xhtml#x1-1320008),
    we should have a strong understanding of the core BDL methods, and we’ll cement
    this with a number of practical examples. Finally, [*Chapter 9*](CH9.xhtml#x1-1780009),
    [*Next Steps in Bayesian Deep Learning*](CH9.xhtml#x1-1780009) will provide an
    overview of the current challenges within the field of BDL and give you an idea
    of where the technology is headed.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout most of the book, the theory will be accompanied by hands-on examples,
    allowing you to develop a strong understanding by implementing these methods yourself.
    In order to follow these coding examples, you will need to have a Python environment
    set up with the necessary prerequisites. We’ll go over these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Setting up the work environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete the practical elements of the book, you’ll need a Python 3.9 environment
    with the necessary prerequisites. We recommend using `conda`, a Python package
    manager specifically designed for scientific computing applications. To install
    `conda`, simply head to [https://conda.io/projects/conda/en/latest/user-guide/install/index.html](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)
    and follow the instructions for your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `conda` installed, you can set up the `conda` environment that you’ll
    use for the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When you hit *Enter* to execute this command, you’ll be asked if you wish to
    continue installing the required packages; simply type `y` and hit **Enter**.
    `conda` will now proceed to install the core packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now activate your environment by typing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll now see that your shell prompt contains `bdl`, indicating that your
    `conda` environment is active. Now you’re ready to install the prerequisites for
    the book. The key libraries required for the book are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**: Numerical Python, or NumPy, is the core package for numerical programming
    in Python. You’re likely very familiar with this already.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SciPy**: SciPy, or Scientific Python, provides the fundamental packages for
    scientific computing applications. The full scientific computing stack comprising
    SciPy, matplotlib, NumPy, and other libraries, is often referred to as the SciPy
    stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**: This is the core Python machine learning library. Built on
    the SciPy stack, it provides easy-to-use implementations of many popular ML methods.
    It also provides a substantial number of helper classes and functions for data
    loading and processing, which we’ll use throughout the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**: TensorFlow, along with PyTorch and JAX, is one of the popular
    Python deep learning frameworks. It provides the tools necessary for developing
    deep learning models, and it will provide the foundation for many of the programming
    examples throughout the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Probability**: Built on TensorFlow, TensorFlow Probability provides
    the tools necessary for working with probabilistic neural networks. We’ll be using
    this along with TensorFlow for many of the Bayesian neural network examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install the full list of dependencies required for the book, with your `conda`
    environment activated, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s summarize what we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we’ve revisited the successes of deep learning, renewing our
    understanding of its enormous potential, and its ubiquity within today’s technology.
    We’ve also explored some key examples of its shortcomings: scenarios in which
    deep learning has failed us, demonstrating the potential for catastrophic consequences.
    While BDL can’t eliminate these risks, it can allow us to build more robust ML
    systems that incorporate both the flexibility of deep learning and the caution
    of Bayesian inference.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll dive deeper into the latter as we cover some of the
    core concepts of Bayesian inference and probability, in preparation for our foray
    into BDL.
  prefs: []
  type: TYPE_NORMAL
