- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Practical Examples with Jina
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build simple real-world applications using Jina’s neural
    search framework. Building on the concepts we have learned in the previous chapters,
    we will now look at how to use Jina to create valuable applications.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about the practical aspects of the Jina framework and how you
    can leverage them to quickly build and deploy sophisticated search solutions.
    We will walk you through the code base of three different applications built on
    Jina, and see how the different components that you learned about in the previous
    chapter work in tandem to create a search application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following three examples in this chapter, which will get
    you started on the journey of building with Jina:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q/A chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fashion image search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this chapter, we aim to get you started by building practical examples
    to understand the potential of Jina’s neural search framework. It is a great stepping
    stone for venturing into the world of neural search for building state-of-the-art
    search solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the application code discussed in this chapter, clone the
    GitHub repository available at https://github.com/jina-ai/jina/tree/master/jina/helloworld.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the Q/A chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Q/A chatbot** is a pre-built example that comes with the Jina installation.
    To experience the power of Jina firsthand and quickly get started, you can run
    the Q/A chatbot example directly from the command line without even getting into
    the code. The Q/A chatbot uses the public Covid Q/A dataset (https://www.kaggle.com/datasets/xhlulu/covidqa)
    from Kaggle, which contains 418 Q/A pairs (https://www.kaggle.com/xhlulu/covidqa).
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these instructions to set up the development environment and run the
    Q/A chatbot example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to install the Jina library from the **Python Package Index**
    (**PyPI**) along with the required dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, simply type the following command to launch your app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After typing this command, you will see the following text on your **command-line
    interface** (**CLI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Q/A chatbot command line  ](img/Figure_6.01_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Q/A chatbot command line
  prefs: []
  type: TYPE_NORMAL
- en: If your screen displays the same text on the command line, it means you have
    successfully launched the Q/A chatbot example. Now, it’s time to open the **user
    interface** (**UI**) and play with the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: By default, a simple chat interface will open up, allowing you to chat with
    the Q/A chatbot. If the page doesn’t open up itself, you can open `index.xhtml`
    by going to `jina/helloworld/chatbot/static`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following web page either by default or after opening the
    `index.xhtml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Q/A chatbot interface ](img/Figure_6.02_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Q/A chatbot interface
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully launched the Q/A chatbot application; it’s time to play
    with it and have some fun. You can ask the chatbot for any Covid-related facts,
    figures, or queries and see the magic in action!
  prefs: []
  type: TYPE_NORMAL
- en: Navigating through the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now go through the logic behind the application and see how Jina’s framework
    ties all the components together to produce a functioning Q/A chatbot application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to see the code and understand the different components that work
    together to bring up this application after installing Jina, go to the chatbot
    directory by following the `jina/helloworld/chatbot` path. This is the main directory
    that contains the code for the chatbot example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the files that you will see within the chatbot directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.py`: This is the main entry point/brain of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_executors.py`: This file is responsible for all the backend processing.
    It includes the logic behind the application, which we call **executors** in Jina
    terminology. It hosts multiple executors to transform, encode, and index the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`static`: This folder hosts all the frontend code responsible for rendering
    the chatbot interface on the web browser that helps you interact with the chatbot
    application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will have a detailed look at the functioning of each of these files in the
    following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: app.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `app.py` file is the entry point of the example application. As soon as
    you type the `jina hello chatbot` command, the control goes to this file. It’s
    the main entry point for the application and performs all the major tasks of bringing
    up the application’s UI and running the backend code.
  prefs: []
  type: TYPE_NORMAL
- en: The `app.py` file performs the following tasks to ensure that multiple components
    work in tandem with each other to produce the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing it does is import the required executors from the `my_executors.py`
    file using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Both of these executors are derived from the base `Executor` class of Jina:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MyTransformer` executor is responsible for encoding and transforming the
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `MyIndexer` executor is used for indexing the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn about the functioning of both of these executors in detail when
    we talk about the `my_executors.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: '`Flow` allows you to add encoding and indexing in the form of executors, and
    in the chatbot example, we use the following executors. You can use the following
    code to create a flow and add these executors to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is one of the simple flows with just two executors. For a complex flow
    with many executors, Jina provides the functionality to distinguish each of the
    executors with distinct names (for example, by using the `name` parameter, you
    can give your executors some really cool names). It then allows you to visualize
    the flow to understand how your data flows through different components. Let’s
    visualize this flow by adding a single line to the existing code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will generate the following `SVG` file that visualizes
    the chatbot flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Chatbot flow ](img/Figure_6.3_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Chatbot flow
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to call our flow from the browser, it’s important to enable Cross-Origin
    Resource Sharing (https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) within
    Flow (`cors=True`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the flow ready, it’s time to dive into the `hello_world` function
    in the `app.py` file, which brings together everything from different sources
    and opens a query endpoint (a backend endpoint) for you to query and interact
    with the chatbot application:'
  prefs: []
  type: TYPE_NORMAL
- en: The `hello_world` function starts by creating a `workspace` directory to store
    the indexed data and ensures that the required dependencies are imported.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this example, we require two major dependencies/Python libraries: `torch`
    and `transformers`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the dependencies by using the following commands before we move forward
    with the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pip install torch`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install transformers`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After installing these dependencies, it’s time to continue with the `hello_world`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to download the data from Kaggle. For that, we will use the
    `download_data` function, which basically uses the `urllib` library to fetch and
    save the data from the given URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `urllib` module takes `url` and `filename` as the target and downloads
    the data. You can refer to the following code to see how we set the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Passing the target variable into the `download_data` function will download
    the data and save it as a `.csv` file in a random folder within the same working
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all the basic components required to index the data, we will use
    the dataset downloaded in the previous step and index it using the flow that we
    created previously. Indexing will follow this logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will use the `MyTransformer` executor to encode and transform the data by
    computing the corresponding embeddings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It will use the `MyIndexer` executor to index the data via the `/index` endpoint
    and open the `/search` endpoint to query and interact with the chatbot.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the code that indexes the data and creates a search endpoint
    to interact with the chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we open the flow and the dataset with a context manager
    and send the data in the form of a `''question'': ''text''` pair to the index
    endpoint. For this example, we will use the web browser to interact with the chatbot,
    which requires configuring and serving the flow on a specific port with the HTTP
    protocol using the `port_expose` parameter, so that the web browser can make requests
    to the flow. Toward the end, we will use `f.block()` to keep the flow open for
    search queries and to prevent it from exiting.'
  prefs: []
  type: TYPE_NORMAL
- en: my_executors.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other key component of the chatbot example is the `my_executors.py` file,
    which contains the logical elements of the application, also known as **executors**.
    It consists of two different executors, which we will discuss in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The MyTransformer executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `MyTransformer` executor performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It loads the pre-trained sentence transformer model from the `sentence-transformers`
    library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It takes in the user arguments and sets up the model parameters (such as `model
    name`/`path`) and `pooling strategy`, fetches the tokenizer corresponding to the
    model, and sets up the device to `cpu`/`gpu`, depending on the user’s preference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After setting up these parameters, it computes the embedding for the textual
    data and encodes textual data/question-answer as a key-value pair in the form
    of an embedding map.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoding is performed through a `sentence-transformers` model (`paraphrase-mpnet-base-v2`,
    by default). We get the text attributes of documents in batches and then compute
    embeddings, which we later set as the embedding attribute for each of the documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `MyTransformer` executor exposes only one endpoint, `encode`, which is called
    whenever we request the flow, either on a query or index. The endpoint creates
    embeddings for the indexed or query documents so the search endpoint can use similarity
    scores to determine the closest match for a given query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s look at a simplified version of the `encode` function for the `MyTransformer`
    executor that we have in the main chatbot application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The MyIndexer executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `MyIndexer` executor performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a document store (SQLite, in our case) that contains all the documents
    of `DocumentArray`. The look and feel of `DocumentArray` with an external store
    are almost the same as a regular in-memory `DocumentArray`, but it makes the process
    more efficient and allows faster retrieval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The executor exposes two endpoints: `index` and `search`. The `index` endpoint
    is responsible for taking in the documents and indexing them, while the `search`
    endpoint is responsible for traversing the indexed `DocumentArray` to find the
    relevant match for the user queries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `search` endpoint uses the `match` method (a built-in method associated
    with `DocumentArray`), which returns the top closest match for the query documents
    using cosine similarity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s look at a simplified version of code for the `MyIndexer` executor that
    we have in the main chatbot application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These two executors are the building blocks of the chatbot application, and
    combining them allows us to create an interactive and intelligent chatbot backend.
    To interact with the chatbot in the web browser via the UI, you can use the HTML
    template provided in the `static` folder. Running the application by default will
    open a web page with the chatbot UI; if it doesn’t, then you can open the `index.xhtml`
    file from the `static` folder.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at the code behind the Q/A chatbot application for
    the Covid-19 dataset. The application is a form of text-to-text search engine
    created using Jina’s framework. The same logic can be used to create a variety
    of text search applications depending on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how to extend the search capabilities for
    unstructured data types such as images, and see how Jina’s neural search makes
    it easy to build an image-to-image search engine using the fashion image search
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding fashion image search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fashion image search** is another pre-built example that comes with the Jina
    installation, which you can run just like the Q/A chatbot example directly from
    the comfort of your command line without even getting into the code.'
  prefs: []
  type: TYPE_NORMAL
- en: The fashion image search example uses the famous *Fashion-MNIST* dataset of
    Zalando’s article images (https://github.com/zalandoresearch/fashion-mnist) consisting
    of 60,000 training examples and 10,000 examples in the test set. Each example
    is a 28x28 grayscale image, associated with a label from 10 classes just like
    the original MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each training and test set example is assigned one of the following labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Label** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | T-shirt/Top |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Trouser |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Pullover |'
  prefs: []
  type: TYPE_TB
- en: '| **Label** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Dress |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Coat |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Sandal |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Shirt |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Sneaker |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bag |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Ankle boot |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Fashion dataset labels and description
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we installed the `jina[demo]` library from PyPI, which
    took care of all the dependencies required to run this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go to the command line and run the fashion image search example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After typing this command, you will see the following text on your CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Fashion image search command line  ](img/Figure_6.04_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Fashion image search command line
  prefs: []
  type: TYPE_NORMAL
- en: If your screen displays the same text on the command line, it means you have
    successfully launched the fashion image search example, so now it’s time to open
    the UI and play with the application.
  prefs: []
  type: TYPE_NORMAL
- en: By default, a simple web page will open up with a random sample of images from
    the test set as queries, along with the retrieved results from the training data.
    Behind the scenes, Jina downloads the *Fashion-MNIST* dataset and indexes 60,000
    training images via the indexing flow. After that, it selects randomly sampled
    unseen images from the test set as queries and asks Jina to retrieve relevant
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the page doesn’t open up itself, you can open the `demo.xhtml` file present
    at the `*/demo.xhtml` path. You will see the following web page either by default
    or after opening the downloaded `demo.xhtml` file manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Fashion image search web interface  ](img/Figure_6.05_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Fashion image search web interface
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the preceding figure how Jina does an amazing job in finding
    the relevant search results for the image queries selected randomly from the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating through the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now go through the logic behind the app and see how Jina’s framework ties
    all the components together to create an image search application.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Jina, go to the chatbot directory by following the `jina/helloworld/fashion`
    path. This is the main directory that contains the code for the fashion image
    search example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the files that you will see within the fashion directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.py`: Similar to the application discussed in the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_executors.py`: Similar to the application discussed in the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helper.py`: This consists of the supplementary logic functions to modularize
    the logical code blocks and keep them in a separate file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`demo.xhtml`: This hosts all the frontend code responsible for rendering the
    chatbot interface on the web browser, which helps you interact with the chatbot
    application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: app.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `app.py` file is the entry point of the example application; as soon as
    you type the `jina hello fashion` command, the control goes to this file. This
    is the main entry point for the application and performs all the major tasks to
    bring up the application’s frontend and the backend.
  prefs: []
  type: TYPE_NORMAL
- en: The `app.py` file performs the following tasks to ensure that multiple components
    work in tandem with each other to produce the desired application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing it does is import the required executors from the `my_executors.py`
    file using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All of these executors are derived from the base `Executor` class of Jina:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MyEncoder` is responsible for transforming and encoding the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MyIndexer` is used for indexing the data; after indexing, it hosts a `/search`
    endpoint for querying the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn about the functioning of all these executors in detail when we
    talk about the `my_executors.py` file. The flow for this example consists of the
    aforementioned executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following code to create and visualize the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code will generate the following flow diagram, which shows how
    the data moves through different components of the applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Fashion image search flow ](img/Figure_6.6_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Fashion image search flow
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, the `replicas` parameter is set to `2` for the `MyEncoder`
    executor to divide the input data stream into two different executors for faster
    processing and encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the flow ready, it’s time to dive into the `hello_world` function
    in the `app.py` files, which brings together everything from different sources.
    The `hello_world` function performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It creates a `workspace` directory in the root folder to store the indexed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It creates a `targets` dictionary to associate the URL of the data with the
    local filenames where the data will be saved. It saves the training data under
    the `index` and `index-label` files, and the test data under the `query` and `query-label`
    files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After that, it passes the `targets` variable to the `download_data` function
    and downloads the *Fashion-MNIST* dataset. The `download_data` function uses the
    `urllib` package to download the data from the given URL and iterate through the
    dictionary to save the data and the labels for the training and the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It creates the flow and adds the `MyEncoder` and `MyIndexer` executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It opens the flow with the context manager and uses the indexing flow to index
    the data by creating the embeddings for all the images in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then includes the ground truth (labels) along with the query images, which
    allows us to evaluate the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After indexing the data, it calls the `search` function, which randomly samples
    128 unseen images as queries and returns the top 50 similar images for each of
    the query images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we use the `write_html` function to render the frontend in the web
    browser using the `demo.xhtml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: my_executors.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other key component of the fashion image search example is the `my_executors.py`
    file. It consists of three different executors that work together in the flow
    to create an end-to-end application experience.
  prefs: []
  type: TYPE_NORMAL
- en: The MyEncoder executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `MyEncoder` executor performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: It is used in both indexing and the querying flow. It is fed with the index
    and query data yielded from the respective generator functions. It uses **singular
    value decomposition** (**SVD**) to encode the incoming data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the constructor, it creates a random matrix of shape (`784,64`) and applies
    SVD to get `oth_mat`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `encode` function, it fetches the content from the docs array (`DocumentArray`
    in Jina), stacks images together, extracts the single-channel content, and reshapes
    images to make it ready to fetch the embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we use the `content` matrix along with `oth_mat` (the result
    of SVD) to get the embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then associates each document tensor with the respective embeddings and converts
    the tensor into a **uniform resource identifier** (**URI**) (a long string that
    is an equivalent representation of an image) for standardized representation and
    then it pops the tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It repeats the same process for all the images in the loop to encode the entire
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The MyIndexer executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `MyIndexer` executor performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Its constructor creates a `workspace` directory to store the indexed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It hosts an `index` endpoint, which takes in the documents as input and structures
    them into the `workspace` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It also hosts the `search` endpoint, which gives out the best matches for a
    given query. It takes in the document and `top-k` as a parameter and performs
    a cosine similarity match to find the `top-k` results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: helper.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `helper.py` file provides the helper functions to support the logical elements
    in the `app.py` file. It implements key functions such as `index_generator` and
    `query_generator`, which we use in the `app.py` file to index and query the data.
    Let’s go through both of these functions and understand what they do.
  prefs: []
  type: TYPE_NORMAL
- en: index_generator()
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This function generates the index tag for the training data using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: This generator will iterate over all 60,000 documents (images) and process each
    one individually to make them index-ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It fetches the 28x28 images from the dictionary and inverts them to make them
    suitable to be displayed on the web browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It converts the black and white image into an RGB image and then converts the
    image into Jina’s internal data type, `Document`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then associates a tag ID with the document and yields it as the index data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the code for the `index_generator()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: query_generator()
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is similar to the `index_generator` function and follows the same logic
    to generate the query data with some modifications. It fetches a random number
    of documents (based on the value of the `num_docs` parameter) from the dataset
    to generate the query data. The following is the code for the `query_generator()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: demo.xhtml
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To view the query results in the web browser, the application uses the `demo.xhtml`
    file to render the frontend. By default, running the application will open a web
    page with the query images along with the search results; if it doesn’t, then
    you can open the `demo.xhtml` file, which will be available in the random folder
    generated at the start.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how Jina’s framework makes it really efficient to build
    search applications for image data types by leveraging state-of-the-art deep learning
    models. The same functionality will be extended to other data types, such as audio,
    video, and even 3D mesh, which you will learn about in [*Chapter 7*](B17488_07.xhtml#_idTextAnchor101),
    *Exploring Advanced Use Cases of Jina*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how to combine two data types to create a multimodal search
    that can easily elevate the search experience for your product or platform. We
    will dive into the multimodal search example, which uses the *people-image* dataset
    consisting of *image-caption* pairs to build a search application that lets you
    query using both the image and the text.
  prefs: []
  type: TYPE_NORMAL
- en: Working with multimodal search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multimodal search is another pre-built example that comes with the Jina installation,
    which you can run directly from the comfort of your command line without even
    getting into the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example uses Kaggle’s public people image dataset (https://www.kaggle.com/ahmadahmadzada/images2000),
    which consists of 2,000 image-caption pairs. The data type here is a multimodal
    document containing multiple data types such as a PDF document that contains text
    and images together. Jina lets you build the search for multimodal data types
    with the same ease and comfort:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this example from the command line, you need to install the following
    dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**pip install transformers**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pip install torch**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pip install torchvision**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pip install “jina[demo]”**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once all the dependencies are installed, simply type the following command
    to launch the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After typing this command, you will see the following text on your CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Multimodal search command line  ](img/Figure_6.07_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Multimodal search command line
  prefs: []
  type: TYPE_NORMAL
- en: If your screen displays the same text on the command line, it means you have
    successfully launched the Jina multimodal example; now, it’s time to open the
    UI and play with the application.
  prefs: []
  type: TYPE_NORMAL
- en: By default, a UI with a query and results section will open up, allowing you
    to query with text and image and get the results in the same form. If the page
    doesn’t open up itself, you can open the `index.xhtml` file by going to `jina/helloworld/multimodal/static`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following web page either by default or after opening the
    `index.xhtml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Multimodal search interface ](img/Figure_6.08_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Multimodal search interface
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully launched the multimodal example application; it’s now
    time to play with it and have some fun.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating through the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now go through the logic behind the app and see how Jina’s framework ties
    all the components together to produce a functioning multimodal search application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you install Jina, go to the chatbot directory by following the `jina/helloworld/multimodal`
    path. This is the main directory and contains the code for the multimodal search
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the files that you will see within the multimodal directory.
    We will go through the functioning of each of them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.py`: Similar to the previous applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_executors.py`: Similar to the previous applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `static` folder: This hosts all the frontend code responsible for rendering
    the UI on the web browser, which helps you interact with the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flow_index.yml`: This contains the YAML code for the indexing flow, which
    is run when we index the data for the first time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flow_search.yml`: This contains the YAML code for the search flow, which runs
    every time we send any query to the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This application uses the MobileNet and MPNet models to index the image-caption
    pairs. The indexing process takes about 3 minutes on the CPU. Then, it opens a
    web page where you can query the multimodal documents. We have also prepared a
    YouTube video (https://youtu.be/B_nH8GCmBfc) to walk you through this demo.
  prefs: []
  type: TYPE_NORMAL
- en: app.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you type the `jina hello multimodal` command, the control of the application
    goes to the `app.py` file. The `app.py` file performs the following tasks to ensure
    that all the components of the multimodal search application work in tandem with
    each other to produce the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing it does is import the required libraries. After that, the control
    goes to the `hello_world()` function, which hosts the main logic of the script.
    The `hello_world()` function creates a random directory using the `mkdir` command
    to store the artifacts, such as the downloaded data. Then, it checks to ensure
    that all the required Python libraries are installed and imported.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this example, we require three major dependencies/Python libraries:
    `torch`, `transformers`, and `torchvision`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the steps to understand the functioning of `app.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: Please check that all the aforementioned dependencies are installed correctly
    in your Python environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After checking that these dependencies are correctly installed, the `hello_world()`
    function calls the `download_data()` function to fetch and download the data from
    Kaggle. The `download_data()` function uses the `urllib` package to fetch and
    save the data from the given URL. `urllib` takes the URL and filename as the targets
    and downloads the data. You can refer to the following code to see how we set
    the targets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Passing the `targets` variable into the `download_data()` function will download
    the data and save it in the random folder created at the beginning of the `hello_world`
    function. It then loads the indexing flow from the YAML file and passes the image
    metadata to the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, it then loads the search flow from the YAML file and sets it to
    fetch the input queries from the HTML frontend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In both of the preceding code snippets, we open the flow with a context manager.
    For this example, we will use the web browser to interact with the application.
    It requires configuring and serving the flow on a specific port with the HTTP
    protocol using the `port_expose` parameter. Toward the end, we use the `f.block()`
    method to keep the flow open for search queries and to prevent it from exiting.
  prefs: []
  type: TYPE_NORMAL
- en: my_executors.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If `app.py` is the brain of this example, then the `my_executors.py` file contains
    the neurons in the form of executors that power the core logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multimodal example contains two modalities of data: image and text, which
    are stored in the document `tags` and `uri` attributes, respectively. To process
    these two modalities of data, at index time, we need to preprocess, encode, and
    index them separately using the following executors.'
  prefs: []
  type: TYPE_NORMAL
- en: The Segmenter executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Segmenter` executor takes the documents as the input and splits them into
    two chunks: image chunk and text chunk. The text chunk will contain the plain
    text data and the image chunk (which we call `chunk_uri` in the code) contains
    the URI of the image. Then, we add them both to the document chunks and send them
    further to the pre-processing stage, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The TextCrafter executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the preprocessing of the text chunk, we use the `TextCrafter` executor,
    which takes the text chunk as the input and returns a flattened traversable sequence
    of all the documents, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The ImageCrafter executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similarly, for the preprocessing of the image chunk, we use the `ImageCrafter`
    executor, which takes the image chunk as the input and returns a flattened traversable
    sequence of all the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The TextEncoder executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After the preprocessing step, the preprocessed data of the text chunk goes
    to the `TextEncoder` executor as the input and produces the text embedding as
    the output. We persist the result in the form of embeddings using the `DocVectorIndexer`
    executor. Let’s look at the functioning of `TextEncoder` by starting with the
    code of its constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To compute the embeddings, it uses the pre-trained `sentence-transformers/paraphrase-mpnet-base-v2`
    model with the `''mean''` pooling strategy. Let’s look at the code for the `compute_embedding()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It then uses the `encode()` function to store the embeddings in the `doc.embeddings`
    attribute of the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The ImageEncoder executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similarly, the preprocessed data of the image chunk goes to the `ImageEncoder`
    executor as the input and produces the embedding as the output. We persist the
    result in the form of embeddings using the `DocVectorIndexer` executor. Let’s
    look at the functioning of `ImageEncoder` by going through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'It uses the pre-trained `mobilenet -v2` model to generate the embeddings. To
    preprocess the images, it uses the `''mean''` pooling strategy to compute the
    average value of all the pixels in the image to compute the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Toward the end, the `encode` function stores the embeddings in the `doc.embeddings`
    attribute of the document.
  prefs: []
  type: TYPE_NORMAL
- en: The DocVectorIndexer executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s look at the `DocVectorIndexer` executor, which persists the encoding
    from both `TextEncoder` and `ImageEncoder` to index them. Here, we have two different
    modalities of data (text and image), so we need to store the indexed results separately
    in two different files. The `DocVectorIndexer` executor takes care of that. It
    stores the indexed text embeddings into the `text.json` file and the image embeddings
    into the `image.json` file, which we will use in the `flow_index.yml` file as
    `index_file_name`. Let’s look at the code for `DocVectorIndexer` to understand
    its functioning in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: It uses `DocumentArray` to store all the documents directly on the disk because
    we have a large number of documents. It hosts two different endpoints to index
    the data and open the `'search'` flow. It uses the cosine similarity score to
    find the relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: The KeyValueIndexer executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Apart from `DocVectorIndexer` to persist embeddings, we also create a `KeyValueIndexer`
    executor to help the chunks (text chunk and image chunk) to find their parent/root
    document. Let’s look at the code to understand its functionality in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: It uses `DocumentArray` just like `DocVectorIndexer` to store all the documents
    directly on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: It hosts two different endpoints to index the data and open the search flow.
    In the search logic, given a document, it loops through the tree to find its root/parent
    document.
  prefs: []
  type: TYPE_NORMAL
- en: The WeightedRanker executor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Toward the end, when both the chunks find their parents, we aggregate the score
    using the `WeightedRanker` executor to produce the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the code to understand its functionality in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It opens a search endpoint to combine the results from both the text and image
    chunks to calculate the final similarity score, which we will use to determine
    the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can assign the `weight` parameter beforehand to determine which modality
    (between text and image) will contribute more toward calculating the final relevance
    score. If you set the weight of the text chunk as `2` and the image chunk as `1`,
    then the text chunk will contribute a higher score to the final relevance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final similarity score is calculated by summing up cosine similarity *
    weight for both the modalities and then sorting them in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have looked at how the executors work together to produce the results. Let’s
    now look at how these executors are arranged and utilized in the index and search
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: flow_index.yml
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you already know, Jina provides two ways to create and work with the flows.
    The first is by using native Python, and the second is by using a YAML file to
    create a flow and call it in the main `app.py` file. Now, we will look at how
    the `flow_index.yml` file is created by leveraging the individual executor components
    that we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `flow_index.yml` file uses different executors that we have defined in
    the `my_executors.py` file and arranges them to produce the indexing flow. Let’s
    go through the YAML code to understand it in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It starts with the `Segmenter` executor, which segments the document into text
    and image chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we have two different pipelines, one for text and the other for
    the image. The text pipeline preprocesses the data using the `TextCrafter` executor,
    encodes it using the `TextEncoder` executor, and then indexes it using `DocVectorIndexer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The image pipeline preprocesses the data using the `ImageCrafter` executor,
    encodes it using the `ImageEncoder` executor, and then indexes it using `DocVectorIndexer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After indexing the text and image to the respective `text.json` and `image.json`
    files, we join both the indexers with `KeyValueIndexer` to link them together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: flow_search.yml
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the `flow_index.yml` file, we also have a `flow_search.yml` file,
    which defines the search/query flow for the multimodal example application. Let’s
    look at the YAML code to understand its functionality in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It gets the input in the form of text and images and treats them both differently
    using a pipeline of executors. For the text input, it uses the `TextCrafter` executor
    to preprocess the data, followed by the `TextEncoder` executor to encode the textual
    data, and finally, indexes it using `DocVectorIndexer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the image input, it uses the `ImageCrafter` executor to preprocess the
    data, followed by the `ImageEncoder` executor to encode the image data, and finally,
    indexes it using `DocVectorIndexer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It then passes the result of both `TextIndexer` and `ImageIndexer` into the
    `WeightedRanker` executor, which calculates the final relevance score and produces
    the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To interact with the multimodal application in the web browser via the UI, you
    can use the `index.xhtml` file provided in the `static` folder. Running the application
    should open the HTML file by default, but if it doesn’t, then you can open the
    `index.xhtml` file from the `static` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered how to put together all the components and
    concepts that we have learned in the previous chapters. We have walked you through
    the process of building basic search examples with Jina for different data types,
    including a text-to-text search, image-to-image search, and multimodal search,
    which combines both the text and the images. The things that we learned in this
    chapter will act as a building block for [*Chapter 7*](B17488_07.xhtml#_idTextAnchor101),
    *Exploring Advanced Use Cases of Jina*, where you will learn about building advanced
    examples using Jina.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue on the same journey and see how to build
    advanced search applications with Jina using what we have learned so far.
  prefs: []
  type: TYPE_NORMAL
