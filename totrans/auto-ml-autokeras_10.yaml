- en: '*Chapter 8*: Topic Classification Using AutoKeras'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, we need to categorize some specific text, such as a product or movie
    review, into one or more categories by assigning tags or topics. Topic classification
    is a supervised machine learning technique that does exactly this job: predicting
    which categories a given text belongs to. Being a supervised model, it needs to
    be trained with a set of already categorized train data, along with the texts
    and the categories that each one belongs to.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will be mainly practical since we laid the foundations for text-based
    tasks in previous chapters. By the end of this chapter, you will have learned
    how to create a topic classifier with AutoKeras, as well as how to apply it to
    any topic or category-based dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding topic classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a topic classifier with AutoKeras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the model search space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let's look at the technical requirements for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples in this book are available as Jupyter notebooks that can
    be downloaded from [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).
  prefs: []
  type: TYPE_NORMAL
- en: Since the code cells can be executed, each notebook can be self-installed; simply
    add a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook, there is a code cell for environment setup that installs AutoKeras
    and its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to run the code examples for this chapter, all you need is a computer with
    Ubuntu Linux as its OS and the Jupyter Notebook installed, which you can do with
    the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can run these notebooks using Google Colaboratory, in which
    case you will only need a web browser. See [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *AutoKeras with Google Colaboratory*, for more details. Furthermore, in the *Installing
    AutoKeras* section of that chapter, you will find other installation options.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's put what we've learned into practice by looking at some practical
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding topic classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw a small example of topic classification in [*Chapter 5*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077),
    *Text Classification and Regression Using AutoKeras*, with the example of the
    spam classifier. In that case, we predicted a category (spam/no spam) from the
    content of an email. In this section, we will use a similar text classifier to
    categorize each article in its corresponding topic. By doing this, we will obtain
    a model that determines which topics (categories) correspond to each news item.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say our model has input the following title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the `weather` and `sports` topics, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Workflow of a news topic classifier](img/B16953_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Workflow of a news topic classifier
  prefs: []
  type: TYPE_NORMAL
- en: The previous diagram shows a simplified version of a topic classifier pipeline.
    The raw text is processed by the classifier and the output will be one or more
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will apply a text classifier to a Reuters newswire
    dataset to put every article into one or more of the 46 categories. Most of the
    concepts of text classification were already explained in [*Chapter 5*](B16953_05_Final_PG_ePub.xhtml#_idTextAnchor077),
    *Text Classification and Regression Using AutoKeras*, so in this chapter, we will
    simply review some of them in a practical way by implementing the topic classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a news topic classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we are going to create will classify news from the Reuters newswire
    classification dataset. It will read the raw text of each news item and classify
    it into sections, assigning a label corresponding to the section that they belong
    to (Sports, Weather, Travel, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Reuters newswire is a dataset that contains 11,228 newswires from Reuters, labeled
    over 46 topics.
  prefs: []
  type: TYPE_NORMAL
- en: The text of each news item is encoded as a list of word indexes. These are integers
    that are indexed by frequency in the dataset. So, here, integer *1* encodes the
    first most frequent word in the data, *2* encodes the second most frequent, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook that contains the complete source code can be found at [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter08/Chapter8_Reuters.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at the relevant cells of the notebook in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Installing AutoKeras**: As we''ve mentioned in previous chapters, this snippet
    at the top of the notebook is responsible for installing AutoKeras and its dependencies
    using the pip package manager:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`tensorflow`, the built-in Keras Reuters dataset, into memory, as well as `numpy`
    and `AutoKeras`, as the necessary dependencies for this project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`reuters_raw` function. Have a look at the code in the notebook for more details:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Visualizing the dataset samples**: Next, we can print some words from the
    first sample to have an idea of what it contains:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at the distribution of the most frequent words in a word cloud.
    A word cloud (also known as a tag cloud) is a text-based data visualization technique
    where words are displayed in different sizes based on how often they appear in
    the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A word cloud of the newswire dataset](img/B16953_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – A word cloud of the newswire dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's create the newswire classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will use the AutoKeras `TextClassifier` to find the best classification
    model. Just for this example, we will set `max_trials` (the maximum number of
    different Keras models to try) to 2\. We will not set the epochs parameter; instead,
    we will define an `EarlyStopping` callback of `2` epochs. We''re doing this so
    that the training process stops if the validation loss does not improve in two
    consecutive epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the training process to search for the optimal classifier for the
    training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Notebook output of text classifier training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Notebook output of text classifier training
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the accuracy of the training dataset is increasing.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we achieved a 0.965 loss value in the validation set. This is
    a really good number just for 1 minute of training. We have limited the search
    to two architectures (`max_trials = 2`). Increasing this number would give us
    a more accurate model, although it would also take longer to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, it''s time to evaluate the best model with the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, 0.77 (77%) is a good final prediction score for the training
    time we've invested (less than a couple of minutes).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at a little summary of the architecture for the best generated
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Best model architecture summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_08_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Best model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, AutoKeras has chosen a convolution model (Conv1D) to perform
    this task. As we explained at the beginning of this chapter, this kind of architecture
    works great when the order of the elements in the sequence is not important for
    the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a visual representation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Best model architecture visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Best model architecture visualization
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you already know, generating models and choosing the best one is done by
    AutoKeras automatically, but let's explain these blocks in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Each block represents a layer and the output of each is connected to the input
    of the next except for the first block, whose input is the text, and the last
    block, whose output is the predicted number. The blocks before Conv1D are all
    data preprocessing blocks and they are in charge of vectorizing the text generating
    embeddings to feed this Conv1D block, as well as reducing the dimension of the
    filters through the max pooling layer. Notice that AutoKeras has also added several
    dropout blocks to reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the model search space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can customize the model's search to restrict the search space by using `AutoModel`
    instead of `TextClassifier`, for example, by setting `TextBlock` for some specific
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we''re telling AutoKeras to only generate models
    that use `''ngram''` to vectorize the sentences. Remember that if we do not specify
    any of these arguments, AutoKeras will automatically try all the possible combinations
    until the number reaches the `max_trial` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's summarize what we've learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to solve a topic classification task by implementing
    a high-performance text classifier that categorizes news articles in just a few
    lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've laid the groundwork for working with text, we're ready to move
    on to the next chapter, where you'll learn how to handle multimodal and multitasking
    data using AutoKeras.
  prefs: []
  type: TYPE_NORMAL
