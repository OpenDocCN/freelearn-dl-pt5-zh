<html><head></head><body>
		<div id="_idContainer077">
			<h1 id="_idParaDest-94"><em class="italic"><a id="_idTextAnchor096"/>Chapter 6</em>: Working with Structured Data Using AutoKeras  </h1>
			<p>In this chapter, we will focus on using AutoKeras to work with structured data, also known as tabular data. We will learn how to explore this type of dataset and what techniques to apply to solve problems based on this data source. </p>
			<p>Once you've completed this chapter, you will be able to explore a structured dataset, transform it, and use it as a data source for specific models, as well as create your own classification and regression models to solve tasks based on structured data.</p>
			<p>Specifically, in this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding structured data</li>
				<li>Working with structured data</li>
				<li> Creating a structured data classifier to predict Titanic survivors</li>
				<li> Creating a structured data regressor to predict Boston house prices</li>
			</ul>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor097"/>Technical requirements</h1>
			<p>All the coding examples in this book are available as Jupyter notebooks that can be downloaded from this book's GitHub repository: <a href="https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_HousingPricePredictor.ipynb">https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_HousingPricePredictor.ipynb</a>.</p>
			<p>Since code cells can be executed, each notebook can be self-installed, so you can add a code snippet with the requirements you need. For this reason, at the beginning of each notebook, there is a code cell for environment setup, which installs AutoKeras and its dependencies.</p>
			<p>So, to run the coding examples in this book, you only need a computer with Ubuntu Linux as your OS and to install the respective Jupyter notebook with the following code:</p>
			<p class="source-code">$ apt-get install python3-pip jupyter-notebook</p>
			<p>Alternatively, you can also run these notebooks using Google Colaboratory. In that case, you will only need a web browser. For further details, see the <em class="italic">AutoKeras with Google Colaboratory</em> section of <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>. Furthermore, in the <em class="italic">Installing AutoKeras</em> section of that chapter, you will find other installation options.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor098"/>Understanding structured data</h1>
			<p>Structured data <a id="_idIndexMarker269"/>is basically tabular data; that is, data represented by rows and columns of a database. These tables contain two types of structured data, as follows:</p>
			<ul>
				<li><strong class="bold">Numerical data</strong>: This is<a id="_idIndexMarker270"/> data that is expressed on a numerical scale. Furthermore, it is <a id="_idIndexMarker271"/>represented in two ways, as follows:<p>    a. <strong class="bold">Continuous</strong>: Data that <a id="_idIndexMarker272"/>can take any value in an interval, such as <a id="_idIndexMarker273"/>temperature, speed, height, and so on. For example, a person's height could be any value (within the range of human heights), not just certain fixed heights.</p><p>    b. <strong class="bold">Discrete</strong>: Data that can <a id="_idIndexMarker274"/>take only non-divisible integer values, such <a id="_idIndexMarker275"/>as counters. Examples include the amount of money in a bank account, the population of a country, and so on.</p></li>
				<li><strong class="bold">Categorical data</strong>: This is<a id="_idIndexMarker276"/> data that can take only a specific set of values <a id="_idIndexMarker277"/>corresponding to possible categories. In turn, they are divided into the following categories:<p>    a. <strong class="bold">Binary</strong>: Data that<a id="_idIndexMarker278"/> can only <a id="_idIndexMarker279"/>accept two values (0/1)</p><p>    b. <strong class="bold">Ordinal</strong>: Data that <a id="_idIndexMarker280"/>has an explicit order, such as the days of the <a id="_idIndexMarker281"/>week</p></li>
			</ul>
			<p>It is necessary to know the data type of each feature so that you can apply the appropriate preprocessing methods. For example, if one of the columns in a DataFrame contains ordinal data, it has to be preprocessed by one-hot encoding it before passing it to the model.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor099"/>Working with structured data </h1>
			<p>AutoKeras allows us to <a id="_idIndexMarker282"/>quickly and easily create high-performance models for solving tasks based on structured data.</p>
			<p>Depending on the format of each column, AutoKeras will preprocess them automatically before feeding the model. For instance, if the column contains text, it will convert it into an embedding, if the column values are fixed categories, it will convert them into one-hot encoding arrays, and so on.</p>
			<p>In the following sections, we will see how easy it is to work with tabular datasets.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor100"/>Creating a structured data classifier to predict Titanic survivors</h1>
			<p>This<a id="_idIndexMarker283"/> model will predict whether a Titanic passenger will survive the sinking of the ship based on characteristics that have been extracted from the Titanic Kaggle dataset. Although luck was an important factor in survival, some groups of people were more likely to survive than others.</p>
			<p>There are a train dataset and a test dataset in this dataset. Both are similar datasets that include passenger information such as name, age, sex, socioeconomic class, and so on.</p>
			<p>The train dataset (<strong class="source-inline">train.csv</strong>) contains details about a subset of the passengers on board (891, to be exact), revealing if they survived or not in the <strong class="source-inline">survived</strong> column.</p>
			<p>The test dataset (<strong class="source-inline">test.csv</strong>) will be used in the final evaluation and contains similar information for the other 418 passengers.</p>
			<p>AutoKeras will find patterns in the train data to predict whether these other 418 passengers on board (found in <strong class="source-inline">test.csv</strong>) survived.</p>
			<p>The full source code notebook can be found at <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_TitanicClassifier.ipynb">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_TitanicClassifier.ipynb</a>.</p>
			<p>Now, let's take a look at the relevant cells of the notebook in detail:</p>
			<ul>
				<li><strong class="bold">Installing AutoKeras</strong>: As we mentioned in other examples, this snippet at the top of the notebook is responsible for installing AutoKeras and its dependencies using the pip package manager:<p class="source-code">!pip3 install autokeras</p></li>
				<li><strong class="bold">Importing the necessary packages</strong>: The following lines load TensorFlow, pandas, and AutoKeras as <a id="_idIndexMarker284"/>the necessary dependencies for this project:<p class="source-code">import tensorflow as tf</p><p class="source-code">import autokeras as ak</p><p class="source-code">import pandas as pd</p></li>
				<li><strong class="bold">Creating the datasets</strong>: First, we will load the Titanic datasets as pandas DataFrames:<p class="source-code">train_file_url = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"</p><p class="source-code">test_file_url = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"</p><p class="source-code">train_df = pd.read_csv(train_file_url)</p><p class="source-code">test_df = pd.read_csv(test_file_url)</p><p>Now, we must separate the label (target) from the rest of the passenger features (inputs):</p><p class="source-code">x_train_df, y_train_df = train_df.drop(['survived'], axis=1), train_df['survived']</p></li>
				<li><strong class="bold">Showing some samples</strong>: Next, we will print the first few rows to see the column's values:<p class="source-code">train_df.head()</p><p>Here is the output of the preceding code:</p></li>
			</ul>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B16953_06_01.jpg" alt="Figure 6.1 – Notebook output of the first few rows of the training dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Notebook output of the first few rows of the training dataset</p>
			<p>The previous<a id="_idIndexMarker285"/> screenshot shows the passenger information represented in the different columns. The first one (<strong class="source-inline">survived</strong>) will be the target to predict.</p>
			<p>Now, it's time to create the classifier model.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Creating the classifier</h2>
			<p>Now, we <a id="_idIndexMarker286"/>will use the AutoKeras <strong class="source-inline">StructuredDataClassifier</strong> to find the best classification model. Just for this example, we will set <strong class="source-inline">max_trials</strong> (the maximum number of different Keras models to try) to <strong class="source-inline">2</strong> and set the epochs parameter to <strong class="source-inline">10</strong>:</p>
			<p class="source-code">clf = ak.StructuredDataClassifier(</p>
			<p class="source-code">max_trials=2, </p>
			<p class="source-code">overwrite=True)</p>
			<p>Let's run the training process to search for the optimal classifier for the training dataset:</p>
			<p class="source-code">clf.fit(</p>
			<p class="source-code">    x_train_df,</p>
			<p class="source-code">    y_train_df,</p>
			<p class="source-code">    epochs=10,</p>
			<p class="source-code">)</p>
			<p><strong class="source-inline">StructuredDataClassifier</strong> accepts different input formats. You can pass it a pandas DataFrame, as we did in the previous code, but it also accepts other formats, such as <a id="_idIndexMarker287"/>NumPy arrays and TensorFlow datasets. It also allows you to directly pass the URL or file path and it will be downloaded and ingested by the model automatically. To use this latter option, you must specify the name of the target column as the second argument:</p>
			<p class="source-code">clf.fit(</p>
			<p class="source-code">    train_file_url,</p>
			<p class="source-code">    'survived',</p>
			<p class="source-code">    epochs=10,</p>
			<p class="source-code">)</p>
			<p>The output will be similar in both cases:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B16953_06_02.jpg" alt="Figure 6.2 – Notebook output of structured data classifier training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Notebook output of structured data classifier training</p>
			<p>The previous output shows the accuracy of the training dataset is increasing. </p>
			<p>As we can see, we achieved <strong class="source-inline">0.84</strong> as the best prediction accuracy in the validation set. This is a <a id="_idIndexMarker288"/>good number just for a few seconds of training. We have limited the search to 10 epochs and two architectures (<strong class="source-inline">max_trials = 2</strong>). Simply increasing these numbers would give us a better accuracy, but it would also take longer to finish.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Evaluating the model</h2>
			<p>Let's evaluate<a id="_idIndexMarker289"/> the best model with the testing dataset:</p>
			<p class="source-code">clf.evaluate(test_file_url, 'survived')</p>
			<p>Here is the output of the preceding code:</p>
			<p class="source-code">9/9 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.8068</p>
			<p class="source-code">[0.4321742355823517, 0.8068181872367859]</p>
			<p>As we can <a id="_idIndexMarker290"/>see, <strong class="source-inline">0.80</strong> is also a really good final prediction score for the training time we've invested.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor103"/>Visualizing the model</h2>
			<p>Now that <a id="_idIndexMarker291"/>we have a winning model, let's look at a little summary of its architecture:</p>
			<p class="source-code">model = clf.export_model()</p>
			<p class="source-code">model.summary()</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B16953_06_03.jpg" alt="Figure 6.3 – Best model architecture summary"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Best model architecture summary</p>
			<p>As we can see, AutoKeras has done all the preprocessing work for us, by transforming the category columns into categories and performing normalizations on them.</p>
			<p>Let's look at a<a id="_idIndexMarker292"/> visual representation of this:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B16953_06_04.jpg" alt="Figure 6.4 – Best model architecture visualization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Best model architecture visualization</p>
			<p>After the <a id="_idIndexMarker293"/>data preprocessing blocks (multicategory and normalization), AutoKeras has opted to choose a fully connected neural network. This is a classical ML architecture that's suitable for tabular data. This makes sense because structured data is easier to train with classical machine learning models, since the patterns within the data are more explicit.</p>
			<p>In the next section, we are going to resolve a structured data regression problem by predicting house prices.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor104"/>Creating a structured data regressor to predict Boston house prices</h1>
			<p>In the <a id="_idIndexMarker294"/>following example, we will try to predict the median home price in a Boston suburb in the mid-1970s, given data features about the suburb at that time, such as the crime rate, tax rate of the property, local property, and so on.</p>
			<p>We will create a model that will find out the house price of a specific suburb based on its features. For this, we will train the model with the <strong class="source-inline">boston_housing</strong> dataset, which we must add to our repository (<a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/boston.csv">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/boston.csv</a>). The dataset we will use is relatively small – 506 samples divided between 404 training samples and 102 test samples. Note that the dataset isn't normalized, which means that each characteristic in the input data applies a different scale to its values. For example, some columns have values in the 0 to 1 range, while others are between 1 and 12, 0 and 100, and so on. So, this is a good dataset to test AutoKeras's auto preprocessing functionalities. </p>
			<p>The dataset's features (columns) can be summarized as follows:</p>
			<ul>
				<li><strong class="bold">CRIM</strong>: Crime rate by town (per capita)</li>
				<li><strong class="bold">ZN</strong>: Proportion of residential land zoned for lots over 25,000 sq.ft</li>
				<li><strong class="bold">INDUS</strong>: Proportion <a id="_idIndexMarker295"/>of non-retail business acres per town</li>
				<li><strong class="bold">CHAS</strong>: Charles River dummy variable (1 if the tract bounds the river; 0 otherwise)</li>
				<li><strong class="bold">NOX</strong>: Nitric oxides concentration (parts per 10 million)</li>
				<li><strong class="bold">RM</strong>: Average number of rooms per dwelling</li>
				<li><strong class="bold">AGE</strong>: Proportion of owner-occupied units built prior to 1940</li>
				<li><strong class="bold">DIS</strong>: Weighted mean of distances to five Boston employment centers</li>
				<li><strong class="bold">RAD</strong>: Index of accessibility to radial highways</li>
				<li><strong class="bold">TAX</strong>: Full-value property tax rate per $10,000</li>
				<li><strong class="bold">PTRATIO</strong>: The pupil-teacher ratio by town</li>
				<li><strong class="bold">LSTAT</strong>: Percentage lower status of the population</li>
				<li><strong class="bold">MEDV</strong>: Median value of owner-occupied homes in $1,000s</li>
			</ul>
			<p>The following screenshot shows some samples from this dataset:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B16953_06_05.jpg" alt="Figure 6.5 – A few samples from the Boston housing dataset"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – A few samples from the Boston housing dataset</p>
			<p>As we want to<a id="_idIndexMarker296"/> approximate a price, we will use a structured data regressor for this task. </p>
			<p>The notebook for this example, along with the complete source code, can be found at <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_HousingPricePredictor.ipynb">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter06/Chapter6_HousingPricePredictor.ipynb</a>.</p>
			<p>Let's explain the relevant code cells of the notebook in detail:</p>
			<ul>
				<li><strong class="bold">Getting the Boston housing dataset</strong>: Before training, we must download the dataset that contains the features of each suburb, including the median price:<p class="source-code"> df = pd.read_csv("https://raw.githubusercontent.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/main/boston.csv")</p><p class="source-code">y = df.pop('MEDV')</p><p class="source-code">X = df</p><p class="source-code">train_data, test_data, train_targets, test_targets = train_test_split(X,y,test_size=0.2)</p></li>
				<li><strong class="bold">Data preprocessing</strong>: Since we have the dataset as a package, we will create the training and test sets while using the <strong class="bold">median price column</strong> (<strong class="bold">MEDV</strong>) as the target value. Note<a id="_idIndexMarker297"/> that some of the columns will be pre-processed before they're fed to our model. AutoKeras will preprocess these columns automatically, performing normalization in continuous values (setting values between 0 and 1) and categorization in discrete values (one-hot encoding). Later in the architecture of the model, we will see the data preprocessing blocks that were<a id="_idIndexMarker298"/> created for this purpose.</li>
			</ul>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Creating a structure data regressor</h2>
			<p>Because we want to predict a price from a set of features, and since this price is a scalar value, we are going to use the AutoKeras <strong class="source-inline">StructuredDataRegressor</strong>, a structured data regression class that creates a regression model that accepts set <strong class="source-inline">x</strong> as a structured dataset (as a CSV filename, a NumPy array, a pandas DataFrame, or a TensorFlow dataset) and set, <strong class="source-inline">y</strong> as a label dataset (a one-column set in the same format as the input set, or a target column name if the input data is from a CSV file) as input.</p>
			<p>In this case, the dataset is small, and the training epochs will be faster than the other examples, so we'll<a id="_idIndexMarker299"/> set <strong class="source-inline">max_trials</strong> to 20 and set the epochs parameter to 50:</p>
			<p class="source-code">reg = ak.StructuredDataRegressor(</p>
			<p class="source-code">    max_trials=20,</p>
			<p class="source-code">    overwrite=True,</p>
			<p class="source-code">    metrics=['mae']</p>
			<p class="source-code">)</p>
			<p>For regression models, AutoKeras <a id="_idIndexMarker300"/>uses <strong class="bold">mean square error</strong> (<strong class="bold">MSE</strong>) as the default loss. As we explained in the previous chapters, this is the square of the difference between the predictions and the targets. But for this example, we are also monitoring a new metric during training that will give us more information: <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>). This is <a id="_idIndexMarker301"/>the absolute value of the difference between the predictions and the targets. For example, an MAE of 1.5 in this problem would mean that your predictions are off by $1,500 on average.</p>
			<p>Let's run the training process to <a id="_idIndexMarker302"/>search for the best model:</p>
			<p class="source-code">reg.fit(</p>
			<p class="source-code">    train_data,</p>
			<p class="source-code">    train_targets,</p>
			<p class="source-code">    epochs=50,</p>
			<p class="source-code">)</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B16953_06_06.jpg" alt="Figure 6.6 – Notebook output of training our house price predictor"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Notebook output of training our house price predictor</p>
			<p>As shown in the previous <a id="_idIndexMarker303"/>output, after less than 5 minutes, we have a model with <strong class="source-inline">5.05</strong> for the best validation loss (MSE). This means that the predictions are failing at an average of <strong class="source-inline">2.24</strong> (the square root of 5.05) in the final score. This is over $2,200. This is not a bad result for just 5 minutes of training time, so let's evaluate it with the test set.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor106"/>Evaluating the model</h2>
			<p>We are<a id="_idIndexMarker304"/> ready to evaluate our final model with the testing dataset. Let's get started:</p>
			<p class="source-code">reg.evaluate(test_data, test_targets)</p>
			<p>Here is the output of the preceding code:</p>
			<p class="source-code">4/4 [==============================] - 0s 5ms/step - loss: 13.9013 - mae: 2.4202</p>
			<p class="source-code">[13.901305198669434, 2.420168161392212]</p>
			<p>Let's look at our new metric, MAE. This has a value of <strong class="source-inline">2.420</strong>, which means that our predictions are off by $2,420 on average. This is a really good prediction error for the time we've invested. If we run AutoKeras with more trials and epochs, we will probably get better results.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor107"/>Visualizing the model</h2>
			<p>Now, it's <a id="_idIndexMarker305"/>time to look at what we have under the hood:</p>
			<p class="source-code">keras_model = reg.export_model()</p>
			<p class="source-code">keras_model.summary()</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B16953_06_07.jpg" alt="Figure 6.7 – Best model architecture summary"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Best model architecture summary</p>
			<p>As in the<a id="_idIndexMarker306"/> previous classification example, AutoKeras has done all the preprocessing work for us, transforming the columns with discrete values into categories through the <strong class="source-inline">multi_category_encoding</strong> block and performing normalizations on the continuous values columns using the <strong class="source-inline">normalization</strong> block.</p>
			<p>Let's see its visual representation:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B16953_06_08.jpg" alt="Figure 6.8 – Best model architecture visualization"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Best model architecture visualization</p>
			<p>In the<a id="_idIndexMarker307"/> previous diagram, we can see the different layers of the model in a more schematic way. Now, let's summarize what we have learned in this chapter.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor108"/>Summary</h1>
			<p>In this chapter, we learned what structured data is and its different categories, how to feed our AutoKeras models with different structured data formats (pandas, CSV files, and so on), and how to load and explore tabular datasets using some pandas functions. </p>
			<p>Finally, we applied these concepts by creating a powerful structured data classifier model to predict Titanic survivors and a powerful structured data regressor model to predict Boston house prices.</p>
			<p>With that, you have learned the basics of how to tackle any problem based on structured data using AutoKeras. With these techniques, any <strong class="source-inline">CSV</strong> file can be a dataset that you can train your model with.</p>
			<p>In the next chapter, we will learn how to perform sentiment analysis on texts using AutoKeras.</p>
		</div>
	</body></html>