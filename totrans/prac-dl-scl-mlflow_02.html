<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer010">
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/><em class="italic">Chapter 1</em>: Deep Learning Life Cycle and MLOps Challenges</h1>
			<p>The<a id="_idIndexMarker000"/> past few years have seen great success in <strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) for solving practical business, industrial, and scientific problems, particularly for tasks such<a id="_idIndexMarker001"/> as <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>), image, video, speech recognition, and conversational understanding. While research in these areas has made giant leaps, bringing these DL models from offline experimentation to production and continuously improving the models to deliver sustainable values is still a challenge. For example, a recent article by VentureBeat (<a href="https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/">https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/</a>) found that 87% of data science projects never make it to production. While there might be business reasons for such a low production rate, a major contributing factor is the difficulty caused by the lack of experiment management and a mature model production and feedback platform.</p>
			<p>This chapter will help us to understand the challenges and bridge these gaps by learning the concepts, steps, and components that are commonly used in the full life cycle of DL model development. Additionally, we will learn about the challenges of an emerging field known as <strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>), which aims to standardize and automate ML <a id="_idIndexMarker002"/>life cycle development, deployment, and operation. Having a solid understanding of these challenges will motivate us to learn the skills presented in the rest of this book using MLflow, an open source, ML full life cycle platform. The business values of adopting MLOps' best practices are numerous; they include faster time-to-market of model-derived product features, lower operating costs, agile A/B testing, and strategic decision making to ultimately improve customer experience. By the end of this chapter, we will have learned about the critical role that MLflow plays in the four pillars of MLOps (that is, data, model, code, and explainability), implemented our first working DL model, and grasped a clear picture of the challenges with data, models, code, and explainability in DL.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding the DL life cycle and MLOps challenges</li>
				<li>Understanding DL data challenges </li>
				<li>Understanding DL model challenges</li>
				<li>Understanding DL code challenges</li>
				<li>Understanding DL explainability challenges</li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements </h1>
			<p>All of the code examples for this book can be found at the following GitHub URL: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow</a>.</p>
			<p>You need to have Miniconda (<a href="https://docs.conda.io/en/latest/miniconda.html">https://docs.conda.io/en/latest/miniconda.html</a>) installed on your development environment. In this chapter, we will walk through the process of installing the PyTorch <strong class="source-inline">lightning-flash</strong> library (<a href="https://github.com/PyTorchLightning/lightning-flash">https://github.com/PyTorchLightning/lightning-flash</a>), which can be used to build our first DL model in the <em class="italic">Implementing a basic DL sentiment classifier</em> section. Alternatively, you can sign up for a free Databricks Community Edition account at <a href="https://community.cloud.databricks.com/login.html">https://community.cloud.databricks.com/login.html</a> and use a GPU cluster and a notebook to carry out the model development described in this book. </p>
			<p>In addition to this, if you are a Microsoft Windows user, we recommend that you install WSL2 (<a href="https://www.windowscentral.com/how-install-wsl2-windows-10">https://www.windowscentral.com/how-install-wsl2-windows-10</a>) so that you have a Linux environment to run the command lines that are present in this book.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Understanding the DL life cycle and MLOps challenges</h1>
			<p>Nowadays, the most successful DL models that are deployed in production primarily observe the following two steps:</p>
			<ol>
				<li><strong class="bold">Self-supervised learning</strong>: This <a id="_idIndexMarker003"/>refers to the pretraining of a model in a data-rich domain that does not require labeled data. This step produces <a id="_idIndexMarker004"/>a pretrained model, which is also called a <strong class="bold">foundation model</strong>, for example, BERT, GPT-3 for NLP, and VGG-NETS for computer vision.</li>
				<li><strong class="bold">Transfer learning</strong>: This<a id="_idIndexMarker005"/> refers to the fine-tuning of the pretrained model in a specific prediction task such as text sentiment classification, which requires labeled training data. </li>
			</ol>
			<p>One ground-breaking and successful example of a DL model in production is the <em class="italic">Buyer Sentiment Analysis</em> model, which is built on top of BERT for classifying sales engagement email messages, providing critical fine-grained insights into buyer emotions and signals beyond simple activity metrics such as reply, click, and open rates (<a href="https://www.prnewswire.com/news-releases/outreach-unveils-groundbreaking-ai-powered-buyer-sentiment-analysis-transforming-sales-engagement-301188622.html">https://www.prnewswire.com/news-releases/outreach-unveils-groundbreaking-ai-powered-buyer-sentiment-analysis-transforming-sales-engagement-301188622.html</a>). There are different variants regarding how this works, but in this book, we will primarily focus on <a id="_idIndexMarker006"/>the <strong class="bold">Transfer Learning</strong> paradigm of developing and deploying DL models, as it exemplifies a practical DL life cycle. </p>
			<p>Let's walk through an <a id="_idIndexMarker007"/>example to understand a typical core DL development paradigm. For example, the popular BERT model released in late 2018 (a basic version of the BERT model can be found at <a href="https://huggingface.co/bert-base-uncased">https://huggingface.co/bert-base-uncased</a>) was initially pretrained on raw texts (without human labeling) from over 11,000 books from BookCorpus and the entire English Wikipedia. This pretrained language model was then fine-tuned to many downstream NLP tasks, such as text classification and sentiment analysis, in different application domains such as movie review classifications by using labeled movie review data (<a href="https://huggingface.co/datasets/imdb">https://huggingface.co/datasets/imdb</a>). Note that sometimes, it might be necessary to further pretrain a foundation model (for example, BERT) within the application domain by using unlabeled data before fine-tuning to boost the final model performance in terms of accuracy. This core DL development paradigm is illustrated in <em class="italic">Figure 1.1</em>:</p>
			<div>
				<div id="_idContainer005" class="IMG---Figure">
					<img src="Images/B18120_01_001.jpg" alt="Figure 1.1 – A typical core DL development paradigm&#13;&#10;" width="1216" height="441"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – A typical core DL development paradigm</p>
			<p>Note that <a id="_idIndexMarker008"/>while <em class="italic">Figure 1.1</em> represents a common development paradigm, not all of these steps are necessary for a specific application scenario. For example, you might only need to do fine-tuning using a publicly available pretrained DL model with your labeled application-specific data. Therefore, you don't need to do your own pretraining or carry out further pretraining using unlabeled data since other people or organizations have already done the pretraining step for you.</p>
			<p class="callout-heading">DL over Classical ML</p>
			<p class="callout">Unlike classical ML<a id="_idIndexMarker009"/> model development, where, usually, a feature engineering step is required to extract and transform raw data into features to train an ML model such as decision tree or logistic regression, DL can learn the features automatically, which is especially attractive for modeling unstructured data such as texts, images, videos, audio, and speeches. DL is <a id="_idIndexMarker010"/>also called <em class="italic">representational learning</em> due to this characteristic. In<a id="_idIndexMarker011"/> addition to this, DL is usually data- and compute-intensive, requiring <strong class="bold">Graphics Process Units</strong> (<strong class="bold">GPUs</strong>), <strong class="bold">Tensor Process Units </strong>(<strong class="bold">TPU</strong>), or other<a id="_idIndexMarker012"/> types of computing hardware accelerators for at-scale training and inference. Explainability for DL models is also harder to implement, compared with traditional ML models, although recent progress has now made that possible. </p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Implementing a basic DL sentiment classifier</h2>
			<p>To set up the development of <a id="_idIndexMarker013"/>a basic DL sentiment classifier, you need to create a virtual environment in your local environment. Let's assume that<a id="_idIndexMarker014"/> you have <strong class="bold">miniconda</strong> installed. You can implement the following in your command-line prompt to create a new virtual environment called <strong class="source-inline">dl_model</strong> and install the PyTorch <strong class="source-inline">lightning-flash</strong> package so that the model can be built:</p>
			<p class="source-code">conda create -n dl_model python==3.8.10</p>
			<p class="source-code">conda activate dl_model</p>
			<p class="source-code">pip install lightning-flash[all]</p>
			<p>Depending on your local machine's memory, the preceding commands might take about 10 minutes to finish. You can verify the success of your installation by running the following command:</p>
			<p class="source-code">conda list | grep lightning</p>
			<p>If you see output similar to the following, your installation was successful:</p>
			<p class="source-code">lightning-bolts     0.3.4                    pypi_0    pypi</p>
			<p class="source-code">lightning-flash     0.5.0                    pypi_0    pypi</p>
			<p class="source-code">pytorch-lightning   1.4.4                    pypi_0    pypi</p>
			<p>Now you are ready to build your first DL model!</p>
			<p>To begin building a <a id="_idIndexMarker015"/>DL model, complete the following steps:</p>
			<ol>
				<li value="1">Import the necessary <strong class="source-inline">torch</strong> and <strong class="source-inline">flash</strong> libraries, and import <strong class="source-inline">download_data</strong>, <strong class="source-inline">TextClassificationData</strong>, and <strong class="source-inline">TextClassifier</strong> from the <strong class="source-inline">flash</strong> subpackages:<p class="source-code">import torch</p><p class="source-code">import flash</p><p class="source-code">from flash.core.data.utils import download_data</p><p class="source-code">from flash.text import TextClassificationData, TextClassifier</p></li>
				<li>To get the dataset for fine-tuning, use <strong class="source-inline">download_data</strong> to download the <strong class="source-inline">imdb.zip</strong> file, which is the public domain binary sentiment classification (positive/negative) dataset <a id="_idIndexMarker016"/>from <strong class="bold">Internet Movie Database</strong> (<strong class="bold">IMDb</strong>) to a local data folder. The <a id="_idIndexMarker017"/>IMDb ZIP file contains three CSV files: <ul><li><strong class="source-inline">train.csv</strong></li><li><strong class="source-inline">valid.csv</strong> </li><li><strong class="source-inline">test.csv</strong> </li></ul></li>
			</ol>
			<p>Each<a id="_idIndexMarker018"/> file contains two columns: <strong class="source-inline">review</strong> and <strong class="source-inline">sentiment</strong>. We then use <strong class="source-inline">TextClassificationData.from_csv</strong> to declare a <strong class="source-inline">datamodule</strong> variable that assigns the "review" to <strong class="source-inline">input_fields</strong>, and the "sentiment" to <strong class="source-inline">target_fields</strong>. Additionally, it assigns the <strong class="source-inline">train.csv</strong> file to <strong class="source-inline">train_file</strong>, the <strong class="source-inline">valid.csv</strong> file to <strong class="source-inline">val_file</strong>, and the <strong class="source-inline">test.csv</strong> file to the <strong class="source-inline">test_file</strong> properties of <strong class="source-inline">datamodule</strong>, respectively:</p>
			<p class="source-code">download_data("https://pl-flash-data.s3.amazonaws.com/imdb.zip", "./data/")</p>
			<p class="source-code">datamodule = TextClassificationData.from_csv(</p>
			<p class="source-code">    input_fields="review",</p>
			<p class="source-code">    target_fields="sentiment",</p>
			<p class="source-code">    train_file="data/imdb/train.csv",</p>
			<p class="source-code">    val_file="data/imdb/valid.csv",</p>
			<p class="source-code">    test_file="data/imdb/test.csv"</p>
			<p class="source-code">)</p>
			<ol>
				<li value="3">Once we have the data, we can now perform fine-tuning using a foundation model. First, we declare <strong class="source-inline">classifier_model</strong> by calling <strong class="source-inline">TextClassifier</strong> with a backbone assigned to <strong class="source-inline">prajjwal1/bert-tiny</strong> (which is a much smaller BERT-like pretrained model <a id="_idIndexMarker019"/>located in the Hugging Face model repository: <a href="https://huggingface.co/prajjwal1/bert-tiny">https://huggingface.co/prajjwal1/bert-tiny</a>). This means our model will be based on the <strong class="source-inline">bert-tiny</strong> model. </li>
				<li>The next step is to set<a id="_idIndexMarker020"/> up the trainer by defining how many epochs we want to run and how many GPUs we want to use to run them. Here, <strong class="source-inline">torch.cuda.device_count()</strong> will return either <em class="italic">0</em> (no GPU) or <em class="italic">1</em> to <em class="italic">N</em>, where <em class="italic">N</em> is <a id="_idIndexMarker021"/>the maximum number of GPUs you can have in your running environment. Now we are ready to call <strong class="source-inline">trainer.finetune</strong> to train a binary sentiment classifier for the IMDb dataset:<p class="source-code">classifier_model = TextClassifier(backbone="prajjwal1/bert-tiny", num_classes=datamodule.num_classes)</p><p class="source-code">trainer = flash.Trainer(max_epochs=3, gpus=torch.cuda.device_count())</p><p class="source-code">trainer.<strong class="bold">finetune</strong>(classifier_model, datamodule=datamodule, strategy="freeze")</p><p class="callout-heading">DL Fine-Tuning Time</p><p class="callout">Depending on your running environment, the fine-tuning step might take a couple of minutes on a GPU or around 10 minutes (if you're only using a CPU). You can reduce <strong class="source-inline">max_epochs=1</strong> if you simply want to get a basic version of the sentiment classifier quickly.</p></li>
				<li>Once the fine-tuning step is complete, we will test the accuracy of the model by running <strong class="source-inline">trainer.test()</strong>:<p class="source-code">trainer.test()</p></li>
			</ol>
			<p>The output of the test should look similar to the following screenshot, whichindicates that the final model accuracy is about 52%:</p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="Images/B18120_01_002.jpg" alt="Figure 1.2 – The test results of our first DL model&#13;&#10;" width="637" height="160"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – The test results of our first DL model</p>
			<p>The<a id="_idIndexMarker022"/> test result<a id="_idIndexMarker023"/> shown in the preceding diagram indicates that we have a basic version of the model, as we only fine-tuned the foundation model for three epochs and haven't used any advanced techniques such as hyperparameter tuning or better fine-tuning strategies. However, this is a great accomplishment since you now have a working knowledge of how the core DL model paradigm works! We will explore more advanced model training techniques in later chapters of this book.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Understanding DL's full life cycle development</h2>
			<p>By now, you should have your first <a id="_idIndexMarker024"/>DL model ready and should feel proud of it. Now, let's explore the full DL life cycle together to fully understand its concepts, components, and challenges.</p>
			<p>You might have gathered that the core DL development paradigm revolves around three key artifacts: <em class="italic">Data</em>, <em class="italic">Model</em>, and <em class="italic">Code</em>. In addition to this, <em class="italic">Explainability</em> is another major artifact that is required in many mission-critical application scenarios such as medical diagnoses, the financial industry, and decision making for criminal justice. As DL is usually considered a black box, providing explainability for DL increasingly becomes a key requirement before and after shipping to production.</p>
			<p>Note that <em class="italic">Figure 1.1</em> is still considered offline experimentation if we are still trying to figure out which model works using a dataset in a lab-like environment. Even in such an offline experimentation environment, things will quickly become complicated. Additionally, we would like to know and track which experiments we have or have not performed so that we don't waste time repeating the same experiments, whatever parameters and datasets we have used, and whatever kind of metrics we have for a specific model. Once we have a model that's good enough for the use cases and customer scenarios, the complexity increases as we need a way to continuously deploy and update the model in production, monitor the model and data drift, and then retrain the model when<a id="_idIndexMarker025"/> necessary. This complexity further increases when at-scale training, deployment, monitoring, and explainability are needed. </p>
			<p>Let's examine what a DL life cycle<a id="_idIndexMarker026"/> looks like (see <em class="italic">Figure 1.3</em>). There are five stages: </p>
			<ol>
				<li value="1">Data collection, cleaning, and annotation/labeling. </li>
				<li>Model development (which is also known as offline experimentation). The core DL development paradigm in <em class="italic">Figure 1.1</em> is considered part of the <em class="italic">model development</em> stage, which itself can be an iterative process.</li>
				<li>Model deployment and serving in production. </li>
				<li>Model validation and A/B testing (which is also known as online experimentation; this is usually in a production environment).</li>
				<li>Monitoring and feedback data collection during production.</li>
			</ol>
			<p><em class="italic">Figure 1.3</em> provides a diagram to show that it is a continuous development cycle for a DL model:</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="Images/B18120_01_003.jpg" alt="Figure 1.3 – The full DL development life cycle &#13;&#10;" width="1141" height="1099"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – The full DL development life cycle </p>
			<p>In addition to this, we want to<a id="_idIndexMarker027"/> point out that the backbone of these five stages, as shown in <em class="italic">Figure 1.3</em>, essentially revolves around the four artifacts: data, model, code, and explainability. We will examine the challenges related to these four artifacts in the life cycle in the following sections. However, first, let's explore and understand MLOps, which is an evolving platform concept and framework that supports the full life cycle of ML. This will help us understand these challenges in a big-picture context.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Understanding MLOps challenges</h2>
			<p>MLOps has some<a id="_idIndexMarker028"/> connections to DevOps, where a set of technology stacks and standard operational procedures are used for software development and deployment combined with IT operations. Unlike traditional software development, ML and especially DL represent a new era of software development paradigms <a id="_idIndexMarker029"/>called <strong class="bold">Software 2.0</strong> (<a href="https://karpathy.medium.com/software-2-0-a64152b37c35">https://karpathy.medium.com/software-2-0-a64152b37c35</a>). The key differentiator of Software 2.0 is that the behavior of the software does not just depend on well-understood programming language code (which is the characteristic of Software 1.0) but depends on the learned weights in a neural network that's difficult to write as code. In other words, there exists an inseparable integration of the code, data, and model that must be managed together. Therefore, MLOps is being developed and is still evolving to accommodate this new Software 2.0 paradigm. In this book, MLOps is defined as an operational automation platform that consists of three foundation layers and four pillars. They are listed as follows:</p>
			<ul>
				<li>Here are the three foundation layers:<ul><li>Infrastructure <a id="_idIndexMarker030"/>management and automation</li><li>Application life cycle management <a id="_idIndexMarker031"/>and <strong class="bold">Continuous Integration and Continuous Deployment</strong> (<strong class="bold">CI/CD</strong>)</li><li>Service system observability</li></ul></li>
				<li>Here are the four <a id="_idIndexMarker032"/>pillars:<ul><li>Data observability and management</li><li>Model observability and life cycle management</li><li>Explainability and <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) observability</li><li>Code reproducibility and observability</li></ul></li>
			</ul>
			<p>Additionally, we will explain MLflow's roles in these MLOps layers and pillars so that we have a clear picture regarding what MLflow can do to build up the MLOps layers in their entirety:</p>
			<ul>
				<li><strong class="bold">Infrastructure management and automation</strong>: This includes, but is not limited to, <em class="italic">Kubernetes</em> (also known as k8s) for<a id="_idIndexMarker033"/> automated container orchestration<a id="_idIndexMarker034"/> and <em class="italic">Terraform</em> (commonly used for managing hundreds of cloud services and access control). These <a id="_idIndexMarker035"/>tools are adapted to manage ML and DL applications that have deployed models as service endpoints. These infrastructure layers are not the focus of this book; instead, we will focus on how to deploy a trained DL model using MLflow's provided capabilities.</li>
				<li><strong class="bold">Application life cycle management and CI/CD</strong>: This includes, but is not limited to, <em class="italic">Docker</em> containers for virtualization, container life cycle management tools such as <a id="_idIndexMarker036"/>Kubernetes, and <em class="italic">CircleCI</em> or <em class="italic">Concourse</em> for <strong class="bold">CI</strong> and <strong class="bold">CD</strong>. Usually, CI <a id="_idIndexMarker037"/>means that <a id="_idIndexMarker038"/>whenever there are code or model changes in a GitHub repository, a series of automatic tests will be triggered to make sure no breaking changes are introduced. Once these tests have been passed, new changes will be automatically released as part of a new package. This will then trigger a new deployment process (CD) to deploy the new package to the production environment (often, this will include human approval as a safety gate). Note that these tools are not unique to ML applications but have been adapted to ML and DL applications, especially when we require GPU and distributed clusters for the training and testing of DL models. In this book, we will not focus on these tools but will mention the integration points or examples when needed.</li>
				<li><strong class="bold">Service system observability</strong>: This is mostly for monitoring the hardware/clusters/CPU/memory/storage, operating system, service availability, latency, and throughput. This includes tools<a id="_idIndexMarker039"/> such as <em class="italic">Grafana</em>, <em class="italic">Datadog</em>, and <a id="_idIndexMarker040"/>more. Again, these are not unique to ML and DL applications and are not the focus of this book.</li>
				<li><strong class="bold">Data observability and management</strong>: This is traditionally under-represented in the DevOps world but becomes very important in MLOps as data is critical within the full life cycle of ML/DL models. This includes <em class="italic">data quality monitoring</em>, <em class="italic">outlier detection</em>, <em class="italic">data drift</em> and <em class="italic">concept drift detection</em>, <em class="italic">bias detection</em>, <em class="italic">secured</em> and <em class="italic">compliant data sharing</em>, <em class="italic">data provenance tracking</em> and <em class="italic">versioning</em>, and more. The tool stacks in this area that are suitable for ML and DL applications are still emerging. A <a id="_idIndexMarker041"/>few <a id="_idIndexMarker042"/>examples include <strong class="bold">DataFold</strong> (<a href="https://www.datafold.com/">https://www.datafold.com/</a>) and <strong class="bold">Databand</strong> (<a href="https://databand.ai/open-source/">https://databand.ai/open-source/</a>). A recent development in data management is a unified lakehouse architecture and implementation called <strong class="bold">Delta Lake</strong> (<a href="http://delta.io">http://delta.io</a>) that<a id="_idIndexMarker043"/> can be used for ML data management. MLflow has native integration points with Delta Lake, and we will cover that integration in this book. </li>
				<li><strong class="bold">Model observability and life cycle management</strong>: This is unique to ML/DL models, and it only became widely available recently due to the rise of MLflow. This includes tools for model training, testing, versioning, registration, deployment, serialization, model drift monitoring, and more. We will learn about the exciting<a id="_idIndexMarker044"/> capabilities that MLflow provides in this area. Note that once we combine CI/CD tools with MLflow training/monitoring, user feedback loops, and human annotations, we can<a id="_idIndexMarker045"/> achieve <strong class="bold">Continuous Training</strong>, <strong class="bold">Continuous Testing</strong>, and <strong class="bold">Continuous Labeling</strong>. MLflow <a id="_idIndexMarker046"/>provides the foundational capabilities so <a id="_idIndexMarker047"/>that further automation in MLOps becomes possible, although such complete automation will not be the focus of this book. Interested readers can find relevant references at the end of this chapter to explore this area further.</li>
				<li><strong class="bold">Explainability and AI observability</strong>: This is unique to ML/DL models and is especially important for DL models, as traditionally, DL models are treated as black boxes. Understanding why the model provides certain predictions is critical for societally important applications. For example, in medical, financial, juridical, and many human-in-the-loop decision support applications, such as civilian and military emergency response, the demand for explainability is increasingly higher. MLflow provides native integration with a popular explainability framework called SHAP, which we will cover in this book.</li>
				<li><strong class="bold">Code reproducibility and observability</strong>: This is not entirely unique to ML/DL applications. However, DL models face some special challenges as the number of DL code frameworks are diverse and the need to reproduce a model is not entirely up to the code alone (we also need data and execution environments such as GPU clusters). In addition to this, notebooks are commonly used in model development and production. How to manage the notebooks along with the model run is important. Usually, GitHub is used to manage the code repository; however, we need to structure the ML project code in a way that's reproducible either locally (such as on a local laptop) or remotely (for example, in a Databricks' GPU cluster). MLflow provides this capability to allow DL projects that have been written once to run anywhere, whether this is in an offline experimentation environment or an online production <a id="_idIndexMarker048"/>environment. We will cover MLflow's MLproject capability in this book.</li>
			</ul>
			<p>In summary, MLflow plays a critical and foundational role in MLOps. It fills in the gaps that DevOps traditionally does not cover and, thus, is the focus of this book. The following diagram (<em class="italic">Figure 1.4</em>) shows the central roles of MLflow in the still-evolving MLOps world:</p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="Images/B18120_01_004.jpg" alt="Figure 1.4 – The three layers and four pillars of MLOps and MLflow's roles&#13;&#10;" width="1439" height="839"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – The three layers and four pillars of MLOps and MLflow's roles</p>
			<p>While the bottom two layers and the topmost layer are common within many software development and deployment processes, the middle four pillars are either entirely unique to ML/DL applications or partially unique to ML/DL applications. MLflow plays a critical role in all four of these pillars in MLOps. This book will help you to confidently apply MLflow to solve the issues of these four pillars while also equipping you to further integrate with other tools in the MLOps layers depicted in <em class="italic">Figure 1.4</em> for full automation depending on your scenario requirements.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Understanding DL data challenges</h1>
			<p>In this section, we will discuss the <a id="_idIndexMarker049"/>data challenges at each stage of the DL life cycle, as illustrated in <em class="italic">Figure 1.3</em>. Essentially, DL is a data-centric AI, unlike symbolic AI where human knowledge can be used without lots of data. The challenges for data in DL are pervasive in all stages of the full life cycle:</p>
			<ul>
				<li><strong class="bold">Data collection/cleaning/annotation</strong>: One of DL's first successes began with <strong class="bold">ImageNet</strong> (<a href="https://www.image-net.org/">https://www.image-net.org/</a>), where millions of images are collected <a id="_idIndexMarker050"/>and annotated according to the English nouns in the WordNet database<a id="_idIndexMarker051"/> (<a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a>). This led to the successful development of pretrained DL models for computer vision such as <a id="_idIndexMarker052"/>VGG-NETS (<a href="https://pytorch.org/hub/pytorch_vision_vgg/">https://pytorch.org/hub/pytorch_vision_vgg/</a>), which can perform state-of-the-art image classification and is widely used for industrial and business applications. The main challenge of this kind of large-scale data collection and annotation is the unknown bias, which is hard to measure in this process (<a href="https://venturebeat.com/2020/11/03/researchers-show-that-computer-vision-algorithms-pretrained-on-imagenet-exhibit-multiple-distressing-biases/">https://venturebeat.com/2020/11/03/researchers-show-that-computer-vision-algorithms-pretrained-on-imagenet-exhibit-multiple-distressing-biases/</a>). Another example is the sales engagement platform <strong class="bold">Outreach</strong> (<a href="https://www.outreach.io/">https://www.outreach.io/</a>), where we <a id="_idIndexMarker053"/>can classify a potential buyer's sentiment. For instance, we might start by collecting email messages of 100 paid organizations to train a DL model. Following this, we would need to collect email messages from more organizations, either due to an accuracy requirement or expanded language coverage (such as from English only to other languages such as German and French). These many iterations of data collection and annotation will generate quite a lot of datasets. There is a tendency to just name the version of the <a id="_idIndexMarker054"/>dataset with hardcoded version numbers as part of a dataset filename such as the following:<p class="source-code"><strong class="bold">MyCoolAnnotatedData-v1.0.csv</strong></p><p class="source-code"><strong class="bold">MyCoolAnnotatedData-v2.0.csv</strong></p><p class="source-code"><strong class="bold">MyCoolAnnotatedData-v3.0.csv</strong></p><p class="source-code"><strong class="bold">MyCoolAnnotatedData-v4.0.csv</strong></p></li>
			</ul>
			<p>This seems to work until some changes are required in any one of the vX.0 datasets due to the need to correct annotation errors or remove email messages because of customer churn. Also, what happens if we need to combine several datasets together or perform some data cleaning and transformation to train a new DL model? What if we need to implement data augmentation to artificially generate some datasets? Evidently, simply changing the names of the files is neither scalable nor sustainable.</p>
			<ul>
				<li><strong class="bold">Model development</strong>: We need to understand that the bias in the data we use to train/pretrain a DL model will reflect in the prediction when applying the model. While we do not focus on de-biasing data in this book, we must implement data versioning and data provenance as first-class artifacts when training and serving a DL model so that we can track all model experiments. When fine-tuning a pretrained model for our use cases, as we did earlier, we also need to track the versioning of the fine-tuning dataset we use. In our previous example, we use a variant of the BERT model to fine-tune the IMDb review data. While, in our first example, we did not care about the versioning or source of the data, this is important for a practical and real application. In summary, DL<a id="_idIndexMarker055"/> models need to link to a particular version of datasets using a scalable approach. We will provide solutions to this topic in this book.</li>
				<li><strong class="bold">Model deployment and serving in production</strong>: This is for deploying into the production environment to serve online traffic. DL model serving latency is of particular importance and is interesting to collect at this stage. This might allow you to adjust the hardware environment used for inference.</li>
				<li><strong class="bold">Model validation and A/B testing</strong>: The data we collect at this stage is mostly for user behavior metrics in the online experimentation environment (<a href="https://www.slideshare.net/pavel/ab-testing-ai-global-artificial-intelligence-conference-2019">https://www.slideshare.net/pavel/ab-testing-ai-global-artificial-intelligence-conference-2019</a>). Online data traffic also needs to be characterized in order to understand whether there is a statistical difference in the input to the model between offline experimentation and online experimentation. Only if we pass the A/B testing and validate that the model indeed works better than its previous version in terms of user behavior metrics do we roll out to production for all users. </li>
				<li><strong class="bold">Monitoring and feedback loops</strong>: In this stage, note that the data will need to be continuously collected to detect data drift and concept drift. For example, in the buyer sentiment classification example discussed earlier, if buyers start to use terminology <a id="_idIndexMarker056"/>that is not encountered in the training data, the performance of the model could suffer.</li>
			</ul>
			<p>In summary, data tracking and observability are major challenges in all stages of the DL life cycle. </p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Understanding DL model challenges</h1>
			<p>In this section, we will <a id="_idIndexMarker057"/>discuss DL model challenges. Let's look at the challenges at each stage of the DL life cycle, as depicted in <em class="italic">Figure 1.3</em>:</p>
			<ul>
				<li><strong class="bold">Data collection/cleaning/annotation</strong>: While the data challenge has already been stated, the challenge of linking data to the model of interest still exists. MLflow has native integration with Delta Lake so that any trained model can be traced back to a particular version within Delta Lake. </li>
				<li><strong class="bold">Model development</strong>: This is the time for trying lots of model frameworks, packages, and model selections. We need to track all the packages we use, along with the model parameters, hyperparameters, and model metrics in all experiments we run. Without a scalable and standardized way to track all experiments, this becomes a very tangled space. This not only causes trouble in terms of not knowing which experiments have been done so that we don't waste time doing them again, but it also creates problems when tracking which model is ready to be deployed or has already been deployed. Model serialization is another major challenge as different DL frameworks tend to use different ways to serialize the model. For<a id="_idIndexMarker058"/> example, <strong class="source-inline">pickle</strong>.<em class="italic"> </em>(<a href="https://github.com/cloudpipe/cloudpickle">https://github.com/cloudpipe/cloudpickle</a>) is usually used in serializing the model written in Python. However, TorchScript (<a href="https://pytorch.org/docs/stable/jit.html">https://pytorch.org/docs/stable/jit.html</a>) is now<a id="_idIndexMarker059"/> highly performant for PyTorch models. In addition, Open Neural Network Exchange or ONNX (<a href="https://onnx.ai/">https://onnx.ai/</a>) tries<a id="_idIndexMarker060"/> to provide more framework-agnostic DL serialization. Finally, we need to log the serialized model and register the model so that we can track model versioning. MLflow is one of the first open source <a id="_idIndexMarker061"/>tools to overcome these challenges.</li>
				<li><strong class="bold">Model deployment and serving in production</strong>: An easy-to-use model deployment tool that can tie into the model registry is a challenge. MLflow can be used to alleviate that, allowing you to load models for production deployment with full provenance tracking. </li>
				<li><strong class="bold">Model validation and A/B testing</strong>: During online validation and experimentation, model performance needs to be validated and user behavior metrics need to be collected. This is so that we can easily roll back or redeploy a particular version of the models. A model registry is critical for at-scale online model production validation and experimentation.</li>
				<li><strong class="bold">Monitoring and feedback loops</strong>: Model drifting and degradation over time is a real challenge. The visibility of model performance in production needs to be continuously monitored. Feedback data can be used to decide whether a model needs to be retrained.</li>
			</ul>
			<p>In summary, DL model challenges in the full life cycle are unique. It is also worth pointing out a common framework that can assist the model development and online production back-and-forth is of great importance, as we don't want to use different tools just because the execution environment is different. MLflow provides this unified framework to bridge such gaps.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Understanding DL code challenges</h1>
			<p>In this section, we will <a id="_idIndexMarker062"/>discuss DL code challenges. Let's look at how these code challenges are manifested in each of the stages described in <em class="italic">Figure 1.3</em>. In this section, and within the context of DL development, code refers to the source code that's written in certain programming languages such as Python for data processing and implementation, while a model refers to the model logic and architecture in its serialized format (for example, pickle, TorchScript, or ONNX):</p>
			<ul>
				<li><strong class="bold">Data collection/cleaning/annotation</strong>: While data is the central piece in this stage, the code that does the query, <strong class="bold">extraction/transformation/loading </strong>(<strong class="bold">ETL</strong>), and <a id="_idIndexMarker063"/>data cleaning and augmentation is of critical importance. We cannot decouple the development of the model from the data pipelines that provide the data feeds to the model. Therefore, data pipelines that implement ETL need to be treated as one of the integrated steps in both offline experimentation and online production. A common mistake is that we use different data ETL and cleaning pipelines in offline experimentation, and then implement different data ETL/cleaning pipelines in online production, which could cause different model behaviors. We need to version and serialize the data pipeline as part of the entire model pipeline. MLflow provides several ways to allow us to implement such multistep pipelines.</li>
				<li><strong class="bold">Model development</strong>: During offline experiments, in addition to different versions of data pipeline code, we might also have different versions of notebooks or use different versions of DL library code. The usage of notebooks is particularly unique in ML/DL life cycles. Tracking which model results are produced by which notebook/model pipeline/data pipeline needs to be done for each run. MLflow does that with automatic code version tracking and dependencies. In addition, code reproducibility in different running environments is unique to DL models, as DL models usually require hardware accelerators such as GPUs or TPUs. The flexibility of running either locally, or remotely, on a CPU or GPU environment is of great importance. MLflow provides a lightweight approach in which to organize the ML projects so that code can be written once and run everywhere. </li>
				<li><strong class="bold">Model deployment and serving in production</strong>: While the model is serving production traffic, any bugs will need to be traced back to both the model and code. Thus, tracking code provenance is critical. It is also critical to track all the dependency code library versions for a particular version of the model.</li>
				<li><strong class="bold">Model validation and A/B testing</strong>: Online experiments could use multiple versions of models using different data feeds. Debugging any experimentation will require not only knowing which model is used but also which code is used to produce that model. </li>
				<li><strong class="bold">Monitoring and feedback loops</strong>: This stage is similar to the previous stage in terms of code challenges, where we need to know whether model degradation is due to code bugs or model and data drifting. The monitoring pipeline needs to<a id="_idIndexMarker064"/> collect all the metrics for both data and model performance.</li>
			</ul>
			<p>In summary, DL code challenges are especially unique because DL frameworks are still evolving (for example, <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, <strong class="bold">Keras</strong>, <strong class="bold">Hugging Face</strong>, and <strong class="bold">SparkNLP</strong>). MLflow provides a lightweight framework to overcome many common challenges and can interface with many DL frameworks seamlessly.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Understanding DL explainability challenges</h1>
			<p>In this section, we will discuss<a id="_idIndexMarker065"/> DL explainability challenges at each of the stages described in <em class="italic">Figure 1.3</em>. It is increasingly important to view explainability as an integral and necessary mechanism to define, test, debug, validate, and monitor models across the entire model life cycle. Embedding explainability early will make subsequent model validation and operations easier. Also, to maintain ongoing trust in ML/DL models, it is critical to be able to explain and debug ML/DL models after they go live in production:</p>
			<ul>
				<li><strong class="bold">Data collection/cleaning/annotation</strong>: As we have gathered, explainability is critical for model prediction. The root cause of any model's trustworthiness or bias can be traced back to the data used to train the model. Explainability for the data is still an emerging area but is critical. So, what could go wrong and become a challenge during the data collection/cleaning/annotation stage? For example, let's suppose we have an ML/DL model, and its prediction outcome is about whether a loan applicant will pay back a loan or not. If the data collected has certain correlations between age and the loan payback outcome, this will cause the model to use age as a predictor. However, a loan decision based on a person's age is against the law and not allowed even if the model works well. So, during data collection, it could be that the sampling strategy is not sufficient to represent certain subpopulations such as different loan applicants in different age groups. </li>
			</ul>
			<p>A subpopulation could have lots of missing fields and then be dropped during data cleaning. This could result in underrepresentation following the data cleaning process. Human annotations could favor the privileged group and other possible unconscious biases. A metric called <strong class="bold">Disparate Impact</strong> could<a id="_idIndexMarker066"/> reveal the hidden biases in the data, which<a id="_idIndexMarker067"/> compares the proportion of individuals that receive a positive outcome for two groups: an unprivileged group and a privileged group. If the unprivileged group (for example, persons with age &gt; 60) receives a positive outcome (for example, loan approval) less than 80% of the proportion of the privileged group (persons with age &lt; 60), this is a disparate impact violation based on the current common industry standard (a four-fifths rule). Tools such <a id="_idIndexMarker068"/>as <strong class="bold">Dataiku</strong> could help to automate the disparate impact and subpopulation analysis to find groups of people who may be treated unfairly or differently because of the data used for model training.</p>
			<ul>
				<li><strong class="bold">Model development</strong>: Model explainability during offline experimentation is very important to not only help understand why a model behaves a certain way but also help with model selection to decide which model to use if we need to put it into production. Accuracy might not be the only criteria to select a winning model. There are a few DL explainability tools, such as SHAP (please refer to <em class="italic">Figure 1.5</em>). MLflow <a id="_idIndexMarker069"/>integration with SHAP provides a way to implement DL explainability:</li>
			</ul>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="Images/B18120_01_005.jpg" alt="Figure 1.5 – NLP text SHAP Variable Importance Plot when using a DL model&#13;&#10;" width="1467" height="915"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – NLP text SHAP Variable Importance Plot when using a DL model</p>
			<p><em class="italic">Figure 1.5</em> shows that this NLP model's prediction results' number one feature is the word <strong class="source-inline">impressive</strong>, followed by <strong class="source-inline">rent</strong>. Essentially, this breaks the black box of the DL model, giving much confidence to the usage of DL models in production.</p>
			<ul>
				<li><strong class="bold">Model deployment and serving in production</strong>: During the production stage, if the explainability of the model prediction can be readily provided to users, then not only<a id="_idIndexMarker070"/> will the usability (user-friendliness) of the model be improved, but also, we can collect better feedback data as users are more incentivized to give more meaningful feedback. A good explainability solution should provide point-level decisions for any prediction outcome. This means that we should be able to answer why a particular person's loan is rejected and how this rejection compares to other people in a similar or different age group. So, the challenge is to have explainability as one of the gated deployment criteria for releasing a new version of the model. However, unlike accuracy metrics, it is very difficult to measure explainability as scores or thresholds, although certain case-based reasoning could be applied and automated. For example, if we have certain hold-out test cases where we expect the same or similar explanations regardless of the versions of the model, then we could use that as a gated release criterion. </li>
				<li><strong class="bold">Model validation and A/B testing</strong>: During online experimentation and ongoing production model validation, we would need explainability to understand whether the model has been applied to the right data or whether the prediction is trustworthy. Usually, ML/DL models encode complex and non-linear relationships. During this stage, it is often desirable to understand how the model influences the metrics of user behavior (for example, a higher conversion rate on a shopping website). Influence sensitivity analysis could provide insights regarding whether a certain user feature such as a user's income has a positive or negative impact on the outcome. If during this stage, we found, for some reason, that higher incomes cause a negative loan approval rate or a lower conversion rate, then this should be automatically flagged. However, automated sensitivity analysis during model validation and A/B testing is still not widely available and remains a challenging problem. A few vendors such as TruEra provide potential solutions to this space.</li>
				<li><strong class="bold">Monitoring and feedback loops</strong>: While model performance metrics and data characteristics are of importance here, explainability can provide an incentive for users to provide valuable feedback and user behavior metrics to identify drivers and causes of model degradation if there are any. As we know, ML/DL models are prone to overfitting and cannot generalize well beyond their training data. One important explainability solution during model production monitoring is to measure how feature importance shifts across different data splits (for example, pre-COVID versus post-COVID). This can help data scientists to identify where <a id="_idIndexMarker071"/>degradation in model performance is due to changing data (such as a statistical distribution shift) or changing relationships between variables (such as a concept shift). A recent example provided by TruEra (<a href="https://truera.com/machine-learning-explainability-is-just-the-beginning/">https://truera.com/machine-learning-explainability-is-just-the-beginning/</a>) illustrates that a loan model changes its prediction behavior due to changes in people's annual income and loan purposes before and after the COVID periods. This explainability<a id="_idIndexMarker072"/> of <strong class="bold">Feature Importance Shift</strong> greatly helps to identify the root causes of changes in model behavior during the model production monitoring stage.</li>
			</ul>
			<p>In summary, DL explainability is a major challenge where ongoing research is still needed. However, MLflow's integration with SHAP now provides a ready-to-use tool for practical DL applications, which we will cover in our advanced chapter later in this book.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Summary</h1>
			<p>In this opening chapter, we implemented our first DL model by following the pretrain plus fine-tuning core DL development paradigm using PyTorch <strong class="source-inline">lightning-flash</strong> for a text sentiment classification model. We learned about the five stages of the full life cycle of DL. We defined the concept of MLOps along with the three foundation layers and four ML/DL pillars, where MLflow plays critical roles in all four pillars (data, model, code, and explainability). Finally, we described the challenges in DL data, model, code, and explainability. </p>
			<p>With the knowledge and first DL model experience gained in this chapter, we are now ready to learn about and implement MLflow in our DL model in the following chapters. In the next chapter, we will start with the implementation of a DL model with MLflow autologging enabled. </p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Further reading</h1>
			<p>To further your knowledge, please consult the following resources and documentation:</p>
			<ul>
				<li><em class="italic">On the Opportunities and Risks of Foundation Models</em> (Stanford University): <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a></li>
				<li><em class="italic">MLOps: not as Boring as it Sounds</em>: <a href="https://itnext.io/mlops-not-as-boring-as-it-sounds-eaebe73e3533">https://itnext.io/mlops-not-as-boring-as-it-sounds-eaebe73e3533</a></li>
				<li><em class="italic">AI is Driving Software 2.0… with Minimal Human Intervention</em>:<a href="https://www.datasciencecentral.com/profiles/blogs/ai-is-driving-software-2-0-with-minimal-human-intervention"> https://www.datasciencecentral.com/profiles/blogs/ai-is-driving-software-2-0-with-minimal-human-intervention</a></li>
				<li><em class="italic">MLOps: Continuous delivery and automation pipelines in machine learning</em> (Google): <a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a></li>
				<li><em class="italic">Deep Learning Development Cycle</em> (Salesforce): <a href="https://metamind.readme.io/docs/deep-learning-dev-cycle">https://metamind.readme.io/docs/deep-learning-dev-cycle</a></li>
				<li><em class="italic">MLOps – The Missing Piece In The Enterprise AI Puzzle</em>: <a href="https://www.forbes.com/sites/janakirammsv/2021/01/05/mlopsthe-missing-piece-in-the-enterprise-ai-puzzle/?sh=3d5c89dd24ad">https://www.forbes.com/sites/janakirammsv/2021/01/05/mlopsthe-missing-piece-in-the-enterprise-ai-puzzle/?sh=3d5c89dd24ad</a></li>
				<li><em class="italic">MLOps: What It Is, Why It Matters, and How to Implement It</em>: <a href="https://neptune.ai/blog/mlops">https://neptune.ai/blog/mlops</a></li>
				<li><em class="italic">Explainable Deep Learning: A Field Guide for the Uninitiated</em>: <a href="https://arxiv.org/abs/2004.14545">https://arxiv.org/abs/2004.14545</a></li>
				<li><em class="italic">Machine learning explainability is just the beginning</em>: <a href="https://truera.com/machine-learning-explainability-is-just-the-beginning/">https://truera.com/machine-learning-explainability-is-just-the-beginning/</a></li>
				<li><em class="italic">AI Fairness — Explanation of Disparate Impact Remover</em>: <a href="https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1">https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1</a></li>
				<li><em class="italic">Datasheets for Datasets</em>: <a href="https://arxiv.org/pdf/1803.09010.pdf">https://arxiv.org/pdf/1803.09010.pdf</a></li>
			</ul>
		</div>
	</div></body></html>