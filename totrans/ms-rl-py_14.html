<html><head></head><body>
		<div id="_idContainer1496">
			<h1 id="_idParaDest-231"><em class="italic"><a id="_idTextAnchor239"/>Chapter 11</em>: Generalization and Domain Randomization</h1>
			<p>Deep <strong class="bold">reinforcement</strong> <strong class="bold">learning</strong> (<strong class="bold">RL</strong>) has achieved what was impossible with earlier AI methods, such as beating world champions in games such as Go, Dota 2, and StarCraft II. Yet, applying RL to real-world problems is still challenging. Two key obstacles that stand in the way of this goal are the generalization of trained policies to a broad set of environment conditions and developing policies that can handle partial observability. As we will see in this chapter, these are closely related challenges, for which we will present solution approaches.</p>
			<p>Here is what we will cover in this chapter:</p>
			<ul>
				<li>An overview of generalization and partial observability</li>
				<li>Domain randomization for generalization</li>
				<li>Using memory to overcome partial observability</li>
			</ul>
			<p>These topics are critical to understand to ensure the successful implementation of RL in real-world settings. So, let's dive right in!</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor240"/>An overview of generalization and partial observability</h1>
			<p>As in all <a id="_idIndexMarker1006"/>machine<a id="_idIndexMarker1007"/> learning, we want our RL models to work beyond training, and under a broad set of conditions during test time. Yet, when you start learning about RL, the concept of overfitting is not at the forefront of the discussion as opposed to supervised learning. In this section, we will compare overfitting and generalization between supervised learning and RL, describe how generalization is closely related to partial observability, and present a general recipe to handle these challenges. </p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor241"/>Generalization and overfitting in supervised learning</h2>
			<p>One of the most<a id="_idIndexMarker1008"/> important<a id="_idIndexMarker1009"/> goals in <a id="_idIndexMarker1010"/>supervised <a id="_idIndexMarker1011"/>learning, such as in image recognition and forecasting, is to prevent overfitting and achieve high accuracy on unseen data – after all, we already know the labels in our training data. We use various methods to this end:</p>
			<ul>
				<li>We use separate training, dev, and test sets for model training, hyperparameter selection, and model performance assessment, respectively. The model should not be modified based on the test set so that it gives a fair assessment of the model's performance.</li>
				<li>We use various regularization methods, such as penalizing model variance (for example, L1 and L2 regularization) and dropout methods, to prevent overfitting.</li>
				<li>We use as much data as possible to train the model, which itself has a regularization effect. In the absence of enough data, we leverage data augmentation techniques to generate more.</li>
				<li>We aim to have a dataset that is diverse and of the same distribution as the kind of data we expect to see after model deployment.</li>
			</ul>
			<p>These concepts come up at the very beginning when you learn about supervised learning, and they form the basis of how to train a model. In RL, though, we don't seem to have the same mindset when it comes to overfitting.</p>
			<p>Let's go into a bit more detail about what is different in RL, why, and whether generalization is actually of secondary importance or not.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor242"/>Generalization and overfitting in RL</h2>
			<p>Deep supervised<a id="_idIndexMarker1012"/> learning <a id="_idIndexMarker1013"/>notoriously requires a<a id="_idIndexMarker1014"/> lot <a id="_idIndexMarker1015"/>of data. But the hunger for data in deep RL dwarfs the need in deep supervised learning due to the noise in feedback signals and the complex nature of RL tasks. It is not uncommon to train RL models over billions of data points, and over many months. Since it is not quite possible to generate such large data using physical systems, deep RL research has leveraged digital environments, such as simulations and video games. This has blurred the distinction between training and testing. </p>
			<p>Think about it: if you train an RL agent to play an Atari game, such as Space Invaders, well (which is what most RL algorithms are benchmarked against) and use a lot of data for that, then if the agent plays Space Invaders very well after training, is there any problem with it? Well, in this case, no. However, as you may have realized, such a training workflow does not involve any effort to prevent overfitting as opposed to a supervised learning model training workflow. It may just be that your agent could have memorized a wide variety of scenes in the game. If you only care about Atari games, it may seem like overfitting is not a concern in RL.</p>
			<p>When we depart from Atari settings and, say, train an agent to beat human competitors, such as in Go, Dota 2, and StarCraft II, overfitting starts to appear as a bigger concern. As we saw in <a href="B14160_09_Final_SK_ePub.xhtml#_idTextAnchor200"><em class="italic">Chapter 9</em></a>, <em class="italic">Multi-Agent Reinforcement Learning</em>, these agents are usually trained through self-play. A major danger for the agents in this setting is them overfitting to each other's strategies. To prevent this, we usually train multiple agents and make them <a id="_idIndexMarker1016"/>play against each other in phases<a id="_idIndexMarker1017"/> so that a single agent sees a diverse set<a id="_idIndexMarker1018"/> of<a id="_idIndexMarker1019"/> opponents (the environment from the agent's perspective) and there is less chance of overfitting.</p>
			<p>Overfitting becomes a huge concern in RL, much more than the two situations we mentioned previously, when we train models in a simulation and deploy them in a physical environment. This is because, no matter how high-fidelity it is, a simulation is (almost) never the same as the<a id="_idIndexMarker1020"/> real world. This is called the <strong class="bold">sim2real gap</strong>. A simulation involves many assumptions, simplifications, and abstractions. It is only a model of the real world, and as we all know, all models are wrong. We work with models everywhere, so why has this all of a sudden become a major concern in RL? Well, again, the training and RL agent require lots and lots of data (and that's why we needed a simulation in the first place), and training any ML model over similar data for a long time is a recipe for overfitting. So, RL models are extremely likely to overfit to the patterns and quirks of the simulation. There, we really need RL policies to generalize beyond the simulation for them to be useful. This is a serious challenge for RL in general, and one of the biggest obstacles in the way of bringing RL to real applications.</p>
			<p>The sim2real gap is a concept that is closely related to partial observability. Let's look at this connection next.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor243"/>The connection between generalization and partial observability</h2>
			<p>We mentioned <a id="_idIndexMarker1021"/>that a simulation is <a id="_idIndexMarker1022"/>never the same as the real world. Two forms that this can manifest itself in are as follows:</p>
			<ul>
				<li>In some problems, you will never get the exact same observations in a simulation that you would get in the real world. Training a self-driving car is an example of that. The real scenes will always be different.</li>
				<li>In certain problems, you can train the agent on the same observations as you would see in the real world – for example, a factory production planning problem where the observation is the demand outlook, current inventory levels, and machine states. If you know what the ranges for your observations are, you can design your simulation to reflect that. Still, simulation and real life will be different.</li>
			</ul>
			<p>In the former case, it is more obvious why your trained agent may not generalize well beyond a simulation. However, the situation with the latter is a bit more subtle. In that case, although observations are the same, the world dynamics may not be between the two environments (admittedly, this will be also a problem for the former case). Can you recall what this is similar to? Partial observability. You can think of the gap between simulation and real world as a result of partial observability: there is a state of the environment hidden from the agent affecting the transition dynamics. So, even though the agent makes the same observation in simulation and the real world, it does not see this hidden state that we assume to capture the differences between them.</p>
			<p>Because of this connection, we cover generalization and partial observability together in this chapter. Having said that, generalization could be still of concern even in fully observable environments, and we may have to deal with partial observability even when generalization is not a major concern. We will look into these dimensions later.</p>
			<p>Next, let's briefly discuss how we can tackle each of these challenges, before going into the details in later sections.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor244"/>Overcoming partial observability with memory</h2>
			<p>Do you remember<a id="_idIndexMarker1023"/> what it was like when<a id="_idIndexMarker1024"/> you first entered your high school classroom? Chances are there were a lot of new faces and you wanted to make friends. However, you would not want to approach people just based on your first impression. Although a first impression does tell us something about people, it reflects only part of who they are. What you really want to do is to make observations over time before you make a judgment.</p>
			<p>The situation is similar in the context of RL. Take this example from the Atari game Breakout:</p>
			<div>
				<div id="_idContainer1428" class="IMG---Figure">
					<img src="image/B14160_11_01.jpg" alt="Figure 11.1 – A single frame of an Atari game gives a partial observation of the environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – A single frame of an Atari game gives a partial observation of the environment</p>
			<p>It is not very clear <a id="_idIndexMarker1025"/>where the ball is moving toward<a id="_idIndexMarker1026"/> from a single game scene. If we had another snapshot from a previous timestep, we could estimate the change in position and the velocity of the ball. One more snapshot could help us estimate the change in velocity, acceleration, and so on. So, when an environment is partially observable, taking actions based on not a single observation but a sequence of them results in more informed decisions. In other words, having a <strong class="bold">memory</strong> allows an RL agent to uncover what is not observable in a single observation.</p>
			<p>There are different ways of maintaining memory in RL models, and we will discuss them in more detail later in the chapter.</p>
			<p>Before doing so, let's also briefly discuss how to overcome overfitting.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor245"/>Overcoming overfitting with randomization</h2>
			<p>If you are a car<a id="_idIndexMarker1027"/> driver, think about how you have<a id="_idIndexMarker1028"/> gained experience over time about driving under different conditions. You may have driven small cars, large vehicles, ones that accelerate fast and slow, ones that are high and low riding, and so on. In addition, you probably have had driving experience in the rain, maybe snow, and on asphalt and gravel. I personally have had these experiences. So when I took a test drive with a Tesla Model S for the first time, it felt like a quite different experience at first. But after several minutes, I got used to it and started driving pretty comfortably.</p>
			<p>Now, as a driver, we often cannot define precisely what differs from one vehicle to another: what the exact differences in weight, torque, traction, and so on are that make the environment partially observable for us. But our past experience under diverse driving conditions helps us quickly adapt to new ones after several minutes of driving. How does this happen? Our brains are able to develop a general physics model for driving (and act accordingly) when the experience is diverse, rather than "overfitting" the driving style to a particular car model and condition.</p>
			<p>The way we will deal with overfitting and achieve generalization will be similar for our RL agents. We will expose the agent to many different environment conditions, including ones that it cannot necessarily fully observe, which is called <strong class="bold">domain randomization</strong> (<strong class="bold">DR</strong>). This<a id="_idIndexMarker1029"/> will give the agent the experience that is necessary to generalize beyond the simulation and the conditions it is trained under.</p>
			<p>In addition to that, the regularization methods we use in supervised learning are also helpful with RL models, as we will discuss.</p>
			<p>Next, let's summarize our discussion related to generalization in the next section.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor246"/>Recipe for generalization</h2>
			<p>As it should be clear <a id="_idIndexMarker1030"/>now after going through the preceding examples, we need three ingredients to achieve generalization:</p>
			<ul>
				<li>Diverse environment conditions to help the agent enrich its experience</li>
				<li>Model memory to help the agent uncover the underlying conditions in the environment, especially if the environment is partially observable</li>
				<li>Using regularization methods as in supervised learning</li>
			</ul>
			<p>Now it is time to make the discussion more concrete and talk about specific methods to handle generalization and partial observability. We start with using domain randomization for<a id="_idIndexMarker1031"/> generalization, and then discuss using memory to overcome partial observability.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor247"/>Domain randomization for generalization</h1>
			<p>We mentioned <a id="_idIndexMarker1032"/>previously how diversity of experience helps with generalization. In RL, we achieve this by randomizing the environment parameters during training, known as DR. Examples of these parameters, say, for a robot hand that carries and manipulates objects, could be as follows:</p>
			<ul>
				<li>Friction coefficient on the object surface</li>
				<li>Gravity</li>
				<li>Object shape and weight</li>
				<li>Power going into actuators</li>
			</ul>
			<p>DR is especially popular in robotics applications to overcome the sim2real gap since the agents are usually trained in a simulation and deployed in the physical world. However, whenever generalization is of concern, DR is an essential part of the training process.</p>
			<p>In order to get more specific about how environment parameters are randomized, we need to discuss how two environments representing the same type of problems could differ.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor248"/>Dimensions of randomization</h2>
			<p>Borrowed from <a id="_idIndexMarker1033"/><em class="italic">Rivilin</em>,<em class="italic"> 2019</em>, a useful categorization of differences between similar environments is shown in the following sections.</p>
			<h3>Different observations for the same/similar states</h3>
			<p>In this case, two<a id="_idIndexMarker1034"/> environments emit different observations although the underlying state and transition functions are the same or very similar. An example of this is the same Atari game scene but with different backgrounds and texture colors. Nothing is different in the game state, but the observations are different. A more realistic example would be, while training a <a id="_idIndexMarker1035"/>self-driving car, having a more "cartoonish" look in the simulation versus a realistic one from the real camera input even for the exact same scene. </p>
			<h4>Solution – adding noise to observations</h4>
			<p>In these cases, what <a id="_idIndexMarker1036"/>can help with generalization is to add noise to observations, so that the RL model focuses on the patterns in the observations that actually matter, rather than overfitting to irrelevant details. Soon, we will discuss a specific approach, called <strong class="bold">network randomization</strong>, that <a id="_idIndexMarker1037"/>will address this problem.</p>
			<h3>Same/similar observations for different states</h3>
			<p>Another way that<a id="_idIndexMarker1038"/> POMDPs could differ from each other is when observations are the same/similar, but the underlying states are actually different, also called <strong class="bold">aliased states</strong>. A simple <a id="_idIndexMarker1039"/>example is two separate versions of the mountain car environment with the exact same look but different gravity. These situations are pretty common in practice with robotics applications. Consider the manipulation of objects with a robot hand, as in the famous OpenAI work (which we will discuss in detail later in the chapter): the friction, weight, actual power going into the actuators, and so on could differ between different environment versions in ways that are not observable to the agent.</p>
			<h4>Solution – training with randomized environment parameters and using memory</h4>
			<p>The approach<a id="_idIndexMarker1040"/> to take here is to train the agent in <a id="_idIndexMarker1041"/>many different versions of the environment with different underlying parameters, along with memory in the RL model. This will help the agent uncover the underlying environment characteristics. A famous example here is OpenAI's robot hand manipulating objects trained in simulation, which is prone to the sim2real gap. We can overcome this by randomizing simulation parameters for the following scenario:</p>
			<p>When used with memory, and having been trained in a vast majority of environment conditions, the<a id="_idIndexMarker1042"/> policy will hopefully gain <a id="_idIndexMarker1043"/>the ability of adapting to the environment it finds itself in.</p>
			<h3>Different levels of complexity for the same problem class</h3>
			<p>This is the case<a id="_idIndexMarker1044"/> where we are essentially dealing with the same type of problem but at different levels of complexity. A nice example from <em class="italic">Rivlin</em>, <em class="italic">2019</em>, is the <strong class="bold">traveling</strong> <strong class="bold">salesman</strong> <strong class="bold">problem</strong> (<strong class="bold">TSP</strong>) with a different number of nodes on a graph. The RL agent's job in this <a id="_idIndexMarker1045"/>environment is to decide which node to visit next at every time step so that all nodes are visited exactly once at minimum cost, while going back to the initial node at the end. In fact, many problems we deal within RL naturally face this type of challenge, such as training a chess agent against opponents of different levels of expertise and so on.</p>
			<h4>Solution – training at varying levels of complexity with a curriculum</h4>
			<p>Not so surprisingly, training<a id="_idIndexMarker1046"/> the agent in <a id="_idIndexMarker1047"/>environments with varying levels of difficulty is necessary to achieve generalization here. Having said that, using a curriculum that starts with easy environment configurations and gets gradually more difficult, as we described earlier in the book. This will potentially make the learning more efficient, and even feasible in some cases that would have been infeasible without a curriculum.</p>
			<p>Now that we have covered high-level approaches to achieve generalization in different dimensions, we will go into some specific algorithms. But first, let's introduce a benchmark environment used to quantify generalization.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor249"/>Quantifying generalization</h2>
			<p>There are various<a id="_idIndexMarker1048"/> ways of testing whether certain algorithms/approaches generalize to unseen environment conditions better than others, such as the following:</p>
			<ul>
				<li>Creating validation and test environments with separate sets of environment parameters</li>
				<li>Assessing policy performance in a real-life deployment</li>
			</ul>
			<p>It is not always practical to do the latter as a real-life deployment may not necessarily be an option. The challenge with the former is to have consistency and to ensure that validation/test data is indeed not used in training. Also, it is possible to overfit to the validation environment when too many models are tried based on validation performance. One approach to overcome these challenges is to use procedurally generated environments. To this end, OpenAI has created the CoinRun environment to benchmark algorithms<a id="_idIndexMarker1049"/> on their generalization capabilities. Let's look into it in more detail.</p>
			<h3>CoinRun environment</h3>
			<p>The CoinRun environment <a id="_idIndexMarker1050"/>is about a character<a id="_idIndexMarker1051"/> trying to collect the coin at the level it is in without colliding with any obstacles. The character starts at the far left and the coin is at the far right. The levels are generated procedurally from an underlying probability distribution at various levels of difficulty, as shown here:</p>
			<div>
				<div id="_idContainer1429" class="IMG---Figure">
					<img src="image/B14160_11_02.jpg" alt="Figure 11.2: Two levels in CoinRun with different levels of difficulty (source: Cobbe et al., 2018)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2: Two levels in CoinRun with different levels of difficulty (source: Cobbe et al., 2018)</p>
			<p>Here are more details about the environment's reward function and terminal conditions:</p>
			<ul>
				<li>There are dynamic and static obstacles, causing the character to die when it collides with them, which terminates the episode.</li>
				<li>The reward is only given when the coin is collected, which also terminates the episode.</li>
				<li>There is a time limit of 1,000 time steps before the episode terminates, unless the character dies or the coin is reached.</li>
			</ul>
			<p>Note that the CoinRun environment generates all (training and test) levels from the same distribution, so<a id="_idIndexMarker1052"/> it does not test the out-of-distribution (extrapolation) performance <a id="_idIndexMarker1053"/>of a policy. </p>
			<p>Next, let's install this environment and experiment with it.</p>
			<h3>Installing the CoinRun environment</h3>
			<p>You can follow <a id="_idIndexMarker1054"/>these steps to install the<a id="_idIndexMarker1055"/> CoinRun environment:</p>
			<ol>
				<li>We start with setting up and activating a virtual Python environment since CoinRun needs specific packages. So, in your directory of choice, run the following:<p class="source-code"><strong class="bold">virtualenv coinenv</strong></p><p class="source-code"><strong class="bold">source coinenv/bin/activate</strong></p></li>
				<li>Then, we install the necessary Linux packages, including a famous parallel computing interface, MPI:<p class="source-code"><strong class="bold">sudo apt-get install mpich build-essential qt5-default pkg-config</strong></p></li>
				<li>Then, we install the Python dependencies and the CoinRun package fetched from the GitHub repo:<p class="source-code"><strong class="bold">git clone https://github.com/openai/coinrun.git</strong></p><p class="source-code"><strong class="bold">cd coinrun</strong></p><p class="source-code"><strong class="bold">pip install tensorflow==1.15.3 # or tensorflow-gpu</strong></p><p class="source-code"><strong class="bold">pip install -r requirements.txt</strong></p><p class="source-code"><strong class="bold">pip install -e .</strong></p><p>Note that we had to install an old TensorFlow version. Officially, TensorFlow 1.12.0 is suggested by the creators of CoinRun. However, using a later TensorFlow 1.x version could help you avoid CUDA conflicts while using the GPU.</p></li>
				<li>You can test the environment out with your keyboard's arrow keys with the following command:<p class="source-code"><strong class="bold">python -m coinrun.interactive</strong></p></li>
			</ol>
			<p>Great; enjoy playing CoinRun! I suggest you familiarize yourself with it to gain a better understanding of the comparisons we will go into later.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">You can visit the CoinRun GitHub repo for the full set of commands at <a href="https://github.com/openai/coinrun">https://github.com/openai/coinrun</a>.</p>
			<p>The paper<a id="_idIndexMarker1056"/> that <a id="_idIndexMarker1057"/>introduced the environment (<em class="italic">Cobbe et al., 2018</em>) also mentions how various regularization techniques affects generalization in RL. We will discuss this next and then go into other approaches for generalization.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor250"/>The effect of regularization and network architecture on the generalization of RL policies</h2>
			<p>The authors<a id="_idIndexMarker1058"/> found that several<a id="_idIndexMarker1059"/> techniques used in supervised learning to prevent overfitting are also helpful in RL. Since reproducing the results in the paper takes a really long time, where each experiment takes hundreds of millions of steps, we will not attempt to do it here. Instead, we have provided you with a summary of the results and the commands to run various versions of algorithms. But you can observe that, even after 500k time steps, applying the regularization techniques we mention as follows improves the test performance.</p>
			<p>You can see all the training options for this environment using the following command:</p>
			<p class="source-code">python -m coinrun.train_agent –help</p>
			<p>Let's start with running a baseline Define acronym without any regularizations applied.</p>
			<h3>Vanilla training</h3>
			<p>You can train<a id="_idIndexMarker1060"/> an<a id="_idIndexMarker1061"/> RL agent using PPO with an Impala architecture without any improvements for generalization, as follows:</p>
			<p class="source-code">python -m coinrun.train_agent --run-id BaseAgent --num-levels 500 --num-envs 60</p>
			<p>Here, <strong class="source-inline">BaseAgent</strong> is an ID for your agent that is decided by you, <strong class="source-inline">--num-levels 500</strong> says to use 500 game levels during training with the default seed used in the paper, and <strong class="source-inline">--num-envs 60</strong> kicks off 60 parallel environments for rollouts, which you can adjust according to the number of CPUs available on your machine.</p>
			<p>In order to test the trained agent in three parallel sessions, with 20 parallel environments in each and five levels for each environment, you can run the following command:</p>
			<p class="source-code">mpiexec -np 3 python -m coinrun.enjoy --test-eval --restore-id BaseAgent -num-eval 20 -rep 5</p>
			<p>The average test reward will be specified in <strong class="source-inline">mpi_out</strong>. In my case, the reward went from around 5.5 in training after 300K time steps to 0.8 in the test environment to give you an idea.</p>
			<p>Also, you can watch your trained agent by running the following:</p>
			<p class="source-code">python -m coinrun.enjoy --restore-id BaseAgent –hres</p>
			<p>This is actually quite fun to do.</p>
			<h3>Using a larger network</h3>
			<p>The authors found <a id="_idIndexMarker1062"/>that, as <a id="_idIndexMarker1063"/>in supervised learning, using a larger neural network increases generalization by successfully solving more test episodes as it comes with a higher capacity. They also note that, however, there are diminishing returns for generalization with the increase in size, so generalization won't improve linearly with the network size.</p>
			<p>To use an architecture with five residual blocks instead of three with double the number of channels in each layer, you can add the <strong class="source-inline">impalalarge</strong> argument:</p>
			<p class="source-code">python -m coinrun.train_agent --run-id LargeAgent --num-levels 500 --num-envs 60 --architecture impalalarge</p>
			<p>Again, you <a id="_idIndexMarker1064"/>can run <a id="_idIndexMarker1065"/>test evaluations with the run ID that you provided for the large agent case.</p>
			<h3>Diversity in training data</h3>
			<p>In order to test the<a id="_idIndexMarker1066"/> importance of <a id="_idIndexMarker1067"/>diverse training data, the authors compared two types of training, both with a total of 256M time steps, across 100 and then 10,000 game levels (controlled by <strong class="source-inline">--num-levels</strong>). The test performance went from 30% to a 90%+ success rate (which is also on par with the training performance) with the more diverse data.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Increasing data diversity acts as a regularizer in supervised learning and RL. This is because the model would have to explain more variation with the same model capacity as diversity increases, forcing it to use its capacity to focus on the most important patterns in the input, rather than overfitting to noise.</p>
			<p>This emphasizes the importance of the randomization of the environment parameters to achieve generalization, which we will talk about separately later in the chapter.</p>
			<h3>Dropout and L2 regularization</h3>
			<p>The experiment <a id="_idIndexMarker1068"/>results show <a id="_idIndexMarker1069"/>both<a id="_idIndexMarker1070"/> dropout <a id="_idIndexMarker1071"/>and L2 regularization improve generalization, bringing in around an additional 5% and 8% success on top of around a 79% baseline test performance. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If you need a refresher on dropout and L2 regularization, check out Chitta Ranjan's blog at <a href="https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275">https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275</a>.</p>
			<p>We can explore this in more detail as follows:</p>
			<ul>
				<li>Empirically, the authors find the best L2 weight as <img src="image/Formula_11_001.png" alt=""/> and the best dropout rate as 0.1.</li>
				<li>L2 regularization, empirically, has a higher impact on generalization compared to dropout.</li>
				<li>As expected, training with dropout is slower to converge, to which twice as many time steps (512M) are allocated.</li>
			</ul>
			<p>To use a dropout with a rate of 0.1 on top of the vanilla agent, you can use the following command:</p>
			<p class="source-code">python -m coinrun.train_agent --run-id AgentDOut01 --num-levels 500 --num-envs 60 --dropout 0.1</p>
			<p>Similarly, to <a id="_idIndexMarker1072"/>use L2 normalization<a id="_idIndexMarker1073"/> with a weight of 0.0001, execute the following:</p>
			<p class="source-code">python -m coinrun.train_agent --run-id AgentL2_00001 --num-levels 500 --num-envs 60 --l2-weight 0.0001</p>
			<p>In your TensorFlow RL model, you can add dropout using the <strong class="source-inline">Dropout</strong> layer, as follows:</p>
			<p class="source-code">from tensorflow.keras import layers </p>
			<p class="source-code">...</p>
			<p class="source-code">x = layers.Dense(512, activation="relu")(x) </p>
			<p class="source-code">x = layers.Dropout(0.1)(x)</p>
			<p class="source-code">...</p>
			<p>To add L2 regularization, do something like this:</p>
			<p class="source-code">from tensorflow.keras import regularizers</p>
			<p class="source-code">...</p>
			<p class="source-code">x = layers.Dense(512, activation="relu", kernel_regularizer=regularizers.l2(0.0001))(x) </p>
			<p class="callout-heading">Info</p>
			<p class="callout">TensorFlow<a id="_idIndexMarker1074"/> has a very <a id="_idIndexMarker1075"/>nice <a id="_idIndexMarker1076"/>tutorial on overfitting and<a id="_idIndexMarker1077"/> underfitting, which you might want to check out: <a href="https://www.tensorflow.org/tutorials/keras/overfit_and_underfit">https://www.tensorflow.org/tutorials/keras/overfit_and_underfit</a>.</p>
			<p>Next, let's discuss data augmentation.</p>
			<h3>Using data augmentation</h3>
			<p>A common method <a id="_idIndexMarker1078"/>to prevent overfitting is data augmentation, which is modifying/distorting <a id="_idIndexMarker1079"/>the input, mostly randomly, so that the diversity in the training data increases. When used on images, these techniques include random cropping, changing the brightness and sharpness, and so on. An example CoinRun scene with data augmentation used looks like the following:</p>
			<div>
				<div id="_idContainer1431" class="IMG---Figure">
					<img src="image/B14160_11_03.jpg" alt="Figure 11.3 – CoinRun with data augmentation (source: Cobbe et al., 2018)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – CoinRun with data augmentation (source: Cobbe et al., 2018)</p>
			<p class="callout-heading">Info</p>
			<p class="callout">For a TensorFlow tutorial on data augmentation, check out <a href="https://www.tensorflow.org/tutorials/images/data_augmentation">https://www.tensorflow.org/tutorials/images/data_augmentation</a>.</p>
			<p>Data<a id="_idIndexMarker1080"/> augmentation, as it turns out, is also helpful in RL, and it gives a lift in the test performance<a id="_idIndexMarker1081"/> that is slightly worse than L2 regularization.</p>
			<h3>Using batch normalization</h3>
			<p>Batch <a id="_idIndexMarker1082"/>normalization is<a id="_idIndexMarker1083"/> one of the key tools in deep learning to stabilize training as well as to prevent overfitting. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">If you need a refresher on batch normalization, check out Chris Versloot's blog at <a href="https://bit.ly/3kjzjno">https://bit.ly/3kjzjno</a>.</p>
			<p>In the CoinRun environment, you can enable the batch normalization layers in the training as follows:</p>
			<p class="source-code">python -m coinrun.train_agent --run-id AgentL2_00001 --num-levels 500 --num-envs 60 --use-data-augmentation 1</p>
			<p>This adds a batch normalization layer after every convolutional layer.</p>
			<p>When you implement your own TensorFlow model, the syntax for the batch normalization layer is <strong class="source-inline">layers.BatchNormalization()</strong>, with some optional arguments<a id="_idIndexMarker1084"/> you can pass.</p>
			<p>The reported <a id="_idIndexMarker1085"/>results show that using batch normalization gives the second-best lift to the test performance among all of the other regularization methods (except increasing the diversity in the training data).</p>
			<h3>Adding stochasticity</h3>
			<p>Finally, introducing <a id="_idIndexMarker1086"/>stochasticity/noise into the environment turns out to be the most<a id="_idIndexMarker1087"/> useful generalization technique, with around a 10% lift to the test performance. Two methods are tried in the paper with the PPO algorithm during training:</p>
			<ul>
				<li>Using <img src="image/Formula_11_002.png" alt=""/>-greedy actions (which are usually used with Q-learning approaches)</li>
				<li>Increasing the entropy bonus coefficient (<img src="image/Formula_11_003.png" alt=""/>) in PPO, which encourages variance in the actions suggested by the policy</li>
			</ul>
			<p>Some good hyperparameter choices for these methods are <img src="image/Formula_11_004.png" alt=""/> and <img src="image/Formula_11_005.png" alt=""/>, respectively. Something worth noting is that if the environment dynamics are already highly<a id="_idIndexMarker1088"/> stochastic, introducing more stochasticity may not be as impactful. </p>
			<h3>Combining all the methods</h3>
			<p>Using all these <a id="_idIndexMarker1089"/>regularization methods together during training only slightly improved<a id="_idIndexMarker1090"/> the boost obtained from the individual methods, suggesting that each of these methods plays a similar role to prevent overfitting. Note that it is not quite possible to say that these methods will have a similar impact on all RL problems. What we need to keep in mind, though, is that the traditional regularization methods used in supervised learning can have a significant impact on generalizing RL policies as well.</p>
			<p>This concludes our discussion on the fundamental regularization techniques for RL. Next, we will look into another method that followed the original CoinRun paper, which is network randomization.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor251"/>Network randomization and feature matching</h2>
			<p>Network <a id="_idIndexMarker1091"/>randomization, proposed by Lee et al., 2020, simply involves using a <a id="_idIndexMarker1092"/>random transformation of observations, <img src="image/Formula_11_006.png" alt=""/>, as follows:</p>
			<div>
				<div id="_idContainer1437" class="IMG---Figure">
					<img src="image/Formula_11_007.jpg" alt=""/>
				</div>
			</div>
			<p>Then, the transformed observation, <img src="image/Formula_11_008.png" alt=""/>, is fed as input to the regular policy network used in the RL algorithm. Here, <img src="image/Formula_11_009.png" alt=""/> is the parameter of this transformation, which is randomly initialized at every training iteration. This can be simply achieved by adding, right after the input layer, a layer that is not trainable and re-initialized periodically. In TensorFlow 2, a randomization layer that transforms the input after each call could be implemented as follows:</p>
			<p class="source-code">class RndDense(tf.keras.layers.Layer):</p>
			<p class="source-code">    def __init__(self, units=32):</p>
			<p class="source-code">        super(RndDense, self).__init__()</p>
			<p class="source-code">        self.units = units</p>
			<p class="source-code">    def build(self, input_shape):  </p>
			<p class="source-code">        self.w_init = tf.keras.initializers.GlorotNormal()</p>
			<p class="source-code">        self.w_shape = (input_shape[-1], self.units)</p>
			<p class="source-code">        self.w = tf.Variable(</p>
			<p class="source-code">            initial_value=self.w_init(shape=self.w_shape, dtype="float32"),</p>
			<p class="source-code">            trainable=True,</p>
			<p class="source-code">        )</p>
			<p class="source-code">    def call(self, inputs):  </p>
			<p class="source-code">        self.w.assign(self.w_init(shape=self.w_shape, dtype="float32"))</p>
			<p class="source-code">        return tf.nn.relu(tf.matmul(inputs, self.w))</p>
			<p>Note that this custom layer exhibits the following traits:</p>
			<ul>
				<li>Has weights that are not trainable</li>
				<li>Assigns random weights to the layer at each call</li>
			</ul>
			<p>A further improvement to this architecture is to do two forward passes, with and without randomized inputs, and enforce the network to give similar outputs. This can be achieved by adding a loss to the RL objective that penalizes the difference:</p>
			<div>
				<div id="_idContainer1440" class="IMG---Figure">
					<img src="image/Formula_11_010.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_11_011.png" alt=""/> is the <a id="_idIndexMarker1093"/>parameters of the policy network, and <img src="image/Formula_11_012.png" alt=""/> is the second-from-last <a id="_idIndexMarker1094"/>layer in the policy network (that is, the one right before the layer that outputs the actions). This is called <strong class="bold">feature matching</strong> and makes the network differentiate noise from the signal in the input.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The TensorFlow 1.x implementation of this architecture for the CoinRun environment is available at <a href="https://github.com/pokaxpoka/netrand">https://github.com/pokaxpoka/netrand</a>. Compare it to the original CoinRun environment by comparing <strong class="source-inline">random_ppo2.py</strong> with <strong class="source-inline">ppo2.py</strong> and the <strong class="source-inline">random_impala_cnn</strong> method to the <strong class="source-inline">impala_cnn</strong> method under <strong class="source-inline">policies.py</strong>.</p>
			<p>Going back to the dimensions of generalization we mentioned earlier, network randomization helps RL policies generalize in all of the three dimensions.</p>
			<p>Next, we will discuss<a id="_idIndexMarker1095"/> one of the key approaches to achieve generalization with proven real-life <a id="_idIndexMarker1096"/>successes.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor252"/>Curriculum learning for generalization</h2>
			<p>We already <a id="_idIndexMarker1097"/>discussed how a rich training experience<a id="_idIndexMarker1098"/> helps an RL policy to better generalize. Let's assume that for your robotics application, you have identified two parameters to randomize in your environment with some minimum and maximum values:</p>
			<ul>
				<li>Friction: <img src="image/Formula_11_013.png" alt=""/></li>
				<li>Actuator torque: <img src="image/Formula_11_014.png" alt=""/></li>
			</ul>
			<p>The goal here is to prepare the agent to act in an environment with an unknown friction-torque combination at test time.</p>
			<p>It turns out that, as we mentioned in the previous chapter when we discussed curriculum learning, the training may result in a mediocre agent if you try to train it over the entire range for these parameters right at the beginning. That is because the extreme values of the parameter ranges are likely to be too challenging (assuming they are centered around some reasonable values for the respective parameters) for the agent who has not even figured out the basics of the task. The idea behind curriculum learning is to start with easy scenarios, such as having the first lesson as <img src="image/Formula_11_015.png" alt=""/> and <img src="image/Formula_11_016.png" alt=""/>, and gradually increase the difficulty in the next lessons by expanding the ranges.</p>
			<p>Then, a key question is how we should construct the curriculum, what the lessons should look like (that is, what the next range of parameters after the agent succeeds in the current lesson should be), and when to declare success for the existing lesson. In this section, we discuss two methods for curriculum learning that automatically generate and <a id="_idIndexMarker1099"/>manage the curriculum for effective domain<a id="_idIndexMarker1100"/> randomization.</p>
			<h3>Automatic domain randomization</h3>
			<p><strong class="bold">Automatic domain randomization</strong> (<strong class="bold">ADR</strong>) is a method proposed by OpenAI in their research of using a<a id="_idIndexMarker1101"/> robot hand to manipulate a Rubik's cube. It is one of the most successful robotics applications of RL for several reasons:</p>
			<ul>
				<li>Dexterous robots are notoriously difficult to control due to their high degrees of freedom.</li>
				<li>The policy is trained completely in simulation and then successfully transferred to a physical robot, successfully bridging the sim2real gap.</li>
				<li>During test time, the robot succeeded under conditions that it had never seen during training, such as the fingers being tied, having a rubber glove on, perturbations with various objects, and so on.<p>These are phenomenal results in terms of the generalization capability of the trained policy.</p><p class="callout-heading">Info</p><p class="callout">You should check out the blog post for this important research at <a href="https://openai.com/blog/solving-rubiks-cube/">https://openai.com/blog/solving-rubiks-cube/</a>. It contains great visualizations and insights into the results.</p></li>
			</ul>
			<p>ADR was the key method in the success of this application. Next, we will discuss how ADR works.</p>
			<h4>The ADR algorithm</h4>
			<p>Each environment<a id="_idIndexMarker1102"/> we create during training is randomized over certain parameters, such as friction and torque, in the preceding example. To denote this formally, we say that an environment, <img src="image/Formula_11_017.png" alt=""/>, is parametrized by <img src="image/Formula_11_018.png" alt=""/>, where <img src="image/Formula_11_019.png" alt=""/> is the number of parameters (2, in this example). When an environment is created, we sample <img src="image/Formula_11_020.png" alt=""/> from a distribution, <img src="image/Formula_11_021.png" alt=""/>. What ADR adjusts is <img src="image/Formula_11_022.png" alt=""/> of the parameter distribution, therefore changing the likelihood of different parameter samples, making the environment more difficult or easier, depending on whether the agent is successful in the current difficulty. </p>
			<p>An example, <img src="image/Formula_11_023.png" alt=""/>, would consist of uniform distributions for each parameter dimension, <img src="image/Formula_11_024.png" alt=""/>, with <img src="image/Formula_11_025.png" alt=""/>. Connecting with our example, <img src="image/Formula_11_026.png" alt=""/> would correspond to the friction <a id="_idIndexMarker1103"/>coefficient, <img src="image/Formula_11_027.png" alt=""/>. Then, for the initial lesson, we would have <img src="image/Formula_11_028.png" alt=""/>. This would be similar for the torque parameter, <img src="image/Formula_11_029.png" alt=""/>. Then, <img src="image/Formula_11_030.png" alt=""/> becomes the following:</p>
			<div>
				<div id="_idContainer1461" class="IMG---Figure">
					<img src="image/Formula_11_031.jpg" alt=""/>
				</div>
			</div>
			<p>ADR suggests the following:</p>
			<ul>
				<li>As the training continues, allocate some of the environments for evaluation to decide whether to update <img src="image/Formula_11_032.png" alt=""/>.</li>
				<li>In each evaluation environment, pick a dimension, <img src="image/Formula_11_033.png" alt=""/>, then pick either the upper or lower bound to focus on, such as <img src="image/Formula_11_034.png" alt=""/> and <img src="image/Formula_11_035.png" alt=""/>.</li>
				<li>Fix the <a id="_idIndexMarker1104"/>environment parameter for the dimension picked to the bound chosen. Sample the rest of the parameters from <img src="image/Formula_11_036.png" alt=""/>.</li>
				<li>Evaluate the agent's performance in the given environment and keep the total reward obtained in that episode in a buffer associated with the dimension and the bound (for example, <img src="image/Formula_11_037.png" alt=""/>).</li>
				<li>When there are enough results in the buffer, compare the average reward to the success and failure thresholds you had chosen a priori.</li>
				<li>If the average performance for the given dimension and bound is above your success threshold, expand the parameter range for the dimension; if it is below the failure threshold, decrease the range.</li>
			</ul>
			<p>In summary, ADR systematically evaluates the agent performance for each parameter dimension at the boundaries of the parameter range, then expands or shrinks the range depending on the agent performance. You can refer to the paper for pseudo-code of the ADR algorithm, which should be easy to follow with the preceding explanations.</p>
			<p>Next, let's discuss another important method for automatic curriculum generation.</p>
			<h3>Absolute learning progress with Gaussian mixture models</h3>
			<p>Another method for <a id="_idIndexMarker1105"/>automatic curriculum generation is the <strong class="bold">absolute learning progress with Gaussian mixture models</strong> (<strong class="bold">ALP-GMM</strong>) method. The essence of this approach is as follows:</p>
			<ul>
				<li>To identify the parts of the environment parameter space that show the most learning progress (called the ALP value)</li>
				<li>To fit multiple GMM models to the ALP data, with <img src="image/Formula_11_038.png" alt=""/>number of kernels, then select the best one</li>
				<li>To sample the environment parameters from the best GMM model</li>
			</ul>
			<p>This idea has roots in cognitive science and is used to model the early vocal developments of infants.</p>
			<p>The ALP score for a newly sampled parameter vector, <img src="image/Formula_11_039.png" alt=""/>, is calculated as follows:</p>
			<div>
				<div id="_idContainer1470" class="IMG---Figure">
					<img src="image/Formula_11_040.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_11_041.png" alt=""/> is the episode reward obtained with <img src="image/Formula_11_042.png" alt=""/>, <img src="image/Formula_11_043.png" alt=""/> is the closest parameter vector that was obtained in a previous episode, and <img src="image/Formula_11_044.png" alt=""/> is the episode reward associated with <img src="image/Formula_11_045.png" alt=""/>. All the <img src="image/Formula_11_046.png" alt=""/>) pairs are kept in a database, denoted by <img src="image/Formula_11_047.png" alt=""/>, with which the ALP scores are calculated. The GMM model, however, is obtained using the most recent <img src="image/Formula_11_048.png" alt=""/> <img src="image/Formula_11_049.png" alt=""/> pairs.</p>
			<p>Note that the parts of the parameter space that have high ALP scores are more likely to be sampled to generate new environments. A high ALP score shows potential for learning for that region, and it can be obtained by observing a large drop or increase in episode reward with the newly sampled <img src="image/Formula_11_050.png" alt=""/>.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The code repo of the ALP-GMM paper is available at <a href="https://github.com/flowersteam/teachDeepRL">https://github.com/flowersteam/teachDeepRL</a>, which also contains animations that show how <a id="_idIndexMarker1106"/>the algorithm works. We are not able to go into the details of the repo here due to space limitations, but I highly recommend you take a look at the implementation and the results.</p>
			<p>Finally, we will present some additional resources on generalization for further reading.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor253"/>Sunblaze environment</h2>
			<p>We don't have space to <a id="_idIndexMarker1107"/>cover all methods for generalization in this book, but a useful resource for you to check out is a blog by Packer &amp; Gao, 2019, which introduces the sunblaze environments to systematically assess generalization approaches for RL. These environments are modifications to the classic OpenAI Gym environments, which are parametrized to test the interpolation and extrapolation performance of algorithms.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">You can find the blog post describing the sunblaze environments and the results at <a href="https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/">https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/</a>.</p>
			<p>Terrific job! You have learned about one of the most important topics concerning real-world RL! Next, we will<a id="_idIndexMarker1108"/> look at a closely connected topic, which is overcoming partial observability.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor254"/>Using memory to overcome partial observability</h1>
			<p>At the beginning <a id="_idIndexMarker1109"/>of the chapter, we described how memory is a useful structure to handle partial observability. We also mentioned that the problem of generalization can often be thought of as the result of partial observability: </p>
			<ul>
				<li>A hidden state differentiating two environments, such as a simulation and the real world, can be uncovered through memory. </li>
				<li>When we implement domain randomization, we aim to create many versions of our training environment in which we expect the agent to build an overarching model for the world dynamics. </li>
				<li>Through memory, we hope that the agent can identify the characteristics of the environment it is in, even if it had not seen that particular environment during training, and then act accordingly.</li>
			</ul>
			<p>Now, memory for a model is nothing more than a way of processing a sequence of observations as the input to the agent policy. If you have worked with other types of sequence data with neural<a id="_idIndexMarker1110"/> networks, such as in time-series prediction or <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), you can adopt similar approaches to use observation memory as the input for your RL model.</p>
			<p>Let's go into more details of how this can be done.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor255"/>Stacking observations</h2>
			<p>A simple way <a id="_idIndexMarker1111"/>of passing an observation sequence to the model is to stitch them together and treat this stack as a single observation. Denoting the observation at time <img src="image/Formula_11_051.png" alt=""/> as <img src="image/Formula_11_052.png" alt=""/>, we can form a new observation, <img src="image/Formula_11_053.png" alt=""/>, to be passed to the model, as follows:</p>
			<div>
				<div id="_idContainer1484" class="IMG---Figure">
					<img src="image/Formula_11_054.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_11_055.png" alt=""/> is the length of the memory. Of course, for <img src="image/Formula_11_056.png" alt=""/>, we need to somehow initialize the earlier parts of the memory, such as using vectors of zeros that are the same dimension as <img src="image/Formula_11_057.png" alt=""/>. </p>
			<p>In fact, simply stacking observations is how the original DQN work handled the partial observability<a id="_idIndexMarker1112"/> in the Atari environments. In more detail, the steps of that preprocessing are as follows:</p>
			<ol>
				<li value="1">A rescaled <img src="image/Formula_11_058.png" alt=""/> RGB frame of the screen is obtained.</li>
				<li>The Y channel (luminance) is extracted to further compress the frame into an <img src="image/Formula_11_059.png" alt=""/> image. That makes a single observation, <img src="image/Formula_11_060.png" alt=""/>.</li>
				<li><img src="image/Formula_11_061.png" alt=""/> most recent frames are concatenated to an <img src="image/Formula_11_062.png" alt=""/>image, forming an observation with memory for the model, <img src="image/Formula_11_063.png" alt=""/>.</li>
			</ol>
			<p>Note that only the last step is about the memory and the former steps are not strictly necessary. </p>
			<p>The obvious benefit of this method is that it is super simple, and the resulting model is easy to train. The downside is, though, that this is not the best way of handling sequence data, which should not be surprising to you if you have dealt with time-series problems or NLP before. Here is an example of why.</p>
			<p>Consider the following sentence you speak to your virtual voice assistant, such as Apple's Siri:</p>
			<p><em class="italic">"Buy me a plane ticket from San Francisco to Boston."</em></p>
			<p>This is the same as saying the following:</p>
			<p><em class="italic">"Buy me a plane ticket to Boston from San Francisco"</em></p>
			<p>Assuming <a id="_idIndexMarker1113"/>each word is passed to an input neuron, it is hard for the neural network to readily interpret them as the same sentences because normally, each input neuron expects a specific input. In this structure, you would have to train your network with all different combinations of this sentence. A further complexity is that your input size is fixed, but each sentence could be of a different length. You can extend this thought to RL problems as well.</p>
			<p>Now, stacking observations is good enough in most problems, such as Atari games. But if you are trying to teach your model how to play Dota 2, a strategy video game, then you are out of luck. </p>
			<p>Fortunately, <strong class="bold">recurrent</strong> <strong class="bold">neural</strong> <strong class="bold">networks</strong> (<strong class="bold">RNNs</strong>) come to the rescue.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor256"/>Using RNNs</h2>
			<p>RNNs are <a id="_idIndexMarker1114"/>designed to handle<a id="_idIndexMarker1115"/> sequence data. A famous RNN variant, <strong class="bold">long</strong> <strong class="bold">short-term</strong> <strong class="bold">memory</strong> (<strong class="bold">LSTM</strong>) networks, can be effectively trained to handle long <a id="_idIndexMarker1116"/>sequences. LSTM is usually the choice when it comes to handling partial observability in complex environments: it is used in OpenAI's Dota 2 and DeepMind's StarCraft II models, among many others.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Describing the full details of how RNNs and LSTMs work is beyond the scope of this chapter. If you want a resource to learn more about them, Christopher Olah's blog is the place to go: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</p>
			<p>When using RLlib, the LSTM layer can be enabled as follows, say while using PPO, followed by<a id="_idIndexMarker1117"/> some optional<a id="_idIndexMarker1118"/> hyperparameter changes over the default values:</p>
			<p class="source-code">import ray</p>
			<p class="source-code">from ray.tune.logger import pretty_print</p>
			<p class="source-code">from ray.rllib.agents.ppo.ppo import PPOTrainer</p>
			<p class="source-code">from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG</p>
			<p class="source-code">config = DEFAULT_CONFIG.copy()</p>
			<p class="source-code">config["model"]["use_lstm"] = True</p>
			<p class="source-code"># The length of the input sequence</p>
			<p class="source-code">config["model"]["max_seq_len"] = 8</p>
			<p class="source-code"># Size of the hidden state</p>
			<p class="source-code">config["model"]["lstm_cell_size"] = 64</p>
			<p class="source-code"># Whether to use</p>
			<p class="source-code">config["model"]["lstm_use_prev_action_reward"] = True</p>
			<p>Note that the input is first fed to the (preprocessing) "model" in RLlib, which is typically a sequence of fully connected layers. The output of the preprocessing is then passed to the LSTM layer.</p>
			<p>The fully connected layer hyperparameters can be similarly overwritten:</p>
			<p class="source-code">config["model"]["fcnet_hiddens"] = [32]</p>
			<p class="source-code">config["model"]["fcnet_activation"] = "linear"</p>
			<p>After specifying the environment within the config as a Gym environment name or your custom environment class, the config dictionary is then passed to the trainer class:</p>
			<p class="source-code">from ray.tune.registry import register_env</p>
			<p class="source-code">def env_creator(env_config):</p>
			<p class="source-code">    return MyEnv(env_config)    # return an env instance</p>
			<p class="source-code">register_env("my_env", env_creator)</p>
			<p class="source-code">config["env"] = "my_env"</p>
			<p class="source-code">ray.init()</p>
			<p class="source-code">trainer = PPOTrainer(config=config)</p>
			<p class="source-code">while True:</p>
			<p class="source-code">    results = trainer.train()</p>
			<p class="source-code">    print(pretty_print(results))</p>
			<p class="source-code">    if results["timesteps_total"] &gt;= MAX_STEPS:</p>
			<p class="source-code">        break</p>
			<p class="source-code">print(trainer.save())</p>
			<p>There are<a id="_idIndexMarker1119"/> couple things to keep in <a id="_idIndexMarker1120"/>mind when using an LSTM model as opposed to the simple stacking of observations:</p>
			<ul>
				<li>LSTM is often slower to train due to the sequential processing of a multi-step input.</li>
				<li>Training LSTM may require more data compared to a feedforward network.</li>
				<li>Your LSTM model could be more sensitive to hyperparameters, so you may have to do some hyperparameter tuning.</li>
			</ul>
			<p>Speaking of hyperparameters, here are some values to try if your training is not progressing well for an algorithm like PPO:</p>
			<ul>
				<li>Learning rate (<strong class="source-inline">config["lr"]</strong>): <img src="image/Formula_11_064.png" alt=""/>.</li>
				<li>LSTM cell size (<strong class="source-inline">config["model"]["lstm_cell_size"]</strong>): 64, 128, 256.</li>
				<li>Layer sharing between the value and policy network (<strong class="source-inline">config["vf_share_layers"]</strong>): Try to make this false if your episode rewards are in the hundreds or above to prevent the value function loss from dominating the policy loss. Alternatively, you can also reduce <strong class="source-inline">config["vf_loss_coeff"]</strong>.</li>
				<li>Entropy coefficient (<strong class="source-inline">config["entropy_coeff"]</strong>): <img src="image/Formula_11_065.png" alt=""/></li>
				<li>Passing reward and previous actions as input (<strong class="source-inline">config["model"]["lstm_use_prev_action_reward"]</strong>): Try to make this true to provide more information to the agent in addition to observations.</li>
				<li>Preprocessing <a id="_idIndexMarker1121"/>model<a id="_idIndexMarker1122"/> architecture (<strong class="source-inline">config["model"]["fcnet_hiddens"]</strong> and <strong class="source-inline">config["model"]["fcnet_activation"]</strong>): Try single linear layers.</li>
			</ul>
			<p>Hopefully, these will be helpful to come up with a good architecture for your model. </p>
			<p>Finally, let's discuss one of the most popular architectures: Transformer.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor257"/>Transformer architecture</h2>
			<p>Over the past <a id="_idIndexMarker1123"/>several<a id="_idIndexMarker1124"/> years, the Transformer architecture has essentially replaced RNNs in NLP applications. </p>
			<p>The Transformer architecture has several advantages over LSTM, the most used RNN type:</p>
			<ul>
				<li>An LSTM encoder packs all the information obtained from the input sequence into a single embedding layer that is then passed to the decoder. This creates a bottleneck between the encoder and the decoder. The Transformer model, however, allows the decoder to look at each of the elements of the input sequence (at their embeddings, to be precise).</li>
				<li>Since LSTM relies on backpropagation through time, the gradients are likely to explode or vanish throughout the update. The Transformer model, on the other hand, simultaneously looks at each of the input steps and does not run into a similar problem.</li>
				<li>As a result, Transformer models can effectively use much longer input sequences.</li>
			</ul>
			<p>For these reasons, Transformer is potentially a competitive alternative to RNNs for RL applications as well.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">A great tutorial on the Transformer architecture is by Jay Alammar, available at <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a>, if you would like to catch up on the topic.</p>
			<p>Despite the advantages<a id="_idIndexMarker1125"/> of the original Transformer model, it has been proven to be unstable in RL applications. An improvement has been proposed (<em class="italic">Parisotto et al., 2019</em>), named <strong class="bold">Gated Transformer-XL</strong> (<strong class="bold">GTrXL</strong>).</p>
			<p>RLlib has GTrXL implemented as a custom model. It can be used as follows:</p>
			<p class="source-code">...</p>
			<p class="source-code">from ray.rllib.models.tf.attention_net import GTrXLNet</p>
			<p class="source-code">...</p>
			<p class="source-code">config["model"] = {</p>
			<p class="source-code">    "custom_model": GTrXLNet,</p>
			<p class="source-code">    "max_seq_len": 50,</p>
			<p class="source-code">    "custom_model_config": {</p>
			<p class="source-code">        "num_transformer_units": 1,</p>
			<p class="source-code">        "attn_dim": 64,</p>
			<p class="source-code">        "num_heads": 2,</p>
			<p class="source-code">        "memory_tau": 50,</p>
			<p class="source-code">        "head_dim": 32,</p>
			<p class="source-code">        "ff_hidden_dim": 32,</p>
			<p class="source-code">    },</p>
			<p class="source-code">}</p>
			<p>This gives us another powerful architecture to try in RLlib.</p>
			<p>Congratulations! We have arrived at the end of the chapter! We have covered important topics<a id="_idIndexMarker1126"/> that deserve<a id="_idIndexMarker1127"/> more attention than what our limited space allows here. Go ahead and read the sources in the <em class="italic">References</em> section and experiment with the repos we introduced to deepen your understanding of the topic.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor258"/>Summary</h1>
			<p>In this chapter, we covered an important topic in RL: generalization and partial observability, which are key for real-world applications. Note that this is an active research area: keep our discussion here as suggestions and the first methods to try for your problem. New approaches come out periodically, so watch out for them. The important thing is you should always keep an eye on generalization and partial observability for a successful RL implementation outside of video games. In the next section, we will take our expedition to yet another advanced level with meta-learning. So, stay tuned!</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor259"/>References</h1>
			<ul>
				<li>Cobbe, K., Klimov, O., Hesse, C., Kim, T., &amp; Schulman, J. (2018). <em class="italic">Quantifying Generalization in Reinforcement Learning</em>: <a href="https://arxiv.org/abs/1812.02341">https://arxiv.org/abs/1812.02341</a></li>
				<li>Lee, K., Lee, K., Shin, J., &amp; Lee, H. (2020). <em class="italic">Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning</em>: <a href="https://arxiv.org/abs/1910.05396">https://arxiv.org/abs/1910.05396</a></li>
				<li>Rivlin, O. (2019, November 21). <em class="italic">Generalization in Deep Reinforcement Learning</em>. Retrieved from Towards Data Science: <a href="https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b">https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155b</a></li>
				<li>Rivlin, O. (2019). <em class="italic">Generalization in Deep Reinforcement Learning</em>. Retrieved from Towards Data Science: <a href="https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155">https://towardsdatascience.com/generalization-in-deep-reinforcement-learning-a14a240b155</a></li>
				<li>Cobbe, K., Klimov, O., Hesse, C., Kim, T., &amp; Schulman, J. (2018). <em class="italic">Quantifying Generalization in Reinforcement Learning</em>: <a href="https://arxiv.org/abs/1812.0234">https://arxiv.org/abs/1812.0234</a></li>
				<li>Lee, K., Lee, K., Shin, J., &amp; Lee, H. (2020). <em class="italic">Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning</em>: <a href="https://arxiv.org/abs/1910.0539">https://arxiv.org/abs/1910.0539</a></li>
				<li>Parisotto, E., et al. (2019). <em class="italic">Stabilizing Transformers for Reinforcement Learning</em>: <a href="http://arxiv.org/abs/1910.06764">http://arxiv.org/abs/1910.06764</a></li>
			</ul>
		</div>
	</body></html>