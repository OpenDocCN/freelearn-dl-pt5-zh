["```py\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib.colors import ListedColormap\n    ```", "```py\n    def tanh(x):\n        return (1.0 - np.exp(-2 * x)) / (1.0 + np.exp(-2 * x))\n    def tanh_derivative(x):\n        return (1 + tanh(x)) * (1 - tanh(x))\n    ```", "```py\n    class NeuralNetwork:\n        # net_arch consists of a list of integers, indicating\n        # the number of units in each layer\n        def __init__(self, net_arch):\n            self.activation_func = tanh\n            self.activation_derivative = tanh_derivative\n            self.layers = len(net_arch)\n            self.steps_per_epoch = 1000\n            self.net_arch = net_arch\n            # initialize the weights with random values in the range (-1,1)\n            self.weights = []\n            for layer in range(len(net_arch) - 1):\n                w = 2 * np.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1\n                self.weights.append(w)\n    ```", "```py\n    def fit(self, data, labels, learning_rate=0.1, epochs=10):\n    ```", "```py\n    bias = np.ones((1, data.shape[0]))\n    input_data = np.concatenate((bias.T, data), axis=1)\n    ```", "```py\n    for k in range(epochs * self.steps_per_epoch):\n    ```", "```py\n    print('epochs: {}'.format(k / self.steps_per_epoch))\n    for s in data:\n        print(s, nn.predict(s))\n    ```", "```py\n    sample = np.random.randint(data.shape[0])\n    y = [input_data[sample]]\n    for i in range(len(self.weights) - 1):\n        activation = np.dot(y[i], self.weights[i])\n        activation_f = self.activation_func(activation)\n        # add the bias for the next layer\n        activation_f = np.concatenate((np.ones(1),\n            np.array(activation_f)))\n        y.append(activation_f)\n    ```", "```py\n    # last layer\n    activation = np.dot(y[-1], self.weights[-1])\n    activation_f = self.activation_func(activation)\n    y.append(activation_f)\n    # error for the output layer\n    error = y[-1] - labels[sample]\n    delta_vec = [error * self.activation_derivative(y[-1])]\n    ```", "```py\n    # we need to begin from the back from the next to last layer\n    for i in range(self.layers - 2, 0, -1):\n        error = delta_vec[-1].dot(self.weights[i][1:].T)\n        error = error * self.activation_derivative(y[i][1:])\n        delta_vec.append(error)\n    # reverse\n    # [level3(output)->level2(hidden)] => [level2(hidden)->level3(output)]\n    delta_vec.reverse()\n    ```", "```py\n    for i in range(len(self.weights)):\n        layer = y[i].reshape(1, nn.net_arch[i] + 1)\n        delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])\n        self.weights[i] -= learning_rate * layer.T.dot(delta)\n    ```", "```py\n    def predict(self, x):\n        val = np.concatenate((np.ones(1).T, np.array(x)))\n        for i in range(0, len(self.weights)):\n            val = self.activation_func(\n                np.dot(val, self.weights[i]))\n            al = np.concatenate((np.ones(1).T,\n                np.array(val)))\n        return val[1]\n    ```", "```py\n    def plot_decision_regions(self, X, y, points=200):\n        markers = ('o', '^')\n        colors = ('red', 'blue')\n        cmap = ListedColormap(colors)\n        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)\n        xx1, xx2 = np.meshgrid(np.arange(x1_min,\n            x1_max, resolution),\n            np.arange(x2_min, x2_max, resolution))\n        input = np.array([xx1.ravel(), xx2.ravel()]).T\n        Z = np.empty(0)\n        for i in range(input.shape[0]):\n            val = nn.predict(np.array(input[i]))\n            if val < 0.5:\n                val = 0\n            if val >= 0.5:\n                val = 1\n            Z = np.append(Z, val)\n        Z = Z.reshape(xx1.shape)\n        plt.pcolormesh(xx1, xx2, Z, cmap=cmap)\n        plt.xlim(xx1.min(), xx1.max())\n        plt.ylim(xx2.min(), xx2.max())\n        # plot all samples\n        classes = [\"False\", \"True\"]\n        for idx, cl in enumerate(np.unique(y)):\n            plt.scatter(x=X[y == cl, 0],\n                y=X[y == cl, 1],\n                alpha=1.0,\n                c=colors[idx],\n                edgecolors='black',\n                marker=markers[idx],\n                s=80,\n                label=classes[idx])\n        plt.xlabel('x1)\n        plt.ylabel('x2')\n        plt.legend(loc='upper left')\n        plt.show()\n    ```", "```py\n    np.random.seed(0)\n    # Initialize the NeuralNetwork with 2 input, 2 hidden, and 1 output units\n    nn = NeuralNetwork([2, 2, 1])\n    X = np.array([[0, 0],\n                  [0, 1],\n                  [1, 0],\n                  [1, 1]])\n    y = np.array([0, 1, 1, 0])\n    nn.fit(X, y, epochs=10)\n    print(\"Final prediction\")\n    for s in X:\n        print(s, nn.predict(s))\n    nn.plot_decision_regions(X, y)\n    ```"]